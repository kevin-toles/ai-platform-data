{
  "metadata": {
    "title": "The Practice of Cloud System Administration - Tom Limoncelli",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 559,
    "conversion_date": "2025-12-19T17:49:04.398918",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "The Practice of Cloud System Administration - Tom Limoncelli.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\nThe Practice of Cloud\nSystem Administration\nVolume 2\nThomas A. Limoncelli\nStrata R. Chalup\nChristina J. Hogan\nUpper Saddle River, NJ • Boston • Indianapolis • San Francisco\nNew York • Toronto • Montreal • London • Munich • Paris • Madrid\nCapetown • Sydney • Tokyo • Singapore • Mexico City\nDevOps and SRE Practices for\nWeb Services\n\n\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed\nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nThe authors and publisher have taken care in the preparation of this book, but make no expressed\nor implied warranty of any kind and assume no responsibility for errors or omissions. No liability is\nassumed for incidental or consequential damages in connection with or arising out of the use of the\ninformation or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales opportunities (which may\ninclude electronic versions; custom cover designs; and content particular to your business, training\ngoals, marketing focus, or branding interests), please contact our corporate sales department at corp-\nsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the United States, please contact intlcs@pearsoned.com.\nVisit us on the Web: informit.com/aw\nLibrary of Congress Cataloging-in-Publication Data\nLimoncelli, Tom.\nThe practice of cloud system administration : designing and operating large distributed systems /\nThomas A. Limoncelli, Strata R. Chalup, Christina J. Hogan.\nvolumes\ncm\nIncludes bibliographical references and index.\nISBN-13: 978-0-321-94318-7 (volume 2 : paperback)\nISBN-10: 0-321-94318-X (volume 2 : paperback)\n1. Computer networks—Management. 2. Computer systems. 3. Cloud computing. 4. Electronic data\nprocessing—Distributed processing. I. Chalup, Strata R. II. Hogan, Christina J. III. Title.\nTK5105.5.L529 2015\n004.67’82068—dc23\n2014024033\nCopyright © 2015 Thomas A. Limoncelli, Virtual.NET Inc., Christina J. Lear née Hogan\nAll rights reserved. Printed in the United States of America. This publication is protected by copyright,\nand permission must be obtained from the publisher prior to any prohibited reproduction, storage in a\nretrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying,\nrecording, or likewise. To obtain permission to use material from this work, please submit a written\nrequest to Pearson Education, Inc., Permissions Department, 200 Old Tappan Road, Old Tappan, New\nJersey 07675, or you may fax your request to (201) 236-3290.\nISBN-13: 978-0-321-94318-7\nISBN-10: 0-321-94318-X\n3\n17\n\n\nContents at a Glance\nContents\nvii\nPreface\nxxiii\nAbout the Authors\nxxix\nIntroduction\n1\nPart I\nDesign: Building It\n7\nChapter 1\nDesigning in a Distributed World\n9\nChapter 2\nDesigning for Operations\n31\nChapter 3\nSelecting a Service Platform\n51\nChapter 4\nApplication Architectures\n69\nChapter 5\nDesign Patterns for Scaling\n95\nChapter 6\nDesign Patterns for Resiliency\n119\nPart II\nOperations: Running It\n145\nChapter 7\nOperations in a Distributed World\n147\nChapter 8\nDevOps Culture\n171\nChapter 9\nService Delivery: The Build Phase\n195\nChapter 10 Service Delivery: The Deployment Phase\n211\nChapter 11 Upgrading Live Services\n225\nChapter 12 Automation\n243\nChapter 13 Design Documents\n275\nChapter 14 Oncall\n285\nChapter 15 Disaster Preparedness\n307\nChapter 16 Monitoring Fundamentals\n331\nv\n\n\nvi\nContents at a Glance\nChapter 17 Monitoring Architecture and Practice\n345\nChapter 18 Capacity Planning\n365\nChapter 19 Creating KPIs\n387\nChapter 20 Operational Excellence\n401\nEpilogue\n417\nPart III\nAppendices\n419\nAppendix A Assessments\n421\nAppendix B The Origins and Future of Distributed Computing\nand Clouds\n451\nAppendix C Scaling Terminology and Concepts\n475\nAppendix D Templates and Examples\n481\nAppendix E Recommended Reading\n487\nBibliography\n491\nIndex\n499\n\n\nContents\nPreface\nxxiii\nAbout the Authors\nxxix\nIntroduction\n1\nPart I\nDesign: Building It\n7\n1\nDesigning in a Distributed World\n9\n1.1\nVisibility at Scale\n10\n1.2\nThe Importance of Simplicity\n11\n1.3\nComposition\n12\n1.3.1\nLoad Balancer with Multiple Backend Replicas\n12\n1.3.2\nServer with Multiple Backends\n14\n1.3.3\nServer Tree\n16\n1.4\nDistributed State\n17\n1.5\nThe CAP Principle\n21\n1.5.1\nConsistency\n21\n1.5.2\nAvailability\n21\n1.5.3\nPartition Tolerance\n22\n1.6\nLoosely Coupled Systems\n24\n1.7\nSpeed\n26\n1.8\nSummary\n29\nExercises\n30\nvii\n\n\nviii\nContents\n2\nDesigning for Operations\n31\n2.1\nOperational Requirements\n31\n2.1.1\nConﬁguration\n33\n2.1.2\nStartup and Shutdown\n34\n2.1.3\nQueue Draining\n35\n2.1.4\nSoftware Upgrades\n36\n2.1.5\nBackups and Restores\n36\n2.1.6\nRedundancy\n37\n2.1.7\nReplicated Databases\n37\n2.1.8\nHot Swaps\n38\n2.1.9\nToggles for Individual Features\n39\n2.1.10\nGraceful Degradation\n39\n2.1.11\nAccess Controls and Rate Limits\n40\n2.1.12\nData Import Controls\n41\n2.1.13\nMonitoring\n42\n2.1.14\nAuditing\n42\n2.1.15\nDebug Instrumentation\n43\n2.1.16\nException Collection\n43\n2.1.17\nDocumentation for Operations\n44\n2.2\nImplementing Design for Operations\n45\n2.2.1\nBuild Features in from the Beginning\n45\n2.2.2\nRequest Features as They Are Identiﬁed\n46\n2.2.3\nWrite the Features Yourself\n47\n2.2.4\nWork with a Third-Party Vendor\n48\n2.3\nImproving the Model\n48\n2.4\nSummary\n49\nExercises\n50\n3\nSelecting a Service Platform\n51\n3.1\nLevel of Service Abstraction\n52\n3.1.1\nInfrastructure as a Service\n52\n3.1.2\nPlatform as a Service\n54\n3.1.3\nSoftware as a Service\n55\n3.2\nType of Machine\n56\n3.2.1\nPhysical Machines\n57\n3.2.2\nVirtual Machines\n57\n3.2.3\nContainers\n60\n\n\nContents\nix\n3.3\nLevel of Resource Sharing\n62\n3.3.1\nCompliance\n63\n3.3.2\nPrivacy\n63\n3.3.3\nCost\n63\n3.3.4\nControl\n64\n3.4\nColocation\n65\n3.5\nSelection Strategies\n66\n3.6\nSummary\n68\nExercises\n68\n4\nApplication Architectures\n69\n4.1\nSingle-Machine Web Server\n70\n4.2\nThree-Tier Web Service\n71\n4.2.1\nLoad Balancer Types\n72\n4.2.2\nLoad Balancing Methods\n74\n4.2.3\nLoad Balancing with Shared State\n75\n4.2.4\nUser Identity\n76\n4.2.5\nScaling\n76\n4.3\nFour-Tier Web Service\n77\n4.3.1\nFrontends\n78\n4.3.2\nApplication Servers\n79\n4.3.3\nConﬁguration Options\n80\n4.4\nReverse Proxy Service\n80\n4.5\nCloud-Scale Service\n80\n4.5.1\nGlobal Load Balancer\n81\n4.5.2\nGlobal Load Balancing Methods\n82\n4.5.3\nGlobal Load Balancing with User-Speciﬁc Data\n82\n4.5.4\nInternal Backbone\n83\n4.6\nMessage Bus Architectures\n85\n4.6.1\nMessage Bus Designs\n86\n4.6.2\nMessage Bus Reliability\n87\n4.6.3\nExample 1: Link-Shortening Site\n87\n4.6.4\nExample 2: Employee Human Resources Data Updates\n89\n4.7\nService-Oriented Architecture\n90\n4.7.1\nFlexibility\n91\n4.7.2\nSupport\n91\n4.7.3\nBest Practices\n91\n",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "x\nContents\n4.8\nSummary\n92\nExercises\n93\n5\nDesign Patterns for Scaling\n95\n5.1\nGeneral Strategy\n96\n5.1.1\nIdentify Bottlenecks\n96\n5.1.2\nReengineer Components\n97\n5.1.3\nMeasure Results\n97\n5.1.4\nBe Proactive\n97\n5.2\nScaling Up\n98\n5.3\nThe AKF Scaling Cube\n99\n5.3.1\nx: Horizontal Duplication\n99\n5.3.2\ny: Functional or Service Splits\n101\n5.3.3\nz: Lookup-Oriented Split\n102\n5.3.4\nCombinations\n104\n5.4\nCaching\n104\n5.4.1\nCache Effectiveness\n105\n5.4.2\nCache Placement\n106\n5.4.3\nCache Persistence\n106\n5.4.4\nCache Replacement Algorithms\n107\n5.4.5\nCache Entry Invalidation\n108\n5.4.6\nCache Size\n109\n5.5\nData Sharding\n110\n5.6\nThreading\n112\n5.7\nQueueing\n113\n5.7.1\nBeneﬁts\n113\n5.7.2\nVariations\n113\n5.8\nContent Delivery Networks\n114\n5.9\nSummary\n116\nExercises\n116\n6\nDesign Patterns for Resiliency\n119\n6.1\nSoftware Resiliency Beats Hardware Reliability\n120\n6.2\nEverything Malfunctions Eventually\n121\n6.2.1\nMTBF in Distributed Systems\n121\n6.2.2\nThe Traditional Approach\n122\n6.2.3\nThe Distributed Computing Approach\n123\n\n\nContents\nxi\n6.3\nResiliency through Spare Capacity\n124\n6.3.1\nHow Much Spare Capacity\n125\n6.3.2\nLoad Sharing versus Hot Spares\n126\n6.4\nFailure Domains\n126\n6.5\nSoftware Failures\n128\n6.5.1\nSoftware Crashes\n128\n6.5.2\nSoftware Hangs\n129\n6.5.3\nQuery of Death\n130\n6.6\nPhysical Failures\n131\n6.6.1\nParts and Components\n131\n6.6.2\nMachines\n134\n6.6.3\nLoad Balancers\n134\n6.6.4\nRacks\n136\n6.6.5\nDatacenters\n137\n6.7\nOverload Failures\n138\n6.7.1\nTrafﬁc Surges\n138\n6.7.2\nDoS and DDoS Attacks\n140\n6.7.3\nScraping Attacks\n140\n6.8\nHuman Error\n141\n6.9\nSummary\n142\nExercises\n143\nPart II\nOperations: Running It\n145\n7\nOperations in a Distributed World\n147\n7.1\nDistributed Systems Operations\n148\n7.1.1\nSRE versus Traditional Enterprise IT\n148\n7.1.2\nChange versus Stability\n149\n7.1.3\nDeﬁning SRE\n151\n7.1.4\nOperations at Scale\n152\n7.2\nService Life Cycle\n155\n7.2.1\nService Launches\n156\n7.2.2\nService Decommissioning\n160\n7.3\nOrganizing Strategy for Operational Teams\n160\n7.3.1\nTeam Member Day Types\n162\n7.3.2\nOther Strategies\n165\n\n\nxii\nContents\n7.4\nVirtual Ofﬁce\n166\n7.4.1\nCommunication Mechanisms\n166\n7.4.2\nCommunication Policies\n167\n7.5\nSummary\n167\nExercises\n168\n8\nDevOps Culture\n171\n8.1\nWhat Is DevOps?\n172\n8.1.1\nThe Traditional Approach\n173\n8.1.2\nThe DevOps Approach\n175\n8.2\nThe Three Ways of DevOps\n176\n8.2.1\nThe First Way: Workﬂow\n176\n8.2.2\nThe Second Way: Improve Feedback\n177\n8.2.3\nThe Third Way: Continual Experimentation and\nLearning\n178\n8.2.4\nSmall Batches Are Better\n178\n8.2.5\nAdopting the Strategies\n179\n8.3\nHistory of DevOps\n180\n8.3.1\nEvolution\n180\n8.3.2\nSite Reliability Engineering\n181\n8.4\nDevOps Values and Principles\n181\n8.4.1\nRelationships\n182\n8.4.2\nIntegration\n182\n8.4.3\nAutomation\n182\n8.4.4\nContinuous Improvement\n183\n8.4.5\nCommon Nontechnical DevOps Practices\n183\n8.4.6\nCommon Technical DevOps Practices\n184\n8.4.7\nRelease Engineering DevOps Practices\n186\n8.5\nConverting to DevOps\n186\n8.5.1\nGetting Started\n187\n8.5.2\nDevOps at the Business Level\n187\n8.6\nAgile and Continuous Delivery\n188\n8.6.1\nWhat Is Agile?\n188\n8.6.2\nWhat Is Continuous Delivery?\n189\n8.7\nSummary\n192\nExercises\n193\n\n\nContents\nxiii\n9\nService Delivery: The Build Phase\n195\n9.1\nService Delivery Strategies\n197\n9.1.1\nPattern: Modern DevOps Methodology\n197\n9.1.2\nAnti-pattern: Waterfall Methodology\n199\n9.2\nThe Virtuous Cycle of Quality\n200\n9.3\nBuild-Phase Steps\n202\n9.3.1\nDevelop\n202\n9.3.2\nCommit\n202\n9.3.3\nBuild\n203\n9.3.4\nPackage\n204\n9.3.5\nRegister\n204\n9.4\nBuild Console\n205\n9.5\nContinuous Integration\n205\n9.6\nPackages as Handoff Interface\n207\n9.7\nSummary\n208\nExercises\n209\n10\nService Delivery: The Deployment Phase\n211\n10.1\nDeployment-Phase Steps\n211\n10.1.1\nPromotion\n212\n10.1.2\nInstallation\n212\n10.1.3\nConﬁguration\n213\n10.2\nTesting and Approval\n214\n10.2.1\nTesting\n215\n10.2.2\nApproval\n216\n10.3\nOperations Console\n217\n10.4\nInfrastructure Automation Strategies\n217\n10.4.1\nPreparing Physical Machines\n217\n10.4.2\nPreparing Virtual Machines\n218\n10.4.3\nInstalling OS and Services\n219\n10.5\nContinuous Delivery\n221\n10.6\nInfrastructure as Code\n221\n10.7\nOther Platform Services\n222\n10.8\nSummary\n222\nExercises\n223\n\n\nxiv\nContents\n11\nUpgrading Live Services\n225\n11.1\nTaking the Service Down for Upgrading\n225\n11.2\nRolling Upgrades\n226\n11.3\nCanary\n227\n11.4\nPhased Roll-outs\n229\n11.5\nProportional Shedding\n230\n11.6\nBlue-Green Deployment\n230\n11.7\nToggling Features\n230\n11.8\nLive Schema Changes\n234\n11.9\nLive Code Changes\n236\n11.10\nContinuous Deployment\n236\n11.11\nDealing with Failed Code Pushes\n239\n11.12\nRelease Atomicity\n240\n11.13\nSummary\n241\nExercises\n241\n12\nAutomation\n243\n12.1\nApproaches to Automation\n244\n12.1.1\nThe Left-Over Principle\n245\n12.1.2\nThe Compensatory Principle\n246\n12.1.3\nThe Complementarity Principle\n247\n12.1.4\nAutomation for System Administration\n248\n12.1.5\nLessons Learned\n249\n12.2\nTool Building versus Automation\n250\n12.2.1\nExample: Auto Manufacturing\n251\n12.2.2\nExample: Machine Conﬁguration\n251\n12.2.3\nExample: Account Creation\n251\n12.2.4\nTools Are Good, But Automation Is Better\n252\n12.3\nGoals of Automation\n252\n12.4\nCreating Automation\n255\n12.4.1\nMaking Time to Automate\n256\n12.4.2\nReducing Toil\n257\n12.4.3\nDetermining What to Automate First\n257\n12.5\nHow to Automate\n258\n12.6\nLanguage Tools\n258\n12.6.1\nShell Scripting Languages\n259\n12.6.2\nScripting Languages\n259\n\n\nContents\nxv\n12.6.3\nCompiled Languages\n260\n12.6.4\nConﬁguration Management Languages\n260\n12.7\nSoftware Engineering Tools and Techniques\n262\n12.7.1\nIssue Tracking Systems\n263\n12.7.2\nVersion Control Systems\n265\n12.7.3\nSoftware Packaging\n266\n12.7.4\nStyle Guides\n266\n12.7.5\nTest-Driven Development\n267\n12.7.6\nCode Reviews\n268\n12.7.7\nWriting Just Enough Code\n269\n12.8\nMultitenant Systems\n270\n12.9\nSummary\n271\nExercises\n272\n13\nDesign Documents\n275\n13.1\nDesign Documents Overview\n275\n13.1.1\nDocumenting Changes and Rationale\n276\n13.1.2\nDocumentation as a Repository of Past\nDecisions\n276\n13.2\nDesign Document Anatomy\n277\n13.3\nTemplate\n279\n13.4\nDocument Archive\n279\n13.5\nReview Workﬂows\n280\n13.5.1\nReviewers and Approvers\n281\n13.5.2\nAchieving Sign-off\n281\n13.6\nAdopting Design Documents\n282\n13.7\nSummary\n283\nExercises\n284\n14\nOncall\n285\n14.1\nDesigning Oncall\n285\n14.1.1\nStart with the SLA\n286\n14.1.2\nOncall Roster\n287\n14.1.3\nOnduty\n288\n14.1.4\nOncall Schedule Design\n288\n14.1.5\nThe Oncall Calendar\n290\n14.1.6\nOncall Frequency\n291\n\n\nxvi\nContents\n14.1.7\nTypes of Notiﬁcations\n292\n14.1.8\nAfter-Hours Maintenance Coordination\n294\n14.2\nBeing Oncall\n294\n14.2.1\nPre-shift Responsibilities\n294\n14.2.2\nRegular Oncall Responsibilities\n294\n14.2.3\nAlert Responsibilities\n295\n14.2.4\nObserve, Orient, Decide, Act (OODA)\n296\n14.2.5\nOncall Playbook\n297\n14.2.6\nThird-Party Escalation\n298\n14.2.7\nEnd-of-Shift Responsibilities\n299\n14.3\nBetween Oncall Shifts\n299\n14.3.1\nLong-Term Fixes\n299\n14.3.2\nPostmortems\n300\n14.4\nPeriodic Review of Alerts\n302\n14.5\nBeing Paged Too Much\n304\n14.6\nSummary\n305\nExercises\n306\n15\nDisaster Preparedness\n307\n15.1\nMindset\n308\n15.1.1\nAntifragile Systems\n308\n15.1.2\nReducing Risk\n309\n15.2\nIndividual Training: Wheel of Misfortune\n311\n15.3\nTeam Training: Fire Drills\n312\n15.3.1\nService Testing\n313\n15.3.2\nRandom Testing\n314\n15.4\nTraining for Organizations: Game Day/DiRT\n315\n15.4.1\nGetting Started\n316\n15.4.2\nIncreasing Scope\n317\n15.4.3\nImplementation and Logistics\n318\n15.4.4\nExperiencing a DiRT Test\n320\n15.5\nIncident Command System\n323\n15.5.1\nHow It Works: Public Safety Arena\n325\n15.5.2\nHow It Works: IT Operations Arena\n326\n15.5.3\nIncident Action Plan\n326\n15.5.4\nBest Practices\n327\n15.5.5\nICS Example\n328\n\n\nContents\nxvii\n15.6\nSummary\n329\nExercises\n330\n16\nMonitoring Fundamentals\n331\n16.1\nOverview\n332\n16.1.1\nUses of Monitoring\n333\n16.1.2\nService Management\n334\n16.2\nConsumers of Monitoring Information\n334\n16.3\nWhat to Monitor\n336\n16.4\nRetention\n338\n16.5\nMeta-monitoring\n339\n16.6\nLogs\n340\n16.6.1\nApproach\n341\n16.6.2\nTimestamps\n341\n16.7\nSummary\n342\nExercises\n342\n17\nMonitoring Architecture and Practice\n345\n17.1\nSensing and Measurement\n346\n17.1.1\nBlackbox versus Whitebox Monitoring\n346\n17.1.2\nDirect versus Synthesized Measurements\n347\n17.1.3\nRate versus Capability Monitoring\n348\n17.1.4\nGauges versus Counters\n348\n17.2\nCollection\n350\n17.2.1\nPush versus Pull\n350\n17.2.2\nProtocol Selection\n351\n17.2.3\nServer Component versus Agent versus Poller\n352\n17.2.4\nCentral versus Regional Collectors\n352\n17.3\nAnalysis and Computation\n353\n17.4\nAlerting and Escalation Manager\n354\n17.4.1\nAlerting, Escalation, and Acknowledgments\n355\n17.4.2\nSilence versus Inhibit\n356\n17.5\nVisualization\n358\n17.5.1\nPercentiles\n359\n17.5.2\nStack Ranking\n360\n17.5.3\nHistograms\n361\n17.6\nStorage\n362\n",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-25)",
      "start_page": 18,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "xviii\nContents\n17.7\nConﬁguration\n362\n17.8\nSummary\n363\nExercises\n364\n18\nCapacity Planning\n365\n18.1\nStandard Capacity Planning\n366\n18.1.1\nCurrent Usage\n368\n18.1.2\nNormal Growth\n369\n18.1.3\nPlanned Growth\n369\n18.1.4\nHeadroom\n370\n18.1.5\nResiliency\n370\n18.1.6\nTimetable\n371\n18.2\nAdvanced Capacity Planning\n371\n18.2.1\nIdentifying Your Primary Resources\n372\n18.2.2\nKnowing Your Capacity Limits\n372\n18.2.3\nIdentifying Your Core Drivers\n373\n18.2.4\nMeasuring Engagement\n374\n18.2.5\nAnalyzing the Data\n375\n18.2.6\nMonitoring the Key Indicators\n380\n18.2.7\nDelegating Capacity Planning\n381\n18.3\nResource Regression\n381\n18.4\nLaunching New Services\n382\n18.5\nReduce Provisioning Time\n384\n18.6\nSummary\n385\nExercises\n386\n19\nCreating KPIs\n387\n19.1\nWhat Is a KPI?\n388\n19.2\nCreating KPIs\n389\n19.2.1\nStep 1: Envision the Ideal\n390\n19.2.2\nStep 2: Quantify Distance to the Ideal\n390\n19.2.3\nStep 3: Imagine How Behavior Will Change\n390\n19.2.4\nStep 4: Revise and Select\n391\n19.2.5\nStep 5: Deploy the KPI\n392\n19.3\nExample KPI: Machine Allocation\n393\n19.3.1\nThe First Pass\n393\n\n\nContents\nxix\n19.3.2\nThe Second Pass\n394\n19.3.3\nEvaluating the KPI\n396\n19.4\nCase Study: Error Budget\n396\n19.4.1\nConﬂicting Goals\n396\n19.4.2\nA Uniﬁed Goal\n397\n19.4.3\nEveryone Beneﬁts\n398\n19.5\nSummary\n399\nExercises\n399\n20\nOperational Excellence\n401\n20.1\nWhat Does Operational Excellence Look Like?\n401\n20.2\nHow to Measure Greatness\n402\n20.3\nAssessment Methodology\n403\n20.3.1\nOperational Responsibilities\n403\n20.3.2\nAssessment Levels\n405\n20.3.3\nAssessment Questions and Look-For’s\n407\n20.4\nService Assessments\n407\n20.4.1\nIdentifying What to Assess\n408\n20.4.2\nAssessing Each Service\n408\n20.4.3\nComparing Results across Services\n409\n20.4.4\nActing on the Results\n410\n20.4.5\nAssessment and Project Planning Frequencies\n410\n20.5\nOrganizational Assessments\n411\n20.6\nLevels of Improvement\n412\n20.7\nGetting Started\n413\n20.8\nSummary\n414\nExercises\n415\nEpilogue\n416\nPart III\nAppendices\n419\nA\nAssessments\n421\nA.1\nRegular Tasks (RT)\n423\nA.2\nEmergency Response (ER)\n426\nA.3\nMonitoring and Metrics (MM)\n428\n\n\nxx\nContents\nA.4\nCapacity Planning (CP)\n431\nA.5\nChange Management (CM)\n433\nA.6\nNew Product Introduction and Removal (NPI/NPR)\n435\nA.7\nService Deployment and Decommissioning (SDD)\n437\nA.8\nPerformance and Efﬁciency (PE)\n439\nA.9\nService Delivery: The Build Phase\n442\nA.10\nService Delivery: The Deployment Phase\n444\nA.11\nToil Reduction\n446\nA.12\nDisaster Preparedness\n448\nB\nThe Origins and Future of Distributed Computing and Clouds\n451\nB.1\nThe Pre-Web Era (1985–1994)\n452\nAvailability Requirements\n452\nTechnology\n453\nScaling\n454\nHigh Availability\n454\nCosts\n454\nB.2\nThe First Web Era: The Bubble (1995–2000)\n455\nAvailability Requirements\n455\nTechnology\n455\nScaling\n456\nHigh Availability\n457\nCosts\n459\nB.3\nThe Dot-Bomb Era (2000–2003)\n459\nAvailability Requirements\n460\nTechnology\n460\nHigh Availability\n461\nScaling\n462\nCosts\n464\nB.4\nThe Second Web Era (2003–2010)\n465\nAvailability Requirements\n465\nTechnology\n465\nHigh Availability\n466\nScaling\n467\nCosts\n468\n\n\nContents\nxxi\nB.5\nThe Cloud Computing Era (2010–present)\n469\nAvailability Requirements\n469\nCosts\n469\nScaling and High Availability\n471\nTechnology\n472\nB.6\nConclusion\n472\nExercises\n473\nC\nScaling Terminology and Concepts\n475\nC.1\nConstant, Linear, and Exponential Scaling\n475\nC.2\nBig O Notation\n476\nC.3\nLimitations of Big O Notation\n478\nD\nTemplates and Examples\n481\nD.1\nDesign Document Template\n481\nD.2\nDesign Document Example\n482\nD.3\nSample Postmortem Template\n484\nE\nRecommended Reading\n487\nBibliography\n491\nIndex\n499\n\n\nThis page intentionally left blank \n\n\nPreface\nWhich of the following statements are true?\n1. The most reliable systems are built using cheap, unreliable components.\n2. The techniques that Google uses to scale to billions of users follow the same\npatterns you can use to scale a system that handles hundreds of users.\n3. The more risky a procedure is, the more you should do it.\n4. Some of the most important software features are the ones that users never see.\n5. You should pick random machines and power them off.\n6. The code for every feature Facebook will announce in the next six months is\nprobably in your browser already.\n7. Updating software multiple times a day requires little human effort.\n8. Being oncall doesn’t have to be a stressful, painful experience.\n9. You shouldn’t monitor whether machines are up.\n10. Operations and management can be conducted using the scientiﬁc principles\nof experimentation and evidence.\n11. Google has rehearsed what it would do in case of a zombie attack.\nAll of these statements are true. By the time you ﬁnish reading this book, you’ll\nknow why.\nThis is a book about building and running cloud-based services on a large\nscale: internet-based services for millions or billions of users. That said, every day\nmore and more enterprises are adopting these techniques. Therefore, this is a book\nfor everyone.\nThe intended audience is system administrators and their managers. We do\nnot assume a background in computer science, but we do assume experience with\nUNIX/Linux system administration, networking, and operating system concepts.\nOur focus is on building and operating the services that make up the cloud,\nnot a guide to using cloud-based services.\nxxiii\n\n\nxxiv\nPreface\nCloud services must be available, fast, and secure. At cloud scale, this is a\nunique engineering feat. Therefore cloud-scale services are engineered differently\nthan your typical enterprise service. Being available is important because the\nInternet is open 24 × 7 and has users in every time zone. Being fast is important\nbecause users are frustrated by slow services, so slow services lose out to faster\nrivals. Being secure is important because, as caretakers of other people’s data, we\nare duty-bound (and legally responsible) to protect people’s data.\nThese requirements are intermixed. If a site is not secure, by deﬁnition, it\ncannot be made reliable. If a site is not fast, it is not sufﬁciently available. If a site\nis down, by deﬁnition, it is not fast.\nThe most visible cloud-scale services are web sites. However, there is a\nhuge ecosystem of invisible internet-accessible services that are not accessed with\na browser. For example, smartphone apps use API calls to access cloud-based\nservices.\nFor the remainder of this book we will tend to use the term “distributed com-\nputing” rather than “cloud computing.” Cloud computing is a marketing term that\nmeans different things to different people. Distributed computing describes an archi-\ntecture where applications and services are provided using many machines rather\nthan one.\nThis is a book of fundamental principles and practices that are timeless.\nTherefore we don’t make recommendations about which speciﬁc products or tech-\nnologies to use. We could provide a comparison of the top ﬁve most popular web\nservers or NoSQL databases or continuous build systems. If we did, then this book\nwould be out of date the moment it is published. Instead, we discuss the quali-\nties one should look for when selecting such things. We provide a model to work\nfrom. This approach is intended to prepare you for a long career where technology\nchanges over time but you are always prepared. We will, of course, illustrate our\npoints with speciﬁc technologies and products, but not as an endorsement of those\nproducts and services.\nThis book is, at times, idealistic. This is deliberate. We set out to give the reader\na vision of how things can be, what to strive for. We are here to raise the bar.\nAbout This Book\nThe book is structured in two parts, Design and Operations.\nPart I captures our thinking on the design of large, complex, cloud-based dis-\ntributed computing systems. After the Introduction, we tackle each element of\ndesign from the bottom layers to the top. We cover distributed systems from the\npoint of view of a system administrator, not a computer scientist. To operate a\nsystem, one must be able to understand its internals.\n\n\nPreface\nxxv\nPart II describes how to run such systems. The ﬁrst chapters cover the most\nfundamental issues. Later chapters delve into more esoteric technical activities,\nthen high-level planning and strategy that tie together all of the above.\nAt the end is extra material including an assessment system for operations\nteams, a highly biased history of distributed computing, templates for forms\nmentioned in the text, recommended reading, and other reference material.\nWe’re excited to present a new feature of our book series: our operational\nassessment system. This system consists of a series of assessments you can use\nto evaluate your operations and ﬁnd areas of improvement. The assessment ques-\ntions and “Look For” recommendations are found in Appendix A. Chapter 20 is\nthe instruction manual.\nAcknowledgments\nThis book wouldn’t have been possible without the help and feedback we received\nfrom our community and people all over the world. The DevOps community was\ngenerous in its assistance.\nFirst, we’d like to thank our spouses and families: Christine Polk, Mike\nChalup, and Eliot and Joanna Lear. Your love and patience make all this possible.\nIf we have seen further, it is by standing on the shoulders of giants. Certain\nchapters relied heavily on support and advice from particular people: John Looney\nand Cian Synnott (Chapter 1); Marty Abbott and Michael Fisher (Chapter 5);\nDamon Edwards, Alex Honor, and Jez Humble (Chapters 9 and 10); John Allspaw\n(Chapter 12); Brent Chapman (Chapter 15); Caskey Dickson and Theo Schlossna-\ngle (Chapters 16 and 17); Arun Kejariwal and Bruce Yan (Chapter 18); Benjamin\nTreynor Sloss (Chapter 19); and Geoff Halprin (Chapter 20 and Appendix A).\nThanks to Gene Kim for the “strategic” inspiration and encouragement.\nDozens of people helped us—some by supplying anecdotes, some by review-\ning parts of or the entire book. The only fair way to thank them all is alphabetically\nand to apologize in advance to anyone we left out: Thomas Baden, George Beech,\nRaymond Blum, Kyle Brandt, Mark Burgess, Nick Craver, Geoff Dalgas, Robert\nP. J. Day, Patrick Debois, Bill Duane, Paul Evans, David Fullerton, Tom Geller, Peter\nGrace, Elizabeth Hamon Reid, Jim Hickstein, Zachary Hueras, Matt Jones, Jennifer\nJoy, Jimmy Kaplowitz, Daniel V. Klein, Steven Levine, Cory Lueninghoener, Shane\nMadden, Jim Maurer, Stephen McHenry, Dinah McNutt, Scott Hazen Mueller,\nSteve Murawski, Mohit Muthanna, Lenny Rachitsky, Amy Rich, Adele Shakal,\nBart Silverstrim, Josh Simon, Joel Spolsky, Desiree Sylvester, Win Treese, Todd\nUnderwood, Nicole Forsgren Velasquez, and Dave Zwieback.\nLast but not least, thanks to everyone from Addison-Wesley. In particular,\nthanks to Debra Williams Cauley, for guiding us to Addison-Wesley and steering\n",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-35)",
      "start_page": 26,
      "end_page": 35,
      "detection_method": "topic_boundary",
      "content": "xxvi\nPreface\nus the entire way; Michael Thurston, for editing our earliest drafts and reshaping\nthem to be much, much better; Kim Boedigheimer, who coordinated and assisted\nus calmly even when we were panicking; Lori Hughes, our LaTeX wizard; Julie\nNahil, our production manager; Jill Hobbs, our copyeditor; and John Fuller and\nMark Taub, for putting up with all our special requests!\nPart I\nDesign: Building It\nChapter 1: Designing in a Distributed World\nOverview of how distributed systems are designed.\nChapter 2: Designing for Operations\nFeatures software should have to enable smooth operations.\nChapter 3: Selecting a Service Platform\nPhysical and virtual machines, private and public clouds.\nChapter 4: Application Architectures\nBuilding blocks for creating web and other applications.\nChapter 5: Design Patterns for Scaling\nBuilding blocks for growing a service.\nChapter 6: Design Patterns for Resiliency\nBuilding blocks for creating systems that survive failure.\nPart II\nOperations: Running It\nChapter 7: Operations in a Distributed World\nOverview of how distributed systems are run.\nChapter 8: DevOps Culture\nIntroduction to DevOps culture, its history and practices.\nChapter 9: Service Delivery: The Build Phase\nHow a service gets built and prepared for production.\nChapter 10: Service Delivery: The Deployment Phase\nHow a service is tested, approved, and put into production.\nChapter 11: Upgrading Live Services\nHow to upgrade services without downtime.\nChapter 12: Automation\nCreating tools and automating operational work.\nChapter 13: Design Documents\nCommunicating designs and intentions in writing.\nChapter 14: Oncall\nHandling exceptions.\nChapter 15: Disaster Preparedness\nMaking systems stronger through planning and practice.\n\n\nPreface\nxxvii\nChapter 16: Monitoring Fundamentals\nMonitoring terminology and strategy.\nChapter 17: Monitoring Architecture and Practice\nThe components and practice of monitoring.\nChapter 18: Capacity Planning\nPlanning for and providing additional resources before we need them.\nChapter 19: Creating KPIs\nDriving behavior scientiﬁcally through measurement and reﬂection.\nChapter 20: Operational Excellence\nStrategies for constant improvement.\nEpilogue\nSome ﬁnal thoughts.\nPart III\nAppendices\nAppendix A: Assessments\nAppendix B: The Origins and Future of Distributed Computing and Clouds\nAppendix C: Scaling Terminology and Concepts\nAppendix D: Templates and Examples\nAppendix E: Recommended Reading\nBibliography\nIndex\n\n\nThis page intentionally left blank \n\n\nAbout the Authors\nThomas A. Limoncelli is an internationally recognized author, speaker, and\nsystem administrator. During his seven years at Google NYC, he was an SRE for\nprojects such as Blog Search, Ganeti, and various internal enterprise IT services. He\nnow works as an SRE at Stack Exchange, Inc., home of ServerFault.com and Stack-\nOverﬂow.com. His ﬁrst paid system administration job was as a student at Drew\nUniversity in 1987, and he has since worked at small and large companies, includ-\ning AT&T/Lucent Bell Labs. His best-known books include Time Management for\nSystem Administrators (O’Reilly) and The Practice of System and Network Adminis-\ntration, Second Edition (Addison-Wesley). His hobbies include grassroots activism,\nfor which his work has been recognized at state and national levels. He lives in\nNew Jersey.\nStrata R. Chalup has been leading and managing complex IT projects for many\nyears, serving in roles ranging from project manager to director of operations.\nStrata has authored numerous articles on management and working with teams\nand has applied her management skills on various volunteer boards, including\nBayLISA and SAGE. She started administering VAX Ultrix and Unisys UNIX in\n1983 at MIT in Boston, and spent the dot-com years in Silicon Valley building inter-\nnet services for clients like iPlanet and Palm. In 2007, she joined Tom and Christina\nto create the second edition of The Practice of System and Network Administration\n(Addison-Wesley). Her hobbies include working with new technologies, including\nArduino and various 2D CAD/CAM devices, as well as being a master gardener.\nShe lives in Santa Clara County, California.\nChristina J. Hogan has twenty years of experience in system administration\nand network engineering, from Silicon Valley to Italy and Switzerland. She has\ngained experience in small startups, mid-sized tech companies, and large global\ncorporations. She worked as a security consultant for many years and her cus-\ntomers included eBay, Silicon Graphics, and SystemExperts. In 2005 she and Tom\nxxix\n\n\nxxx\nAbout the Authors\nshared the SAGE Outstanding Achievement Award for their book The Practice of\nSystem and Network Administration (Addison-Wesley). She has a bachelor’s degree\nin mathematics, a master’s degree in computer science, a doctorate in aeronautical\nengineering, and a diploma in law. She also worked for six years as an aero-\ndynamicist in a Formula 1 racing team and represented Ireland in the 1988 Chess\nOlympiad. She lives in Switzerland.\n\n\nIntroduction\nThe goal of this book is to help you build and run the best cloud-scale service\npossible. What is the ideal environment that we seek to create?\nBusiness Objectives\nSimply stated, the end result of our ideal environment is that business objectives\nare met. That may sound a little boring but actually it is quite exciting to work\nwhere the entire company is focused and working together on the same goals.\nTo achieve this, we must understand the business objectives and work back-\nward to arrive at the system we should build.\nMeeting business objectives means knowing what those objectives are, having\na plan to achieve them, and working through the roadblocks along the way.\nWell-deﬁned business objectives are measurable, and such measurements can\nbe collected in an automated fashion. A dashboard is automatically generated so\neveryone is aware of progress. This transparency enhances trust.\nHere are some sample business objectives:\n• Sell our products via a web site\n• Provide service 99.99 percent of the time\n• Process x million purchases per month, growing 10 percent monthly\n• Introduce new features twice a week\n• Fix major bugs within 24 hours\nIn our ideal environment, business and technical teams meet their objectives and\nproject goals predictably and reliably. Because of this, both types of teams trust\nthat other teams will meet their future objectives. As a result, teams can plan\nbetter. They can make more aggressive plans because there is conﬁdence that exter-\nnal dependencies will not fail. This permits even more aggressive planning. Such\nan approach creates an upward spiral that accelerates progress throughout the\ncompany, beneﬁting everyone.\n1\n\n\n2\nIntroduction\nIdeal System Architecture\nThe ideal service is built on a solid architecture. It meets the requirements of the\nservice today and provides an obvious path for growth as the system becomes\nmore popular and receives more trafﬁc. The system is resilient to failure. Rather\nthan being surprised by failures and treating them as exceptions, the architecture\naccepts that hardware and software failures are a part of the physics of information\ntechnology (IT). As a result, the architecture includes redundancy and resiliency\nfeatures that work around failures. Components fail but the system survives.\nEach subsystem that makes up our service is itself a service. All subsys-\ntems are programmable via an application programming interface (API). Thus,\nthe entire system is an ecosystem of interconnected subservices. This is called a\nservice-oriented architecture (SOA). Because all these systems communicate over\nthe same underlying protocol, there is uniformity in how they are managed.\nBecause each subservice is loosely coupled to the others, all of these services can\nbe independently scaled, upgraded, or replaced.\nThe geometry of the infrastructure is described electronically. This electronic\ndescription is read by IT automation systems, which then build the production\nenvironment without human intervention. Because of this automation, the entire\ninfrastructure can be re-created elsewhere. Software engineers use the automation\nto make micro-versions of the environment for their personal use. Quality and test\nengineers use the automation to create environments for system tests.\nThis “infrastructure as code” can be achieved whether we use physical\nmachines or virtual machines, and whether they are in datacenters we run or are\nhosted by a cloud provider. With virtual machines there is an obvious API available\nfor spinning up a new machine. However, even with physical machines, the entire\nﬂow from bare metal to working system can be automated. In our ideal world the\nautomation makes it possible to create environments using combinations of phys-\nical and virtual machines. Developers may build the environment out of virtual\nmachines. The production environment might consist of a mixture of physical and\nvirtual machines. The temporary and unexpected need for additional capacity may\nrequire extending the production environment into one or more cloud providers\nfor some period of time.\nIdeal Release Process\nOur ideal environment has a smooth ﬂow of code from development to operations.\nTraditionally (not in our ideal environment) the sequence looks like this:\n1. Developers check code into a repository.\n2. Test engineers put the code through a number of tests.\n\n\nIntroduction\n3\n3. If all the tests pass, the a release engineer builds the packages that will be used\nto deploy the software. Most of the ﬁles come from the source code repos-\nitory, but some ﬁles may be needed from other sources such as a graphics\ndepartment or documentation writers.\n4. A test environment is created; without an “infrastructure as code” model, this\nmay take weeks.\n5. The packages are deployed into a test environment.\n6. Test engineers perform further tests, focusing on the interaction between\nsubsystems.\n7. If all these tests succeed, the code is put into production.\n8. System administrators upgrade systems while looking for failures.\n9. If there are failures, the software is rolled back.\nDoing these steps manually incurs a lot of risk, owing to the assumptions that the\nright people are available, that the steps are done the same way every time, that\nnobody makes mistakes, and that all the tasks are completed in time.\nMistakes, bugs, and errors happen, of course—and as a result defects are\npassed down the line to the next stage. When a mistake is discovered the ﬂow of\nprogress is reversed as the team members who were responsible for the previous\nstage are told to ﬁx their problem. This means progress is halted and time is lost.\nA typical response to a risky process is to do it as rarely as possible. Thus\nthere is a temptation to do as few releases as possible. The result is “mega-releases”\nlaunched only a few times a year.\nHowever, by batching up so many changes at once, we actually create more\nrisk. How can we be sure thousands of changes, released simultaneously, will\nall work on the ﬁrst try? We can’t. Therefore we become even more recalcitrant\ntoward and fearful of making changes. Soon change becomes nearly impossible\nand innovation comes to a halt.\nNot so in our ideal environment.\nIn our ideal environment, we ﬁnd automation that eliminates all manual steps\nin the software build, test, release, and deployment processes. The automation\naccurately and consistently performs tests that prevent defects from being passed\nto the next step. As a consequence, the ﬂow of progress is in one direction: forward.\nRather than mega-releases, our ideal environment creates micro-releases. We\nreduce risk by doing many deployments, each with a few small changes. In fact,\nwe might do 100 deployments per day.\n1. When the developers check in code, a system detects this fact and triggers a\nseries of automated tests. These tests verify basic code functionality.\n2. If these tests pass, the process of building the packages is kicked off and runs\nin a completely automated fashion.\n\n\n4\nIntroduction\n3. The successful creation of new packages triggers the creation of a test envi-\nronment. Building a test environment used to be a long week of connecting\ncables and installing machines. But with infrastructure as code, the entire\nenvironment is created quickly with no human intervention.\n4. When the test environment is complete, a series of automated tests are run.\n5. On successful completion the new packages are rolled out to production. The\nroll-out is also automated but orderly and cautious.\n6. Certain systems are upgraded ﬁrst and the system watches for failures. Since\nthe test environment was built with the same automation that built the\nproduction environment, there should be very few differences.\n7. Seeing no failures, the new packages are rolled out to more and more systems\nuntil the entire production environment is upgraded.\nIn our ideal environment all problems are caught before they reach production.\nThat is, roll-out is not a form of testing. Failure during a roll-out to production is\nessentially eliminated. However, if a failure does happen, it would be considered\na serious issue warranting pausing new releases from going into production until\na root causes analysis is completed. Tests are added to detect and prevent future\noccurrences of this failure. Thus, the system gets stronger over time.\nBecause of this automation, the traditional roles of release engineering, qual-\nity assurance, and deployment are practically unrecognizable from their roles at a\ntraditional company. Hours of laborious manual toil are eliminated, leaving more\ntime for improving the packaging system, improving the software quality, and\nreﬁning the deployment process. In other words, people spend more time making\nimprovements in how work is done rather than doing work itself.\nA similar process is used for third-party software. Not all systems are home-\ngrown or come with source code. Deploying third-party services and products\nfollows a similar pattern of release, testing, deployment. However, because these\nproducts and services are developed externally, they require a slightly different\nprocess. New releases are likely to occur less frequently and we have less control\nover what is in each new release. The kind of testing these components require is\nusually related to features, compatibility, and integration.\nIdeal Operations\nOnce the code is in production, operational objectives take precedence. The soft-\nware is instrumented so that it can be monitored. Data is collected about how long\nit takes to process transactions from external users as well as from internal APIs.\nOther indicators such as memory usage are also monitored. This data is collected\nso that operational decisions can be made based on data, not guesses, luck, or\n\n\nIntroduction\n5\nhope. The data is stored for many years so it may be used to predict the future\ncapacity needs.\nMeasurements are used to detect internal problems while they are small, long\nbefore they result in a user-visible outage. We ﬁx problems before they become\noutages. An actual outage is rare and would be investigated with great diligence.\nWhen problems are detected there is a process in place to make sure they are\nidentiﬁed, worked on, and resolved quickly.\nAn automated system detects problems and alerts whoever is oncall. Our\noncall schedule is a rotation constructed so that each shift typically receives a man-\nageable number of alerts. At any given time one person is the primary oncall person\nand is ﬁrst to receive any alerts. If that individual does not respond in time, a sec-\nondary person is alerted. The oncall schedule is prepared far enough in advance\nthat people can plan vacations, recreational activities, and personal time.\nThere is a “playbook” of instructions on how to handle every alert that can be\ngenerated. Each type of alert is documented with a technical description of what\nis wrong, what the business impact is, and how to ﬁx the issue. The playbook is\ncontinually improved. Whoever is oncall uses the playbook to ﬁx the problem. If\nit proves insufﬁcient, there is a well-deﬁned escalation path, usually to the oncall\nperson for the related subsystem. Developers also participate in the oncall rotation\nso they understand the operational pain points of the system they are building.\nAll failures have a corresponding countermeasure, whether it is manually or\nautomatically activated. Countermeasures that are activated frequently are always\nautomated. Our monitoring system detects overuse, as this may indicate a larger\nproblem. The monitoring system collects internal indicator data used by engineers\nto reduce the failure rate as well as improve the countermeasure.\nThe less frequently a countermeasure is activated, the less conﬁdent we are\nthat it will work the next time it is needed. Therefore infrequently activated coun-\ntermeasures are periodically and automatically exercised by intentionally causing\nfailures. Just as we require school children to practice ﬁre drills so that everyone\nknows what to do in an emergency, so we practice ﬁre drills with our operational\npractices. This way our team becomes experienced at implementing the counter-\nmeasures and is conﬁdent that they work. If a database failover process doesn’t\nwork due to an unexpected dependency, it is better to learn this during a live drill\non Monday at 10 rather than during an outage at 4 on a Sunday morning.\nAgain, we reduce risk by increasing repetition rather than shying away from it. The\ntechnical term for improving something through repetition is called “practice.” We\nstrongly believe that practice makes perfect.\nOur ideal environment scales automatically. As more capacity is needed, addi-\ntional capacity comes from internal or external cloud providers. Our dashboards\nindicate when re-architecting will be a better solution than simply allocating more\nRAM, disk, or CPU.\n",
      "page_number": 26
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 36-46)",
      "start_page": 36,
      "end_page": 46,
      "detection_method": "topic_boundary",
      "content": "6\nIntroduction\nScaling down is also automatic. When the system is overloaded or degraded,\nwe never turn users away with a “503—Service Unavailable” error. Instead, the\nsystem automatically switches to algorithms that use less resources. Bandwidth\nfully utilized? Low-bandwidth versions of the service kick in, displaying fewer\ngraphics or a more simpliﬁed user interface. Databases become corrupted? A read-\nonly version of the service keeps most users satisﬁed.\nEach feature of our service can be individually enabled or disabled. If a feature\nturns out to have negative consequences, such as security holes or unexpectedly\nbad performance, it can be disabled without deploying a different software release.\nWhen a feature is revised, the new code does not eliminate the old\nfunctionality. The new behavior can be disabled to reveal the old behavior. This is\nparticularly useful when rolling out a new user interface. If a release can produce\nboth the old and new user interface, it can be enabled on a per-user basis. This\nenables us to get feedback from “early access” users. On the ofﬁcial release date,\nthe new feature is enabled for successively larger and larger groups. If performance\nproblems are found, the feature can easily be reverted or switched off entirely.\nIn our ideal environment there is excellent operational hygiene. Like brush-\ning our teeth, we regularly do the things that preserve good operational health.\nWe maintain clear and updated documentation for how to handle every counter-\nmeasure, process, and alert. Overactive alerts are ﬁne-tuned, not ignored. Open\nbug counts are kept to a minimum. Outages are followed by the publication\nof a postmortem report with recommendations on how to improve the system\nin the future. Any “quick ﬁx” is followed by a root causes analysis and the\nimplementation of a long-term ﬁx.\nMost importantly, the developers and operations people do not think of them-\nselves as two distinct teams. They are simply specializations within one large\nteam. Some people write more code than others; some people do more operational\nprojects than others. All share responsibility for maintaining high uptime. To that\nend, all members participate in the oncall (pager) rotation. Developers are most\nmotivated to improve code that affects operations when they feel the pain of oper-\nations, too. Operations must understand the development process if they are to be\nable to constructively collaborate.\nNow you know our vision of an ideal environment. The remainder of this book\nwill explain how to create and run it.\n\n\nPart I\nDesign: Building It\n\n\nThis page intentionally left blank \n\n\nChapter 1\nDesigning in a Distributed\nWorld\nThere are two ways of constructing\na software design: One way is to\nmake it so simple that there are\nobviously no deﬁciencies and the\nother way is to make it so\ncomplicated that there are no\nobvious deﬁciencies.\n—C.A.R. Hoare, The 1980 ACM\nTuring Award Lecture\nHow does Google Search work? How does your Facebook Timeline stay updated\naround the clock? How does Amazon scan an ever-growing catalog of items to tell\nyou that people who bought this item also bought socks?\nIs it magic? No, it’s distributed computing.\nThis chapter is an overview of what is involved in designing services that use\ndistributed computing techniques. These are the techniques all large web sites use\nto achieve their size, scale, speed, and reliability.\nDistributed computing is the art of building large systems that divide the work\nover many machines. Contrast this with traditional computing systems where a\nsingle computer runs software that provides a service, or client–server computing\nwhere many machines remotely access a centralized service. In distributed com-\nputing there are typically hundreds or thousands of machines working together to\nprovide a large service.\nDistributed computing is different from traditional computing in many ways.\nMost of these differences are due to the sheer size of the system itself. Hundreds or\nthousands of computers may be involved. Millions of users may be served. Billions\nand sometimes trillions of queries may be processed.\n9\n\n\n10\nChapter 1\nDesigning in a Distributed World\n.\nTerms to Know\nServer: Software that provides a function or application program interface\n(API). (Not a piece of hardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nTraffic: A generic term for queries, API calls, or other requests sent to a\nserver.\nPerformant: A system whose performance conforms to (meets or exceeds)\nthe design requirements. A neologism from merging “performance”\nand “conformant.”\nApplication Programming Interface (API): A protocol that governs how\none server talks to another.\nSpeed is important. It is a competitive advantage for a service to be fast and\nresponsive. Users consider a web site sluggish if replies do not come back in 200 ms\nor less. Network latency eats up most of that time, leaving little time for the service\nto compose the page itself.\nIn distributed systems, failure is normal. Hardware failures that are rare, when\nmultiplied by thousands of machines, become common. Therefore failures are\nassumed, designs work around them, and software anticipates them. Failure is an\nexpected part of the landscape.\nDue to the sheer size of distributed systems, operations must be automated.\nIt is inconceivable to manually do tasks that involve hundreds or thousands\nof machines. Automation becomes critical for preparation and deployment of\nsoftware, regular operations, and handling failures.\n1.1 Visibility at Scale\nTo manage a large distributed system, one must have visibility into the system.\nThe ability to examine internal state—called introspection—is required to operate,\ndebug, tune, and repair large systems.\nIn a traditional system, one could imagine an engineer who knows enough\nabout the system to keep an eye on all the critical components or “just knows”\nwhat is wrong based on experience. In a large system, that level of visibility must\nbe actively created by designing systems that draw out the information and make\nit visible. No person or team can manually keep tabs on all the parts.\n\n\n1.2\nThe Importance of Simplicity\n11\nDistributed systems, therefore, require components to generate copious logs\nthat detail what happened in the system. These logs are then aggregated to a central\nlocation for collection, storage, and analysis. Systems may log information that is\nvery high level, such as whenever a user makes a purchase, for each web query,\nor for every API call. Systems may log low-level information as well, such as the\nparameters of every function call in a critical piece of code.\nSystems should export metrics. They should count interesting events, such as\nhow many times a particular API was called, and make these counters accessible.\nIn many cases, special URLs can be used to view this internal state.\nFor example, the Apache HTTP Web Server has a “server-status” page\n(http://www.example.com/server-status/).\nIn addition, components of distributed systems often appraise their own\nhealth and make this information visible. For example, a component may have\na URL that outputs whether the system is ready (OK) to receive new requests.\nReceiving as output anything other than the byte “O” followed by the byte “K”\n(including no response at all) indicates that the system does not want to receive\nnew requests. This information is used by load balancers to determine if the\nserver is healthy and ready to receive trafﬁc. The server sends negative replies\nwhen the server is starting up and is still initializing, and when it is shutting\ndown and is no longer accepting new requests but is processing any requests\nthat are still in ﬂight.\n1.2 The Importance of Simplicity\nIt is important that a design remain as simple as possible while still being able\nto meet the needs of the service. Systems grow and become more complex\nover time. Starting with a system that is already complex means starting at a\ndisadvantage.\nProviding competent operations requires holding a mental model of the sys-\ntem in one’s head. As we work we imagine the system operating and use this\nmental model to track how it works and to debug it when it doesn’t. The more\ncomplex the system, the more difﬁcult it is to have an accurate mental model. An\noverly complex system results in a situation where no single person understands\nit all at any one time.\nIn The Elements of Programming Style, Kernighan and Plauger (1978) wrote:\nDebugging is twice as hard as writing the code in the ﬁrst place. Therefore, if you write\nthe code as cleverly as possible, you are, by deﬁnition, not smart enough to debug it.\nThe same is true for distributed systems. Every minute spent simplifying a design\npays off time and time again when the system is in operation.\n\n\n12\nChapter 1\nDesigning in a Distributed World\n1.3 Composition\nDistributed systems are composed of many smaller systems. In this section, we\nexplore three fundamental composition patterns in detail:\n• Load balancer with multiple backend replicas\n• Server with multiple backends\n• Server tree\n1.3.1 Load Balancer with Multiple Backend Replicas\nThe ﬁrst composition pattern is the load balancer with multiple backend replicas.\nAs depicted in Figure 1.1, requests are sent to the load balancer server. For each\nrequest, it selects one backend and forwards the request there. The response comes\nback to the load balancer server, which in turn relays it to the original requester.\nThe backends are called replicas because they are all clones or replications of\neach other. A request sent to any replica should produce the same response.\nThe load balancer must always know which backends are alive and ready to\naccept requests. Load balancers send health check queries dozens of times each\nsecond and stop sending trafﬁc to that backend if the health check fails. A health\ncheck is a simple query that should execute quickly and return whether the system\nshould receive trafﬁc.\nPicking which backend to send a query to can be simple or complex. A\nsimple method would be to alternate among the backends in a loop—a practice\ncalled round-robin. Some backends may be more powerful than others, however,\nFigure 1.1: A load balancer with many replicas\n\n\n1.3\nComposition\n13\nand may be selected more often using a proportional round-robin scheme.\nMore complex solutions include the least loaded scheme. In this approach, a\nload balancer tracks how loaded each backend is and always selects the least\nloaded one.\nSelecting the least loaded backend sounds reasonable but a naive implemen-\ntation can be a disaster. A backend may not show signs of being overloaded until\nlong after it has actually become overloaded. This problem arises because it can be\ndifﬁcult to accurately measure how loaded a system is. If the load is a measure-\nment of the number of connections recently sent to the server, this deﬁnition is\nblind to the fact that some connections may be long lasting while others may be\nquick. If the measurement is based on CPU utilization, this deﬁnition is blind to\ninput/output (I/O) overload. Often a trailing average of the last 5 minutes of load\nis used. Trailing averages have a problem in that, as an average, they reﬂect the\npast, not the present. As a consequence, a sharp, sudden increase in load will not\nbe reﬂected in the average for a while.\nImagine a load balancer with 10 backends. Each one is running at 80 percent\nload. A new backend is added. Because it is new, it has no load and, therefore,\nis the least loaded backend. A naive least loaded algorithm would send all trafﬁc\nto this new backend; no trafﬁc would be sent to the other 10 backends. All too\nquickly, the new backend would become absolutely swamped. There is no way a\nsingle backend could process the trafﬁc previously handled by 10 backends. The\nuse of trailing averages would mean the older backends would continue reporting\nartiﬁcially high loads for a few minutes while the new backend would be reporting\nan artiﬁcially low load.\nWith this scheme, the load balancer will believe that the new machine is less\nloaded than all the other machines for quite some time. In such a situation the\nmachine may become so overloaded that it would crash and reboot, or a system\nadministrator trying to rectify the situation might reboot it. When it returns to\nservice, the cycle would start over again.\nSuch situations make the round-robin approach look pretty good. A less naive\nleast loaded implementation would have some kind of control in place that would\nnever send more than a certain number of requests to the same machine in a row.\nThis is called a slow start algorithm.\n.\nTrouble with a Naive Least Loaded Algorithm\nWithout slow start, load balancers have been known to cause many prob-\nlems. One famous example is what happened to the CNN.com web site on\nthe day of the September 11, 2001, terrorist attacks. So many people tried to\naccess CNN.com that the backends became overloaded. One crashed, and then\ncrashed again after it came back up, because the naive least loaded algorithm\n\n\n14\nChapter 1\nDesigning in a Distributed World\n.\nsent all trafﬁc to it. When it was down, the other backends became overloaded\nand crashed. One at a time, each backend would get overloaded, crash, and\nbecome overloaded from again receiving all the trafﬁc and crash again.\nAs a result the service was essentially unavailable as the system adminis-\ntrators rushed to ﬁgure out what was going on. In their defense, the web was\nnew enough that no one had experience with handling sudden trafﬁc surges\nlike the one encountered on September 11.\nThe solution CNN used was to halt all the backends and boot them at\nthe same time so they would all show zero load and receive equal amounts of\ntrafﬁc.\nThe CNN team later discovered that a few days prior, a software upgrade\nfor their load balancer had arrived but had not yet been installed. The upgrade\nadded a slow start mechanism.\n1.3.2 Server with Multiple Backends\nThe next composition pattern is a server with multiple backends. The server\nreceives a request, sends queries to many backend servers, and composes the ﬁnal\nreply by combining those answers. This approach is typically used when the orig-\ninal query can easily be deconstructed into a number of independent queries that\ncan be combined to form the ﬁnal answer.\nFigure 1.2a illustrates how a simple search engine processes a query with the\nhelp of multiple backends. The frontend receives the request. It relays the query\nto many backend servers. The spell checker replies with information so the search\nengine may suggest alternate spellings. The web and image search backends reply\nwith a list of web sites and images related to the query. The advertisement server\nFigure 1.2: This service is composed of a server and many backends.\n\n\n1.3\nComposition\n15\nreplies with advertisements relevant to the query. Once the replies are received,\nthe frontend uses this information to construct the HTML that makes up the search\nresults page for the user, which is then sent as the reply.\nFigure 1.2b illustrates the same architecture with replicated, load-balanced,\nbackends. The same principle applies but the system is able to scale and survive\nfailures better.\nThis kind of composition has many advantages. The backends do their work\nin parallel. The reply does not have to wait for one backend process to complete\nbefore the next begins. The system is loosely coupled. One backend can fail and the\npage can still be constructed by ﬁlling in some default information or by leaving\nthat area blank.\nThis pattern also permits some rather sophisticated latency management. Sup-\npose this system is expected to return a result in 200 ms or less. If one of the\nbackends is slow for some reason, the frontend doesn’t have to wait for it. If it takes\n10 ms to compose and send the resulting HTML, at 190 ms the frontend can give\nup on the slow backends and generate the page with the information it has. The\nability to manage a latency time budget like that can be very powerful. For exam-\nple, if the advertisement system is slow, search results can be displayed without\nany ads.\nTo be clear, the terms “frontend” and “backend” are a matter of perspective.\nThe frontend sends requests to backends, which reply with a result. A server can be\nboth a frontend and a backend. In the previous example, the server is the backend\nto the web browser but a frontend to the spell check server.\nThere are many variations on this pattern. Each backend can be replicated for\nincreased capacity or resiliency. Caching may be done at various levels.\nThe term fan out refers to the fact that one query results in many new queries,\none to each backend. The queries “fan out” to the individual backends and the\nreplies fan in as they are set up to the frontend and combined into the ﬁnal result.\nAny fan in situation is at risk of having congestion problems. Often small\nqueries may result in large responses. Therefore a small amount of bandwidth is\nused to fan out but there may not be enough bandwidth to sustain the fan in. This\nmay result in congested network links and overloaded servers. It is easy to engineer\nthe system to have the right amount of network and server capacity if the sizes of\nthe queries and replies are consistent, or if there is an occasional large reply. The\ndifﬁcult situation is engineering the system when there are sudden, unpredictable\nbursts of large replies. Some network equipment is engineered speciﬁcally to deal\nwith this situation by dynamically provisioning more buffer space to such bursts.\nLikewise, the backends can rate-limit themselves to avoid creating the situation in\nthe ﬁrst place. Lastly, the frontends can manage the congestion themselves by con-\ntrolling the new queries they send out, by notifying the backends to slow down, or\nby implementing emergency measures to handle the ﬂood better. The last option\nis discussed in Chapter 5.\n\n\n16\nChapter 1\nDesigning in a Distributed World\n1.3.3 Server Tree\nThe other fundamental composition pattern is the server tree. As Figure 1.3 illus-\ntrates, in this scheme a number of servers work cooperatively with one as the root\nof the tree, parent servers below it, and leaf servers at the bottom of the tree. (In\ncomputer science, trees are drawn upside-down.) Typically this pattern is used to\naccess a large dataset or corpus. The corpus is larger than any one machine can\nhold; thus each leaf stores one fraction or shard of the whole.\nTo query the entire dataset, the root receives the original query and forwards it\nto the parents. The parents forward the query to the leaf servers, which search their\nparts of the corpus. Each leaf sends its ﬁndings to the parents, which sort and ﬁlter\nthe results before forwarding them up to the root. The root then takes the response\nfrom all the parents, combines the results, and replies with the full answer.\nImagine you wanted to ﬁnd out how many times George Washington was\nmentioned in an encyclopedia. You could read each volume in sequence and arrive\nat the answer. Alternatively, you could give each volume to a different person and\nhave the various individuals search their volumes in parallel. The latter approach\nwould complete the task much faster.\nFigure 1.3: A server tree\n",
      "page_number": 36
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 47-54)",
      "start_page": 47,
      "end_page": 54,
      "detection_method": "topic_boundary",
      "content": "1.4\nDistributed State\n17\nThe primary beneﬁt of this pattern is that it permits parallel searching of a\nlarge corpus. Not only are the leaves searching their share of the corpus in parallel,\nbut the sorting and ranking performed by the parents are also done in parallel.\nFor example, imagine a corpus of the text extracted from every book in the\nU.S. Library of Congress. This cannot ﬁt in one computer, so instead the informa-\ntion is spread over hundreds or thousands of leaf machines. In addition to the leaf\nmachines are the parents and the root. A search query would go to a root server,\nwhich in turn relays the query to all parents. Each parent repeats the query to all\nleaf nodes below it. Once the leaves have replied, the parent ranks and sorts the\nresults by relevancy.\nFor example, a leaf may reply that all the words of the query exist in the same\nparagraph in one book, but for another book only some of the words exist (less\nrelevant), or they exist but not in the same paragraph or page (even less relevant).\nIf the query is for the best 50 answers, the parent can send the top 50 results to the\nroot and drop the rest. The root then receives results from each parent and selects\nthe best 50 of those to construct the reply.\nThis scheme also permits developers to work within a latency budget. If fast\nanswers are more important than perfect answers, parents and roots do not have\nto wait for slow replies if the latency deadline is near.\nMany variations of this pattern are possible. Redundant servers may exist with\na load-balancing scheme to divide the work among them and route around failed\nservers. Expanding the number of leaf servers can give each leaf a smaller por-\ntion of the corpus to search, or each shard of corpus can be placed on multiple\nleaf servers to improve availability. Expanding the number of parents at each level\nincreases the capacity to sort and rank results. There may be additional levels of\nparent servers, making the tree taller. The additional levels permit a wider fan-\nout, which is important for an extremely large corpus. The parents may provide a\ncaching function to relieve pressure on the leaf servers; in this case more levels of\nparents may improve cache effectiveness. These techniques can also help mitigate\ncongestion problems related to fan-in, as discussed in the previous section.\n1.4 Distributed State\nLarge systems often store or process large amounts of state. State consists of data,\nsuch as a database, that is frequently updated. Contrast this with a corpus, which\nis relatively static or is updated only periodically when a new edition is published.\nFor example, a system that searches the U.S. Library of Congress may receive a\nnew corpus each week. By comparison, an email system is in constant churn with\nnew data arriving constantly, current data being updated (email messages being\nmarked as “read” or moved between folders), and data being deleted.\n\n\n18\nChapter 1\nDesigning in a Distributed World\nDistributed computing systems have many ways to deal with state. How-\never, they all involve some kind of replication and sharding, which brings about\nproblems of consistency, availability, and partitioning.\nThe easiest way to store state is to put it on one machine, as depicted in\nFigure 1.4. Unfortunately, that method reaches its limit quite quickly: an individ-\nual machine can store only a limited amount of state and if the one machine dies\nwe lose access to 100 percent of the state. The machine has only a certain amount\nof processing power, which means the number of simultaneous reads and writes\nit can process is limited.\nIn distributed computing we store state by storing fractions or shards of the\nwhole on individual machines. This way the amount of state we can store is lim-\nited only by the number of machines we can acquire. In addition, each shard is\nstored on multiple machines; thus a single machine failure does not lose access\nto any state. Each replica can process a certain number of queries per second, so\nwe can design the system to process any number of simultaneous read and write\nrequests by increasing the number of replicas. This is illustrated in Figure 1.5,\nwhere N QPS are received and distributed among three shards, each replicated\nthree ways. As a result, on average one ninth of all queries reach a particular\nreplica server.\nWrites or requests that update state require all replicas to be updated. While\nthis update process is happening, it is possible that some clients will read from\nstale replicas that have not yet been updated. Figure 1.6 illustrates how a write can\nbe confounded by reads to an out-of-date cache. This will be discussed further in\nthe next section.\nIn the most simple pattern, a root server receives requests to store or retrieve\nstate. It determines which shard contains that part of the state and forwards the\nrequest to the appropriate leaf server. The reply then ﬂows up the tree. This looks\nsimilar to the server tree pattern described in the previous section but there are two\nFigure 1.4: State kept in one location; not distributed computing\n\n\n1.4\nDistributed State\n19\nFigure 1.5: This distributed state is sharded and replicated.\nFigure 1.6: State updates using cached data lead to an inconsistent view.\n\n\n20\nChapter 1\nDesigning in a Distributed World\ndifferences. First, queries go to a single leaf instead of all leaves. Second, requests\ncan be update (write) requests, not just read requests. Updates are more complex\nwhen a shard is stored on many replicas. When one shard is updated, all of the\nreplicas must be updated, too. This may be done by having the root update all\nleaves or by the leaves communicating updates among themselves.\nA variation of that pattern is more appropriate when large amounts of data\nare being transferred. In this case, the root replies with instructions on how to get\nthe data rather than the data itself. The requestor then requests the data from the\nsource directly.\nFor example, imagine a distributed ﬁle system with petabytes of data spread\nout over thousands of machines. Each ﬁle is split into gigabyte-sized chunks. Each\nchunk is stored on multiple machines for redundancy. This scheme also permits the\ncreation of ﬁles larger than those that would ﬁt on one machine. A master server\ntracks the list of ﬁles and identiﬁes where their chunks are. If you are familiar with\nthe UNIX ﬁle system, the master can be thought of as storing the inodes, or per-ﬁle\nlists of data blocks, and the other machine as storing the actual blocks of data. File\nsystem operations go through a master server that uses the inode-like information\nto determine which machines to involve in the operation.\nImagine that a large read request comes in. The master determines that the ﬁle\nhas a few terabytes stored on one machine and a few terabytes stored on another\nmachine. It could request the data from each machine and relay it to the system\nthat made the request, but the master would quickly become overloaded while\nreceiving and relaying huge chunks of data. Instead, it replies with a list of which\nmachines have the data, and the requestor contacts those machines directly for the\ndata. This way the master is not the middle man for those large data transfers. This\nsituation is illustrated in Figure 1.7.\n.\nFigure 1.7: This master server delegates replies to other servers.\n\n\n1.5\nThe CAP Principle\n21\n1.5 The CAP Principle\nCAP stands for consistency, availability, and partition resistance. The CAP Prin-\nciple states that it is not possible to build a distributed system that guarantees\nconsistency, availability, and resistance to partitioning. Any one or two can be\nachieved but not all three simultaneously. When using such systems you must be\naware of which are guaranteed.\n1.5.1 Consistency\nConsistency means that all nodes see the same data at the same time. If there are\nmultiple replicas and there is an update being processed, all users see the update\ngo live at the same time even if they are reading from different replicas. Systems\nthat do not guarantee consistency may provide eventual consistency. For exam-\nple, they may guarantee that any update will propagate to all replicas in a certain\namount of time. Until that deadline is reached, some queries may receive the new\ndata while others will receive older, out-of-date answers.\nPerfect consistency is not always important. Imagine a social network that\nawards reputation points to users for positive actions. Your reputation point total\nis displayed anywhere your name is shown. The reputation database is replicated\nin the United States, Europe, and Asia. A user in Europe is awarded points and that\nchange might take minutes to propagate to the United States and Asia replicas. This\nmay be sufﬁcient for such a system because an absolutely accurate reputation score\nis not essential. If a user in the United States and one in Asia were talking on the\nphoneas onewasawardedpoints,theother userwouldseetheupdatesecondslater\nand that would be okay. If the update took minutes due to network congestion or\nhours due to a network outage, the delay would still not be a terrible thing.\nNow imagine a banking application built on this system. A person in the\nUnited States and another in Europe could coordinate their actions to withdraw\nmoney from the same account at the same time. The ATM that each person uses\nwould query its nearest database replica, which would claim the money is avail-\nable and may be withdrawn. If the updates propagated slowly enough, both people\nwould have the cash before the bank realized the money was already gone.1\n1.5.2 Availability\nAvailability is a guarantee that every request receives a response about whether\nit was successful or failed. In other words, it means that the system is up. For\n1.\nThe truth is that the global ATM system does not require database consistency. It can be defeated by\nleveraging network delays and outages. It is less expensive for banks to give out a limited amount of\nmoney when the ATM network is down than to have an unhappy customer stranded without cash.\nFraudulent transactions are dealt with after the fact. Daily withdrawal limits prevent major fraud.\nAssessing overage fees is easier than implementing a globally consistent database.\n\n\n22\nChapter 1\nDesigning in a Distributed World\nexample, using many replicas to store data such that clients always have access\nto at least one working replica guarantees availability.\nThe CAP Principle states that availability also guarantees that the system is\nable to report failure. For example, a system may detect that it is overloaded and\nreply to requests with an error code that means “try again later.” Being told this\nimmediately is more favorable than having to wait minutes or hours before one\ngives up.\n1.5.3 Partition Tolerance\nPartition tolerance means the system continues to operate despite arbitrary mes-\nsage loss or failure of part of the system. The simplest example of partition\ntolerance is when the system continues to operate even if the machines involved\nin providing the service lose the ability to communicate with each other due to a\nnetwork link going down (see Figure 1.8).\nReturning to our example of replicas, if the system is read-only it is easy to\nmake the system partition tolerant, as the replicas do not need to communicate with\neach other. But consider the example of replicas containing state that is updated\non one replica ﬁrst, then copied to other replicas. If the replicas are unable to com-\nmunicate with each other, the system fails to be able to guarantee updates will\npropagate within a certain amount of time, thus becoming a failed system.\nNow consider a situation where two servers cooperate in a master–slave rela-\ntionship. Both maintain a complete copy of the state and the slave takes over the\nmaster’s role if the master fails, which is determined by a loss of heartbeat—that is,\nFigure 1.8: Nodes partitioned from each other\n\n\n1.5\nThe CAP Principle\n23\na periodic health check between two servers often done via a dedicated network.\nIf the heartbeat network between the two is partitioned, the slave will promote\nitself to being the master, not knowing that the original master is up but unable\nto communicate on the heartbeat network. At this point there are two masters and\nthe system breaks. This situation is called split brain.\nSome special cases of partitioning exist. Packet loss is considered a temporary\npartitioning of the system as it applies to the CAP Principle. Another special case\nis the complete network outage. Even the most partition-tolerant system is unable\nto work in that situation.\nThe CAP Principle says that any one or two of the attributes are achievable in\ncombination, but not all three. In 2002, Gilbert and Lynch published a formal proof\nof the original conjecture, rendering it a theorem. One can think of this as the third\nattribute being sacriﬁced to achieve the other two.\nThe CAP Principle is illustrated by the triangle in Figure 1.9. Traditional rela-\ntional databases like Oracle, MySQL, and PostgreSQL are consistent and available\n(CA). They use transactions and other database techniques to assure that updates\nare atomic; they propagate completely or not at all. Thus they guarantee all users\nwill see the same state at the same time. Newer storage systems such as Hbase,\nFigure 1.9: The CAP Principle\n\n\n24\nChapter 1\nDesigning in a Distributed World\nRedis, and Bigtable focus on consistency and partition tolerance (CP). When par-\ntitioned, they become read-only or refuse to respond to any requests rather than\nbe inconsistent and permit some users to see old data while others see fresh data.\nFinally, systems such as Cassandra, Riak, and Dynamo focus on availability and\npartition tolerance (AP). They emphasize always being able to serve requests even\nif it means some clients receive outdated results. Such systems are often used in\nglobally distributed networks where each replica talks to the others by less reliable\nmedia such as the Internet.\nSQL and other relational databases use the term ACID to describe their side\nof the CAP triangle. ACID stands for Atomicity (transactions are “all or nothing”),\nConsistency (after each transaction the database is in a valid state), Isolation (con-\ncurrent transactions give the same results as if they were executed serially), and\nDurability (a committed transaction’s data will not be lost in the event of a crash\nor other problem). Databases that provide weaker consistency models often refer\nto themselves as NoSQL and describe themselves as BASE: Basically Available\nSoft-state services with Eventual consistency.\n1.6 Loosely Coupled Systems\nDistributed systems are expected to be highly available, to last a long time, and to\nevolve and change without disruption. Entire subsystems are often replaced while\nthe system is up and running.\nTo achieve this a distributed system uses abstraction to build a loosely cou-\npled system. Abstraction means that each component provides an interface that\nis deﬁned in a way that hides the implementation details. The system is loosely\ncoupled if each component has little or no knowledge of the internals of the other\ncomponents. As a result a subsystem can be replaced by one that provides the same\nabstract interface even if its implementation is completely different.\nTake, for example, a spell check service. A good level of abstraction would be\nto take in text and return a description of which words are misspelled and a list of\npossible corrections for each one. A bad level of abstraction would simply provide\naccess to a lexicon of words that the frontends could query for similar words. The\nreason the latter is not a good abstraction is that if an entirely new way to check\nspelling was invented, every frontend using the spell check service would need\nto be rewritten. Suppose this new version does not rely on a lexicon but instead\napplies an artiﬁcial intelligence technique called machine learning. With the good\nabstraction, no frontend would need to change; it would simply send the same\nkind of request to the new server. Users of the bad abstraction would not be so\nlucky.\nFor this and many other reasons, loosely coupled systems are easier to evolve\nand change over time.\n",
      "page_number": 47
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 55-63)",
      "start_page": 55,
      "end_page": 63,
      "detection_method": "topic_boundary",
      "content": "1.6\nLoosely Coupled Systems\n25\nContinuing our example, in preparation for the launch of the new spell check\nservice both versions could be run in parallel. The load balancer that sits in front\nof the spell check system could be programmed to send all requests to both the\nold and new systems. Results from the old system would be sent to the users, but\nresults from the new system would be collected and compared for quality control.\nAt ﬁrst the new system might not produce results that were as good, but over time\nit would be enhanced until its results were quantiﬁably better. At that point the\nnew system would be put into production. To be cautious, perhaps only 1 percent\nof all queries would come through the new system—if no users complained, the\nnew system would take a larger fraction. Eventually all responses would come\nfrom the new system and the old system could be decommissioned.\nOther systems require more precision and accuracy than a spell check system.\nFor example, there may be requirements that the new system be bug-for-bug com-\npatible with the old system before it can offer new functionality. That is, the new\nsystem must reproduce not only the features but also the bugs from the old sys-\ntem. In this case the ability to send requests to both systems and compare results\nbecomes critical to the operational task of deploying it.\n.\nCase Study: Emulation before Improvements\nWhen Tom was at Cibernet, he was involved in a project to replace an older\nsystem. Because it was a ﬁnancial system, the new system had to prove it was\nbug-for-bug compatible before it could be deployed.\nThe old system was built on obsolete, pre-web technology and had\nbecome so complex and calciﬁed that it was impossible to add new features.\nThe new system was built on newer, better technology and, being a cleaner\ndesign, was more easily able to accommodate new functionality. The systems\nwere run in parallel and results were compared.\nAt that point engineers found a bug in the old system. Currency conver-\nsion was being done in a way that was non-standard and the results were\nslightly off. To make the results between the two systems comparable, the\ndevelopers reverse-engineered the bug and emulated it in the new system.\nNow the results in the old and new systems matched down to the penny.\nWith the company having gained conﬁdence in the new system’s ability to be\nbug-for-bug compatible, it was activated as the primary system and the old\nsystem was disabled.\nAt this point, new features and improvements could be made to the sys-\ntem. The ﬁrst improvement, unsurprisingly, was to remove the code that\nemulated the currency conversion bug.\n\n\n26\nChapter 1\nDesigning in a Distributed World\n1.7 Speed\nSo far we have elaborated on many of the considerations involved in designing\nlarge distributed systems. For web and other interactive services, one item may\nbe the most important: speed. It takes time to get information, store information,\ncompute and transform information, and transmit information. Nothing happens\ninstantly.\nAn interactive system requires fast response times. Users tend to perceive any-\nthing faster than 200 ms to be instant. They also prefer fast over slow. Studies have\ndocumented sharp drops in revenue when delays as little as 50 ms were artiﬁcially\nadded to web sites. Time is also important in batch and non-interactive systems\nwhere the total throughput must meet or exceed the incoming ﬂow of work.\nThe general strategy for designing a system that is performant is to design a\nsystem using our best estimates of how quickly it will be able to process a request\nand then to build prototypes to test our assumptions. If we are wrong, we go back\nto step one; at least the next iteration will be informed by what we have learned. As\nwe build the system, we are able to remeasure and adjust the design if we discover\nour estimates and prototypes have not guided us as well as we had hoped.\nAt the start of the design process we often create many designs, estimate how\nfast each will be, and eliminate the ones that are not fast enough. We do not auto-\nmatically select the fastest design. The fastest design may be considerably more\nexpensive than one that is sufﬁcient.\nHow do we determine if a design is worth pursuing? Building a prototype is\nvery time consuming. Much can be deduced with some simple estimating exer-\ncises. Pick a few common transactions and break them down into smaller steps,\nand then estimate how long each step will take.\nTwo of the biggest consumers of time are disk access and network delays.\nDisk accesses are slow because they involve mechanical operations. To read a\nblock of data from a disk requires the read arm to move to the right track; the platter\nmust then spin until the desired block is under the read head. This process typically\ntakes 10 ms. Compare this to reading the same amount of information from RAM,\nwhich takes 0.002 ms, which is 5,000 times faster. The arm and platters (known as\na spindle) can process only one request at a time. However, once the head is on\nthe right track, it can read many sequential blocks. Therefore reading two blocks\nis often nearly as fast as reading one block if the two blocks are adjacent. Solid-\nstate drives (SSDs) do not have mechanical spinning platters and are much faster,\nthough more expensive.\nNetwork access is slow because it is limited by the speed of light. It takes\napproximately 75 ms for a packet to get from California to the Netherlands. About\nhalf of that journey time is due to the speed of light. Additional delays may be\nattributable to processing time on each router, the electronics that convert from\n\n\n1.7\nSpeed\n27\nwired to ﬁber-optic communication and back, the time it takes to assemble and\ndisassemble the packet on each end, and so on.\nTwo computers on the same network segment might seem as if they commu-\nnicate instantly, but that is not really the case. Here the time scale is so small that\nother delays have a bigger factor. For example, when transmitting data over a local\nnetwork, the ﬁrst byte arrives quickly but the program receiving the data usually\ndoes not process it until the entire packet is received.\nIn many systems computation takes little time compared to the delays from\nnetwork and disk operation. As a result you can often estimate how long a trans-\naction will take if you simply know the distance from the user to the datacenter\nand the number of disk seeks required. Your estimate will often be good enough\nto throw away obviously bad designs.\nTo illustrate this, imagine you are building an email system that needs to\nbe able to retrieve a message from the message storage system and display it\nwithin 300 ms. We will use the time approximations listed in Figure 1.10 to help us\nengineer the solution.\n.\nJeff Dean, a Google Fellow, has popularized this chart of common numbers\nto aid in architectural and scaling decisions. As you can see, there are many\norders of magnitude difference between certain options. These numbers\nimprove every year. Updates can be found online.\nAction\nTypical Time\nL1 cache reference\n0.5 ns\nBranch mispredict\n5 ns\nL2 cache reference\n7 ns\nMutex lock/unlock\n100 ns\nMain memory reference\n100 ns\nCompress 1K bytes with Zippy\n10,000 ns\n(0.01 ms)\nSend 2K bytes over 1 Gbps network\n20,000 ns\n(0.02 ms)\nRead 1 MB sequentially from memory\n250,000 ns\n(0.25 ms)\nRound trip within same datacenter\n500,000 ns\n(0.5 ms)\nRead 1 MB from SSD\n1,000,000 ns\n(3 ms)\nDisk seek\n10,000,000 ns\n(10 ms)\nRead 1 MB sequentially from network\n10,000,000 ns\n(10 ms)\nRead 1 MB sequentially from disk\n30,000,000 ns\n(30 ms)\nSend packet from California to\nNetherlands to California\n150,000,000 ns\n(150 ms)\nFigure 1.10: Numbers every engineer should know\n\n\n28\nChapter 1\nDesigning in a Distributed World\nFirst we follow the transaction from beginning to end. The request comes from\na web browser that may be on another continent. The request must be authenti-\ncated, the database index is consulted to determine where to get the message text,\nthe message text is retrieved, and ﬁnally the response is formatted and transmitted\nback to the user.\nNow let’s budget for the items we can’t control. To send a packet between\nCalifornia and Europe typically takes 75 ms, and until physics lets us change the\nspeed of light that won’t change. Our 300 ms budget is reduced by 150 ms since we\nhave to account for not only the time it takes for the request to be transmitted but\nalso the reply. That’s half our budget consumed by something we don’t control.\nWe talk with the team that operates our authentication system and they\nrecommend budgeting 3 ms for authentication.\nFormatting the data takes very little time—less than the slop in our other\nestimates—so we can ignore it.\nThis leaves 147 ms for the message to be retrieved from storage. If a typical\nindex lookup requires 3 disk seeks (10 ms each) and reads about 1 megabyte of\ninformation (30 ms), that is 60 ms. Reading the message itself might require 4 disk\nseeks and reading about 2 megabytes of information (100 ms). The total is 160 ms,\nwhich is more than our 147 ms remaining budget.\n.\nHow Did We Know That?\nHow did we know that it will take 3 disk seeks to read the index? It requires\nknowledge of the inner workings of the UNIX ﬁle system: how ﬁles are looked\nup in a directory to ﬁnd an inode and how inodes are used to look up the data\nblocks. This is why understanding the internals of the operating system you\nuse is key to being able to design and operate distributed systems. The inter-\nnals of UNIX and UNIX-like operating systems are well documented, thus\ngiving them an advantage over other systems.\nWhile disappointed that our design did not meet the design parameters, we\nare happy that disaster has been averted. Better to know now than to ﬁnd out when\nit is too late.\nIt seems like 60 ms for an index lookup is a long time. We could improve that\nconsiderably. What if the index was held in RAM? Is this possible? Some quick\ncalculations estimate that the lookup tree would have to be 3 levels deep to fan\nout to enough machines to span this much data. To go up and down the tree is\n5 packets, or about 2.5 ms if they are all within the same datacenter. The new total\n(150 ms+3 ms+2.5 ms+100 ms = 255.5 ms) is less than our total 300 ms budget.\n\n\n1.8\nSummary\n29\nWe would repeat this process for other requests that are time sensitive. For\nexample, we send email messages less frequently than we read them, so the time\nto send an email message may not be considered time critical. In contrast, delet-\ning a message happens almost as often reading messages. We might repeat this\ncalculation for a few deletion methods to compare their efﬁciency.\nOne design might contact the server and delete the message from the stor-\nage system and the index. Another design might have the storage system simply\nmark the message as deleted in the index. This would be considerably faster but\nwould require a new element that would reap messages marked for deletion and\noccasionally compact the index, removing any items marked as deleted.\nEven faster response time can be achieved with an asynchronous design. That\nmeans the client sends requests to the server and quickly returns control to the user\nwithout waiting for the request to complete. The user perceives this system as faster\neven though the actual work is lagging. Asynchronous designs are more complex\nto implement. The server might queue the request rather than actually performing\nthe action. Another process reads requests from the queue and performs them in\nthe background. Alternatively, the client could simply send the request and check\nfor the reply later, or allocate a thread or subprocess to wait for the reply.\nAll of these designs are viable but each offers different speed and complexity of\nimplementation. With speed and cost estimates, backed by prototypes, the business\ndecision of which to implement can be made.\n1.8 Summary\nDistributed computing is different from traditional computing in many ways. The\nscale is larger; there are many machines, each doing specialized tasks. Services are\nreplicated to increase capacity. Hardware failure is not treated as an emergency or\nexception but as an expected part of the system. Thus the system works around\nfailure.\nLarge systems are built through composition of smaller parts. We discussed\nthree ways this composition is typically done: load balancer for many backend\nreplicas, frontend with many different backends, and a server tree.\nThe load balancer divides trafﬁc among many duplicate systems. The front-\nend with many different backends uses different backends in parallel, with each\nperforming different processes. The server tree uses a tree conﬁguration, with each\ntree level serving a different purpose.\nMaintaining state in a distributed system is complex, whether it is a large\ndatabase of constantly updated information or a few key bits to which many sys-\ntems need constant access. The CAP Principle states that it is not possible to build\na distributed system that guarantees consistency, availability, and resistance to\npartitioning simultaneously. At most two of the three can be achieved.\n\n\n30\nChapter 1\nDesigning in a Distributed World\nSystems are expected to evolve over time. To make this easier, the components\nare loosely coupled. Each embodies an abstraction of the service it provides, such\nthat the internals can be replaced or improved without changing the abstraction.\nThus, dependencies on the service do not need to change other than to beneﬁt from\nnew features.\nDesigning distributed systems requires an understanding of the time it takes\nvarious operations to run so that time-sensitive processes can be designed to meet\ntheir latency budget.\nExercises\n1. What is distributed computing?\n2. Describe the three major composition patterns in distributed computing.\n3. What are the three patterns discussed for storing state?\n4. Sometimes a master server does not reply with an answer but instead replies\nwith where the answer can be found. What are the beneﬁts of this method?\n5. Section 1.4 describes a distributed ﬁle system, including an example of how\nreading terabytes of data would work. How would writing terabytes of data\nwork?\n6. Explain the CAP Principle. (If you think the CAP Principle is awesome, read\n“The Part-Time Parliament” (Lamport & Marzullo 1998) and “Paxos Made\nSimple” (Lamport 2001).)\n7. What does it mean when a system is loosely coupled? What is the advantage\nof these systems?\n8. Give examples of loosely and tightly coupled systems you have experience\nwith. What makes them loosely or tightly coupled?\n9. How do we estimate how fast a system will be able to process a request such\nas retrieving an email message?\n10. In Section 1.7 three design ideas are presented for how to process email dele-\ntion requests. Estimate how long the request will take for deleting an email\nmessage for each of the three designs. First outline the steps each would take,\nthen break each one into individual operations until estimates can be created.\n\n\nChapter 2\nDesigning for Operations\nYour job is to design\nsystems that operate.\n—Theo Schlossnagle\nThis chapter catalogs the most common operations tasks and discusses how to\ndesign for them. It also discusses how to rework an existing architecture that was\nnot designed with operations in mind.\nDesigning for operations means making sure all the normal operational func-\ntions can be done well. Normal operational functions include tasks such as periodic\nmaintenance, updates, and monitoring. These issues must be kept in mind in early\nstages of planning.\nWhen you consider the full life cycle of a given service, only a small portion\nof that life cycle is spent building the features of the service. The vast majority of\nthe life cycle is spent operating the service. Yet traditionally the operational func-\ntions of software are considered lower priority than features, if they are considered\nat all.\nThe best strategy for providing a highly available service is to build features\ninto the software that enhance one’s ability to perform and automate operational\ntasks. This is in contrast to strategies where operations is an after-thought and oper-\nations engineers are forced into a position of “running what other people build.”\nThat’s the outdated way.\n2.1 Operational Requirements\nSoftware is usually designed based on requirements related to what the ultimate\nuser will see and do. The functionality required for smooth operations is rarely\nconsidered. As a consequence, systems administrators ﬁnd themselves lacking\ncontrol points for key interactions. When we design for operations, we take into\n31\n\n\n32\nChapter 2\nDesigning for Operations\naccount the normal functions of an infrastructure life cycle. They include, but are\nnot necessarily limited to, the following:\n• Conﬁguration\n• Startup and shutdown\n• Queue draining\n• Software upgrades\n• Backups and restores\n• Redundancy\n• Replicated databases\n• Hot swaps\n• Toggles for individual features\n• Graceful degradation\n• Access controls and rate limits\n• Data import controls\n• Monitoring\n• Auditing\n• Debug instrumentation\n• Exception collection\nFeatures like conﬁguration and backups/restores make typical operational tasks\npossible. Features like queue draining and toggles for individual features allow\nsuch tasks to be done seamlessly. Many of these features create the introspec-\ntion required to debug, tune, and repair large systems, as previously discussed\nin Section 1.1.\nTypically, developers and managers don’t think of these issues and they are\noften left off of lists of requirements. At best, they are after-thoughts or it is assumed\nthat the operations team will “ﬁgure something out” through improvisation. The\ntruth is that these tasks are important and cannot be done well or at all without\nspeciﬁc features. In the worst case, systems are designed in ways that work against\nthe ability to “improvise” a solution.\nRather than the term “operational requirements,” some organizations use the\nterm“non-functionalrequirements.”Weconsiderthistermmisleading.Whilethese\nfeatures are not directly responsible for the function of the application or service, the\nterm “non-functional” implies that these features do not have a function. A service\ncannot exist without the support of these features; they are essential.\nThe remainder of this chapter discusses these operational aspects and features\nthat enable them. Many of these will seem obvious to someone with operational\nexperience. Yet, each of them appears on this list because we have observed at least\none system that suffered from its omission.\n\n\n2.1\nOperational Requirements\n33\n2.1.1 Configuration\nThe system must be conﬁgurable by automated means. This includes initial conﬁg-\nuration as well as changes made later. It must be possible to perform the following\ntasks:\n• Make a backup of a conﬁguration and restore it\n• View the difference between one archived copy and another revision\n• Archive the running conﬁguration without taking the system down\nA typical way to achieve all of these goals is for the conﬁguration to take the form of\na text ﬁle with a well-deﬁned format. Automated systems can easily generate such\na ﬁle. Text ﬁles are easy to parse and therefore auditable. They can be archived eas-\nily. They can also be stored in a source code repository and analyzed with standard\ntext comparison tools such as UNIX diff.\nIn some systems the conﬁguration is dynamically updated as the system runs.\nThis “state” may be reﬂected back into the primary conﬁguration ﬁle or may be a\nseparate entity. In this case there are additional requirements.\nThere must be a way for automation to read and update the state. This step\nmay be carried out through an API or by reading and updating a conﬁguration ﬁle.\nIf a ﬁle is used, a locking protocol must exist to prevent both the service and exter-\nnal automation from reading the ﬁle in an incomplete state and to prevent update\ncollisions. Tools should be available for doing sanity checks on conﬁgurations that\ndo not involve activating the conﬁguration on a live system.\nAn inferior option would be a conﬁguration ﬁle that is an opaque binary blob.\nSuch a ﬁle is not human readable. In these types of systems it is impossible to keep\na history of the conﬁguration and see change over time. Often strange problems\nare debugged by analyzing changes to a conﬁguration ﬁle, where a change may be\ntoo small to be remembered but just big enough to cause a problem. This type of\nanalysis is not possible if the ﬁle is not plain text.\nWe have been burned by systems that provide an API for extracting the entire\nconﬁguration but where the result turns out not to actually represent the entire\nconﬁguration. All too often, the omission is found only during a disaster recovery\nexercise or emergency. For that reason each new release should be tested to verify\nthat the conﬁguration data does not omit anything.\nFrom an operational perspective, the ideal is for the conﬁguration to consist of\none or more plain text ﬁles that can be easily examined, archived, and compared.\nSome systems read their conﬁguration directly from a source code repository,\nwhich is convenient and highly recommended. However, it must also be possible\nto disable this feature and provide conﬁgurations directly. Such an approach may\nbe used in an emergency, when the source code repository is down, and for exper-\nimentation. The use of this feature must be exposed in a way that the monitoring\n",
      "page_number": 55
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 64-71)",
      "start_page": 64,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "content": "34\nChapter 2\nDesigning for Operations\nsystem can detect it. Other users can then be made aware that it is happening—for\nexample, by showing this status in dashboards. It may also be an alertable event,\nin which case alerts can be generated if this feature is used on production systems.\nAlerting if this feature is disabled for more than a certain amount of time assures\nthat temporary ﬁxes are not forgotten.\n.\nEasy Configuration Does Not Require a GUI\nA product manager from IBM once told Tom that the company had spent a lot\nof money adding a graphical user interface (GUI) to a system administration\ntool. This was done to make it easier to conﬁgure. To the team’s dismay, the\nmajority of their customers did not use the GUI because they had written Perl\nscripts to generate the conﬁguration ﬁles.\n2.1.2 Startup and Shutdown\nThe service should restart automatically when a machine boots up. If the machine is\nshut down properly, the system should include the proper operating system (OS)\nhooks to shut the service down properly. If the machine crashes suddenly, the\nnext restart of the system should automatically perform data validations or repairs\nbefore providing service.\nEnsuring that a service restarts after a reboot can be as simple as installing a\nboot-time script, or using a system that monitors processes and restarts them (such\nas Ubuntu Upstart). Alternatively, it can be an entire process management sys-\ntem like Apache Mesos (Metz 2013) or Google Omega (Schwarzkopf, Konwinski,\nAbd-El-Malek & Wilkes 2013), which not only restarts a process when a machine\nreboots, but also is able to restart the process on an entirely different machine in\nthe event of machine death.\nThe amount of time required to start up or shut down a system should be doc-\numented. This is needed for preparing for disaster recovery situations. One needs\nto know how quickly a system can be safely shut down to plan the battery capac-\nity of uninterruptible power supply (UPS) systems. Most UPS batteries can sustain\na system for about ﬁve minutes. After a power outage, starting up thousands of\nservers can be very complex. Knowing expected startup times and procedures can\ndramatically reduce recovery time.\nTesting for how a system behaves when all systems lose power concurrently\nis important. It’s a common datacenter stressor. Thousands of hard disk motors\nspinning up at the same time create a huge power draw that can overload power\n\n\n2.1\nOperational Requirements\n35\nsystems. In general, one can expect 1 to 5 percent of machines to not boot on the\nﬁrst try. In a system with 1000 machines, a large team of people might be required\nto resuscitate them all.\nRelated to this is the concept of “crash-only” software. Candea & Fox (2003)\nobserve that the post-crash recovery procedure in most systems is critical to sys-\ntem reliability, yet receives a disproportionately small amount of quality assurance\n(QA) testing. A service that is expected to have high availability should rarely use\nthe orderly shutdown process. To align the importance of the recovery procedure\nwith the amount of testing it should receive, these authors propose not implement-\ning the orderly shutdown procedure or the orderly startup procedures. Thus, the\nonly way to stop the software is to crash it, and the only way to start it is to exer-\ncise the crash recovery system. In this way, the crash recovery process is exercised\nfrequently and test processes are less likely to ignore it.\n2.1.3 Queue Draining\nThere must be an orderly shutdown process that can be triggered to take the sys-\ntem out of service for maintenance. A drain occurs when the service is told to\nstop accepting new requests but complete any requests that are “in ﬂight.” This\nis sometimes called lame-duck mode.\nThis mechanism is particularly important when using a load balancer with\nmultiple backend replicas, as described in Section 1.3.1. Software upgrades are\nimplemented by removing one replica at a time, upgrading it, and returning it\nto service. If each replica is simply “killed,” any in-ﬂight requests will be lost. It\nis better to have a draining mode, where the replica continues to process requests\nbut intentionally fails the load balancer’s health check requests. If it sees the health\ncheck requests fail, the load balancer stops sending new requests to the replica.\nOnce no new requests have been received for a while and the existing requests are\ncompleted, it is safe to kill the replica and perform the upgrade.\n.\nEmptying the Queue\nWhile developing the pioneering Palm VII wireless messaging service, the\nteam realized the main application did not have the ability to be drained. Any\nattempt to shut it down would lose any messages that were in ﬂight. Strata\nnegotiated to add this feature. This “drain and exit” feature enabled the oper-\nations team to be able to take servers down for maintenance or swap them out\nfor service reliability without losing messages.\n\n\n36\nChapter 2\nDesigning for Operations\nSimilarly, it is useful for the service to be able to start in drained mode. In this\ncase, the load balancer will not send new trafﬁc to the replica, but operations can\nsend messages directly to it for testing. Once conﬁdence is achieved, undraining\nthe replica signals the load balancer to send trafﬁc.\n2.1.4 Software Upgrades\nIt must be possible for software upgrades to be implemented without taking down\nthe service. Usually the software is located behind a load balancer and upgraded\nby swapping out replicas as they are upgraded.\nSome systems can be upgraded while running, which is riskier and requires\ncareful design and extensive testing.\nNonreplicated systems are difﬁcult to upgrade without downtime. Often the\nonly alternative is to clone the system, upgrade the clone, and swap the newly\nupgraded system into place faster than customers will notice. This is usually a\nrisky—and sometimes improvised—solution. Clones of production systems tend\nto be imperfect copies because it is difﬁcult to assure that the clone was made\nprecisely when no changes are in progress.\n2.1.5 Backups and Restores\nIt must be possible to back up and restore the service’s data while the system is\nrunning.\nOften legacy systems must be taken down to do backups or restores. This\napproach may sufﬁce for a small ofﬁce with 10 understanding users, but it is not\nreasonable for a web site with hundreds or millions of users. The design of a system\nthat permits live backups and restores is very different\nOne way to achieve live backups without interfering with the service is to\nperform the backup on a read-only replica of the database. If the system can\ndynamically add and remove replicas, a replica is removed from service, frozen,\nand used to make the backup. This replica is then added back to the system later.\nIt is common to have a particular replica dedicated to this process.\nLive restores are often done by providing a special API for inserting data dur-\ning a restore operation. The architecture should allow for the restoration of a single\naccount, preferably without locking that user or group out of the service.\nFor example, an email system should be able to restore a single user’s account\nwithout having to restore all accounts. It should be able to do this live so that the\nuser may continue using the service while the restoring messages appear.\nBoth backups and restores create additional load on a system. This burden\nmust be accounted for in capacity planning (headroom) and latency calculations.\n\n\n2.1\nOperational Requirements\n37\n2.1.6 Redundancy\nMany reliability and scaling techniques are predicated on the ability to run mul-\ntiple, redundant replicas of a service. Therefore services should be designed to\nsupport such conﬁgurations. Service replicas are discussed in Section 1.3.1. The\nchallenge of replicating state between replicas is discussed in Section 1.5.\nIf a service wasn’t designed to work behind a load balancer, it may work\nthrough “luck,” which is not a recommended way to do system administration.\nOnly the most rudimentary services will work in such a situation. It is more likely\nthat the system will not work or, worse, will seem to work but develop problems\nlater that are difﬁcult to trace.\nA common issue is that a user’s login state is stored locally by a web server\nbut not communicated to replicas. When the load balancer receives future requests\nfrom the same user, if they are sent to a different replica the user will be asked to\nlog in again. This will repeat until the user has logged into every replica. Solutions\nto this problem are discussed in Section 4.2.3.\n2.1.7 Replicated Databases\nSystems that access databases should do so in a way that supports database scaling.\nThe most common way to scale database access in a distributed system is to cre-\nate one or more read-only replicas. The master database does all transactions that\nmutate (make changes to) the database. Updates are then passed to the read-only\nreplicas via bulk transfers. Services can access the master database as normal, or if a\nquery does not make any mutations it is sent to a read-only replica. Most database\naccess is read-only, so the majority of work is off-loaded to the replicas. The replicas\noffer fast, though slightly out of date, access. The master offers full-service access\nto the “freshest” data, though it might be slower.\nSoftware that uses a database must be speciﬁcally engineered to support read-\nonly replicas. Rather than opening a connection to the database, two connections\nare created: a connection to the database master and a connection to one of the\nread-only replicas. As developers code each query, they give serious consideration\nto which connection it should be sent over, trading off speed for freshness and\nrealizing that every query sent directly to the master “just in case” is consuming\nthe master database’s very precious resources.\nIt is good practice to segregate these kinds of queries even if the database does\nnot have any replicas and both connections go to the same server. Someday you\nwill want to add read-only replicas. Deciding which connection a query should use\nis best done when the query is originally being invented, not months or years later.\nThat said, if you ﬁnd yourself retroﬁtting a system after the fact, it may be a better\nuse of your time to identify a few heavy hitters that can be moved to the read-only\n\n\n38\nChapter 2\nDesigning for Operations\nreplica, rather than examine every single query in the source code. Alternatively,\nyou may create a read-only replica for a speciﬁc purpose, such as backups.\n.\nUnlucky Read-Only Replicas\nAt Google Tom experienced a race condition between a database master and its\nreplicas. The system carefully sent writes to the master and did all reads from\nthe replicas. However, one component read data soon after it was updated and\noften became confused because it saw outdated information from the replica.\nAs the team had no time to recode the component, it was simply reconﬁgured\nso that both the write and read connections went to the master. Even though\nonly one read query out of many had to go to the master, all were sent to the\nmaster. Since the component was used just once or twice a day, this did not\ncreate an undue burden on the master.\nAs time went on, usage patterns changed and this component was used\nmore frequently, until eventually it was used all day long. One day there was\nan outage because the master became overloaded due to the load from this\ncomponent. At that point, the component had to be re-engineered to properly\nsegregate queries.\nOne more warning against relying on luck rather than ofﬁcial support from\nthe developers: luck runs out. Relying on luck today may result in disaster at the\nnext software release. Even though it may be a long-standing policy for the oper-\nations staff, it will appear to the developers as a “surprise request” to support this\nconﬁguration. The developers could rightfully refuse to ﬁx the problem because\nit had not been a supported conﬁguration in the ﬁrst place. You can imagine the\nconfusion and the resulting conﬂict.\n2.1.8 Hot Swaps\nService components should be able to be swapped in or out of their service roles\nwithout triggering an overall service outage. Software components may be self-\nsufﬁcient for completing a hot swap or may simply be compatible with a load\nbalancer or other redirection service that controls the process.\nSome physical components, such as power supplies or disk drives, can be\nswapped while still electrically powered on, or “hot.” Hot-swappable devices can\nbe changed without affecting the rest of the machine. For example, power sup-\nplies can be replaced without stopping operations. Hot-pluggable devices can be\ninstalled or removed while the machine is running. Administrative tasks may be\nrequired before or after this operation is performed. For example, a hard drive may\n\n\n2.1\nOperational Requirements\n39\nnot be recognized unless the operating system is told to scan for new disks. A new\nnetwork interface card may be recognized, but the application server software may\nnot see it without being restarted unless it has been speciﬁcally programmed to\nperiodically scan for new interfaces.\nIt is often unclear what vendors mean by “hot-pluggable” and “hot-\nswappable.” Directly test the system to understand the ramiﬁcations of the process\nand to see how application-level software responds.\n2.1.9 Toggles for Individual Features\nA conﬁguration setting (a toggle) should be present to enable or disable each new\nfeature. This allows roll-outs of new software releases to be independent of when\nthe new feature is made available to users. For example, if a new feature is to appear\non the site precisely at noon on Wednesday, it is very difﬁcult to coordinate a new\nbinary “push” exactly at that time. However, if each new feature can be individu-\nally enabled, the software can be deployed early and the feature can be enabled via\nchanging a conﬁguration setting at the desired time. This is often called a flag flip.\nThis approach is also useful for dealing with new features that cause problems.\nIt is easier to disable the individual feature via a ﬂag ﬂip than to roll back to the\nprevious binary.\nMore sophisticated toggles can be enabled for particular groups of users.\nA feature may be enabled for a small group of trusted testers who receive early\naccess. Once it is validated, the toggle can enable the feature for all users, perhaps\nby enabling it for successively larger groups.\nSee Section 11.7 for more details.\n2.1.10 Graceful Degradation\nGraceful degradation means software acts differently when it is becoming over-\nloaded or when systems it depends on are down. For example, a web site might\nhave two user interfaces: one is rich and full of images, while the other is\nlightweight and all text. Normally users receive the rich interface. However, if\nthe system is overloaded or at risk of hitting bandwidth limits, it switches to the\nlightweight mode.\nGraceful degradation also requires the software to act smartly during outages.\nA service may become read-only if the database stops accepting writes (a com-\nmon administrative defense when corruption is detected). If a database becomes\ncompletely inaccessible, the software works from its cache so that users see partial\nresults rather than an error message.\nWhen a service can no longer access services it depends on, related features\nmay disappear rather than just displaying a broken web page or a “404 page not\nfound” error.\n\n\n40\nChapter 2\nDesigning for Operations\nEven small sites have learned that it is better to put up a temporary web server\nthat displays the same “under construction” page no matter what the query, than\nto have the users receive no service at all. There are simple web server software\npackages made just for this situation.\n.\nCase Study: Graceful Degradation in Google Apps\nGoogle Docs deploys many graceful degradation techniques. Google’s word\nprocessor can switch into read-only mode when only a read-only database\nreplica is available. The client-side JavaScript can work with the cached data\nin the browser if the server is inaccessible. Gmail provides a rich, JavaScript-\nbased user interface as well as a slimmer HTML-only interface that appears\nautomatically as needed. If the entire system is unavailable, the user is sent to a\ngeneric front page that displays the system status rather than simply receiving\nno response.\n2.1.11 Access Controls and Rate Limits\nIf a service provides an API, that API should include an Access Control List (ACL)\nmechanism that determines which users are permitted or denied access, and also\ndetermines rate-limiting settings.\nAn ACL is a list of users, along with an indication of whether they are\nauthorized to access the system. For example, access could be restricted to cer-\ntain Internet Protocol (IP) addresses or blocks, to certain users or processes, or by\nother identiﬁcation mechanisms. IP addresses are the weakest form of identiﬁca-\ntion because they can be easily forged. Something better should be used, such as a\npublic key infrastructure (PKI) that uses digital certiﬁcates to prove identity.\nThe most simple ACL is a list of users that are permitted access; everyone else\nis banned. This is called a default closed policy; the list is called the whitelist. The\nreverse would be a default open policy, where the default is to give access to all\nusers unless they appear on a blacklist.\nA more sophisticated ACL is an ordered list of users and/or groups annotated\nas either being “permitted” or “denied.” If a user is not mentioned in the ACL, the\ndefault action might be to permit the user (fail open) or, alternatively, to deny the\nuser (fail closed).\nIn addition to indicating permission, ACLs can indicate rate limits. Different\nusers might be permitted different queries per second (QPS) rates, with requests\nthat go over that rate being denied. For example, the service may give premium\n\n\n2.1\nOperational Requirements\n41\ncustomers an unlimited QPS rate, regular paid customers a moderate QPS rate,\nand unpaid customers a low QPS rate or no access at all.\n.\nCase Study: ACLs at Google\nGoogle’s Remote Procedure Call (RPC) protocol, used by all internal APIs, has\na powerful ACL system. Connections are authenticated via a PKI so that the\nservice is assured of the client’s identity and knows which groups that client is\na member of. Identity and groups are globally deﬁned and represent products\nand services as opposed to individual external customers. The ACLs specify\nthe access allowed for that individual or group: permit, deny, or permit with\na rate limit. Teams negotiate QPS rates for accessing a service as part of the\nservice’s capacity planning. Teams that have not negotiated rates get access\nbut at a very low rate limit. This enables all teams to try out new services and\neliminates the need for the service team to expend effort negotiating hundreds\nof lightweight or casual use requests.\n2.1.12 Data Import Controls\nIf a service periodically imports data, mechanisms should be established that\npermit operations staff to control which data is accepted, rejected, or replaced.\nThe quality of incoming data varies, and the system importing the data needs\na way to restrict what is actually imported so that known bad data can be disre-\ngarded. If a bad record causes a problem with the system, one must be able to block\nit via conﬁguration rather than waiting for a software update.\nSuch a system uses the same whitelist/blacklist terminology we saw earlier.\nA blacklist is a way of specifying input that is to be rejected, with the assump-\ntion that all other data is accepted. A whitelist is used to specify data that is to be\naccepted; all other data is rejected.\nIn addition to control the incoming data stream, we need a way to augment\nan imported data source with locally provided data. This is accomplished using an\naugmentation ﬁle of data to import.\nEstablishing a change limit can also prevent problems. For example, if a\nweekly data import typically changes less than 20 percent of all records, one might\nwant to require manual approval if the change will affect 30 or more percent of all\nrecords. This can prevent a disaster caused by a software bug or a bad batch of new\ndata.\n",
      "page_number": 64
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 72-79)",
      "start_page": 72,
      "end_page": 79,
      "detection_method": "topic_boundary",
      "content": "42\nChapter 2\nDesigning for Operations\n.\nCase Study: Google Maps Local Business Listing\nGoogle subscribes to listings of businesses from various “local business direc-\ntory” services for use in its map-related products. These information providers\nperiodically send data that must be processed and imported into the map\nsystem. The quality of the data is disheartening: listings are often incorrect,\nmangled, or somehow useless. Corrections are reported to the provider but\ncould take months to appear in the data feed. Some sources provide data that\nhas good quality for certain states and countries but categorically bad data for\nothers. Therefore the system that imports this data has a whitelist, a blacklist,\nand an augmentation ﬁle.\nThe whitelist indicates which regions to include. It is used when the infor-\nmation from a particular provider might be of high quality only for certain\nregions. Once the quality of data for a particular region is veriﬁed, it is added\nto the whitelist. If the quality drops, the data for that region is removed.\nThe blacklist identiﬁes known bad records. It includes records that have\nbeen identiﬁed as bad or incorrect in past batches.\nThe data from the business directories is augmented by data that Google\nproduces independently of the source. This includes information sourced by\nGoogle itself, overrides for known bad data that Google has independently\ncorrected, and “Easter eggs” (jokes to be included in the listings).\n2.1.13 Monitoring\nOperations requires visibility into how the system is working. Therefore each com-\nponent of a system must expose metrics to the monitoring system. These metrics\nare used to monitor availability and performance, for capacity planning, and as\npart of troubleshooting.\nChapters 16 and 17 cover this topic in greater detail.\n2.1.14 Auditing\nLogging, permissions, and role accounts are set up to enable the service to be exam-\nined for, and pass, security and compliance audits. This area is changing rapidly,\nso it is always best to consult your legal department for information about the lat-\nest laws. The biggest concerns for corporations that are considering using public\ncloud services revolve around compliance with the relevant laws governing their\nbusiness: if they choose to use a public cloud service, will they fail their next audit,\nand subsequently face massive ﬁnes or be prevented from conducting business\nuntil they pass another audit?\n\n\n2.1\nOperational Requirements\n43\nAlthough local labor laws usually do not directly affect compliance or gov-\nernance issues, some countries strongly believe that people from other countries\ndoing IT administration can be a compliance issue. Consider the system admin-\nistrator who is sitting in Orange, New Jersey, and doing some administration on\na server in Frankfurt, Germany. The system administrator has no knowledge of\nthe local labor laws or the European Union (EU) Data Protection Directive and\nmoves a virtual server across the company intranet as a scheduled and approved\nchange. That system administrator may have violated at least two EU mandates,\ninadvertently making her employer non compliant and subject to sanctions, ﬁnes,\nor both.\nExamples of regulations with speciﬁc IT audit requirements are SOX, j-SOX,\nc-SOX, PCI DSS, the EU Data Protection Directive, and Singapore MAS. More\nthan 150 such regulatory mandates can be found across the world. In addition,\nsome global standards apply to various governance scenarios, such as CobiT 5 and\nISO/IEC 27001, 27002, and 27005. IT governance and compliance are covered more\nfully in Volume 1 of this series (Limoncelli, Hogan & Chalup, forthcoming 2015).\n2.1.15 Debug Instrumentation\nSoftware needs to generate logs that are useful when debugging. Such logs should\nbe both human-readable and machine-parseable. The kind of logging that is appro-\npriate for debugging differs from the kind of logging that is needed for audit-\ning. A debug log usually records the parameters sent to and returned from any\nimportant function call. What constitutes “important” varies.\nLarge systems should permit debug logging to be enabled on individual\nmodules. Otherwise, the volume of information can be overwhelming.\nIn some software methodologies, any logged information must be matched\nwith documentation that indicates what the message means and how to use it.\nThe message and the documentation must be translated into all (human) lan-\nguages supported by the system and must be approved by marketing, product\nmanagement, and legal personnel. Such a policy is a fast path to ending any and\nall productivity through bureaucratic paralysis. Debugging logs should be exempt\nfrom such rules because these messages are not visible to external users. Every\ndeveloper should feel empowered to add a debug logging statement for any infor-\nmation he or she sees ﬁt. The documentation on how to consume such information\nis the source code itself, which should be available to operations personnel.\n2.1.16 Exception Collection\nWhen software generates an exception, it should be collected centrally for anal-\nysis. A software exception is an error so severe that the program intentionally\nexits. For example, the software author may decide that handling a particular\n\n\n44\nChapter 2\nDesigning for Operations\nsituation is unlikely to happen and will be difﬁcult to recover from; therefore the\nprogram declares an exception and exits in this situation. Certain data corruption\nscenarios are better handled by a human than by the software itself. If you’ve ever\nseen an operating system “panic” or present a “blue screen of death,” that is an\nexception.\nWhen designing software for operability, it is common to use a software\nframework that detects exceptions, gathers the error message and other informa-\ntion, and submits it to a centralized database. Such a framework is referred to as\nan exception collector.\nException collection systems offer three beneﬁts. First, since most software\nsystems have some kind of automatic restart capability, certain exceptions may\ngo unnoticed. If you never see that the exceptions are occurring, of course, you\ncan’t deal with the underlying causes. An exception collector, however, makes the\ninvisible visible.\nSecond, exception collection helps determine the health of a system. If there\nare many exceptions, maintenance such as rolling out new software releases should\nbe cancelled. If a sharp increase in exceptions is seen during a roll-out, it may be\nan indication that the release is bad and the roll-out should stop.\nThe third beneﬁt from using an exception collector is that the history of excep-\ntions can be studied for trends. A simple trend to study is whether the sheer volume\nof exceptions is going up or down. Usually exception levels can be correlated to a\nparticular software release. The other trend to look for is repetition. If a particular\ntype of exception is recorded, the fact that it is happening more or less frequently\nis telling. If it occurs less frequently, that means the software quality is improving.\nIf it is increasing in frequency, then there is the opportunity to detect it and ﬁx the\nroot cause before it becomes a bigger problem.\n2.1.17 Documentation for Operations\nDevelopers and operational staff should work together to create a playbook of\noperating procedures for the service. A playbook augments the developer-written\ndocumentation by adding operations steps that are informed by the larger busi-\nness view. For example, the developers might write the precise steps required to\nfail over a system to a hot spare. The playbook would document when such a\nfailover is to be done, who should be notiﬁed, which additional checks must be\ndone before and after failover, and so on. It is critical that every procedure include\na test suite that veriﬁes success or failure. Following is an example database failover\nprocedure:\n1. Announce the impending failover to the db-team and manager-team mailing\nlists.\n2. Verify the hot spare has at least 10 terabytes of free disk space.\n\n\n2.2\nImplementing Design for Operations\n45\n3. Verify these dependencies are all operating within parameters: (link to server\ncontrol panel) (link to data feed control panel).\n4. Perform the failover using the system failover procedure (link).\n5. Verify that these dependencies have successfully switched to the hot spare and\nare operating correctly.\n6. Reply-all to the previously sent email regarding the operation’s success or\nfailure.\nDocumenting the basic operational procedures for a service cannot be the sole\nresponsibility of either the development or operations team. Instead, it needs to\nbe a collaborative effort, with the operations team ensuring that all the operational\nscenarios they can foresee are addressed, and the developers ensuring that the doc-\numentation covers all error situations that can arise in their code, and how and\nwhen to use the supporting tools and procedures.\nDocumentation is a stepping stone to automation. Processes may change fre-\nquently when they are new. As they solidify, you can identify good candidates for\nautomation (see Chapter 12). Writing documentation also helps you understand\nwhat can be automated easily and what will be more challenging to automate,\nbecause documenting things means explaining the steps in detail.\n2.2 Implementing Design for Operations\nFeatures designed for operations need to be implemented by someone; they are\nnot magically present in software. For any given project, you will ﬁnd that the\nsoftware may have none, some, or all of the features listed in this chapter. There\nare four main ways that you can get these features into software:\n• Build them in from the beginning.\n• Request features as they are identiﬁed.\n• Write the features yourself.\n• Work with a third-party vendor.\n2.2.1 Build Features in from the Beginning\nIn this instance, a savvy development and operations team has built the features\ninto the product you are using to run your service. It is extremely rare that you will\nencounter this scenario outside of large, experienced shops like Google, Facebook,\nor Yahoo.\nIf you are fortunate enough to be involved in early development of a system,\nwork with developers and help them set priorities so that these features are “baked\nin” from the beginning. It also helps if the business team driving the requirements\ncan recognize the needs of operations.\n\n\n46\nChapter 2\nDesigning for Operations\n2.2.2 Request Features as They Are Identified\nMore likely a service does not contain all of the operational features desired\nbecause it is impossible to know what will be needed before the system is in oper-\nation. If you have access to the developers, these features can be requested over\ntime. First and foremost, speak up about your operations needs—ﬁle a feature\nrequest for every missing feature. The feature request should identify the prob-\nlem that needs to be solved rather than the speciﬁc implementation. List the risks\nand impact to the business so that your request can be prioritized. Work collabora-\ntively with the developers as they implement the features. Make yourself available\nfor consultation with the developers, and offer encouragement.\nDeveloper time and resources are limited, so it is important to prioritize your\nrequests. One strategy for prioritization is to select the item that will have the\nbiggest impact for the smallest amount of effort. Figure 2.1 shows a graph where\nthe x-axis is the expected impact of a change, ranging from low- to high-impact. The\ny-axis represents the amount of effort required to create the change, also ranging\nfrom easy (low effort) to hard (high effort). It is tempting to focus on the easiest\ntasks or “low-hanging fruit.” However, this often ends up wasting resources on\neasy tasks that have very little impact. That outcome may be emotionally satisfying\nbut does not solve operational problems. Instead, you should focus on the high-\nimpact items exclusively, starting with the low-effort projects while selectively\nchoosing the ones that require larger effort.\nFixing the biggest bottleneck usually has the biggest impact. This point will\nbe discussed in greater detail in Section 12.4.3.\nOne of the differences we have found between high-performing teams and\nlow-performing teams is that the high-performing teams focus on impact.\n.\nFigure 2.1: Implementation priorities for design for operations\n\n\n2.2\nImplementing Design for Operations\n47\n.\nCase Study: An 80/20 Rule for Operational Features\nWhen Tom was at Lumeta, a disagreement arose over how much developer\ntime should be spent on operational issues versus new features. The prod-\nuct manager came up with a very creative solution. The product alternated\nbig releases and small releases. Big releases were expected to have major new\nfeatures. Small releases were expected to ﬁx bugs from the previous major\nrelease.\nIt was negotiated that big releases would have 20 percent of developer\ntime spent on issues requested by the operations team. The small releases\nwere not intended to add major new features, but it was useful to have one or\ntwo high-priority features included. Therefore, for small releases, 80 percent\nof developer time was spent on operational requests. Since the releases were\nsmaller, the same number of hours was spent on operational requests for both\nbig and small releases.\n2.2.3 Write the Features Yourself\nWhen developers are unwilling to add operational features, one option is to write\nthe features yourself. This is a bad option for two reasons.\nFirst, the developers might not accept your code. As an outsider, you do not\nknow their coding standards, the internal infrastructure, and their overall vision\nfor the future software architecture. Any bugs in your code will receive magniﬁed\nblame.\nSecond, it sets a bad precedent. It sends a message that developers do not need\nto care about operational features because if they delay long enough you’ll write\nthem yourself.\nOperational staff should spend time coding operational services that create\nthe ecosystem in which services run. Write frameworks that developers can use to\nimprove operations. For example, write a library that can be linked to that makes\nit easy to report status to the monitoring system. Write tools that let developers be\nself-sufﬁcient rather than dependent on operations. For example, write a tool that\ngathers exceptions and core dumps for analysis rather than emailing requests to\noperations anytime that step is needed.\nThere are exceptions to this rule. Code submissions from outsiders are easier\nto accept when they are small. In a highly collaborative organization, people may\nsimply be more accustomed to receiving code contributions from many sources.\nThis is typical on open source projects. At Google there is a code approval pro-\ncess that makes it easy for outsiders to contribute to a project and assure the code\nmeets team standards. This system permits feedback and revisions until the code\n\n\n48\nChapter 2\nDesigning for Operations\nchange is deemed acceptable by both parties; only then is the code accepted into\nthe system. It also helps that there is a corporate-wide high standard for code qual-\nity and style. In such a system, the code quality and style you are used to writing\nwill probably be compatible with that of other teams.\nWhen possible, operational staff should be embedded with developers so they\ncan learn the code base, become familiar with the release and testing process, and\nbuild a relationship with the code. However, even that is no replacement for getting\noperational features added by the developers themselves. Therefore it is better to\nembed developers with the operations staff, possibly in six-month rotations, so that\nthey understand the operational need for such features.\nWhat works best may be dictated by the size of the organization and the scale\nof the system being operated. For example, a high degree of collaboration may be\neasier to achieve in small organizations.\n2.2.4 Work with a Third-Party Vendor\nWorking with a third-party vendor is quite similar to working with your own\ndevelopment team. Many of the same processes need to be followed, such as ﬁling\nbugs and having periodic meetings to discuss feature requests.\nAlways raise the visibility of your issues in a constructive way, as vendors are\nsensitive to criticism of their products. For example, write a postmortem report that\nincludes the feature request so that the vendor can see the context of the request.\n(See Section 14.3.2 for more details on writing good postmortem reports.)\nIf the vendor is unresponsive to your requests, you may be able to write code\nthat builds frameworks around the vendor’s software. For example, you might\ncreate a wrapper that provides startup and shutdown services in a clean manner\naround vendor software that handles those tasks ungracefully. We highly recom-\nmend publishing such systems externally as open source products. If you need\nthem, someone else will, too. Developing a community around your code will\nmake its support less dependent on your own efforts.\n2.3 Improving the Model\nGood design for operations makes operations easy. Great design for operations\nhelps eliminate some operational duties entirely. It’s a force multiplier often equiv-\nalent to hiring an extra person. When possible, strive to create systems that embed\nknowledge or capability into the process, replacing the need for operational inter-\nvention. The job of the operations staff then changes from performing repetitive\noperational tasks to building, maintaining, and improving the automation that\nhandles those tasks.\n\n\n2.4\nSummary\n49\nTom once worked in an environment where resource allocations were\nrequested via email and processed manually. Once an API was made available,\nthe entire process became self-service; users could manage their own resources.\nSome provisioning systems let you specify how much RAM, disk, and CPU\neach “job” will need. A better system does not require you to specify any resources\nat all: it monitors use and allocates what is needed, maintaining an effective balance\namong all the jobs on a cluster, and reallocating them and shifting resources around\nover time.\nA common operational task is future capacity planning—that is, predicting\nhow many resources will be needed 3 to 12 months out. It can be a lot of work.\nAlternatively, a thoughtfully constructed data collection and analysis system can\nmake these predictions for you. For more information on capacity planning, see\nChapter 18.\nCreating alert thresholds and ﬁne-tuning them can be an endless task. That\nwork can be eliminated if the monitoring system sets its own thresholds. For exam-\nple, one web site developed an accurate prediction model for how many QPS it\nshould receive every hour of the year. The system administrators could then set\nan alert if the actual QPS was more than 10 percent above or below the prediction.\nMonitoring hundreds of replicas around the world can’t be done manually without\nhuge investments in staff. By eliminating this operational duty, the system scaled\nbetter and required less operational support.\n2.4 Summary\nServices should include features that beneﬁt operations, not just the end users.\nFeatures requested by operations staff are aimed at building a stable, reliable, high-\nperforming service, which scales well and can be run in a cost-effective manner.\nEven though these features are not directly requested by customers, the better\noperational effectiveness ultimately beneﬁts the customer.\nOperations staff need many features to support day-to-day operations. They\nalso need full documentation of all the operational processes, failure scenarios, and\nfeature controls. They need authentication, authorization, and access control mech-\nanisms, as well as rate-limiting functionality. Operations staff need to be able to\nenable and disable new features with toggles, globally for roll-out and roll-back,\nand on a per-user basis for beta testing and premium services.\nServices should degrade gracefully when there are problems, rather than\nbecome completely unusable. Services that import data from other sources must\nallow the operations staff to apply controls to those data sources, based on their\ndata quality or other criteria.\n",
      "page_number": 72
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 80-87)",
      "start_page": 80,
      "end_page": 87,
      "detection_method": "topic_boundary",
      "content": "50\nChapter 2\nDesigning for Operations\nSystems are easier to maintain when this functionality is designed into them\nfrom inception. Operations staff can work with developers to ensure that opera-\ntional features are included in a system, particularly if the software is developed\nin-house.\nExercises\n1. Why is design for operations so important?\n2. How is automated conﬁguration typically supported?\n3. List the important factors for redundancy through replication.\n4. Give an example of a partially implemented process in your current environ-\nment. What would you do to fully implement it?\n5. Why might you not want to solve an issue by coding the solution yourself?\n6. Which type of problems should appear ﬁrst on your priority list?\n7. Which factors can you bring to an outside vendor to get the vendor to take\nyour issue seriously?\n\n\nChapter 3\nSelecting a Service Platform\nWhen I hear someone touting the\ncloud as a magic-bullet for all\ncomputing problems, I silently\nreplace “cloud” with “clown” and\ncarry on with a zen-like smile.\n—Amy Rich\nA service runs on a computing infrastructure called a platform. This chapter pro-\nvides an overview of the various types of platforms available in cloud computing,\nwhat each of them provides, and their strengths and weaknesses. It does not offer\nan examination of speciﬁc products but rather a categorization that will help you\nunderstand the variety of offerings. Strategies for choosing between these different\nservices are summarized at the end of the chapter.\nThe term “cloud” is ambiguous; it means different things to different people\nand has been made meaningless by marketing hype. Instead, we use the following\nterms to be speciﬁc:\n• Infrastructure as a Service (IaaS): Computer and network hardware, real or\nvirtual, ready for you to use.\n• Platform as a Service (PaaS): Your software running in a vendor-provided\nframework or stack.\n• Software as a Service (SaaS): An application provided as a web site.\nFigure 3.1 depicts the typical consumer of each service. SaaS applications are\nfor end users and fulﬁll a particular market niche. PaaS provides platforms for\ndevelopers. IaaS is for operators looking to build their own platforms on which\napplications will be built, thus providing the most customizability.\nIn this chapter, we will discuss these services in terms of being provided by a\nthird-party vendor since that is the general case.\n51\n\n\n52\nChapter 3\nSelecting a Service Platform\nFigure 3.1: The consumers of SaaS, PaaS, and IaaS\nA platform may be described along three axes:\n• Level of service abstraction: IaaS, PaaS, SaaS\n• Type of machine: Physical, virtual, or process container\n• Level of resource sharing: Shared or private\n3.1 Level of Service Abstraction\nAbstraction is, essentially, how far users are kept from the details of the raw\nmachine itself. That is, are you offered a raw machine (low abstraction) or are ser-\nvices provided as a high-level API that encapsulates what you need done rather\nthan how to do it (high abstraction)? The closer you are to the raw machine, the\nmore control you have. The higher the level of abstraction, the less you have to\nconcern yourself with technical details of building infrastructure and the more you\ncan focus on the application.\n3.1.1 Infrastructure as a Service\nIaaS provides bare machines, networked and ready for you to install the operating\nsystem and your own software. The service provider provides the infrastructure\nso that the customer can focus on the application itself.\nThe machines provided by the vendor are usually virtual machines but may\nbe physical machines. The provider takes care of the infrastructure: the machines\nthemselves, power, cooling, and networking, providing internet access, and all\ndatacenter operations.\n\n\n3.1\nLevel of Service Abstraction\n53\n.\nTerms to Know\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nOversubscribed: A system that provides capacity X is used in a place where\nY capacity is needed, when X < Y. Used to describe a potential or actual\nneed.\nUndersubscribed: The opposite of oversubscribed.\nAlthough the service provider manages its layers of the infrastructure, IaaS\ndoes not relieve you from all work. A lot of work must be done to coordinate all\nthe pieces, understand and tune the various parts so they work well together, and\nmanage the operating system (since you have total control of the OS).\nProviders charge for compute time, storage, and network trafﬁc. These costs\nwill affect how your application is architected. Keeping information locally ver-\nsus retrieving it over a network may have different costs, affecting your design\nchoices. If information is accessed frequently over the network, the network\ncharges can be reduced by caching or storing more information locally. However,\nthe additional local storage may have its own cost. Such engineering details are\nimportant, because otherwise you may ﬁnd yourself with a startlingly large bill\nat the end of the month. These are important points to work out with the devel-\nopment and business teams. The software and operational choices have real costs\nand tradeoffs.\nThe performance characteristics of providers may vary wildly. When compar-\ning providers, it is important to benchmark local storage, remote storage, CPU, and\nnetwork performance. Some providers’ remote storage is signiﬁcantly faster than\nothers. Repeat any such benchmarks at different times of the day—some service\nproviders may experience high packet loss at daily peak times. Design decisions\nmade for one provider may not be the right choice for other providers.\nWithin an IaaS offering, partitions or “reliability zones” segment the service\ngeographically to provide regional uptime guarantees. While all attempts are made\nto ensure reliable service, it is inevitable that some downtime will be required for\nmaintenance or due to unavoidable circumstances such as a natural disaster. The\nservice provider should segment its service into multiple zones and provide guar-\nantees that planned downtime will not occur in multiple zones at the same time.\nEach zone should be far enough apart from the others that natural disasters are\nunlikely to strike more than one zone at a time. This permits customers to keep\ntheir service in one zone and fail over to another zone if necessary.\n\n\n54\nChapter 3\nSelecting a Service Platform\nFor example, a service provider may offer four zones: U.S. East Coast, U.S.\nWest Coast, Western Europe, and Eastern Europe. Each zone is built and man-\naged to have limited dependencies on the others. At a minimum, customers should\nlocate a service in one zone with plans for failover in another zone. A more sophis-\nticated plan would be to have the service run in each zone with load balancing\nbetween all locations, automatically shifting trafﬁc away from any zone that is\ndown. We will cover this in more detail in Chapter 6.\nSuch geographic diversity also permits customers to better manage the latency\nof their service. Information takes time to travel, so it is generally faster to provide\nservice to someone from a nearby datacenter. For example, a service may be archi-\ntected such that a user’s data is stored in one zone with a backup kept in one other\nzone. Users from New York would have their data stored in the U.S. East Coast\nzone, with backup copies stored in the Western Europe zone. During an outage,\nthe user is served from the backup zone; the service would not be as fast in such a\ncase, but at least the data would be accessible.\nIaaS providers have expanded beyond offering just simple machines and net-\nworks. Some provide a variety of storage options, including relational (SQL) and\nnon-relational (NoSQL or key/value) databases, high-speed storage options, and\ncold storage (bulk data storage that is inexpensive but has latency on the order\nof hours or days). More advanced networking options include virtual private\nnetwork (VPN)–accessible private networks and load balancing services. Many\nprovide both local load balancing and global load balancing, as will be described\nin Chapter 4. Some provide elastic scaling services, which automatically allocate\nand conﬁgure additional machines on demand as capacity is needed.\nProviders that offer both IaaS and PaaS often blur the line between the two by\nproviding high-level managed services that are available to both.\n3.1.2 Platform as a Service\nPaaS enables you to run your applications from a vendor-provided framework.\nThese services offer you a high level of value, as they manage all aspects of the\ninfrastructure, even much of the application stack. They offer very elastic scaling\nservices, handling additional load without any input required from you. Generally\nyou are not even aware of the speciﬁc resources dedicated to your application.\nFor example, in Google AppEngine, you upload application-level software\nand Google takes care of the rest. The framework (platform) automatically pro-\nvides load balancing and scaling. The more active your users are, the more\nmachines Google allocates to your application. Internally the system is managing\nbandwidth, CPU allocations, and even authentication. Your application and hun-\ndreds of others might be sharing the same machine or your application may require\nthe resources of hundreds of dedicated machines. You do not have to manage such\ndecisions except to limit resource use to control costs.\n\n\n3.1\nLevel of Service Abstraction\n55\nPaaS providers charge for their services based on how much CPU, band-\nwidth, and storage are used. This is similar to IaaS except the charges are higher to\ncompensate for the more extensive framework that is provided.\nThe downside of PaaS is that you are restricted to using what the vendor’s\nplatform provides. The platform is generally programmable but not necessarily\nextensible. You do not have direct access to the operating system. For instance,\nyou may not be able to add binaries or use popular libraries until the vendor makes\nthem part of its service. Generally processes run in a secure “jail” (similar to UNIX’s\n“chroot” restricted environment), which aims to prevent them from breaking out of\nthe service’s framework. For example, one PaaS offered the Python language but\nnot the Python Imaging Library (PIL). It could not be installed by users because\nthe framework does not permit Python libraries that include portions written in\ncompiled languages.\nPaaS provides many high-level services including storage services, database\nservices, and many of the same services available in IaaS offerings. Some offer\nmore esoteric services such as Google’s Machine Learning service, which can\nbe used to build a recommendation engine. Additional services are announced\nperiodically.\n3.1.3 Software as a Service\nSaaS is what we used to call a web site before the marketing department decided\nadding “as a service” made it more appealing. SaaS is a web-accessible application.\nThe application is the service, and you interact with it as you would any web site.\nThe provider handles all the details of hardware, operating system, and platform.\nSome common examples include Salesforce.com, which replaces locally run\nsales team management software; Google Apps, which eliminates the need for\nlocally run email and calendaring software; and Basecamp, which replaces locally\nrun project management software. Nearly any business process that is common\namong many companies is offered as SaaS: human resources (HR) functions such\nas hiring and performance management; accounting functions such as payroll,\nexpense tracking, and general ledgers; IT incident, request, and change manage-\nment systems; and many aspects of marketing and sales management.\nThe major selling point of SaaS is that customers do not have to concern them-\nselves with software installation, upgrades, and operations. There is no client soft-\nware to download. The service is fully managed, upgraded, and maintained by the\nprovider.Becausetheserviceisaccessedviatheweb,itcanbeusedfromanylocation.\nAs a SaaS provider, you need to design the service to obscure upgrades and\nother operational details. Developers must avoid features that require client soft-\nware or browser plug-ins. In designing the service you need to recognize that since\nit can be accessed from anywhere, it will be accessed from anywhere, including\nmobile devices. This affects architecture and security decisions.\n\n\n56\nChapter 3\nSelecting a Service Platform\nMake it easy for customers to get started using your service. Rather than hav-\ning to speak with a salesperson to sign up, signing up should be possible via the\nweb site, possibly requiring submission of a credit card or other payment informa-\ntion. Facebook would not have gotten to where it is today if each user had to ﬁrst\nspeak to a customer service representative and arrange for an account to be cre-\nated. Importing data and enabling features should also be self-service. A product\nlike Salesforce.com would not have been able to grow at the rate it has if importing\ndata or other operations required working with customer support personnel.\nIt is also important that people can leave the service in a self-service manner.\nThis means users should be able to export or retrieve their data and close their\naccounts, even if this makes it easier to leave and move to a competitor. Customers\nwill be concerned about their ability to migrate out of the application at the end of\nthe contract or if they are dissatisﬁed. We believe it is unethical to lock people into\na product by making it impossible or difﬁcult to export their data. This practice,\ncalled vendor lock-in, should be considered a “red ﬂag” that the product is not\ntrustworthy. The best way to demonstrate conﬁdence in your product is to make it\neasy to leave. It also makes it easy for users to back up their data.\nMany SaaS offerings are upgraded frequently, often without warning, pro-\nviding little opportunity for training. Users should be able to access major new\nreleases for the purpose of planning, training, and user acceptance testing. Pro-\nvide a mechanism for users to select the day they will be moved to major releases,\nor provide two tracks: a “rapid release” track for customers that want new features\nwithout delay and a “scheduled release” track for customers that would like new\nfeatures to appear on a published schedule, perhaps two to three weeks after the\nrapid release track.\nFinally, conﬂicts may exist between your data privacy and application host-\ning policies and those of your customers. Your privacy policy will need to be\na superset of all your customers’ privacy policies. You may need to provide\nheightened security for certain customers, possibly segmenting them from other\ncustomers.\n3.2 Type of Machine\nThere are three options for the type of machine that a service runs on: physical\nmachine, virtual machine, and process container. The decision between physi-\ncal, virtual, and container is a technical decision. Each has different performance,\nresource efﬁciency, and isolation capabilities. The desired technical attributes\nshould guide your decision on which to use.\nIaaS generally provides the widest variety of options. PaaS generally obscures\nwhat is used, as the user works in a framework that hides the distinction. That said,\nmost PaaS providers use containers.\n\n\n3.2\nType of Machine\n57\n3.2.1 Physical Machines\nA physical machine is a traditional computer with one or more CPUs, and sub-\nsystems for memory, disk, and network. These resources are controlled by the\noperating system, whose job it is to act as the trafﬁc cop coordinating all the pro-\ncesses that want to share these resources. The resources allocated to a running\nprocess (a program running on the system) are actual hardware resources. As a\nresult their performance is relatively predictable.\n3.2.2 Virtual Machines\nVirtual machines are created when a physical machine is partitioned to run a sep-\narate operating system for each partition. Processes running on a virtual machine\nhave little or no awareness that they are not on a physical machine. They cannot\naccess the resources, such as disk or memory, of other virtual machines running\non the same physical machine.\nVirtual machines can make computing more efﬁcient. Physical machines\ntoday are so fast and powerful that some applications do not need the full resources\nof a single machine. The excess capacity is called stranded capacity because it is\nunusable in its current form. Sharing a large physical machine’s power among\nmany smaller virtual machines helps reduce stranded capacity by permitting the\ncreation of virtual machines that are the right size for their requirements.\nStranded capacity can also be mitigated by running multiple servers on\nthe same machine. However, virtualization provides better isolation than simple\nmultitasking.\nFor example, when two applications share a machine, when one application\ngets overloaded or has a problem that causes it to consume large amounts of CPU,\ndisk space, or memory, it will affect the performance of the other application.\nNow suppose those two programs each ran on their own virtual machines, each\nwith a certain amount of CPU, disk space, and memory allocated. This arrange-\nment provides better isolation for each application from problems caused by the\nother.\nSometimes the reason for using virtual machines is organizational. Differ-\nent departments within an organization may not trust each other enough or have\nsufﬁcient cross-department billing options to run software on the same machine.\nNevertheless, they can share a pool of physical machines if each is able to create its\nown virtual machine.\nSometimes the reason for using virtual machines is logistical. Running ﬁve\nservices on one machine requires that any OS patches or upgrades be approved by\nall ﬁve services. If each service runs in its own virtual machine, then upgrades and\npatches can be done on different schedules for different services. In all these cases,\nvirtual machines permit isolation at the OS level.\n",
      "page_number": 80
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "detection_method": "topic_boundary",
      "content": "58\nChapter 3\nSelecting a Service Platform\nBenefits of Virtual Machines\nVirtual machines are fast to create and destroy. There is very little lead time\nbetween when one is requested and when the virtual machine is usable. Some sys-\ntems can spin up a new virtual machine in less than a minute. Consequently, it is\neasy to create a virtual machine for a speciﬁc task and delete it when the task is\ncompleted. Such a virtual machine is called an ephemeral machine or short-lived\nmachine. Some systems create hundreds of ephemeral machines across many phys-\nical machines, run a parallel compute job, and then destroy the machines when the\njob is complete.\nBecause virtual machines are controlled through software, virtualization sys-\ntems are programmable. An API can be used to create, start, stop, modify, and\ndestroy virtual machines. Software can be written to orchestrate these functions on\na large scale. This is not possible with physical machines, which have to be racked,\ncabled, and conﬁgured via manual labor.\nVirtual machine functionality is provided by a combination of virtualization\nsupport in modern CPUs and virtualization control software called the virtual\nmachine monitor (VMM). Modern CPUs have special features for partitioning\nmemory and CPU time to create virtual machines. Access to disk, network, and\nother I/O devices is handled by emulation at the chip or device level.\nSome virtualization systems permit a virtual machine to be moved between\nphysical machines. Like a laptop that is put to sleep and woken up later, the VMM\nputs the machine to sleep, copies its memory and all other state to a different phys-\nical machine, and continues its activities there. The process can be coordinated so\nthat most of the copying happens ahead of time, so that the machine freeze lasts for\nless than a second. This permits a virtual machine to be moved to a different phys-\nical machine when the current one needs to be upgraded or repaired, or moved to\na different failure domain in advance of a planned maintenance outage.\nI/O in a Virtual Environment\nA hardware virtual machine (HVM) performs I/O emulation at the chip level.\nWith HVMs, the virtual machine’s operating system thinks there is, for example,\nan actual SATA hard drive controller installed. This lets the virtual machine use\nan unmodiﬁed operating system. However, such emulation is rather slow. On an\nHVM, every time the OS tries to access the SATA controller, the CPU’s virtual-\nization feature detects this access, stops the virtual machine, and gives control to\nthe VMM. The VMM performs the disk request, emulating a real SATA controller\nand placing the result where the actual chip would have. The virtual machine\nis then allowed to continue where it left off. It sees the result and is none the\nwiser.\nParavirtualization (PV) performs I/O emulation at the device level. PV\nrequires the operating system to be modiﬁed so that the I/O calls it would normally\n\n\n3.2\nType of Machine\n59\nperform are instead done by requests to the VMM. The VMM handles the I/O and\nreturns the result. The modiﬁcations usually take the form of device drivers that\nlook like standard hard disks, video displays, keyboards, and so on, but are actu-\nally talking to the VMM. PV is able to perform the requests more efﬁciently since\nit captures the request at a higher level of abstraction.\nVirtual machines are allocated a ﬁxed amount of disk space, memory, and CPU\nfrom the physical machine. The hard drive a VM sees actually may be a single large\nﬁle on the physical machine.\nDisadvantages of Virtual Machines\nSome resources, such as CPU cores, are shared. Suppose a physical machine has\na four-core CPU. Three virtual cores may be allocated to one virtual machine and\nthree virtual cores may be allocated to another virtual machine. The VMM will\nload-share the six virtual cores on the four physical cores. There may be times when\none virtual machine is relatively idle and does not need all three virtual cores it was\nallocated. However, if both virtual machines are running hard and require all six\nvirtual cores, each will receive a fraction of the CPU’s attention and so will run\nmore slowly.\nA virtual machine can detect CPU contention. In Linux and the Xen hyper-\nvisor, this is called “steal time”: it is the amount of CPU time that your virtual\nmachine is missing because it was allocated to other virtual machines (Haynes\n2013). IaaS providers usually cannot provide guarantees of how much steal time\nwill exist, nor can they provide mechanisms to control it. Netﬂix found the only\nway it could deal with this issue was to be reactionary. If high steal time was\ndetected on a virtual machine in Amazon Web Services (AWS), Netﬂix would\ndelete the virtual machine and have it re-created. If the company was lucky,\nthe new virtual machine would be created on a physical machine that was less\noversubscribed. This is a sorry state of affairs (Link 2013).\nSome resources are shared in an unbounded manner. For example, if one vir-\ntual machine is generating a huge amount of network trafﬁc, the other virtual\nmachines may suffer. This is also typical of disk I/O. A hard drive can perform\nonly so much disk I/O per second, with the amount being limited by the band-\nwidth from the computer to the disk. Where there is a resource shortage such as\ndisk I/O bandwidth, the situation is called resource contention.\nVirtual machines are very heavy-weight. They run a full operating system,\nwhich requires a lot of disk space. They hold on to all the memory allocated to\nthem, even if it isn’t being used. The underlying OS cannot reallocate this memory\nto other machines. Because virtual machines run a complete operating system, the\noperational burden is similar to a full machine that needs to be monitored, patched,\nupgraded, and so on. Also, because a complete operating system is running, each\nOS is running many background service processes such as maintenance tasks and\n\n\n60\nChapter 3\nSelecting a Service Platform\nservice daemons. Those take up resources and add to the operational burden on\nthe system administration team.\n3.2.3 Containers\nA container is a group of processes running on an operating system that are iso-\nlated from other such groups of processes. Each container has an environment with\nits own process name space, network conﬁguration, and other resources. The ﬁle\nsystem to which the processes have access consists of a subdirectory on the host\nmachine. The processes in a particular container see that subdirectory as their root\ndirectory, and cannot access ﬁles outside that subdirectory (and its subdirectories)\nwithout special accommodation from the host machine. The processes all run on\nthe same operating system or kernel. As a consequence, you cannot, for example,\nhave some processes running under Linux and others running under Windows as\nyou can with virtual machines.\nUnlike a virtual machine, which is allocated a large chunk of RAM and disk,\ncontainers consume resources at the same ﬁne-grained level as processes. Thus\nthey are less wasteful.\nProcesses in a container are controlled as a group. If the container is conﬁg-\nured to have a memory limit, the sum total of memory used by all processes in that\ncontainer cannot exceed that limit. If the container is allocated a certain amount of\ndisk bandwidth, that limit is enforced on the processes in the container as a whole.\nSolaris containers, called Zones, can be allocated network interfaces and have their\nnetwork bandwidth regulated to control bandwidth resource contention. Contain-\ners on Linux can assign a different amount of disk cache to each container so that\none container’s buffer thrashing will not affect the buffers of another container.\nProcesses in a container are isolated in other ways. A container can kill or\notherwise interact with only processes in its container. In contrast, processes that\nare not in containers can kill or interact with all processes, even ones in individ-\nual containers. For example, the shell command ps, when running in a FreeBSD\ncontainer (called a “jail”), displays only processes running in that container. This\nis not a parlor trick; the container has no visibility to other processes. However,\nwhen the same command is run on the host from outside any container, it shows\nall processes, including those inside the each container. Thus, if you are logged into\nthe main host (no particular container), you have global visibility and can serve as\nadministrator for all containers.\nEach container has its own copy of the packages, shared libraries, and other\nsupporting ﬁles that it requires. Two containers running on the same machine\ncannot have dependency or version conﬂicts. For example, without containers\none program might require a particular version of a library while another requires\na very different version and cannot operate with the other version installed.\nThis “dependency hell” is common. When each program is put in a different\n\n\n3.2\nType of Machine\n61\ncontainer, however, each can have its own copy of the library and thus the conﬂict\nis avoided.\nContainers are very lightweight because they do not require an entire OS. Only\nthe speciﬁc system ﬁles needed by the software are copied into the container. The\nsystem allocates disk space as ﬁles are needed, as opposed to allocating a large\nvirtual disk ahead of time. A container runs fewer processes because it needs to\nrun only the ones related to the software. System background processes such as\nSSH and other daemons do not run in the container since they are available in the\noutside operating system. When using virtual machines, each machine has a full\ncomplement of such daemons.\nContainers are different from virtual machines. Each virtual machine is a\nblackbox. An administrator logged into the physical machine cannot (without\ntricks) peer into the individual virtual machines. A virtual machine can run a dif-\nferent operating system than its host physical machine because it is emulating a\nfull machine. A virtual machine is a larger, less granular allocation of resources.\nWhen the virtual machine starts, a certain amount of RAM and disk space is allo-\ncated and dedicated to that virtual machine. If it does not use all of the RAM, the\nRAM can’t be used by anything else. Virtual disks are often difﬁcult to resize,\nso you create them larger than needed to reduce the chance that the container\nwill need to be enlarged. The extra capacity cannot be used by other virtual\nmachines—a situation called having stranded resources.\nContainers do share some of the downsides of virtual machines. Downtime\nof the host machine affects all containers. This means that planned downtime for\npatching the host as well as unplanned outages affect all containers. Nevertheless,\nthe host machine doesn’t have to do much, so it can run a stripped-down version\nof the operating system. Thus there is less to patch and maintain.\nSo far we have discussed the technical aspects of containers. What can be done\nwith them, however, is much more exciting.\nContainers are usually the underlying technology in PaaS. They enable\ncustomers to be isolated from each other while still sharing physical machines.\nBecause they consume the exact amount of resources they need at the time,\ncontainers are also much more efﬁcient means of providing such shared services.\nSystems like Docker deﬁne a standardized container for software. Rather than\ndistributing software as a package, one can distribute a container that includes the\nsoftware and everything needed for it to run. This container can be created once\nand run on many systems.\nBeing self-contained, containers eliminate dependencies and conﬂicts. Rather\nthan shipping a software package plus a list of other dependent packages and\nsystem requirements, all that is needed is the standardized container and a sys-\ntem that supports the standard. This greatly simpliﬁes the creation, storage, and\ndelivery and distribution of software. Since many containers can coexist on the\nsame machine, the resulting machine works much like a large hotel that is able to\n\n\n62\nChapter 3\nSelecting a Service Platform\nprovide for many customers, treating them all the same way, even though they are\nall unique.\n.\nStandardized Shipping Containers\nA common way for industries to dramatically improve processes is to stan-\ndardize their delivery mechanism. The introduction of standardized shipping\ncontainers revolutionized the freight industry.\nPreviously individual items were loaded and unloaded from ships, usu-\nally by hand. Each item was a different size and shape, so each had to be\nhandled differently.\nStandardized shipping containers resulted in an entirely different way to\nship products. Because each shipping container was the same shape and size,\nloading and unloading could be done much faster. A single container might\nhold many individual items, but since they were transported as a group, trans-\nferring the items between modes of transport was quick work. Customs could\napprove all the items in a particular container and seal it, eliminating the need\nfor customs checks at remaining hops on the container’s journey as long as the\nseal remained unbroken.\nAsothermodesoftransportationadoptedthestandardshippingcontainer,\nthe concept of intermodal shipping was born. A container would be loaded at\na factory and remain as a unit whether it was on a truck, train, or ship.\nAll of this started in April 1956, when Malcom McLean’s company\nSeaLand organized the ﬁrst shipment using standardized containers from\nNew Jersey (where Tom lives) to Texas. (Levinson 2008).\n3.3 Level of Resource Sharing\nIn a “public cloud,” a third party owns the infrastructure and uses it to provide\nservice for many customers. The sharing may be ﬁne-grained, mixing processing\nand data of different customers on the same machine. Alternatively, the sharing\nmay be more segmented, like tenants in an apartment building with well-deﬁned\npartitions between them. In a “private cloud,” a company runs its own computing\ninfrastructure on its own premises. This infrastructure may be set up for a ded-\nicated internal project or, more commonly, done as an internal service provider\nthat makes the offering available to projects and departments within the company.\nHybrids may also be created, such as private clouds run in rented datacenter space.\nThe choice between private or public use of a platform is a business decision\nbased on four factors: compliance, privacy, cost, and control.\n\n\n3.3\nLevel of Resource Sharing\n63\n3.3.1 Compliance\nCompanies are governed by varying amounts of regulation depending on their\nbusiness, size, locations, and public or private status. Their compliance with all\napplicable regulations can be audited, and failing an audit can have signiﬁcant con-\nsequences, such as the company being unable to conduct business until it passes a\nsubsequent audit.\nUsing a public cloud for certain data or services may cause a company to fail a\ncompliance audit. For example, the EU Data Protection Directive dictates that cer-\ntain data about EU citizens may not leave the EU. Unless the public cloud provider\nhas sufﬁcient controls in place to ensure that will not happen, even in a failover\nscenario, a company that moves the data into the public cloud would fail an audit.\n3.3.2 Privacy\nUsing a public cloud means your data and code reside on someone else’s equip-\nment, in someone else’s facility. They may not have direct access to your data, but\nthey could potentially gain access without your knowledge. Curious employees,\nwith or without malicious intent, could poke around using diagnostic tools that\nwould enable them to view your data. Data might be accidentally leaked by a ser-\nvice provider that disposed of old equipment without properly erasing storage\nsystems.\nBecause of these risks, service providers spell out how they will take care of\nyour data in their contracts. Contracts aside, vendors know that they must earn\ntheir users’ trust if they are to retain them as customers. They maintain that trust\nby being transparent about their policies, and they submit to external audits to\nverify that they are abiding by the rules they set out.\nAnother issue with the public cloud is how law enforcement requests are han-\ndled. If law enforcement ofﬁcials have a warrant to access the data, they can make\na third party provide access without telling you. In contrast, in a private cloud,\ntheir only avenue to access your data involves making you aware of their request\n(although clandestine techniques can be hidden even at your own site).\nThere is also the possibility of accidental exposure of your data. Due to soft-\nware bugs, employee mistakes, or other issues, your data could be exposed to other\ncustomers or the entire world. In a private cloud, the other customers are all from\nthe same company, which may be considered an acceptable risk; the incident can\nbe contained and not become public knowledge. In a public cloud, the exposure\ncould be to anyone, possibly your competitors, and could be front-page news.\n3.3.3 Cost\nThe cost of using a public cloud may or may not be less than the cost of build-\ning the necessary infrastructure yourself. Building such infrastructure requires\n\n\n64\nChapter 3\nSelecting a Service Platform\nlarge amounts of engineering talent, from physical engineering of a datacenter’s\ncooling system, electric service, and design, to technical expertise in running a\ndatacenter and providing the services themselves. All of this can be very expen-\nsive. Amortizing the expense over many customers reduces cost. By comparison,\ndoing it yourself saves money due to the beneﬁts of vertical integration. Vertical\nintegration means saving money by handling all levels of service delivery your-\nself, eliminating the uplift in cost due to “middle men” and service provider proﬁt\nmargins. There is a break-even point where vertical integration becomes more eco-\nnomical. Calculating the total cost of ownership (TCO) and return on investment\n(ROI) will help determine which is the best option.\nDeciding whether the cost of private versus public clouds is appropriate is\nsimilar to making a rent versus own decision about where you live. In the long\nterm, it is less expensive to own a house than to rent one. In the short term, however,\nit may be less expensive to rent. Renting a hotel room for a night makes more sense\nthan buying a building in a city and selling it the next day.\nSimilarly, building a private cloud makes sense if you will use all of it and\nneed it for as long as it takes to pay for itself. Using a third-party provider makes\nsense if the need is small or short term.\n3.3.4 Control\nA private cloud affords you more control. You can specify exactly what kind of\nhardware will be used, which network topology will be set up, and so on. Any fea-\nture you need is a matter of creating it yourself, acquiring it, or licensing it. Changes\ncan happen as fast as your organization can make them. In a public cloud you have\nless control. You must choose from a ﬁnite set of features. While most providers\nare responsive to feature requests, you are merely one customer among many.\nProviders need to focus on those features that will be used by many customers\nand may not be able to provide the specialized features you need.\nLetting the vendor take care of all hardware selection means losing the abil-\nity to specify low-level hardware requirements (speciﬁc CPU types or storage\nproducts).\n.\nContract Questions for Hosting Providers\nThe contract you sign is the baseline of what to expect and what obligations\nthe provider has toward you, the customer. Here are some key questions to\nask your potential providers:\n1. If you want to exit the contract, will you be able to take all your data with\nyou?\n\n\n3.4\nColocation\n65\n.\n2. In the case of physical machines, if you wish to leave, can you buy the\nmachine itself?\n3. What happens to your servers and data if the vendor goes bankrupt? Will\nthey be tied up in bankruptcy proceedings?\n4. Is internet bandwidth provided by the vendor or do you have to arrange\nfor it yourself? If provided, which Internet service providers (ISPs) do you\nconnect to and how much oversubscription is done? What’s the hardware\nand peering transit redundancy?\n5. Are backups performed? If so, with what frequency and retention policy?\nCan you access the backups by request or are they solely for use by the\nvendor in case of a vendor emergency? How often are restores tested?\n6. How does the vendor guarantee its service level agreement (SLA) num-\nbers for capacity and bandwidth? Are refunds given in the event of an\nSLA violation?\n3.4 Colocation\nWhile not particularly considered a “cloud environment,” colocation is a use-\nful way to provide services. Colocation occurs when a datacenter owner rents\nspace to other people, called tenants. Renting datacenter space is very economi-\ncal for small, medium, and even large businesses. Building your own datacenter\nis a huge investment and requires specialized knowledge about cooling, design,\nnetworking, location selection, real-estate management, and so on.\nThe term “colocation” comes from the telecommunications world. In the past,\ntelecommunication companies were among the rare businesses that built data-\ncenters, which they used to house their equipment and systems. Some third-party\ncompanies offered services to the customers of the telecommunication companies,\nand it was easier to do so if they could put their own equipment in the telecom-\nmunication company’s datacenters. Thus, they colocated their equipment with\nthe telecommunication company’s equipment. In more recent years, any rental of\ndatacenter space has been called colocation service.\nThis service is ideal when you need a small or medium amount of datacenter\nspace. Such datacenters tend to be well designed and well run. While it can take\nyears to procure space and build a datacenter, using a colocation facility can get\nyou up and running quickly.\nColocation is also useful when you need many small spaces. For example,\na company might want to have a single rack of equipment in each of a dozen\ncountries to provide better access for its customers in those countries.\nISPs often extend their network into colocation provider spaces so that tenants\ncan easily connect to them directly. Being directly connected to an ISP improves\n",
      "page_number": 88
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 96-107)",
      "start_page": 96,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": "66\nChapter 3\nSelecting a Service Platform\naccess time for the ISP’s users. Alternatively, colocation providers may provide\ninternet access as a service to tenants, often by blending connections from two or\nmore ISPs. Tenants can take advantage of this capability rather than manage their\nown ISP connections and relationships.\n3.5 Selection Strategies\nThere are many strategies one may use to choose between IaaS, PaaS, and SaaS.\nHere are a few we have used:\n• Default to Virtual: Use containers or virtual machines wherever possible, opt-\ning for physical machines only when performance goals cannot be achieved\nany other way. For example, physical machines are generally better at disk\nI/O and network I/O. It is common to see a fully virtualized environment\nwith the exception of a particularly demanding database with high I/O\nrequirements. A web load balancer may also be a candidate for physical hard-\nware because of the availability of high-bandwidth network interface cards\nor the beneﬁt of predictable performance gained from dedicated network\nconnections.\n• Make a Cost-Based Decision: Select a public cloud solution or a private cloud\nsolution based on cost. Create a business plan that describes the total cost of a\nproject if it is done in-house, on a private infrastructure, versus using a public\ncloud provider. Choose the option with the lower cost. For a multiyear project,\nit may be less expensive to do it yourself. For a short-term project, a public\ncloud is probably the cheaper option.\n• Leverage Provider Expertise: Use the expertise of cloud providers in creating\nan infrastructure so your employees can focus on the application. This strat-\negy is especially appealing for small companies and startups. With only one\nor two developers, it is difﬁcult to imagine building large amounts of infra-\nstructure when public cloud services are available. Public clouds are generally\nrun more professionally than private clouds because of the dynamics of com-\npeting against other providers for customers. Of course, private clouds can be\nrun professionally, but it is difﬁcult to do at a small scale, especially without\na dedicated staff.\n• Get Started Quickly: Leverage cloud providers to get up and running as fast\nas possible. Often the biggest cost is “opportunity cost”: if you miss an oppor-\ntunity because you moved too slowly, it doesn’t matter how much money\nyou were going to save. Contracting with a public cloud provider may be\nmore expensive than doing it yourself, but doing it yourself may take months\nor years to build the infrastructure and by then the opportunity may have\ndisappeared. It is also true that building infrastructure is a waste of time when\n\n\n3.5\nSelection Strategies\n67\nthe product’s success is uncertain. Early in a product’s life the entire idea may\nbe experimental, involving trying new things rapidly and iterating over and\nover to try new features. Getting a minimal product available to early testers\nto see if the product is viable can be quickly done using public cloud services.\nIf the product is viable, private infrastructure can be considered. If the prod-\nuct is not viable, no time was wasted building infrastructure that is no longer\nneeded.\n• Implement Ephemeral Computing: Use ephemeral computing for short-term\nprojects. Ephemeral computing entails setting up a large computing infrastruc-\nture for a short amount of time. For example, imagine a company is doing\nan advertising campaign that will draw millions of users to its web site for a\nfew weeks, with a sharp drop-off occurring after that. Using a public provider\nenables the company to expand to thousands of machines brieﬂy and then dis-\npose of them when they are no longer needed. This is commonly done for large\ndata processingand analysisprojects,proof-of-conceptprojects,biotechnology\ndata mining, and handling unexpected bursts of web site trafﬁc. It is unreason-\nable to build a large infrastructure to be used for such a short span time, but a\ncloud service provider may specialize in providing such computing facilities.\nIn the aggregate the utilization will smooth out and be a constant load.\n• Use the Cloud for Overflow Capacity: Establish baseline capacity require-\nments and build those in-house. Then use the cloud to burst beyond your\nnormal capacity. This is often more cost-effective than either building in-house\ncapacity or exclusively using a public provider.\n• Leverage Superior Infrastructure: Gain an edge through superior infra-\nstructure. In this strategy the ability to create customized infrastructure is\nleveraged to gain competitive advantage. This may include building your own\ndatacenters to control cost and resource utilization, or selecting IaaS over PaaS\nto take advantage of the ability to customize at the OS and software level.\n• Develop an In-House Service Provider: Create an in-house service provider\nto control costs and maintain privacy. Often computing infrastructures are\nonly cost-effective at very large scale, which is why public cloud providers can\noffer services so economically. However, a large company can achieve similar\neconomies of scale by building a large infrastructure that is shared by many in-\nhouse customers. Because it is in-house, it is private—a criterion often required\nby industries that are highly regulated.\n• Contract for an On-Premises, Externally Run Service: Some companies will\nrun an in-house cloud service for you. They differ in how much control and\ncustomization they provide. You beneﬁt from their expertise and mitigate\nprivacy issues by owning and controlling the equipment.\n• Maximize Hardware Output: Pursue a strategy of squeezing every bit of\nproductivity and efﬁciency out of computers by eschewing virtualization.\n\n\n68\nChapter 3\nSelecting a Service Platform\nWhen an infrastructure has hundreds of thousands of computers or millions\nof cores, improving efﬁciency by 1 percent can be the equivalent of gaining\nthousands of new computers. The loss of efﬁciency from virtualization can\nbe a huge expense. In this strategy physical machines, rather than virtual\nmachines, are used and services are tightly packed on them to maximize\nutilization.\n• Implement a Bare Metal Cloud: Manage physical machine infrastructure\nlike a virtual machine cloud. Provide physical machines via the same API\nused for provisioning virtual machines. The beneﬁts of being able to spin\nup virtual machines can be applied to physical machines with some plan-\nning. Rather than selecting the exact custom conﬁguration needed for each\nphysical machine, some companies opt to purchase hundreds or thousands of\nmachines all conﬁgured the same way and manage them as a pool that can be\nreserved by departments or individuals. They do this by providing an API for\nallocating machines, wiping and reinstalling their OS, rebooting them, con-\ntrolling access, and returning them to the pool. The allocations may not be as\nfast or as dynamic as virtual machines, but many of the same beneﬁts can be\nachieved.\n3.6 Summary\nThis chapter examined a number of platforms. Infrastructure as a Service (IaaS)\nprovides a physical or virtual machine for your OS and application installs. Plat-\nform as a Service (PaaS) provides the OS and application stack or framework for\nyou. Software as a Service (SaaS) is a web-based application.\nYou can create your own cloud with physical or virtual servers, hosting them\nyourself or using an IaaS provider for the machines. Your speciﬁc business needs\nwill guide you in determining the best course of action for your organization.\nThere is a wide palette of choices from which to construct a robust and reliable\narchitecture for any given service or application. In the next chapter we examine\nthe various architectures themselves.\nExercises\n1. Compare IaaS, PaaS, and SaaS on the basis of cost, conﬁgurability, and control.\n2. What are the caveats to consider in adopting Software as a Service?\n3. List the key advantages of virtual machines.\n4. Why might you choose physical over virtual machines?\n5. Which factors might make you choose private over public cloud services?\n6. Which selection strategy does your current organization use? What are the\nbeneﬁts and caveats of using this strategy?\n\n\nChapter 4\nApplication Architectures\nPatterns are solutions to recurring\nproblems in a context.\n—Christopher Alexander\nThis chapter examines the building blocks used when designing applications and\nother services. The previous chapter discussed cloud platform options. Now we\nmove up one layer to the application architecture.\nWe start with an examination of common web service architectures beginning\nwith a single web server, to multi-machine designs, growing larger and larger until\nwe have a design that is appropriate for a large global service. Then we exam-\nine architectures that are common behind the scenes of web applications: message\nbuses and service-oriented architectures.\nMost examples in this chapter will assume that the service is a web-based\napplication using the Hyper-Text Transfer Protocol (HTTP). The user runs a web\nbrowser such as Firefox, Chrome, or Internet Explorer. In HTTP terminology, this is\ncalled the client. Each request for a web page involves speaking the HTTP protocol\nto a web server, usually running on a machine elsewhere on the internet. The web\nserver speaks the server side of the HTTP protocol, receives the HTTP connection,\nparses the request, and processes it to generate the reply. The reply is an HTML\npage or other ﬁle that is sent to the client. The client then displays the web page or\nﬁle to the user. Generally each HTTP request, or query, is a separate TCP/IP con-\nnection, although there are extensions to the protocol that let one session process\nmany HTTP requests.\nSome applications use protocols other than HTTP. For example, they may\nimplement their own protocol. Some non-web applications use HTTP. For exam-\nple, mobile phone apps may use HTTP to talk to APIs to make requests or gather\ninformation. While most of our examples will assume web browsers speaking the\nHTTP protocol, the principles apply to any client/server application and protocol.\n69\n\n\n70\nChapter 4\nApplication Architectures\n4.1 Single-Machine Web Server\nThe ﬁrst design pattern we examine is a single self-sufﬁcient machine used to\nprovide web service (Figure 4.1). The machine runs software that speaks the\nHTTP protocol, receiving requests, processing them, generating a result, and send-\ning the reply. Many typical small web sites and web-based applications use this\narchitecture.\nThe web server generates web pages from three different sources:\n• Static Content: Files are read from local storage and sent to the user\nunchanged. These may be HTML pages, images, and other content like music,\nvideo, or downloadable software.\n• Dynamic Content: Programs running on the web server generate HTML and\npossibly other output that is sent to the user. They may do so independently\nor based on input received from the user.\n• Database-Driven Dynamic Content: This is a special case of dynamic con-\ntent where the programs running on the web server consult a database for\ninformation and use that to generate the web page. In this architecture, the\ndatabase software and its data are on the same machine as the web server.\nFigure 4.1: Single-machine web service architecture\n\n\n4.2\nThree-Tier Web Service\n71\nNot all web servers have all three kinds of trafﬁc. A static web server has no\ndynamic content. Dynamic content servers may or may not need a database.\nThe single-machine web server is a very common conﬁguration for web sites\nand applications. It is sufﬁcient for many small applications, but it does have limits.\nFor example, it cannot store or access more data than can ﬁt on a single machine.\nThe number of simultaneous users it can service is limited by the capacity of the\nmachine’s CPU, memory, and I/O capacity.\nThe system is also only as reliable as one machine can be. If the machine\ncrashes or dies, web service stops until the machine is repaired. Doing mainte-\nnance on the machine is also difﬁcult. Software upgrades, content changes, and so\non all are disruptions that may require downtime.\nAs the amount of trafﬁc received by the machine grows, the single-machine\nweb server may become overloaded. We can add more memory, more disk, and\nmore CPUs, but eventually we will hit the limits of the machine. These might be\ndesign limits that dictate the hardware’s physical connections (number of physical\nslots and ports) or internal limits such as the amount of bandwidth on the internal\ninterconnections between disk and memory. We can purchase a larger, more pow-\nerful machine, but we will eventually reach limits with that, too. As trafﬁc grows,\nthe system will inevitably reach a limit and the only solution will be to implement\na different architecture.\nAnother problem has to do with buffer thrashing. Modern operating systems\nuse all otherwise unused memory as a disk cache. This improves disk I/O per-\nformance. An operating system can use many different algorithms to tune the\ndisk cache, all having to do with deciding which blocks to discard when mem-\nory is needed for other processes or newer disk blocks. For example, if a machine\nis running a web server, the OS will self-tune for the memory footprint of the web\nserver. If a machine is running a database server, the OS will tune itself differently,\npossibly even selecting an entirely different block replacement algorithm.\nThe problem is that if a machine is running a web server and a database server,\nthe memory footprint may be complex enough that the operating system cannot\nself-tune for the situation. It may pick an algorithm that is optimal for only one\napplication, or it may simply give up and pick a default scheme that is non-optimal\nfor all. Attempts at improving this include Linux “containers” systems like LXC\nand Lmctfy.\n4.2 Three-Tier Web Service\nThe three-tier web service is a pattern built from three layers: the load balancer\nlayer, the web server layer, and the data service layer (see Figure 4.2). The web\nservers all rely on a common backend data server, often an SQL database. Requests\nenter the system by going to the load balancer. The load balancer picks one of the\n\n\n72\nChapter 4\nApplication Architectures\nFigure 4.2: Three-tier web service architecture\nmachines in the middle layer and relays the request to that web server. The web\nserver processes the request, possibly querying the database to aid it in doing so.\nThe reply is generated and sent back via the load balancer.\nA load balancer works by receiving requests and forwarding them to one of\nmany replicas—that is, web servers that are conﬁgured such that they can all ser-\nvice the same URLs. Users talk to the load balancer as if it is a web server; they do\nnot realize it is a frontend for many replicas.\n4.2.1 Load Balancer Types\nThere are many ways to create a load balancer. In general, they fall into three\ncategories:\n• DNS Round Robin: This works by listing the IP addresses of all replicas in\nthe DNS entry for the name of the web server. Web browsers will receive all\nthe IP addresses but will randomly pick one of them to try ﬁrst. Thus, when\na multitude of web browsers visit the site, the load will be distributed almost\n\n\n4.2\nThree-Tier Web Service\n73\nevenly among the replicas. The beneﬁt to this technique is that it is easy to\nimplement and free. There is no actual hardware involved other than the DNS\nserver, which already exists. However, this technique is rarely used because it\ndoesn’t work very well and is difﬁcult to control. It is not very responsive. If\none replica dies unexpectedly, clients will continue to try to access it as they\ncache DNS heavily. The site will appear to be down until those DNS caches\nexpire. There is very little control over which servers receive which requests.\nThere is no simple way to reduce the trafﬁc sent to one particular replica if it\nis becoming unusually overloaded.\n• Layer 3 and 4 Load Balancers: L3 and L4 load balancers receive each TCP ses-\nsion and redirect it to one of the replicas. Every packet of the session goes ﬁrst\nthrough the load balancer and then to the replica. The reply packets from the\nreplica go back through the load balancer. The names come from the ISO pro-\ntocol stack deﬁnitions: Layer 3 is the network (IP) layer; Layer 4 is the session\n(TCP) layer. L3 load balancers track TCP sessions based on source and des-\ntination IP addresses (i.e., the network layer). All trafﬁc from a given source\naddress will be sent to the same server regardless of the number of TCP ses-\nsions it has generated. L4 load balancers track source and destination ports\nin addition to IP addresses (i.e., the session layer). This permits a ﬁner granu-\nlarity. The beneﬁt of these load balancers is that they are simple and fast. Also,\nif a replica goes down, the load balancer will route trafﬁc to the remaining\nreplicas.\n• Layer 7 Load Balancer: L7 load balancers work similarly to L3/L4 load bal-\nancers but make decisions based on what can be seen by peering into the\napplication layer (Layer 7) of the protocol stack. They can examine what is\ninside the HTTP protocol itself (cookies, headers, URLs, and so on) and make\ndecisions based on what was found. As a result they offer a richer mix of fea-\ntures than the previous load balancers. For example, the L7 load balancer can\ncheck whether a cookie has been set and send trafﬁc to a different set of servers\nbased on that criterion. This is how some companies handle logged-in users\ndifferently. Some companies set a special cookie when their most important\ncustomers log in and conﬁgure the load balancer to detect that cookie and send\ntheir trafﬁc to especially fast servers.\nSome load balancers are transparent: the source IP address of the request is unal-\ntered. Most, however, are not: the source IP address of each request the backend\nsees is the IP address of the load balancer itself. That is, from the backend’s perspec-\ntive, it looks as if all requests are coming from a single source, the load balancer.\nThe actual source IP of the requests is obscured.\nWhen all requests appear to come from the same IP address, debugging and\nlog analysis may be impossible at worst and confusing at best. The usual way to\n\n\n74\nChapter 4\nApplication Architectures\ndeal with this issue is for the load balancer to inject an additional header that indi-\ncates the IP address of the original requester. Backends can access this information\nas needed. This header is called X-Forwarded-For:. It contains a list of IP addresses\nstarting with the client’s and includes all the previous proxies or load balancers that\nthe request has passed through. Note that the client and intermediate devices can\nadd invalid or forged addresses to the list. Therefore you can only trust the address\nadded by your own load balancer. Using the rest is insecure and risky.\n4.2.2 Load Balancing Methods\nFor each request, an L3, L4, or L7 load balancer has to decide which backend to\nsend it to. There are different algorithms for making this decision:\n• Round Robin (RR): The machines are rotated in a loop. If there are three repli-\ncas, the rotation would look something like A-B-C-A-B-C. Down machines are\nskipped.\n• Weighted RR: This scheme is similar to RR but gives more queries to the back-\nends with more capacity. Usually a manually conﬁgured weight is assigned\nto each backend. For example, if there are three backends, two of equal capac-\nity but a third that is huge and can handle twice as much trafﬁc, the rotation\nwould be A-C-B-C.\n• Least Loaded (LL): The load balancer receives information from each backend\nindicating how loaded it is. Incoming requests always go to the least loaded\nbackend.\n• Least Loaded with Slow Start: This scheme is similar to LL, but when a new\nbackend comes online it is not immediately ﬂooded with queries. Instead, it\nstarts receiving a low rate of trafﬁc that slowly builds until it is receiving an\nappropriate amount of trafﬁc. This ﬁxes the problems with LL as described in\nSection 1.3.1.\n• Utilization Limit: Each server estimates how many more QPS it can handle\nand communicates this to the load balancer. The estimates may be based on\ncurrent throughput or data gathered from synthetic load tests.\n• Latency: The load balancer stops forwarding requests to a backend based\non the latency of recent requests. For example, when requests are taking\nmore than 100 ms, the load balancer assumes this backend is overloaded.\nThis technique manages bursts of slow requests or pathologically overloaded\nsituations.\n• Cascade: The ﬁrst replica receives all requests until it is at capacity. Any over-\nﬂow is directed to the next replica, and so on. In this case the load balancer\nmust know precisely how much trafﬁc each replica can handle, usually by\nstatic conﬁguration based on synthetic load tests.\n\n\n4.2\nThree-Tier Web Service\n75\n4.2.3 Load Balancing with Shared State\nAnother issue with load balancing among many replicas is shared state. Suppose\none HTTP request generates some information that is needed by the next HTTP\nrequest. A single web server can store that information locally so that it is available\nwhen the second HTTP request arrives. But what if the load balancer sends the next\nHTTP request to a different backend? It doesn’t have that information (state). This\ncan cause confusion.\nConsider the commonly encountered case in which one HTTP request takes a\nuser’s name and password and validates it, letting the user log in. The server stores\nthe fact that the user is logged in and reads his or her proﬁle from the database. This\nis stored locally for fast access. Future HTTP requests to the same machine know\nthat the user is logged in and have the user proﬁle on hand, so there’s no need to\naccess the database.\nWhat if the load balancer sends the next HTTP request to a different backend?\nThis backend will not know that the user is logged in and will ask the user to log\nin again. This is annoying to the user and creates extra work for the database.\nThere are a few strategies for dealing with this situation:\n• Sticky Connections: Load balancers have a feature called stickiness, which\nmeans if a user’s previous HTTP request went to a particular backend, the next\none should go there as well. That solves the problem discussed earlier, at least\ninitially. However, if that backend dies, the load balancer will send requests\nfrom that user to another backend; it has no choice. This new backend will not\nknow the user is logged in. The user will be asked to log in again. Thus, this\nis only a partial solution.\n• Shared State: In this case the fact that the user has logged in and the user’s pro-\nﬁle information are stored somewhere that all backends can access. For each\nHTTP connection, the user’s state is fetched from this shared area. With this\napproach, it doesn’t matter if each HTTP request goes to a different machine.\nThe user is not asked to log in every time the backends are switched.\n• Hybrid: When a user’s state moves from one backend to another, it generally\ncreates a little extra work for the web server. Sometimes this burden is small\nand tolerable. For some applications, however, it is extremely inconvenient\nand requires a lot more processing. In that case using both sticky connections\nand shared state is the best solution.\nThere are many schemes for storing and retrieving the sharing state. A simple\napproach is to use a database table on a database server to which all backends have\naccess. Unfortunately, databases may respond slowly, as they are not optimized\nfor this type of operation. Other systems are speciﬁcally designed for shared state\n\n\n76\nChapter 4\nApplication Architectures\nstorage, often holding all the information in RAM for fastest access. Memcached\nand Redis are two examples. In any event, you should avoid using a directory on\nan NFS server, as that does not provide failover in the event of a server outage or\nreliable ﬁle locking.\n4.2.4 User Identity\nAlthough the interaction with a web site appears to the user to be one seamless\nsession, in truth it is made up of many distinct HTTP requests. The backends need\nto know that many HTTP requests are from the same user. They cannot use the\nsource IP address of the HTTP request: due to the use of network address transla-\ntion (NAT), many different machines are often seen as using the same IP address.\nEven if that weren’t the case, the IP address of a particular machine changes from\ntime to time: when a laptop moves from one WiFi network to another, when a\nmobile device moves from WiFi to cellular and back, or if any machine is turned\noff and turned on again on a different (or sometimes even the same) network. Using\nthe IP address as an identity wouldn’t even work for one user running two web\nbrowsers on the same machine.\nInstead, when a user logs into a web application, the web application generates\na secret and includes it with the reply. The secret is something generated randomly\nand given to only that user on that web browser. In the future, whenever that web\nbrowser sends an HTTP request to that same web app, it also sends the secret.\nBecause this secret was not sent to any other user, and because the secret is difﬁcult\nto guess, the web app can trust that this is the same user. This scheme is known as\na cookie and the secret is often referred to as a session ID.\n4.2.5 Scaling\nThe three-tier web service has many advantages over the single web server. It is\nmore expandable; replicas can be added. If each replica can process 500 queries\nper second, they can continue to be added until the total required capacity is\nachieved. By splitting the database service onto its own platform, it can be grown\nindependently.\nThis pattern is also very ﬂexible. This leads to many variations:\n• Replica Groups: The load balancer can serve many groups of replicas, not just\none. Each group serves a different web site. The number of replicas in each\ngroup can be grown independently as that web site requires.\n• Dual Load Balancers: There can be multiple load balancers, each a replica of\nthe other. If one fails, the other can take over. This topic is discussed further in\nSection 6.6.3.\n\n\n4.3\nFour-Tier Web Service\n77\n• Multiple Data Stores: There may be many different data stores. Each replica\ngroup may have its own data store, or all of the data stores may be shared.\nThe data stores may each use the same or different technology. For example,\nthere may be one data store dedicated to shared state and another that stores\na product catalog or other information needed for the service.\n4.3 Four-Tier Web Service\nA four-tier web service is used when there are many individual applications with a\ncommon frontend infrastructure (Figure 4.3). In this pattern, web requests come in\nas usual to the load balancer, which divides the trafﬁc among the various frontends.\nFigure 4.3: Four-tier web service architecture\n",
      "page_number": 96
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 108-118)",
      "start_page": 108,
      "end_page": 118,
      "detection_method": "topic_boundary",
      "content": "78\nChapter 4\nApplication Architectures\nThe frontends handle interactions with the users, and communicate to the applica-\ntion servers for content. The application servers access shared data sources in the\nﬁnal layer.\nThe difference between the three-tier and four-tier designs is that the applica-\ntion and the web servers run on different machines. The beneﬁts of using the latter\ndesign pattern are that we decouple the customer-facing interaction, protocols, and\nsecurity issues from the applications. The downside is that it takes a certain amount\nof trust for application service teams to rely on a centralized frontend platform\nteam. It also takes management discipline to not allow exceptions.\n4.3.1 Frontends\nThe frontends are responsible for tasks that are common to all applications, thus\nreducing the complexity of the applications. Because the user interactions and\napplication services are decoupled, each can focus on doing one thing well.\nThe frontends handle all cookie processing, session pipelining (handling mul-\ntiple HTTP requests over the same TCP connection), compression, and encryption.\nThe frontends can implement HTTP 2.0 even though the application servers may\nstill be stuck on HTTP 1.1. In fact, application servers often implement a simpler\nsubset of the HTTP protocol, knowing they will be behind frontends that imple-\nment HTTP fully. End users still beneﬁt from these features because the frontend\nsupports them and acts as a gateway.\nFrontend software can track the ever-changing HTTP protocol deﬁnition so\nthat the application servers do not have to. HTTP started out as a simple protocol,\nhandling simple requests. Over time, it has evolved to have a much richer feature\nset, which makes it more complex and difﬁcult to implement. Being able to inde-\npendently upgrade the frontends means not having to wait for each application\nserver to do so.\nFrontends process everything related to the users logging in and logging out.\nHandling these tasks at the frontend tier makes it easy to have a uniﬁed username\nand password infrastructure for all applications. The requests from the frontends\nto the application layer include the username that has been pre-authenticated. The\napplications can trust this information implicitly.\nFrontends often ﬁx problems so that the application servers don’t have to. For\nexample, HTTP headers are case insensitive, but some application servers have\nbroken implementations that assume they are case sensitive. Frontends can auto-\nmatically downcase all headers so that the application servers only see the clean\nprotocol headers that they naively expect. If an application server doesn’t support\nIPv6, the frontend can receive requests via IPv6 but speak IPv4 to the backends.\nSome application servers don’t even use HTTP. Some companies have\ninvented their own protocols for communication between the frontends and the\napplication servers that are much faster and more efﬁcient.\n\n\n4.3\nFour-Tier Web Service\n79\nEncryption and Certificate Management\nEncryption is particularly important to centralize at one layer because certiﬁcate\nmanagement is quite complex. It is easy for the non-expert to make mistakes that\nweaken the security of the system at best and make the service stop working at\nworst. By centralizing the encryption function, you can assure that the people\nmanaging it are experts in their ﬁeld. Typically each application server is run by\na different team. Expecting each team to be highly qualiﬁed at crypto certiﬁcate\nmanagement as well as their application is unreasonable.\nEven if they all had a high degree of security expertise, there is still another\nissue: trust. Each person who manages the crypto certiﬁcates has to be trusted not\nto expose or steal the keys. Giving that trust to one specialized team is more secure\nthan giving that trust to members of many individual teams. In some cases all ser-\nvices use the same key, which means an accidental leak of the key by any one\nteam would weaken security for all applications. As Cheswick, Bellovin, and Rubin\n(2003) suggest, often the best security policy is to put all your eggs in one basket\nand then make sure it is a really strong basket.\nSecurity Benefits\nThe frontends are the one part of the system that is directly exposed to the Inter-\nnet. This reduces the number of places that have to be secured against attacks.\nIn the security ﬁeld this approach is called reducing the attack surface area. By\ndecoupling these functions from the applications, bugs and security holes can be\nﬁxed more rapidly.\nHTTP is a complex protocol and becomes more complex with every revision.\nThe more complex something is, the more likely it is to contain bugs or security\nholes. Being able to upgrade the frontends rapidly and independently of any appli-\ncation server upgrade schedule is important. Application teams have their own\npriorities and may not be willing or able to do a software upgrade at the drop of a\nhat. There are often dozens or hundreds of individual applications and application\nteams, and tracking all of their upgrades would be impossible.\n4.3.2 Application Servers\nThe frontends send queries to the application servers. Because all HTTP processing\nis handled by the frontends, this permits the frontend-to-application protocol to be\nsomething other than HTTP. HTTP is a general protocol, so it is slow and not as\ngood at serving API requests as a purpose-built protocol can be.\nSplitting application servers from the frontends also means that different\napplications can run on different servers. Having dedicated servers for frontends\nand for each application means that each component can be scaled independently.\nAlso, it brings even greater reliability because problems in one application do not\naffect other applications or the frontends.\n\n\n80\nChapter 4\nApplication Architectures\n4.3.3 Configuration Options\nSplitting functionality into frontend, application server, and data service layers\nalso permits one to pick hardware speciﬁcally appropriate for each layer. The front-\nends generally need high network throughput and very little storage. They are also\nlocated in a special place in the network so they have direct internet access. The\napplication servers may be on machines that are conﬁgured differently for each\napplication based on their needs. The database service most likely has a need for\nlarge amounts of disk storage or may be a server tree (as described in Section 1.3.3).\n4.4 Reverse Proxy Service\nA reverse proxy enables one web server to provide content from another web\nserver transparently. The user sees one cohesive web site, even though it is actually\nmade up of a patchwork of applications.\nFor example, suppose there is a web site that provides users with sports news,\nweather reports, ﬁnancial news, an email service, plus a main page:\n• www.company.com/ (the main page)\n• www.company.com/sports\n• www.company.com/weather\n• www.company.com/finance\n• www.company.com/email\nEach of those web features is provided by a very different web service, but all\nof them can be combined into a seamlessly uniﬁed user experience by a reverse\nproxy. Requests go to the reverse proxy, which interprets the URL and collects the\nrequired pages from the appropriate server or service. This result is then relayed\nto the original requester.\nLike the frontend four-tier web service, this scheme permits centralizing secu-\nrity and other services. Having many applications behind one domain simpliﬁes\nmany administrative tasks, such as maintaining per-domain crypto certiﬁcates.\nThe difference between a reverse proxy and the frontend of the four-tier web\nservice is that a reverse proxy is simpler and usually just connects the web browsers\nand the patchwork of HTTP servers that sit behind it. Sometimes reverse proxies\nare stand-alone software, but the functionality is so simple that it is often a feature\nbuilt into general web server software.\n4.5 Cloud-Scale Service\nCloud-scale services are globally distributed. The service infrastructure uses one\nof the previously discussed architectures, such as the four-tier web service, which\n\n\n4.5\nCloud-Scale Service\n81\nis then replicated in many places around the world. A global load balancer is used\nto direct trafﬁc to the nearest location (Figure 4.4).\nIt takes time for packets to travel across the internet. The farther the distance,\nthe longer it takes. Anyone who has spent signiﬁcant time in Australia accessing\ncomputers in the United States will tell you that even though data can travel at the\nspeed of light, that’s just not fast enough. The latency over such a great distance\ngives such terrible performance for interactive applications that they are unusable.\nWe ﬁx this problem by bringing the data and computation closer to the users.\nWe build multiple datacenters around the world, or we rent space in other people’s\ndatacenters, and replicate our services in each of them.\n4.5.1 Global Load Balancer\nA global load balancer (GLB) is a DNS server that directs trafﬁc to the nearest data-\ncenter. Normally a DNS server returns the same answer to a query no matter where\nthe query originated. A GLB examines the source IP address of the query and\nreturns a different result depending on the geolocation of the source IP address.\nGeolocation is the process of determining the physical location of a machine on\ntheinternet.Thisisaverydifﬁculttask.Unlikeaphonenumber,whosecountrycode\nandareacodegiveafairlyaccurateindicationofwheretheuseris(althoughthis,too,\nis less accurate in the age of cell phones), an IP address has no concrete geographic\nmeaning. There is a small industry consisting of companies that use various means\n(and a lot of guessing) to determine where each IP subnet is physically located.\nThey sell databases of this information for the purpose of geolocation.\nFigure 4.4: Cloud-scale web service architecture\n\n\n82\nChapter 4\nApplication Architectures\n4.5.2 Global Load Balancing Methods\nA GLB maintains a list of replicas, their locations, and their IP addresses. When\na GLB is asked to translate a domain name to an IP address, it takes into account\nthe geolocation of the requester when determining which IP address to send in the\nreply.\nGLBs use many different techniques:\n• Nearest: Strictly selects the nearest datacenter to the requester.\n• Nearest with Limits: The nearest datacenter is selected until that site is full.\nAt that point, the next nearest datacenter is selected. Slow start, as described\npreviously, is included for the same reasons as on local load balancers.\n• Nearest by Other Metric: The best location may be determined not by distance\nbut rather by another metric such as latency or cost. Latency and distance\nare usually the same but not always. For example, for a long time the only\nroute between most South American countries was via the United States. In\nSection 4.5.4 we’ll see that cost is not always a function of distance, either.\n4.5.3 Global Load Balancing with User-Specific Data\nNow that the HTTP request is directed to a particular datacenter, it will be handled\nby local load balancers, frontends, and whatever else makes up the service. This\nbrings up another architectural issue. Suppose the service stores information for a\nuser. What happens if that user’s data is stored at some other datacenter?\nFor example, a global email service provider might have datacenters around\nthe world. It may store a person’s email in the datacenter nearest to the user\nwhen he or she creates an account. But what if the person moves? Or what if that\ndatacenter is decommissioned and the person’s account is moved to some other\ndatacenter?\nA global load balancer works on the DNS level, which has no idea who the user\nis. It cannot determine that Mary sent the DNS query and return the IP address of\nthe service replica with her data.\nThere are a few solutions to this problem. First, usually each user’s email is\nstored in two different datacenters. That way, if one datacenter goes down, the data\nis still available. Now there are twice as many chances that Mary will be directed\nto a datacenter with her email, but there is still a chance that her HTTP requests\nwill reach the wrong datacenter.\nTo resolve this dilemma, the frontend communicates with the email service to\nﬁnd out where Mary’s email is and, from then on, accesses the application servers\nin the correct datacenter. The web frontends are generic, but they pull email from\nthe speciﬁc datacenter.\n\n\n4.5\nCloud-Scale Service\n83\nTo do this the company must have connections between each datacenter so\nthat the frontends can talk to any application server. They could communicate\nbetween datacenters over the internet, but typically a company in this situation\nowns private, dedicated wide area network (WAN) connections between data-\ncenters. A dedicated WAN gives the company more control and more reliable\nperformance.\n4.5.4 Internal Backbone\nThe private WAN links that connect datacenters form an internal backbone. An\ninternal backbone is not visible to the internet at large. It is a private network.\nThis internal network connects to the internet in many places. Wherever there\nis a datacenter, the datacenter will generally connect to many ISPs in the area. Con-\nnecting to an ISP directly has speed and cost beneﬁts. If you do not have a direct\nconnection to a particular ISP, then sending data to users of that ISP involves send-\ning the data through another ISP that connects your ISP and theirs. This ISP in the\nmiddle is called a transit ISP. The transit ISP charges the other ISPs for the privilege\nof permitting packets to travel over its network. There are often multiple transit\nISPs between you and your customers. The more transits, the slower, less reliable,\nand more expensive the connections become.\nPOPs\nA point of presence (POP) is a small, remote facility used for connection to local\nISPs. It is advantageous to connect to many ISPs but they cannot always connect to\nyour datacenter. For example, your datacenter may not be in the state or country\nthey operate in. Since they cannot go to you, you must extend your network to\nsomeplace near them. For example, you might create a POP in Berlin to connect to\nmany different German ISPs.\nA POP is usually a rack of equipment in a colocation center or a small space\nthat resembles a closet. It contains network equipment and connections from\nvarious telecom providers, but no general-purpose computers.\nA POP plus a small number of computers is called a satellite. The computers\nare used for frontend and content distribution services. The frontends terminate\nHTTP connections and proxy to application servers in other datacenters. Con-\ntent distribution servers are machines that cache large amounts of content. For\nexample, they may store the 1000 most popular videos being viewed at the time\n(Figure 4.5).\nThus an internal network connects datacenters, POPs, and satellites.\nGlobal Load Balancing and POPs\nThis brings up an interesting question: should the GLB direct trafﬁc to the near-\nest frontend or to the nearest datacenter that has application servers for that\n\n\n84\nChapter 4\nApplication Architectures\nFigure 4.5: A private network backbone connecting many datacenters (DCs) and\npoints of presence (POPs) on the Internet\nparticular service? For example, imagine (and these numbers are completely ﬁcti-\ntious) Google had 20 POPs, satellites, and datacenters but Google Maps was served\nout of only 5 of the datacenters. There are two paths to get to a server that provides\nthe Google Maps service.\nThe ﬁrst path is to go to the nearest frontend. Whether that frontend is in a\ndatacenter or satellite doesn’t matter. There’s a good chance it won’t be in one of\nthe ﬁve datacenters that host Google Maps, so the frontend will then talk across\nthe private backbone to the nearest datacenter that does host Maps. This solution\nwill be very fast because Google has total control over its private backbone, which\ncan be engineered to provide the exact latency and bandwidth required. There are\nno other customers on the backbone that could hog bandwidth or overload the\nlink. However, every packet sent on the backbone has a cost associated with it, and\nGoogle pays that expense.\nThe second path is to go to a frontend at the nearest datacenter that hosts\nGoogle Maps. If that datacenter is very far away, the query may traverse multi-\nple ISPs to get there, possibly going over oceans. This solution will be rather slow.\nThese transcontinental links tend to be overloaded and there is no incentive for\nthe ISPs involved to provide stellar performance for someone else’s trafﬁc. Even\nso, the cost of the transmission is a burden on the ISP, not Google. It may be slow,\nbut someone else is bearing the cost.\n\n\n4.6\nMessage Bus Architectures\n85\nWhich path does Google Maps trafﬁc take? The fast/expensive one or the\ncheap/slow one? The answer is: both!\nGoogle wants its service to be fast and responsive to the user. Therefore trafﬁc\nrelated to the user interface (UI) is sent over the ﬁrst path. This data is HTML and\nis generally very small.\nThe other part of Google Maps is delivering the map tiles—that is, the graph-\nics that make up the maps. Even though they are compressed, they are big and\nbulky and use a lot of bandwidth. However, they are loaded “off screen” by very\ncrafty JavaScript code, so responsiveness isn’t required. The map tiles can load\nvery slowly and it would not hinder the users’ experience. Therefore the map tile\nrequests take the slow but inexpensive path.\nIf you look at the HTML code of Google Maps, you will see that the URLs\nof the UI and map tiles refer to different hostnames. This way the global DNS\nload balancer can assign different paths to the different types of trafﬁc. The GLB\nis conﬁgured so that maps.google.com, which is used for all the elements related\nto the UI, returns the IP address of the nearest frontend. The map tiles are loaded\nusing URLs that contain a different hostname. The GLB is conﬁgured so that this\nhostname returns the IP address of a frontend in the nearest datacenter that serves\nGoogle Maps. Thus users get fast interaction and Google pays less for bandwidth.\n4.6 Message Bus Architectures\nA message bus is a many-to-many communication mechanism between servers.\nIt is a convenient way to distribute information among different services. Mes-\nsage buses are becoming a popular architecture used behind the scenes in sys-\ntem administration systems, web-based services, and enterprise systems. This\napproach is more efﬁcient than repeatedly polling a database to see if new infor-\nmation has arrived.\nA message bus is a mechanism whereby servers send messages to “channels”\n(like a radio channel) and other servers listen to the channels they need. A server\nthat sends messages is a publisher and the receivers are subscribers. A server can\nbe a publisher or subscriber of a given channel, or it can simply ignore the channel.\nThis permits one-to-many, many-to-many, and many-to-one communication. One-\nto-many communication enables one server to quickly send information to a large\nset of machines. Many-to-many communication resembles a chat room applica-\ntion, where many people all hear what is being said. Many-to-one communication\nenables a funnel-like conﬁguration where many machines can produce informa-\ntion and one machine takes it in. A central authority, or master, manages which\nservers are connected to which channels.\nThe messages being sent may contain any kind of data. They may be real-time\nupdates such as chat room messages, database updates, or notiﬁcations that update\n\n\n86\nChapter 4\nApplication Architectures\na user’s display to indicate there are messages waiting in the inbox. They may be\nlow-priority or batch updates communicating status changes, service requests, or\nlogging information.\nMessage bus technology goes by many names, including message queue,\nqueue service, or pubsub service. For example, Amazon provides the Simple\nQueue Service (SQS), MCollective is described as publish subscribe middleware,\nand RabbitMQ calls itself a message broker.\nA message bus system is efﬁcient in that clients receive a message only if\nthey are subscribed. There may be hundreds or thousands of machines involved,\nbut different subsets of machines will typically be subscribed to different chan-\nnels. Messages are transmitted only to the subscribed machines. Thus network\nbandwidth and processing are conserved. This approach is more efﬁcient than a\nbroadcast system that sends all messages to all machines and lets the receiving\nmachines ﬁlter out the messages they aren’t interested in.\nMessage bus systems are operations-friendly. It’s trivial to connect a\ncommand-line client to listen to messages and see what is being emitted. This\ncapability is very handy when debugging requires peering into information\nﬂows.\n4.6.1 Message Bus Designs\nThe message bus master learns the underlying network topology and for each\nchannel determines the shortest path a message needs to follow. IP multicast is\noften used to send a message to many machines on the same subnet at the same\ntime. IP unicast is used to transmit a message between subnets or when IP multicast\nisn’t available. Determining and optimizing the unique topology of each channel\nseparately requires a lot of calculations, especially when there are thousands of\nchannels.\nSome message bus systems require all messages to ﬁrst go to the master for\ndistribution. Consequently, the master may become a bottleneck. Other systems\nare more sophisticated and either have multiple masters, one master per channel,\nor have a master that controls topology but does not require all messages to go\nthrough it.\nChannels may be open to anyone, or they may be tightly controlled with ACLs\ndetermining who can publish and who can subscribe. On some systems the listen-\ners can send a reply to the message sender. Other systems do not have this feature\nand instead create a second channel for publishing replies.\nThere may be one channel, or thousands. Different message bus systems\nare optimized for different sizes. Google’s PubSub2 system (Publish Subscribe\nVersion 2) can handle hundreds of channels and tens of thousands of hosts.\nGoogle’s Thialﬁsystem can handle 2.3 million subscribers (Adya, Cooper,\nMyers & Piatek 2011).\n\n\n4.6\nMessage Bus Architectures\n87\n4.6.2 Message Bus Reliability\nMessage bus systems usually guarantee that every message will be received. For\nexample, some message bus systems require subscribers to acknowledge each mes-\nsage received. If the subscriber crashes, when it returns to service it is guaranteed\nto receive all the unacknowledged messages again; if the acknowledgment did not\nmake it back to the message bus system, the subscriber may receive a second copy\nof the message. It is up to the subscriber to detect and skip duplicates.\nHowever, message bus systems do vary in how they handle long subscriber\noutages. In some systems, subscribers miss messages when they are down. In oth-\ners, messages are stored up for a certain amount of time and are lost only if the\nsubscriber is down for more than a deﬁned length of time. Some message bus\nsystems might hold things in RAM for just a few minutes before giving up and\nqueueing messages to disk. When a subscriber comes back online, it will get the\nbacklog of messages in a ﬂood.\nMessage bus systems do not guarantee that messages will be received in the\nsame order as they were sent. Doing so would mean that if one message was being\nretried, all other messages would have to wait for that message to succeed. In the\nmeantime, millions of messages might be delayed.\nFor this reason, the subscriber must be able to handle messages arriving out of\norder. The messages usually include timestamps, which help the subscriber detect\nwhen messages do arrive out of order. However, reordering them is difﬁcult, if\nnot impossible. You could hold messages until any late ones have arrived, but\nhow would you know how long to wait? If you waited an hour, a slow-poke mes-\nsage might arrive 61 minutes late. It is best to write code that does not depend on\nperfectly ordered messages than to try to reorder them.\nBecause messages may be missed or lost, usually services have a mechanism\nthat is not based on the message bus but that enables clients to catch up on anything\nthey missed. For example, if the message bus is used to keep a database in sync,\nthere may be a way to receive a list of all database keys and the date they were last\nupdated. Receiving this list once a day enables the subscriber to notice any missing\ndata and request it.\nAs systems get larger, message bus architectures become more appealing\nbecause they are fast, they are efﬁcient, and they push control and operational\nresponsibility to the listeners. A good resource for using message bus architec-\ntures is Enterprise Integration Patterns: Designing, Building, and Deploying Messaging\nSolutions by Hohpe and Woolf (2003).\n4.6.3 Example 1: Link-Shortening Site\nA link-shortening site very much like bit.ly had a message bus architecture used by\nits components to communicate. The application had two user-visible components:\nthe control panel (a web UI for registering new URLs to be shortened) and the\n\n\n88\nChapter 4\nApplication Architectures\nweb service that took in short links and responded with the redirect code to the\nexpanded URL.\nThe company wanted fast updates between the user interface and the redirect\nservers. It was common for users to create a short link via the control panel and\nthen immediately try to use the link to make sure that it worked. Initially there was\na multi-minute delay between when the link was created and when the redirec-\ntion service was able to redirect it. The new link had to be indexed and processed,\nthen added to the database, and the database changes had to propagate to all the\nredirection servers.\nTo ﬁx this problem the company set up a message bus system that connected\nall machines. There were two channels: one called “new shortlinks” and one called\n“shortlinks used.”\nAs depicted in Figure 4.6, the architecture had four elements:\n• Control Panel: A web frontend that was the portal people used to create new\nshortlinks.\n• Main Database: A database server that stored all the shortlink information.\n• Trend Server: A server that kept track of “trending links” statistics.\nFigure 4.6: Link-shortening architecture\n",
      "page_number": 108
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 119-126)",
      "start_page": 119,
      "end_page": 126,
      "detection_method": "topic_boundary",
      "content": "4.6\nMessage Bus Architectures\n89\n• Link Redirect Servers: Web servers that received requests with short URLs\nand replied with a redirect to the expanded URL.\nWhen any control panel server received a new shortlink from a user, it would pub-\nlish it on the “new shortlink” channel. Subscribers to that channel included the\ndatabase (which stored the new shortlink information) as well as all the redirect\nservers (which stored the new information in their lookup cache). When perform-\ning a redirect, each redirect server relied on information from its local cache and\nconsulted the database only in case of a cache miss. Since it could take minutes or\nhours for the main database to ingest a new shortlink (due to indexing and other\nissues), the message bus architecture enabled the redirect servers to always be up\nto date.\nWhen a redirect server used a shortlink, it would publish this fact on the\n“shortlinks used” channel. There were only two subscribers to this channel: the\nmain database and the trend server. The main database updated the usage statis-\ntics for that database entry. The trend server kept an in-memory database of which\nURLs had been used recently so that the company could always display an accurate\nlist of trending URLs.\nImagine how difﬁcult creating this functionality would have been without a\nmessage bus. The control panel machines would need an “always up-to-date” list\nof all the redirect servers. It would have to handle down machines, new machines,\nand so on. The communication would not be optimized for the network topology.\nIt would be very difﬁcult to achieve the same quality of service as that provided\nby message bus.\nWithout a message bus system, adding a new service would mean changing all\nthe data providers to also send a copy to it. Suppose an entirely new mechanism for\ncalculating trending links was devised. Without a message bus, the redirect servers\nwould have to be updated to send information to them. With the message bus\nsystem, the new trend server could be added and run in parallel without requiring\nchanges to the other servers. If it proved successful, the old trend server could be\ndisconnected just as easily.\n4.6.4 Example 2: Employee Human Resources Data Updates\nAn enterprise had a large employee base. Each day many new employees were\nhired, left the company, changed names, or changed other database attributes.\nMeanwhile, there were numerous consumers of this information: the login/\nauthentication system, the payroll system, the door-lock systems, and many\nothers.\nIn the ﬁrst incarnation of the system, a single database was maintained. All\nsystems queried the database for changes. As the company grew, however, the\ndatabase got more and more overloaded.\n\n\n90\nChapter 4\nApplication Architectures\nIn the second incarnation, any department that needed to be informed of\nuser change requests had to write a plug-in that would be called for each change.\nThe plug-in had to handle subcommands including add, update, remove, name-\nchange, and others. The plug-ins all ran under the same role account, which meant\nthis one account had privileged access to every critical system in the company. It\nwas a nightmare. If one department’s plug-in had a bug, it would take down the\nentire system. This system remained in operation for years even though it was very\nbrittle and required a lot of work to keep it running.\nThe newest version of the system implemented a message bus. All changes\nwent through one system that was the publisher on a channel called “user\nupdates.” Any department that needed to be informed of changes would subscribe\nto that channel. The department could run its system on its own role account, which\nisolated each department into its own security domain. Each department also had\nits own failure domain, as the failure of one listener did not affect any of the others.\nA synchronization mechanism talked to each department once a day so that any\nmissed updates could be processed.\nNew departments could join the system at will without affecting any others.\nBest of all, this system was very easy to maintain. Responsibility was distributed\nto each department. The main group simply had to make sure the publisher was\nworking.\n4.7 Service-Oriented Architecture\nService-oriented architecture (SOA) enables large services to be managed more\neasily. With this architecture, each subsystem is a self-contained service provid-\ning its functionality as a consumable service via an API. The various services\ncommunicate with one another by making API calls.\nA goal of SOAs is to have the services be loosely coupled. That is, each API\npresents its service at a high level of abstraction. This makes it easier to improve\nand even replace a service. The replacement must simply provide the same abstrac-\ntion. Loosely coupled systems do not know the internal workings of the other\nsystems that are part of the architecture. If they did, they would be tightly bound\nto each other.\nAs an example, imagine a job scheduler service. It accepts requests to perform\nvarious actions, schedules them, coordinates them, and reports back progress as it\nexecutes. In a tightly coupled system, the API would be tightly linked to the inner\nworkings of the job scheduler. Users of the API could specify details related to how\nthe jobs work rather than what is needed. For example, the API might provide\ndirect access to the status of the lock mechanism used to prevent the same job from\nbeing executed twice.\nSuppose a new internal design was proposed that prevented duplicate job exe-\ncution but did locking some other way. This change could not be made without\n\n\n4.7\nService-Oriented Architecture\n91\nchanging the code of all the services that used the API. In a loosely coupled sys-\ntem, the API would provide job status at a higher level of abstraction: is the job\nwaiting, is it running, where is it running, can it be stopped, and so on. No matter\nwhat the internal implementation was, these requests could be processed.\n4.7.1 Flexibility\nAn SOA makes it easier for services to cooperate. Services can be combined in\nmany, often unexpected, ways to create new applications. New applications can\nbe designed without consultation of all the other services as long as the API meets\nthe new application’s needs. Each subsystem can be managed as a discrete sys-\ntem. It is easier to manage a few, small, easy-to-understand services than one large\nsystem with ill-deﬁned internal interconnections. When the touch-points between\nservices are well-deﬁned APIs that implement high-level abstractions, it makes it\neasier to evolve and/or replace the service.\n4.7.2 Support\nSOAs have beneﬁts at the people management level, too. As a system grows, the\nteam that develops and operates it tends to grow as well. Large teams are more\ndifﬁcult to manage than smaller, focused teams. With an SOA it is easy to split up\na team every time it grows beyond a manageable limit. Each team can focus on a\nrelated set of subsystems. They can even trade subsystems between teams as skills\nand demands require. Contrast this to a large, tightly coupled system that cannot\nbe easily divided and managed by separate teams.\n.\nSplitting Teams by Functionality\nAt Google, Gmail was originally maintained by one group of Google site reli-\nability engineers (SREs). As the system grew, subteams split off to focus on\nsubsystems such as the storage layer, the anti-spam layer, the message receiv-\ning system, the message delivery system, and so on. This was possible because\nof the SOA design of the system.\n4.7.3 Best Practices\nFollowing are some best practices for running an SOA:\n• Use the same underlying RPC protocol to implement the APIs on all services.\nThis way any tool related to the RPC mechanism is leveraged for all services.\n\n\n92\nChapter 4\nApplication Architectures\n• Have a consistent monitoring mechanism. All services should expose mea-\nsurements to the monitoring system the same way.\n• Use the same techniques with each service as much as possible. Use the same\nload balancing system, management techniques, coding standards, and so on.\nAs services move between teams, it will be easier for people to get up to speed\nif these things are consistent.\n• Adopt some form of API governance. When so many APIs are being designed,\nit becomes important to maintain standards for how they work. These\nstandards often impart knowledge learned through painful failures in the past\nthat the organization does not want to see repeated.\nWhen a tightly coupled system becomes difﬁcult to maintain, one option is to\nevolve it into a loosely coupled system. Often when systems are new, they start\nout tightly coupled with the justiﬁcation, real or not, of being more resource efﬁ-\ncient or faster to develop. Decoupling the components can be a long and difﬁcult\njourney. Start by identifying pieces that can be spilt out as services one at a time.\nDo not pick the easiest pieces but rather the pieces most in need of the beneﬁts of\nSOA: ﬂexibility, ease of upgrade and replacement, and so on.\n4.8 Summary\nWeb-based applications need to grow and scale. A small service can run on a sin-\ngle machine, serving content that is static, dynamic, or database driven. When the\nlimits of one machine are reached, a three-tier architecture is used. It moves each\nfunction to a different machine: a load balancer to direct trafﬁc among web servers,\nand one or more data servers providing static or database content.\nLocal load balancers distribute trafﬁc between replicas within a datacenter.\nThey work by intercepting trafﬁc and redirecting it to web replicas. There are many\nload balancing technologies, and many algorithms for deciding how to distribute\ntrafﬁc. When an application is divided among replicas, synchronizing user identity\nand other state information becomes complex. This goal is usually achieved by\nhaving some kind of server that stores state common to all replicas.\nA four-tier architecture creates a frontend to many three-tier systems. The new\ntier handles common services, often related to user sessions, security, and logging.\nA reverse proxy ensures that many application servers appear to be a single large\napplication.\nCloud-scale services take this architecture and replicate it to many datacenters\naround the world. They use global load balancers to direct trafﬁc to particular data-\ncenters. A private network between datacenters may be used if one would beneﬁt\nfrom being able to control inter-datacenter network quality. This private network\nmay connect to the internet at many points of presence to improve connectivity at\n\n\nExercises\n93\nmany ISPs. POPs may have servers that terminate HTTP sessions and relay data\nto datacenters using more efﬁcient protocols.\nOther architecture patterns are appropriate for non-web applications. Message\nbus architectures create a message-passing system that decouples communication\nfrom the services that need the information. Service-oriented architectures involve\nmany small services that cooperate to create larger services. Each service in an SOA\ncan be independently scaled, upgraded, and even replaced.\nChapter 1 discussed composition, server trees, and other patterns. These\nrudimentary elements make up many of the patterns discussed in this chapter.\nChapter 5 will discuss patterns for data storage; in particular, Section 5.5 will\ndescribe the distributed hash table.\nAll of these architecture patterns have tradeoffs on cost, scalability, and\nresiliency to failure. Understanding these tradeoffs is key to knowing when to use\neach pattern.\nExercises\n1. Describe the single-machine, three-tier, and four-tier web application architec-\ntures.\n2. Describe how a single-machine web server, which uses a database to generate\ncontent, might evolve to a three-tier web server. How would this be done with\nminimal downtime?\n3. Describe the common web service architectures, in order from smallest to\nlargest.\n4. Describe how different local load balancer types work and what their pros and\ncons are. You may choose to make a comparison chart.\n5. What is “shared state” and how is it maintained between replicas?\n6. What are the services that a four-tier architecture provides in the ﬁrst tier?\n7. What does a reverse proxy do? When is it needed?\n8. Suppose you wanted to build a simple image-sharing web site. How would\nyou design it if the site was intended to serve people in one region of the\nworld? How would you then expand it to work globally?\n9. What is a message bus architecture and how might one be used?\n10. What is an SOA?\n11. Why are SOAs loosely coupled?\n12. How would you design an email system as an SOA?\n13. Who was Christopher Alexander and what was his contribution to\narchitecture?\n\n\nThis page intentionally left blank \n\n\nChapter 5\nDesign Patterns for Scaling\nThe only real problem is scaling.\nEverything else is a sub-problem.\n—O’Dell’s Axiom\nA system’s ability to scale is its ability to process a growing workload, usually\nmeasured in transactions per second, amount of data, or number of users. There\nis a limit to how far a system can scale before reengineering is required to permit\nadditional growth.\nDistributed systems must be built to be scalable from the start because growth\nis expected. Whether you are building a web-based service or a batch-processing\ndata analytics platform, the goal is always to be successful, which usually means\nattracting more users, uses, or data.\nMaking sure a service is fast and stays fast is critical. If your service does not\nscale, or if it gets too slow as it becomes more popular, users will go elsewhere.\nGoogle found that an artiﬁcial 400-ms delay inserted into its search results would\nmake users conduct 0.2 to 0.6 percent fewer searches. This could translate into\nmillions or billions of dollars of lost revenue.\nIronically, a slow web service is more frustrating than one that is down. If a\nsite is down, users understand that fact immediately and can go to a competing\nsite or ﬁnd something else to do until it comes back up. If it is slow, the experience\nis just painful and frustrating.\nBuilding a scalable system does not happen by accident. A distributed system\nis not automatically scalable. The initial design must be engineered to scale to meet\nthe requirements of the service, but it also must include features that create options\nfor future growth. Once the system is in operation, we will always be optimizing\nthe system to help it scale better.\n95\n\n\n96\nChapter 5\nDesign Patterns for Scaling\nIn previous chapters we’ve discussed many techniques that enable distributed\nsystems to grow to extreme sizes. In this chapter we will revisit these techniques in\ngreater detail. We will review terminology related to scaling, examine the theory\nbehind scaling techniques, and describe speciﬁc techniques used to scale. Math-\nematical terms used to describe how systems perform and scale can be found in\nAppendix C.\n5.1 General Strategy\nThe basic strategy for building a scalable system is to design it with scalability\nin mind from the start and to avoid design elements that will prevent additional\nscaling in the future.\nThe initial requirements should include approximations of the desired scale:\nthe size of data being stored, the throughput of the systems that process it, the\namount of trafﬁc the service currently receives, and expected growth rates. All\nof these factors then guide the design. This process was described previously in\nSection 1.7.\nOnce the system is running, performance limits will be discovered. This is\nwhere the design features that enable further scaling come into play.\nWhile every effort is made to foresee potential scaling issues, not all of them\ncan receive engineering attention. The additional design and coding effort that will\nhelp deal with future potential scaling issues is lower priority than writing code\nto ﬁx the immediate issues of the day. Spending too much time preventing scal-\ning problems that may or may not happen is called premature optimization and\nshould be avoided.\n5.1.1 Identify Bottlenecks\nA bottleneck is a point in the system where congestion occurs. It is a point that is\nresource starved in a way that limits performance. Every system has a bottleneck.\nIf a system is underperforming, the bottleneck can be ﬁxed to permit the system to\nperform better. If the system is performing well, knowing the location of the bot-\ntleneck can be useful because it enables us to predict and prevent future problems.\nIn this case the bottleneck can be found by generating additional load, possibly in\na test environment, to see at which point performance suffers.\nDeciding what to scale is a matter of ﬁnding the bottleneck in the system and\neliminating it. The bottleneck is where a backlog of work accumulates. Optimiza-\ntions done to the process upstream of the bottleneck simply make the backlog grow\nfaster. Optimizations made downstream of the bottleneck may improve the efﬁ-\nciency of that part but do not improve the total throughput of the system. Therefore\nany effort not spent focused on the bottleneck is wasteful.\n",
      "page_number": 119
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 127-140)",
      "start_page": 127,
      "end_page": 140,
      "detection_method": "topic_boundary",
      "content": "5.1\nGeneral Strategy\n97\n5.1.2 Reengineer Components\nSome scaling issues can be resolved through adjustments to the current system.\nFor example, enlarging a cache might be as simple as adjusting a conﬁguration\nﬁle. Other scaling issues require engineering effort.\nRewriting parts of a system is called reengineering and is usually done to\nimprove speed, functionality, or resource consumption. Sometimes reengineering\nis difﬁcult because of earlier design decisions that led to particular code or design\nstructures. It is often best to ﬁrst replace such code with code that is functionally\nequivalent but has an internal organization that makes other reengineering efforts\neasier to accomplish. Restructuring an existing body of code—namely, altering its\ninternal structure without changing its external behavior—is called refactoring.\n5.1.3 Measure Results\nScaling solutions must be evaluated using evidence, meaning data collected from a\nreal system. Take measurements, try a solution, and repeat the same measurements\nto see the effect. If the effect is minimal or negative, the solution is not deployed.\nFor example, if performance is slow and measurements indicate that a cache\nhit rate is very low, the cache is probably too small. In such a case, we would mea-\nsure performance, resize the cache, and then measure performance again. While\nthe cache may be performing better with such an adjustment, the overall effect on\nthe system may not be a signiﬁcant improvement or may not be big enough to\njustify the cost of the additional RAM required.\nIf we do not measure before and after a change is made, we cannot be sure\nwhether our changes were actually effective. Making changes without measure-\nment would be system administration by luck at best, and by ego at worst. It is\noften tempting to rush ahead with a solution and measure only after the change\nis made. This is as bad as not measuring at all, because there is no baseline for\ncomparison.\nWhile past experience should inform and guide us, we must resist the temp-\ntation to skip the scientiﬁc process of using measurement and analysis to guide\nour decisions. Distributed systems are always too large for any one person to “just\nknow” the right thing to do. A hunch or guess by an experienced system admin-\nistrator should be trumped by the recommendation of a more junior person who\nhas taken the time to perform scientiﬁc analysis: set up data collection mechanisms,\ntake measurements, verify a hypothesis about what is wrong, test a theory of what\nmight ﬁx it, and analyze the results.\n5.1.4 Be Proactive\nThe best time to ﬁx a bottleneck is before it becomes a problem. Ideally, the ﬁx will\narrive just in time, immediately before this point is reached. If we ﬁx a problem\n\n\n98\nChapter 5\nDesign Patterns for Scaling\ntoo soon, we may be wasting effort on a problem that never arises—effort that\ncould have been better spent elsewhere. If we begin to design and implement a\nsolution too late, the problem will arise before the solution is deployed. If we wait\nmuch too long, the problem will surprise us, catching us off guard, and we will be\n“ﬁreﬁghting” to ﬁnd a solution. Engineering takes time and doing it in a rushed\nfashion leads to mistakes and more problems. We want the Goldilocks solution:\nnot too early, not too late, just right.\nEvery system has a bottleneck or constraint. This is not a bad thing. Constraints\nare inherent in all systems. A constraint dictates the rate at which processing can\nhappen, or how much work can ﬂow through the process. If the current rate is\nsufﬁcient, the bottleneck is not a problem. In other words, the constraint becomes\na problem only if it actually hampers the system’s ability to achieve its goal.\nThe best strategy for scaling a running system, then, is to predict problems far\nenough in advance that there is enough time to engineer a proper solution. This\nmeans one should always be collecting enough measurements to be aware of where\nthe bottlenecks are. These measurements should be analyzed so that the point at\nwhich the bottleneck will become a problem can be predicted. For example, simply\nmeasuring how much internet bandwidth is being consumed and graphing it can\nmake it easy to predict when the capacity of your internet link will be exhausted.\nIf it takes 6 weeks to order more bandwidth, then you need to be able to order that\nbandwidth at least 6 weeks ahead of time, and preferably 12 weeks ahead of time\nto permit one failed attempt and do-over.\nSome solutions can be implemented quickly, by adjusting a conﬁguration set-\nting. Others require weeks or months of engineering effort to solve, often involving\nrewriting or replacing major systems.\n5.2 Scaling Up\nThe simplest methodology for scaling a system is to use bigger, faster equipment.\nA system that runs too slowly can be moved to a machine with a faster CPU, more\nCPUs, more RAM, faster disks, faster network interfaces, and so on. Often an exist-\ning computer can have one of those attributes improved without replacing the\nentire machine. This is called scaling up because the system is increasing in size.\nWhen this solution works well, it is often the easiest solution because it does\nnot require a redesign of the software. However, there are many problems with\nscaling this way.\nFirst, there are limits to system size. The fastest, largest, most powerful com-\nputer available may not be sufﬁcient for the task at hand. No one computer can\nstore the entire corpus of a web search engine or has the CPU power to process\npetabyte-scale datasets or respond to millions of HTTP queries per second. There\nare limits as to what is available on the market today.\n\n\n5.3\nThe AKF Scaling Cube\n99\nSecond, this approach is not economical. A machine that is twice as fast costs\nmore than twice as much. Such machines are not sold very often and, therefore, are\nnot mass produced. You pay a premium when buying the latest CPU, disk drives,\nand other components.\nFinally, and most importantly, scaling up simply won’t work in all situations.\nBuying a faster, more powerful machine without changing the design of the soft-\nware being used usually won’t result in proportionally faster throughput. Software\nthat is single-threaded will not run faster on a machine with multiple processors.\nSoftware that is written to spread across all processors may not see much perfor-\nmance improvement beyond a certain number of CPUs due to bottlenecks such as\nlock contention.\nLikewise, improving the performance of any one component is not guaran-\nteed to improve overall system performance. A faster network connection will not\nimprove throughput, for example, if the protocol performs badly when latency\nis high.\nAppendix B goes into more detail about these issues and the history of how\nsuch problems led to the invention of distributed computing.\n5.3 The AKF Scaling Cube\nMethodologies for scaling to massive proportions boil down to three basic options:\nreplicate the entire system (horizontal duplication); split the system into individual\nfunctions, services, or resources (functional or service splits); and split the system\ninto individual chunks (lookup or formulaic splits).\nFigure 5.1, the AKF Scaling Cube, was developed by Abbott, Keeven, and\nFisher and conceptualizes these categories as x-, y-, and z-axes (Abbott & Fisher\n2009).\n5.3.1 x: Horizontal Duplication\nHorizontal duplication increases throughput by replicating the service. It is also\nknown as horizontal scaling or scaling out.\nThis kind of replication has been discussed in past chapters. For example, the\ntechnique of using many replicas of a web server behind a load balancer is an\nexample of horizontal scaling.\nA group of shared resources is called a resource pool. When adding resources\nto a pool, it is necessary for each replica to be able to handle the same transactions,\nresulting in the same or equivalent results.\nThe x-axis does not scale well with increases in data or with complex transac-\ntions that require special handling. If each transaction can be completed indepen-\ndently on all replicas, then the performance improvement can be proportional to\nthe number of replicas. There is no loss of efﬁciency at scale.\n\n\n100\nChapter 5\nDesign Patterns for Scaling\nFigure 5.1: The AKF Scaling Cube. Trademark AKF Partners. Reprinted with\npermission from Scalability Rules: 50 Principles for Scaling Web Sites.\n.\nRecommended Books on Scalability\nThe Art of Scalability: Scalable Web Architecture, Processes, and Organizations\nfor the Modern Enterprise by Abbott and Fisher (2009) is an extensive cat-\nalog of techniques and discussion of scalability of people, processes, and\ntechnologies.\nScalability Rules: 50 Principles for Scaling Web Sites, also by Abbott and\nFisher (2011), is a slimmer volume, focused on technical strategy and tech-\nniques.\nWhen the transactions require replicas to communicate, the scaling is less efﬁ-\ncient. For example, transactions that write new data that must be communicated\nto all replicas may require all replicas to hold off on any future transactions related\nto the update until all replicas have received the change. This is related to the CAP\nPrinciple (discussed in Section 1.5).\n\n\n5.3\nThe AKF Scaling Cube\n101\nTechniques that involve x-axis scaling include the following:\n• Adding more machines or replicas\n• Adding more disk spindles\n• Adding more network connections\n5.3.2 y: Functional or Service Splits\nA functional or service split means scaling a system by splitting out each individ-\nual function so that it can be allocated additional resources.\nAn example of this was discussed in Section 4.1, where we had a single\nmachine that was used for a web server, a database, and an application server\n(dynamic content generation). The three functions all compete for resources such\nas disk buffer cache, CPU, and the bandwidth to the disk, memory, and memory\nsubsystems. By moving the three major functions to separate machines, each is able\nto perform better because it has dedicated resources.\nSeparating the functions requires making them less tightly coupled to each\nother. When they are loosely coupled, it becomes easier to scale each one inde-\npendently. For example, we could apply x-axis scaling techniques to a single\nsubsystem. Scaling individual parts has advantages. It is less complicated to repli-\ncate a small part rather than an entire system. It is also often less expensive to\nreplicate one part that needs more capacity than the entire system, much of which\nmay be performing adequately at the current scale.\nIn addition to splitting along subsystem boundaries, y-axis scaling may\ninvolve splitting workﬂows or transaction types.\nPerhaps some category of transactions might be better handled as a special\ncase rather than being treated the same way as all the other requests. This type of\ntransaction might be split off to be processed by a dedicated pool of machines.\nFor example, it is expensive to engineer a system that has very low latency for\nall requests. If all trafﬁc is placed in the same bucket, you need far more hardware\nto keep latencies low for the few requests that care. A better alternative might be\nto separate requests that come from batch processing systems versus interactive\nservices. The latter can be processed by machines that are less oversubscribed or\nare on networks with different quality of service (QoS) settings.\nOne can also mark special customers for special treatment. One ﬁnancial ser-\nvices web site sets a cookie if a user invests multiple millions of dollars with the\nﬁrm. The web load balancer detects the cookie and sends its trafﬁc to a pool of\nservers that are dedicated to very important customers.\nAlternatively, there may be an infrequent type of transaction that is partic-\nularly burdensome, such that moving it to its own pool would prevent it from\n\n\n102\nChapter 5\nDesign Patterns for Scaling\noverloading the general transactions. For example, a particular query might nega-\ntively affect the cache infrastructure. Such queries might be directed to a separate\nset of machines that uses a different cache algorithm.\nTechniques that involve y-axis scaling include the following:\n• Splitting by function, with each function on its own machine\n• Splitting by function, with each function on its own pool of machines\n• Splitting by transaction type\n• Splitting by type of user\n.\nCase Study: Separating Traffic Types at ServerFault.com\nServerFault.com is a question-and-answer web site for system administrators.\nWhen displaying a question (and its answers) to a logged-in user, the page\nis augmented and customized for the particular person. Anonymous users\n(users who are not logged in) all see the same generic page.\nTo scale the system on the y-axis, the two ways to generate the same page\nwere split out. The anonymous pages are now handled by a different system\nthat generates the HTML once and caches it for future requests. Since the vast\nmajority of queries are from anonymous users, this division greatly improved\nperformance.\nAnonymous page views are subdivided one additional way. Search\nengines such as Google and Bing crawl every page at Serverfault.com look-\ning for new content. Since this crawling hits every page, it might potentially\noverload the service, due to both the volume of requests and the fact that hit-\nting every page in order exhausts the cache. Both factors make performance\nsuffer for other users. Therefore requests from web crawlers are sent to a ded-\nicated pool of replicas. These replicas are conﬁgured not to cache the HTML\npages that are generated. Because the pool is separate, if the crawlers overload\nit, regular users will not be affected.\n5.3.3 z: Lookup-Oriented Split\nA lookup-oriented split scales a system by splitting the data into identiﬁable seg-\nments, each of which is given dedicated resources. z-axis scaling is similar to y-axis\nscaling except that it divides the data instead of the processing.\nA simple example of this is to divide, or segment, a database by date. If the\ndatabase is an accumulation of data, such as log data, one can start a new database\n\n\n5.3\nThe AKF Scaling Cube\n103\nserver every time the current one ﬁlls up. There may be a database for 2013 data,\n2014 data, and so on. Queries that involve a single year go to the appropriate\ndatabase server. Queries that span years are sent to all the appropriate database\nservers and the responses are combined. If a particular year’s database is accessed\nso often that it becomes overloaded, it can be scaled using the x-axis technique of\nreplication. Since no new data is written to past years’ servers, most servers can be\nsimple read-only replicas.\n.\nCase Study: Twitter’s Early Database Architecture\nWhen Twitter was very new, the history of all Tweets ﬁt on a single database\nserver running MySQL. When that server ﬁlled up, Twitter started a new\ndatabase server and modiﬁed its software to handle the fact that its data was\nnow segmented by date.\nAs Twitter became more popular, the amount of time between a new\nsegment being started and that new database ﬁlling up decreased rapidly. It\nbecame a race for the operations team to keep up with demand. This solu-\ntion was not sustainable. Load was unbalanced, as older machines didn’t get\nmuch trafﬁc. This solution was also expensive, as each machine required many\nreplicas. It was logistically complex as well.\nEventually, Twitter moved to a home-grown database system called\nT-bird, based on Gizzard, which smoothly scales automatically.\nAnother way to segment data is by geography. In a global service it is common\npractice to set up many individual data stores around the world. Each user’s data\nis kept on the nearest store. This approach also gives users faster access to their\ndata because it is stored closer to them.\nGoing from an unsegmented database to a segmented one may require con-\nsiderable refactoring of application code. Thus scaling on the z-axis is often\nundertaken only when scaling using the x- and y-axes is exhausted.\nAdditional ways to segment data include the following:\n• By Hash Prefix: This is known as sharding and is discussed later.\n• By Customer Functionality: For example, eBay segments by product—cars,\nelectronics, and so on.\n• By Utilization: Putting high-use users in dedicated segments.\n• By Organizational Division: For example, sales, engineering, business devel-\nopment.\n\n\n104\nChapter 5\nDesign Patterns for Scaling\n• Hierarchically: The segments are kept in a hierarchy. DNS uses this pattern,\nlooking up an address like www.everythingsysadmin.com ﬁrst in the root\nservers, then in the servers for com, and ﬁnally in the servers for the domain\neverythingsysadmin.\n• By Arbitrary Group: If a cluster of machines can reliably scale to 50,000 users,\nthen start a new cluster for each 50,000 users. Email services often use this\nstrategy.\n5.3.4 Combinations\nMany scaling techniques combine multiple axes of the AKF Scaling Cube. Some\nexamples include the following:\n• Segment plus Replicas: Segments that are being accessed more frequently can\nbe replicated at a greater depth. This enables scaling to larger datasets (more\nsegments) and better performance (more replicas of a segment).\n• Dynamic Replicas: Replicas are added and removed dynamically to achieve\nrequired performance. If latency is too high, add replicas. If utilization is too\nlow, remove replicas.\n• Architectural Change: Replicas are moved to faster or slower technology\nbased on need. Infrequently accessed shards are moved to slower, less expen-\nsive technology such as disk. Shards in higher demand are moved to faster\ntechnology such as solid-state drives (SSD). Extremely old segments might be\narchived to tape or optical disk.\n5.4 Caching\nA cache is a small data store using fast/expensive media, intended to improve a\nslow/cheap bigger data store. For example, recent database queries may be stored\nin RAM so that if the same query is repeated, the disk access can be avoided.\nCaching is a distinct pattern all its own, considered an optimization of the z-axis\nof the AKF Scaling Cube.\nConsider lookups in a very large data table. If the table was stored in RAM,\nlookups could be very fast. Assume the data table is larger than will ﬁt in RAM,\nso it is stored on disk. Lookups on the disk are slow. To improve performance, we\nallocate a certain amount of RAM and use it as a cache. Now when we do a lookup,\nﬁrst we check whether the result can be found in the cache. If it is, the result is used.\nThis is called a cache hit. If it is not found, the normal lookup is done from the disk.\nThis is called a cache miss. The result is returned as normal and in addition is stored\nin the cache so that future duplicate requests will be faster.\n\n\n5.4\nCaching\n105\nFigure 1.10 lists performance comparisons useful for estimating the speed of\na cache hit and miss. For example, if your database is in Netherlands and you are\nin California, a disk-based cache is faster if it requires fewer than 10 seeks and two\nor three 1MB disk reads. In contrast, if your database queries are within the same\ndatacenter, your cache needs to be signiﬁcantly faster, such as RAM or a cache\nserver on the same subnet.\n5.4.1 Cache Effectiveness\nThe effectiveness of a cache is measured by the cache hit ratio, sometimes called a\ncache hit rate. It is the ratio of the number of cache hits over the total number of\nlookups. For example, if 500 lookups are performed and 100 were serviced from\nthe cache, the cache hit ratio would be 1/5 or 20 percent.\nPerformance\nA cache is a net beneﬁt in performance if the time saved during cache hits exceeds\nthe time lost from the additional overhead. We can estimate this using weighted\naverages. If the typical time for a regular lookup is L, a cache hit is H, a cache\nmiss is M, and the cache hit ratio is R, then using the cache is more effective if\nH × R + M × (1 −R) < L.\nWhen doing engineering estimates we can simplify the formula if cache\nlookups and updates are extremely fast or nearly zero. In that case we can assume\nthe performance beneﬁt will be a function of the cache hit ratio. For example, sup-\npose a typical lookup took 6 seconds and we predict a cache hit rate of 33 percent.\nWe know that in a perfect world with an instant, zero-overhead cache, the average\nlookup would fall by 33 percent to 4 seconds. This gives us a best-case scenario,\nwhich is useful for planning purposes. Realistically, performance will improve\nslightly less than this ideal.\nCost-Effectiveness\nA cache is cost-effective only if the beneﬁt from the cache is greater than the cost of\nimplementing the cache. Recall that accessing RAM is faster than accessing disk,\nbut much more expensive. The performance and costs of the items in Figure 1.10\nwill inevitably change over time. We recommend building your own chart based\non the performance and costs in your speciﬁc environment.\nSuppose a system without caching required 20 replicas, but with caching\nrequired only 15. If each replica is a machine, this means the cache is more\ncost-effective if it costs less than purchasing 5 machines.\nPurchase price is not always the only consideration. If being able to provide a\nfaster service will improve sales by 20 percent, then cost should be weighed against\nthat improvement.\n\n\n106\nChapter 5\nDesign Patterns for Scaling\n5.4.2 Cache Placement\nCaching can be local, in which case the software is performing its own caching,\nsaving itself the trouble of requesting a lookup for each cache hit (Figure 5.2a).\nCaching can be external to the application, with a cache placed between the\nserver and the external resources. For example, a web cache sits between a web\nbrowser and the web server, intercepting requests and caching them when possible\n(Figure 5.2b). Caches can also be at the server side. For example, a server that pro-\nvides an API for looking up certain information might maintain its own cache that\nservices requests when possible (Figure 5.2c). If there are multiple caches, some\nmight potentially be outdated. The CAP Principle, described in Section 1.5, then\napplies.\nNot all caches are found in RAM. The cache medium simply must be faster\nthan the main medium. A disk can cache for data that has to be gathered from a\nremote server because disks are generally faster than remote retrieval. For example,\nYouTube videos are cached in many servers around the world to conserve internet\nbandwidth. Very fast RAM can cache for normal RAM. For example, the L1 cache\nof a CPU caches the computer’s main memory. Caches are not just used to improve\ndata lookups. For example, calculations can be cached. A function that does a dif-\nﬁcult mathematical calculation might cache recent calculation results if they are\nlikely to be requested again.\n5.4.3 Cache Persistence\nWhen a system starts, the cache is usually empty, or cold. The cache hit ratio will\nbe very low and performance will remain slow until enough queries have warmed\nFigure 5.2: Cache placement\n\n\n5.4\nCaching\n107\nthe cache. Some caches are persistent, meaning they survive restarts. For example,\na cache stored on disk is persistent across restarts. RAM is not persistent and is lost\nbetween restarts. If a system has a cache that is slow to warm up and is stored in\nRAM, it may be beneﬁcial to save the cache to disk before a shutdown and read it\nback in on startup.\n.\nCase Study: Saving a RAM Cache to Disk\nStack Exchange’s web sites depend on a database that is heavily cached in\nRAM by Redis. If Redis is restarted, its performance will be unacceptably slow\nfor 10 to 15 minutes while the cache warms. Redis has many features that pre-\nvent this problem, including the ability to snapshot the data held in RAM to\ndisk, or to use a second Redis server to store a copy of the cache.\n5.4.4 Cache Replacement Algorithms\nWhen a cache miss is processed, the data gathered by the regular lookup is added\nto the cache. If the cache is full some data must be thrown away to make room.\nThere are many different replacement algorithms available to handle the cache\nmanipulation.\nIn general, better algorithms keep track of more usage information to improve\nthe cache hit ratio. Different algorithms work best for different data access patterns.\nThe Least Recently Used (LRU) algorithm tracks when each cache entry was\nused and discards the least recently accessed entry. It works well for access patterns\nwhere queries are repeated often within a small time period. For example, a DNS\nserver might use this algorithm: if a domain has not been accessed in a long time,\nchances are it won’t be accessed again. Typos, for example, rarely repeat and will\neventually expire from the cache.\nThe Least Frequently Used (LFU) algorithm counts the number of times a\ncache entry is accessed and discards the least active entries. It may track total\naccesses, or keep an hourly or daily count. This algorithm is a good choice when\nmore popular data tends to be accessed the most. For example, a video service\nmight cache certain popular videos that are viewed often while other videos are\nviewed once and rarely ever rewatched.\nNew algorithms are being invented all the time. Tom’s favorite algorithm,\nAdaptive Replacement Cache (ARC), was invented in 2003 (Megiddo & Modha\n2003). Most algorithms do not perform well with a sudden inﬂux of otherwise\nlittle-used data. For example, backing up a database involves reading every record\none at a time and leaves the cache ﬁlled with otherwise little-used data. At that\npoint, the cache is cold, so performance suffers. ARC solves this problem by putting\nnewly cached data in a probationary state. If it is accessed a second time, it gets out\n\n\n108\nChapter 5\nDesign Patterns for Scaling\nof probation and is put into the main cache. A single pass through the database\nﬂushes the probationary cache, not the main cache.\nMany other algorithms exist, but most are variations on the LRU, LFU, and\nARC options.\n5.4.5 Cache Entry Invalidation\nWhen data in the primary storage location changes, any related cache entries\nbecome obsolete. There are many ways to deal with this situation.\nOne method is to ignore it. If the primary storage does not change, the cache\nentries do not become obsolete. In other cases, the cache is very small and obsolete\nentries will be eventually be replaced via the cache replacement algorithm. If the\nsystem can tolerate occasional outdated information, this may be sufﬁcient. Such\nsituations are rather rare.\nAnother method is to invalidate the entire cache anytime the database\nchanges. The beneﬁt of this method is that it is very simple to implement. It leaves\nthe cache cold, however, and performance suffers until it warms again. This is\nacceptable in applications where the cache warms quickly.\nThe main storage may communicate to the cache the need to invalidate an\nentry whenever the data in the cache has changed. This may create a lot of work if\nthere are many caches and updates occur frequently. This is also an example of how\nthe CAP Principle, described in Section 1.5, comes into play: for perfect consistency,\nprocessing of queries involving the updated entry must pause until all caches have\nbeen notiﬁed.\nSome methods eliminate the need for the main storage to communicate\ndirectly to the caches. The cache can, for example, assume a cache entry is valid\nonly for a certain number of seconds. The cache can record a timestamp on each\nentry when it is created and expire entries after a certain amount of time.\nAlternatively, the server can help by including how long an entry may be\ncached when it answers a query. For example, suppose a DNS server responds\nto all queries with not just the answer, but also how many seconds each entry in\nthe answer may be cached. This is called the time to live (TTL) value. Likewise, an\nHTTP server can annotate a response with an expiration date, an indication of how\nlong the item can be cached. If you do not control the client software, you cannot be\nassured that clients will abide by such instructions. Many ISPs cache DNS entries\nfor 24 hours at a minimum, much to the frustration of those using DNS as part of\na global load balancer. Many web browsers are guilty of ignoring expiration dates\nsent by HTTP servers.\nLastly, the cache can poll the server to see if a local cache should be invali-\ndated. For example, an HTTP client can query the server about whether an item\nhas changed to ﬁnd out if an item in its cache is fresh. This method works when the\nquery for freshness can be processed signiﬁcantly faster than the full answer is sent.\n\n\n5.4\nCaching\n109\n5.4.6 Cache Size\nPicking the right cache size is difﬁcult but important. If it is too small, perfor-\nmance will suffer, which may potentially be worse than having no cache at all. If it\nis too large, we have wasted money when a smaller cache would do. If we have\nthousands of machines, a few dollars wasted on each machine adds up quickly.\nCaches are usually a ﬁxed size. This size usually consists of a ﬁxed number\nof cache entries or a ﬁxed amount of total storage, in which case the number of\npotential entries is adjusted accordingly.\nThere are several approaches to selecting a cache size. The most accurate way\nto determine the correct cache size for a given situation is to take measurements\non a running system. Caches are complex, especially due to interactions between\nreplacement algorithms and cache size. Measuring a running system involves all\nthese factors, but may not always be possible.\nOne approach is to run the system with a variety of cache sizes, measuring\nperformance achieved by each one. Benchmarks like this can be done with real or\nsimulated data on a separate system set up for benchmark tests. Such operations\ncan be done on live systems for the most accurate measurements, though this may\nbe risky. One way to limit such risks is to adjust cache sizes on just one replica of\nmany. Negative effects are then less likely to be noticed.\nAnother approach is to conﬁgure a system with a larger cache size than the\napplication could possibly need. If the cache entries expire fast enough, the cache\nwill grow to a certain size and then stop growing. This gives an upper bound of\nhow big the cache needs to be. If the cache size does not stabilize, eventually the\nreplacement algorithm will kick in. Take a snapshot of the age of all the entries.\nReview the snapshot to see which entires are hot (frequently being used) and cold\n(less frequently used). Often you will see a pattern surface. For example, you may\nsee that 80 percent of the cache entries have been used recently and the rest are\nsigniﬁcantly older. The 80 percent size represents approximately the amount of\ncache that is contributing to improving performance.\nAnother approach is to estimate the cache hit ratio. If we are considering\nadding a cache to a running system, we can collect logs of data and estimate what\nthe cache hit ratio will be. For example, we could collect 24 hours’ worth of query\nlogs and count duplicate requests. The ratio of duplicates to total queries is a good\npredictor of what the cache hit ratio will be be if caching is added. This assumes\nan inﬁnite cache with no expiration. If even under these theoretically perfect con-\nditions the cache hit ratio is low, we know that a cache will not help. However, if\nthere are duplicate queries, the cumulative size of the responses to those queries\nwill give a good estimate for sizing the cache.\nThe problem with measurements from live or benchmark systems is that they\nrequire the system to exist. When designing a system it is important to be able to\nmake a reasonable prediction of what cache size will be required.\n\n\n110\nChapter 5\nDesign Patterns for Scaling\nWe can improve our estimate by using a cache simulator. These tools can be\nused to provide “what if” analysis to determine the minimum cache size.\nOnce the cache is in place, the cache hit ratio should be monitored. The\ncache size can be reevaluated periodically, increasing it to improve performance\nas needed.\n5.5 Data Sharding\nSharding is a way to segment a database (z-axis) that is ﬂexible, scalable, and\nresilient. It divides the database based on the hash value of the database keys.\nA hash function is an algorithm that maps data of varying lengths to a\nﬁxed-length value. The result is considered probabilistically unique. For exam-\nple, the MD5 algorithm returns a 128-bit number for any input. Because there\nare 340,282,366,920,938,463,463,374,607,431,768,211,456 possible combinations, the\nchance of two inputs producing the same hash is very small. Even a small\nchange in the data creates a big change in the hash. The MD5 hash of\n“Jennifer” is e1f6a14cd07069692017b53a8ae881f6 but the MD5 hash of “Gennifer”\nis 1e49bbe95b90646dca5c46a8d8368dab.\nTo divide a database into two shards, generate the hash of the key and store\nkeys with even hashes in one database and keys with odd hashes in the other\ndatabase. To divide a database into four shards, split the database based on the\nremainder of the key’s hash divided by 4 (i.e., the hash mod4). Since the remain-\nder will be 0, 1, 2, or 3, this will indicate which of the four shards will store that\nkey. Because the hash values are randomly distributed between the shards, each\nshard will store approximately the same number of keys automatically. This pat-\ntern is called a distributed hash table (DHT) since it distributes the data over many\nmachines, and uses hashes to determine where the data is stored.\n.\nThe Power of 2\nWe use a power of 2 to optimize the hash-to-shard mapping process. When\nyou want the remainder of the hash when divided by 2n, you just need to look\nat the last n bits of the hash. This is a very fast operation, much faster than\ngetting the modulus using a number that is not a power of 2.\nShards can be replicated on multiple machines to improve performance. With\nsuch an approach, each replica processes a share of the queries destined for that\nshard. Replication can also provide better availability. If multiple machines store\nany shard, then any machine can crash or be taken down for maintenance and the\nother replicas will continue to service the requests.\n",
      "page_number": 127
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 141-148)",
      "start_page": 141,
      "end_page": 148,
      "detection_method": "topic_boundary",
      "content": "5.5\nData Sharding\n111\nAs more data are stored, the shards may outgrow the machine. The database\ncan then be split over twice as many shards. The number of shards doubles, and the\nold data is divided among the new shards. Each key’s hash is evaluated to deter-\nmine if the key should stay in the current shard or if it belongs to the new shard.\nSystems that perform this step while live have complex algorithms to manage\nqueries received during the expansion.\nIt is rather inﬂexible to require that the number of segments be a power of 2.\nFor example, going from one shard to two requires adding just one machine, which\nis easy to purchase. However, as the system grows, you may ﬁnd yourself needing\nto go from 32 machines to 64 machines, which is quite a large purchase. The next\njump is twice as big. If the new machines are more powerful, this extra capacity will\nbe wasted until all the smaller machines are eliminated. Also, while the number of\nkeys is evenly divided between shards, each key may not store exactly the same\namount of data. Thus a machine may have one fourth of the keys, but more data\nthan can ﬁt on the machine. One must increase the number of shards based on the\nlargest shard, which could be considerably bigger than the smallest.\nThe solution to these problems is to create more, smaller shards and store\nmultiple shards on each machine. Now you can vary the number of shards on a\nmachine to compensate for uneven shard sizes and different machine sizes. For\nexample, you could divide the hash value by a larger power of 2 and produce, for\nexample, 8192 buckets. Divide those buckets across as many machines as needed.\nFor example, if there are three machines, one twice as large as the other two, the\nlarger machine might store keys that fall into bucket 0...4095 (4096 total) in the\nlarger server and store buckets 4096...6143 and 6144...8191 (2048 keys each) in\nthe second and third machines, respectively.\nAs new, more powerful hardware becomes available, one can pack more\nshards on a machine. More shards mean more queries will be directed to that\nmachine. Thus, more CPU and network bandwidth is required. It is possible that\nwhen the machine stores the maximum number of shards, the CPU or network\nbandwidth will be exhausted and performance suffer. New hardware models\nshould be benchmarked to determine the usable capacity before being put into\nservice. Ideally CPU, shard storage, and network bandwidth will all top out at the\nsame time.\nIf shards are used for a read/write database, each write updates the appro-\npriate shards. Replicas must be kept up-to-date, abiding by the CAP Principle\ndiscussed in Chapter 1.\nIn reality, shards are often used to distribute a read-only corpus of information.\nFor example, a search engine collects data and indexes it, producing shards of a\nread-only database. These then need to be distributed to each search cluster replica.\nDistribution can be rather complex. Transmitting an updated shard can take a long\ntime. If it is copied over the old data, the server cannot respond to requests on that\n\n\n112\nChapter 5\nDesign Patterns for Scaling\nshard until the update is complete. If the transfer fails for some reason, the shard\nwill be unusable. Alternatively, one could set aside storage space for temporary use\nby shards as they are transmitted. Of course, this means there is unused space when\nupdates are not being done. To eliminate this waste, one can stop advertising that\nthe shard is on this machine so that the replicas will process any requests instead.\nNow it can be upgraded. The process of unadvertising until no new requests are\nreceived is called draining. One must be aware that while a shard is being drained,\nthere is one fewer replica—so performance may suffer. It is important to globally\ncoordinate shard upgrades so that enough replicas exist at any given moment to\nmaintain reliability and meet performance goals.\n5.6 Threading\nData can be processed in different ways to achieve better scale. Simply processing\none request at a time has its limits. Threading is a technique that can be used to\nimprove system throughput by processing many requests at the same time.\nThreading is a technique used by modern operating systems to allow\nsequences of instructions to execute independently. Threads are subsets of pro-\ncesses; it’s typically faster to switch operations among threads than among pro-\ncesses. We use threading to get a ﬁne granularity of control over processing for use\nin complex algorithms.\nIn a single-thread process, we receive a query, process it, send the result, and\nget the next query. This is simple and direct. A disadvantage is that a single long\nrequest will stall the requests behind it. It is like wanting to buy a pack of gum but\nbeing in line behind a person with a full shopping cart. In this so-called head of\nline blocking, the head of the line is blocked by a big request. The result is high\nlatency for requests that otherwise could be serviced quickly.\nA second disadvantage to single-threading is that in a ﬂood of requests, some\nrequests will be dropped. The kernel will queue up incoming connections while\nwaiting for the program to take the next one and process it. The kernel limits how\nmany waiting connections are permitted, so if there is a ﬂood of new connections,\nsome will be dropped.\nFinally, in a multi-core machine, the single thread will be bound to a single\nCPU, leaving the other cores idle. Multithreaded code can take advantage of all\ncores, thereby making maximum use of a machine.\nIn multithreading, a main thread receives new requests. For each request, it\ncreates a new thread, called a worker thread, to do the actual work and send the\nreply. Since thread creation is fast, the main thread can keep up with a ﬂood of\nnew requests and none will be dropped. Throughput is improved because requests\nare processed in parallel, multiple CPUs are utilized, and head of line blocking is\nreduced or eliminated.\n\n\n5.7\nQueueing\n113\nThat said, multithreading is more difﬁcult to implement. If multiple threads\nhave to access the same resources in memory, locks or signaling ﬂags (semaphores)\nare needed to prevent resource collision. Locking is complex and error prone.\nThere are limits to the number of threads a machine can handle, based on\nRAM and CPU core limits. If any one core becomes overloaded, performance will\nquickly drop for that core. Connection ﬂoods can still cause dropped requests, but\nthe number of connections being handled is increased.\n5.7 Queueing\nAnother way that data can be processed differently to achieve better scale is called\nqueuing. A queue is a data structure that holds requests until the software is ready\nto process them. Most queues release elements in the order that they were received,\ncalled ﬁrst in, ﬁrst out (FIFO) processing.\nQueueing is similar to multithreading in that there is a master thread and\nworker threads. The master thread collects requests and places them in the queue.\nThere is usually a ﬁxed number of worker threads. Each one takes a request from\nthe queue, processes it, sends a reply, and then takes another item from the queue.\nThis workﬂow is called feeding from a queue.\n5.7.1 Benefits\nQueueing shares many of the advantages and disadvantages of multithreading. At\nthe same time, it has several advantages over basic multithreading.\nWith queueing, you are less likely to overload the machine since the num-\nber of worker threads is ﬁxed and remains constant. There is also an advantage\nin retaining the same threads to service multiple requests. This avoids the over-\nhead associated with new thread creation. Thread creation is lightweight, but on a\nmassive scale the overhead can add up.\nAnother beneﬁt of the queuing model is that it is easier to implement a prior-\nity scheme. High-priority requests can go to the head of the queue. A plethora of\nqueueing algorithms may be available depending on the type of priority scheme\nyou want to implement. In fair queueing, the algorithm prevents a low-priority\nitem from being “starved” by a ﬂood of high-priority items. Other algorithms\ndynamically shift priorities so that bursty or intermittent trafﬁc does not overload\nthe system or starve other priorities from being processed.\n5.7.2 Variations\nVariations of the queueing model can optimize performance. It is common to have\nthe ability to shrink or grow the number of threads. This may be automatic based on\n\n\n114\nChapter 5\nDesign Patterns for Scaling\ndemand, or it may be manually controlled. Another variation is for threads to kill\nand re-create themselves periodically so that they remain “fresh.” This mitigates\nmemory leaks and other problems, but in doing so hides them and makes them\nmore difﬁcult to ﬁnd.\nFinally, it is common practice to use processes instead of threads. Process\ncreation can be expensive, but the ﬁxed population of worker processes means\nthat you pay that overhead once and then get the beneﬁt of using processes. Pro-\ncesses can do things that threads cannot, because they have their own address\nspace, memory, and open ﬁle tables. Processes are self-isolating, in that a process\nthat is corrupted cannot hurt other processes, whereas one ill-behaved thread can\nadversely affect other threads.\nAn example of queueing implemented with processes is the Prefork process-\ning module for the Apache web server. On startup, Apache forks off a certain\nnumber of subprocesses. Requests are distributed to subprocesses by a master pro-\ncess. Requests are processed faster because the subprocess already exists, which\nhides the long process creation time. Processes are conﬁgured to die and be\nrefreshed to every n requests so that memory leaks are averted. The number of\nsubprocesses used can be adjusted dynamically.\n5.8 Content Delivery Networks\nA content delivery network (CDN) is a web-acceleration service that delivers con-\ntent (web pages, images, video) more efﬁciently on behalf of your service. CDNs\ncache content on servers all over the world. Requests for content are serviced from\nthe cache nearest the user. Geolocation techniques are used to identify the network\nlocation of the requesting web browser.\nCDNs do not copy all content to all caches. Instead, they notice usage trends\nand determine where to cache certain content. For example, seeing a surge of use\nfrom Germany for a particular image, the CDN might copy all images for that cus-\ntomer to its servers in Germany. These images may displace cached images that\nhave not been accessed as recently.\nCDNs have extremely large, fast connections to the internet. They have more\nbandwidth to the internet than most web sites.\nCDNs often place their cache servers in the datacenters of ISPs, in arrange-\nments called colocation. As a result, the ISP-to-ISP trafﬁc is reduced. Considering\nthat ISPs charge each other for this trafﬁc, such a reduction can pay for itself\nquickly.\nTypically, an image in the middle of a web page might come from a URL on\nthe same server. However, this image rarely, if ever, changes. A web site that uses\na CDN would upload a copy of this image to the CDN, which then serves it from\n\n\n5.8\nContent Delivery Networks\n115\na URL that points to the CDN’s servers. The web site then uses the CDN’s URL\nto refer to the image. When users load the web page, the image comes from the\nCDN’s servers. The CDN uses various techniques to deliver the image faster than\nthe web site could.\nUploading content to a CDN is automatic. Your web site serves the content\nas it normally does. A link to this content is called a native URL. To activate the\nCDN, you replace the native URLs in the HTML being generated with URLs that\npoint to the CDN’s servers. The URL encodes the native URL. If the CDN has the\ncontent cached already, it serves the content as one expects. If this is the ﬁrst time\nthat particular content is accessed, the CDN loads the content from the native URL,\ncaches it, and serves the content to the requester. The idea to encode the native URL\nin the CDN URL is quite smart; it means that there is no special step for uploading\nto the CDN that must be performed.\nBest practice is to use a ﬂag or software switch to determine whether native\nURLs or CDN URLs are output as your system generates web pages. Sometimes\nCDNs have problems and you will want to be able to switch back to native URLs\neasily. Sometimes the problem is not the CDN but rather a conﬁguration error that\nyou have caused. No amount of marketing material expounding the reliability of a\nCDN product will save you from this situation. Also, while a new web site is in the\ntesting phase, you may not want to use a CDN, especially if you are testing new,\nsecret features that should not be exposed to the world yet. Lastly, having such a\nswitch enables you to switch between CDN vendors easily.\nCDNs are great choices for small sites. Once the site becomes extremely large,\nhowever, it may be more cost-effective to run your own private CDN. Google ini-\ntially used a third-party CDN to improve performance and achieved an order of\nmagnitude better uptime than it could achieve when it was a young company. As\nGoogle grew, it established its own datacenter space all over the world. At that\npoint Google built its own private CDN, which permitted it to achieve another\norder of magnitude better uptime.\nCDNs didn’t appear until the late 1990s. At that time they focused on static\ncontent delivery for images and HTML ﬁles. The next generation of CDN products\nadded video hosting. In the past, ﬁle sizes were limited, but video hosting requires\nthe CDN to be able to handle larger content plus deal with protocols related to skip-\nping around within a video. The current generation of CDN products, therefore,\ncan act as a proxy. All requests go through the CDN, which acts as a middle man,\nperforming caching services, rewriting HTML to be more efﬁcient, and supporting\nother features.\nCDNs now compete on price, geographic coverage, and an ever-growing list\nof new features. Some CDNs specialize in a particular part of the world, either\noffering lower prices for web sites that have users only in their part of the world,\nor offering special services such as being licensed to serve content in China from\n\n\n116\nChapter 5\nDesign Patterns for Scaling\ninside the Great Firewall of China (that is, assisting clients with the difﬁcult and\ncomplex censorship requirements placed on such trafﬁc).\nNew features include the ability to serve dynamic content, serve different con-\ntent to mobile devices, and provide security services. HTTPS (encrypted HTTP)\nservice can be complex and difﬁcult to administer. Some CDNs can process the\nHTTPS connections on your behalf, relieving you of managing such complex-\nity. The connection between your web servers and the CDN can then use an\neasier-to-manage transport mechanism, or no encryption at all.\n5.9 Summary\nMost approaches to scaling fall under one of the axes of the AKF Scaling Cube. The\nx-axis (horizontal scaling) is a power multiplier, cloning systems or increasing their\ncapacities to achieve greater performance. The y-axis (vertical scaling) scales by\nisolating transactions by their type or scope, such as using read-only database repli-\ncas for read queries and sequestering writes to the master database only. Finally,\nthe z-axis (lookup-based scaling) is about splitting data across servers so that the\nworkload is distributed according to data usage or physical geography.\nSharding scales large databases by putting horizontal partitions of your\ndatabase (rows) on multiple servers, gaining the advantage of smaller indices and\ndistributed queries. Replicating the shards onto additional servers produces speed\nand reliability beneﬁts, at the cost of data freshness.\nAnother optimization for data retrieval is a cache, a comparatively small data\nstore on fast and/or expensive media. A cache aggregates recently requested data,\nperforming updates on itself when data that isn’t in the cache is requested. Sub-\nsequent queries will then go directly to the cache and an overall improvement in\nperformance will be realized.\nThreading and queueing give us the tools to deal with ﬂoods of requests,\naggregating them into a structure that allows us to service them individually.\nContent delivery networks provide web-acceleration services, usually by caching\ncontent closer to the user.\nExercises\n1. What is scaling?\n2. What are the options for scaling a service that is CPU bound?\n3. What are the options for scaling a service whose storage requirements are\ngrowing?\n4. The data in Figure 1.10 is outdated because hardware tends to get less expen-\nsive every year. Update the chart for the current year. Which items changed\nthe least? Which changed the most?\n\n\nExercises\n117\n5. Rewrite the data in Figure 1.10 in terms of proportion. If reading from main\nmemory took 1 second, how long would the other operations take? For extra\ncredit, draw your answer to resemble a calendar or the solar system.\n6. Take the data table in Figure 1.10 and add a column that identiﬁes the cost of\neach item. Scale the costs to the same unit—for example, the cost of 1 terabyte\nof RAM, 1 terabyte of disk, and 1 terabyte of L1 cache. Add another column\nthat shows the ratio of performance to cost.\n7. What is the theoretical model that describes the different kinds of scaling\ntechniques?\n8. How do you know when scaling is needed?\n9. What are the most common scaling techniques and how do they work? When\nare they most appropriate to use?\n10. Which scaling techniques also improve resiliency?\n11. Describe how your environment uses a CDN or research how it could be used.\n12. Research Amdahl’s Law and explain how it relates to the AKF Scaling Cube.\n\n\nThis page intentionally left blank \n",
      "page_number": 141
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 149-156)",
      "start_page": 149,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\nDesign Patterns for Resiliency\nSuccess is not ﬁnal, failure is not\nfatal: it is the courage to continue\nthat counts.\n—Winston Churchill\nResiliency is a system’s ability to constructively deal with failures. A resilient\nsystem detects failure and routes around it. Nonresilient systems fall down when\nfaced with a malfunction. This chapter is about software-based resiliency and\ndocuments the most common techniques used.\nResiliency is important because no one goes to a web site that is down. Hard-\nware fails—that is a fact of life. You can buy the most reliable, expensive hardware\nin the world and there will be some amount of failures. In a sufﬁciently large\nsystem, a one in a million failure is a daily occurrence.\nDuring the ﬁrst year of a typical Google datacenter, there will be ﬁve rack-wide\noutages, three router failures large enough to require diverting processing away\nfrom connected machines, and eight network scheduled maintenance windows,\nhalf of which cause 30-minute random connectivity losses. At the same time 1 to\n5 percent of all disks will die and each machine will crash at least twice (2 to 4\npercent failure rate) (Dean 2009).\nGraceful degradation, discussed previously, means software is designed to\nsurvive failures or periods of high load by providing reduced functionality. For\nexample, a movie streaming service might automatically reduce video resolution\nto conserve bandwidth when some of its internet connections are down or oth-\nerwise overloaded. The other strategy is defense in depth, which means that all\nlayers of design detect and respond the failures. This includes failures as small as\na single process and as large as an entire datacenter.\nAn older, more traditional strategy for achieving reliability is to reduce the\nchance of failure at every place it can happen. Use the best servers and the best\nnetwork equipment, and put it in the most reliable datacenter: There will still\n119\n\n\n120\nChapter 6\nDesign Patterns for Resiliency\n.\nTerms to Know\nOutage: A user-visible lack of service.\nFailure: A system, subsystem, or component that has stopped working.\nMalfunction: Used interchangeably with “failure.”\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of one or more servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls are\nreceived per second.\nbe outages when this strategy is pursued, but they will be rare. This is the most\nexpensive strategy. Another strategy is to perform a dependency analysis and\nverify that each system depends on high-quality parts. Manufacturers calculate\ntheir components’ reliability and publish their mean time between failure (MTBF)\nratings. By analyzing the dependencies within the system, one can predict MTBF\nfor the entire system. The MTBF of the system is only as high as that of its\nlowest-MTBF part.\nSuch a strategy is predictive, meaning that it predicts the likelihood of failure\nor the reliability of the system.\nResilient systems continue where predictive strategies leave off. Assuming\nthat failure will happen, we build systems that react and respond intelligently so\nthat the system as a whole survives and continues to provide service. In other\nwords, resilient systems are responsive to failure. Rather than avoiding failure\nthrough better hardware or responding to it with human effort (and apologies),\nthey take a proactive stance and put in place mechanisms that expect and survive\nfailure.\nResilient systems decouple component failure from user-visible outages. In\ntraditional computing, where there is a failed component, there is a user-visible\noutage. When we build survivable systems, the two concepts are decoupled.\nThis chapter is about the various ways we can design systems that detect\nfailure and work around it. This is how we build survivable systems. The tech-\nniques are grouped into four categories: physical failures, attacks, human errors,\nand unexpected load.\n6.1 Software Resiliency Beats Hardware Reliability\nYou can build a reliable system by selecting better hardware or better software.\nBetter hardware means special-purpose CPUs, components, and storage systems.\n\n\n6.2\nEverything Malfunctions Eventually\n121\nBetter software means adding intelligence to a system so that it detects failures and\nworks around them.\nSoftware solutions are favored for many reasons. First and foremost, they are\nmore economical. Once software is written, it can be applied to many services\nand many machines with no additional cost (assuming it is home-grown, is open\nsource, or does not require a per-machine license.) Software is also more malleable\nthan hardware. It is easier to ﬁx, upgrade, and replace. Unlike hardware upgrades,\nsoftware upgrades can be automated. As a result, software is replaced often. New\nfeatures can be introduced faster and more frequently. It is easy to experiment.\nAs software gets older, it gets stronger: Bugs are ﬁxed; rare edge cases are han-\ndled better. Spolsky’s (2004) essay, “Things You Should Never Do,” gives many\nexamples.\nUsing better hardware, by comparison, is more expensive. The initial pur-\nchase price is higher. More reliable CPUs, components, and storage systems are\nmuch more expensive than commodity parts. This strategy is also more expensive\nbecause you pay the extra expense with each machine as you grow. Upgrading\nhardware has a per-machine cost for the hardware itself, installation labor, capi-\ntal depreciation, and the disposal of old parts. Designing hardware takes longer,\nso upgrades become available less frequently and it is more difﬁcult to experi-\nment and try new things. As hardware gets older, it becomes more brittle and fails\nmore often.\n6.2 Everything Malfunctions Eventually\nMalfunctions are a part of every environment. They can happen at every level. For\nexample, they happen at the component level (chips and other electronic parts),\nthe device level (hard drives, motherboards, network interfaces), and the system\nlevel (computers, network equipment, power systems). Malfunctions also occur\nregionally: racks lose power, entire datacenters go ofﬂine, cities and entire regions\nof the world are struck with disaster. Humans are also responsible for malfunctions\nranging from typos to software bugs, from accidentally kicking a power cable out\nof its socket to intentionally malicious attacks.\n6.2.1 MTBF in Distributed Systems\nLarge systems magnify small problems. In large systems a “one in a million” prob-\nlem happens a lot. A hard drive with an MTBF of 1 million hours has a 1 in 114\nchance of failing this year. If you have 100,000 such hard disks, you can expect two\nto fail every day.\nA bug in a CPU that is triggered with a probability of one in 10 million might\nbe why your parents’ home PC crashed once in 2010. They cursed, rebooted, and\n\n\n122\nChapter 6\nDesign Patterns for Resiliency\ndidn’t think of it again. Such a bug would be hardly within the chip maker’s\nability to detect. That same bug in a distributed computing system, however,\nwould be observed frequently enough to show up as a pattern in a crash detec-\ntion and analysis system. It would be reported to the vendor, which would be\ndismayed that it existed, shocked that anyone found it, and embarrassed that it\nhad been in the core CPU design for multiple chip generations. The vendor would\nalso be unlikely to give permission to have the speciﬁcs documented in a book on\nsystem administration.\nFailures cluster so that it appears as if the machines are ganging up on us.\nRacks of machines trying to boot at the same time after a power outage expose\nmarginal power supplies unable to provide enough juice to spin up dozens of disks\nat once. Old solder joints shrink and crack, leading to mysterious failures. Compo-\nnents from the same manufacturing batch have similar mortality curves, resulting\nin a sudden rush of failures.\nWith our discussion of the many potential malfunctions and failures, we hope\nwe haven’t scared you away from the ﬁeld of system administration!\n6.2.2 The Traditional Approach\nTraditional software assumes a perfect, malfunction-free world. This leaves the\nhardware systems engineer with the impossible task of delivering hardware that\nnever fails. We fake it by using redundant array of inexpensive [independent]\ndisks (RAID) systems that let the software go on pretending that disks never fail.\nSheltered from the reality of a world full of malfunctions, we enable software devel-\nopers to continue writing software that assumes a perfect, malfunction-free world\n(which, of course, does not exist).\nFor example, UNIX applications are written with the assumption that reading\nand writing ﬁles will happen without error. As a result, applications do not check\nfor errors when writing ﬁles. If they did, it would be a waste of time because the\nblocks may not be written to disk until later, possibly after the application has been\nexited. Microsoft Word is written with the assumption that the computer it runs on\nwill continue to run.\n.\nHyperbole Warning\nThe previous paragraph included two slight exaggerations. The application\nlayer of UNIX assumes a perfect ﬁle system but the underlying layers do not\nassume perfect disks. Microsoft Word checkpoints documents so that the user\ndoes not lose data in the event of a crash. However, during that crash the user\nis unable to edit the document.\n\n\n6.2\nEverything Malfunctions Eventually\n123\nAttempts to achieve this impossible malfunction-free world cause companies\nto spend a lot of money. CPUs, components, and storage systems known for high\nreliability are demonstrably more expensive than commodity parts. Appendix B\ndetails the history of this strategy and explains the economic beneﬁts of distributed\ncomputing techniques discussed in this chapter.\n6.2.3 The Distributed Computing Approach\nDistributed computing, in contrast to the traditional approach, embraces compo-\nnents’ failures and malfunctions. It takes a reality-based approach that accepts\nmalfunctions as a fact of life. Google Docs continues to let a user edit a document\neven if a machine fails at Google: another machine takes over and the user does\nnot even notice the handoff.\nTraditional computing goes to great lengths to achieve reliability through\nhardware and then either accepts a small number of failures as “normal” or adds\nintelligence to detect and route around failures. If your software can route around\nfailure, it is wasteful to also spend money on expensive hardware.\nA popular bumper-sticker says “Eat right. Exercise. Die anyway.” If your\nhardware is going to fail no matter how expensive it is, why buy the best? Why\npay for reliability twice?\n.\nBuying Failed Memory\nEarly in its history, Google tried to see how far it could push the limits of using\nintelligent software to manage unreliable hardware. To do so, the company\npurchased failed RAM chips and found ways to make them useful.\nGoogle was purchasing terabytes of RAM for machines that ran software\nthat was highly resilient to failure. If a chip failed, the OS would mark that\narea of RAM as unusable and kill any process using it. The killed processes\nwould be restarted automatically. The fact that the chip was bad was recorded\nso that it was ignored by the OS even after reboot.\nAs a result, a machine didn’t need to be repaired just because one chip\nhad failed. The machine could run until the machine’s capacity was reduced\nbelow usable limits.\nTo understand what happened next, you must understand that the differ-\nence between high-quality RAM chips and normal-quality chips is how much\ntesting they pass. RAM chips are manufactured and then tested. The ones that\npass the most QA testing are sold as “high quality” at a high price. The ones\nthat pass the standard QA tests are sold as normal for the regular price. All\nothers are thrown away.\n\n\n124\nChapter 6\nDesign Patterns for Resiliency\nGoogle’s purchasing people are formidable negotiators. Google had\nalready been saving money by purchasing the normal-quality chips, relying\non the custom software Google wrote to work around failures. One day the\npurchasing department thought to ask if it was possible to purchase the chips\nthat were being thrown away. The manufacturers had never received such a\nrequest before and were willing to sell the defective chips for pennies on the\ndollar.\nThe “failed” RAM chips worked perfectly for Google’s need. Some didn’t\nwork from the start and others failed soon after. However, services were able\nto keep running.\nGoogle eventually ended this practice but for many years the company\nwas able to build servers with enormous amounts of RAM for less money\nthan any of its competitors. When your business is charging pennies for\nadvertisements, saving dollars is a big advantage!\n6.3 Resiliency through Spare Capacity\nThe general strategy used to gain resiliency is to have redundant units of capacity\nthat can fail independently of each other. Failures are detected and those units are\nremoved from service. The total capacity of the system is reduced but the system\nis still able to run. This means that systems must be built with spare capacity to\nbegin with.\nLet’s use the example of a web server that serves the static images displayed\non a web site. Such a server is easy to replicate because the content does not change\nfrequently. We can, for example, build multiple such servers and load balance\nbetween them. (How load balancers work was discussed in Section 4.2.1.) We call\nthese servers replicas because the same service is replicated by each server. They\nare duplicates in that they all respond to the same queries and give equivalent\nresults. In this case the same images are accessed at the same URLs.\nSuppose each replica can handle 100 QPS and the service receives 300 QPS at\npeak times. Three servers would be required to provide the 300 QPS capacity. An\nadditional replica is needed to provide the spare capacity required to survive one\nfailed replica. Failure of any one replica is detected and that replica is taken out of\nservice automatically. The load is now balanced over the surviving three replicas.\nThe total capacity of the system is reduced to 300 QPS, which is sufﬁcient.\nWe call this N + M redundancy. Such systems require N units to provide\ncapacity and have M units of extra capacity. Units are the smallest discrete system\nthat provides the service. The term N + 1 redundancy is used when we wish to\nindicate that there is enough spare capacity for one failure, such as in our example.\n\n\n6.3\nResiliency through Spare Capacity\n125\nIf we added a ﬁfth server, the system would be able to survive two simultaneous\nfailures and would be described as N + 2 redundancy.\nWhat if we had 3 + 1 redundancy and a series of failures? After the ﬁrst failure,\nthe system is described as 3 + 0. It is still running but there is no redundancy. The\nsecond failure (a double failure) would result in the system being oversubscribed.\nThat is, there is less capacity available than needed.\nContinuing our previous example, when there are two failed replicas, there\nis 200 QPS of capacity. The system is now 3:2 oversubscribed: two replicas exist\nwhere three are needed. If we are lucky, this has happened at a time of day that\ndoes not draw many users and 200 QPS is sufﬁcient. However, if we are unlucky,\nthis has happened at peak usage time and our two remaining servers are faced with\n300 QPS, more than they are designed to handle. Dealing with such an overload is\ncovered later in Section 6.7.1.\n6.3.1 How Much Spare Capacity\nSpare capacity is like an insurance policy: it is an expense you pay now to prepare\nfor future trouble that you hope does not happen. It is better to have insurance and\nnot need it than to need insurance and not have it. That said, paying for too much\ninsurance is wasteful and not good business. Selecting the granularity of our unit\nof capacity enables us to manage the efﬁciency. For example, in a 1 + 1 redundant\nsystem, 50 percent of the capacity is spare. In a 20 + 1 redundant system, less than\n5 percent of the capacity is spare. The latter is more cost-efﬁcient.\nThe other factors in selecting the amount of redundancy are how quickly we\ncan bring up additional capacity and how likely it is that a second failure will\nhappen during that time. The time it takes to repair or replace the down capacity\nis called the mean time to repair (MTTR). The probability an outage will happen\nduring that time is the reciprocal of the mean time between failures. The per-\ncent probability that a second failure will happen during the repair window is\nMTTR/MTBF × 100.\nIf a second failure means data loss, the probability of a second failure becomes\nan important factor in how many spares you should have.\nSuppose it takes a week (168 hours) to repair the capacity and the MTBF is\n100,000 hours. There is a 168/1, 000, 000 × 100 = 1.7 percent, or 1 in 60, chance of\na second failure.\nNow suppose the MTBF is two weeks (336 hours). In this case, there is a\n168/336 × 100 = 50 percent, or 1 in 2, chance of a second failure—the same as a\ncoin ﬂip. Adding an additional replica becomes prudent.\nMTTR is a function of a number of factors. A process that dies and needs to\nbe restarted has a very fast MTTR. A broken hardware component may take only a\nfew minutes to replace, but if that server is in a datacenter 9000 miles away, it may\n\n\n126\nChapter 6\nDesign Patterns for Resiliency\ntake a month before someone is able to reach it. Spare parts need to be ordered,\nshipped, and delivered. Even if a disk can be replaced within minutes of failure,\nif it is in a RAID conﬁguration there may be a long, slow rebuild time where the\nsystem is still N + 0 until the rebuild is complete.\nIf all this math makes your head spin, here is a simple rule of thumb: N + 1 is\na minimum for a service; N + 2 is needed if a second outage is likely while you are\nﬁxing the ﬁrst one.\nDigital computers are either on or off, and we think in terms of a service as\neither running or not: it is either up or down. When we use resiliency through repli-\ncation, the service is more like an analog device: it can be on, off, or anywhere in\nbetween. We are no longer monitoring the service to determine if it is up or down.\nInstead, we are monitoring the amount of capacity in the system and determining\nwhether we should be adding more. This changes the way we think about our sys-\ntems and how we do operations. Rather than being awakened in the middle of the\nnight because a machine is down, we are alerted only if the needle of a gauge gets\nnear the danger zone.\n6.3.2 Load Sharing versus Hot Spares\nIn the previous examples, the replicas are load sharing: all are active, are sharing\nthe workload equally (approximately), and have equal amounts of spare capacity\n(approximately). Another strategy is to have primary and secondary replicas. In\nthis approach, the primary replica receives the entire workload but the secondary\nreplica is ready to take over at any time. This is sometimes called the hot spare or\n“hot standby” strategy since the spare is connected to the system, running (hot),\nand can be switched into operation instantly. It is also known as an active–passive\nor master–slave pair. Often there are multiple secondaries. Because there is only\none master, these conﬁgurations are 1 + M conﬁgurations.\nSometimes the term “active–active” or “master–master” pair will be used to\nrefer to two replicas that are load sharing. “Active–active” is more commonly\nused with network links. “Master–master” is more commonly used in the database\nworld and in situations where the two are tightly coupled.\n6.4 Failure Domains\nA failure domain is the bounded area beyond which failure has no impact. For\nexample, when a car fails on a highway, its failure does not make the entire highway\nunusable. The impact of the failure is bounded to its failure domain.\nThe failure domain of a fuse in a home circuit breaker box is the room or two\nthat is covered by that circuit. If a power line is cut, the failure domain affects a\nnumber of houses or perhaps a city block. The failure domain of a power grid might\n",
      "page_number": 149
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": "6.4\nFailure Domains\n127\nbe the town, region, or county that it feeds (which is why some datacenters are\nlocated strategically so they have access to two power grids).\nA failure domain may be prescriptive—that is, a design goal or requirement.\nYou might plan that two groups of servers are each their own failure domain and\nthen engineer the system to meet that goal, assuring that the failure domains that\nthey themselves rely on are independent. Each group may be in different racks,\ndifferent power circuits, and so on. Whether they should be in different datacenters\ndepends on the scope of the failure domain goal.\nAlternatively, a failure domain may be descriptive. Often we ﬁnd ourselves\nexploring a system trying to determine, or reverse-engineer, what the resulting\nfailure domain has become. Due to a failed machine, a server may have been moved\ntemporarily to a spare machine in another rack. We can determine the new failure\ndomain by exploring the implications of this move.\nDetermining a failure domain is done within a particular scope or assump-\ntions about how large an outage we are willing to consider. For example, we may\nkeep off-site backups 1000 miles away, assuming that an outage that affects two\nbuildings that far apart is an acceptable risk, or that a disaster that large would\nmean we’d have other problems to worry about.\n.\nUnaligned Failure Domains Increase Outage Impact\nA company with many large datacenters used an architecture in which a\npower bus was shared by every group of six racks. A network subsystem\nprovided network connectivity for every eight racks. The network subsystem\nreceived power from the ﬁrst rack of each of its groups.\nIf a power bus needed to be turned off for maintenance, the outage this\nwould create would involve the six racks directly attached to it for power, plus\nother racks would lose network connectivity if they were unlucky enough\nto be on a network subsystem that got power from an affected rack. This\nextended the failure domain to as many as 13 racks. Many users felt it was\nunfair that they were suffering even though the repair didn’t directly affect\nthem.\nThere were additional unaligned failure domains related to cooling and\nwhich machines were managed by which cluster manager. As a result, these\nmisalignments were not just an inconvenience to some but a major factor\ncontributing to system availability.\nEventually a new datacenter design was created that aligned all physical\nfailure domains to a common multiple. In some cases, this meant working with\nvendors to create custom designs. Old datacenters were eventually retroﬁtted\nto the new design at great expense.\n\n\n128\nChapter 6\nDesign Patterns for Resiliency\nWe commonly hear of datacenters that have perfect alignment of power,\nnetworking, and other factors but in which an unexpected misalignment results\nin a major outage. For example, consider a datacenter with 10 domains, each\nindependently powered, cooled, and networked. Suppose the building has two\nconnections to the outside world and the related equipment is located based on\nwhere the connections come into the building. If cooling fails in the two domains\nthat include those connections, suddenly all 10 domains have no connectivity to\nthe outside world.\n6.5 Software Failures\nAs long as there has been software, there have been software bugs. Long-running\nsoftware can die unexpectedly. Software can hang and not respond. For all these\nreasons, software needs resilience features, too.\n6.5.1 Software Crashes\nA common failure in a system is that software crashes, or prematurely exits. There\nare many reasons software may crash and many ways to respond. Server software\nis generally intended to be long lived. For example, a server that provides a partic-\nular API is expected to run forever unless the conﬁguration changes in a way that\nrequires a restart or the service is decommissioned.\nThere are two categories of crashes:\n• A regular crash occurs when the software does something prohibited by the\noperating system. For example, due to a software bug, the program may try\nto write to memory that is marked read-only by the operating system. The OS\ndetects this and kills the process.\n• A panic occurs when the software itself detects something is wrong and\ndecides the best course is to exit. For example, the software may detect a situ-\nation that shouldn’t exist and cannot be corrected. The software’s author may\nhave decided the safest thing to do in such a scenario is to exit. For example, if\ninternal data structures are corrupted and there is no safe way to rectify them,\nit is best to stop work immediately rather than continue with bad data. A panic\nis, essentially, an intentional crash.\nAutomated Restarts and Escalation\nThe easiest way to deal with a software crash is to restart the software. Sometimes\nthe problem is transient and a restart is all that is needed to ﬁx it. Such restarts\nshould be automated. With thousands of servers, it is inefﬁcient for a human to\nconstantly be checking processes to see if they are down and restarting them as\nneeded. A program that handles this task called a process watcher.\n\n\n6.5\nSoftware Failures\n129\nHowever, restarting a down process is not as easy as it sounds. If it immedi-\nately crashes again and again, we need to do something else; otherwise, we will be\nwasting CPU time without improving the situation. Usually the process watcher\nwill detect that the process has been restarted x times in y minutes and consider\nthat behavior cause to escalate the issue. Escalation involves not restarting the pro-\ncess and instead reporting the problem to a human. An example threshold might\nbe that something has restarted more than ﬁve times in a minute.\nLess frequent restarts are often a sign of other problems. One restart every\nhour is not cause for alarm but it should be investigated. Often these slower restart\nissues are detected by the monitoring system rather than the process watcher.\nAutomated Crash Data Collection and Analysis\nEvery crash should be logged. Crashes usually leave behind a lot of information\nin a crash report. The crash report includes statistics such as amount of RAM\nand CPU usage at the time of the process’s death, as well as detailed informa-\ntion such as a traceback of which function call and line of code was executing\nwhen the problem occurred. A coredump—a ﬁle containing the contents of the\nprocess’s memory—is often written out during a crash. Developers use this ﬁle\nto aid debugging.\nAutomated collection and storage of crash reports is useful because this infor-\nmation may be lost if it is not collected quickly; the information may be deleted or\nthe machine may go away. Collecting the information is inconvenient for humans\nbut easy for automation. This is especially true in a system with hundreds of\nmachines and hundreds of thousands of processes. Storing the reports centrally\npermits data mining and analysis. A simple analytical result, such as which sys-\ntems crash the most, can be a useful engineering metric. More intricate analysis can\nﬁnd bugs in common software libraries, the operating system, hardware, or even\nparticular chips.\n6.5.2 Software Hangs\nSometimes when software has a problem it does not crash, but instead hangs or\ngets caught in an inﬁnite loop.\nA strategy for detecting hangs is to monitor the server and detect if it has\nstopped processing requests. We can passively observe request counts or actively\ntest the system by sending requests and verifying that a reply is generated within a\ncertain amount of time. These active requests, which are called pings, are designed\nto be light-weight, simply verifying basic functionality.\nIf pings are sent at a speciﬁc, periodic rate and are used to detect hangs as well\nas crashes, they are called heartbeat requests. When hangs are detected, an error\ncan be generated, an alert sent, or an attempt to restart the service can be made.\nIf the server is one of many replicas behind a load balancer, rather than simply\n\n\n130\nChapter 6\nDesign Patterns for Resiliency\nrestarting it, you can remove it from the load balancing rotation and investigate\nthe problem. Sometimes adding a new replica is signiﬁcantly more work than\nreturning a replica that has been repaired to service. For example, in the Google\nFile System, a new replica added to the system requires replicating possibly tera-\nbytes of ﬁles. This can ﬂood the network. Fixing a hung replica and returning it to\nservice simply results in the existing data being revalidated, which is a much more\nlight-weight task.\nAnother technique for dealing with software hangs is called a watchdog timer.\nA hardware clock keeps incrementing a counter. If the counter exceeds a certain\nvalue, a hardware subsystem will detect this and reboot the system. Software run-\nning on the system resets the counter to zero after any successful operation. If the\nsoftware hangs, the resets will stop and soon the system will be rebooted. As long as\nthe software keeps running, the counter will be reset frequently enough to prevent\na reboot.\nA watchdog timer is most commonly used with operating system kernels and\nembedded systems. Enabling the Linux kernel watchdog timer on a system with\nappropriate hardware can be used to reduce the need to physically visit a machine\nwhen the kernel hangs or to avoid the need to purchase expensive remote power\ncontrol systems.\nLike crashes, hangs should be logged and analyzed. Frequent hangs are an\nindication of hardware issues, locking problems, and other bugs that should be\nﬁxed before they become big problems.\n6.5.3 Query of Death\nSometimes a particular API call or query exercises an untested code path that\ncauses a crash, a long delay, or an inﬁnite loop. We call such a query a query of\ndeath because it kills the service.\nWhen users discover a query of death for a popular web site, they let all of\ntheir friends know. Soon much of the internet will also be trying it to see what a\ncrashing web site looks like. The better known your company is, the faster word\nwill spread.\nThe best ﬁx is to eliminate the bug that causes the problem. Unfortunately, it\ncan take a long time to ﬁx the code and push a new release. A quick ﬁx is needed\nin the meantime.\nA widely used strategy is to have a banned query list that is easy to update\nand communicate to all the frontends. The frontends automatically reject any query\nthat is found on the banned query list.\nHowever, that solution still requires human intervention. A more automated\nmechanism is required, especially when a query has a large fan-out. For example,\nsuppose the query is received and then sent to 1000 other servers, each one holding\n\n\n6.6\nPhysical Failures\n131\n1/1000th of the database. A query of death would kill 1000 servers along with all\nthe other queries that are in ﬂight.\nDean and Barroso (2013) describe a preventive measure pioneered at Google\ncalled canary requests. In situations where one would normally send the same\nrequest to thousands of leaf servers, systems using this approach send the query\nto one or two leaf servers. These are the canary requests. Queries are sent to the\nremaining servers only if replies to the canary requests are received in a reasonable\nperiod of time. If the leaf servers crash or hang while the canary requests are being\nprocessed, the system ﬂags the request as potentially dangerous and prevents fur-\nther crashes by not sending it to the remaining leaf servers. Using this technique\nGoogle is able to achieve a measure of robustness in the face of difﬁcult-to-predict\nprogramming errors as well as malicious denial-of-service attacks.\n6.6 Physical Failures\nDistributed systems also need to be resilient when faced with physical failures.\nThe physical devices used in a distributed system can fail on many levels. Physi-\ncal failures can range from the smallest electronic component all the way up to a\ncountry’s power grid. Providing resiliency through the use of redundancy at every\nlevel is expensive and difﬁcult to scale. You need a strategy for providing resiliency\nagainst hardware failures without adding excessive cost.\n6.6.1 Parts and Components\nMany components of a computer can fail. The parts whose utilization you mon-\nitor can fail, such as the CPU, the RAM, the disks, and the network interfaces.\nSupporting components can also fail, such as fans, power supplies, batteries, and\nmotherboards.\nHistorically, when the CPU died, the entire machine was unusable. Multi-\nprocessor computers are now quite common, however, so it is more likely that\na machine can survive so long as one processor is still functioning. If the machine\nis already resilient in that way, we must monitor for N + 0 situations.\nRAM\nRAM often fails for strange reasons. Sometimes a slight power surge can affect\nRAM. Other times a single bit ﬂips its value because a cosmic ray from another\nstar system just happened to ﬂy through it. Really!\nMany memory systems store with each byte an additional bit (a parity bit)\nthat enables them to detect errors, or two additional bits (error-correcting code or\nECC memory) that enable them to perform error correction. This adds cost. It also\ndrags down reliability because now there are 25 percent more bits and, therefore,\n\n\n132\nChapter 6\nDesign Patterns for Resiliency\nthe MTTF becomes 25 percent worse. (Although most of these failures are now cor-\nrected invisibly, the failures are still happening and can be detected via monitoring\nsystems. If the failures persist, the component needs to be replaced.)\nWhen writing to parity bit memory, the system counts how many 1 bits are in\nthe byte and stores a 0 in the parity bit if the total is even, or a 1 if the total is odd.\nAnytime memory is read, the parity is checked and mismatches are reported to the\noperating system. This is sufﬁcient to detect all single-bit errors, or any multiple-\nbit errors that do not preserve parity. ECC memory uses two additional bits and\nHamming code algorithms that can correct single-bit errors and detect multiple-bit\nerrors.\nThe likelihood of two or more bit errors increases the longer that values sit in\nmemory unread and the more RAM there is in a system.\nOne can save money by having no parity or ECC bits—an approach commonly\nused with low-end chipsets—but then all software has to do its own checksum-\nming and error correction. This is slow and costly, and you or your developers\nprobably won’t do it. So spend the money on ECC, instead.\nDisks\nDisks fail often because they have moving parts. Solid-state drives (SSDs), which\nhave no moving parts, wear out since each block is rated to be written only a certain\nnumber of times.\nThe usual solution is to use RAID level 1 or higher to achieve N + 1 redun-\ndancy or better. However, RAID systems are costly and their internal ﬁrmware\nis often a source of frustration, as it is difﬁcult to conﬁgure without interrupting\nservice. (A full explanation of RAID levels is not included here but can be found in\nour other book, The Practice of System and Network Administration.)\nFile systems such as ZFS, Btrfs and Hadoop HDFS store data reliably by pro-\nviding their own RAID or RAID-like functionality. In those cases hardware RAID\ncontrollers are not needed.\nWe recommend the strategic use of RAID controllers, deploying them only\nwhere required. For example, a widely used distributed computing environment\nis the Apache Hadoop system. The ﬁrst three machines in a Hadoop cluster are\nspecial master service machines that store critical conﬁguration information. This\ninformation is not replicated and is difﬁcult to rebuild if lost. The other machines\nin a Hadoop cluster are data nodes that store replicas of data. In this environment\nRAID is normally used on the master machines. Implementing RAID there has a\nﬁxed cost, as no more than three machines with RAID controllers are needed. Data\nnodes are added when more capacity is needed. They are built without RAID since\nHadoop replicates data as needed, detecting failures and creating new replicas as\nneeded. This strategy has a cost beneﬁt in that the expensive hardware is a ﬁxed\nquantity while the nodes used to expand the system are the inexpensive ones.\n\n\n6.6\nPhysical Failures\n133\nPower Supplies\nEach machine has a power supply that converts standard electric voltages into\nlevels needed by the computer. Power supplies frequently die. Servers, network\nequipment, and many other systems can be purchased with redundant power\nsupplies. N + 1 and N + 2 conﬁgurations are commonly available.\nAs with RAID, a strategic use of redundant power supplies is best. They are\nnot needed when the system itself is a replica or some other resilience technique is\nused at a higher level. Do use such power supplies for the remaining systems that\nare not redundant.\nNetwork Interfaces\nNetwork interfaces or network connections themselves often fail. Multiple links\ncan be used in N + 1 conﬁgurations. There are many standards, too many to detail\nhere.\nSome are load sharing, others are active–passive. Some require that all the\nnear-end (machine) connections be plugged into the same network interface con-\ntroller (NIC) daughterboard. If two physical ports share the same daughterboard,\nthe failure of one may cause the other to fail. Some require that all the far-end\n(switch) connections be plugged into the same switch, while others do not have\nsuch a limit. The latter approach provides resiliency against switch failure, not just\nNIC failure.\nMany different algorithms are available for determining which packets go\nover which physical link. With some, it is possible for packets to arrive out of order.\nWhile all protocols should handle this situation, many do not do it well.\nLongitudinal Studies on Hardware Failures\nGoogle has published to two longitudinal studies of hardware failures. Most\nstudies of such failures are done in laboratory environments. Google meticu-\nlously collects component failure information on its entire ﬂeet of machines,\nproviding probably the best insight into actual failure patterns. Both studies\nare worth reading.\n“Failure Trends in a Large Disk Drive Population” (Pinheiro, Weber &\nBarroso 2007) analyzed a large population of hard disks over many years. The\nauthors did not ﬁnd temperature or activity levels to correlate with drive fail-\nures. They found that after a single scan error was detected, drives are 39 times\nmore likely to fail within the next 60 days. They discovered the “bathtub fail-\nure curve” where failures tend to happen either in the ﬁrst month or only many\nyears later.\n\n\n134\nChapter 6\nDesign Patterns for Resiliency\n“DRAM Errors in the Wild: A Large-Scale Field Study” (Schroeder,\nPinheiro & Weber 2009) analyzed memory errors in a large ﬂeet of machines\nin datacenters over a period of 2.5 years. These authors found that error\nrates were orders of magnitude higher than previously reported and were\ndominated by hard errors—the kind that ECC can detect but not correct.\nTemperature had comparatively small effect compared to other factors.\n6.6.2 Machines\nMachine failures are generally the result of components that have died. If the\nsystem has subsystems that are N + 1, a double failure results in machine death.\nA machine that crashes will often come back to life if it is power cycled off and\nback on, often with a delay to let the components drain. This process can be auto-\nmated, although it is important that the automation be able to distinguish between\nnot being able to reach the machine and the machine being down.\nIf a power cycle does not revive the machine, the machine must be diagnosed,\nrepaired, and brought back into service. Much of this can be automated, especially\nthe reinstallation of the operating system. This topic is covered in more detail in\nSection 10.4.1.\nEarlier we described situations where machines fail to boot up after a power\noutage. These problems can be discovered preemptively by periodically rebooting\nthem. For example, Google drains machines one by one for kernel upgrades. As a\nresult of this practice, each machine is rebooted in a controlled way approximately\nevery three months. This reduces the number of surprises found during power\noutages.\n6.6.3 Load Balancers\nWhether a server fails because of a dead machine, a network issue, or a bug, a\nresilient way to deal with this failure is by use of replicas and some kind of load\nbalancer.\nThe same load balancer described previously to gain scale is also used to\ngain resiliency. However, when using this approach to gain scale, each replica\nadded was intended to add capacity that would be used. Now we are adding spare\ncapacity that is an insurance policy we hope not to use.\nWhen using a load balancer it is important to consider whether it is being used\nfor scaling, resiliency, or both. We have observed situations where it was assumed\nthat the presence of a load balancer means the system scales and is resilient auto-\nmatically. This is not true. The load balancer is not magic. It is a technology that\ncan be used for many different things.\n",
      "page_number": 157
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 165-172)",
      "start_page": 165,
      "end_page": 172,
      "detection_method": "topic_boundary",
      "content": "6.6\nPhysical Failures\n135\nScale versus Resiliency\nIf we are load balancing over two machines, each at 40 percent utilization, then\neither machine can die and the remaining machines will be 80 percent utilized. In\nsuch a case, the load balancer is used for resiliency.\nIf we are load balancing over two machines, each at 80 percent utilization,\nthen there is no spare capacity available if one goes down. If one machine died,\nthe remaining replica would receive all the trafﬁc, which is 160 percent of what the\nmachine can handle. The machine will be overloaded and may cease to function.\nTwo machines each at 80 percent utilization represents an N + 0 conﬁguration. In\nthis situation, the load balancer is used for scale, not resiliency.\nIn both of the previous examples, the same conﬁguration was used: two\nmachines and a load balancer. Yet in one case resiliency was achieved and in the\nother case scale was achieved. The difference between the two was the utilization,\nor trafﬁc, being processed. In other words, 50 percent is 100 full when you have\nonly two servers.\nIf we take the second example and add a third replica but the amount of trafﬁc\ndoes not change, then 160 percent of the total 300 percent capacity is in use. This is\nan N + 1 conﬁguration since one replica can die and the remaining replicas can still\nhandle the load. In this case, the load balancer is used for both scale and resiliency.\nA load balancer provides scale when we use it to keep up with capacity, and\nresiliency when we use it to exceed capacity. If utilization increases and we have\nnot added additional replicas, we run the risk of no longer being able to claim\nresiliency. If trafﬁc is high during the day and low at night, we can end up with a\nsystem that is resilient during some hours of the day and not others.\nLoad Balancer Resiliency\nLoad balancers themselves can become a single point of failure (SPOF). Redundant\npairs of load balancers are often used to remedy this shortcoming.\nOne strategy is a simple failover. One load balancer (the primary) receives\nall trafﬁc, and the other load balancer (the secondary) monitors the health of the\nprimary by sending heartbeat messages to it. If a loss of heartbeat is detected, the\nsecondary takes over and becomes the active load balancer. Any TCP connections\nthat were “in ﬂight” are disconnected since the primary is unaware of them.\nAnother strategy is stateful failover. It resembles simple failover except that\nthe two load balancers exchange enough information, or state, so that both know\nall existing TCP connections. As a consequence, those connections are not lost in\ncase of failover.\nA single pair of load balancers are often used for many different services—for\nexample, many different web sites. All the web sites are homed at one load bal-\nancer and the other is used for failover. When using non-stateful load balancers, a\ncommon trick is to home half the web sites on one load balancer and half the web\n\n\n136\nChapter 6\nDesign Patterns for Resiliency\nsites on the other load balancer. In this case, in the event of a failover half as many\nconnections are lost.\nHardware or Software Load Balancers\nLoad balancers may be hardware appliances that are purpose-built for the task.\nThey may also be software-based programs that run on standard computers. The\nhardware appliances are usually highly tuned and feature-rich. The software-\nbased ones are more ﬂexible.\nFor smaller services, the load balancer software might run on the machines\nproviding the services. Pushing the load balancing function to the machines them-\nselves reduces the amount of hardware to be managed. However, now the process-\ning of the load balancing software competes for CPU and other resources with the\nservices running on the box. This approach is not recommended for high-volume\nload balancing but is ﬁne for many situations.\nSoftware load balancing can also be pushed even further down the stack to\nthe clients themselves. Client-side load balancing requires that the client software\nknow which servers are available and do its own health checking, server selection,\nand so on. This is frequently done for internal services since they are usually more\ntightly controlled. The client library can load the conﬁguration from a central place,\nload balance requests, and detect and route around failures. The downside is that\nto change algorithms or ﬁx bugs, the client library must be changed, which requires\nupdating the software anywhere it is used.\n6.6.4 Racks\nRacks themselves do not usually fail. They are steel and have no active compo-\nnents. However, many failures are rack-wide. For example, a rack may have a\nsingle power feed or network uplink that is shared by all the equipment in the\nrack. Intrusive maintenance is often done one rack at a time.\nAs a result, a rack is usually a failure domain. In fact, intentionally designing\neach rack to be its own failure domain turns out to be a good, manageable size for\nmost distributed systems.\nRack Diversity\nYou can choose to break a service into many replicas and put one replica in each\nrack. With this arrangement, the service has rack diversity. A simple example\nwould be a DNS service where each DNS server is in a different rack so that a\nrack-wide failure does not cause a service outage.\nIn a Hadoop cluster, data ﬁles are stored on multiple machines for safety. The\nsystem tries to achieve rack diversity by making sure that at least one replica of\nany data block is in a different rack than the other data blocks.\n\n\n6.6\nPhysical Failures\n137\nRack Locality\nMaking a service component self-contained within a single rack also offers cer-\ntain beneﬁts. Bandwidth is plentiful within a rack but sparse between racks. All\nthe machines in the rack connect to the same switch at the top of the rack. This\nswitch has enough internal bandwidth that any machine can talk to any machine\nwithin the rack at full bandwidth, and all machines can do this at the same time—a\nscheme called non-blocking bandwidth. Between racks there is less bandwidth.\nRack uplinks are often 10 times the links between the machines, but they are a\nshared resource used by all the machines in the rack (typically 20 or 40). There\nis contention for bandwidth between racks. The article “A Guided Tour through\nData-center Networking” (Abts & Felderman 2012) drills down into this topic\nusing Google’s networks as examples.\nBecause bandwidth is plentiful inside the rack and the rack is a failure domain,\noften a service component is designed to ﬁt within a rack. Small queries come in,\nthey use a lot of bandwidth to generate the answer, and a small or medium-size\nreply leaves. This model ﬁts well given the bandwidth restrictions.\nThe service component is then replicated on many racks. Each replica has rack\nlocality, in that it is self-contained within the rack. It is designed to take advantage\nof the high bandwidth and the rack-sized failure domain.\nRack-sized replicas are sometimes called pods. A pod is self-contained and\noften forms its own security domain. For example, a billing system may be made\nup of pods, each one self-contained and designed to handle bill processing for a\nspeciﬁc group of customers.\n.\nClos Networking\nIt is reasonable to expect that eventually there will be network products on the\nopen market that provide non-blocking, full-speed connectivity between any\ntwo machines in an entire datacenter. We’ve known how to do this since 1953\n(Clos 1953). When this product introduction happens, it will change how we\ndesign services.\n6.6.5 Datacenters\nDatacenters can also be failure domains. An entire datacenter can go down due to\nnatural disasters, cooling failures, power failures, or an unfortunate backhoe dig\nthat takes out all network connections in one swipe.\nSimilar to rack diversity and rack locality, datacenter diversity and datacenter\nlocality also exist. Bandwidth within a datacenter is generally fast, though not\n\n\n138\nChapter 6\nDesign Patterns for Resiliency\nas fast as within a rack. Bandwidth between datacenters is generally slower and,\nunlike with data transmitted within a datacenter, is often billed for by the gigabyte.\nEach replica of a service should be self-contained within a datacenter but the\nentire service should have datacenter diversity. Google requires N + 2 diversity\nas a minimum requirement for user-facing services. That way, when one data-\ncenter is intentionally brought down for maintenance, another can go down due\nto unforeseen circumstances without impacting the service.\n6.7 Overload Failures\nDistributed systems need to be resilient when faced with high levels of load that\ncan happen as the result of a temporary surge in trafﬁc, an intentional attack,\nor automated systems querying the system at a high rate, possibly for malicious\nreasons.\n6.7.1 Traffic Surges\nSystems should be resilient against temporary periods of high load. For example, a\nsmall service may become overloaded after being mentioned in a popular web site\nor news broadcast. Even a large service can become overloaded due to load being\nshifted to the remaining replicas when one fails.\nThe primary strategy for dealing with this problem in user-facing services is\ngraceful degradation. This topic was covered in Section 2.1.10.\nDynamic Resource Allocation\nAnother strategy is to add capacity dynamically. With this approach, a sys-\ntem would detect that a service is becoming overloaded and allocate an unused\nmachine from a pool of idle machines that are running but otherwise unconﬁgured.\nAn automated system would conﬁgure the machine and use it to add capacity to\nthe overloaded service, thereby resolving the issue.\nIt can be costly to have idle capacity but this cost can be mitigated by using\na shared pool. That is, one pool of idle machines serves a group of services. The\nﬁrst service to become overloaded allocates the machines. If the pool is large\nenough, more than one service can become overloaded at the same time. There\nshould also be a mechanism for services to give back machines when the need\ndisappears.\nAdditional capacity can be found at other service providers as well. A public\ncloud computing provider can be used as the shared pool. Usually you will not\nhave to pay for unused capacity.\nShared resource pools are not just appropriate for machines, but may also be\nused for storage and other resources.\n\n\n6.7\nOverload Failures\n139\nLoad Shedding\nAnother strategy is load shedding. With this strategy the service turns away some\nusers so that other users can have a good experience.\nTo make an analogy, an overloaded phone system doesn’t suddenly discon-\nnect all existing calls. Instead, it responds to any new attempts to make a call with\na “fast busy” tone so that the person will try to make the call later. An overloaded\nweb site should likewise give some users an immediate response, such as a simple\n“come back later” web page, rather than requiring them to time out after minutes\nof waiting.\nA variation of load shedding is stopping certain tasks that can be put off until\nlater. For example, low-priority database updates could be queued up for process-\ning later; a social network that stores reputation points for users might store the\nfact that points have been awarded rather than processing them; nightly bulk ﬁle\ntransfers might be delayed if the network is overloaded.\nThat said, tasks that can be put off for a couple of hours might cause problems\nif they are put off forever. There is, after all, a reason they exist. For any activity that\nis delayed due to load shedding, there must be a plan on how such a delay is han-\ndled. Establish a service level agreement (SLA) to determine how long something\ncan be delayed and to identify a timeline of actions that should be undertaken to\nmitigate problems or extend the deadlines. Low-priority updates might become a\nhigh priority after a certain amount of time. If many systems are turned off due\nto load shedding, it might be possible to enable them, one at a time, to let each\ncatch up.\nTo be able to manage such situations one must have visibility into the system\nso that prioritization decisions can be made. For example, knowing the age of a\ntask (how long it has been delayed), predicting how long it will take to process,\nand indicating how close it is to a deadline will permit operations personnel to\ngauge when delayed items should be continued.\n.\nDelayed Work Reduces Quality\nAn old version of Google Web Search had two parts: the user-facing web front-\nend and the system that received and processed updates to the search index\n(corpus). These updates arrived in large chunks that had to be distributed to\neach frontend.\nThe quality of the search system was measured in terms of how fresh the\ncorpus was across all the web frontends.\nThe monitoring dashboard displayed the freshness of shards in each\nfrontend. It listed how many shards were in each freshness bucket: up-to-date,\n1 hour old, 2 hours old, 4 hours old, and so on. With this visibility, operations\n\n\n140\nChapter 6\nDesign Patterns for Resiliency\n.\nstaff could see when something was wrong and gain an insight into which\nfrontends were the most out of date.\nIf the system was overloaded, the updater system was paused to free up\nresources for handling the additional load. The dashboard enabled operations\nstaff to understand the effects of the pause. They could unpause high-priority\nupdates to maintain at least minimal freshness.\n6.7.2 DoS and DDoS Attacks\nA denial-of-service (DoS) attack is an attempt to bring down a service by sending\na large volume of queries. A distributed denial-of-service (DDoS) attack occurs\nwhen many computers around the Internet are used in a coordinated fashion to\ncreate an extremely large DoS attack. DDoS attacks are commonly initiated from\nbotnets, which are large collections of computers around the world that have been\nsuccessfully inﬁltrated and are now controlled centrally, without the knowledge of\ntheir owners.\nBlocking the requests is usually not a successful defense against a DDoS attack.\nAttackers can forge packets in a way that obscures where the attack is coming from,\nthereby making it impossible for you to construct ﬁlters that would block the attack\nwithout blocking legitimate trafﬁc. If they do come from a ﬁxed set of sources,\nsimply not responding to the requests still hogs bandwidth used to receive the\nattack—and that alone can overload a network. The attack must be blocked from\noutside your network, usually by the ISP you connect to. Most ISPs do not provide\nthis kind of ﬁltering.\nThe best defense is to simply have more bandwidth than the attacker. This is\nvery difﬁcult considering that most DDoS attacks involve thousands of machines.\nVery large companies are able to use this line of defense. Smaller companies can\nuse DDoS attack mitigation services. Many CDN vendors (see Section 5.8) provide\nthis service since they have bandwidth available.\nSometimes a DDoS attack does not aim to exhaust bandwidth but rather to\nconsume large amounts of processing time or load. For example, one might ﬁnd\na small query that demands a large amount of resources to reply to. In this case\nthe banned query list described previously can be used to block this query until a\nsoftware release ﬁxes the problem.\n6.7.3 Scraping Attacks\nA scraping attack is an automated process that acts like a web browser to query\nfor information and then extracts (scrapes) the useful information from the HTML\npages it receives. For example, if you wanted a list of every book ever published\n\n\n6.8\nHuman Error\n141\nbut didn’t want to pay for such a database from a library supply company, you\ncould write a program that sends millions of search requests to Amazon.com, parse\nthe HTML pages, and extract the book titles to build your database. This use of\nAmazon is considered an attack because it violates the company’s terms of service.\nSuch an attack must be defended against to prevent theft of information,\nto prevent someone from violating the terms of service, and because a very fast\nscraper is equivalent to a DoS attack. Detecting such an attack is usually done by\nhaving all frontends report information about the queries they are receiving to a\ncentral scraping detector service.\nThe scraping detector warns the frontends of any suspected attacks. If there\nis high conﬁdence that a particular source is involved in an attack, the frontends\ncan block or refuse to answer the queries. If conﬁdence in the source of the attack\nis low, the frontends can respond in other ways. For example, they can ask the user\nto prove that he or she is a human by using a Captcha or other system that can\ndistinguish human from machine input.\nSome scraping is permitted, even desired. A scraping detector should have\na whitelist that permits search engine crawlers and other permitted agents to do\ntheir job.\n6.8 Human Error\nAs we design systems to be more resilient to hardware and software failures,\nthe remaining failures are likely to be due to human error. While this sounds\nobvious, this trend was not recognized until the groundbreaking paper “Why Do\nInternet Services Fail, and What Can Be Done about It?” was published in 2003\n(Oppenheimer, Ganapathi & Patterson 2003).\nThe strategies for dealing with human error can be categorized as getting\nbetter humans, removing humans from the loop, and detecting human errors and\nworking around them.\nWe get better humans by having better operational practices, especially\nthose that exercise the skills and behaviors that most need improvement. (See\nChapter 15.)\nWe remove humans from the loop through automation. Humans may get\nsloppy and not do as much checking for errors during a procedure, but automation,\nonce written, will always check its work (See Chapter 12.)\nDetecting human errors and working around them is also a function of auto-\nmation. A pre-check is automation that checks inputs and prevents a process from\nrunning if the tests fail. For example, a pre-check can verify that a recently edited\nconﬁguration ﬁle has no syntax errors and meets certain other quality criteria.\nFailing the pre-check would prevent the conﬁguration ﬁle from being put into use.\n\n\n142\nChapter 6\nDesign Patterns for Resiliency\nWhile pre-checks are intended to prevent problems, the reality is that they\ntend to lag behind experience. That is, after each outage we add new pre-checks to\nprevent that same human error from creating future outages.\nAnother common pre-check is for large changes. If a typical change usually\nconsists of only a few lines, a pre-check might require additional approval if the\nchange is larger than a particular number of lines. The change might be in the size\nof the input, the number of changed lines between the current input and new input,\nor the number of changed lines between the current and new output. For example,\na conﬁguration ﬁle may be used to control a system that generates other ﬁles. The\ngrowth of the output by more than a certain percentage may trigger additional\napproval.\nAnother way to be resilient to human error is to have two humans check\nall changes. Many source code control systems can be conﬁgured to not accept\nchanges from a user until a second user approves them. All system administration\nthat is done via changes to ﬁles in a source code repository are then checked by a\nsecond pair of eyes. This is a very common operational method at Google.\n6.9 Summary\nResiliency is a system’s ability to constructively deal with failures. A resilient\nsystem detects failure and routes around it.\nFailure is a normal part of operations and can occur at any level. Large systems\nmagnify the risk of small failures. A one in a million failure is a daily occurrence if\nyou have enough machines.\nFailures come from many sources. Software can fail unintentionally due to\nbugs or intentionally to prevent a bad situation from getting worse. Hardware can\nalso fail, with the scope of the failure ranging from the smallest component to the\nlargest network. Failure domains can be any size: a device, a computer, a rack, a\ndatacenter, or even an entire company.\nThe amount of capacity in a system is N + M, where N is the amount of capac-\nity used to provide a service and M is the amount of spare capacity available, which\ncan be used in the event of a failure. A system that is N + 1 fault tolerant can survive\none unit of failure and remain operational.\nThe most common way to route around failure is through replication of ser-\nvices. A service may be replicated one or more times per failure domain to provide\nresilience greater than the domain.\nFailures can also come from external sources that overload a system, and\nfrom human mistakes. There are countermeasures to nearly every failure imag-\ninable. We can’t anticipate all failures, but we can plan for them, design solutions,\nprioritize their implementation, and repeat the process.\n",
      "page_number": 165
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 173-180)",
      "start_page": 173,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": "Exercises\n143\nExercises\n1. What are the major sources of failure in distributed computing systems?\n2. What are the most common failures: software, hardware, or human? Justify\nyour answer.\n3. Select one resiliency technique and give an example of a failure and the way\nin which the resiliency technique would prevent a user-visible outage. Do this\nfor one technique in each of these sections: 6.5, 6.6, 6.7, and 6.8.\n4. If a load balancer is being used, the system is automatically scalable and\nresilient. Do you agree or disagree with this statement? Justify your answer.\n5. Which resiliency techniques or technologies are in use in your environment?\n6. Where would you like to add resiliency in your current environment? Describe\nwhat you would change and which techniques you would apply.\n7. In your environment, give an example of graceful degradation under load, or\nexplain how you would implement it if it doesn’t currently exist.\n8. How big can a RAID5 array be? For example, how large can it be before the\nparity checking scheme is likely to miss an error? How long can the rebuild\ntime be before MTBF puts the system at risk of a second failure?\n9. The phrase “Eat right. Exercise. Die anyway.” is mentioned on page 123.\nExplain how this relates to distributed computing.\n\n\nThis page intentionally left blank \n\n\nPart II\nOperations: Running It\n\n\nThis page intentionally left blank \n\n\nChapter 7\nOperations in a Distributed\nWorld\nThe rate at which organizations\nlearn may soon become the only\nsustainable source of competitive\nadvantage.\n—Peter Senge\nPart I of this book discussed how to build distributed systems. Now we discuss\nhow to run such systems.\nThe work done to keep a system running is called operations. More speciﬁ-\ncally, operations is the work done to keep a system running in a way that meets or\nexceeds operating parameters speciﬁed by a service level agreement (SLA). Oper-\nations includes all aspects of a service’s life cycle: from initial launch to the ﬁnal\ndecommissioning and everything in between.\nOperational work tends to focus on availability, speed and performance, secu-\nrity, capacity planning, and software/hardware upgrades. The failure to do any\nof these well results in a system that is unreliable. If a service is slow, users will\nassume it is broken. If a system is insecure, outsiders can take it down. With-\nout proper capacity planning, it will become overloaded and fail. Upgrades, done\nbadly, result in downtime. If upgrades aren’t done at all, bugs will go unﬁxed.\nBecause all of these activities ultimately affect the reliability of the system, Google\ncalls its operations team Site Reliability Engineering (SRE). Many companies have\nfollowed suit.\nOperations is a team sport. Operations is not done by a single person but\nrather by a team of people working together. For that reason much of what we\ndescribe will be processes and policies that help you work as a team, not as a group\nof individuals. In some companies, processes seem to be bureaucratic mazes that\nslow things down. As we describe here—and more important, in our professional\nexperience—good processes are exactly what makes it possible to run very large\n147\n\n\n148\nChapter 7\nOperations in a Distributed World\n.\nTerms to Know\nInnovate: Doing (good) things we haven’t done before.\nMachine: A virtual or physical machine.\nOncall: Being available as ﬁrst responder to an outage or alert.\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of one or more servers.\nSoft launch: Launching a new service without publicly announcing it. This\nway trafﬁc grows slowly as word of mouth spreads, which gives opera-\ntions some cushion to ﬁx problems or scale the system before too many\npeople have seen it.\nSRE: Site Reliability Engineer, the Google term for systems administrators\nwho maintain live services.\nStakeholders: People and organizations that are seen as having an interest\nin a project’s success.\ncomputing systems. In other words, process is what makes it possible for teams to\ndo the right thing, again and again.\nThis chapter starts with some operations management background, then dis-\ncusses the operations service life cycle, and ends with a discussion of typical\noperations work strategies. All of these topics will be expanded upon in the\nchapters that follow.\n7.1 Distributed Systems Operations\nTo understand distributed systems operations, one must ﬁrst understand how it is\ndifferent from typical enterprise IT. One must also understand the source of tension\nbetween operations and developers, and basic techniques for scaling operations.\n7.1.1 SRE versus Traditional Enterprise IT\nSystem administration is a continuum. On one end is a typical IT department,\nresponsible for traditional desktop and client–server computing infrastructure,\noften called enterprise IT. On the other end is an SRE or similar team responsi-\nble for a distributed computing environment, often associated with web sites and\nother services. While this may be a broad generalization, it serves to illustrate some\nimportant differences.\nSRE is different from an enterprise IT department because SREs tend to be\nfocused on providing a single service or a well-deﬁned set of services. A traditional\nenterprise IT department tends to have broad responsibility for desktop services,\n\n\n7.1\nDistributed Systems Operations\n149\nback-ofﬁce services, and everything in between (“everything with a power plug”).\nSRE’s customers tend to be the product management of the service while IT cus-\ntomers are the end users themselves. This means SRE efforts are focused on a few\nselect business metrics rather than being pulled in many directions by users, each\nof whom has his or her own priorities.\nAnother difference is in the attitude toward uptime. SREs maintain services\nthat have demanding, 24 × 7 uptime requirements. This creates a focus on pre-\nventing problems rather than reacting to outages, and on performing complex\nbut non-intrusive maintenance procedures. IT tends to be granted ﬂexibility with\nrespect to scheduling downtime and has SLAs that focus on how quickly service\ncan be restored in the event of an outage. In the SRE view, downtime is some-\nthing to be avoided and service should not stop while services are undergoing\nmaintenance.\nSREs tend to manage services that are constantly changing due to new soft-\nware releases and additions to capacity. IT tends to run services that are upgraded\nrarely. Often IT services are built by external contractors who go away once the\nsystem is stable.\nSREs maintain systems that are constantly being scaled to handle more trafﬁc\nand larger workloads. Latency, or how fast a particular request takes to process,\nis managed as well as overall throughput. Efﬁciency becomes a concern because\na little waste per machine becomes a big waste when there are hundreds or thou-\nsands of machines. In IT, systems are often built for environments that expect a\nmodest increase in workload per year. In this case a workable strategy is to build\nthe system large enough to handle the projected workload for the next few years,\nwhen the system is expected to be replaced.\nAs a result of these requirements, systems in SRE tend to be bespoke systems,\nbuilt on platforms that are home-grown or integrated from open source or other\nthird-party components. They are not “off the shelf” or turn key systems. They are\nactively managed, while IT systems may be unchanged from their initial delivery\nstate. Because of these differences, distributed computing services are best man-\naged by a separate team, with separate management, with bespoke operational\nand management practices.\nWhile there are many such differences, recently IT departments have begun to\nsee a demand for uptime and scalability similar to that seen in SRE environments.\nTherefore the management techniques from distributed computing are rapidly\nbeing adopted in the enterprise.\n7.1.2 Change versus Stability\nThere is a tension between the desire for stability and the desire for change. Oper-\nations teams tend to favor stability; developers desire change. Consider how each\ngroup is evaluated during end-of-the-year performance reviews. A developer is\npraised for writing code that makes it into production. Changes that result in a\n\n\n150\nChapter 7\nOperations in a Distributed World\ntangible difference to the service are rewarded above any other accomplishment.\nTherefore, developers want new releases pushed into production often. Opera-\ntions, in contrast, is rewarded for achieving compliance with SLAs, most of which\nrelate to uptime. Therefore stability is the priority.\nA system starts at a baseline of stability. A change is then made. All changes\nhave some kind of a destabilizing effect. Eventually the system becomes stable\nagain, usually through some kind of intervention. This is called the change-\ninstability cycle.\nAll software roll-outs affect stability. A change may introduce bugs, which are\nﬁxed through workarounds and new software releases. A release that introduces\nno new bugs still creates a destabilizing effect due to the process of shifting work-\nloads away from machines about to be upgraded. Non-software changes also have\na destabilizing effect. A network change may make the local network less stable\nwhile the change propagates throughout the network.\nBecause of the tension between the operational desire for stability and the\ndeveloper desire for change, there must be mechanisms to reach a balance.\nOne strategy is to prioritize work that improves stability over work that adds\nnew features. For example, bug ﬁxes would have a higher priority than feature\nrequests. With this approach, a major release introduces many new features, the\nnext few releases focus on ﬁxing bugs, and then a new major release starts the cycle\nover again. If engineering management is pressured to focus on new features and\nneglect bug ﬁxes, the result is a system that slowly destabilizes until it spins out of\ncontrol.\nAnother strategy is to align the goals of developers and operational staff. Both\nparties become responsible for SLA compliance as well as the velocity (rate of\nchange) of the system. Both have a component of their annual review that is tied\nto SLA compliance and both have a portion tied to the on-time delivery of new\nfeatures.\nOrganizations that have been the most successful at aligning goals like this\nhave restructured themselves so that developers and operations work as one\nteam. This is the premise of the DevOps movement, which will be described in\nChapter 8.\nAnother strategy is to budget time for stability improvements and time for\nnew features. Software engineering organizations usually have a way to estimate\nthe size of a software request or the amount of time it is expected to take to com-\nplete. Each new release has a certain size or time budget; within that budget a\ncertain amount of stability-improvement work is allocated. The case study at the\nend of Section 2.2.2 is an example of this approach. Similarly, this allocation can be\nachieved by assigning dedicated people to stability-related code changes.\nThe budget can also be based on an SLA. A certain amount of instability is\nexpected each month, which is considered a budget. Each roll-out uses some of\nthe budget, as do instability-related bugs. Developers can maximize the number\n",
      "page_number": 173
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 181-188)",
      "start_page": 181,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "7.1\nDistributed Systems Operations\n151\nof roll-outs that can be done each month by dedicating effort to improve the code\nthat causes this instability. This creates a positive feedback loop. An example of\nthis is Google’s Error Budgets, which are more fully explained in Section 19.4.\n7.1.3 Defining SRE\nThe core practices of SRE were reﬁned for more than 10 years at Google before\nbeing enumerated in public. In his keynote address at the ﬁrst USENIX SREcon,\nBenjamin Treynor Sloss (2014), Vice President of Site Reliability Engineering at\nGoogle, listed them as follows:\nSite Reliability Practices\n1. Hire only coders.\n2. Have an SLA for your service.\n3. Measure and report performance against the SLA.\n4. Use Error Budgets and gate launches on them.\n5. Have a common stafﬁng pool for SRE and Developers.\n6. Have excess Ops work overﬂow to the Dev team.\n7. Cap SRE operational load at 50 percent.\n8. Share 5 percent of Ops work with the Dev team.\n9. Oncall teams should have at least eight people at one location, or six people\nat each of multiple locations.\n10. Aim for a maximum of two events per oncall shift.\n11. Do a postmortem for every event.\n12. Postmortems are blameless and focus on process and technology, not people.\nThe ﬁrst principle for site reliability engineering is that SREs must be able to code.\nAn SRE might not be a full-time software developer, but he or she should be able\nto solve nontrivial problems by writing code. When asked to do 30 iterations of\na task, an SRE should do the ﬁrst two, get bored, and automate the rest. An SRE\nmust have enough software development experience to be able to communicate\nwith developers on their level and have an appreciation for what developers do,\nand for what computers can and can’t do.\nWhen SREs and developers come from a common stafﬁng pool, that means\nthat projects are allocated a certain number of engineers; these engineers may be\ndevelopers or SREs. The end result is that each SRE needed means one fewer devel-\noper in the team. Contrast this to the case at most companies where system adminis-\ntrators and developers are allocated from teams with separate budgets. Rationally a\nproject wants to maximize the number of developers, since they write new features.\nThe common stafﬁng pool encourages the developers to create systems that can be\noperated efﬁciently so as to minimize the number of SREs needed.\n\n\n152\nChapter 7\nOperations in a Distributed World\nAnother way to encourage developers to write code that minimizes opera-\ntional load is to require that excess operational work overﬂows to the developers.\nThis practice discourages developers from taking shortcuts that create undue oper-\national load. The developers would share any such burden. Likewise, by requiring\ndevelopers to perform 5 percent of operational work, developers stay in tune with\noperational realities.\nWithin the SRE team, capping the operational load at 50 percent limits the\namount of manual labor done. Manual labor has a lower return on investment than,\nfor example, writing code to replace the need for such labor. This is discussed in\nSection 12.4.2, “Reducing Toil.”\nMany SRE practices relate to ﬁnding balance between the desire for change\nand the need for stability. The most important of these is the Google SRE practice\ncalled Error Budgets, explained in detail in Section 19.4.\nCentral to the Error Budget is the SLA. All services must have an SLA, which\nspeciﬁes how reliable the system is going to be. The SLA becomes the standard by\nwhich all work is ultimately measured. SLAs are discussed in Chapter 16.\nAny outage or other major SLA-related event should be followed by the cre-\nation of a written postmortem that includes details of what happened, along with\nanalysis and suggestions for how to prevent such a situation in the future. This\nreport is shared within the company so that the entire organization can learn from\nthe experience. Postmortems focus on the process and the technology, not ﬁnd-\ning who to blame. Postmortems are the topic of Section 14.3.2. The person who is\noncall is responsible for responding to any SLA-related events and producing the\npostmortem report.\nOncall is not just a way to react to problems, but rather a way to reduce future\nproblems. It must be done in a way that is not unsustainably stressful for those\noncall, and it drives behaviors that encourage long-term ﬁxes and problem pre-\nvention. Oncall teams are made up of at least eight members at one location, or\nsix members at two locations. Teams of this size will be oncall often enough that\ntheir skills do not get stale, and their shifts can be short enough that each catches\nno more than two outage events. As a result, each member has enough time to fol-\nlow through on each event, performing the required long-term solution. Managing\noncall this way is the topic of Chapter 14.\nOther companies have adopted the SRE job title for their system administra-\ntors who maintain live production services. Each company applies a different set\nof practices to the role. These are the practices that deﬁne SRE at Google and are\ncore to its success.\n7.1.4 Operations at Scale\nOperations in distributed computing is operations at a large scale. Distributed com-\nputing involves hundreds and often thousands of computers working together. As\na result, operations is different than traditional computing administration.\n\n\n7.1\nDistributed Systems Operations\n153\nManual processes do not scale. When tasks are manual, if there are twice as\nmany tasks, there is twice as much human effort required. A system that is scaling\nto thousands of machines, servers, or processes, therefore, becomes untenable if\na process involves manually manipulating things. In contrast, automation does\nscale. Code written once can be used thousands of times. Processes that involve\nmany machines, processes, servers, or services should be automated. This idea\napplies to allocating machines, conﬁguring operating systems, installing software,\nand watching for trouble. Automation is not a “nice to have” but a “must have.”\n(Automation is the subject of Chapter 12.)\nWhen operations is automated, system administration is more like an assem-\nbly line than a craft. The job of the system administrator changes from being the\nperson who does the work to the person who maintains the robotics of an assembly\nline. Mass production techniques become viable and we can borrow operational\npractices from manufacturing. For example, by collecting measurements from\nevery stage of production, we can apply statistical analysis that helps us improve\nsystem throughput. Manufacturing techniques such as continuous improvement\nare the basis for the Three Ways of DevOps. (See Section 8.2.)\nThree categories of things are not automated: things that should be automated\nbut have not been yet, things that are not worth automating, and human processes\nthat can’t be automated.\nTasks That Are Not Yet Automated\nIt takes time to create, test, and deploy automation, so there will always be things\nthat are waiting to be automated. There is never enough time to automate every-\nthing, so we must prioritize and choose our methods wisely. (See Section 2.2.2 and\nSection 12.1.1.)\nFor processes that are not, or have not yet been, automated, creating proce-\ndural documentation, called a playbook, helps make the process repeatable and\nconsistent. A good playbook makes it easier to automate the process in the future.\nOften the most difﬁcult part of automating something is simply describing the\nprocess accurately. If a playbook does that, the actual coding is relatively easy.\nTasks That Are Not Worth Automating\nSome things are not worth automating because they happen infrequently, they are\ntoo difﬁcult to automate, or the process changes so often that automation is not pos-\nsible. Automation is an investment in time and effort and the return on investment\n(ROI) does not always make automation viable.\nNevertheless, there are some common cases that are worth automating. Often\nwhen those are automated, the more rare cases (edge cases) can be consolidated or\neliminated. In many situations, the newly automated common case provides such\nsuperior service that the edge-case customers will suddenly lose their need to be\nso unique.\n\n\n154\nChapter 7\nOperations in a Distributed World\n.\nBenefits of Automating the Common Case\nAt one company there were three ways that virtual machines were being pro-\nvisioned. All three were manual processes, and customers often waited days\nuntil a system administrator was available to do the task. A project to automate\nprovisioning was stalled because of the complexity of handling all three vari-\nations. Users of the two less common cases demanded that their provisioning\nprocess be different because they were (in their own eyes) unique and beau-\ntiful snowﬂakes. They had very serious justiﬁcations based on very serious\n(anecdotal) evidence and waved their hands vigorously to prove their point.\nTo get the project moving, it was decided to automate just the most common\ncase and promise the two edge cases would be added later.\nThis was much easier to implement than the original all-singing, all-\ndancing, provisioning system. With the initial automation, provisioning time\nwas reduced to a few minutes and could happen without system administra-\ntor involvement. Provisioning could even happen at night and on weekends.\nAt that point an amazing thing happened. The other two cases suddenly dis-\ncovered that their uniqueness had vanished! They adopted the automated\nmethod. The system administrators never automated the two edge cases and\nthe provisioning system remained uncomplicated and easy to maintain.\nTasks That Cannot Be Automated\nSome tasks cannot be automated because they are human processes: maintaining\nyour relationship with a stakeholder, managing the bidding process to make a large\npurchase, evaluating new technology, or negotiating within a team to assemble an\noncall schedule. While they cannot be eliminated through automation, they can be\nstreamlined:\n• Many interactions with stakeholders can be eliminated through better\ndocumentation. Stakeholders can be more self-sufﬁcient if provided with\nintroductory documentation, user documentation, best practices recommen-\ndations, a style guide, and so on. If your service will be used by many other\nservices or service teams, it becomes more important to have good documen-\ntation. Video instruction is also useful and does not require much effort if you\nsimply make a video recording of presentations you already give.\n• Some interactions with stakeholders can be eliminated by making common\nrequests self-service. Rather than meeting individually with customers to\nunderstand future capacity requirements, their forecasts can be collected via a\nweb user interface or an API. For example, if you provide a service to hundreds\n\n\n7.2\nService Life Cycle\n155\nof other teams, forecasting can be become a full-time job for a project manager;\nalternatively, it can be very little work with proper automation that integrates\nwith the company’s supply-chain management system.\n• Evaluating new technology can be labor intensive, but if a common case is\nidentiﬁed, the end-to-end process can be turned into an assembly-line process\nand optimized. For example, if hard drives are purchased by the thousand, it is\nwise to add a new model to the mix only periodically and only after a thorough\nevaluation. The evaluation process should be standardized and automated,\nand results stored automatically for analysis.\n• Automation can replace or accelerate team processes. Creating the oncall\nschedule can evolve into a chaotic mess of negotiations between team mem-\nbers battling to take time off during an important holiday. Automation turns\nthis into a self-service system that permits people to list their availability and\nthat churns out an optimal schedule for the next few months. Thus, it solves\nthe problem better and reduces stress.\n• Meta-processes such as communication, status, and process tracking can be\nfacilitated through online systems. As teams grow, just tracking the interac-\ntion and communication among all parties can become a burden. Automating\nthat can eliminate hours of manual work for each person. For example, a web-\nbased system that lets people see the status of their order as it works its way\nthrough approval processes eliminates the need for status reports, leaving\npeople to deal with just exceptions and problems. If a process has many com-\nplex handoffs between teams, a system that provides a status dashboard and\nautomatically notiﬁes teams when hand-offs happen can reduce the need for\nlegions of project managers.\n• The best process optimization is elimination. A task that is eliminated does not\nneed to be performed or maintained, nor will it have bugs or security ﬂaws.\nFor example, if production machines run three different operating systems,\nnarrowing that number down to two eliminates a lot of work. If you provide a\nservice to other service teams and require a lengthy approval process for each\nnew team, it may be better to streamline the approval process by automatically\napproving certain kinds of users.\n7.2 Service Life Cycle\nOperations is responsible for the entire service life cycle: launch, maintenance\n(both regular and emergency), upgrades, and decommissioning. Each phase\nhas unique requirements, so you’ll need a strategy for managing each phase\ndifferently.\n\n\n156\nChapter 7\nOperations in a Distributed World\nThe stages of the life cycle are:\n• Service Launch: Launching a service the ﬁrst time. The service is brought to\nlife, initial customers use it, and problems that were not discovered prior to\nthe launch are discovered and remedied. (Section 7.2.1)\n• Emergency Tasks: Handling exceptional or unexpected events. This includes\nhandling outages and, more importantly, detecting and ﬁxing conditions that\nprecipitate outages. (Chapter 14)\n• Nonemergency Tasks: Performing all manual work required as part of the\nnormally functioning system. This may include periodic (weekly or monthly)\nmaintenance tasks (for example, preparation for monthly billing events) as\nwell as processing requests from users (for example, requests to enable the\nservice for use by another internal service or team). (Section 7.3)\n• Upgrades: Deploying new software releases and hardware platforms. The bet-\nter we can do this, the more aggressively the company can try new things and\ninnovate. Each new software release is built and tested before deployment.\nTests include system tests, done by developers, as well as user acceptance\ntests (UAT), done by operations. UAT might include tests to verify there are\nno performance regressions (unexpected declines in performance). Vulner-\nability assessments are done to detect security issues. New hardware must\ngo through a hardware qualification to test for compatibility, performance\nregressions, and any changes in operational processes. (Section 10.2)\n• Decommissioning: Turning off a service. It is the opposite of a service launch:\nremoving the remaining users, turning off the service, removing references to\nthe service from any related service conﬁgurations, giving back any resources,\narchiving old data, and erasing or scrubbing data from any hardware before\nit is repurposed, sold, or disposed. (Section 7.2.2)\n• Project Work: Performing tasks large enough to require the allocation of\ndedicated resources and planning. While not directly part of the service life\ncycle, along the way tasks will arise that are larger than others. Examples\ninclude ﬁxing a repeating but intermittent failure, working with stakehold-\ners on roadmaps and plans for the product’s future, moving the service to a\nnew datacenter, and scaling the service in new ways. (Section 7.3)\nMost of the life-cycle stages listed here are covered in detail elsewhere in this book.\nService launches and decommissioning are covered in detail next.\n7.2.1 Service Launches\nNothing is more embarrassing than the failed public launch of a new service. Often\nwe see a new service launch that is so successful that it receives too much trafﬁc,\nbecomes overloaded, and goes down. This is ironic but not funny.\n\n\n7.2\nService Life Cycle\n157\nEach time we launch a new service, we learn something new. If we launch new\nservices rarely, then remembering those lessons until the next launch is difﬁcult.\nTherefore, if launches are rare, we should maintain a checklist of things to do and\nrecord the things you should remember to do next time. As the checklist grows\nwith each launch, we become better at launching services.\nIf we launch new services frequently, then there are probably many peo-\nple doing the launches. Some will be less experienced than others. In this case\nwe should maintain a checklist to share our experience. Every addition increases\nour organizational memory, the collection of knowledge within our organization,\nthereby making the organization smarter.\nA common problem is that other teams may not realize that planning a launch\nrequires effort. They may not allocate time for this effort and surprise operations\nteams at or near the launch date. These teams are unaware of all the potential pitfalls\nand problems that the checklist is intended to prevent. For this reason the launch\nchecklist should be something mentioned frequently in documentation, socialized\namong product managers, and made easy to access. The best-case scenario occurs\nwhen a service team comes to operations wishing to launch something and has been\nusing the checklist as a guide throughout development. Such a team has “done their\nhomework”; they have been working on the items in the checklist in parallel as the\nproduct was being developed. This does not happen by accident; the checklist must\nbe available, be advertised, and become part of the company culture.\nA simple strategy is to create a checklist of actions that need to be completed\nprior to launch. A more sophisticated strategy is for the checklist to be a series\nof questions that are audited by a Launch Readiness Engineer (LRE) or a Launch\nCommittee.\nHere is a sample launch readiness review checklist:\nSample Launch Readiness Review Survey\nThe purpose of this document is to gather information to be evaluated by a Launch Readi-\nness Engineer (LRE) when approving the launch of a new service. Please complete the\nsurvey prior to meeting with your LRE.\n• General Launch Information:\n– What is the service name?\n– When is the launch date/time?\n– Is this a soft or hard launch?\n• Architecture:\n– Describe the system architecture. Link to architecture documents if possible.\n– How does the failover work in the event of single-machine, rack, and\ndatacenter failure?\n– How is the system designed to scale under normal conditions?\n\n\n158\nChapter 7\nOperations in a Distributed World\n• Capacity:\n– What is the expected initial volume of users and QPS?\n– How was this number arrived at? (Link to load tests and reports.)\n– What is expected to happen if the initial volume is 2× expected? 5×? (Link\nto emergency capacity documents.)\n– What is the expected external (internet) bandwidth usage?\n– What are the requirements for network and storage after 1, 3, and 12\nmonths? (Link to conﬁrmation documents from the network and storage\nteams capacity planner.)\n• Dependencies:\n– Which systems does this depend on? (Link to dependency/data ﬂow\ndiagram.)\n– Which RPC limits are in place with these dependencies? (Link to limits and\nconﬁrmation from external groups they can handle the trafﬁc.)\n– What will happen if these RPC limits are exceeded ?\n– For each dependency, list the ticket number where this new service’s use\nof the dependency (and QPS rate) was requested and positively acknowl-\nedged.\n• Monitoring:\n– Are all subsystems monitored? Describe the monitoring strategy and doc-\nument what is monitored.\n– Does a dashboard exist for all major subsystems?\n– Do metrics dashboards exist? Are they in business, not technical, terms?\n– Was the number of “false alarm” alerts in the last month less than x?\n– Is the number of alerts received in a typical week less than x?\n• Documentation:\n– Does a playbook exist and include entries for all operational tasks and\nalerts?\n– Have an LRE review each entry for accuracy and completeness.\n– Is the number of open documentation-related bugs less than x?\n• Oncall:\n– Is the oncall schedule complete for the next n months?\n– Is the oncall schedule arranged such that each shift is likely to get fewer\nthan x alerts?\n• Disaster Preparedness:\n– What is the plan in case ﬁrst-day usage is 10 times greater than expected?\n– Do backups work and have restores been tested?\n• Operational Hygiene:\n– Are “spammy alerts” adjusted or corrected in a timely manner?\n",
      "page_number": 181
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "7.2\nService Life Cycle\n159\n– Are bugs ﬁled to raise visibility of issues—even minor annoyances or issues\nwith commonly known workarounds?\n– Do stability-related bugs take priority over new features?\n– Is a system in place to assure that the number of open bugs is kept low?\n• Approvals:\n– Has marketing approved all logos, verbiage, and URL formats?\n– Has the security team audited and approved the service?\n– Has a privacy audit been completed and all issues remediated?\nBecause a launch is complex, with many moving parts, we recommend that a single\nperson (the launch lead) take a leadership or coordinator role. If the developer\nand operations teams are very separate, one person from each might be selected to\nrepresent each team.\nThe launch lead then works through the checklist, delegating work, ﬁling bugs\nfor any omissions, and tracking all issues until launch is approved and executed.\nThe launch lead may also be responsible for coordinating post-launch problem\nresolution.\n.\nCase Study: Self-Service Launches at Google\nGoogle launches so many services that it needed a way to make the launch pro-\ncess streamlined and able to be initiated independently by a team. In addition\nto providing APIs and portals for the technical parts, the Launch Readiness\nReview (LRR) made the launch process itself self-service.\nThe LRR included a checklist and instructions on how to achieve each\nitem. An SRE engineer was assigned to shepherd the team through the process\nand hold them to some very high standards.\nSome checklist items were technical—for example, making sure that the\nGoogle load balancing system was used properly. Other items were caution-\nary, to prevent a launch team from repeating other teams’ past mistakes. For\nexample, one team had a failed launch because it received 10 times more\nusers than expected. There was no plan for how to handle this situation. The\nLRR checklist required teams to create a plan to handle this situation and\ndemonstrate that it had been tested ahead of time.\nOther checklist items were business related. Marketing, legal, and other\ndepartments were required to sign off on the launch. Each department had\nits own checklist. The SRE team made the service visible externally only after\nverifying that all of those sign-offs were complete.\n\n\n160\nChapter 7\nOperations in a Distributed World\n7.2.2 Service Decommissioning\nDecommissioning (or just “decomm”), or turning off a service, involves three major\nphases: removal of users, deallocation of resources, and disposal of resources.\nRemoving users is often a product management task. Usually it involves mak-\ning the users aware that they must move. Sometimes it is a technical issue of\nmoving them to another service. User data may need to be moved or archived.\nResource deallocation can cover many aspects. There may be DNS entries to\nbe removed, machines to power off, database connections to be disabled, and so on.\nUsually there are complex dependencies involved. Often nothing can begin until\nthe last user is off the service; certain resources cannot be deallocated before others,\nand so on. For example, typically a DNS entry is not removed until the machine is\nno longer in use. Network connections must remain in place if deallocating other\nservices depends on network connectivity.\nResource disposal includes securely erasing disks and other media and dis-\nposing of all hardware. The hardware may be repurposed, sold, or scrapped.\nIf decommissioning is done incorrectly or items are missed, resources will\nremain allocated. A checklist, that is added to over time, will help assure decom-\nmissioning is done completely and the tasks are done in the right order.\n7.3 Organizing Strategy for Operational Teams\nAn operational team needs to get work done. Therefore teams need a strategy that\nassures that all incoming work is received, scheduled, and completed. Broadly\nspeaking, there are three sources of operational work and these work items fall\ninto three categories. To understand how to best organize a team, ﬁrst you must\nunderstand these sources and categories.\nThe three sources of work are life-cycle management, interacting with stake-\nholders, and process improvement and automation. Life-cycle management is the\noperational work involved in running the service. Interacting with stakeholders\nrefers to both maintaining the relationship with people who use and depend on\nthe service, and prioritizing and fulﬁlling their requests. Process improvement and\nautomation is work inspired by the business desire for continuous improvement.\nNo matter the source, this work tends to fall into one of these three broad\ncategories:\n• Emergency Issues: Outages, and issues that indicate a pending outage that can\nbe prevented, and emergency requests from other teams. Usually initiated by\nan alert sent by the monitoring system via SMS or pager. (Chapter 14)\n\n\n7.3\nOrganizing Strategy for Operational Teams\n161\n• Normal Requests: Process work (repeatable processes that have not yet been\nautomated), non-urgent trouble reports, informational questions, and initial\nconsulting that results in larger projects. Usually initiated by a request ticket\nsystem. (Section 14.1.3)\n• Project Work: Small and large projects that evolve the system. Managed with\nwhatever project management style the team selects. (Section 12.4.2)\nTo assure that all sources and categories of work receive attention, we recommend\nthis simple organizing principle: people should always be working on projects,\nwith exceptions made to assure that emergency issues receive immediate attention\nand non-project customer requests are triaged and worked in a timely manner.\nMore speciﬁcally, at any given moment, the highest priority for one person on\nthe team should be responding to emergencies, the highest priority for one other\nperson on the team should be responding to normal requests, and the rest of the\nteam should be focused on project work.\nThis is counter to the way operations teams often work: everyone running\nfrom emergency to emergency with no time for project work. If there is no effort\ndedicated to improving the situation, the team will simply run from emergency to\nemergency until they are burned out.\nMajor improvements come from project work. Project work requires concen-\ntration and focus. If you are constantly being interrupted with emergency issues\nand requests, you will not be able to get projects done. If an entire team is focused\non emergencies and requests, nobody is working on projects.\nIt can be tempting to organize an operations team into three subteams, each\nfocusing on one source of work or one category of work. Either of these approaches\nwill create silos of responsibility. Process improvement is best done by the people\ninvolved in the process, not by observers.\nTo implement our recommended strategy, all members of the team focus on\nproject work as their main priority. However, team members take turns being\nresponsible for emergency issues as they arise. This responsibility is called oncall.\nLikewise, team members take turns being responsible for normal requests from\nother teams. This responsibility is called ticket duty.\nIt is common that oncall duty and ticket duty are scheduled in a rotation.\nFor example, a team of eight people may use an eight-week cycle. Each person is\nassigned a week where he or she is on call: expected to respond to alerts, spending\nany remaining time on projects. Each person is also assigned a different week\nwhere he or she is on ticket duty: expected to focus on triaging and responding\nto request tickets ﬁrst, working on other projects only if there is remaining time.\nThis gives team members six weeks out of the cycle that can be focused on project\nwork.\n\n\n162\nChapter 7\nOperations in a Distributed World\nLimiting each rotation to a speciﬁc person makes for smoother handoffs to the\nnext shift. In such a case, there are two people doing the handoff rather than a large\noperations team meeting. If more than 25 percent of a team needs to be dedicated\nto ticket duty and oncall, there is a serious problem with ﬁreﬁghting and a lack of\nautomation.\nThe team manager should be part of the operational rotation. This practice\nensures the manager is aware of the operational load and ﬁreﬁghting that goes\non. It also ensures that nontechnical managers don’t accidentally get hired into the\noperations organization.\nTeams may combine oncall and ticket duty into one position if the amount of\nwork in those categories is sufﬁciently small. Some teams may need to designate\nmultiple people to ﬁll each role.\nProject work is best done in small teams. Solo projects can damage a team by\nmaking members feel disconnected or by permitting individuals to work without\nconstructive feedback. Designs are better with at least some peer review. Without\nfeedback, members may end up working on projects they feel are important but\nhave marginal beneﬁt. Conversely, large teams often get stalled by lack of consen-\nsus. In their case, focusing on shipping quickly overcomes many of these problems.\nIt helps by making progress visible to the project members, the wider team, and\nmanagement. Course corrections are easier to make when feedback is frequent.\nThe Agile methodology, discussed in Section 8.6, is an effective way to\norganize project work.\n.\nMeta-work\nThere is also meta-work: meetings, status reports, company functions. These\ngenerally eat into project time and should be minimized. For advice, see\nChapter 11, “Eliminating Time Wasters,” in the book Time Management for\nSystem Administrators by Limoncelli (2005).\n7.3.1 Team Member Day Types\nNow that we have established an organizing principle for the team’s work, each\nteam member can organize his or her work based on what kind of day it is: a\nproject-focused day, an oncall day, or a ticket duty day.\nProject-Focused Days\nMost days should be project days for operational staff. Speciﬁcally, most days\nshould be spent developing software that automates or optimizes aspects of the\nteam’s responsibilities. Non-software projects include shepherding a new launch\nor working with stakeholders on requirements for future releases.\n\n\n7.3\nOrganizing Strategy for Operational Teams\n163\nOrganizing the work of a team through a single bug tracking system has the\nbeneﬁt of reducing time spent checking different systems for status. Bug tracking\nsystems provide an easy way for people to prioritize and track their work. On a\ntypical project day the staff member starts by checking the bug tracking system to\nreview the bugs assigned to him or her, or possibly to review unassigned issues of\nhigher priority the team member might need to take on.\nSoftware development in operations tends to mirror the Agile methodology:\nrather than making large, sudden changes, many small projects evolve the system\nover time. Chapter 12 will discuss automation and software engineering topics in\nmore detail.\nProjects that do not involve software development may involve technical\nwork. Moving a service to a new datacenter is highly technical work that cannot\nbe automated because it happens infrequently.\nOperations staff tend not to physically touch hardware not just because of the\nheavy use of virtual machines, but also because even physical machines are located\nin datacenters that are located far away. Datacenter technicians act as remote\nhands, applying physical changes when needed.\nOncall Days\nOncall days are spent working on projects until an alert is received, usually by\nSMS, text message, or pager.\nOnce an alert is received, the issue is worked until it is resolved. Often there\nare multiple solutions to a problem, usually including one that will ﬁx the problem\nquickly but temporarily and others that are long-term ﬁxes. Generally the quick\nﬁx is employed because returning the service to normal operating parameters is\nparamount.\nOnce the alert is resolved, a number of other tasks should always be done. The\nalert should be categorized and annotated in some form of electronic alert jour-\nnal so that trends may be discovered. If a quick ﬁx was employed, a bug should\nbe ﬁled requesting a longer-term ﬁx. The oncall person may take some time to\nupdate the playbook entry for this alert, thereby building organizational mem-\nory. If there was a user-visible outage or an SLA violation, a postmortem report\nshould be written. An investigation should be conducted to ascertain the root\ncause of the problem. Writing a postmortem report, ﬁling bugs, and root causes\nidentiﬁcation are all ways that we raise the visibility of issues so that they get\nattention. Otherwise, we will continually muddle through ad hoc workarounds\nand nothing will ever get better. Postmortem reports (possibly redacted for tech-\nnical content) can be shared with the user community to build conﬁdence in the\nservice.\nThe beneﬁt of having a speciﬁc person assigned to oncall duty at any given\ntime is that it enables the rest of the team to remain focused on project work. Studies\nhave found that the key to software developer productivity is to have long periods\n\n\n164\nChapter 7\nOperations in a Distributed World\nof uninterrupted time. That said, if a major crisis appears, the oncall person will\npull people away from their projects to assist.\nIf oncall shifts are too long, the oncall person will be overloaded with follow-\nup work. If the shifts are too close together, there will not be time to complete the\nfollow-up work. Many great ideas for new projects and improvements are ﬁrst\nimagined while servicing alerts. Between oncall shifts people should have enough\ntime to pursue such projects.\nChapter 14 will discuss oncall in greater detail.\nTicket Duty Days\nTicket duty days are spent working on requests from customers. Here the cus-\ntomers are the internal users of the service, such as other service teams that use\nyour service’s API. These are not tickets from external users. Those items should\nbe handled by customer support representatives.\nWhile oncall is expected to have very fast reaction time, tickets generally have\nan expected response time measured in days.\nTypical tickets may consist of questions about the service, which can lead to\nsome consulting on how to use the service. They may also be requests for activa-\ntion of a service, reports of problems or difﬁculties people are experiencing, and so\nforth. Sometimes tickets are created by automated systems. For example, a moni-\ntoring system may detect a situation that is not so urgent that it needs immediate\nresponse and may open a ticket instead.\nSome long-running tickets left from the previous shift may need follow-up.\nOften there is a policy that if we are waiting for a reply from the customer, every\nthree days the customer will be politely “poked” to make sure the issue is not for-\ngotten. If the customer is waiting for follow-up from us, there may be a policy that\nurgent tickets will have a status update posted daily, with longer stretches of time\nfor other priorities.\nIf a ticket will not be completed by the end of a shift, its status should be\nincluded in the shift report so that the next person can pick up where the previous\nperson left off.\nBy dedicating a person to ticket duty, that individual can be more focused\nwhile responding to tickets. All tickets can be triaged and prioritized. There is more\ntime to categorize tickets so that trends can be spotted. Efﬁciencies can be realized\nby batching up similar tickets to be done in a row. More importantly, by dedicating\na person to tickets, that individual should have time to go deeper into each ticket:\nto update documentation and playbooks along the way, to deep-dive into bugs\nrather than ﬁnd superﬁcial workarounds, to ﬁx complex broken processes. Ticket\nduty should not be a chore, but rather should be part of the strategy to reduce the\noverall work faced by the team.\nEvery operations team should have a goal of eliminating the need for people\nto open tickets with them, similar to how there should always be a goal to automate\n\n\n7.3\nOrganizing Strategy for Operational Teams\n165\nmanual processes. A ticket requesting information is an indication that documen-\ntation should be improved. It is best to respond to the question by adding the\nrequested information to the service’s FAQ or other user documentation and then\ndirecting the user to that document. Requests for service activation, allocations, or\nconﬁguration changes indicate an opportunity to create a web-based portal or API\nto make such requests obsolete. Any ticket created by an automated system should\nhave a corresponding playbook entry that explains how to process it, with a link\nto the bug ID requesting that the automation be improved to eliminate the need to\nopen such tickets.\nAt the end of oncall and ticket duty shifts, it is common for the person to email\nout a shift report to the entire team. This report should mention any trends noticed\nand any advice or status information to be passed on to the next person. The oncall\nend-of-shift report should also include a log of which alerts were received and what\nwas done in response.\nWhen you are oncall or doing ticket duty, that is your main project. Other\nproject work that is accomplished, if any, is a bonus. Management should not\nexpect other projects to get done, nor should people be penalized for having the\nproper focus. When people end their oncall or ticket duty time, they should not\ncomplain that they weren’t able to get any project work done; their project, so to\nspeak, was ticket duty.\n7.3.2 Other Strategies\nThere are many other ways to organize the work of a team. The team can rotate\nthough projects focused on a particular goal or subsystem, it can focus on reducing\ntoil, or special days can be set aside for reducing technical debt.\nFocus or Theme\nOne can pick a category of issues to focus on for a month or two, changing themes\nperiodically or when the current theme is complete. For example, at the start of a\ntheme, a number of security-related issues can be selected and everyone commit\nto focusing on them until they are complete. Once these items are complete, the\nnext theme begins. Some common themes include monitoring, a particular service\nor subservice, or automating a particular task.\nIf the team cohesion was low, this can help everyone feel as if they are work-\ning as a team again. It can also enhance productivity: if everyone has familiarized\nthemselves with the same part of the code base, everyone can do a better job of\nhelping each other.\nIntroducing a theme can also provide a certain amount of motivation. If the\nteam is looking forward to the next theme (because it is more interesting, novel, or\nfun), they will be motivated to meet the goals of the current theme so they can start\nthe next one.\n\n\n166\nChapter 7\nOperations in a Distributed World\nToil Reduction\nToil is manual work that is particularly exhausting. If a team calculates the number\nof hours spent on toil versus normal project work, that ratio should be as low as\npossible. Management may set a threshold such that if it goes above 50 percent,\nthe team pauses all new features and works to solve the big problems that are the\nsource of so much toil. (See Section 12.4.2.)\nFix-It Days\nA day (or series of days) can be set aside to reduce technical debt. Technical debt is\nthe accumulation of small unﬁnished amounts of work. By themselves, these bits\nand pieces are not urgent, but the accumulation of them starts to become a problem.\nFor example, a Documentation Fix-It Day would involve everyone stopping all\nother work to focus on bugs related to documentation that needs to be improved.\nAlternatively, a Fix-It Week might be declared to focus on bringing all monitoring\nconﬁgurations up to a particular standard.\nOften teams turn ﬁx-its into a game. For example, at the start a list of tasks (or\nbugs) is published. Prizes are given out to the people who resolve the most bugs.\nIf done company-wide, teams may receive T-shirts for participating and/or prizes\nfor completing the most tasks.\n7.4 Virtual Office\nMany operations teams work from home rather than an ofﬁce. Since work is virtual,\nwith remote hands touching hardware when needed, we can work from anywhere.\nTherefore, it is common to work from anywhere. When necessary, the team meets\nin chat rooms or other virtual meeting spaces rather than physical meeting rooms.\nWhen teams work this way, communication must be more intentional because you\ndon’t just happen to see each other in the ofﬁce.\nIt is good to have a policy that anyone who is not working from the ofﬁce\ntakes responsibility for staying in touch with the team. They should clearly and\nperiodically communicate their status. In turn, the entire team should take respon-\nsibility for making sure remote workers do not feel isolated. Everyone should know\nwhat their team members are working on and take the time to include everyone in\ndiscussions. There are many tools that can help achieve this.\n7.4.1 Communication Mechanisms\nChat rooms are commonly used for staying in touch throughout the day. Chat\nroom transcripts should be stored and accessible so people can read what they\nmay have missed. There are many chat room “bots” (software robots that join the\n",
      "page_number": 189
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 197-207)",
      "start_page": 197,
      "end_page": 207,
      "detection_method": "topic_boundary",
      "content": "7.5\nSummary\n167\nchat room and provide services) that can provide transcription services, pass mes-\nsages to ofﬂine members, announce when oncall shifts change, and broadcast any\nalerts generated by the monitoring system. Some bots provide entertainment: At\nGoogle, a bot keeps track of who has received the most virtual high-ﬁves. At Stack\nExchange, a bot notices if anyone types the phrase “not my fault” and responds\nby selecting a random person from the room and announcing this person has been\nrandomly designated to be blamed.\nHigher-bandwidth communication systems include voice and video systems\nas well as screen sharing applications. The higher the bandwidth, the better the\nﬁdelity of communication that can be achieved. Text-chat is not good at conveying\nemotions, while voice and video can. Always switch to higher-ﬁdelity communi-\ncation systems when conveying emotions is more important, especially when an\nintense or heated debate starts.\nThe communication medium with the highest ﬁdelity is the in-person meet-\ning. Virtual teams greatly beneﬁt from periodic in-person meetings. Everyone\ntravels to the same place for a few days of meetings that focus on long-term\nplanning, team building, and other issues that cannot be solved online.\n7.4.2 Communication Policies\nMany teams establish a communication agreement that clariﬁes which methods\nwill be used in which situations. For example, a common agreement is that chat\nrooms will be the primary communication channel but only for ephemeral discus-\nsions. If a decision is made in the chat room or an announcement needs to be made,\nit will be broadcast via email. Email is for information that needs to carry across\noncall shifts or day boundaries. Announcements with lasting effects, such as major\npolicies or design decisions, need to be recorded in the team wiki or other docu-\nment system (and the creation of said document needs to be announced via email).\nEstablishing this chat–email–document paradigm can go a long way in reducing\ncommunication problems.\n7.5 Summary\nOperations is different from typical enterprise IT because it is focused on a par-\nticular service or group of services and because it has more demanding uptime\nrequirements.\nThere is a tension between the operations team’s desire for stability and the\ndevelopers’ desire to get new code into production. There are many ways to reach\na balance. Most ways involve aligning goals by sharing responsibility for both\nuptime and velocity of new features.\n\n\n168\nChapter 7\nOperations in a Distributed World\nOperations in distributed computing is done at a large scale. Processes that\nhave to be done manually do not scale. Constant process improvement and\nautomation are essential.\nOperations is responsible for the life cycle of a service: launch, maintenance,\nupgrades, and decommissioning. Maintenance tasks include emergency and non-\nemergency response. In addition, related projects maintain and evolve the service.\nLaunches, decommissioning of services, and other tasks that are done infre-\nquently require an attention to detail that is best assured by use of checklists.\nChecklists ensure that lessons learned in the past are carried forward.\nThe most productive use of time for operational staff is time spent automating\nand optimizing processes. This should be their primary responsibility. In addition,\ntwo other kinds of work require attention. Emergency tasks need fast response.\nNonemergency requests need to be managed such that they are prioritized and\nworked in a timely manner. To make sure all these things happen, at any given\ntime one person on the operations team should be focused on responding to emer-\ngencies; another should be assigned to prioritizing and working on nonemergency\nrequests. When team members take turns addressing these responsibilities, they\nreceive the dedicated resources required to assure they happen correctly by sharing\nthe responsibility across the team. People also avoid burning out.\nOperations teams generally work far from the actual machines that run their\nservices. Since they operate the service remotely, they can work from anywhere\nthere is a network connection. Therefore teams often work from different places,\ncollaborating and communicating in a chat room or other virtual ofﬁce. Many tools\nare available to enable this type of organizational structure. In such an environ-\nment, it becomes important to change the communication medium based on the\ntype of communication required. Chat rooms are sufﬁcient for general commu-\nnication but voice and video are more appropriate for more intense discussions.\nEmail is more appropriate when a record of the communication is required, or if it\nis important to reach people who are not currently online.\nExercises\n1. What is operations? What are its major areas of responsibilities?\n2. How does operations in distributed computing differ from traditional desktop\nsupport or enterprise client–server support?\n3. Describe the service life cycle as it relates to a service you have experience\nwith.\n4. Section 7.1.2 discusses the change-instability cycle. Draw a series of graphs\nwhere the x-axis is time and the y-axis is the measure of stability. Each graph\nshould represent two months of project time.\n\n\nExercises\n169\nEach Monday, a major software release that introduces instability (9 bugs) is\nrolled out. On Tuesday through Friday, the team has an opportunity to roll out\na “bug-ﬁx” release, each of which ﬁxes three bugs. Graph these scenarios:\n(a) No bug-ﬁx releases\n(b) Two bug-ﬁx releases after every major release\n(c) Three bug-ﬁx releases after every major release\n(d) Four bug-ﬁx releases after every major release\n(e) No bug-ﬁx release after odd releases, ﬁve bug-ﬁx releases after even\nreleases\n5. What do you observe about the graphs from Exercise 4?\n6. For a service you provide or have experience with, who are the stakeholders?\nWhich interactions did you or your team have with them?\n7. What are some of the ways operations work can be organized? How does this\ncompare to how your current team is organized?\n8. For a service you are involved with, give examples of work whose source is\nlife-cycle management, interacting with stakeholders, and process improve-\nment and automation.\n9. For a service you are involved with, give examples of emergency issues,\nnormal requests, and project work.\n\n\nThis page intentionally left blank \n\n\nChapter 8\nDevOps Culture\nThe opposite of DevOps\nis despair.\n—Gene Kim\nThis chapter is about the culture and set of practices known as “DevOps.” In\na DevOps organization, software developers and operational engineers work\ntogether as one team that shares responsibility for a web site or service. This is in\ncontrast to organizations where developers and operational personnel work inde-\npendently and often with conﬂicting goals. DevOps is the modern way to run web\nservices.\nDevOps combines some cultural and attitude shifts with some common-sense\nprocesses. Originally based on applying Agile methodology to operations, the\nresult is a streamlined set of principles and processes that can create reliable\nservices.\nAppendix B will make the case that cloud or distributed computing was the\ninevitable result of the economics of hardware. DevOps is the inevitable result of\nneeding to do efﬁcient operations in such an environment.\nIf hardware and software are sufﬁciently fault tolerant, the remaining prob-\nlems are human. The seminal paper “Why Do Internet Services Fail, and What\nCan Be Done about It?” by Oppenheimer et al. (2003) raised awareness that if web\nservices are to be a success in the future, operational aspects must improve:\nWe ﬁnd that (1) operator error is the largest single cause of failures in two of the three\nservices, (2) operator errors often take a long time to repair, (3) conﬁguration errors\nare the largest category of operator errors, (4) failures in custom-written front-end\nsoftware are signiﬁcant, and (5) more extensive online testing and more thoroughly\nexposing and detecting component failures would reduce failure rates in at least one\nservice.\n171\n\n\n172\nChapter 8\nDevOps Culture\nIn other words, technology has become so reliable that the remaining problems\nare in the processes used to manage it. We need better practices.\nThe canonical way to improve large-scale operations is through “quality man-\nagement” techniques such as W. Edwards Deming’s “Shewhart cycle” (Deming\n2000) or “Lean Manufacturing” (Spear & Bowen 1999). DevOps’s key principles\nare an application of these principles to web system administration. The book The\nPhoenix Project (Kim, Behr & Spafford 2013) explains these principles in the form\nof a ﬁctional story about a team that learns these principles as they reform a failing\nIT organization.\n8.1 What Is DevOps?\nDevOps is a combination of culture and practices—system administrators, soft-\nware developers, and web operations staff all contribute to the DevOps environ-\nment. With DevOps, sysadmins and developers share responsibility for a service\nand its availability. DevOps aligns the priorities of developers (dev) and sys-\ntem administrators or operations staff (ops) by making them both responsible for\nuptime. DevOps also brings all of the various environments, from development\nthrough test and production, under software version management and control.\nAt its most fundamental level, DevOps is about breaking down silos and remov-\ning bottlenecks and risks that screw up an organization’s Development to Opera-\ntions delivery lifecycle. The goal is to enable change to ﬂow quickly and reliably\nfrom speciﬁcation through to running features in a customer-facing environment.\n(Edwards 2012)\nDevOps is an emerging ﬁeld in operations. The practice of DevOps typically\nappears in web application and cloud environments, but its inﬂuence is spreading\nto all parts of all industries.\nDevOps is about improving operations. Theo Schlossnagle (2011) says\nDevOps is “the operationalism of the world.” Increasingly companies are putting\na greater importance on the operational part of their business. This is because of\nan increasing trend to be concerned with the total cost of ownership (TCO) of a\nproject, not just the initial purchase price, as well as increasing pressure to achieve\nhigher reliability and velocity of change. The ability to make changes is required\nto improve efﬁciency and to introduce new features and innovations. While tra-\nditionally change has been seen as a potential destabilizer, DevOps shows that\ninfrastructure change can be done rapidly and frequently in a way that increases\noverall stability.\nDevOps is not a job title; you cannot hire a “DevOp.” It is not a product;\nyou cannot purchase “DevOps software.” There are teams and organizations that\n\n\n8.1\nWhat Is DevOps?\n173\nexhibit DevOps culture and practices. Many of the practices are aided by one\nsoftware package or another. But there is no box you can purchase, press the\nDevOps button, and magically “have” DevOps. Adam Jacob’s seminal “Choose\nYour Own Adventure” talk at Velocity 2010 (Jacob 2010) makes the case that\nDevOps is not a job description, but rather an inclusive movement that codiﬁes\na culture. In this culture everyone involved knows how the entire system works,\nand everyone is clear about the underlying business value they bring to the table.\nAs a result availability becomes the problem for the entire organization, not just\nfor the system administrators.\nDevOps is not just about developers and system administrators. In his blog\npost “DevOps is not a technology problem. DevOps is a business problem,” Damon\nEdwards (2010) emphasizes that DevOps is about collaboration and optimization\nacross the whole organization. DevOps expands to help the process from idea to\ncustomer. It isn’t just about leveraging cool new tools. In fact, it’s not just about\nsoftware.\nThe organizational changes involved in creating a DevOps environment are\nbest understood in contrast to the traditional software development approach. The\nDevOps approach evolved because of the drawbacks of such methods when devel-\noping custom web applications or cloud service offerings, and the need to meet the\nhigher availability requirements of these environments.\n8.1.1 The Traditional Approach\nFor software packages sold in shrink-wrapped packages at computer stores or\ndownloaded over the Internet, the developer is ﬁnished when the software is\ncomplete and ships. Operational concerns directly affect only the customer; the\ndeveloper is far removed from the operations. At best, operational problems may\nbe fed back to the developer in the form of bug reports or requests for enhancement.\nBut developers are not directly affected by operational issues caused by their code.\nTraditional software development uses the waterfall methodology, where\neach step—gather requirements, design, implement, test, verify, and deploy—is\ndone by a different team, each in isolation from the other steps. Each step (team)\nproduces a deliverable to be handed off to the next step (team).\nThis is called “waterfall development” because the steps look like a cascading\nwaterfall (see Figure 8.1). Information ﬂows down, like the water.\nIt is impossible to understand the operational requirements of a system until\nat least the design is complete, or in many cases until it is deployed and in active\nuse. Therefore the operational requirements cannot be taken into account in the\nrequirements gathering stage, after which the features are “locked.” Thus the result\nof this approach is that operational requirements are not considered until it is too\nlate to do anything about them.\n\n\n174\nChapter 8\nDevOps Culture\nFigure 8.1: The waterfall methodology: information ﬂows down. Unidirectional\ninformation ﬂows are the antithesis of DevOps.\nIn the waterfall method, system administrators are involved only in deploy-\ning the software, and thereafter are solely responsible for operations and meeting\nuptime requirements. System administrators have very little chance of inﬂuencing\nthe development of the software to better meet their needs. In many cases they\nhave no direct contact with the software developers.\nCompanies that have a business model based on the web, or a signiﬁcant web\npresence, develop their own custom software. Traditionally there was very little\ninteraction between software developers and system administrators even if they\nworked for the same company. They worked in “silos,” with each group unaware\nof the concerns of the other, and neither side seeing the “big picture.” In an orga-\nnizational chart, their hierarchies might meet only at the CEO level. The software\ndevelopers continued to develop software in isolation, without a motivating sense\nof its future use, and the sysadmins continued to struggle to meet high availability\nrequirements with buggy software.\nIn such a situation, operations is generally improved through ad hoc solutions,\nby working around problems and creating optimizations that are limited in scope.\nExcellence in operational efﬁciency, performance, and uptime is hindered by this\n\n\n8.1\nWhat Is DevOps?\n175\napproach. Alas, the waterfall approach is from a time when we didn’t know any\nbetter.1\n8.1.2 The DevOps Approach\nCompanies with a business model based on the web found that traditional soft-\nware development practices did not work well for meeting very high availability\nrequirements. Operational concerns are key for high availability, so these compa-\nnies needed a new approach with tightly coupled development and operations.\nA new set of practices evolved as a result, and the term “DevOps” was coined to\ndescribe them. (See Appendix B for a historical perspective.)\nWeb-based companies typically introduce new features much more frequently\nthan those that sell packaged software. Since it is so easy for end users to switch\nfrom one web search engine to another, for example, these companies need to keep\nimproving their products to maintain their customer base. Also, web companies\nrelease more often because they can—they don’t have to manufacture physical\nmedia and distribute it to a customer. With the traditional packaged software\napproach, each new release is viewed as having a destabilizing inﬂuence—it is a\nsource of new, unknown bugs.\nIn a DevOps environment, developers and sysadmins share the responsibility\nfor meeting uptime requirements, so much so that both share oncall duties. Devel-\nopers have a vested interest in making sure that their software can meet the high\navailability requirements of the site. Developers collaborate in creating operational\nstrategies, and operations staff work closely with developers to provide implemen-\ntation and development input. Development and operations are both handled by\na single team, with developers and sysadmins participating in all stages and being\njointly responsible for the ﬁnal result. The development cycle should be a seam-\nless set of procedures and processes that result in a ﬁnished product—the service.\nThere is no concept of “them,” as in “hand off to them”; there is only “us”—the\nteam working on the product. Team members are largely generalists with deep\nspecialties.\nMost DevOps organizations are focused on clear business objectives such as\nscale, efﬁciency, and high uptime. By emphasizing people and process over ad hoc\ntool use, DevOps allows tight alignment of operations with business needs and\nthus with customer needs.\n1.\nOr did we? Royce’s 1970 paper, which is credited with “inventing” the model, actually identiﬁes it\nso Royce can criticize it and suggest improvements. He wrote it is “risky and invites failure” because\n“design iterations are never conﬁned to the successive step.” What Royce suggests as an alternative\nis similar to what we now call Agile. Sadly, multiple generations of software developers have had to\nsuffer through waterfall projects thanks to people who, we can only assume, didn’t read the entire paper\n(Pfeiffer 2012).\n\n\n176\nChapter 8\nDevOps Culture\nBy mandating that operations, development, and business departments work\ntogether, the operations process in a DevOps environment becomes a shared\nresponsibility that can respond more quickly and efﬁciently to needs of the service\nbeing operated. The result of the DevOps approach is higher uptime and lower\noperational costs.\n8.2 The Three Ways of DevOps\n“The Three Ways of DevOps” is a strategy for improving operations. It describes\nthe values and philosophies that frame the processes, procedures, and practices of\nDevOps. The Three Ways strategy was popularized by Kim et al.’s (2013) book The\nPhoenix Project. It borrows from “Lean Manufacturing” (Spear & Bowen 1999) and\nthe Toyota Production System’s Kaizen improvement model.\n8.2.1 The First Way: Workflow\nWorkﬂow looks at getting the process correct from beginning to end and improv-\ning the speed at which the process can be done. The process is a value stream—it\nprovides value to the business. The speed is referred to as ﬂow rate or just simply\nﬂow.\nIf the steps in the process are listed on a timeline, one can think of this as\nimproving the process as it moves from left to right. On the left is the business\n(development) and on the right is the customer (operations).\nFor example, a software release process has multiple stages: code is committed\nto a code repository, unit-tested, packaged, integration-tested, and deployed into\nproduction. To put an emphasis on getting the process correct from start to end:\n• Ensure each step is done in a repeatable way. Haphazard and ad hoc steps\nare replaced with repeatable processes.\n• Never pass defects to the next step. Testing is done as early as possible rather\nthan only on the ﬁnal product. Each step has validation or quality assurance\nchecks.\n• Ensure no local optimizations degrade global performance. For example, it\nmight be faster to not package the software but instead have each step pull\nthe software from the source repository. This saves time for the developers\nbecause it eliminates a step for them. At the same time, it introduces uncer-\ntainty that the remaining steps will all be working with the same exact bits,\ncausing confusion and increasing errors. Therefore it is a global regression and\nwe would not do it.\n• Increase the flow of work. Now that the steps are done in a repeatable\nway, the process can be analyzed and improved. For example, steps could\n\n\n8.2\nThe Three Ways of DevOps\n177\nbe automated to improve speed. Alternatively there may be steps where work\nis redone multiple times; the duplicate work can be eliminated.\n8.2.2 The Second Way: Improve Feedback\nA feedback loop is established when information (a complaint or request) is com-\nmunicated upstream or downstream. Amplifying feedback loops means making\nsure that what is learned while going from left (dev) to right (ops) is communicated\nback to the left and through the system again. Feedback (information about prob-\nlems, concerns, or potential improvements) is made visible rather than hidden. As\nwe move from left to right, we learn things; if the lessons learned are thrown away\nat the end, we have missed an opportunity to improve the system. Conversely, if\nwhat we learn is ampliﬁed and made visible, it can be used to improve the system.\nContinuing our software release example, to put an emphasis on amplifying\nfeedback loops:\n• Understand and respond to all customers, internal and external. Each step is\na customer of the previous steps in addition to the obvious “customer” at the\nend of the process. Understanding the customer means understanding what\nthe subsequent steps need. Responding means there is a way for the customer\nto communicate and a way to assure that the request is responded to.\n• Shorten feedback loops. Shortening a feedback loop means making the com-\nmunication as direct as possible. The more stages a message must pass through\nto communicate, the less effective it will be. If the feedback is given to a\nmanager, who types it up and presents it to a vice president, who commu-\nnicates it down to a manager, who tells an engineer, you know you have too\nmany steps. The loop is as short as possible if the person who experienced\nthe problem is able to directly communicate it to the person who can ﬁx the\nproblem.\n• Amplify all feedback. The opposite would be someone noticing a problem\nand muddling through it with their own workaround. The person may think\nhe or she is being a hero for working around the problem, but actually the indi-\nvidual is hiding the problem and preventing it from being ﬁxed. Amplifying\nfeedback makes the issue more visible. It can be as simple as ﬁling a bug report\nor as dramatic as stopping the process until a management decision is made\nwith regard to how to proceed. When all feedback is brought to the surface,\nthen we have the most information available to improve a process.\n• Embed knowledge where it is needed. Specialized knowledge such as con-\nﬁguration information or business requirements is “embedded” in the process\nthrough the use of appropriate documentation and managed via source code\ncontrol. As you move from left to right in the process, the details of what is\n",
      "page_number": 197
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 208-215)",
      "start_page": 208,
      "end_page": 215,
      "detection_method": "topic_boundary",
      "content": "178\nChapter 8\nDevOps Culture\nneeded are available at every stage and do not require going outside of the\nloop to acquire them.\n8.2.3 The Third Way: Continual Experimentation and\nLearning\nThe third way involves creating a culture where everyone is encouraged to try new\nthings. This is a requirement for innovation to happen. In the third way, every-\none understands two things: (1) that we learn from the failures that happen when\nwe experiment and take risks and (2) that to master a skill requires repetition and\npractice.\nIn our software release example this means:\n• Rituals are created that reward risk taking. Trying new things, even when the\nnew thing fails, is rewarded at review time if valuable lessons were learned.\n• Management allocates time for projects that improve the system. The back-\nlog of “technical debt” is considered important. Resources are allocated to ﬁx\nthe bugs ﬁled when feedback is ampliﬁed. Mistakes are not repeated.\n• Faults are introduced into the system to increase resilience. Fire drills\n(discussed in Chapter 15) intentionally take down machines or networks to\nmake sure redundant systems kick in.\n• You try “crazy” or audacious things. For example, you might try to get the\nﬂow time from one week down to one day.\nWhen a team can identify its “value streams” (the processes that the business\ndepends on) and apply the Three Ways of DevOps to them, the processes don’t\njust get better—the company also values the IT team more.\n8.2.4 Small Batches Are Better\nAnother principle of DevOps is that small batches are better.\nSmall batches means doing a lot of small releases with a few features rather\nthan a small number of large releases with lots of features. It’s very risky to do large,\ninfrequent releases. The abundance of new features makes it difﬁcult to home in\non bugs in the code. Features may interfere with each other, creating new bugs.\nTo lower overall risk, it’s better to do many small releases containing only a few\nfeatures each.\nThe ﬁrst beneﬁt of this pattern is that each new release is smaller, enabling\nyou to isolate bugs more easily. The second beneﬁt is that code latency is reduced.\nCode latency is how fast code gets from the ﬁrst check-in to production, where it\ncan be making money for you. From a ﬁnancial standpoint, the code going into\n\n\n8.2\nThe Three Ways of DevOps\n179\nproduction sooner means the code can be generating return on investment (ROI)\nsooner. Lastly, small batches mean the process is done over many iterations. This\nmeans getting more practice at it, so you have more opportunities to get better at\nthe process. This reduces risk.\nVelocity is how many times you ship in a month. High velocity and low latency\nare realized through releasing small batches.\nThe small batches principle is counter-intuitive because there is a human ten-\ndency to avoid risky behavior. Deploying software in production involves risk;\ntherefore businesses traditionally minimize the frequency of deployments. While\nthis makes them feel better, they actually are shooting themselves in the foot\nbecause the deployments that are done are bigger and riskier, and the team doing\nthem is out of practice by the time the next one rolls around.\nThis principle applies to deployments and any other process that involves\nmaking frequent changes.\n8.2.5 Adopting the Strategies\nThe ﬁrst step in adopting the Three Ways is to identify the team’s value\nstreams—processes done for the business, or requested by the business.\nGo through each process several times until it can be done from beginning to\nend without failure. It doesn’t have to be optimal, but each step needs to be clearly\ndeﬁned so that it can be done in a repeatable fashion. That is, a reasonably well-\ntrained person should be able to do the step and the result will be the same as the\nresult from another reasonably trained person. Now the process is deﬁned.\nOnce the process is deﬁned, amplify the feedback loops. That is, make sure\neach step has a way to raise the visibility of problems so that they are worked on,\nnot ignored. Collect measurements on the length, frequency, and failure rate of the\nsteps. Make this data available to all involved.\nThis feedback is used to optimize the process. Find the steps that are the most\nerror prone, unreliable, or slow. Replace them, improve them, or eliminate them.\nThe two biggest inefﬁciencies are rework (ﬁxing mistakes) and redundant work\n(duplicate effort that can be consolidated).\nEvery process has a bottleneck—a place where work is delayed while it waits\non other dependencies. The most beneﬁcial place to put energy into improvement\nis at the bottleneck. In fact, optimizations anywhere else are wasted energy. Above\nthe bottleneck, incomplete work accumulates. Below the bottleneck, workers are\nstarved for things to do. Optimizing steps above the bottleneck simply makes more\nwork accumulate. Optimizing steps below the bottleneck simply improves steps\nthat are underutilized. Therefore ﬁxing the bottleneck is the only logical thing to do.\nMaking all of this happen requires a culture of innovation and a willingness\nto take risks. Risk must be rewarded and failure embraced. In fact, once the Three\nWays of DevOps have been used to make the process smooth and optimized,\n\n\n180\nChapter 8\nDevOps Culture\nyou should introduce defects into the system to verify that they are detected and\nhandled. By embracing failure this way, we go from optimized to resilient.\n8.3 History of DevOps\nThe term “DevOps” was coined by Patrick Debois in 2008. Debois noticed that\nsome sites had evolved the practice of system administration into something fun-\ndamentally different. That is, they had independently reached the conclusion that\nweb sites could be better run when development and operations were done in col-\nlaboration. Debois thought there would be value in getting these people together\nto share what they had learned. He started a series of mini-conferences called\n“DevOps Days” starting in 2009 in Belgium. The name came from the concept of\nbringing developers (dev) and operations people (ops) together.\nDevOps Days was a big success and helped popularize the term “DevOps.”\nThe conversations continued on mailing lists and blogs. In May 2010, John Willis\nand Damon Edwards started the DevOps Cafe Podcast, which soon became a clear-\ninghouse for DevOps ideas and discussion. The hashtag “#devops” arose as a way\nfor DevOps followers to identify themselves on Twitter, which was a relatively\nnew service at the time. The 2011 USENIX LISA Conference (Large Installation\nSystem Administration) selected DevOps as its theme and since then has evolved\nto incorporate a DevOps focus.\n8.3.1 Evolution\nSome practitioners say that DevOps is a logical evolution of having sysadmins and\ndevelopers participating in an Agile development cycle together and using Agile\ntechniques for system work. While the use of Agile tools is common in DevOps,\nAgile is merely one of many ways to apply DevOps principles. Techniques such\nas pair programming or scrum teams are not required to create a DevOps envi-\nronment. However, adherence to some of the basic Agile principles is deﬁnitely\nrequired.\nOther practitioners say that DevOps is the logical evolution of developers\ndoing system administration themselves due to the popularity of Amazon AWS\nand (later) similar services. In other words, it is developers reinventing system\nadministration. In the past, setting up a new machine required the skill and train-\ning of a sysadmin. Now developers were allocating virtual machines using API\ncalls. Without the need for full-time sysadmins, developers were learning more\nand more system skills and bringing with them their penchant to automate tasks.\nBy making deployment and test coding part of the development cycle, they created\na new emphasis on repeatability that led to many of the techniques discussed in\nthis chapter.\n\n\n8.4\nDevOps Values and Principles\n181\nOther practitioners counter by saying that some sysadmins have always had\nan emphasis on automation, though outside of web environments management\ndid not value such skills. From their perspective, DevOps was driven by sysad-\nmins who concentrated on their coding skills and began collaborating with the\ndevelopers on deployment and code testing. Taking these steps into the develop-\nment cycle led to closer ties with developers and working as a tight-knit group to\naccomplish the shared goal of increased uptime and bug-free deploys. More cyn-\nically, DevOps can be viewed as system administration reinvented by sysadmins\nwho ﬁnally had management support to do it the right way.\n8.3.2 Site Reliability Engineering\nAround the same time, companies such as Google started being more open about\ntheir internet sysadmin practices. Google had evolved system administration into\nthe concept of a Site Reliability Engineer (SRE) by recognizing that all functions\nof system administration, from capacity planning to security, were crucial to the\nreliability of a site. Since 2005, Google’s SRE model has organized the company’s\ndevelopers and operational engineers to share responsibility for reliability and per-\nformance. The SRE model can be thought of as DevOps at large scale: how do you\nempower 10,000 developers and 1000 SREs to work together? First, each product or\ninfrastructure component has a small team responsible for it. For critical and highly\nvisible systems, developers and SREs work together in an arrangement that mirrors\nthe DevOps model. Unfortunately, there are not enough SREs to go around. There-\nfore the vast majority of these teams consist of developers using tools developed by\nthe SREs. The tools are engineered to make operations self-service for developers.\nThis empowers developers to do their own operations. The SREs build tools that\nare speciﬁcally engineered to make it easy to achieve high-quality results without\nadvanced knowledge.\nDevOps is rapidly expanding from a niche technique for running web sites\nto something that can also be applied to enterprise and industrial system admin-\nistration. There is nothing uniquely web-centric about DevOps. Marc Andreessen\nfamously said, “Software eats the world” (Anderson 2012). As this happens, all\nfacets of society will require well-run operations. Thus, DevOps will be applied to\nall facets of computing.\n8.4 DevOps Values and Principles\nDevOps can be divided into roughly four main areas of practice (Kartar 2010):\n• Relationships\n• Integration\n\n\n182\nChapter 8\nDevOps Culture\n• Automation\n• Continuous improvement\n8.4.1 Relationships\nIn a traditional environment, the tools and scripts are seen as the primary focus of\noperational maintenance. DevOps gives more weight to the relationships among\nthe teams and the various roles in the organization. Developers, release managers,\nsysadmins, and managers—all need to be in close coordination to achieve the\nshared goal of highly reliable and continuously improving services.\nRelationships are so important in a DevOps environment that a common\nmotto is “People over process over tools.” Once the right people are performing\nthe right process consistently, only then does one create a tool to automate the\nfunction. One of the key deﬁning principles of DevOps is the focus on people and\nprocess over writing a script and then ﬁguring out who should run it and when.\n8.4.2 Integration\nPart of breaking down silos is ensuring that processes are integrated across teams.\nRather than seeing operational duties as merely following a script, in a DevOps\nenvironment one views them as end-to-end processes that combine tools and\ndata with people processes such as peer reviews or coordination meetings. Pro-\ncesses must be linked across domains of responsibility to deliver end-to-end\nfunctionality.\nIntegration of the communities responsible for different parts of the service\noperation is also a given for DevOps. A quick way to assess the DevOps culture in\nyour environment might be to ask sysadmins who they have lunch with regularly.\nIf the answer is “Mostly sysadmins,” and hardly ever folks from software devel-\nopment, web operations, networking, or security, this is a sign that integration of\nteams has not been achieved.\n8.4.3 Automation\nUnder the auspices of automation, DevOps strives for simplicity and repeatability.\nConﬁgurations and scripts are handled as source code and kept under version con-\ntrol. Building and management of the source code are scripted to the fullest extent\npossible once the entire process is understood.\nSimplicity increases the efﬁciency and speed of communication and avoids\nconfusion. It also saves time in training, documentation, and support. The goal is\nto design simple, repeatable, reusable solutions.\n\n\n8.4\nDevOps Values and Principles\n183\n8.4.4 Continuous Improvement\nEach time a process is carried out, the goal is to make it dependably repeatable\nand more functional. For example, every time there is a failure, tests are added to\nthe release process to detect that failure mode and prevent another release with\nthe same failure from being passed to the next step. Another example might be\nimproving a process that needs occasional manual intervention by handling more\nedge cases in the tool, until eventually manual intervention is no longer required.\nBy taking an end-to-end view, we often ﬁnd opportunities to eliminate pro-\ncesses and tools, thus simplifying the system. Problems are ﬁxed by looking for root\ncauses rather than making local optimizations that degrade global performance.\nThe mindset required is eloquently summed up by Kartar (2010):\nTreat your processes like applications and build error handling into them. You can’t\npredict every ... pitfall ... but you can ensure that if you hit one your process isn’t\nderailed.\n8.4.5 Common Nontechnical DevOps Practices\nDevOps includes many nontechnical practices that fall under the DevOps\numbrella. Not all DevOps organizations use all of these techniques. In fact, it is\nimportant to pick and choose among them, using the techniques that are needed\nrather than blindly following all the practices for completeness. These are the\n“people processes”; the more technical practices will be covered in the next section.\n• Early Collaboration and Involvement: Ops staff are included in development\nplanning meetings, and developers have full access to ops monitoring. Key\nissues such as architecting for scalability are jointly developed in the planning\nstage. (See Chapter 5.)\n• New Features Review: Ops staff participate in and guide development toward\nbest practices for operability during design time, not as an after-thought. Key\nmonitoring indicators for services are deﬁned through collaboration. Deploy-\nment details are sketched out so development of deployment code and tests\nare part of the main development effort. (See Chapter 2.)\n• Shared Oncall Responsibilities: These responsibilities include not only pager\nduties shared between developers and Ops staff, but also shared review of\noncall trends—for example, a weekly meeting to review SLA compliance and\nany outages. Developers have full access to all monitoring output, and Ops\nstaff have full access to all build/deploy output. That way everyone is fully\nempowered to research any issues that come up while oncall or during a\nfailure analysis.\n\n\n184\nChapter 8\nDevOps Culture\n• Postmortem Process: In addition to a regular stand-up meeting to review out-\nages and trends, there should be a thorough postmortem or failure analysis\ndone for every outage. Recurring patterns of minor failures can point to a\nlarger gap in process. Findings of a postmortem—speciﬁcally, tasks needed\nto correct issues—should be added to the current development backlog and\nprioritized accordingly.\n• Game Day Exercises: Sometimes known as “ﬁre drills,” these are deliberate\nattempts to test failover and redundancy by triggering service disruption in a\nplanned fashion. Teams of people are standing by to ensure that the “right\nthing” happens, and to ﬁx things manually if it does not. Only by induc-\ning failure can you actually test what will happen when service components\nfail. A simple example of a game-day exercise is rebooting randomly selected\nmachines periodically to make sure all failover systems function properly.\n• Error Budgets: Striving for perfection discourages innovation, but too much\ninnovation means taking on too much risk. A system like Google’s Error\nBudgets brings the two into equilibrium. A certain amount of downtime is\npermitted each month (the budget). Until the budget is exhausted, develop-\ners may do as many releases as they wish. Once the budget is exhausted, they\nmay do only emergency security ﬁxes for the rest of the month. To conserve\nthe Error Budgets, they can dedicate more time for testing and building frame-\nworks that assure successful releases. This aligns the priorities of operations\nand developers and helps them work together better. See Section 19.4 for a full\ndescription.\n8.4.6 Common Technical DevOps Practices\nDevOps is, fundamentally, a structural and organizational paradigm. However, to\nmeet the goals of DevOps, a number of technical practices have been adopted or\ndeveloped. Again, not all of them are used by every DevOps organization. These\npractices are tools in your toolbox, and you should choose those that will best serve\nyour situation.\n• Same Development and Operations Toolchain: Development and operations\ncan best speak the same language by using the same tools wherever possible.\nThis can be as simple as using the same bug-tracking system for both develop-\nment and operations/deployment issues. Another example is having a uniﬁed\nsource code management system that stores not just the product’s source code,\nbut also the source code of operational tools and system conﬁgurations.\n• Consistent Software Development Life Cycle (SDLC): Bringing both the\napplication itself and the deployment/operations code together into the same\n\n\n8.4\nDevOps Values and Principles\n185\nSDLC is key to keeping the two in sync. The “throw it over the wall to deploy-\nment” model is anathema in DevOps, where development and operations are\ntightly coupled. The deployment tools are developed and tested in lockstep\nwith the applications themselves, following a shared release cycle.\n• Managed Configuration and Automation: The conﬁguration ﬁles of all appli-\ncations that are required for the service are kept in source code control and are\nsubject to the same change management as the rest of the code. The same is\ntrue for all automation scripts.\n• Infrastructure as Code: With a software-deﬁned datacenter (i.e., virtual\nmachines), you can keep a description of the entire infrastructure as code that\ncan be maintained under revision control. Infrastructure as code is further\ndescribed in Section 10.6.\n• Automated Provisioning and Deployment: Every step of the deployment\nprocess is automated and/or scripted so that one can trigger a build that will\ngo all the way through self-test to a deployment, or can trigger a deployment\nvia a separate build command.\n• Artifact-Scripted Database Changes: Rather than manual manipulation of\ndatabase schema, changes to databases are also treated as code. They are\nscripted, tested, versioned, and released into staging environments.\n• Automated Build and Release: The output of a build cycle is a valid set of\napplication and deployment objects that can be deployed to a staged environ-\nment. Builds have makeﬁles or other conﬁguration ﬁles that treat the build as\na series of dependencies and contracts to fulﬁll, and can be triggered by check-\nins or by speciﬁc command. Assembling stages of a build by hand is counter\nto repeatability and ease of operation.\n• Release Vehicle Packaging: As noted earlier, the build cycle creates packaging\nfor the application to facilitate its deployment. The end product of a build does\nnot require by-hand packaging to prepare it for deployment, nor is software\ndeployed to live systems via checkout from a repository or compiled on each\nhost before use.\n• Abstracted Administration: Abstracted administration describes system\nadministration tasks at a high level and lets automation decide the right\nsteps to perform for a given operating system. Thus we might have a con-\nﬁguration ﬁle to provision a new user that says “create user” rather than\nthe steps required for Linux (“append this line to /etc/passwd, this line to\n/etc/shadow, and this line to /etc/group”) or Windows (“create the user in\nActiveDirectory”). By doing the initial setup work with the tools, we simplify\nthe interface between goals and conﬁgurations. Some commonly used tools in\nthis area include CFEngine, Puppet, and Chef.\n",
      "page_number": 208
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 216-224)",
      "start_page": 216,
      "end_page": 224,
      "detection_method": "topic_boundary",
      "content": "186\nChapter 8\nDevOps Culture\n8.4.7 Release Engineering DevOps Practices\nCertain release engineering practices have become closely related to DevOps prac-\ntices. Release engineering is the process of taking software in source form, building\nit, packaging it, testing it, and deploying it into the ﬁeld.\nWhile not DevOps practices in themselves, these development practices have\na great deal to offer in achieving operational efﬁciency (DevOps-Toolchain 2010).\nEach of these practices is discussed in more detail in Chapters 9, 10 and 11.\n• Continuous Build: With each change, attempt to compile the code base and\ngenerate the packaged software. This detects build-related problems as soon\nas possible.\n• Continuous Test: The software is tested in an automated fashion with each\nchange to the code base. This prevents problems from becoming embedded in\nthe system.\n• Automated Deployment: The process of deploying the software for use in test\nand live environments is automated.\n• Continuous Deployment: With fully automated build, test, and deployment,\nthe decision whether to deploy a particular release is also automated. Multiple\nreleases are deployed in the ﬁeld each day.\n• Automated Provisioning: Additional resources, such as CPU, storage, mem-\nory, and bandwidth, are allocated based on a predictive model. As the system\ndetects that more resources are needed, they are allocated for use by the\nsystem.\n8.5 Converting to DevOps\nBefore implementing any of these recommendations, developers and operations\nneed to open a dialogue. Building bridges between the groups needs to start by\nforming collegial connections. This is often best done away from the ofﬁce, prefer-\nably over a beer or other beverage. That is when operations and developers really\nstart talking to each other, sharing their perspectives, and ﬁnding common ground\nthat make adopting DevOps practices happen. In an interview on the DevOps Cafe\npodcast, Jesse Robbins noted that spending $50 on fries and drinks may be the best\ninvestment some companies ever make (Willis & Edwards 2011).\nWhen adopting DevOps principles in a traditional, non-DevOps organization,\nit is important to start slowly. Adopt a few new practices at ﬁrst and add more\npractices as they get buy-in from the team. There are three basic phases involved\nin this type of conversion.\nFirst, make operations feedback available to the overall project team. This\ncan take several forms. Most commonly, organizations make monitoring available\n\n\n8.5\nConverting to DevOps\n187\nto developers and ask them to do joint root causes analysis of issues or failures,\nidentify recurring problems, and collaborate on solutions.\nSecond, begin to embed product knowledge into operations. Invite developers\nto key operations meetings on deployment and maintenance, and set escalation\npaths that involve developers being reachable after hours.\nThird, enable operations knowledge to be available during all project phases.\nThis involves operations being included in daily or weekly status meetings, being\ninvolved in prioritization of the product backlog tasks, and being a full partner in\nplanning meetings.\n8.5.1 Getting Started\nTo start a DevOps relationship, you must ﬁrst get off the computer and begin\nface-to-face discussions. Where do you start? Begin with development, product\nmanagers, or other members of the product team for the product(s) you support.\nIt is best to choose someone approachable, with whom you may already have\nsome rapport. You can arrange a meeting or simply go get coffee some afternoon.\nYour initial conversation should be about mutual problems that can be solved.\nAs part of the conversation, explain the improvements that could come from closer\ncollaboration, such as improved release efﬁciency or better operations response to\ndevelopers’ needs.\nAs you discuss problems to solve, you will ﬁnd one that will be a good starting\npoint for a joint DevOps project. Choose something that has obvious beneﬁts to the\ndevelopment team rather than something that is primarily operations focused. It\nis best to describe the project in terms of mutual beneﬁts.\nAs part of the collaboration on this project, get in the habit of holding reg-\nular meetings with the development and product teams. Attend their planning\nmeetings, and invite them to yours.\nAs you successfully complete your starter project, identify another project for\ncollaboration. Good candidates are processes that affect both development and\noperations, such as roll-outs or build toolchains. Involving multiple stakehold-\ners from development, operations, and the product team is a good way to build\nrelationships.\nOnce again, one easy way to measure your DevOps success is to ask, “Who do\nyou go to lunch with?” If you’re routinely going to lunch with folks from develop-\nment, networking, release, or similar groups, chances are very good that you are\ndoing DevOps.\n8.5.2 DevOps at the Business Level\nThe next stage of conversion to a DevOps environment is getting your manage-\nment to buy into the DevOps philosophy. A true DevOps environment often\n\n\n188\nChapter 8\nDevOps Culture\ninvolves changes to the organizational chart that break down the barriers between\ndevelopment and operations. The organizational structure needs to foster a close\nrelationship between development and operations. Ideally, get development and\noperations under one management team, the same vice president, or something\nsimilar. Also try to colocate the groups so that they are near each other, or at least\nin the same building or time zone. This kind of change can open up a whole new\nlevel of functionality. Getting buy-in is fairly difﬁcult. Management needs to have\nthe value explained in business terms.\nDevOps doesn’t stop with collaboration between developers and operations\nstaff. It can be useful up and down the entire organizational chain. Recently one\nof the authors witnessed an internal project where developers, operations staff,\nproduct management, and the legal department worked side by side to create a\nsolution that would properly handle a tight set of constraints. The staff from the\nlegal department were amazed at the level of collaboration and commitment that\ncould be focused on the problem. Buying an expensive third-party system that\nwould have needed conﬁguration and outside management was avoided.\n.\nDevOps: Not Just for the Web\nA Practical Approach to Large-Scale Agile Development: How HP Transformed\nHP LaserJet FutureSmart Firmware by Gruver, Young, and Fulghum (2012)\ndescribes applying DevOps to the creation of HP LaserJet software. The result\nwas that developers spent less time doing manual testing, which gave them\nmore time to develop new features. Their success story is summarized in\nEpisode 33 of the DevOps Cafe Podcast (Willis, Edwards & Humble 2012).\n8.6 Agile and Continuous Delivery\nDevOps is a natural outgrowth of the software development methodology known\nas “Agile” and the practices called “continuous delivery.” While this book is not\nabout either of those topics, a brief look at both Agile and continuous delivery is\nhelpful to show the origins of many DevOps practices. The principles involved are\ndirectly transferable to DevOps practices and serve as a strong foundation for a\nDevOps mindset.\n8.6.1 What Is Agile?\nAgile is a collection of software development principles that originated in an\nunusual summit of representatives from various nontraditional software practices\n\n\n8.6\nAgile and Continuous Delivery\n189\nsuch as Extreme Programming, Scrum, and Pragmatic Programming, to name a\nfew. They called themselves “The Agile Alliance” and created the highly inﬂuential\n“Agile Manifesto” (Beck et al. 2001).\nThe Agile Manifesto\n• Individuals and interactions over processes and tools\n• Working code over comprehensive documentation\n• Customer collaboration over contract negotiation\n• Responding to change over following a plan\nAs a coda to the Agile Manifesto, the authors added, “While there is value in the\nitems on the right, we value the items on the left more.” Agile practices stress direct\nconnections between the business objectives and the development team. Develop-\ners work closely with product owners to build software that meets speciﬁc business\nobjectives. The waterfall method of development is bypassed in favor of collabo-\nrative methods such as pair programming, where two developers work on code\ntogether, or scrum, where a whole team commits to “sprints” of one to four weeks\nworking on a prioritized backlog of features. Plain statements called “user stories”\nprovide requirements for software development—for example, “As a bank cus-\ntomer, I want to receive an electronic bill for my credit card statement” or “As a\nphoto site user, I want to crop and edit pictures in my web browser.”\nIn Agile development, waterfall methods are also bypassed with respect to\ntesting and integration. Unit and integration tests are created with new feature\ncode, and testing is applied automatically during the build process. Often the only\ndocumentation for code releases is the user stories that provided the initial require-\nments, in strong contrast to waterfall’s functional speciﬁcations and requirements\ndocuments. The user stories come out of a prioritized backlog of feature stories\nmaintained by the product owner. By keeping a prioritized list that can change with\nbusiness needs, the agility of the development process is maintained and devel-\nopment can respond to changing business needs easily without wasted effort and\nrework.\nDevOps is the application of Agile methodology to system administration.\n8.6.2 What Is Continuous Delivery?\nContinuous delivery (CD) is a set of principles and practices for delivery of soft-\nware and updates on an ongoing basis. Software delivery means the process\nby which software goes from source code to ready-to-use installation packages.\nIncluded in this process are the building (compiling) of the software packages as\nwell as any quality testing. The process stops if the build or testing fails. CD began\n\n\n190\nChapter 8\nDevOps Culture\nas a design pattern in Extreme Programming but has since become a discipline of\nits own.\nFor example, an organization that practices CD builds the software frequently,\noften immediately after any new source code changes are detected. The change\ntriggers the build process, which, on successful completion, triggers an automated\ntesting process. On successful completion of the tests, the software packages are\nmade available for use.\nCD is different from traditional software methodologies where new software\nreleases are infrequent, perhaps yearly. In the latter approach, when the software\nrelease date is near, the packages are built, possibly involving a very manual pro-\ncess involving human intervention and perhaps ad hoc processes. The testing is a\nmixture of automation and manual testing that may last for days. If any problems\nare found, the entire process starts all over.\nContinuous delivery stresses automation, packaging, and repeatability, with a\nculture of shared responsibility for good outcomes. In continuous delivery we rec-\nognize that every step of the process is part of the ultimate delivery of the software\nand updates, and that the process is always occurring at every stage.\nSystem operations and maintenance can be thought of as a form of contin-\nuous delivery. Operations tasks are part of how software gets from a packaged\nbuild on the repository onto the system. While these principles and practices came\nfrom software development, it is easy to map them onto operations tasks and gain\nefﬁciency and reliability.\nThe eight principles of continuous delivery are codiﬁed by Humble and Farley\n(2010).\nThe Eight Principles of Continuous Delivery\n1. The process for releasing/deploying must be repeatable and reliable.\n2. Automate everything.\n3. If something is difﬁcult or painful, do it more often to improve and automate\nit.\n4. Keep everything in source control.\n5. “Done” means “released, working properly, in the hands of the end user.”\n6. Build quality in.\n7. Everybody has responsibility for the release process.\n8. Improve continuously.\nWhile most of these principles are self-explanatory, it is worthwhile expanding on\nthe third principle. Often our response to error-prone, painful, and difﬁcult tasks is\nto ﬁnd ways to do them less often. Continuous delivery says to do them more often\n\n\n8.6\nAgile and Continuous Delivery\n191\nto improve our skill (“Practice makes perfect”), ﬁx the problems in the processes,\nand automate them.\nThere are also four practices of continuous delivery.\nThe Four Practices of Continuous Delivery\n1. Build binaries only once.\n2. Use the same repeatable process for deployment to every environment.\n3. Do basic functionality tests (“smoke tests”) on your deployment (e.g., include\ndiagnostics).\n4. If anything fails, stop the line and start again.\nFor any given release cycle, binary packages are built only once. Contrast this with\nenvironments where QA builds a package and tests it, and then deployment checks\nout the same code and builds the package to be used in deployment. Not only is\nthis duplication of effort, but errors can also creep in. Multiple build requests may\nbe confused or miscommunicated, resulting in packages being built from slightly\ndifferent source code revisions each time. One team may be tempted to “slip in a\nfew small ﬁxes,” which might seem helpful but is dangerous because it means some\nnew code did not receive the full, end-to-end testing. It is difﬁcult to verify that the\nsecond time the package is built, the same exact OS release, compiler release, and\nbuild environment are used.\n.\nVersion-Controlled Builds\nAt one of Tom’s former employers, the build system checked the MD5 hash\nof the OS kernel, compiler, and a number of other ﬁles. The build tools team\nwould determine that version x.y was to be built with a speciﬁc tool-chain and\nthe build system wouldn’t let you build that version with any other toolchain.\nIf a customer ﬁve years later required a patch to version X, the company knew\nit could build the software exactly as needed with only the patch being the dif-\nferent item, not the compiler, OS, or other tool. This was particularly important\nbecause the company made software for designing chips, and chip designers\ndidn’t change toolchains once a design was started. If you started a design\nwith version X, the vendor promised it could ﬁx bugs in that speciﬁc version\nuntil your chip design was ﬁnished.\nBy automating as much as possible and creating documented processes for\nsteps that cannot be automated, we create a repeatable process for deployment.\n\n\n192\nChapter 8\nDevOps Culture\nIn a traditional software development methodology, release engineers hand-build\nbinaries for the QA environment and deploy them in an ad hoc fashion. In contin-\nuous deployment, the build is automated and each stage uses the same process for\ndeployment.\nA “smoke test” is a basic functionality test, like plugging in a device and seeing\nif it starts to smoke (it shouldn’t!). Including built-in diagnostic tests as part of your\nbuild and deployment automation gives you high conﬁdence that what you just\ndeployed meets the basic functionality requirements. The build or deploy will error\nout if the built-in tests fail, letting you know something is wrong.\nInstead of patching a failure and proceeding with patches, continuous delivery\nwants us to ﬁnd a root cause and ﬁx the problem. We then work the process again\nto verify that the problem has disappeared.\nCD produces an installation package that is tested sufﬁciently so that it can\nbe deployed into production. However, actually pushing that release into pro-\nduction is a business decision. Deployments are usually less frequent, perhaps\ndaily or weekly. There may be additional tests that the deployment team performs\nand the actual deployment itself may be complex and manual. Automating those\npractices and doing them frequently (perhaps as frequently as new packages are\navailable) constitutes continuous deployment. This approach is discussed further\nin Section 11.10.\n8.7 Summary\nIn this chapter, we examined the culture and principles of DevOps and looked at\nits historical antecedents—namely, Agile and continuous delivery. The principles\nand practices in this chapter serve as a strong foundation for a shift in culture and\nattitude in the workplace about operations and maintenance tasks. This shift brings\nmeasurable efﬁciency and increased uptime when it is strongly applied, and you\nshould explore it for yourself.\nTheo Schlossnagle (2011) describes DevOps as the natural response to “the\noperationalism of the world.” As velocity increases in companies of every type,\nthe most successful competitors will be those organizations that are “operationally\nfocused” in every business unit.\nDevOps is not about a technology. DevOps is about solving business problems\n(Edwards 2010). It starts with understanding the business needs and optimizing\nprocesses to best solve them.\nDevOps makes businesses run more smoothly and enables the people\ninvolved to work together more effectively. It gives us hope that operations can\nbe done well at any scale.\n\n\nExercises\n193\nExercises\n1. What are the fundamental principles behind DevOps?\n2. How does a waterfall environment differ from a DevOps environment?\n3. Why is DevOps not intended to be a job title?\n4. Describe the relationship between Agile and DevOps practices.\n5. Whose participation is vital to DevOps collaboration, at a minimum?\n6. Name three release engineering best practices.\n7. What are the basic steps involved in converting a process to DevOps?\n8. Describe your release process (code submission to running in production).\nIdentify the manual steps, team-to-team handoffs, and steps that are ill deﬁned\nor broken. Based on the Three Ways of DevOps, which improvements could\nbe made?\n\n\nThis page intentionally left blank \n",
      "page_number": 216
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 225-232)",
      "start_page": 225,
      "end_page": 232,
      "detection_method": "topic_boundary",
      "content": "Chapter 9\nService Delivery: The Build\nPhase\nAnd they’re certainly not showing\nany signs that they are slowing.\n—Willy Wonka\nService delivery is the technical process of how a service is created. It starts with\nsource code created by developers and ends with a service running in production.\nThe system that does all of this is called the service delivery platform. It includes\ntwo major phases: build and deploy. The build phase starts with the software\nsource code and results in installable packages. That phase is covered in this chap-\nter. The deployment phase takes those packages, readies the service infrastructure\n(machines, networks, storage, and so on), and produces the running system. The\ndeployment phase is covered in Chapter 10.\nThe service delivery ﬂow is like an assembly line. Certain work is done at\neach step. Tests are performed along the way to verify that the product is work-\ning correctly before being passed along to the next step. Defects are detected and\nmonitored.\nThe faster the entire process can run, the sooner the test results are known.\nHowever, comprehensive tests take a long time. Therefore the earlier tests should\nbe the fastest, broadest tests. As conﬁdence builds, the slower, more detailed tests\nare done. This way the most likely failures happen early on, removing the need for\nthe other tests. For example, the early stages compile the software, which is sensi-\ntive to easy-to-detect syntax errors but provides little guarantee that the software\nworks. Testing performance, security, and usability can be the most time consum-\ning. Manual tests are saved for the end, after all other failures have been detected,\nconserving manual labor.\n195\n\n\n196\nChapter 9\nService Delivery: The Build Phase\n.\nTerms to Know\nInnovate: Doing (good) things we haven’t done before.\nStakeholders: People and organizations that are seen as having an interest\nin a project’s success.\nArtifacts: Any kind of tangible by-product produced during the develop-\nment of software—source ﬁles, executables, documentation, use cases,\ndiagrams, packages, and so on.\nService Delivery Flow: The trip through the system from beginning to end;\noften shortened to flow.\nCycle Time: How frequently a ﬂow completes.\nDeployment: The process of pushing a release into production; often short-\nened to deploy.\nGate: An intentional limit that prevents ﬂow. For example, the deploy step\nis gated by whether quality assurance tests succeeded.\nRelease Candidate: The end result of the build phase. Not all release candi-\ndates are deployed.\nRelease: The successful completion of an entire ﬂow, including deploy. May\nresult in users seeing a visible change.\nA good service delivery platform takes into account the fact that a service is not\njust software, but also the infrastructure on which the service runs. This includes\nmachines with an operating system loaded and properly conﬁgured, the software\npackages installed and conﬁgured, plus network, storage, and other resources\navailable. While infrastructure can be set up manually, it is best to automate that\nprocess. Virtualization enables this kind of automation because virtual machines\ncan be manipulated via software, as can software-deﬁned networks. The more we\ntreat infrastructure as code, the more we can beneﬁt from software development\ntechniques such as revision control and testing. This automation should be treated\nlike any other software product and put through the same service delivery ﬂow as\nany application.\nWhen service delivery is done right, it provides conﬁdence, speed, and\ncontinuous improvement. Building, testing, and deployment of both an applica-\ntion and virtual infrastructure can be done in a completely automated fashion,\nwhich is streamlined, consistent, and efﬁcient. Alternatively, it can be done via\nmanual, ad hoc processes, inconsistently and inefﬁciently. It is your mission to\nachieve the former.\n\n\n9.1\nService Delivery Strategies\n197\n9.1 Service Delivery Strategies\nThere are many possible service delivery strategies. Most methodologies fall along\nthe continuum between the older waterfall methodology and the more modern\nmethodology associated with the DevOps world. We recommend the latter because\nit gets better results and encourages faster rates of innovation. It focuses on\nautomation, instrumentation, and improvement based on data.\n9.1.1 Pattern: Modern DevOps Methodology\nThe DevOps methodology divides the platform into two phases: the build phase\nand the deployment phase. The build phase is concerned with taking the source\ncode and producing installation packages. The deployment phase takes those\npackages and installs them in the environment where they are to be run. At each\nstep along the way tests are performed. If any test fails, the process is stopped.\nSource code is revised and the process starts again from the beginning.\nFigure 9.1 represents this service delivery platform. There are two primary\nﬂows. The upper ﬂow delivers the application. The lower ﬂow delivers the infra-\nstructure. The four quadrants represent the build and deployment phases of each\nﬂow.\nEach phase has a repository: The build phase uses the source repository. The\ndeployment phase uses the package repository. Each phase has a console that pro-\nvides visibility into what is going on. The application ﬂow and the infrastructure\nﬂow have the same steps and should use the same tools when possible.\nDeployments are done in one of at least two environments: the test environ-\nment and the live production environment. The service is created initially in the\ntest environment, which is not exposed to customers. In this environment, a series\nof tests are run against the release. When a release passes these tests, it becomes a\nrelease candidate. A release candidate is a version with the potential to be a ﬁnal\nproduct—that is, one that is deployed in production for customer use.\nThe software may be deployed in other environments, such as a private sand-\nbox environment where the engineering team conducts experiments too disruptive\nto attempt in the test environment. Individual developers should each have at least\none private environment for their own development needs. These environments\nare generally scaled-down versions of the larger system and may even run on the\ndevelopers’ own laptops. There may be other separate environments for other rea-\nsons. For example, there may be a demo environment used to give previews of new\nreleases to stakeholders. Alternatively, new releases may be deployed in an inter-\nmediate production environment used by “early access customers” before they are\ndeployed into the main production environment.\n\n\n198\nChapter 9\nService Delivery: The Build Phase\nFigure 9.1: The parts of the modern service delivery platform pattern. (Reprinted\nwith permission from Damon Edwards of DTO Solutions.)\nAt a minimum, every site must have separate testing and production environ-\nments, where the testing environment is built exactly like production. Developers’\nown environments are typically less well controlled, and testing in such an envi-\nronment may miss some issues. It is negligent to move a release straight from\ndeveloper testing into production, or to not have an environment for testing the\nconﬁguration management systems. There is no good reason to not invest in the\nadditional infrastructure for a proper test environment—it always pays for itself\nin increased service availability.\nWhile one may think of the build phase as the domain of developers and the\ndeployment phase as the domain of operations, this is not true in a DevOps envi-\nronment. Both groups share responsibility for the construction and use of the entire\n\n\n9.1\nService Delivery Strategies\n199\nsystem. The handoff between steps marks the ﬂow of work, not organizational\nboundaries.\n9.1.2 Anti-pattern: Waterfall Methodology\nThe waterfall methodology works differently from the modern DevOps methodol-\nogy. It is predicated on multiple phases, each controlled by a different organization.\nHandoffs not only mark the ﬂow of work, but also indicate the end of each orga-\nnization’s responsibility. The waterfall methodology was previously discussed in\nSection 8.1.1.\nThe waterfall methodology has many phases. The ﬁrst phase is controlled by\nthe development organization, which has two teams: software engineers (SWEs)\nand quality assurance (QA) engineers. The SWEs create the source code, which\nthey compile and package. The QA team tests the packages in its own environment.\nWhen the team approves the software, it is designated as a release candidate and\npassed to the next phase. The next phase is controlled by the system administration\nteam. This team uses the release candidate to build a beta environment; the beta\nenvironment is used to verify that the software works. Product management then\ngets involved and veriﬁes that the software is functioning as expected. Once the\nrelease is approved, the system administrators use it to upgrade the live production\nenvironment.\nThere are many problems with the waterfall approach. First, both QA and\noperations build their own environments, each using different methods. This\nmeans there is a duplication of effort, with each team developing overlapping tools.\nSecond, because the QA environment is built using different methods, it is not a\nvalid test of how the system will work in production. The production environment\nmight not have the same OS release, host conﬁguration, or supporting packages.\nAs a consequence, the testing is incomplete. It also makes it difﬁcult for developers\nto reproduce bugs found in QA.\nAnother problem with the waterfall methodology is that because the handoff\nbetween phases is also a handoff between organizations, the discovery of bugs or\nother problems can become a game of ﬁnger pointing. Is the softwarenot working in\nproduction because developers didn’t do their job, or did the problem arise because\noperations didn’t build the test environment correctly? Why ﬁnd out the truth when\nit is easier to create a political battle and see who can shift blame the fastest?\nThe DevOps methodology has many beneﬁts. Rather than the two phases cre-\nating a dividing wall between two organizations, the developers and operations\nstaff work collaboratively on each phase. The testing and production environments\nare built using the same tools, so testing is more accurate. The reuse of tools is more\nefﬁcient and means that the improvements made to the tools beneﬁt all. Because\nboth teams have a shared responsibility for the entire process, cooperation trumps\nﬁnger pointing.\n\n\n200\nChapter 9\nService Delivery: The Build Phase\nThe DevOps methodology is also more simple. There are just two distinct\nphases, each with well-deﬁned concerns, inputs, and outputs.\n9.2 The Virtuous Cycle of Quality\nGood ﬂow creates a virtuous cycle of quality. Rigorous testing creates a solid\nfoundation that results in better releases. This improves conﬁdence, which in turn\nencourages faster and better releases. Because they are smaller releases, testing is\nimproved. The cycle then repeats.\nWhen discussing service delivery, people often focus on how fast their release\ncycle has become. When someone brags about improving cycle time from six weeks\nto an hour, they are missing the point.\nWhat’s really important is conﬁdence in the quality of the ﬁnal product.\nImproved code management, speed, packaging, and cycle time are all means, not\nends. Conﬁdence is a result of the platform’s ability to provide better testing and\nother processes that are automated to assure consistency. An excellent discussion\nof this can be heard in Episode 33 of the DevOps Cafe Podcast (Willis, Edwards &\nHumble 2012).\nMore speciﬁcally, a good service delivery platform should result in the follow-\ning outcomes:\n• Confidence: We want a process that assures a high likelihood that each deploy-\nment into production will be successful. Success means application bugs are\nfound and resolved early, ensuring a trouble-free deployment without out-\nages. The more conﬁdent we are in our service delivery process, the more\naggressively we can try new things. Innovation requires the ability to aggres-\nsively and fearlessly experiment. Consider the opposite case: a company full\nof people who resist change does not innovate. As fear of change increases,\ninnovation declines. If we can conﬁdently try new things, then we can experi-\nment and innovate, knowing that we can count on our service delivery system\nto support our innovations.\n• Reduced Risk: Faster iterations are less risky. As discussed in Section 8.2.4,\nmore frequent releases mean that each release will contain a smaller number\nof changes and, therefore, is less risky.\n• Shorter Interval from Keyboard to Production: We want the end-to-end pro-\ncess to happen quickly. We want to have our capital—the code developers\ncreate—in the hands of the customers as quickly as possible. Compare yearly\nreleases to weekly releases. With the former, new features sit idle for months\nbefore they see the light of day. That would be like an automotive factory mak-\ning cars all year but selling them only in December. Faster iterations mean\nfeatures get into production faster. This is important because the investment\nrequired to create a new feature is huge.\n\n\n9.2\nThe Virtuous Cycle of Quality\n201\n• Less Wait Time: Faster iterations also mean code gets to the testing process\nsooner. This improves productivity because it is easier to debug code that\nwas written recently. Developers lose context over time; regaining lost con-\ntext takes time and is error prone. Less work has to be redone because testing\nhelps get things right the ﬁrst time.\n• Less Rework: We want to reduce the amount of effort spent redoing\nwork that was done previously. It is more efﬁcient to get things right the\nﬁrst time.\n• Improved Execution: Doing faster iterations improves our ability to execute\nall the phases. When there is a lot of time between each iteration, any man-\nual steps become less practiced and we don’t do them as well. If releases are\nextremely infrequent, the processes will have changed enough that it gives us\nan excuse not to automate. We throw up our hands and revert to old, manual\nmethods. Frequent releases keep automation fresh and encourage us to update\nit to reﬂect small changes before they turn into major ones.\n• A Culture of Continuous Improvement: The ideal process is always evolving\nand improving. Initially it might have manual steps. That’s to be expected, as\nprocesses are malleable when initially being invented. Once the end-to-end\nprocess is automated, it can be instrumented and metrics can be collected auto-\nmatically. With metrics we can make data-driven improvements. For a process\nto be continuously improved, we not only need the right technology but also\nneed a culture that embraces change.\n• Improved Job Satisfaction: It is exciting and highly motivating to see our\nchanges rapidly put into production. When the interval between doing work\nand receiving the reward is small enough, we associate the two. Our job\nsatisfaction improves because we get instant gratiﬁcation from the work\nwe do.\nRather than focusing purely on cycle time, a team should have metrics that balance\nthe velocity of individual aspects of the software delivery platform. We recommend\nthat every DevOps team collect the following metrics:\n1. Bug lead time: Time from initial bug report to production deployment of ﬁxed\ncode.\n2. Code lead time: Time from code commit to production deployment.\n3. Patch lead time: Time from vendor patch release to production deployment.\n4. Frequency of deployment: How many deployments to production are done\neach month.\n5. Mean time to restore service: Duration of outages; from initial discovery to\nreturn to service.\n6. Change success rate: Ratio of successful production deployments to total\nproduction deployments.\n\n\n202\nChapter 9\nService Delivery: The Build Phase\n9.3 Build-Phase Steps\nThe goal of the build phase is to create installation packages for use by the\ndeployment phase. It has ﬁve steps:\n1. Code is developed.\n2. Code is committed to the source repository.\n3. Code is built.\n4. Build results are packaged.\n5. Packages are registered.\nEach step includes some kind of testing. For example, building the software veriﬁes\nthat it compiles and packaging it veriﬁes that all the required ﬁles are available.\nAs Figure 9.1 shows, the source repository is used as the primary storage facil-\nity for this phase. The ﬁnal output is handed off to the next phase by stashing it in\nthe package repository.\n9.3.1 Develop\nDuring the develop step, engineers write code or produce other ﬁles. For example,\nthey may write C++, Python, or JavaScript code. Graphic designers create images\nand other artifacts.\nEngineers check out the existing ﬁles from the source repository, download-\ning the source onto the engineer’s machine or workspace. From there, the ﬁles are\nedited, revised, and altered. New ﬁles are created. For example, a software engineer\nworking on a new feature may make many revisions to all related ﬁles, compiling\nand running the code, and repeating this process until the source code compiles\nand functions as desired.\n9.3.2 Commit\nDuring the commit step, the ﬁles being developed are uploaded to the source\nrepository. Generally this is done infrequently, as it indicates the ﬁles have reached\na certain level of completeness. All committed code should be working code. If not,\nother developers’ work will come to a halt. They will pull recent changes into their\nworkspaces and the result will be code that doesn’t work. They will not be able to\ntell if the problem is due to their own error or if the source is just in bad shape.\nChecking in code that does not work is known as “breaking the build.” It may be\nbroken in that the code no longer compiles, or because the code compiles but auto-\nmated tests (discussed later) fail. If the build is broken, returning it to working state\nshould be considered a high priority.\nThe gate for this step is the pre-submit check. To prevent obviously bad or\nbroken code from entering, source repository systems can be conﬁgured to call\n",
      "page_number": 225
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 233-240)",
      "start_page": 233,
      "end_page": 240,
      "detection_method": "topic_boundary",
      "content": "9.3\nBuild-Phase Steps\n203\nprograms that will check the new ﬁles for basic validity and reject attempts to\ncommit code that fails. The checks are not able to determine that the code is perfect\nand bug free, but obvious mistakes can be detected such as syntax errors. Often unit\ntests, described later, are run to verify the change does not break basic functionality.\nPre-submit checks often check for style guide conformance (discussed later in\nSection 12.7.4).\nBecause pre-submit checks can call any program, people have found many cre-\native uses for them beyond simple sanity checks. Pre-submit checks can be used to\nenforce policies, update status displays, and check for common bugs. For example,\nwe once experienced an outage caused by a ﬁle with incorrect permissions. Now\na pre-submit check prevents that same problem from reoccurring.\n9.3.3 Build\nDuring the build step, source ﬁles are processed to generate new artifacts. This\nusually means source code is compiled to produce executable ﬁles. Other tasks\nsuch as converting images from one format to another, extracting documentation\nfrom source code, running unit tests, and so on might also be performed during\nthis step.\nThis step is gated based on whether all the build processes complete success-\nfully. The most important of these checks are the unit tests. Unit tests are quality\nassurance tests that can run on compilation units such as function libraries and\nobject class deﬁnitions. In contrast, system tests, discussed in the next chapter,\ninvolve running the service and testing its complete functionality.\nUnit tests often take the form of an additional executable that does nothing\nexcept call functions in the code in many different ways, checking whether the\nresults are as expected. For example, suppose the source code includes a library of\nfunctions for manipulating usernames. Suppose one function in the library tests\nwhether a string can be used as a valid username. The unit test may call that func-\ntion many times, each time testing a string that is known to be invalid a different\nway (too short, too long, contains spaces, contains invalid characters) to verify that\nall of those cases are rejected. Another unit test may do the inverse for strings that\nare known to be valid.\nUnit tests can be rather sophisticated. For example, to test functions that need\naccess to a database, the unit test code may set up a mini-database with sample\ndata. Setting up a database is complex, so testing frameworks have been developed\nthat permit one to replace functions for testing purposes. For example, suppose\nyou are testing a function that opens a connection to a database, sends a query,\nand manipulates the results. You might replace the “connect to database” function\nwith one that does nothing and replace the “query database” function with one that\nalways returns a particular set of results. Now you can test the function without\nneeding to have an actual database.\n\n\n204\nChapter 9\nService Delivery: The Build Phase\nThis step provides us with an opportunity to perform aggressive testing early\nin the process to avoid wasted effort later.\n9.3.4 Package\nDuring the package step, the ﬁles left behind from the previous step are used\nto create the installation packages. A package is a single ﬁle that encodes all the\nﬁles to be installed plus the machine-readable instructions for how to perform the\ninstallation. Because it is a single ﬁle, it is more convenient to transport.\nThis step is gated based on whether package creation happened successfully.\nA simple package format would be a Zip or tar ﬁle that contains all the ﬁles\nthat will be installed plus an installation script. When the installer runs, it reads\nthe package, extracts all the ﬁles, and then runs the installation script. A more\ndetailed description appears in the “Software Repositories” chapter of the third\nedition of The Practice of System and Network Administration (Limoncelli, Hogan &\nChalup 2015).\nSoftware packages should be designed to run in any environment. Do not\ncreate separate packages for the testing environment and the production environ-\nment. Or worse, do not build the package for testing, then after testing rebuild it\nfor the production environment. Production should run packages that were tested,\nnot packages that are similar to ones that were tested.\n.\nWhat Is a Software Package?\nA software package is a container. This single ﬁle contains everything needed\nto install an application, patch, or library. Packages typically include the\nbinary executables to be installed, any related data ﬁles, conﬁguration data,\nand machine-interpretable instructions describing how to install and remove\nthe software. You may be familiar with ﬁle formats such as Zip, UNIX tar, and\ncpio. Such ﬁles contain the contents of many smaller ﬁles plus metadata. The\nmetadata as a whole is like a table of contents or index. It encodes information\nneeded to unpack the individual ﬁles plus ﬁle ownership, permissions, and\ntimestamps.\n9.3.5 Register\nDuring the register step, the package is uploaded to the package repository. At this\npoint the package is ready to be handed off to the deploy phase. This step is gated\nbased on whether upload is a success.\n\n\n9.4\nBuild Console\n205\n9.4 Build Console\nThe build console is software that manages all of the build steps, making it easy\nto view results and past history, and to keep statistics on success rates, the amount\nof time the process takes, and more. Build consoles are invariably web-based tools\nthat provide a dashboard to view status as well as control panels to manage the\nprocesses. There are many such tools, including Hudson, Jenkins CI, TeamCity, Go\nContinuous Delivery, and Atlassian Bamboo.\nOnce you ﬁnd a tool that you like, you will ﬁnd yourself wanting to use it\nwith everything. Therefore, when selecting such a tool, make sure it operates with\nthe tools you currently use—source code repository software, compilers and other\nbuild tools—and offers support for all your operating systems and platforms. Such\ntools generally can be extended through a plug-in mechanism. This way you are\nnot at the whim of the vendor to extend it. If there is a community of open source\ndevelopers who maintain freely available plug-ins, that is a good sign. It usually\nmeans that the tool is extensible and well maintained.\nA build console also should have an API for controlling it. You will want to be\nable to write tools that interact with it, kick off jobs, query it, and so on. One of the\nmost simple and useful APIs is an RSS feed of recently completed builds. Many\nother systems can read RSS feeds.\n.\nCase Study: RSS Feeds of Build Status\nStackExchange has an internal chat room system. It has the ability to monitor\nan RSS feed and announce any new entries in a given room. The SRE chat room\nmonitors an RSS feed of build completions. Every time a build completes,\nthere is an announcement of what was built and whether it was successful,\nplus a link to the status page. This way the entire team has visibility to their\nbuilds.\n9.5 Continuous Integration\nContinuous integration (CI) is the practice of doing the build phase many times\na day in an automated fashion. Each run of the build phase is triggered by\nsome event, usually a code commit. All the build-phase steps then run in a fully\nautomated fashion.\nAll builds are done from the main trunk of the source code repository. All\ndevelopers contribute code directly to the trunk. There are no long-lived branches\nor independent work areas, created for feature development.\n\n\n206\nChapter 9\nService Delivery: The Build Phase\nThe term “continuous” refers to the fact that every change is tested. Imagine\na graph depicting which changes were tested. In CI, the line is unbroken—that is,\ncontinuous. In other methodologies, not every release is tested and the line would\nbe broken or discontinuous.\nThe beneﬁt of triggering the build process automatically instead of via a man-\nual trigger is not just that no one has to stand around starting the process. Because\nthe process is triggered for each code commit, errors and bad code are discov-\nered shortly thereafter. In turn, problems are easier to ﬁnd because of the “small\nbatches” principle discussed in Section 8.2.4, and they are easier to ﬁx because the\ncontext of the change is still in the developer’s short-term memory. There is also\nless of a chance that new changes will be made on top of buggy code; such changes\nmay need to be reversed and reengineered, wasting everyone’s time.\nDoing the process so frequently also ensures that it stays automated. Small\nchanges in the process that break the system can be ﬁxed while they are small.\nContrast this to monthly builds: by the time the next build runs, much of the pro-\ncess may have changed. The breakage may be large and the temptation to returning\nto doing things manually becomes a real issue.\n.\nDon’t Register Packages Yourself\nOnly packages built via the console are allowed to be registered in the package\nrepository. In other words, people can’t register packages. Usually developers\nwill have a way to run the entire build phase from their own workstation so\nthat they can maintain and debug it. However, just because they can create\na package doesn’t mean they should be allowed to upload that package into\nthe repository. The packages they build may be somehow dependent on their\nworkstation or environment, either accidentally or intentionally. Having all\npackages be built through the console ensures that anyone can build the pack-\nage. Thus a good rule is this: ofﬁcial packages are always built by the build\nconsole automation.\nGoing from manual builds to fully automated builds can be very difﬁcult.\nThere are three key ingredients. First, make sure that you can do the process manu-\nally from beginning to end. This may be a challenge if builds are done infrequently,\ndifferent people do the process differently, or different people do separate steps.\nSecond, make sure the source ﬁles are all stored in a source control system. If one\nstep in the build process is to walk over to the graphic artist to ask if there are\nany updated images, you have a broken process. Graphic artists, when they have\nan updated image, should be able to check the ﬁle into the repository. The build\n\n\n9.6\nPackages as Handoff Interface\n207\nsystem should simply take the most recently committed ﬁle. The third ingredient\nis to automate each step until it requires no interactive (keyboard/mouse) input.\nAt that point all the steps can be loaded into the console and debugged, to ﬁnally\nbecome the new ofﬁcial process.\nCI may seem like the obvious thing to do but you would be surprised at how\nmany major companies do builds very infrequently.\n.\nThe Risky 14-Day Build\nA San Francisco software company with which most system administrators\nare familiar had a build process that was manual and took two weeks to com-\nplete. Its software releases happened on the ﬁrst day of each quarter. Two\nweeks beforehand, an engineer would start doing the build. If the develop-\ners were late, the build would have even less time. The software was built\nfor three Windows releases and a dozen Linux/UNIX variations. Some of the\nbuild steps were so unreliable and ad hoc that it put the entire process at risk.\nBugs found along the way would be hot patched for that particular OS so as\nto not put the schedule at risk by starting over. Each quarter the company was\nat risk of shipping late, which would be highly visible and embarrassing.\nA newly hired build engineer was shocked at the process and informed\nthe chief technology ofﬁcer (CTO) that automating it would be a top priority.\nThe CTO disagreed and explained that it didn’t need to be automated: “I hired\nyou to do it!”\nThe engineer automated it anyway. Within a few months the entire pro-\ncess was automated for all operating systems. Later it was put into a build\nconsole and CI was achieved.\nThis transformed the development organization. Developers were now\nmore productive and produced better software. The release engineer could\nthen focus on more important and more interesting work.\nIn this situation, defying the CTO was the right thing to do. This person\nis a hero.\n9.6 Packages as Handoff Interface\nBefore we move on to the deployment phase, we need to pause and discuss the\nhandoff that happens between phases.\nThe handoff step between the build phase and the deployment phase involves\nthe delivery of an installation package. Using packages makes it easier to ensure\n\n\n208\nChapter 9\nService Delivery: The Build Phase\nthat the same bits are used in testing as well as deployment. Deployment may\nhappen hours or weeks after testing and it is important that the same ﬁles be used\nfor other deployments. Otherwise, untested changes can slip in.\nIt is easier to manage a package than the individual ﬁles. Packages are usually\ncryptographically hashed or digitally signed so that the receiving end can verify\nthat the ﬁle was not altered along the way. It is safer to upload a single ﬁle than a\nhierarchy of ﬁles. Because the metadata is encoded inside the package, there is no\nworry that it will be accidentally changed along the way.\nOther mechanisms for passing ﬁles between phases should be avoided. We’ve\nseen individual ﬁles emailed between teams, losing any permissions and owner-\nship metadata. We’ve seen organizations that do their handoff by placing all the\nindividual ﬁles in a particular subdirectory on a ﬁle server. There was no easy way\nto tell if the ﬁles had changed between test and deploy. The person maintaining the\ndirectory could not prepare the next release until we were done with the current\nﬁles. In this system there was no way to access older releases. Even worse, we’ve\nseen this process involve a subdirectory in a particular person’s home directory,\nwhich meant if that individual left the company, the entire process would break.\nAlso avoid using the source code repository as the handoff mechanism. Source\nrepositories usually have a feature that lets you label, or tag, all ﬁles at a particu-\nlar moment in time. The tag name is then given to the deployment phase as the\nhandoff. This approach may create many problems. For example, it is difﬁcult to\nverify the integrity of all the ﬁles and the metadata. Permissions may be acciden-\ntally changed. File ownership, by design, is changed. If this technique is used and\nbinaries are checked into the repository, the repository will grow quite large and\nunwieldy. If binaries are not checked in, it means deployment will have to build\nthem. This is a duplication of effort and risks introducing subtle changes that will\nhave bypassed testing.\n9.7 Summary\nThis chapter was an overview of service delivery and a detailed examination of the\nbuild phase. The next chapter will examine the deployment phase in detail.\nService delivery comprises the technical processes involved in turning source\ncode into a running service. The service delivery platform is the software and\nautomation that drives the process.\nService delivery has two phases: build and deploy. Build turns source code\ninto packages. Deploy takes the packages and deploys them in an environment.\nThere are different environments for different purposes. The test environment is\nused to test the service. The live environment is where the service runs to pro-\nvide service for customers. The two environments should be engineered and built\nthe same way to make testing as meaningful as possible. The test environment\n\n\nExercises\n209\ntypically differs only in its size and the fact that it stores ﬁctional data instead\nof real user data. There are many other environments, including some used for\nexploratory testing, performance testing, beta testing, and so on.\nA ﬂow is one trip through the service delivery platform. Each step includes\ntesting with the aim of ﬁnding bugs as early as possible. Bugs found in the live envi-\nronment are the result of insufﬁcient testing in earlier phases and environments.\nWhen a service delivery platform is functioning at its best, the result is high\nconﬁdence in the service being delivered. As a result, the organization can be\nmore aggressive with making changes, and features can be released faster. In other\nwords, innovation is accelerated. Compare this to an organization that is unsure of\nits delivery system. In such a case, releases take months to produce. Bugs are found\nlong after code is written, making it more difﬁcult to correct problems. Innovation\nis hindered because change becomes stiﬂed as fear and despair rule.\nThe handoff from build to deploy is a package, a ﬁle that contains many ﬁles. A\npackage is easier to transport, more secure, and less error prone than transporting\nmany individual ﬁles.\nExercises\n1. Describe the steps of the service delivery platform’s build phase.\n2. Why are the build and deployment phases separate?\n3. Why are the application and infrastructure ﬂows separate?\n4. What are the pros and cons of the waterfall methodology compared with the\nmodern DevOps methodology?\n5. How does “good ﬂow” enable a business to reach its goals?\n6. Describe continuous integration and its beneﬁts.\n7. Why do testing and production need separate environments?\n8. Describe the service delivery platform used in your organization. Based on the\nbeneﬁts described in this chapter, which changes would you recommend?\n9. What are the beneﬁts of using packages as the handoff mechanism?\n10. Does your your organization use CI? Which parts of your current platform\nwould need to change to achieve CI?\n\n\nThis page intentionally left blank \n",
      "page_number": 233
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 241-248)",
      "start_page": 241,
      "end_page": 248,
      "detection_method": "topic_boundary",
      "content": "Chapter 10\nService Delivery: The\nDeployment Phase\nLet ’er roll!\n—Elvia Allman\nto Lucy and Ethel\nIn the previous chapter we examined the build phase, which ends with the creation\nof a software package. In this chapter we’ll examine the deployment phase, which\nuses the package to create a running service.\nThe deployment phase creates the service in one or more testing and produc-\ntion environments. Deciding if a release used in the testing environment is ready\nto be used in the production environment requires approval.\nThe goal of the deployment phase is to create a running environment. This\nenvironment is then used for testing or for live production services.\nAs Figure 9.1 (page 198) showed, packages are retrieved from the package\nrepository and then installed and conﬁgured to create an environment. The envi-\nronment created may be the testing environment, which is set up to verify that all\nthe pieces of the system work together. It may also be the live environment, which\nprovides service to users. Alternatively, it may be one of the other environments\ndescribed previously in Section 9.1.1.\n10.1 Deployment-Phase Steps\nThere are three steps in the deployment phase: packages are promoted, installed,\nand conﬁgured.\n211\n\n\n212\nChapter 10\nService Delivery: The Deployment Phase\n10.1.1 Promotion\nThe promotion step is where a release is selected and promoted for use in the\ndesired environment. The desired version is selected and marked as the right\nversion for the environment being built.\nFor example, suppose building an environment requires three packages called\nA, B, and C. Each trip through the build phase results in a new package. Package\nA has versions 1.1, 1.2, and 1.3. B has versions 1.1, 1.2, 1.3, and 1.4. There are more\nversions because there have been more check-ins. C has versions 1.1 and 1.3 (1.2 is\nmissing because there was a build failure).\nLet’s say that the combination of A-1.2, B-1.4, and C-1.3 has been tested\ntogether and approved for production. The promotion step would tell the package\nrepository to mark them as the designated production versions.\nSelecting speciﬁc versions like this is generally done for production, beta, and\nearly access environments, as mentioned in Section 9.1.1. However, development\nand testing environments may simply use the latest release: A-1.3, B-1.4, and C-1.3.\nHow packages are marked for particular environments depends on the pack-\nage repository system. Some have a tagging mechanism, such that only one version\nof a particular package can have the “production” tag at a time. There may be many\npackages, each with one version designated as the production version. There is\nusually a virtual tag called “latest” that automatically refers to the newest version.\nSome repository systems use a technique called pinning. A package is pinned\nat a particular version, and that version is always used even if newer versions are\navailable. While tags are usually global, pinning is done at the environment level.\nFor example, testing and live environments would each have packages pinned at\ndifferent versions.\nOther package repository systems work very differently. They can store only\none version of a package at a given time. In this case, there will be multiple repos-\nitories and packages will be copied between them. For example, all new packages\nwill be put in a repository called “development.” When the package is ready to\nbe used in the testing environment, it is copied to a repository called “testing.”\nAll machines in the testing environment point at this repository. If the package\nis approved for use in production, it is copied to a third repository called “pro-\nduction”; all the machines in the production environment read packages from this\nrepository. The beneﬁt of this kind of system is that the production environment\ncannot accidentally install unapproved packages because it does not know they\nexist. Nevertheless, keeping a history of past package versions is more difﬁcult,\nsometimes done by some kind of separate archive subsystem.\n10.1.2 Installation\nIn the installation step, the packages are copied to machines and installed. This\nis done by an installer that understands the package format. Most operating\n\n\n10.1\nDeployment-Phase Steps\n213\nsystems have their own installer software, each generally tied to its native package\nrepository system.\nA package can include scripts to run before and after installation. The actual\ninstallation process involves running the pre-install script, copying ﬁles from the\npackage to their ﬁnal destination, and then running the post-install script. Pre-\ninstall scripts do tasks like creating directories, setting permissions, verifying\npreconditions are met, and creating accounts and groups that will own the ﬁles\nabout to be installed. Post-install scripts do tasks like copying a default conﬁgura-\ntion if one doesn’t already exist, enabling services, and registering the installation\nwith an asset manager. Post-install scripts can also perform smoke tests, such as\nverifying the ability to access remote services, middleware, or other infrastructure\nservices such as databases.\n10.1.3 Configuration\nIn the configuration step, local settings and data are put in place to turn the\ninstalled package into the running service.\nWhile packages often include installation scripts that do some generic conﬁg-\nuration, this step does machine-speciﬁc work required to create a working service.\nFor example, installing a web server package creates a generic web server conﬁg-\nured to host static ﬁles from a particular directory. However, determining which\ndomains are served from this machine, making the load balancer aware of its\npresence, and other tasks are speciﬁc to the machine and would be done here.\nThis step is gated by health checks—that is, a few simple tests that verify the\nsystem is running. For example, one common health check is done by requesting\na particular URL that responds only after carrying out a few quick internal tests.\nThere are many software frameworks for conﬁguration management. Some\npopular ones include CFEngine, Puppet, and Chef. They all permit the creation\nof modules for conﬁguring speciﬁc services and applying those modules to dif-\nferent machines as needed. Conﬁguration management is discussed further in\nSection 12.6.4.\nThe two major strategies for conﬁguration are called convergent orchestra-\ntion and direct orchestration. Convergent orchestration takes a description of how\nthe environment should be conﬁgured, and the conﬁguration management system\nthen makes individual changes that lead to the entire system converging on that\ndesired state. If for some reason an undesired change is made (accidentally by a\nuser, on purpose by a user, or by an external event such as a machine failure), the\norchestration system will detect this and make changes until the desired conﬁg-\nuration has converged again. When the next conﬁguration is deﬁned, the system\nstarts to converge toward this new deﬁnition, making the fewest changes required\nto get there. Convergent orchestration can be described as getting the environment\nto a particular state and keeping it there.\n\n\n214\nChapter 10\nService Delivery: The Deployment Phase\nDirect orchestration can be described as a method to execute a multistep pro-\ncess during which certain invariants hold true. For example, moving a database\nfrom one machine to another requires many steps that must happen in a certain\norder, all while the the invariant of “clients always have access to the database”\nremains true.\nThe steps might be as follows:\n1. Machine B is conﬁgured to be a database replica.\n2. Wait for the replica to become synchronized with the primary database.\n3. The database clients are put in temporary read-only mode.\n4. The roles of machines A and B are swapped, making A the read-only replica.\n5. The database clients are taken out of read-only mode and conﬁgured to send\nwrites to machine B.\nAchieving this goal with convergent orchestration would require an unwieldy\nprocess. It would require creating a desired state for each step, and waiting for\none state to be achieved before switching to the next.\nA challenge in direct orchestration is how to handle multiple processes hap-\npening at the same time. For example, imagine this process happening at the same\ntime as a load balancer is being added to the system and the web servers are being\nreconﬁgured to add a new service. These processes all involve overlapping sets of\nmachines and resources. The steps have to be ordered and coordinated in ways\nthat prevent conﬂicts and assure that the system does not paint itself into a corner.\nCurrently this is done manually, which is an error-prone process. Automating such\nthings at a large scale is the kind of thing that researchers are only just beginning\nto consider.\nOne of the barriers to moving to convergent orchestration is that systems have\nto be architected to support it. This is a major problem for enterprises automating\ntheir deployment and infrastructure management processes, especially with com-\nmercial products that cannot be modiﬁed. Home-grown systems can be designed\nwith support from the start or, alternatively, modiﬁed after the fact.\n10.2 Testing and Approval\nBefore a release is used in production, it must be tested and approved. First, auto-\nmated testing is done. Next, manual testing, if there is any, is performed. Lastly,\nmanagement approves or signs off on the release. The list of people or departments\nthat must sign off on a release is called the approval chain. After all this activity is\ncomplete, the release can be promoted and pushed into production.\n\n\n10.2\nTesting and Approval\n215\n10.2.1 Testing\nTesting involves many different categories of tests. In the build phase, unit testing\nis performed on each component. There are four kinds of testing in the deployment\nphase:\n• System Testing: This testing brings together all the various pieces of the ser-\nvice and tests the ﬁnal product or system. It is performed on the service\nrunning in the testing environment. Passing these tests is a precondition for\nthe release being used in production and any other environment that includes\nexternal customers. Every individual feature should be tested. Multistep\nworkﬂows such as making a purchase should also be tested. There are test-\ning frameworks that can perform tests as if they are being done by a user.\nFor example, Selenium WebDriver is an open source project that automates\nweb site testing by sending HTTP requests as if they came from various web\nbrowsers. No matter how a user interacts with software, there is a testing tool\nthat can automate the tests. This includes PC-based GUIs, APIs, consoles/\nkeyboards, mobile phones, and, as documented in Gruver, Young & Fulghum\n(2012), even the front panels of laser printers.\n• Performance Testing: These tests determine the speed of the service under\nvarious conditions. This testing is performed on the service while in the\ntesting environment or in a specially built performance testing environ-\nment. It should determine if the performance meets written speciﬁcations or\nrequirements. All too often, however, such speciﬁcations are nonexistent or\nvague. Therefore often the results of this testing are just compared to pre-\nvious results. If the entire system, or a speciﬁc feature, works signiﬁcantly\nmore slowly than the previous release—a performance regression—the test\nfails.\n• Load Testing: This special kind of performance testing determines how much\nload the system can sustain. It is usually done in the testing environment or\nin a special performance testing environment. Such testing involves subject-\ning the service to increasingly larger amounts of trafﬁc, or load, to determine\nthe maximum the system is able to process. As an example, Google does not\nuse a new Linux kernel without ﬁrst doing load testing to verify the kernel\nchanges have not negatively affected how much load a search cluster can sus-\ntain. An entire cluster is built with machines running this kernel release. Search\nqueries are artiﬁcially generated at larger and larger QPS. Eventually the clus-\nter maxes out, unable to do more QPS, or the system gets so slow that it cannot\nanswer queries in the required number of milliseconds. If this maximum QPS\nis signiﬁcantly less than the previous release, Google does not upgrade to that\nkernel.\n\n\n216\nChapter 10\nService Delivery: The Deployment Phase\n• User Acceptance Testing (UAT): This testing is done by customers to verify\nthat the system meets their needs and to verify claims by the producer. Cus-\ntomers run their own tests to verify the new release meets their requirements.\nFor example, they might run through each business process that involves\nthe service. System testing involves developers making sure that they don’t\nship products with defects. UAT involves customers making sure they don’t\nreceive products with defects. Ideally, any test developed for UAT will be\nmade known to the developers so that it can be added to their own battery\nof tests. This would verify such concerns earlier in the process. Sadly this\nis not always possible. UAT may include tests that use live data that cannot\nbe shared, such as personally identiﬁable information (PII). UAT also may be\nused to determine if an internal process needs to be revised.\n.\nEvery Step Has a Gate\nThe build process at StackExchange has a few different handoffs. Each is\ngated by a test so that defects aren’t passed forward, to use the assembly-line\nanalogy. There are many kinds of tests, each a separate module. The service\ndelivery ﬂow has the following handoffs and gates:\n1. Code is built and then packaging is gated by unit tests.\n2. Digital signature veriﬁcation gates whether the packager can pass code\nto the test environment.\n3. Promoting a release from the test environment to production is gated by\nsystem tests.\n4. Production environment upgrade success is gated by health checks.\n10.2.2 Approval\nIf all the tests pass, the release is called a production candidate. Candidates are put\nthrough an approval process. If they are approved they are installed in production.\nAt this point, the members of the approval chain are asked to sign off on\nthe production candidate. The approval chain is a list of speciﬁc people, or their\ndelegates, who must sign off on production releases. For example, the list might\ninclude the product manager, the director of marketing, and the director of engi-\nneering. Often departmental approval is required from the security, legal, and\nprivacy compliance departments.\nEach environment may have a different set of tests and approval chain to gate\nwhich releases may enter it. Deploying releases in the development and testing\n\n\n10.4\nInfrastructure Automation Strategies\n217\nenvironments is automatically approved. UAT and performance testing generally\nselect the latest release that has passed system testing. Environments that will have\nlive users, such as beta and demo environments, may be automatically upgraded\nperiodically. For example, it is common for demo environments to be wiped and\nreloaded on a speciﬁc day of the week or a certain duration before the ofﬁcial\nrelease date.\nEnvironments that will be exposed to live, or revenue-generating, customers\nshould be gated with the most scrutiny. Thus the production environment gen-\nerally requires all of the preceding tests plus positive conﬁrmation by the entire\napproval chain.\n10.3 Operations Console\nThe operations console is software that manages the operational processes, espe-\ncially the deployment steps. Like the build console, it is a web-based system that\nmakes it easy to view results and past history, and keeps statistics about success\nrates, process duration, and more.\nNearly everything said about the build console can be repeated in regard to\nthe operations console, so refer to Section 9.4. Security and authorization might\nbe more important here because processes can affect live services. For example,\nthere may tighter controls over who may initiate a launch for a new release into\nproduction.\n10.4 Infrastructure Automation Strategies\nA few strategic tips will help you fully automate the deploy phase so that it can\nrun unattended in the console. As discussed earlier, there is a ﬂow for deploy-\ning infrastructure and another ﬂow for deploying the service itself. Deploying the\nentire stack can be broken down even further: preparing and testing the physical\nor virtual machine, installing the operating system, installing and conﬁguring the\nservice. Each of these is a discrete step that can be automated separately. In fact, in\nlarge environments you’ll ﬁnd different teams responsible for each step.\n10.4.1 Preparing Physical Machines\nPreparing a physical machine involves unboxing it, mounting it in a rack, cabling\nit, conﬁguring BIOS settings, and testing. The earlier steps require physical work\nand are very difﬁcult to automate. Very few companies can afford robotics to do\nthese steps. However, we can reduce the labor by installing them one rack at a time\ninstead of one machine at a time, thereby taking advantage of economies of mass\nproduction. Another way to improve the process is to use blade servers. Blade\nservers are a technology made up of one chassis for many individual computers,\n\n\n218\nChapter 10\nService Delivery: The Deployment Phase\neach on a “blade,” which makes installation and maintenance easier. At very large\nscale, machines are designed with speciﬁc features to enable fast and efﬁcient mass\ninstallation. Google and other companies design their own hardware to ensure\ndesign features meet their needs.\nWe have seen some impressive systems that automate the new hardware\nintake process. The Tumblr Invisible Touch system automates upgrading ﬁrmware,\nsetting up the Baseboard Management Controller (BMC), adding the machine to\nTumblr’s inventory system, performing a multi-hour stress test, and conﬁguring\nthe network.\nA quick-and-dirty solution is to manually install hardware and conﬁgure BIOS\nsettings but automate the process of verifying that the settings are correct. Since\nBIOS settings change rarely, if at all, this may be good enough for some sites, or at\nleast a good half-measure on the way to full automation.\nAnother strategy is to reduce complexity through standardization. Standard-\nizing on a few hardware conﬁgurations makes machines interchangeable. Usually\nyou create one model for heavy computation with lots of RAM and CPU, and one\nmodel for mass storage with lots of disk. Now all machines in a category can be\ntreated as a pool of machines, allocating one when needed and returning it to the\npool when it is not. You can create an API that permits such allocations to happen\nalmost as easily as creating virtual machines.\n10.4.2 Preparing Virtual Machines\nPreparing virtual machines should be a matter of making an API call. That said,\nhaving a few standard sizes can make management easier.\nFor example, one might allocate VMs in sizes such that either four small, two\nmedium, or one large VM perfectly ﬁlls the physical machine. Then there is less\nchance that a physical machine may have some space unused, but not enough to\ncreate a new VM.\nOne can also use multiples of Fibonacci numbers. If a 5-unit machine is deallo-\ncated, for example, that leaves room for ﬁve 1-unit machines, a 2-unit plus a 3-unit\nmachine, and so on.\nThis not only helps fully utilize the physical machines but also makes reor-\nganizing them easier. Imagine a situation where a medium VM is needed but the\nonly free space is the size of a small VM on one physical machine and a small VM\non another physical machine. If the VM sizes are standardized, it is easy to deter-\nmine how VMs could be moved to create a medium-sized space on one physical\nmachine. If each VM is a custom size, moving VMs around might still be possible\nbut the movements could be like the Towers of Hanoi problem, requiring many\nintermediate steps and inefﬁciencies.\n",
      "page_number": 241
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 249-259)",
      "start_page": 249,
      "end_page": 259,
      "detection_method": "topic_boundary",
      "content": "10.4\nInfrastructure Automation Strategies\n219\n10.4.3 Installing OS and Services\nInstalling the operating system and service conﬁguration can be done in many\nways. The two main approaches are an image method and a conﬁguration man-\nagement method.\nThe image method involves creating a disk image for each kind of service.\nThis image is then copied onto the disk and, after a reboot, the machine comes up\npreconﬁgured. Images can be deployed to physical machines, virtual machines, or\ncontainers, as described in Section 3.2. The image contains the operating system, all\nrequired packages, and the service in a ready-to-run form. It is known as a baked\nimage because the service is “baked in.”\nThe configuration management strategy involves using an installer or other\nmechanism to get a minimal operating system running. Conﬁguration manage-\nment tools can then install any additional packages and bring up the service. This\ntechnique is known as frying the image, because the image is cooked up while\nyou wait.\nAutomated Baking\nBaked images can be installed more rapidly because all the conﬁguration is done\nahead of time. When turning up hundreds of machines, this can be a signiﬁcant\nwin. Unfortunately, maintaining many images can be a burden. Installing a patch\non many images, for example, is labor intensive. When installation is done this\nway, there is no version control, which is bad.\nThe solution is to automate the creation of baked images. Software frame-\nworks for creating baked images include Vagrant, Docker, and Netﬂix Aminator.\nAll of these options provide languages for describing how to build the image from\nscratch, by specifying the base OS release, packages, ﬁle settings, and so on. The\nimage is then created from this description. The description can be kept under ver-\nsion control, such that the service delivery platform can be used to build, test, and\ndeploy images.\nBaked images can be used for building a new service as well as upgrading\nan existing one. For example, if there are 10 web servers behind a load balancer,\nupgrading them involves taking each one out of the load balancer’s rotation,\ndeleting it, and re-creating it from the image. All 10 web servers running the new\nrelease will then have a well-understood conﬁguration.\nPersistent Data\nNot all machines can simply be wiped and reinstalled in this way. For example,\na database server or ﬁle server has irreplaceable data that is not part of the\nbuild or conﬁguration process. The solution in such a case is to put such data\non virtual disks, mounted from a storage area network (SAN) or other remote\n\n\n220\nChapter 10\nService Delivery: The Deployment Phase\nstorage system. The boot disk is replaced, but on boot-up it mounts its virtual\ndisk, thereby reattaching the system to the data it needs. By decoupling where\nstorage is provided from where it is used, machines become more disposable.\nBaked versus Fried\nConﬁguration management is often faster than upgrading a machine by installing\nan image. Conﬁguration management is also less disruptive, because it makes just\nthe minimum number of changes needed to achieve the desired conﬁguration.\nThose same 10 web servers needing to be upgraded can simply be individually\nrotated out of the load balancer and only the desired package upgraded. With this\ntechnique, the machines are not wiped, so no data is lost.\nOn the downside, the machines are now in a less well-understood conﬁgura-\ntion. Imagine that an 11th web server is added. It starts out with the new software\nrelease. Does this 11th machine have the same conﬁguration as the 10 that got to\ntheir current state through upgrades? In theory, yes, but small differences may\ncreep in. Future upgrades require testing that covers both situations, which adds\ncomplexity. Proponents of baked images would argue that it is better to refresh a\nmachine from scratch than to let entropy accumulate.\n.\nDifferent Files for Different Environments\nOften there is a group of ﬁles that must be different in the testing and produc-\ntion environments. For example, logos and other images might have special\nversions used solely in the actual live service. There may also be special\ncredentials, certiﬁcates, or other data.\nOne way to handle this situation is to include both sets of ﬁles in the pack-\nage and use conﬁguration management to point the server at the proper set.\nHowever, this approach does not work for certiﬁcates and other ﬁles that one\nmight not want to expose to all environments.\nAnother solution is to move the environment-speciﬁc ﬁles into separate\npackages. Each environment then has its own configuration package. The\ninstallation process would install the main application package plus the one\nconﬁguration package appropriate for that environment.\nThe problem with using environment-speciﬁc packages is that these ﬁles\nhave, essentially, bypassed the testing process. For that reason it is best to keep\nthe use of this mechanism to an absolute minimum, preferably restricting it\nto ﬁles that are low risk or ﬁles that can be tested other ways, such as via\npre-submit checks.\n\n\n10.6\nInfrastructure as Code\n221\n10.5 Continuous Delivery\nContinuous delivery (CD) is the technique in which testing is fully automated and\ntriggered to run for each build. With each build, the testing environment is created,\nthe automated testing runs, and the release is “delivered,” ready to be considered\nfor use in other environments. This doesn’t mean every change is deployed to\nproduction, but rather that every change is proven to be deployable at any time.\nCD has similar beneﬁts as continuous integration. In fact, it can be consid-\nered an extension to CI. CD makes it economical and low risk to work in small\nbatches, so that problems are found sooner and, therefore, are easier to ﬁx. (See\nSection 8.2.4.)\nCD incorporates all of continuous integration, plus system tests, performance\ntests, user acceptance tests, and all other automated tests. There’s really no excuse\nnot to adopt CD once testing is automated. If some tests are not automated, CD\ncan deliver the release to a beta environment used for manual testing.\n10.6 Infrastructure as Code\nRecall in Figure 9.1 that the service delivery platform (SDP) pattern ﬂow has quad-\nrants that represent infrastructure as well as applications. The infrastructure for all\nenvironments should be built through automation that is treated just like any other\nservice delivered by the SDP.\nThis approach is called infrastructure as code. Like application code, the code\nthat describes the infrastructure is stored in the source repository, revision con-\ntrolled, and tested in a test environment before being approved for deployment in\nthe live production environment.\nConﬁguration code includes the automation to do conﬁguration management\nas well as any conﬁguration ﬁles and data. Conﬁguration management code and\ndata are built, packaged, and so on just like application software. Even images for\nVMs and containers can be built and packaged.\nWhen the infrastructure as code technique is done correctly, the same code\ncan build development, testing, and production environments. Each environment\ndiffers only in terms of which machines to use, the number of replicas, and\nother settings. This minimizes the difference between the testing and production\nenvironments and makes testing more accurate.\nAnyone should be able to build an environment for development or testing.\nThis lets teams and individuals operate and experiment without needing to bother\noperations. Developers and QA personnel can build multiple test environments.\nSWEs can build their own sandbox environments, using virtual machines on their\nlaptops.\n\n\n222\nChapter 10\nService Delivery: The Deployment Phase\nInfrastructure as code becomes easier to implement as hardware becomes\nmore and more virtual. Over time, storage, machines, and networks have all gone\nvirtual. Storage volumes, virtual machines, and network topologies can be created,\nmanipulated, and deallocated through software and controlled by APIs.\n10.7 Other Platform Services\nA few other services are involved in an SDP and are worth a brief mention in this\nchapter:\n• Authentication: There needs to be some form of authentication so that the\nsystem can restrict who can access what. In turn, security facilities are needed\nthat provide authentication, authorization, and accounting (AAA). Often this\nservice consists of LDAP plus Kerberos, Open Directory, or Active Directory.\n• DNS: DNS translates names of machines to IP addresses. In an SDP, machines\nare brought up and turned down rapidly. It is important to have the ability to\nupdate DNS zones via an API or dynamic DNS (DynDNS/DDNS).\n• Configuration Management Database (CMDB): The deployment phase\nshould be database driven. Conﬁgurations and machine relationships are\nstored in a database, and the tools that build the environment use this informa-\ntion to guide their work. This means modifying the environment is as simple\nas updating a database. For example, the database might store the list of web\nserver frontends associated with a particular service. By adding to this list, the\nconﬁguration management tools will bring up web servers on those machines.\n10.8 Summary\nThe deployment phase of the software delivery platform is where software pack-\nages are turned into a running service. First the service is run in a testing environ-\nment. After an approval process is passed, the packages are used to build the live\nproduction environment.\nThe deployment phase involves selecting a group of packages that represent\nall the parts of the service. Together they are considered a release candidate. They\nare installed on the machines that make up the environment. The service is then\nconﬁgured and becomes ready for testing or use.\nAn environment requires infrastructure: hardware, network, and other com-\nponents. Virtual infrastructure can be conﬁgured through automation. Physical\nmachines can also be manipulated through software, although doing so requires\nmore planning and is slower and less ﬂexible. When infrastructure conﬁguration\nis encoded in software, the entire infrastructure can be treated as software, with all\nthe beneﬁts of source code: revision control, testing, and so on.\n\n\nExercises\n223\nBefore a release is used in the live environment, it must pass many tests.\nSystem tests check the system as a whole. Performance and load testing verify the\nsoftware’s performance. User acceptance testing is an opportunity for stakeholders\nto approve a release. There may be other approvals such as those issued by legal,\nmarketing, and product management.\nContinuous delivery is achieved when the deployment phase and all testing\nis automated. New release candidates are produced automatically and conﬁdence\nis improved as the testing becomes more and more extensive.\nService delivery engineering is a large and changing ﬁeld. We have only\nscratched the surface in this chapter. We recommend the book Continuous Delivery\n(Humble & Farley 2010) for a deeper look at the subject.\nExercises\n1. Describe the steps of the service delivery platform’s deployment phase.\n2. Describe continuous delivery and its beneﬁts.\n3. Which kinds of testing are common in service delivery?\n4. In your organization’s service delivery platform, which of the kinds of testing\nlisted in Section 10.2 are and aren’t done? Which beneﬁts would you hope to\ngain by adding the missing tests?\n5. What is the approval chain for software in your organization’s service delivery\nprocess? How are the approvals requested and responses communicated?\n6. How could the approval chain in your organization be automated?\n7. Apply the deployment-phase techniques from this chapter to an organization\nthat does not do software development, but instead chooses to use off-the-\nshelf commercial software.\n8. In your environment, what are the steps to prepare a new machine? Which are\nautomated? How could the non-automated steps be automated?\n9. Does your your organization use continuous delivery? Which parts of your\ncurrent platform would need to change to achieve CD?\n\n\nThis page intentionally left blank \n\n\nChapter 11\nUpgrading Live Services\nThe things that make you strong,\nand make you feel as though you’ve\naccomplished something, are not\nthe easy ones.\n—Dr. Jerri Nielsen\nThis chapter is about deploying new releases to the production environment. It is\ndifferent from deploying into any other environment.\nThe process of upgrading an environment with a new software release is called\na code push. Pushing code into production can be tricky because we are modify-\ning a system while it is running. This is like changing the tires of a car while it is\nspeeding down the highway at 90 km/h: you can do it, but it requires a lot of care\nand planning.\nLuckily there are many techniques available, each appropriate for different\nsituations. This chapter catalogs the most common techniques. We then discuss\nbest practices such as continuous deployment and other practical matters.\n11.1 Taking the Service Down for Upgrading\nOne way to upgrade a service is to take it down, push the new code out to all\nsystems, and bring the service back up. This has the beneﬁt of being very simple to\nimplement, and it permits testing of the service before real users are given access\nto the newly upgraded service.\nSadly, this technique requires downtime, which makes it unacceptable for\nmost services. However, it may be appropriate for development and demo envi-\nronments where prescheduled downtime may be permitted.\nThis technique also works when the service is replicated in its entirety. Each\nservice replica can be taken down, upgraded, and brought back up if there is\nenough spare capacity. In this case, usually a global load balancer (GLB) divides\n225\n\n\n226\nChapter 11\nUpgrading Live Services\ntrafﬁc among the working replicas. One replica at a time is drained by removing\nit from the GLB and waiting until all in-ﬂight requests are completed. The replica\ncan then be taken down without affecting the service.\n.\nUpgrading Blog Search\nWhen Tom was an SRE for Google’s Blog Search service, the customer-facing\nstack was replicated in four datacenters. Each replica was independent of the\nothers. There was enough capacity that any one stack could be down and the\nothers could handle the entire trafﬁc load. One at a time, each stack would\nbe drained by removing it from the GLB, upgrading it, checking it, and then\nadding it back to the GLB.\nMeanwhile, another part of the system was the “pipeline”: a service that\nscanned for new blog posts, ingested them, produced the new corpus, and dis-\ntributed it to the four customer-facing stacks. The pipeline was very important\nto the entire service, but if it was down customers would not notice. However,\nthe freshness of the search results would deteriorate the longer the pipeline\nwas down. Therefore uptime was important but not essential and upgrades\nwere done by bringing down the entire pipeline.\nMany services at Google were architected in a similar way and upgrades\nwere done in a similar pattern.\n11.2 Rolling Upgrades\nIn a rolling upgrade, individual machines or servers are removed from ser-\nvice, upgraded, and put back in service. This is repeated for each element being\nupgraded; the process rolls through all of them until it is complete.\nThe customer sees continuous service because the individual outages are hid-\nden by a local load balancer. During the upgrade, some customers will see the\nnew software and some will see the old software. There is a chance that a partic-\nular customer will see new features appear and disappear as sequential requests\ngo to new and old machines. This is rare due to load balancer stickiness, discussed\nin Section 4.2.3, and other factors, such as deploying new features toggled off, as\ndescribed in Section 2.1.9.\nDuring the upgrade, there is a temporary reduction in capacity. If there are\n10 servers, as each is upgraded the service is at 90 percent capacity. Therefore this\ntechnique requires planning to assure there is sufﬁcient capacity.\nThe process works as follows. First the server or machine is drained. This can\nbe done by reconﬁguring the load balancer to stop sending requests to it or by\n\n\n11.3\nCanary\n227\nhaving the replica enter “lame duck mode,” as described in Section 2.1.3, where\nit “lies,” telling the load balancer it is unhealthy so that the load balancer stops\nsending requests to it. Eventually no new trafﬁc will have been received for a while\nand all in-ﬂight requests will be ﬁnished. Next the server is upgraded, the upgrade\nis veriﬁed, and the draining process is undone. Then the upgrade process begins\nagain with the next server.\n.\nAvoiding Code Pushes When Sleepy\nThe best time to do a code push is during the day. You are wide awake and\nmore co-workers are available if something goes wrong.\nMany organizations do code pushes very late at night. The typical excuse\nfor a 3 upgrade is that the upgrade is risky and doing it late at night\ndecreases exposure.\nDoing critical upgrades while half-asleep is a much bigger risk. Ideally,\nby now we’ve convinced you that a much better strategy for reducing risk is\nautomated testing and small batches.\nAlternatively, you can have a team eight time zones east of your primary\nlocation that does code pushes. Those deployments will occur in the middle\nof the night for your customers but not for your team.\n11.3 Canary\nThe canary process is a special form of the rolling upgrade that is more appropri-\nate when large numbers of elements need to be upgraded. If there are hundreds\nor thousands of servers or machines, the rolling upgrade process can take a long\ntime. If each server takes 10 minutes, upgrading 1000 servers will take about\na week. That would be unacceptable—yet upgrading all the servers at once is\ntoo risky.\nThe canary process involves upgrading a very small number of replicas, wait-\ning to see if obvious problems develop, and then moving on to progressively larger\ngroups of machines. In the old days of coal mining, miners would bring caged\ncanaries into the mines. These birds are far more sensitive than humans to harmful\ngases. If your canary started acting sick or fell from its perch, it was time to get out\nof the mine before you became incapacitated by the gases.\nLikewise, the canary technique upgrades a single machine and then tests it for\na while. Problems tend to appear in the ﬁrst 5 or 10 minutes. If the canary lives, a\ngroup of machines are upgraded. There is another wait and more testing, and then\na larger group is upgraded.\n\n\n228\nChapter 11\nUpgrading Live Services\nA common canary process is to upgrade one server, then one server per minute\nuntil 1 percent of all servers are upgraded, and then one server per second until all\nare upgraded. Between each group there may be an extended pause. While this is\nhappening, veriﬁcation tests are run against all the upgraded servers. These tests\nare usually very simplistic, generally just verifying that the code is not crashing\nand live queries are succeeding.\nIf trouble is found (i.e., if the canary dies), the process is stopped. At this point\nthe servers that were upgraded can be rolled back. Alternatively, if there is enough\ncapacity, they can be shut down until a new release becomes available.\nCanarying is not a testing process. The canary process is a method for deploy-\ning a release into production that detects bad pushes and prevents them from\nbeing visible to users. The main difference between a testing process and a canary\nprocess is that it is acceptable for the test process to fail. If a testing process fails,\nyou’ve prevented a bad release from reaching live users. More pedantically, the\ntesting process has succeeded in detecting a defective release. That is a good thing.\nConversely, you don’t want a canary process to fail. A failed canary means\nsomething was missed by the testing process. A failed canary should be so rare\nthat it is cause to stop development and dedicate resources to determining what\nwent wrong and which additional testing needs to be added to prevent this failure\nin the future. Only then can new roll-outs begin. Canarying is an insurance policy\nagainst accidental bad releases, not a way to detect bad releases.\nTesting and canarying are often conﬂated, but should not be. What some peo-\nple call canarying is really testing new releases on live users. Testing should be\ndone in a testing environment, not on live users.\n.\nCanarying Is Not a Substitute for System Testing\nWe’ve observed situations where canarying was used to test new releases on\nlive users. In one case it was done unintentionally—a fact that was not realized\nuntil a major outage occurred. The SREs received a thoroughly tested package\nand would canary it into their production environment. This worked ﬁne for\nmany years because the test and live environments were very similar.\nOver time, however, many tools were developed by the SREs for use in\nthe production environment. These tools were not tested by the developers’\ntesting system. The developers were not responsible for the tools, plus many\nof the tools were considered ad hoc or temporary.\nThere’s an old adage in engineering, “Nothing is more permanent than a\ntemporary solution.” Soon these tools grew and begat complex automated sys-\ntems. Yet, they were not tested by the developers’ testing system. Each major\n\n\n11.4\nPhased Roll-outs\n229\n.\nrelease broke the tools and the operations staff had to scurry to update them.\nThese problems were trivial, however, compared to what happened next.\nOne day a release was pushed into production and problems were not\ndiscovered until the push was complete. Service for particular users came to\na halt.\nBy now the hardware used in the two environments had diverged enough\nthat kernel drivers and virtualization technology versions had diverged. The\nresult was that virtual machines running certain operating systems stopped\nworking.\nAt this point the SREs realized the environments had diverged too much.\nThey needed to completely revamp their system testing environment to make\nsure it tested the speciﬁc combination of main service release, kernel version,\nvirtualization framework version, and hardware that was used in production.\nIn addition, they needed to incorporate their tools into the repository and\nthe development and testing process so that each time they wouldn’t have\nto scramble to ﬁx incompatibilities with the tools they had developed.\nCreating a proper system testing environment, and a mechanism to keep\ntest and production in sync, required many months of effort.\n11.4 Phased Roll-outs\nAnother strategy is to partition users into groups that are upgraded one at a time.\nEach group, or phase, is identiﬁed by its tolerance for risk.\nFor example, Facebook has clusters dedicated to providing service to its own\nemployees. These clusters receive upgrades ﬁrst because their employees are will-\ning testers of new releases—it’s part of their job. Next, a small set of outside-user\nclusters are upgraded. Lastly, the remaining clusters are upgraded.\nStack Exchange’s upgrade process involves many phases. Stack Exchange has\nmore than 110 web communities, plus each community has a meta-community\nassociated with it for discussing the community itself. The same software is used\nfor all of these communities, though the colors and designs are different. The\ndeployment phases are the test environment, then the meta-communities, then\nthe less populated communities, and lastly the largest and most active commu-\nnity. Each phase starts automatically if the previous phase saw no problems for\na certain amount of time. By the time the upgrade reaches the last phase, Stack\nExchange has high conﬁdence in the release. The earliest phases can tolerate more\noutages for many reasons, including the fact that they are not revenue-generating\nunits.\n",
      "page_number": 249
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 260-267)",
      "start_page": 260,
      "end_page": 267,
      "detection_method": "topic_boundary",
      "content": "230\nChapter 11\nUpgrading Live Services\n11.5 Proportional Shedding\nProportional shedding is a deployment technique whereby the new service is built\non new machines in parallel to the old service. Then the load balancer sends, or\nsheds, a small percentage of trafﬁc to the new service. If this succeeds, a larger\npercentage is sent. This process continues until all trafﬁc is going to the new service.\nProportional shedding can be used to move trafﬁc between two systems. The\nold cluster is not turned down until the entire process is complete. If problems are\ndiscovered, the load can be transferred back to the old cluster.\nThe problem with this technique is that twice as much capacity is required\nduring the transition. If the service ﬁts on a single machine, having two machines\nrunning for the duration of the upgrade is reasonable.\nIf there are 1000 machines, proportional shedding can be very expensive.\nKeeping 1000 spare machines around may be beyond your budget. In this case,\nonce a certain percentage of trafﬁc is diverted to the new cluster, some older\nmachines can be recycled and redeployed as part of the new cluster.\n11.6 Blue-Green Deployment\nBlue-green deployment is similar to proportional shedding but does not require\ntwice as many resources. There are two environments on the same machine, one\ncalled “blue” and the other called “green.” Green is the live environment and blue\nis the environment that is dormant. Both exist on the same machine by a mecha-\nnism as simple as two different subdirectories, each of which is used as a different\nvirtual host of the same web server. The blue environment consumes very little\nresources.\nWhen the new release is to go live, trafﬁc is directed to the blue environment.\nWhen the process is ﬁnished, the names of the environments are swapped. This\nsystem permits rolling back to the previous environment to take place easily.\nThis is a very simple way of providing zero-downtime deployments on appli-\ncations that weren’t designed for it, as long as the applications support being\ninstalled in two different places on the same machine.\n11.7 Toggling Features\nAs discussed in Section 2.1.9, it is a common practice to tie each new feature to\na software ﬂag or conﬁguration setting so that such features can be individually\nturned on or off. Toggling the switch is known as flag flipping. New features that\nare incomplete and not ready for use by live users have their ﬂags off. When they\nare ready for use, the ﬂag is turned on. The feature can be disabled if problems are\nfound. Having a ﬂag off is also called hiding a feature behind a ﬂag.\n\n\n11.7\nToggling Features\n231\nFeature toggling is one way to implement the general principle of decoupling\ndeployment and release. Deployment is putting a package into an environment.\nRelease is making a feature available to users. We often achieve release through\ndeployment and, therefore, assume one implies the other. In fact, feature toggles\ndecouples these two concepts so that deployment can happen all the time while\nrelease happens on demand. Deployment becomes a purely technical activity that\nusers are unaware of.\nThere are many ways to implement the ﬂag mechanism. The ﬂags can be\ncommand-line ﬂags, used when starting the service. For example, starting a spell\nchecker service with version 2 of the spell check algorithm enabled, and the mor-\nphological algorithm disabled, might involve running a command such as the\nfollowing:\n$ spellcheck-server --sp-algorithm-v2 --morphological=off\nThe ﬂag might be set via shell environment variable. The variables are set prior\nto running the command:\n$ export SP_ALGORITHM_V2=yes\n$ export SP_MORPHOLOGICAL=off\n$ spellcheck-server\nThis becomes cumbersome as the number of ﬂags increases. A service may\nhave dozens or hundreds of ﬂags at any given time. Therefore ﬂag systems can\nread ﬂags from ﬁles instead:\n$ cat spell.txt\nsp-algorithm-v2=on\nmorphological=off\n$ spellcheck-server --flagfile=spell.txt\nAll of these approaches suffer from the fact that to change any ﬂag, the pro-\ncess must be restarted. This interrupts service. Other techniques permit ﬂags to\nbe updated at will. Systems like Google’s Chubby and the Apache Zookeeper\nproject can be used to store ﬂags and efﬁciently notify thousands of servers when\nthey change. With this approach, only one ﬂag is needed—the one specifying the\nzookeeper namespace—to ﬁnd the conﬁguration:\n$ spellcheck-server --zkflags=/zookeeper/config/spell\nFlag ﬂips are used for many reasons:\n• Rapid Development: To enable rapid development, features are built up by\na series of small changes to a main source branch. Large features take a long\n\n\n232\nChapter 11\nUpgrading Live Services\ntime to develop. The longer one waits to merge code changes into the main line\nsource, the more difﬁcult and risky the merge becomes. Sometimes the source\ncode may have been changed by other developers, which creates a merge con-\nﬂict. Merging small amounts of code is less error prone. (See Section 8.2.4 for\nmore details.) Given this fact, incomplete features are hidden by ﬂags that are\ndisabled until the feature is ready. The ﬂags may be enabled earlier in some\nenvironments than in others. For example, previewing a feature to product\nmanagement might be done by enabling that ﬂag in the demo environment.\n• Gradual Introduction of New Features: Often some features are introduced\nto some users but not others. Beta users, for example, may receive earlier\naccess to new features. When users are logged in, whether they have access to\nthe beta features is dynamically controlled by adjusting the ﬂags. Prior to the\ninvention of this technique, beta users would go to an entirely different set\nof servers. It is very costly to set up a complete replica of the server environ-\nment just to support beta users. Moreover, granularity was an all-or-nothing\nproposition. With ﬂags, each individual feature can be taken in and out of\nbeta testing.\n• Finely Timed Release Dates: Timing a ﬂag ﬂip is easier than timing a soft-\nware deployment. If a new feature is to be announced at noon on Tuesday, it\nis difﬁcult to roll out new software exactly at that time, especially if there are\nmany servers. Instead, ﬂags can be controlled dynamically to ﬂip at a speciﬁc\ntime.\n• Dynamic Roll Backs: It is easier to disable an ill-behaved new feature by dis-\nabling a ﬂag than by rolling back to an older release of the software. If a new\nrelease has many new features, it would be a shame to have to roll back the\nentire release because of one bad feature. With ﬂag ﬂips, just the one feature\ncan be disabled.\n• Bug Isolation: Having each change associated with a ﬂag helps isolate a bug.\nImagine a memory leak that may be in one of 30 recently added features. If\nthey are all attached to toggles, a binary search can identify which feature is\ncreating the problem. If the binary search fails to isolate the problem in a test\nenvironment, doing this bug detection in production via ﬂags is considerably\nmore sane than via many individual releases.\n• A-B Testing: Often the best way to tell if users prefer one design or another is to\nimplement both, show certain users the new feature, and observe their behav-\nior. For example, suppose sign-ups for a product are very low. Would sign-ups\nbe improved if a check box defaulted to checked, or if instead of a check box\nan entirely different mechanism was used? A group of users could be selected,\nhalf with their check box default checked (group A) and the other half with\nthe new mechanism (group B). Whichever design has the better results would\n\n\n11.7\nToggling Features\n233\nbe used for all users after the test. Flag ﬂips can be used both to control the test\nand to enable the winning design when the test is ﬁnished.\n• One Percent Testing: This testing exposes a feature to a statistical sample of\nusers. Sometimes, similar to the canary process, it is done to test a new fea-\nture before deploying it globally. Sometimes, similar to A-B testing, it is done\nto identify the reaction to an experimental feature before deciding whether\nit should be deployed at all. Sometimes it is done to collect information via\nstatistical sampling. For example, a site might like to gather performance\nstatistics about page load times. JavaScript code can be inserted into HTML\npages that transmits back to the service data about how long the page took\nto load. Collecting this information would, essentially, double the number of\nqueries that reach the service and would overload the system. Therefore, the\nJavaScript is inserted only 1 out of every 100 times. The ﬂag would be set to 0\nto disable collection, or a value specifying the sampling percentage.\n• Differentiated Services: Sometimes there is a need to enable different ser-\nvices for different users. A good ﬂag system can enable paid customers to\nsee different features than unpaid users see. Many membership levels can be\nimplemented by associating a set of ﬂags with each one.\nSometimes a ﬂag is used to disable everything to do with a new feature; at other\ntimes it just obscures its visibility. Suppose a web site is going to add auto-\ncompletion to an input ﬁeld. This feature will require changes in the HTML that is\ngenerated for the input prompt, the addition of a new API call to the database for\npartial queries, and possibly other code. The ﬂag could disable everything to do\nwith the feature or it may simply control whether the feature appears in the UI.\n.\nCase Study: Facebook Chat’s Dark Launch\nIn 2011 Facebook’s Chuck Rossi gave a presentation titled “Pushing Millions\nof Lines of Code Five Days a Week” (Rossi 2011) that described an in-house\ntool called “Gatekeeper” that manages dark launches: code launched into\nproduction with no user-visible component. Rossi’s talk revealed that at any\ngiven moment on Facebook.com, there is already the code for every major\nthing Facebook is going to launch in the next six months and beyond. Gate-\nkeeper permits ﬁne-grained control over which features are revealed to which\nusers. Some ﬁlters are obvious, such as country, age, and datacenter. Others\nare unexpected, including one to exclude known employees of certain media\noutlets such as TechCrunch (Siegler 2011). (Also see the related case study in\nSection 18.4.)\n\n\n234\nChapter 11\nUpgrading Live Services\n11.8 Live Schema Changes\nSometimes a new software release expects a different database schema from the\nprevious release. If the service could withstand downtime, one would bring down\nthe service, upgrade the software and change the database schema, and then\nrestart the service. However, downtime is almost always unacceptable in a web\nenvironment.\nIn a typical web environment there are many web server frontends, or repli-\ncas, that all talk to the same database server. The older software release is unable\nto understand the new database schema and will malfunction or crash. The newer\nsoftware release is unable to understand the old database schema and will also\nmalfunction. Therefore you cannot change the database schema and then do the\nsoftware upgrade: the older replicas will fail as soon as the database is modiﬁed.\nThere will be no service until replicas are upgraded. You cannot upgrade the repli-\ncas and then change the database schema because any upgraded replica will fail.\nThe schema cannot be upgraded during the rolling upgrade: new replicas will fail,\nand then at the moment the database schema changes, all the failing replicas will\nstart to work and the working replicas will start to fail. What chaos! Plus, none of\nthese methods has a decent way to roll back changes if there is a problem.\nOne way to deal with this scenario is to use database views. Each view pro-\nvides a different abstraction to the same database. A new view is coded for each\nnew software version. This decouples software upgrades from schema changes.\nNow when the schema changes, each view’s code must change to provide the same\nabstraction to the new schema. The change in schema and the upgrade of view code\nhappen atomically, enabling smooth upgrades. For example, if a ﬁeld is stored in a\nnew format, one view would store it in the new format and the other view would\nconvert between the old and new formats. When the schema changes, the views\nwould reverse roles.\nSadly, this technique is not used very frequently. Views are a rare feature on\nrelational databases. As of this writing, no key–value store (NoSQL) systems sup-\nport views. Even when the feature is available, it isn’t always used. In these cases\nanother strategy is required.\nAn alternative is to change the database schema by making the change over\na period of two software releases: one after adding any new ﬁelds to the database\nand another before removing any obsolete ﬁelds. We learned this technique from\nStephen McHenry and refer to it as the McHenry Technique. Similar techniques\nhave been called “expand/contract.”\nHere are the phases of this technique:\n1. The running code reads and writes the old schema, selecting just the ﬁelds that\nit needs from the table or view. This is the original state.\n\n\n11.8\nLive Schema Changes\n235\n2. Expand: The schema is modiﬁed by adding any new ﬁelds, but not removing\nany old ones. No code changes are made. If a roll back is needed, it’s painless\nbecause the new ﬁelds are not being used.\n3. Code is modiﬁed to use the new schema ﬁelds and pushed into production. If a\nroll back is needed, it just reverts to to Phase 2. At this time any data conversion\ncan be done while the system is live.\n4. Contract: Code that references the old, now unused ﬁelds is removed and\npushed into production. If a roll back is needed, it just reverts to Phase 3.\n5. Old, now unused, ﬁelds are removed from the schema. In the unlikely event\nthat a roll back is needed at this point, the database would simply revert to\nPhase 4.\nA simple example of this technique involves a database that stores user proﬁles.\nSuppose the ability to store the user’s photograph is being added. The new ﬁeld\nthat stores photographic information is added to the schema (Phase 2). A software\nrelease is pushed that handles the new ﬁeld, including the situation where the ﬁeld\nis empty. The next software release removes the legacy code and the need for the\nﬂag. There is no need for Phase 5 because the schema change only adds ﬁelds.\nAs the next example, suppose a ﬁeld is replaced by new ﬁelds. The user’s\nentire name was stored in one ﬁeld. The new schema uses three ﬁelds to store the\nﬁrst name, middle name, and last name separately. In Phase 2, the three new ﬁelds\nare added to the schema. In Phase 3, we introduce a software release that reads the\nold and new ﬁelds. The code does the right thing depending on whether the old\nor new ﬁelds are populated. Updates from the users write to the new individual\nﬁelds and mark the old ﬁeld as unused. At this time a batch job may be run that\nﬁnds any proﬁles still using the old ﬁeld and converts them to the new ﬁelds.\nOnce all legacy data are converted, any code that uses the old ﬁeld is removed.\nThis new release is pushed into production (Phase 4). Once this release is deployed,\nPhase 5 removes the old ﬁeld from the database schema.\nOne way to streamline the process is to combine or overlap Phases 4 and 5.\nAnother optimization is to lazily remove old ﬁelds, perhaps the next time the\nschema is changed. In other words, Phase 5 from schema version n is combined\nwith Phase 2 from schema version n + 1.\nDuring Phase 2, software will see the new ﬁelds. This may cause problems.\nSoftware must be written to ignore them and otherwise not break when unex-\npected ﬁelds exist. Generally this is not a problem, as most SQL queries request\nthe exact ﬁelds they need, and software accessing NoSQL databases tends to do\nthis by convention. In SQL terms, this means that any use of SELECT * should\nnot make assumptions about the number or position of ﬁelds in the data that is\nreceived. This is generally a good practice anyway, because it makes your code\nmore robust.\n\n\n236\nChapter 11\nUpgrading Live Services\n11.9 Live Code Changes\nSome systems permit live code changes. This makes performing upgrades much\neasier. Generally we frown on the technique of modifying a live system but some\nlanguages are designed speciﬁcally to support it.\nErlang is one such language. A service written in Erlang can be upgraded\nwhile it is running. Properly structured Erlang programs are designed as event-\ndriven ﬁnite-state machines (FSM). For each event received, a speciﬁc function is\ncalled. One event that the service can receive is a notiﬁcation that code has been\nupgraded. The function that is called is responsible for upgrading or converting\nany data structures. All subsequent events will trigger the new versions of the\nfunctions. No processes survive a restart of the Erlang engine itself, but carefully\nplanned upgrades can be arranged with zero downtime.\nA very readable introduction to Erlang’s basic features and structure is avail-\nable online at http://learnyousomeerlang.com/what-is-otp.\nWe expect to see more systems like this in the future. Geordi edited live code\non Star Trek: The Next Generation. We are optimistic that someday we will be able to\ndo so, too.\n11.10 Continuous Deployment\nContinuous deployment means every release that passes tests is deployed to\nproduction automatically. Continuous deployment should be the goal of most\ncompanies unless constrained by external regulatory or other factors.\nAs depicted in Figure 11.1, this requires continuous integration and con-\ntinuous delivery, plus automating any other testing, approval, and code push\nprocesses.\nRecall that continuous delivery results in packages that are production ready,\nbut whether to actually deploy each package into production is a business decision.\nContinuous deployment is a business decision to automate this approval and\nalways deploy approved releases.\nFigure 11.1: Continuous integration, delivery, and deployment build on each other.\n\n\n11.10\nContinuous Deployment\n237\nIt may sound risky to always be doing pushes, and it is. Therefore many pre-\ncautions are taken. One precaution is to use continuous deployment only to push\nto noncritical environments such as beta and UAT environments.\nAnother is to use this approach in production, but only for speciﬁc subsystems\nthat have achieved high conﬁdence in their releases. For example, it is common to\nsee continuous deployment for subsystems that provide APIs and backend services\nbut not web UI subsystems, which may still be tested manually. This is another\nbeneﬁt of service-oriented architectures.\nEven then, some people may be uncomfortable with continuous deploy-\nment. As an experienced system administrator, you may pay attention to various\n“hunches” that help you decide whether today is a good day for a code push. For\nexample, you might have a policy of not doing pushes on critical days such as when\nthe ﬁnance department is closing the books and outages would be especially bad.\nYou might notice an unusually large number of broken builds lately, which leads\nyou to realize the developers may not be having a good week. This might make\nyou wary of accepting a release without ﬁrst having a chat with the lead develop-\ners to see if they’ve been under a lot of stress. These issues seem like reasonable\nthings to consider.\nFor some people, these hunches may be an excuse to avoid continuous deploy-\nment. In reality, hunches provide a roadmap for how to embrace it. There is a bit\nof truth to all such hunches, so why not turn them into measurable things that can\nautomatically disable automatic deployments?\nHere is a list of factors that should be taken into consideration when decid-\ning whether to pause continuous delivery. Some of them may initially sound like\nthings only humans can determine. However, with a little creativity, they can all\nbe automated and used to gate automated code pushes:\n• Build Health: A recent bout of build failures often indicates other stability\nissues. Automatic pushes can be blocked if n of the last m builds failed. This\ncan serve as a proxy for measuring developer stress or rushed work.\n• Test Comprehensiveness: Only highly tested code should be automatically\npushed. There are metrics such as “percentage of code coverage” that can be\nused as indicators that testing has become lax.\n• Test Reproducibility: Flakey tests are tests that sometimes pass and some-\ntimes fail. These can be detected by running the tests multiple times and in\nrandom order. Delivery can be paused automatically if test reproducibility\nis low.\n• Production Health: There is no single test that indicates a system is healthy,\nbut a good proxy is that the monitoring system has no outstanding alerts.\n",
      "page_number": 260
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 268-277)",
      "start_page": 268,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "238\nChapter 11\nUpgrading Live Services\n• Schedule Permission: Automated pushes are forbidden if there is a scheduled\n“change freeze” for holidays, quarterly ﬁnancial reporting, or days when too\nmany key people are on vacation. A simple list of dates to pause deployments\ncovers these situations.\n• Oncall Schedule: To avoid dragging the current oncall engineer out of bed,\npushes may be paused during typical sleeping hours. If there are multiple\noncall teams, each covering different times of the day, each may have a differ-\nent idea of waking hours. Even a three-shift, follow-the-sun arrangement may\nhave hours when no one is entirely awake.\n• Manual Stop: There should be a list of people who can, with the click of a\nbutton, halt automated pushes. This is akin to assembly lines where anyone\ncan halt production if a defect is found. The reason for a manual pause does\nnot have to be an emergency.\n• Push Conflicts: A service may be made up of many subservices, each on its\nown release schedule. It can be prudent to permit only one subservice deploy-\nment at a time. Similarly, the next push should not start if the current one\nhasn’t ﬁnished.\n• Intentional Delays: It can be useful to have a pause between pushes to let the\ncurrent one “soak”—that is, to run long enough to verify that it is stable. Doing\npushes too rapidly may make it difﬁcult to isolate when a problem began.\n• Resource Contention: Pushes should be paused if resources are low—for\nexample, if disk space is low or there is unusually high CPU utilization. Load\nmust be below a particular threshold: don’t push when when the system is\nﬂooded. Sufﬁcient redundancy must exist: don’t push if replicas are not N + 2.\nIt might seem risky to turn these hunches into automated processes. The truth\nis that it is safer to have them automated and always done than letting a per-\nson decide to veto a push because he or she has a gut feeling. Operations should\nbe based on data and science. Automating these checks means they are executed\nevery time, consistently, no matter who is on vacation. They can be ﬁne-tuned\nand improved. Many times we’ve heard people comment that an outage happened\nbecause they lazily decided not to do a certain check. The truth is that automated\nchecks can improve safety.\nIn practice, humans aren’t any better at catching regressions than automated\ntests. In fact, to think otherwise is absurd. For example, one regression that people\nusually watch for is a service that requires signiﬁcantly more RAM or CPU than\nthe previous release. This is often an indication of a memory leak or other coding\nerror. People often miss such issues even when automated systems detect them and\nprovide warnings. Continuous deployment, when properly implemented, will not\nignore such warnings.\n\n\n11.11\nDealing with Failed Code Pushes\n239\nImplementing continuous deployment is nontrivial, and easier if done at the\nstart of new projects when the project is small. Alternatively, one can adopt a\npolicy of using continuous deployment for any new subsystems. Older systems\neventually are retired or can have continuous deployment added if they stick\naround.\n11.11 Dealing with Failed Code Pushes\nDespite all our testing and rigor, sometimes code pushed into production fails.\nSometimes it is a hard failure where the software refuses to start or fails soon after\nstarting. In theory, our canarying should detect hard failures and simply take the\nlone replica out of production. Unfortunately, not all services are replicated or are\nnot replicated in a way that canarying is possible. At other times the failure is more\nsubtle. Features may, for example, fail catastrophically in a way that is not noticed\nright away and cannot be mitigated through ﬂags or other techniques. As a result,\nwe must change the software itself.\nOne method is to roll back to the last known good release. When problems are\nfound, the software is uninstalled and the most recent good release is reinstalled.\nAnother method is to roll forward to the next release, which presumably ﬁxes\nthe problem discovered in the failed release. The problem with this technique is\nthat the next release might be hours or days away. The failed release must be\nresilient enough to be usable for the duration, or workarounds must be avail-\nable. The resilience techniques discussed in Chapter 6 can reduce the time pressure\ninvolved. Teams wishing to adopt this technique need to focus on reducing SDP\ncode lead time until it is short enough to make roll forward viable.\nRoll forward works best when servers are highly replicated and canarying\nis used for deployments. A catastrophic failure, such as the server not starting,\nshould be found in the test environment. If for some reason it was not, the ﬁrst\ncanary would fail, thus preventing the other replicas from being upgraded. There\nwould be a slight reduction in capacity until a working release is deployed.\nCritics of roll back point out that true roll back is impossible. Uninstalling soft-\nware and reinstalling a known good release is still a change. In fact, it is a change\nthat has likely not been tested in production. Doing untested processes in produc-\ntion should be avoided at all costs. Doing it only as an emergency measure means\ndoing a risky thing when risk is least wanted.\nRoll forward has overwhelming beneﬁts and a continuous deployment envi-\nronment creates the conﬁdence that makes roll forward possible and less risky.\nPragmatically speaking, sometimes roll forward is not possible. Therefore most\nsites use a hybrid solution: roll forward when you can, roll back when you have\nto. Also, in this situation, sometimes it is more expedient to push though a small\n\n\n240\nChapter 11\nUpgrading Live Services\nchange, one that ﬁxes a speciﬁc, otherwise unsurmountable problem. This emer-\ngency hotfix is risky, as it usually has not received full testing. The emergency\nhotﬁxes and roll backs should be tracked carefully and projects should be spawned\nto eliminate them.\n11.12 Release Atomicity\nDoes a release involve a speciﬁc component or the entire system?\nLoosely coupled systems include many subsystems, each with its own\nsequence of versions. This increases the complexity of testing. It is difﬁcult, and\noften impossible, to test all combinations of all versions. It is also often a waste of\nresources, as not all combinations will be used.\nOne technique is to test a particular combination of component versions: for\nexample, Service A running release 101, Service B running release 456, and Service\nD running release 246. The “(101 + 456 + 246) tuple” is tested as a set, approved as a\nset, and deployed as a set. If a test fails, one or more components is improved and a\nnew tuple is tested from scratch. Other combinations might work, but we develop\na list of combinations that have been approved and use only those combinations.\nIf roll back is required, we roll back to the previous approved tuple.\nThis technique is generally required when components are tightly coupled.\nFor example, load balancer software, its plug-ins, and the OS kernel being used are\noften tested as a tuple in highly demanding environments where a performance\nregression would be devastating. Another example might be the tightly coupled\ncomponents of a virtualization management system, such as Ganeti or OpenStack;\nvirtualization technology, such as KVM or Xen; storage system (DRBD); and OS\nkernel.\nThe problem with this technique is that it reduces the potential rate of change.\nWith all components moving in lock-step, one delayed release will mean the\ncomponents that are ready to move forward have to wait.\nIf the components are loosely coupled, then each component can be tested\nindependently and pushed at its own velocity. Problems are more isolated. The\nchanged component may fail, in which case we know it is a problem with that com-\nponent. The other components may fail, in which case we know that the changed\ncomponent has introduced an incompatibility.\nSuch a system works only when components are loosely coupled. For exam-\nple, at Google the entire infrastructure is an ecosystem of small, loosely coupled\nservices. Services are constantly being upgraded. It is not possible to ask the entire\ncompany to stop so that your system can be tested. Google’s infrastructure is more\nlike a biological system than a mechanical clock. Because of this, each service has\nto take upward compatibility seriously. Incompatible changes are announced long\nin advance. Tools are created speciﬁcally to help manage and track such changes.\n\n\nExercises\n241\nFor example, when an API will change in an incompatible way, there is a way\nto detect uses that will soon be deprecated and warn the service owners who are\naffected.\n11.13 Summary\nUpgrading the software running in an environment is called a code push. Push-\ning code requires planning and techniques engineered speciﬁcally for upgrading\nservices while they are running.\nSometimes a service can be taken down for upgrade because it runs “behind\nthe scenes.” More frequently a service is upgraded by taking down parts, upgrad-\ning them, and bringing them back up (i.e., a rolling upgrade). If there are many\nreplicas, one can be designated as the canary, which is upgraded before the oth-\ners as a test. Proportional shedding involves slowly migrating trafﬁc from an old\nsystem to a new system, until the old system is receiving no new trafﬁc.\nFlag ﬂips are a technique whereby new features are included in a release\nbut are disabled. A ﬂag or software toggle enables the feature at a designated\ntime. This makes it easy to revert a change if problems are found: simply turn the\ntoggle off.\nDatabase schema changes on live systems can be difﬁcult to coordinate. It\nis not possible to change the software of all clients at the exact same time as the\ndatabase schema. Instead, the McHenry Technique is used to decouple the changes.\nNew ﬁelds are added, software is upgraded to use the old and new ﬁelds, software\nis upgraded to use only the new ﬁelds, and any old ﬁelds are then removed from\nthe schema.\nWhen a push fails, it is sometimes possible to roll forward to the next release\nrather than reverting code back to the last known good release. Reverting code is\nrisky, and a failed roll back can be a bigger problem than the original failed push.\nContinuous deployment is the process of automating code pushes, automati-\ncally pausing or delaying them when certain criteria indicate the risk would be too\nhigh or if business conditions require a pause. As with continuous integration and\ncontinuous delivery, by doing something in an automated manner, frequently, we\nreduce risk and improve our processes.\nExercises\n1. Summarize the different techniques for upgrading software in live services.\n2. Why is a failed canary process so tragic that development should stop until\nthe cause is found?\n3. Which push techniques in this chapter are in use in your current environment?\n\n\n242\nChapter 11\nUpgrading Live Services\n4. Which push techniques should your current environment adopt? What would\nbe their beneﬁts?\n5. How would, or could, a dark launch be used in your environment?\n6. What is continuous deployment and what are the beneﬁts?\n7. Why are roll backs discouraged? How could they be made safer?\n8. How would you begin to implement continuous deployment in your environ-\nment? If you wouldn’t do so, why not?\n9. Who is Dr. Jerri Nielsen?\n\n\nChapter 12\nAutomation\nProgressive improvement beats\ndelayed perfection.\n—Mark Twain\nAutomation is when computers do work for us. The desire for automation histor-\nically has been motivated by three main goals: more precision, more stability, and\nmore speed. Other factors, such as increased safety, increased capacity, and lower\ncosts are desired side effects of these three basic goals. As a system administra-\ntor, automating the work that needs to be done should account for the majority of\nyour job. We should not make changes to the system; rather, we should instruct\nthe automation to make those changes.\nManual work has a linear payoff. That is, it is performed once and has beneﬁts\nonce. By comparison, time spent automating has a beneﬁt every time the code is\nused. The payoff multiplies the more you use the code.\nMaking changes to the running system should involve making changes to\ncode and data ﬁles stored in a revision-controlled central repository. Those changes\nare then picked up by the software delivery platform, tested, and deployed to pro-\nduction. To work in this way modern system administrators must be software\ndevelopers. Ever since installing a new computer became an API call, we have\nall become programmers. This sort of scripting isn’t heavy computer science and\ndoes not require formal training. It leads to learning more and more software\ndevelopment skills over time (Rockwood 2013). Automation does not put system\nadministrators out of work. Indeed, there is always more work to be done. Sys-\ntem administrators who can write code are more valuable to an employer. In fact,\nbecause system administration can scale only through automation, not knowing\nhow to code puts your career at risk.\nHowever, automation should not be approached as simply developing faster,\nmore predictable functional replacements for the tasks that people perform. The\nhuman–computer system needs to be viewed as a whole. It is a rare system that can\n243\n\n\n244\nChapter 12\nAutomation\n.\nTerms to Know\nDomain-Specific Language (DSL): A language that was purpose-built for\na particular use, such as system administration, mathematics, or text\nmanipulation.\nToil: Exhausting physical labor.\nbe fully automated. Most systems experience exceptions that cannot be handled by\nthe automation, but rather need to be handled by people. However, if the people\nrunning the automation have no insight into how it works, it is much harder for\nthem to ﬁgure out how and why it failed, and therefore handle the exception appro-\npriately, than it would be if they were more involved in the process. This situation\nis common where the person handling the exception is not the person who wrote\nthe automation. And at some point, that is the situation for all automation.\n12.1 Approaches to Automation\nThere are three primary approaches to automation design. The ﬁrst, and most\ncommon, approach is to automate using the “left-over” principle, where the\nautomation handles as much as possible, with people being expected to handle\nwhatever is left over. The second approach is the “compensatory” principle, where\nthe work is divided between people and the automation based on which one\nis better at which task. The third approach is based on the complementarity\nprinciple, which aims to improve the long-term health of the combined system\nof people and computers.\nAutomation impacts things beyond the immediate intended goal. Under-\nstanding what the unintended consequences of automation are likely to be should\naid you in building a healthier overall system. For example, antilock brakes (ABS)\nmake it easier for a driver to stop a car more quickly in an emergency. In isolation,\nthis automation should reduce accidents. However, the presence of this automa-\ntion affects how people drive. They drive more quickly in slippery conditions than\nthey would without ABS, and they leave shorter gaps to the car in front because\nof their conﬁdence in the ABS automation. These are unintended consequences of\nthe automation. When human factors are not taken into account, automation can\nhave unexpected, and perhaps negative, consequences.\nBear in mind that not all tasks should be automated. Some difﬁcult tasks\nare so rare that they happen exactly once and cannot happen again—for exam-\nple, installing a large, complex, software system. This is where you outsource.\nHire a consultant who has done it before, since the situation is so rare that the\n\n\n12.1\nApproaches to Automation\n245\nlearning curve has no payoff. The result of the consultant’s work should include a\nset of common tasks that are performed more frequently and are handed off to the\nregular operations staff or to the automation.\nAlso bear in mind that eliminating a task, whether it is easy, difﬁcult, frequent,\nor rare, is beneﬁcial because it becomes one less thing to know, do, or maintain.\nIf you can eliminate the task, rather than automating it, that will be the most\nefﬁcient approach. The co-creator of UNIX and the C programming language,\nKen Thompson, famously wrote, “One of my most productive days was throwing\naway 1,000 lines of code.”\n12.1.1 The Left-Over Principle\nUsing the left-over principle, we automate everything that can be automated\nwithin reason. What’s left is handled by people: situations that are too rare or too\ncomplicated to automate. This view makes the unrealistic assumption that people\nare are inﬁnitely versatile and adaptable, and have no capability limitations.\nThe left-over principle starts by looking at the tasks that people do, and\nuses a number of factors to determine what to automate. In the area of system\nadministration, there are many things that operations staff do. All of them can be\nclassiﬁed along the axes shown in Figure 12.1.\nFigure 12.1: Tasks can be classiﬁed by effort and frequency, which determines the\nnext step in how to optimize them.\n\n\n246\nChapter 12\nAutomation\nThe x-axis is labeled from “rare” to “frequent,” representing how often a task\nis done. The y-axis is labeled from “easy” to “difﬁcult,” representing how much\neffort the task requires each time it is performed.\n• Tasks classiﬁed as rare/easy can remain manual. If they are easy, anyone\nshould be able to do them successfully. A team’s culture will inﬂuence if the\nperson does the right thing.\n• Tasks classiﬁed as rare/difﬁcult should be documented and tools should\nbe created to assist the process. Documentation and better tools will make\nit easier to do the tasks correctly and consistently. This quadrant includes\ntroubleshooting and recovery tasks that cannot be automated. However,\ngood documentation can assist the process and good tools can remove the\nburden of repetition or human error.\n• Tasks classiﬁed as frequent/easy should be automated. The return on invest-\nment is obvious. Interestingly enough, once something is documented, it\nbecomes easier to do, thus sliding it toward this quadrant.\n• Tasks classiﬁed as frequent/difﬁcult should be automated, but it may be best\nto acquire that automation rather than write it yourself. Purchasing commer-\ncial software or using free or open source projects leverages the skills and\nknowledge of hundreds or thousands of other people.\nWith this principle, the aim is to achieve efﬁciency by automating everything\nthat it is feasible to automate. The human component is not explicitly considered.\nHowever, it is easy to describe and reasonably easy to decide what to auto-\nmate. Some of the lessons learned using this approach can be applied even\nif this is not the only principle considered. In particular, considering tasks on\nthe easy–difﬁcult and rare–frequent axes is a useful tool for those looking into\nautomation.\n12.1.2 The Compensatory Principle\nThe compensatory principle is based on Fitts’s list, named after Fitts (1951), who\nproposed a set of attributes to use when deciding what to automate. The attributes\nare shown in Table 12.1. Despite the more than 60 years that have passed since Fitts\nperformed his work, these attributes still apply reasonably well.\nThis principle is based on the assumption that the capabilities of people\nand machines are reasonably static, with the work being divided accordingly.\nRather than implicitly considering humans to be inﬁnitely versatile machines, this\napproach aims to avoid putting excessive demands on people.\nUsing the compensatory principle, we would determine that a machine is\nbetter suited than a person to collecting monitoring data at 5-minute intervals\nfrom thousands of machines. Therefore we would automate monitoring. We\n\n\n12.1\nApproaches to Automation\n247\nTable 12.1: The Principles of the Fitts List\nAttribute\nMachine\nOperator/Human\nSpeed\nMuch superior.\nComparatively slow,\nmeasured in seconds.\nPower output Much superior in level\nComparatively weak, about\nand consistency.\n1500 W peak, less than 150 W\nduring a working day.\nConsistency\nIdeal for consistent,\nUnreliable, subject to learning\nrepetitive actions.\n(habituation) and fatigue.\nInformation\nMultichannel. Information\nMainly single channel,\ncapacity\ntransmission in megabits/second. low rate < 10 bits/second\nMemory\nIdeal for literal reproduction,\nBetter for principles and\naccess restricted and formal.\nstrategies, access versatile\nand innovative.\nReasoning,\nDeductive, tedious to program.\nInductive. Easy to program.\ncomputation\nFast, accurate. Poor error\nSlow inaccurate. Good error\ncorrection.\ncorrection.\nSensing\nSpecialized, narrow range.\nWide energy ranges, some\nGood at quantitative assessment.\nmultifunction capability.\nPoor at pattern recognition.\nGood at pattern recognition.\nPerceiving\nCopes poorly with variations in\nCopes well with variation in\nwritten/spoken material.\nwritten/spoken material.\nSusceptible to noise.\nSusceptible to noise.\nalso might determine that a person cannot survive walking into a highly toxic\nnuclear accident site, but that properly designed robots can.\nAccording to this principle, people are considered information processing sys-\ntems, and the work is described in terms of the interaction between people and\ncomputers, with each having separate, identiﬁable tasks. It looks at optimizing the\nhuman–computer interactions to make the processes more efﬁcient.\n12.1.3 The Complementarity Principle\nThe complementarity principle looks at automation from the human perspective. It\naims to help people to perform efﬁciently in the long term, rather than just looking\nat short-term effects. It looks at how people’s behavior will change as a result of\nthe automation, as well as without the automation.\nIn this approach, one would consider what people learn over time by doing\nthe task manually, and how that would change with the automation. For example,\nsomeone who starts doing a new task starts by understanding the primary goal of\n",
      "page_number": 268
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 278-286)",
      "start_page": 278,
      "end_page": 286,
      "detection_method": "topic_boundary",
      "content": "248\nChapter 12\nAutomation\nthe task and the basic functions required to meet that goal. Over time the person\nunderstands more about the ecosystem surrounding that task, exceptional cases,\nand some bigger picture goals, and adapts his or her practices based on this knowl-\nedge. With automation, we build in the knowledge that we have accumulated so\nfar. Does that inhibit further learning? How does that change the learning ability\nof a person who is new to the environment? How do a person’s habits change over\nthe longer term due to the automation?\nThe complementarity principle takes more of a cognitive systems engineering\n(CSE) approach of looking at the automation and the people together as a joint\ncognitive system (JCS). It takes into account the fact that people are driven by goals\n(proactive) as well as by events (reactive). A joint cognitive system is characterized\nby its ability to stay in control of a situation, despite disrupting inﬂuences from the\nprocess itself or from the environment. It takes into account the dynamics of the\nsituation, including the fact that capabilities and needs may vary over time and\nwith different situations.\nOne way of coping with changing capabilities and needs is to allow for\nfunctional overlap between the people and the automation, rather than a rigid seg-\nregation of tasks. This allows functions to be redistributed as needed. People are\nviewed as taking an active part in the system and are adaptive, resourceful learning\npartners who are essential to the functioning of the system as a whole.\nUnfortunately, there is no silver bullet or easy-to-follow formula for automa-\ntion using the complementarity principle. But if you remember to consider the\nhuman factor and the long-term effects of the automation on the people running\nthe system, you have a better chance of designing good automation.\n12.1.4 Automation for System Administration\nIn “A Model for Types and Levels of Human Interaction with Automation,”\nParasuraman, Sheridan, and Wickens (2000) propose an approach to automation.\nMuch like the discussion in Section 14.2.4, they observe that there are four stages\nof information processing: information is gathered, that information is analyzed,\na decision is made, and action is taken. These authors also observe that there\nare gradations between fully manual and fully automated. The various degrees\ninclude automation levels where the system makes a suggestion for a person to\napprove, the system makes a suggestion and executes it if a person does not veto\nit within a certain amount of time, the system executes its decision and informs\npeople after the fact, and the computer decides everything and acts autonomously,\nnot requiring or asking for any human input.\nParasuraman et al.’s conclusion is that the information gathering and analysis\nstages are appropriate for a high level of automation. The decision stage should be\nautomated at a level appropriate for the function’s risk. Low-risk functions should\nbe highly automated and high-risk functions should be less automated. When it\n\n\n12.1\nApproaches to Automation\n249\ncomes to taking action, however, the appropriate level of automation is somewhere\nin the middle for high-risk functions.\nAllspaw’s (2012c) two-part blog post “A Mature Role for Automation” gives\na detailed explanation of Parasuraman et al.’s work and relates it to system\nadministration. He concludes that an even better model for automation is that\nof a partnership—applying the complementarity principle. When automation and\noperations staff work together, they enhance each other’s value the same way that\nmembers of a team do.\nA key element of teamwork is trust and safety. Allspaw cites Lee and See’s\n(2004) methods for making automation trustworthy:\n• Design for appropriate trust, not greater trust.\n• Show the past performance of the automation.\n• Show the process and algorithms of the automation by revealing intermediate\nresults in a way that is comprehensible to the operators.\n• Simplify the algorithms and operation of the automation to make it more\nunderstandable.\n• Show the purpose of the automation, design basis, and range of applications\nin a way that relates to the users’ goals.\n• Train operators regarding its expected reliability, the mechanisms governing\nits behavior, and its intended use.\n• Carefully evaluate any anthropomorphizing of the automation, such as using\nspeech to create a synthetic conversational partner, to ensure appropriate trust.\n12.1.5 Lessons Learned\nEfforts to automate can backﬁre. Automation is generally viewed as a pure engi-\nneering task, so the human component may all too often be neglected. Systems\nthat aim to eliminate boring and tedious tasks so that people can tackle more dif-\nﬁcult tasks leave the hardest parts to humans because they are too complex to be\nautomated. Thus mental fatigue due to many tedious tasks is eliminated, but it is\nreplaced by an even more burdensome mental fatigue due to the need to tackle\ndifﬁcult problems on a continual basis.\nAutomation can bring stability to a system, yet this stability results in opera-\ntors becoming less skilled in maintaining the system. Emergency response becomes\nparticularly brittle, a subject that we will address in Chapter 15.\nWhen designing automation, ask yourself which view of the human compo-\nnent is being assumed by this automation. Are people a bottleneck, a source of\nunwanted variability, or a resource? If people are a bottleneck, can you remove\nthe bottleneck without removing their visibility into what the system is doing, and\nwithout making it impossible for them to adjust how the system works, when nec-\nessary? If people are a source of unwanted variability, then you are constraining\n\n\n250\nChapter 12\nAutomation\nthe environment and inputs of the automation to make it more reliable. What effect\ndoes that have on the people running the automation? How does it constrain their\nwork? How can exceptions be handled? If people are to be a resource, then you\nneed closer coupling between the people and the automation. But in that case,\nthere are two thorny issues: who does which tasks (allocation) and who is in charge\n(responsibility)?\nThe long-term operation of a system can be broken down into four stages:\ntracking, regulating, monitoring, and targeting. Tracking covers event detection\nand short-term control in response to inputs or detected events. Automation typ-\nically starts at this level. Regulation covers long-term control, such as managing\ntransition between states. The case study in Section 12.3 is an example of automa-\ntion at the regulation level. Monitoring covers longer-term controls, interpreting\nsystem state and selecting plans to remain within the desired parameters. For\nexample, capacity planning, hardware evaluation and selection, and similar tasks\nwould fall into this category. Targeting covers setting the overall goals for the\nsystem based on the overall corporate goals—for example, using key performance\nindicators (KPIs; see Chapter 19) to drive the desired behavior. As you move up the\nchain, the higher-level tasks are generally more suited to people than to machines.\nAs you automate further up the chain, people become more disconnected from\nwhat the system is doing and why, and the long-term health of the system as a\nwhole may be jeopardized.\n.\nHidden Cost of Automation\nSuper automated systems often require super training, which can be super\nexpensive. Hiring becomes super difﬁcult, which begins to limit the com-\npany’s ability to grow at its desired rate. The missed opportunities that result\nbecome a burdensome cost. This opportunity cost may be more expensive\nthan what the system saves. Such dilemmas are why companies like Google\nimplement super aggressive recruiting campaigns to hire SREs.\n12.2 Tool Building versus Automation\nThere is a distinction between tool building and automation. Tool building\nimproves a manual task so that it can be done better. Automation seeks to elimi-\nnate the need for the person to do the task. A process is automated when a person\ndoes not have to do it anymore, yet this does not eliminate the need for people.\nOnce a process is automated, a system administrator’s role changes from doing\nthe task to maintaining the automation.\n\n\n12.2\nTool Building versus Automation\n251\n12.2.1 Example: Auto Manufacturing\nThis is analogous to what happened in auto manufacturing. Originally the process\nof painting the metal panels that make up the outer body of the car was a task\ndone by people. It was a slow, delicate, and difﬁcult process requiring great skill.\nThen a high-power paint sprayer was invented to improve this process. The same\nperson could do a better job, with less wasted paint, in less time. This technology\nalso reduced the amount of skill required, thereby lowering the barrier to entry for\nthis job. However, there was still a car panel painter job. The process had not been\nautomated, but there was a better tool for the job.\nIn the 1970s, auto manufacturing plants automated the car painting process.\nThey deployed robotic painting systems and the job of car panel painter was elimi-\nnated. Employees now maintain the robotic painting system, or automation, which\nis a very different job from painting metal panels.\n12.2.2 Example: Machine Configuration\nIn IT we are making a similar transformation. In a typical cloud computing envi-\nronment, every new machine must be conﬁgured for its role in the service. The\nmanual process might involve loading the operating system, installing certain\npackages, editing conﬁguration ﬁles, running commands, and starting services.\nA system administrator (SA) could write a script that does these things. For each\nnew machine, the SA runs the script and the machine is conﬁgured. This is an\nimprovement over the manual process. It is faster and less error prone, and the\nresulting machines will be more consistently conﬁgured. However, an SA still\nneeds to run the script, so the process of setting up a new machine is not fully\nautomated.\nAutomated processes do not require system administrator action, or else they\nreduce it to handling special cases. To continue our example, an automated solution\nmeans that when a machine boots, it discovers its identity, conﬁgures itself, and\nbecomes available to provide service. The role for an SA who conﬁgures machines\nis eliminated in most cases. The SA’s role is transformed into maintaining the\nautomation that conﬁgures machines and handling unusual hardware or operating\nsystems.\n12.2.3 Example: Account Creation\nCloud administrators often maintain the systems that make up a service delivery\nplatform. To give each new developer access, an SA might have to create accounts\non several systems. The SA can create the accounts manually, but it would save\na lot of time if the SA wrote a script that creates the accounts on all the relevant\nmachines, and there would be less chance of skipping a step. Each time a new\ndeveloper joins the company, the SA would use this account creation tool.\n\n\n252\nChapter 12\nAutomation\nBetter yet would be if the SA wrote a job that runs periodically to check if\nnew developers are listed in the human resources database and then automatically\ncreate the new accounts. In this case, the SA no longer creates accounts; the human\nresources department does. The SA’s job is maintaining and enhancing the account\ncreation automation.\n12.2.4 Tools Are Good, But Automation Is Better\nMuch of operational work consists of repeated tasks, such as conﬁguring machines,\ncreating accounts, building software packages, testing new releases, deploying\nnew releases, increasing capacity, failing over services, moving services, and mov-\ning or reducing capacity. All of these tasks can be improved with better tools, and\nthese tools are often stepping stones to automation.\nAnother advantage of automation is that it enables the collection of statis-\ntics about defects or, in IT terms, failures. If certain situations tend to make the\nautomation fail, those situations can be tracked and investigated. Often automa-\ntion is incomplete and certain edge cases require manual intervention. Those cases\ncan also be tracked and categorized, and the more pervasive ones can be prioritized\nfor automation.\nTool building is good, but automation is required for scalable cloud\ncomputing.\n12.3 Goals of Automation\nIn cloud services, automation is a must, not a “nice to have”—it is required for\ngrowth. Cloud-based systems grow in many ways: more machines, more sub-\nsystems and services, and new operational responsibilities. If a new person had\nto be hired for each new service or every n new machines, a cloud-based service\nwould not be able to function. No company could ﬁnd enough new qualiﬁed SAs,\nnor could it afford to pay so many people. Considering that larger organizations\nare more difﬁcult to manage, the people management challenge alone would be\ninsurmountable.\nThere is a popular misconception that the goal of automation is to do tasks\nfaster than they could be done manually. That is just one of the goals. Other goals\ninclude the following:\n• Help scaling. Automation is a workforce multiplier. It permits one person to\ndo the work of many.\n• Improve accuracy. Automation is less error prone than people are. Automa-\ntion does not get distracted or lose interest, nor does it get sloppy over\ntime. Over time software is improved to handle more edge cases and error\nsituations. Unlike hardware, software gets stronger over time (Spolsky 2004,\np. 183).\n\n\n12.3\nGoals of Automation\n253\n• Increase repeatability. Software is more consistent than humans when doing\ntasks. Consistency is part of a well-controlled environment.\n• Improve reliability. Once a process is automated, it is easier to collect statis-\ntics and metrics about the process. These data can then be used to identify\nproblems and improve reliability.\n• Save time. There is never enough time to do all the work that needs to be done.\nAn automated task should require less SA time than a manual one.\n• Make processes faster. Manual processes are slower because they involve\nthinking and typing. Both are error prone and correcting mistakes often has a\nlarge time penalty itself.\n• Enable more safeguards. Adding additional pre- and post-checks to an auto-\nmated process is easy. Doing so incurs a one-time cost, but improves the\nautomation for all future iterations of the process. Adding more checks to a\nmanual process adds a burden and creates temptation for the person to skip\nthem.\n• Empower users. Automation often makes it possible for a non-SA to do a\ntask. Automation turns a task that only an expert can do into one that an end\nuser can do using a self-service tool. In the example in Section 12.2.3, someone\nfrom the human resources department is now able to provision new accounts\nwithout the involvement of system administrators. Delegation saves time and\nresources.\n• Reduce user wait time. Manual processes can be done only when an SA is\navailable. Automation can be running all day and all night, and will usually\nhave completed a task before an SA would have been available to start it. Even\nif automation were slower than a person doing the same task, the net user wait\ntime would likely be shorter.\n• Reduce system administrator wait time. Many manual processes involve\ndoing one step, then waiting some time before the next step can proceed—for\nexample, waiting for new data to propagate through a system or waiting for\na machine to reboot. A process like this is said be full of “hurry up and wait”\nsteps. If the wait is long, an SA may be able to ﬁll the time with other work.\nHowever, this is often inefﬁcient. All too often, we start working on some-\nthing else, get distracted, and forget to return to the ﬁrst task or lose context.\nComputers are better at waiting than people are.\nFor example, building software packages can be very complex. A good tool will,\nwhen the user enters one command, compile the software, run unit tests, build the\npackage, and possibly more. Automation, however, should eliminate the need for\nsomeone to run the tool. Buildbot is a system that continuously monitors a source\ncode repository for changes. After any change, the software is checked out, built,\npackaged, and tested. If all tests pass, the new package is placed in a repository so it\nis available to be deployed. The latest working package is always available. Many\n\n\n254\nChapter 12\nAutomation\npackages may be created and never used, but since no SA effort is involved, there\nis no additional cost, assuming that the CPU cycles are available. In addition, the\nautomated build process should give the developers immediate feedback about\nany automated tests that fail, resulting in faster bug ﬁxes and better software (see\nChapter 9).\nAnother example involves conﬁguring new machines. A good tool for\nmachine conﬁguration is run by a system administrator, perhaps with a few\nparameters like hostname and IP address, and the result is a fully conﬁgured\nmachine. Automation, however, would be applied where each freshly installed\nmachine looks up its hostname in a directory or external database to ﬁnd its func-\ntion. It then conﬁgures the machine’s OS, installs various packages, conﬁgures\nthem, and starts the services the machine is intended to run. The manual steps\nare eliminated, such that machines come to life on their own.\nSometimes automation eliminates the need for a process entirely. Typically\nload balancers require manual conﬁguration to add replicas. Many tools are avail-\nable that make such conﬁguration easier or less error prone. However, a fully\nautomated solution eliminates the need for constant reconﬁguration. We’ve seen\nsystems where the load balancer is conﬁgured once, and then replicas commu-\nnicate their availability to the load balancer, which includes them in its rotation.\nRather than automating an existing process, this project eliminates the constant\nreconﬁguration by inventing a new way to operate.\n.\nCase Study: Automated Repair Life Cycle\nGoogle uses the Ganeti open source virtual cluster management system to\nrun many large clusters of physical machines, which in turn provide virtual\nmachines to thousands of users. A physical machine rarely fails, but because\nof the sheer number of machines, hardware failures became quite frequent.\nAs a result, SAs spent a lot of time dealing with hardware issues. Tom became\ninvolved in a project to automate the entire repair life cycle.\nFirst, tools were developed to assist in common operations, all of which\nwere complex, error prone, and required a high level of expertise:\n• Drain Tool: When monitoring detected signs of pending hardware prob-\nlems (such as correctable disk or RAM errors), all virtual machines would\nbe migrated to other physical machines.\n• Recovery Tool: When a physical machine unexpectedly died, this tool\nmade several attempts to power it off and on. If these efforts failed to\nrecover the machine, the virtual machines would be restarted from their\nlast snapshot on another physical machine.\n\n\n12.4\nCreating Automation\n255\n.\n• Send to Repairs Tool: When a machine needed physical repairs, there\nwas a procedure for notifying the datacenter technicians about which\nmachine had a problem and what needed to be done. This tool gathered\nproblem reports and used the machine repair API to request the work. It\nincluded the serial number of any failing disks, the memory slot of any\nfailing RAM, and so on. In most cases the repair technician was directed\nto the exact problem, reducing repair time.\n• Re-assimilate Tool: When a machine came back from repairs, it needed\nto be evaluated, conﬁgured, and readded to the cluster.\nEach of these tools was improved over time. Soon the tools did their tasks\nbetter than people could, with more error checking than a person would be\nlikely to do. Oncall duties involved simply running combinations of these\ntools.\nNow the entire system could be fully automated by combining these tools.\nA system was built that tracked the state of a machine (alive, having problems,\nin repairs, being re-assimilated). It used the APIs of the monitoring system and\nthe repair status console to create triggers that activated the right tool at the\nright time. As a result the oncall responsibilities were reduced from multiple\nalerts each day to one or two alerts each week.\nThe logs from the automation were used to drive business decisions.\nDowntime was improved dramatically by accelerating the move away from a\nmodel of hardware that proved to be the least reliable.\n12.4 Creating Automation\nAutomation has many beneﬁts, but it requires dedicated time and effort to cre-\nate. Automation, like any programming, is best created during a block of time\nwhere there are no outside interruptions. Sometimes there is so much other work\nto be done that it is difﬁcult to ﬁnd a sufﬁcient block of time to focus on creating\nthe automation. You need to deliberately make the time to create the automa-\ntion, not hope that eventually things will quiet down sufﬁciently so that you have\nthe time.\nIn a well-run team, the majority of the team’s time should be spent creating\nand maintaining automation, rather than on manual tasks. When manual tasks\nstart to become the greater part of the workload, it is time to take a step back\nand see how new automation might restore the balance. It is important to be\nable to identify which automation will have the biggest impact, and to tackle\nthat ﬁrst. It is also important to understand the places where automation is not\nappropriate.\n\n\n256\nChapter 12\nAutomation\n12.4.1 Making Time to Automate\nSometimes there isn’t enough time to automate because we’re so busy with urgent\nwork that blocks long-term work such as creating automation. To use an analogy,\nwe don’t have time to shut off the leaking faucet because we’re spending all of our\ntime mopping the ﬂoor. When this happens, it can be difﬁcult to get ourselves out\nof the rut and ﬁx the root causes that prevent us from having a healthy amount of\ntime to create automation.\nHere are some suggestions for making the time to work on automation:\n• Get management involved. Managers should reprioritize the work to empha-\nsize automation.\n• Find the top thing that is wrecking your ability to get the big things done\nand ﬁx, mitigate, or eliminate it. This may mean ignoring other work for a\ntime—even letting some things fail for a bit—while you ﬁx the leaking faucet.\n• Eliminate rather than automate. Find work that can be eliminated. For exam-\nple, spot tasks that you do on behalf of other people and push the responsi-\nbilities back onto those people. Eliminate duplicate effort. For example, if you\nare maintaining 10 different Linux versions, maybe you would be best served\nby narrowing that number down to only a few, or one.\n• Hire a temporary consultant to put into place the high-level automation\nframework and train people on how to use it.\n• Hire a junior person (maybe even temporarily) to do the “grunge work” and\nfree up senior people for bigger projects that would ﬁx the root problem.\n• Start small. Automating one small task can be contagious. For example, use\nconﬁguration management tools such as CFEngine or Puppet to automate one\naspect of conﬁguring new machines. Once one thing is automated, doing more\nis much easier. Don’t try to ﬁx every problem with the ﬁrst bit of automation\nyou create.\n• Don’t work harder; manage your time better. Books like Time Management for\nSystem Administrators (Limoncelli 2005) have a lot of useful advice.\n.\nLess Is More\nEtsy wrote a blog post explaining why the company decided not to adopt a\nnew database technology. The issue wasn’t that the technology wasn’t good; it\nwas that then Etsy would be maintaining two different database software sys-\ntems, two ways to do backups, two testing processes, two upgrade processes,\nand so on. It would be too much work (McKinley 2012).\n",
      "page_number": 278
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 287-295)",
      "start_page": 287,
      "end_page": 295,
      "detection_method": "topic_boundary",
      "content": "12.4\nCreating Automation\n257\n12.4.2 Reducing Toil\nToil is exhausting physical labor. It is the extreme opposite of the goal of automa-\ntion. However, toil tends to build up in an operations team. It starts small and\ngrows. For example, there may be a small project that can’t be automated because\nit is highly speciﬁc and unlikely to repeat. Over time, it does repeat, and soon that\none action becomes a huge burden. Soon the team is overloaded by toil.\nA little toil isn’t so bad. In fact, it is normal. Not everything is worth automat-\ning. If everything is automated perfectly and there is no work to be done, this\ngenerally means that the rate of change and innovation has been stiﬂed.\nIf more than half of a team’s collective time is spent on operational “toil,” that\nfact should raise a proverbial red ﬂag. The team should review how the members\nare spending their time. Usually there are a few deeper “root cause” problems that\ncould be ﬁxed to eliminate large chunks of manual work. Establish projects that\nwill strike at the root of those problems. Put all other projects on hold until the\n50/50 balance is achieved again.\nIn companies like Google, there is an established policy to deal with this\ntype of situation. Speciﬁcally, there is an ofﬁcial process for a team to declare a\ntoil emergency. The team pauses to consider its options and make a plan to ﬁx\nthe biggest sources of toil. Management reviews the reprioritization plans and\napproves putting other projects on hold until balance is achieved.\nSystem administrator time is too valuable to be spent on something with a\nlinear pay-off like manual work. Toil leads to burn-out and lack of morale. By\nreducing toil, we not only help the company but we also help ourselves.\n12.4.3 Determining What to Automate First\nApply any and all effort to ﬁx the biggest bottleneck ﬁrst. There may be multiple\nareas where automation is needed. Choose the one with the biggest impact ﬁrst.\nAnalyze the work and processes that the team is involved with to ﬁnd the\nbiggest bottleneck. A bottleneck is where a backlog of work accumulates. By\neliminating the bottleneck, you improve throughput.\nThink about the work being done as an assembly line. A project has many\nsteps and moves down the assembly line one step at a time. One of those steps is\ngoing to be the bottleneck.\nAny improvements made upstream of the bottleneck just increase the backlog.\nIt is tempting to make improvements above the bottleneck. Often they are easy or\nthe ﬁrst thing you noticed. Sometimes ﬁxing these items improves the efﬁciency of\nyour team, but the bottleneck is buried deep in some other team’s process. There-\nfore you have made an improvement but the total throughput of the system has\nnot changed. It may even make the problem worse by creating a bigger trafﬁc jam\nof work waiting at the bottleneck.\n\n\n258\nChapter 12\nAutomation\nImprovements made downstream of the bottleneck don’t help overall\nthroughput. Again, they may be the easiest improvements that can be made, but if\nwork is not getting to this part of the system then improving this part will not help\nthe entire system.\nThe bottleneck might not involve a technical problem. It may be a particular\nperson who is overloaded or an entire team that is unable to get important tasks\ndone because they are drowning in small, urgent tasks.\n12.5 How to Automate\nYou can’t automate what you can’t do manually. The ﬁrst step in creating automa-\ntion is knowing what work needs to be done. Begin by doing the process manually\nand documenting what is done in a step-by-step manner. Repeat the procedure,\nfollowing and correcting the documentation as you go. Have someone else follow\nthe same documentation to make sure it is clear. This is the best way to discover\nwhich steps are ill deﬁned or missing.\nThe next step toward automation is prototyping. Create tools to do the indi-\nvidual steps in isolation, making certain that each one is self-contained and works\ncorrectly.\nNow combine the individual tools into a single program. Mature automation\nneeds logging and sanity checking of input parameters and the return values of\nall calls. These features should be incorporated into each stage as well as the ﬁnal\nversion. The automation should check the return value of the previous step and, if\nit failed, should be able to undo the prior steps.\nThe next stage is to make a fully automated system. Identify what would cause\nan SA to run the tool, and have those conditions trigger automation instead. Com-\nplete logging and robust error handling must be in place before this ﬁnal step is\nimplemented.\nIn some cases it is appropriate to create a self-service tool so that non-SAs can\ndo this task. In this situation, security becomes a larger issue. There needs to be\na permission model to control who may use the tool. There need to be checks to\nverify that people can’t accidentally do bad things or circumvent the system to gain\nprivileged access.\n12.6 Language Tools\nMany products, languages, and systems are available for creating automation, and\neach has its own pros and cons. The capabilities of these tools range from basic to\nquite high level.\n\n\n12.6\nLanguage Tools\n259\n12.6.1 Shell Scripting Languages\nShell languages provide the commands one can type at the operating system\ncommand-line prompt. It is easy to turn a sequence of commands typed inter-\nactively at the Bash or PowerShell prompt into a script that can be run as a single\ncommand—either manually or by another process, such as cron.\nShell scripts have many advantages. For example, they allow for very fast\nprototyping. They let the programmer work at a very high level. You can com-\nbine commands or programs to make larger, more powerful programs. There are\nalso disadvantages to using shell scripts for your automation. Most importantly,\nshell-scripted solutions do not scale as well as other languages, are difﬁcult to test,\nand cannot easily do low-level tasks.\nThe most common UNIX shell is the Bourne Again Shell (bash), a superset\nof the Bourne Shell (the UNIX /bin/sh command). In the Microsoft Windows\nworld, PowerShell is a very powerful system that makes it easy to automate and\ncoordinate all the Windows systems and products.\n12.6.2 Scripting Languages\nScripting languages are interpreted languages designed for rapid development,\noften focusing on systems programming.\nSome common examples include Perl, Python, and Ruby. Perl is older and\nvery popular with system administrators because it is similar to C and awk,\nlanguages that UNIX system administrators traditionally know. Python has a\ncleaner design and the code is much more readable than Perl code. Ruby has a\nstrong following in the system administrator community. It is similar to Perl and\nPython in syntax, but adds features that make it easy to create mini-languages,\npurpose-built for a speciﬁc task. More programs can then be written in that\nmini-language.\nScripting languages are more ﬂexible and versatile than shell scripts. They are\nmore expressive, permit better code organization, scale better, and encourage more\nmodern coding practices such as object-oriented coding and functional program-\nming. Better testing tools are available, and there are more prewritten libraries.\nScripting languages have the ability to access networks, storage, and databases\nmore easily than shell scripts. Better error checking is also available.\nPerl, Python, and Ruby all have large libraries of modules that perform\ncommon system administration tasks such as ﬁle manipulation, date and time\nhandling, transactions using protocols such as HTTP, and database access. You\nwill often ﬁnd the program you write is leveraging many modules, gluing them\ntogether to create the functionality you need.\n\n\n260\nChapter 12\nAutomation\nA disadvantage of scripting languages is that they execute more slowly than\ncompiled languages. This drawback has been mitigated in recent years by new,\nfaster interpreter technology. Nevertheless, for the reasons described earlier, speed\nis not always essential in automation. Thus language speed is often not a factor in\nsystem administration tools whose speed is bounded by other factors. For example,\nif the program is always waiting on disk I/O, it might not matter if the program\nitself is written in a fast or slow language.\nThe primary disadvantage of scripting languages is that they are inappropri-\nate for very large software projects. While nothing prevents you from writing a\nvery large software project in Perl, Python, or Ruby (and many projects have been\ndone this way), it becomes logistically difﬁcult. For example, these languages are\nnot strongly typed. That is, the type of a variable (integer, string, or object) is not\nchecked until the variable is used. At that point the language will try to do the\nright thing. For example, suppose you are concatenating a string and a number: the\nlanguage will automatically convert the number to a string so that it can be con-\ncatenated. Nevertheless, there are some situations the language can’t ﬁx, in which\ncase the program will crash. These problems are discovered only at run-time, and\noften only in code that is rarely used or tested. Contrast this to a compiled language\nthat checks types at compilation time, long before the code gets into the ﬁeld.\nFor this reason scripting languages are not recommended for projects that will\ninclude tens of thousands of lines of code.\n12.6.3 Compiled Languages\nCompiled languages can be a good choice for large-scale automation. Automation\nwritten in a compiled language typically scales better than the same automation\nwritten in a scripting language.\nCompiled languages often used by system administrators include C, C++, and\nGo. As described earlier, compiled languages are usually statically typed and catch\nmore errors at compile time.\n12.6.4 Configuration Management Languages\nConﬁguration management (CM) languages are domain-speciﬁc languages (DSLs)\ncreated speciﬁcally for system administration tasks. CM systems are created for\nmaintaining the conﬁguration of machines, from low-level settings such as the net-\nwork conﬁguration to high-level settings such as which services should run and\ntheir conﬁguration ﬁles.\nConﬁguration languages are declarative. That is, the programmer writes code\nthat describes how the world should be and the language ﬁgures out which\nchanges are needed to achieve that goal. As discussed in Section 10.1.3 (page 213),\n\n\n12.6\nLanguage Tools\n261\nsome CM systems are designed around convergent orchestration (bringing the\nsystem to a desired state) while others favor directed orchestration (the ability to\nfollow a multistep plan of action).\nTwo popular CM systems are CFEngine and Puppet. Listing 12.1 and List-\ning 12.2 illustrate how to specify a symbolic link in CFEngine and Puppet, respec-\ntively. Listing 12.3 and Listing 12.4 show the equivalent tasks in Python and Perl,\nlanguages that are imperative (not declarative). Notice that ﬁrst two simply specify\nthe desired state: the link name and its destination. The Python and Perl versions,\nin contrast, need to check if the link already exists, correct it if it is wrong, cre-\nate it if it doesn’t exist, and handle error conditions. A high level of expertise is\nneeded to know that such edge cases exist, to handle them properly, and to test\nthe code under various conditions. This is very tricky and difﬁcult to get exactly\nright. The CM languages simply specify the desired state and leverage the fact\nthat the creator of the CM system has the knowledge and experience to do things\ncorrectly.\nListing 12.1: Specifying a symbolic link in CFEngine\nfiles:\n\"/tmp/link-to-motd\"\nlink_from => ln_s(\"/etc/motd\");\nListing 12.2: Specifying a symbolic link in Puppet\nfile { '/tmp/link-to-motd ':\nensure => 'link',\ntarget => '/etc/motd',\n}\nListing 12.3: Creating a symbolic link in Python\nimport os\ndef make_symlink(filename , target):\nif os.path.lexists(filename):\nif os.path.islink(filename):\nif os.readlink(filename) == target:\nreturn\nos.unlink(filename)\nos.symlink(target , filename)\nmake_symlink('/tmp/link-to-motd', '/etc/motd ')\n\n\n262\nChapter 12\nAutomation\nListing 12.4: Creating a symbolic link in Perl\nsub make_symlink {\nmy ($filename , $target) = ($_[0], $_[1]);\nif (-l $filename) {\nreturn if readlink($filename) eq $target;\nunlink $filename;\n} elsif (-e $filename) {\nunlink $filename;\n}\nsymlink($target , $filename);\n}\nmake_symlink('/tmp/link-to-motd', '/etc/motd ');\nConﬁguration management systems usually have features that let you build\nup deﬁnitions from other deﬁnitions. For example, there may be a deﬁnition of a\n“generic server,” which includes the settings that all servers must have. Another\ndeﬁnition might be for a “web server,” which inherits all the attributes of a “generic\nserver” but adds web serving software and other attributes. A “blog server” may\ninherit the “web server” deﬁnition and add blogging software. In contrast, an\n“image server” may inherit the “web server” attributes but create a machine that\naccesses an image database and serves the images, perhaps tuning the web server\nwith settings that are more appropriate for serving static images.\nBy building up these deﬁnitions or “classes,” you can build up a library that\nis very ﬂexible and efﬁcient. For example, a change to the “server” deﬁnition\nautomatically affects all the deﬁnitions that inherit it.\nThe key advantages of a CM system are that SAs can deﬁne things concisely at\na high level, and that it is easy to enshrine best practices in shared deﬁnitions and\nprocesses. The primary disadvantage is the steep learning curve for the domain-\nspeciﬁc language and the need to initially create all the necessary deﬁnitions.\n12.7 Software Engineering Tools and Techniques\nAutomation is just like any other software development project, so it needs the\nfacilities that beneﬁt all modern software development projects. You are probably\nfamiliar with the tools that are used for automation. Even so, we constantly ﬁnd\noperations teams not using them or not using them to their fullest potential.\nThe automation tools and their support tools such as bug trackers and source\ncode repositories should be a centralized, shared service used by all involved\nin software development. Such an approach makes it easier to collaborate. For\n\n\n12.7\nSoftware Engineering Tools and Techniques\n263\nexample, moving bugs and other issues between projects is easier if all teams use\nthe same bug tracking system.\nThe service delivery platform and related issues such as the need for contin-\nuous test, build, and deploy capabilities were discussed in Chapters 9 and 10. In\nthis section, we discuss issue tracking systems, version control systems, packaging,\nand techniques such as test-driven development.\n12.7.1 Issue Tracking Systems\nIssue tracking systems are for recording and managing bug reports and feature\nrequests. Every bug report and feature request should go through the system.\nThis raises the issue’s visibility, ensures the related work is recognized in statis-\ntics generated by management, and potentially allows someone else to get around\nto working on the issue before you do. Managing all work through one system\nalso makes it easier to prevent overlapping work. It allows members of the team\nto understand what the others are working on, and it makes it easier to hand off\ntasks to others (this last point is especially valuable when you won’t be available\nto work on these tasks).\nOften it is tempting to not report an issue because it takes time and we are\nbusy people, or we ﬁgure “everyone knows this issue exists” and therefore it\ndoesn’t need to be reported. These are exactly the kind of issues that need to be\nreported. Everyone doesn’t know the issue exists, especially management. Raising\nthe visibility of the issue is the ﬁrst step toward getting it ﬁxed.\nBugs versus Feature Requests\nWe colloquially refer to issue tracking systems as “bug trackers,” but this is actu-\nally a misnomer. Feature requests are not bugs and need to be tracked differently.\nResources are often allocated separately for bug ﬁxing and new feature develop-\nment. These workﬂows are often different as well. If you choose not to implement\na feature request, the matter is settled and therefore it is appropriate to mark the\nissue as being resolved. If you choose not to ﬁx a bug, the bug still exists. The issue\nshould remain open, just marked at a low priority designated for issues that will\nnot be ﬁxed.\nIssue tracking systems usually can be conﬁgured to serve different teams or\nprojects, with each one having its bugs stored in its own queue. You may choose\nto establish a different queue for bugs and one for features (“Operations—Feature\nRequests” and “Operations—Bugs”) or the system may have a way to tag issues\nas being one or the other.\nLink Tickets to Subsystems\nEstablish a way to relate tickets to a speciﬁc subsystem. For example, you could\nestablish a separate queue for each subsystem or use other kinds of categorization\n\n\n264\nChapter 12\nAutomation\nor tagging mechanisms. A small amount of planning now can prevent headaches\nin the future. For example, imagine there are three SRE teams, responsible for ﬁve\nsubsystems each. Suppose a reorganization is going to change which teams are\nresponsible for which subsystems. If each subsystem has its own queue or category,\nmoving tickets to the new team is as simple as changing who owns a particular\nqueue or mass-moving all tickets with a particular category marker. If there is no\nclassiﬁcation, the reorganization will be painstakingly complex. Each issue will\nneed to be individually evaluated and moved to the proper team. This will have\nto be done for all current issues and, if you want to maintain history, for all past\nissues.\nEstablish Issue Naming Standards\nIssues should be named in a uniform way. It simply makes it easier to read and\nprocess many issues if they are all phrased clearly. Bugs should be phrased in terms\nof what is wrong. If some are phrased in terms of what is wrong and others are\nphrased in terms of how the feature should work, it can be quite confusing when a\ntitle is ambiguous. If someone reports that “The help button links to a page about\nthe project,” for example, it is unclear if this statement describes a bug that needs\nto be ﬁxed (it should link to the help page) or explains how the system should\nwork (in which case the bug could be closed and marked “seems to already work\nas requested”).\nFeature requests should be described from the perspective of the person who\ndesires the new capability. In Agile methodology, the recommended template is\n“As a [type of user], I want [some goal] so that [some reason].” For example,\na request might be stated as follows: “As an SRE, I want a new machine type\n‘ARM64’ to be supported by the conﬁguration management system so that we can\nmanage our new tablet-based Hadoop cluster” or “As a user, I want to be able to\nclone a virtual machine via an API call so that I can create clones programatically.”\nChoose Appropriate Issue Tracking Software\nSoftware issue tracking systems are similar to IT helpdesk ticket systems. At the\nsame time, they are different enough that you will need different software for the\ntwo functions. Issue tracking systems focus on the bug or feature request, whereas\nIT helpdesk ticket systems focus on the user. For example, in an issue tracking\nsystem, if two different people report the same bug, they are merged or the second\none is closed as a duplicate. Every issue exists only once. In contrast, an IT helpdesk\nticket system is a mechanism for communicating with users and helping them with\nproblems or fulﬁlling requests. If two people submit similar requests, they would\nnot be merged as each is as unique as the person who made the request.\nSoftware issue tracking systems and IT helpdesk ticket systems also have dif-\nferent workﬂows. An issue tracking system should have a workﬂow that reﬂects\n\n\n12.7\nSoftware Engineering Tools and Techniques\n265\nthe software development process: bugs are received, veriﬁed, and ﬁxed; a differ-\nent person veriﬁes they are ﬁxed; and then the issue is closed. This involves many\nhandoffs. The statistics one needs to be able to generate include how much time is\nspent in each step of the process. The workﬂow for an IT helpdesk is more about\nthe back-and-forth communication with a person.\n12.7.2 Version Control Systems\nA version control system (VCS) is a central repository for storing, accessing, and\nupdating source code. Having all source code in one place makes it easier to col-\nlaborate and easier to centralize functions such as backups, build processes, and\nso on. A VCS stores the history of each ﬁle, including all the changes ever made.\nAs a consequence, it is possible to see what the software looked like at a particular\ndate, revert changes, and so on. Although version control systems were originally\nused for source code control, a VCS can store any ﬁle, not just source code.\nVCS frameworks all have a similar workﬂow. Suppose you want to make a\nchange to a system. You “check out” the source code, thus copying the entire source\nto your local directory. You can then edit it as needed. When your work is complete,\nyou “commit” or “check out” your changes.\nOlder VCS frameworks permit only one person to check out a particular\nproject at a given time so that people can’t create conﬂicting changes. Newer sys-\ntems permit multiple people to check out the same project. The ﬁrst person to check\nin his or her changes has it easy; all of the others go through a process of merging\ntheir changes with past changes. VCS software helps with merges, doing non-\noverlapping merges automatically and bringing up an editor for you to manually\nmerge the rest.\nA distributed version control system (DVCS) is a new breed of VCS. In a DVCS,\neveryone has his or her own complete repository. Sets of changes are transmitted\nbetween repositories. Check-outs don’t just give you a copy of the source code at\na particular version, but create a local copy of the entire repository including the\nentire revision history. You can check in changes to the local repository indepen-\ndently of what is going on in the central system. You then merge a group of such\nchanges to the master repository when you are done. This democratizes source\ncode control. Before DVCS, you could make only one change at a time and that\nchange had to be approved by whoever controlled the repository. It was difﬁcult to\nproceed with the next change until the ﬁrst change was accepted by the repository\nowner. In a DVCS, you are the master of your own repository. You do development\nin parallel with the main repository. Changes made to the main repository can be\npulled into your repository in a controlled manner and only when you want to\nimport the changes. Changes you make in your repository can be merged upstream\nto the main repository when desired, or not at all.\n",
      "page_number": 287
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 296-303)",
      "start_page": 296,
      "end_page": 303,
      "detection_method": "topic_boundary",
      "content": "266\nChapter 12\nAutomation\nA VCS should not be used only for source code; that is, conﬁguration ﬁles\nmust also be revision controlled. When automation or use of tools involves conﬁg-\nuration ﬁle changes, you should automate the steps of checking the conﬁg ﬁle out\nof version control, modifying it, and then checking it back in. Tools should not be\nallowed to modify conﬁg ﬁles outside of the VCS.\n12.7.3 Software Packaging\nSoftware, once developed, should be distributed in packages. Although this topic\nwas discussed in Section 9.6, we raise the issue again here because the same oper-\nations people who maintain a service delivery platform often distribute their own\nsoftware tools by ad hoc means.\nDistributing your own software via packages enables you to take advantage\nof all the systems used to keep software up-to-date. Without this capability, oper-\national tools end up being copied manually. The result is many systems, all out of\ndate, and all the problems that can bring.\nIdeally the packages should go through the same development, beta, and\nproduction phases as other software. (See Chapter 9.)\n12.7.4 Style Guides\nA style guide is a standard indicating how source code should be formatted and\nwhich language features are encouraged, discouraged, or banned. Having all code\nconform to the same style makes it easier to maintain the code and raises code\nquality. It sets a high bar for quality and consistency, raising standards for the entire\nteam. A style guide also has a mentoring effect on less experienced people.\nTypically programmers spend the vast majority of their time modifying code\nand adding features to existing code. Rarely do we have the opportunity to write\na new program from scratch and be the sole maintainer for its entire life. When\nwe work together as a team, it becomes critical to be able to look at a ﬁle for the\nﬁrst time and assimilate its contents quickly. This enhances productivity. When we\nwrite code that conforms to the style guide, we pay this productivity forward.\nStyle Guide Basics\nStyle guides make both formatting and feature recommendations. Formatting\nrecommendations include specifying how indenting and whitespace are used. For\nexample, most companies standardize on indenting with four (or less commonly\ntwo) spaces instead of tabs, eliminating extra blank lines at the end of ﬁles or\nwhitespace at the end of lines, and placing a single blank line between functions,\nand possibly two blank lines before major sections or classes.\nStyle guides prescribe more than just aesthetics. They also recommend par-\nticular language features over others, and often outright ban certain language\n\n\n12.7\nSoftware Engineering Tools and Techniques\n267\nfeatures that have proved to be difﬁcult to support. Languages often have two ways\nof doing something, and the style guide will select which is to be used. Languages\nmay also have features that are error prone or troublesome for other reasons; the\nstyle guide may ban or discourage those features.\nMany languages have style guides of their own (Python and Puppet). Every\nmajor open source project has a style guide for its community. Most companies\nhave a style guide for internal use. Google runs many open source projects and has\npublished its internal style guides (minus redactions) for more than a dozen lan-\nguages (https://code.google.com/p/google-styleguide/). This enables com-\nmunity members to participate and adhere to the current style. The Google style\nguides are very mature and are a good basis for creating your own.\nAdditional Recommendations\nOften special notations are recommended in style guides. For example, the Google\nstyle guide recommends special notation for comments. Use TODO comments for\ncode that is temporary, a short-term solution, or good enough but not perfect. Use\nNB comments to explain a non-obvious decision. Use FIXME comments to point\nout something that needs to be ﬁxed and list the bug ID of the issue. The annotation\nis followed by the username of the person who wrote the comment. Figure 12.2\nshows examples.\nAs mentioned in Section 9.3.2, source repositories can call programs to val-\nidate ﬁles before they are committed. Leverage these “pre-submit tests” to call\nstyle-checking programs and stop ﬁles with style violations from being commit-\nted. For example, run PyLint on any Python ﬁle, puppet-lint on any Puppet ﬁles,\nand even home-grown systems that pedantically reject CHANGELOG entries if their\nentries are not perfectly formatted. The result is consistency enforced consistently\nno matter how large the team grows.\n12.7.5 Test-Driven Development\nTest-driven development (TDD) is a software engineering practice that leads to\ncode with minimal bugs and maximizes conﬁdence.\n.\nTODO(george): The following works but should be refactored to use\ntemplates.\nNB(matt): This algorithm is not popular but gets the right result.\nFIXME(kyle): This will break in v3, fix by 2013-03-22. See bug\n12345.\nFigure 12.2: Special-purpose comments from the Google style guide\n\n\n268\nChapter 12\nAutomation\nTraditional software testing methodology involves ﬁrst writing code and then\nwriting code that tests the code. TDD reverses these steps. First one writes the code\nto test the code. Running these tests fails because the code hasn’t been written yet.\nThe developer then makes fast iterations of writing code and rerunning the tests;\nthis continues until all the tests pass. At that point, the developer is done.\nFor example, imagine writing a function that takes a URL and breaks it down\ninto its component parts. The function needs to handle many different kinds of\nURLs, with and without embedded usernames and passwords, different protocols,\nand so on. You would come up with a list of URLs that are examples of all the\ndifferent kinds that the code should be able to handle. Next, you would write code\nthat calls the function with each example URL and compares the result against\nwhat you think the result should be. As we saw in Section 9.3.3 and Section 10.2,\nthese are called unit tests.\nNow that the tests are written, the code itself is written. Step by step, the code\nis improved. The tests are run periodically. At ﬁrst only the tests involving the most\nsimple URLs work. Then more and more fancy URL formats work. When all the\ntests work, the function is complete.\nThe tests are retained, not deleted. Later you can make big changes to code\nwith conﬁdence that if the unit tests do not break, the changes should not have\nunintended side effects. Conversely, if you need to modify the code’s behavior,\nyou can start by updating the tests to expect different results and then modify the\ncode until these tests pass again.\nAll modern languages have systems that make it easy to list tests and run them\nin sequence. They enable you to focus on writing tests and code, not managing the\ntests themselves.\n12.7.6 Code Reviews\nCode reviews are a process by which at least one other person reviews any changes\nto ﬁles before they are committed to the VCS. The code review system (CRS) pro-\nvides a user interface that lets the reviewer(s) add commentary and feedback to\nparticular lines of the ﬁle. The original author uses the feedback to update the ﬁles.\nThe process repeats until all the reviewers approve the change and it is integrated\ninto the VCS.\nSoftware engineers conduct code reviews on source code to have a “second\npair of eyes” ﬁnd errors and make suggestions before code is incorporated into\nthe main source. It is also useful to do this on conﬁguration ﬁles. In fact, in an\nenvironment where conﬁguration ﬁles are kept in a source repository, code reviews\nare a great way to ask others what they think about a change you’re making.\nCRS can also be used to delegate tasks to others while still being able to pro-\nvide quality control. This enables scaling of authority via delegation. For example,\n\n\n12.7\nSoftware Engineering Tools and Techniques\n269\nmost companies would not let just anyone edit their load balancer conﬁgurations.\nHowever, if they are kept in VCS and require approval by someone in the load\nbalancer team, the process of load balancer updates becomes self-service. Given a\ngood “how to” document, anyone can edit the conﬁg ﬁle and submit the proposed\nchange to the load balancer team, who check it over and then submit the change\nto activate it.\nThere are other reasons to use a code review system:\n• Better written code. Code review is not just a style check; it is a time to deeply\nconsider the code and suggest better algorithms and techniques.\n• Bidirectional learning. Whether the reviewer or reviewee is more experi-\nenced, both improve their skill and learn from each other.\n• Prevent bugs. A second set of eyes catches more bugs.\n• Prevent outages. Using a CRS for conﬁguration ﬁles catches problems before\nthey hit production.\n• Enforce a style guide. Code reviewers can give feedback on style violations\nso they can be ﬁxed.\nThe time spent on a code review should be proportional to the importance of a\nﬁle. If a change is temporary and has a small inﬂuence on the overall system, the\nreview should be thorough but not overly so. If the ﬁle is in the core of an important\nsystem with many dependencies, more thought should go into the review.\nMembers of a healthy team accept criticism well. However, there is a trick\nto doing code reviews without being mean, or being perceived as mean (which\nis just as important): criticize the code, not the person. For example, “This algo-\nrithm is slow and should be replaced with a faster one” is neutral. “You shouldn’t\nuse this algorithm” is blaming the person. This subtle kind of blame undermines\nteam cohesion. The ability for team members to give criticism without being crit-\nical should be role-modeled by managers and subtly policed by all. Not doing\nso risks poisoning the team. Managers who do not follow this advice shouldn’t\nact surprised when members of their team unfairly criticize each other in other\nforums.\n12.7.7 Writing Just Enough Code\nWrite just enough code to satisfy the requirements of the feature or to ﬁx the bug.\nNo more. No less.\nWriting too little code means the feature request is not satisﬁed. Alternatively,\nwe may have written code that is so terse that it is difﬁcult to understand. Writing\ntoo much code creates a maintenance burden, creates more bugs, and wastes time.\n\n\n270\nChapter 12\nAutomation\nWriting too much code is an easy trap to fall into. We may write code that\nis more ﬂexible, or more conﬁgurable, than we currently require, or we may add\nfeatures that we think might be needed someday. We call this practice “future-\nprooﬁng” and justify it by suggesting that it will save us time in the future when\nthe feature is needed. It is fun and exciting to go beyond the requirements of what\nwe are creating.\nIn reality, history teaches us that we are bad at predicting the future. What\nwe think will be needed in the future is wrong 80 percent of the time. Extra code\ntakes time to develop. In an industry where everyone complains there isn’t enough\ntime to do their work, we shouldn’t be creating extra work. Extra code is a burden\nfor people in the future who will have to maintain the code. A program that is\n20 percent larger because of unused code is more than 20 percent more difﬁcult\nto maintain. Writing less code now saves your entire team time in the future. The\nvast majority of coding involves maintaining and adding features to existing code;\nrarely do we start a new project from scratch. Given this fact, making maintenance\neasier is critical.\nUnused code tends to include more bugs because it doesn’t get exercised or\ntested. If it is tested, that means we spent time adding the tests, the automated test-\ning system now has more work to do, and future maintainers will be unsure if they\ncan delete a feature because all the automated testing “must be there for a reason.”\nWhen the feature ﬁnally is needed, possibly months or years in the future, new\nbugs will be discovered as the requirements and environment will have changed.\nThat said, there is some reasonable future-prooﬁng to do. Any constant such as\na hostname or ﬁlename should be settable via a command-line ﬂag or conﬁguration\nsetting. Split out major functions or classes into separate libraries so that they may\nbe reused by future programs.\nThere are three tips we’ve found that help us resist the temptation to future-\nproof. First, use test-driven development and force yourself to stop coding when\nall tests pass. Second, adding TODO() comments listing features you’d like to add\noften reduces the emotional need to actually write the code. Third, the style guide\nshould explicitly discourage excessive future-prooﬁng and encourage aggressively\ndeleting unused code or features that have become obsolete. This establishes a high\nstandard that can be applied at code review time.\n12.8 Multitenant Systems\nMultitenancy is the situation in which one system provides service for multiple\ngroups, each compartmentalized so that it is protected from the actions of the\nothers.\nBetter than creating automation for just your team is ﬁnding a way to automate\nsomething in such a way that all other teams can beneﬁt from it. You could provide\n\n\n12.9\nSummary\n271\nthe software releases internally so that others can run similar systems. Even better,\nyou could create your system such that it can serve other teams at the same time.\nWhen laying out the conﬁguration management aspects of your system, you\ncan achieve greater ﬂexibility and delegate management through multitenancy.\nThis simply means setting up the system to allow individual groups, or “tenants,”\nto control their own code base.\nImportant qualities of a multitenant framework are that it can be used by mul-\ntiple groups on a self-service basis, where each group’s usage is isolated from that\nof the other groups. Doing so requires a permission model so that each team is\nprotected from changes by the other teams. Each team is its own security domain,\neven though just one service is providing services to all teams.\n.\nCase Study: Multitenant Puppet\nGoogle has one centralized Puppet server system that provides multitenant\naccess to the individual teams that use it. The Mac team, Ubuntu team, Ganeti\nteam, and others are all able to manage their own conﬁgurations without\ninterfering with one another.\nThe system is very sophisticated. Each tenant is provided with a fully\nsource code–controlled area for its ﬁles plus separate staging environments\nfor development, testing, and production. Any feature added to the cen-\ntral system beneﬁts everyone. For example, any work the Puppet team does\nto make the servers handle a larger number of clients beneﬁts all tenants.\nWhen the Puppet team made it possible for the Puppet servers to be securely\naccessed from outside the corporate ﬁrewall, all teams gained the ability for\nall machines to stay updated even when mobile.\nWhile Google’s system enables each tenant to work in a self-service man-\nner, protections exist so that no team can modify any other’s ﬁles. Puppet\nmanifests (programs) run as root and can change any ﬁle on the machine\nbeing run on. Therefore it is important that (for example) the Ubuntu team\ncannot make changes to the Mac team’s ﬁles, and vice versa. Doing so would,\nessentially, give the Ubuntu team access to all the Macs. This is implemented\nthrough a simple but powerful permission system.\n12.9 Summary\nThe majority of a system administrator’s job should focus on automating SA tasks.\nA cloud computing system administrator’s goal should be to spend less than half\nthe time doing manual operational work.\n\n\n272\nChapter 12\nAutomation\nTool building optimizes the work done by a system administrator and is an\nimportant step on the way to automation. Automation means replacing a human\ntask with one done by software, often working in partnership with a person.\nAutomation starts by having a documented, well-deﬁned, repeatable process. That\ndocument can be used to create scripts that can be used as tools to speed up the\nprocess and make it more reliable. Finally, the task can be fully automated by cre-\nating a self-service tool or a process that is automatically triggered by changes in a\ndatabase or conﬁguration ﬁle. A self-service tool is particularly useful, as it renders\nothers self-sufﬁcient. When something is automated, the SA’s job changes from\ndoing the task to maintaining the automation that does the task. A more enlight-\nened way of thinking of automation is as a partner for operations rather than a\nreplacement.\nMany levels of automation exist, from fully manual to systems that work\nautonomously without human veto or approval. Different levels are appropriate\ndepending on the task type and risk.\nSAs can choose between scripting languages and compiled languages for cre-\nating automation. Understanding the beneﬁts and disadvantages of each approach\nenables the SA to choose the right tool for the job. Conﬁguration management sys-\ntems are also useful for some forms of automation. Conﬁguration management\ntools use a declarative syntax so that you can specify what the end result should\nbe; the CM tool then ﬁgures out how to bring the system to that state.\nBest practices dictate that all code for automation and conﬁguration ﬁles be\nkept under revision control or in a database that tracks changes. As is true for the\nsoftware development environment, automation tools should not be developed or\ntested in the production environment. Instead, they should be developed in a dedi-\ncated development environment, then staged to a testing area for quality assurance\ntests, before being pushed to the production environment.\nExercises\n1. List ﬁve beneﬁts of automation. Which ones apply to your own environment\nand why?\n2. Document a process that needs automation in your current environment.\nRemember that the document should include every step needed to do the\nprocess!\n3. Describe how to prioritize automation using a decision matrix.\n4. When would you use a scripting language instead of a compiled language,\nand why? In which circumstances would you use a compiled language for\nautomation?\n\n\nExercises\n273\n5. Imagine that you are going to implement a conﬁguration management system\nin your current environment. Which one would you choose and why?\n6. What percentage of your time is spent on one-off tasks and operational work\nthat is not automated? What could you do to change this for the better?\n7. List three challenges that automation can introduce. For each of them, describe\nwhich steps you could take to address that challenge.\n",
      "page_number": 296
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 304-315)",
      "start_page": 304,
      "end_page": 315,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\nChapter 13\nDesign Documents\nBe sincere; be brief; be seated.\n—Franklin D. Roosevelt\nIn this chapter we introduce design documents and discuss their uses and the best\npractices surrounding them. Design documents are written descriptions of pro-\nposed and completed projects, big or small. Design documents serve as a roadmap\nfor your projects and documentation of your accomplishments.\n13.1 Design Documents Overview\nDesign documents are descriptions of proposed or completed projects. They record\nthe goals, the design itself, alternatives considered, plus other details such as cost,\ntimeline, and compliance with corporate policies.\nWriting out what you are going to do forces you to think through the details.\nIt forces you to plan. Having a written document can make design collabora-\ntive when the document becomes a communication vehicle for ideas. Written\ndocuments mean fewer surprises for your teammates, and they help you to get\nconsensus among the team before moving forward. After the project is completed,\nthe design document serves as an artifact that documents the work, a reference for\nthe team.\nA good design document goes into speciﬁc detail about the proposed project\nor change, including information on why choices were made. For instance, the\ndocument might contain detailed descriptions of algorithms, speciﬁc packaging\nparameters, and locations where binary ﬁles are to be installed, and explain why\nthe conﬁg ﬁles are going into /etc instead of elsewhere. A design document about\nnamespace selection, such as the design of a namespace for server/rack names,\nwill not only describe the naming structure, but also give the background on why\nthis namespace was chosen and how it does or does not integrate with existing\nnamespaces.\n275\n\n\n276\nChapter 13\nDesign Documents\nThe document itself is useful for achieving consensus on a project. Some-\ntimes an “obvious” change isn’t really so obvious, and people ﬁnd errors or have\nquestions. As Linus Torvolds said, “Many eyes make all bugs shallow.”\n.\nCase Study: Working Backward at Amazon\nAmazon encourages engineers to start by describing what the customer will\nsee, then work backward to build the design. Engineers start by writing the\npress release that they would like to see announce the product. They design the\nmarketing materials describing how the customer will beneﬁt from the prod-\nuct, identifying the features, and answering FAQs. This develops the vision of\nwhat will be created. Only after this work is ﬁnished is the design created that\nwill achieve the vision. The process is fully described in Black (2009).\n13.1.1 Documenting Changes and Rationale\nDesign documents can also be used to describe changes instead of projects. A short\ndesign document is a good format to get consensus on a small change such as a\nnew new router conﬁguration, a plan to adopt a new feature of a conﬁguration\nmanagement system, or a new naming scheme for a ﬁle hierarchy.\nA design document is created to capture the speciﬁcs of the proposed change,\nand used as a sounding board for team input. If a formal change control process is\nin place, the same document can be included in the change request.\n13.1.2 Documentation as a Repository of Past Decisions\nAn archive of design documents becomes a reference for anyone needing to under-\nstand how things work or how things got the way they are. New team members can\ncome up to speed more quickly if there is documentation they can study. Existing\nteam members ﬁnd it useful to have a reference when they need a quick refresher\non a particular subject. When collaborating with other teams, it is useful, and looks\nmuch more professional, to point people to documents rather than to explain things\nextemporaneously.\nWhen knowledge can be conveyed without requiring personal interaction\nwith an expert, it makes the team more efﬁcient and effective. Teams can grow\nfaster, other people can adopt our service more rapidly, and we have the freedom\nto transfer between teams because knowledge is not locked to one person.\nDesign documents convey not just the design, but also goals and inspirations.\nSince people will refer to design documents for a long time, they are a good place\nto store background and context.\n\n\n13.2\nDesign Document Anatomy\n277\nOften teams retain the history of decisions in their email archive. The problem\nis that email is, by nature, sequestered in a person’s mailbox. New team members\ncannot ﬁnd those emails, nor will they appear in a search of a documents archive.\nMultiply that one simple email by a year’s worth of changes and you have an entire\nset of information that is basically inaccessible to the team.\nDon’t say it; write it down. Procedures, designs, and wisdom count only when\nthey are written down. People can’t refer to your words unless they are in writing.\nIn that regard, email is speech, not writing (Hickstein 2007).\n13.2 Design Document Anatomy\nA design document has many parts. They start with the highest-level information,\ngetting more detailed as the document proceeds. We aid reader comprehension by\nstandardizing the format, headings, and order of these parts.\nAppendix D includes an example of a complete design document in Sec-\ntion D.2. A sample template appears in Section D.1.\nThe sections are:\nTitle: The title of the document.\nDate: The date of the last revision.\nAuthor(s)/Reviewer(s)/Approver(s): Reviewers are people whose feedback is\nrequested. Approvers are people who must approve the document.\nRevision Number: Documents should have revisions numbered like software\nreleases.\nStatus: Status can be draft, in review, approved, or in progress. Some organi-\nzations have fewer or more categories.\nExecutive Summary: A two- to four-sentence summary of the project that\ncontains the major goal of the project and how it is to be achieved.\nGoals (Alternatively, “In Scope”): What is to be achieved by the project,\ntypically presented as a bullet list. Include non-tangible, process goals such as\nstandardization or metrics achievements. It is important to look at your goals\nand cross-check each one to see that your design document addresses each goal.\nIn some organizations, each goal is given a number and the goals are referred\nto by number throughout the document, with each design description citing the\nappropriate goals.\nNon-goals (Alternatively, “Out of Scope”): A bullet list is typically used to\npresent this information. A list of non-goals should explicitly identify what is not\nincluded in the scope for this project. This heads off review comments like “This\nproject doesn’t solve XYZ” because we are communicating here that this project is\nnot intended to solve XYZ.\nBackground: What a typical reader needs to know to be able to understand the\ndesign. This section might provide a brief history of how we got here. Identify any\n\n\n278\nChapter 13\nDesign Documents\nacronyms or unusual terminology used. Document any previous decisions made\nthat have placed limitations or constraints on this project, such as “Our company\npolicy of using only open source software when possible precludes commercial\nsolution XYZ in this project.”\nHigh-Level Design: A brief overview of the design including how it works\nat a high level. The design principles should be described but not necessarily the\nimplementation details.\nDetailed Design: The full design, including diagrams, sample conﬁguration\nﬁles, algorithms, and so on. This will be your full and detailed description of what\nyou plan to accomplish on this project.\nAlternatives Considered: A list of alternatives that were rejected, along with\nwhy they were rejected. Some of this information could have been included\nin the “Background” section, but including it in a speciﬁc location quiets the\ncritics.\nSpecial Constraints: A list of special constraints regarding things like security,\nauditing controls, privacy, and so on. These are sometimes optional. Document\nthings that are part of the process at your company, such as architectural review,\ncompliance checks, and similar activities. You can lump all of the constraints\ntogether in this section or include a section for each of them. Any mandatory pro-\ncess for project review at your company should probably have its own section on\nyour template.\nThe document may have many more sections, some of which may be optional.\nHere are some sections that might be useful in a template:\n• Cost Projections: The cost of the project—both initial capital and operational\ncosts, plus a forecast of the costs to keep the systems running.\n• Support Requirements: Operational maintenance requirements. This ties\ninto Cost Projections, as support staff have salary and beneﬁt costs, hardware\nand licensing have costs, and so on.\n• Schedule: Timeline of which project events happen when, in relation to each\nother.\n• Security Concerns: Special mention of issues regarding security related to\nthe project, such as protection of data.\n• Privacy and PII Concerns: Special mention of issues regarding user privacy\nor anonymity, including plans for handling personally identiﬁable informa-\ntion (PII) in accordance with applicable rules and regulations.\n• Compliance Details: Compliance and audit plans for meeting regulatory\nobligations under SOX, HIPPA, PCI, FISMA, or similar laws.\n• Launch Details: Roll-out or launch operational details and requirements.\n\n\n13.4\nDocument Archive\n279\n13.3 Template\nGiving someone a template to ﬁll out guides that person through the process of\ncreating his or her own design documents better than a step-by-step guide or a\nsample document could ever do. The template should include all headings that\nthe ﬁnal document should have, even optional ones. The template can be annotated\nwith a description of what is expected in each section as well as helpful hints and\ntips. These annotations should be in a different font or color and are usually deleted\nas the user ﬁlls out the template.\nAn example template can be found in Section D.1 of Appendix D. Use it as the\nbasis for your organization’s template.\nThe template should be easy to ﬁnd. When the template is introduced, every-\none should be notiﬁed that it exists and provided with a link to where the template\ncan be found. The link should appear in other places, such at the table of contents\nof the design document archive or another place that people frequently see. Make\nsure the template shows up in intranet searches.\nThe template should be available in all the formats that users will be using,\nor any format with which they feel comfortable. If people use MS-Word, HTML,\nMarkDown (a wiki format), or OpenOfﬁce/LibreOfﬁce, provide the template in all\nof those formats. Providing it in your favorite format only and assuming everyone\nelse can improvise is not making it easy for others to adopt the system.\nIt can also be useful to provide a short template and a long template. The short\ntemplate might be used for initial proposals and small projects. The long template\nmight be used for complex projects or formal proposals.\nThe template should also be easy to use. Anytime you see someone not using\nthe template is an opportunity to debug the problem. Ask them, in a noncon-\nfrontational way, why the template wasn’t used. For example, tell them that you’re\nlooking for feedback about how to get more people to adopt the template and ask\nwhat you could do to have made it easier for them to use it. If they say they couldn’t\nﬁnd the template, ask where they looked for it and make sure all those places are\nupdated to include links.\n13.4 Document Archive\nThere should be a single repository for all design documents. Generally this can\nbe as simple as a list of documents, each linked to the document itself. For the\ndocument archive to be useful to readers, it should be easy for people to ﬁnd the\ndocuments that they are looking for. A simple search mechanism can be useful,\nalthough people will generally be happy if there is one page that lists the titles of\nall documents and that page itself can be searched.\n\n\n280\nChapter 13\nDesign Documents\nIt should be easy to add a design document to the document archive. The\nprocess of adding to or updating the archive should be self-service. If the process\ninvolves emailing a person, then that person will inevitably become a bottleneck.\nSuch an obstacle will discourage people from adding new documents.\nThe easiest way to create an archive is to use a wiki. With this approach,\npeople add links to their documents on a main page. The actual documents are\nthen stored either in the wiki or elsewhere. People will generally add to whatever\nstructure already exists. Consequently, it is important to put time and thought into\nmaking a main page that is simple and easy for others to maintain.\nAnother way to maintain a repository is to create a location for people to add\ndocuments—for example, a source repository or subdirectory on a ﬁle server—and\nwrite scripts that automatically generate the index. Create a system for ﬁle nam-\ning so that the document organization happens automatically. The easiest method\nis for each team to have a unique preﬁx and to let the teams self-select their pre-\nﬁxes. Generating the index requires either a document format that can be parsed\nto extract data or a standardized place for people to list the data that is needed for\nthe index—for example, a YAML or JSON ﬁle that people include along with the\ndocument itself. The scripts then automatically generate an index of all documents\nthat lists the ﬁlename, title, and most recent revision number.\n13.5 Review Workflows\nThere should be a deﬁned way for design documents to be approved. An approval\nprocess does not need to be a high-overhead activity, but rather can be as simple\nas replying to an email containing the design document. People need to know that\ngetting approval does not entail a huge workload or they won’t seek it.\nThere are as many approval workﬂows as there are organizations using design\ndocuments. Most styles fall into one of three main groups:\n• Simple: You create a draft, email it around for comments, and revise it if\nnecessary. No formal approval.\n• Informal: Similar to the simple workﬂow, but with review from project\napprovers or an external review board. This review board exists to give feed-\nback and warn against repeating past mistakes rather than providing a stamp\nof approval.\n• Formal Workflow: Multiple approval stages, including project approvers and\nanexternalreviewboard.Typicalexamplesincludeaﬁrewallorsecurityreview,\na project budget review, and a release schedule review. Each review stage is\nnoted with any comments, and revision numbers or dates track changes.\nDifferent kinds of documents may need different approval workﬂows. Informa-\ntional documents may have a very lightweight or no approval process. More\n\n\n13.5\nReview Workﬂows\n281\nextensive approval may be required depending on the scope or impact of the doc-\nument. Your approval workﬂow is likely to be determined by your change control\nor compliance requirements.\n13.5.1 Reviewers and Approvers\nA design document usually lists authors, reviewers, and approvers. The authors\nare the people who contributed to the document. Reviewers are the people whose\nfeedback is requested. Approvers are the people whose approval is required to\nmove forward.\nReviewers typically include team members who might end up doing the\nactual work of the project. They can also include subject-matter experts and any-\none whose opinion the author wants to seek. Adding someone as a reviewer is\na good way to “FYI” someone in another group that might be impacted by the\nproject work. You should expect meaningful comments from a reviewer, including\npossibly comments that require you to make changes to the document.\nApprovers typically include internal customers who are dependent on the\nresults of the project as well as managers who need to sign off on the resources\nused for the project. Sometimes approvers include other audit and control pro-\ncesses mandated by the company—for example, a privacy assessment or review\nof PII used in the project. People who conduct audits of compliance with regu-\nlations such as SOX, HIPAA, or PCI are also approvers in the design document\nprocess. Approvers are generally a yes-or-no voice, with commentary usually\naccompanying only a “no” decision. It will be up to your individual process\nto determine whether approvals must be unanimous, or whether one or more\nnegative approvals can be overridden by a majority of positive approvals.\n13.5.2 Achieving Sign-off\nThe reason to make a distinction between reviewers and approvers is that\napprovers can block a project. Trying to get consensus from everyone will pre-\nvent any progress, especially if one asks for approval from people who are not\ndirectly affected by the success of a project. People on the perimeter of a project\nshould not be able to block it. Everyone has a voice, but not everyone has a vote.\nThe reviewer/approver distinction clariﬁes everyone’s role.\n.\nMultiple, Highly Specialized Approvals\nThe design documents approval process used at Google includes sign-off from\nthe group managing various centralized services. For example, if the project\nwill be sending data to the centralized log archive, the design document must\n\n\n282\nChapter 13\nDesign Documents\ninclude a section called “Logging” that details how many bytes of log data\neach transaction will generate and justiﬁes the storage and retention policy.\nSign-off by the central log team is required. There are similar sections for\nsecurity, privacy, and other approvers.\nWorking with a review board can be made easier if the board publishes a\nchecklist of items it is looking for in a document. This permits the author to be\nbetter prepared at the review. Publishing the review board checklist is respectful\nof your colleagues’ time as well as your own, as you will see fewer deﬁcient doc-\numents once your checklists are made public. A review board may be a group of\napprovers or of reviewers, or a mix of both—it is in your best interest to determine\nthis ahead of time as you prepare your document.\n13.6 Adopting Design Documents\nImplementing a design document standard within an organization can be a chal-\nlenge because it requires a cultural change. People change slowly. Make it easy\nfor people to adopt the new behavior, be a role model, and enlist management\nsupport.\nProviding a template is the most important thing you can do to drive adoption\nof such a policy. Make sure the template is easy to access and available in the format\npeople most want to use.\nModel the behavior you wish to see in others. Use the design document\ntemplate everywhere that it is appropriate and strictly adhere to the format.\nPeople emulate the behaviors they see successful people demonstrate. Talk\ndirectly with highly respected members of the organization about being early\nadopters. Their feedback on why they are or are not willing to adopt the design\ndocument standard will be enlightening.\nEarly adopters should be rewarded and their success highlighted. This doesn’t\nneed to be a contest with prizes. It can be as simple as saying something positive\nat a weekly staff meeting. Compliment them even if they didn’t use the template\nperfectly; you are rewarding them for taking the ﬁrst steps. Save the suggestions\non how to use the template more properly for one-on-one feedback.\nEnlisting management support is another avenue to adoption. If you are a\nmanager, you have the luxury of making design documents a top-down require-\nment and insisting that your team use them. Another way to start using design\ndocuments is to make them a requirement for certain approval processes, such\nas budget review, security or ﬁrewall review boards, or change control. Avoid\n\n\n13.7\nSummary\n283\nrequiring blind adherence to the template format as a requirement for approvals.\nContent is more important than presentation.\nWhen they see that the template is easy to use, team members will start\nadopting it for other projects.\nUse the template yourself. If the template is not beneﬁting you personally, it\nis probably not beneﬁcial to others. Using it also helps spot problems that often\ncan be ﬁxed as easily as updating the template. When Tom joined Stack Exchange,\none of the ﬁrst things he did was create a design document template and put\nit on the department wiki. He used it for two proposals before mentioning it,\nrevising and perfecting the template each time he used it. Only after it had been\nused a few times did he mention the template to his teammates. Soon others were\nusing it, too.\n13.7 Summary\nA design document serves as a project roadmap and documentation. Its primary\nfunction is to describe a project, but it also communicates key aspects of the project\n(who, what, why, when, and how) to the team. Design documents are useful for\nchange control as well as for project speciﬁcations. Good design documents will\ncapture not only the decisions made but also the reasoning behind the decisions.\nAdopting design documents can pose cultural challenges for a team. Start with\na good template in an easy-to-ﬁnd location, made available in the formats your\nteam uses for documents. Create a central repository for design documents, orga-\nnized in order of most recent updates. Make it easy to add a design document to\nthe repository’s main directory page.\nApprovals are part of good design document culture. Create a draft document\nand solicit comments from some portion of the team. For simple workﬂows, email\nis sufﬁcient to pass the document for review. For more formal workﬂows, there\nmay be mandatory review boards or multiple approval steps. In all workﬂows, the\nversion number of the design document is updated each time review comments\nare added to the document. Ideally, the document repository’s main page will also\nbe updated as revisions occur.\nTemplates should contain the major points of who, what, why, when, and how.\nShorter templates may be used for small or informal projects, with larger, more\ncomplete templates being preferred for complex or formal projects. At a minimum,\nthe template should have title, date, author(s), revision number, status, executive\nsummary, goals, high-level design, and detailed design sections. Alternative or\noptional sections can be added to customize the template based on the needs of\nyour organization.\n\n\n284\nChapter 13\nDesign Documents\nExercises\n1. What are the primary functions of a design document?\n2. List qualities of a good document management system for design documents.\n3. Does your organization currently have a standardized design document for-\nmat? If not, what would be required to implement it in your organization?\n4. Which mandatory processes are part of a design review in your organization?\nWhich stakeholders are mandatory reviewers or approvers?\n5. Describe your organization’s equivalent of design documents, and their\nstrengths and weaknesses when contrasted with the model presented here.\n6. Create a design document template for your organization. Show it to co-\nworkers and get feedback about the format and ways that you can make it\neasier to use.\n7. Write the design for a proposed or existing project using the design document\nformat.\n8. Take an existing document that your organization uses and rewrite it using\nthe design document format.\n9. Use the design document format for something silly or fun—for example, a\ndesign for a company picnic or plans to get rich quick.\n\n\nChapter 14\nOncall\nBe alert... the world\nneeds more lerts.\n—Woody Allen\nOncall is the way we handle exceptional situations. Even though we try to auto-\nmate all operational tasks, there will always be responsibilities and edge cases that\ncannot be automated away. These exceptional situations can happen at any time\nof the day; they do not schedule themselves nicely between the hours of 9 and\n5 .\nExceptional situations are, in brief, outages and anything that, if left unat-\ntended, would lead to an outage. More speciﬁcally, they are situations where the\nservice is, or will become, in violation of the SLA.\nAn operations team needs a strategy to assure that exceptional situations\nare attended to promptly and receive appropriate action. The strategy should be\ndesigned to reduce future reoccurrence of such exceptions.\nThe best strategy is to establish a schedule whereby at any given time at least\none person is responsible for attending to such issues as his or her top priority.\nFor the duration of the oncall shift, that person should remain contactable and\nwithin reach of computers and other facilities required to do his or her job. Between\nexceptions, the oncall person should be focused on follow-up work related to the\nexceptions faced during his or her shift.\nIn this chapter we will discuss this basic strategy plus many variations.\n14.1 Designing Oncall\nOncall is the practice of having a group of people take turns being responsible for\nexceptional situations, more commonly known as emergencies or, less dauntingly,\nalerts. Oncall schedules typically provide 24 × 7 coverage. By taking turns, people\nget a break from such heightened responsibilities, can lead normal lives, and take\nvacations.\n285\n",
      "page_number": 304
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 316-324)",
      "start_page": 316,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": "286\nChapter 14\nOncall\nWhen an alert is received, the person on call responds and resolves the issue,\nusing whatever means necessary to prevent SLA violations, including shortcut\nsolutions that will not solve the problem in the long term. If he or she cannot resolve\nthe issue, there is an escalation system whereby other people become involved.\nAfter the issue is managed, any follow-up work should be done during normal\nbusiness hours—in particular, root causes analysis, postmortems, and working on\nlong-term solutions.\nNormally one person is designated the “oncall person” at any given time. If\nthere is an alert from the monitoring system, that individual receives the alert and\nmanages the issue until it is resolved. During business hours this person works\nas normal, except that he or she always works on projects that can be interrupted\neasily. After normal business hours, the oncall person should be near enough to a\ncomputer so he or she can respond quickly.\nThere also needs to be a strategy to handle the situation when the oncall person\ncannot be reached. This can happen due to commuting, network outages, health\nemergencies, or other issues. Generally a secondary oncall person is designated to\nrespond if the primary person does not respond after a certain amount of time.\n14.1.1 Start with the SLA\nWhen designing an oncall scheme for an organization, begin with the SLA for the\nservice. Work backward to create an SLA for oncall that will result in meeting the\nSLA for the service. Then design the oncall scheme that will meet the oncall SLA.\nFor example, suppose a service has an SLA that permits 2 hours of downtime\nbefore penalties accrue. Suppose also that typical problems can be solved in 30\nminutes, and extreme problems take 30 minutes to cause system failover but usu-\nally only after 30 minutes of trying other solutions. This would mean that the time\nbetween when an outage starts and when the issue is being actively worked on\nmust be less than an hour.\nIn that hour, the following things must happen. First, the monitoring sys-\ntem must detect the outage. If it polls every 5 minutes and alerts only after three\nattempts, a maximum of 15 minutes may pass before someone is alerted. This\nassumes the worst case of the last good poll happening right before the outage.\nLet’s assume that alerts are sent every 5 minutes until someone responds; every\nthird alert results in escalation from primary to secondary or from secondary to\nthe entire team. The worst case (assuming the team isn’t alerted) is six alerts, or\n30 minutes. From receiving the alert, the oncall person may need 5–10 minutes to\nlog into the system and begin working. So far we have accumulated about 50–55\nminutes of outage before “hands on keyboard” has been achieved. Considering we\nestimated a maximum of 60 minutes to ﬁx a problem, this leaves us with 5 minutes\nto spare.\nEvery service is different, so you must do these calculations for each one. If\nyou are managing many services, it can be worthwhile to simplify the process by\n\n\n14.1\nDesigning Oncall\n287\ncreating a few classes of service based on the required response time: 5 minutes,\n15 minutes, 30 minutes, and longer. Monitoring, alerting, and compensation\nschemes for each class can be deﬁned and reused for all new services rather than\nreinventing the wheel each time.\n14.1.2 Oncall Roster\nThe roster is the list of people who take turns being oncall. The list is made up of\nqualiﬁed operations staff, developers, and managers. All operations staff should\nbe on the roster. This is generally considered part of any operations staff member’s\nresponsibility.\nWhen operations staff are new to the team, their training plan should be\nfocused on getting them up to speed on the skills required to be oncall. They should\nfollow or shadow someone who is oncall as part of their training. In most compa-\nnies a new hire should be able to handle oncall duties within three to six months,\nthough this time varies.\nSome organizations do not require senior operations staff to share oncall\nresponsibilities. This is unwise. The senior staff risk becoming disconnected from\nthe operational realities for which they are responsible.\nDevelopers should be on the roster. This gives them visibility into the opera-\ntional issues inherent to the system that they have made and helps guide them in\nfuture design decisions. In other words, their resistance to add the features listed\nin Chapter 2 goes away when the lack of such features affects them, too. It creates\nan incentive to ﬁx operational issues rather than provide workarounds.\nFor example, if a problem wakes up the oncall person at 3 , he or she\nmight use a workaround and ﬁle a bug report to request a more permanent ﬁx,\none that would prevent the oncall person from being woken up at odd hours in\nthe future. Such a bug is easy to ignore by developers if they do not have oncall\nshifts themselves. If they are on the oncall roster, they have an incentive to ﬁx\nsuch bugs.\nIt is important that the priorities of operations and developers are aligned,\nbecause otherwise operational issues will not get the attention they deserve. In\nthe old days of shrink-wrapped software, people accepted that developers were\ndisconnected from operational needs, but those days are long gone. (If you do not\nknow what shrink-wrapped software is, ask your grandparents.)\nTechnical managers, team leads, and technical project managers should also\nshare oncall responsibilities. This keeps them in touch with the realities of opera-\ntions and helps them to be better managers. We also ﬁnd that oncall playbooks and\ntools tend to be held to a higher standard for accuracy and efﬁciency when a wider\ndiversity of talent will be using them. In other words, if your playbook is written\nwell enough that a technical manager can follow it, it’s going to be pretty good. (To\nthe managers reading this: we mean that people keep to higher standards when\nthey know you’ll see the results of their work.)\n\n\n288\nChapter 14\nOncall\n14.1.3 Onduty\nOnduty is like oncall but focuses on dealing with non-emergency requests from\nusers. Onduty is different from oncall.\nMany operations teams also have a queue of requests from users managed\nsimilarly to the way IT helpdesks receive and track requests via a helpdesk automa-\ntion tool. Onduty is a function that assures there is always one person working on\nthese tickets. Otherwise, tickets may be ignored for extended periods of time. Col-\nloquially this function goes by many names: ticket time, ticket week, ticket duty,\nor simply onduty. Generally the onduty roster is made up of the same people who\nare on the oncall roster.\nThe primary responsibility of the onduty person is to respond to tickets within\nthe SLA—for example, 1 business day for initial response; no service at night, week-\nends, and holidays. Usually the onduty person is responsible for triaging new\ntickets, prioritizing them, working most of them, and delegating special cases to\nappropriate people.\nMost organizations have different oncall and onduty schedules. In some orga-\nnizations, whoever is oncall is automatically onduty. This simpliﬁes scheduling\nbut has the disadvantage that if an emergency occurs, tickets will be ignored. That\nsaid, for large emergencies tickets will be ignored anyway. Moreover, if there is\nan emergency that keeps the oncall person awake late at night, the next day that\nperson may not be at his or her best; expecting this individual to respond to tickets\nunder such conditions isn’t a good idea. For that reason, you should try to keep a\nseparate oncall schedule unless ticket load is extremely light.\nSome organizations do not have onduty at all. They may not be in a situa-\ntion where there are users who would ﬁle tickets with them. Some teams feel that\naccepting tickets goes against the DevOps philosophy. They are collaborators, they\nsay, not a service desk that people come to with requests. If there is a request to\ncollaborate, it should be handled through the normal business channels. During\ncollaboration, any tasks assigned should be in the work tracking system just like\nall other project-related tasks.\n14.1.4 Oncall Schedule Design\nThere are many variations on how to structure the oncall schedule. The duration\nof a person’s oncall shift should be structured in a way that makes the most sense\nfor your team. Here are some variations that are commonly used:\n• Weekly: A person is oncall for one week at a time. The next shift starts the\nsame time each week, such as every Wednesday at noon. Having the change\noccur mid-week is better than during the weekend. If the change happens on a\nWednesday, each onduty person has one complete weekend where travel and\n\n\n14.1\nDesigning Oncall\n289\nother fun are limited. If the transition happens on Saturday, then it ruins the\nweekend of both oncall people for two weekends in a row. On Mondays, there\nmay be a backlog of follow-up work from the weekend. It is best to let the per-\nson who handled the weekend alerts complete the follow-up work, or at least\ngenerate the appropriate tickets and documentation, while still oncall. This\nenables a clean handover and should allow each person to return to project\nwork as quickly as possible after oncall duty ends.\n• Daily: A person is oncall for one day at a time. This may seem better than\na weekly schedule because the shift is not as long, but it means being oncall\nmuch more often. A weekly schedule might mean being oncall one week out of\nevery six. With a small team, a daily schedule might mean being oncall every\nsix days, never having a complete week to take a vacation.\n• Split Days: On a given day multiple people are oncall, each one responsible\nfor a different part of the day or shift. For example, a two-shift schedule might\ninvolve two 12-hour shifts per day. One person works 9 to 9 and another\nis oncall for the overnight. This way, if an alert happens in each shift, someone\nis always able to sleep. A three-shift schedule might be 8 hours each: 9 to\n5 , 5 to 1 , and 1 to 9 .\n• Follow the Sun: Members of the operations team live in different time zones,\nand each is oncall for the hours that he or she would normally be awake (sun-\nlight hours). If the team resides in California and Dublin, a shift change at\n10 and 10 California time means all members have some responsibil-\nities during ofﬁce hours and sleeping hours, plus there are enough overlap\nhours for inter-team communication. A team split between many time zones\nmay have three or four shifts per day.\nMany variations are also possible. Some teams prefer half-weeks instead of full\nweeks. Follow the sun can be done with with two, three, or four shifts per day\ndepending on where people are located.\nAlert Frequency\nWith so many variations, it can be difﬁcult to decide which to use. Develop a\nstrategy where the frequency and duration of oncall shifts are determined by how\nfrequent alerts are. People do their best oncall work when they are not overloaded\nand have not been kept awake for days on end. An oncall system improves oper-\nations when each alert receives follow-up work such as causal analysis. Before\nsomeone goes oncall again, that person should have had enough time to complete\nall follow-up work.\nFor example, if alerts are extremely rare, occurring less than once a week, a\nsingle person being oncall for 7 days at a time is reasonable. If there are three\nalerts each day, two or three shifts gives people time to sleep between alerts and do\n\n\n290\nChapter 14\nOncall\nfollow-up tasks. If follow-up work is extensive, half-week rotations of two shifts\nmay be required.\nIf there is a signiﬁcantly high alert ratio, more complex schemes are needed.\nFor example, in some schedules, two people are oncall: one receives the alert if the\nother is busy, plus two secondaries shadow the primaries. Some systems divide the\nwork geographically, with a different rotation scheme and schedule for each major\nregion of the world.\nSchedule Coordination\nOncall schedules may be coordinated with other schedules. For example, it may be\na guideline that the week before your oncall shift, you are onduty. There may be a\ndedicated schedule of escalation points. For example, people knowledgeable in a\nparticular service may coordinate to make sure that they aren’t all on vacation at the\nsame time. That ensures that if oncall needs to escalate to them, someone is avail-\nable. Sometimes an escalation schedule is an informal agreement, and sometimes\nit is a formal schedule with an SLA.\nCompensation\nCompensation drives some design elements of the schedule. In some countries,\nbeing oncall requires compensation if response time is less than a certain interval.\nThe compensation is usually a third of the normal hourly salary for any hour oncall\noutside of normal business hours. It may be paid in cash or by giving the oncall\nperson time off. Compensation rates may be different if the person is called to\naction. Your human resources department should be able to provide all the details\nrequired. In some countries, there is no legal obligation for oncall compensation\nbut good companies do it anyway because it is unethical otherwise. One beneﬁt of\nfollow-the-sun coverage is that it can be constructed in a way that maximizes time\noncall during normal business hours for a location, while minimizing the amount\nof additional compensation that needs to be budgeted.\n14.1.5 The Oncall Calendar\nThe oncall calendar documents who is oncall when. It turns the theory of the\nschedule and roster into speciﬁcs. The monitoring system uses this information\nto decide who to send alerts to.\nSet the calendar far enough ahead to permit all concerned to plan vacations,\ntravel, and other responsibilities in advance. Three to six months is usually sufﬁ-\ncient. The details of building the calendar are as varied as there are teams. A team of\nsix that changes the oncall person every Wednesday may simply use the “repeating\nevent” functionality of an online calendar to schedule who is oncall when. Conﬂicts\nand other issues can be worked out between members.\n\n\n14.1\nDesigning Oncall\n291\nA more complex schedule and a larger team require proportionately more\ncomplex calendar building strategies. One such system used a shared, online\nspreadsheet such as Google Drive. The spreadsheet cells represented each time slot\nfor the next three months. Due to the size of the team, everyone was expected to\ntake three time slots. The system was “ﬁrst come, ﬁrst served,” and there was a lot\nof back-channel discussion that enabled people to trade time slots. The negotiating\ncontinued until a certain cut-off point, at which time the schedule was locked. This\nsystem was unfair to people who happened to be out the day the schedule was\nmade.\nSome companies take a more algorithmic approach. Google had hundreds of\nindividual calendars to create for any given month due to the existence of many\ninternal and external services. Each team spent a lot of time negotiating and assem-\nbling calendars until someone wrote a program that did the task for them. To use\nthe system, a team would create a Google Calendar and everyone inserted events to\nmark which days they were entirely unavailable, available but not preferred, avail-\nable, or preferred. The system took a conﬁguration ﬁle that described parameters\nsuch as how long each shift was, whether there was a required gap of time before\nsomeone could have another rotation, and so on. The system then read people’s\npreferences from the Google Calendar and churned on the data until a reasonable\noncall calendar was created.\n14.1.6 Oncall Frequency\nThe frequency of how often a person goes oncall needs careful consideration. Each\nalert has a certain amount of follow-up work that should be completed before the\nnext turn at oncall begins. Each person should also have sufﬁcient time between\noncall shifts to work on projects, not just follow-up work.\nThe follow-up work from an alert can be extensive. Writing a postmortem can\nbe an arduous task. Root cause analysis can involve extensive research that lasts\ndays or weeks.\nThe longer the oncall shift, the more alerts will be received and the more\nfollow-up projects the person will be trying to do at the same time. This can\noverload a person.\nThe more closely the shifts are spaced, the more likely the work will not be\ncompleted by the time the next shift starts.\nDoing one or two postmortems simultaneously is reasonable, but much more\nis impossible. Therefore shifts should be long enough that only one or two signif-\nicant alerts have accumulated. Depending on the service, this may be one day, a\nweek of 8-hour periods, or a week of 24 × 7 service. The next such segment should\nbe spaced at least three weeks apart if the person is expected to complete both\npostmortems, do project work, and be able to go on an occasional vacation. If a\n\n\n292\nChapter 14\nOncall\nservice receives so many alerts that this is not possible, then the service has deeper\nissues.\nOncall shifts can be stressful. If the source of stress is that the shift is too busy,\nconsider using shorter shifts or having a second person oncall to handle overﬂow.\nIf the source of stress is that people do not feel conﬁdent in their ability to handle\nthe alerts, additional training is recommended. Ways to train a team to be more\ncomfortable dealing with outage situations are discussed in Chapter 15.\n14.1.7 Types of Notifications\nThere are many levels of urgency at which monitoring and other services need to\nraise the attention of human operators. Only the most urgent is an alert.\nEach level of urgency should have its own communication method. If urgent\nalerts are simply sent to someone’s email inbox, they may not be noticed in time. If\nnon-urgent messages are communicated by sending an SMS to the person oncall,\nthe “Boy Who Cried Wolf” syndrome will develop.\nThe best option is to build a very high-level classiﬁcation system:\n• Alert Oncall: The SLA is in violation, or if a condition is detected that, if left\nunattended, will result in an SLA violation.\n• Create a Ticket: The issue needs attention within one business day.\n• Log to a File: The condition does not require human attention. We do not want\nto lose the information, but we do not need to be notiﬁed.\n• Do Nothing: There is no useful information; nothing should be sent.\nIn some organizations, all of these situations are communicated by email to the\nentire system administration team. Under these conditions, all team members\nmight be compelled to ﬁlter all messages to a folder that is ignored. This defeats\nthe purpose of sending the messages in the ﬁrst place.\nEmail is, quite possibly, the worst alerting mechanism. Expecting someone to\nsit and watch an email inbox is silly, and a waste of everyone’s time. With this\nstrategy, staff will be unaware of new alerts if they step away or get involved in\nother projects.\nDaily emails that report on the result of a status check are also a bad idea.\nIf the status is ﬁne, log this fact. If a problem was detected, automatically open a\nticket. This prevents multiple people from accidentally working on the same issue\nat the same time. If the problem is urgent enough that someone should be alerted\nimmediately, then why is the check being done only once per day? Instead, report\nthe status to the monitoring system frequently and send alerts normally.\nHowever, it is a good idea to use email as a secondary mechanism. That is,\nwhen sending a message to a pager or creating a ticket, also receiving a copy via\n\n\n14.1\nDesigning Oncall\n293\nemail is useful. Many systems have mechanisms to subscribe to such messages in a\nway that permits precise ﬁltering. For example, it is usually possible to conﬁgure a\nticket system to email notiﬁcations of new or updated tickets in a particular queue.\nOnce an alert is triggered, there are many ways to notify the person who is\noncall. The most common alert methods are identiﬁed here:\n• One-Way and Two-Way Pagers: Hand-held devices that receive text mes-\nsages via terrestrial broadcasts. Two-way pagers permit sending a reply to\nacknowledge that the message was received.\n• SMS or Text Message to a Mobile Phone: Sending a text or SMS message to\na person’s mobile phone is convenient because most people already carry a\nmobile phone. In some countries, pagers are signiﬁcantly more reliable than\nSMS; in others, the reverse is true. If you are creating an alerting system for\nco-workers in another country, do not assume that what works well for you\nwill be viable elsewhere. Local people should test both.\n• Smart Phone App: Smart phone apps are able to display additional informa-\ntion beyond a short text message. However, they often depend on Internet\nconnectivity, which may not always be available.\n• Voice Call: A voice synthesizer and other software is used to call a per-\nson’s phone and talk to him or her, asking the person to press a button to\nacknowledge the message (otherwise, the escalation list will be activated).\n• Chat Room Bot: A chat room bot is a software robot that sits in the team’s chat\nroom and announces any alerts. This is a useful way to keep the entire team\nengaged and ready to help the oncall person if needed.\n• Alerting Dashboard: The alerting dashboard is a web page that shows the\nhistory of alerts sent. It provides useful context information.\n• Email: Email should never be the only way the oncall person is alerted. Sitting\nat your computer watching your inbox is a terrible use of your time. Never-\ntheless, it is useful to have every alert emailed to the oncall person as a backup\nmethod. This way the full message is received; SMS truncates messages to 160\ncharacters.\nAt least two methods should be used to make sure the message gets through.\nA pager service might have an outage. A sleeping oncall person might require an\nSMS tone and a voice call to wake him or her. One method should communicate\nthe complete, non-truncated, message to a stable storage medium. Email works\nwell for this. Lastly, chat room bots and other methods should also be deployed,\nespecially given that this strategy enables the entire team to stay aware of issues\nas they happen. They may seem like novelties at ﬁrst but they quickly become\nindispensable.\n\n\n294\nChapter 14\nOncall\n14.1.8 After-Hours Maintenance Coordination\nSometimes maintenance must be done after hours. For example, another team may\ndo maintenance that requires your participation, such as failing over services so\nthey are not affected by scheduled outages, or being around to verify status, and\nso on.\nIt is very common to assign such tasks to whoever will be oncall at the time.\nFor most situations, this is ﬁne. However, it does create the possibility that an oncall\nperson might be dealing with an alert and coordinating an unrelated issue at the\nsame time.\nIf the oncall person is used for such tasks, the secondary oncall person should\ntake any alerts that happen during the maintenance window. Alternatively, assign\nthe maintenance window coordination to the secondary person.\n14.2 Being Oncall\nNow that we know how to develop a roster, schedule, and calendar, we can\nconsider the responsibilities associated with being oncall. An oncall person has\nresponsibilities before, during, and after each shift.\n14.2.1 Pre-shift Responsibilities\nBefore a shift begins, you should make sure you are ready. Most teams have an\noncall shift preparedness checklist. Items on the checklist verify reachability and\naccess. Reachability means that the alert notiﬁcation system is working. You might\nsend a test alert to yourself to verify that everything is working. Access means\nthat you have access to the resources needed when responding to an alert: your\nVPN software works, your laptop’s batteries are charged, you are sober enough to\nperform operations, and so on.\nCorrecting any problems found may take time. Therefore the checklist should\nbe activated appropriately early, or early enough to negotiate extending the current\noncall person’s shift or ﬁnding a replacement. For example, discovering that your\ntwo-factor authenticator is not working may require time to set up a new one.\n14.2.2 Regular Oncall Responsibilities\nOnce the shift begins you should do…nothing special. During working hours you\nshould work as normal but take on only tasks that can be interrupted if needed. If\nyou attend a meeting, it is a good idea to warn people at the start that you are oncall\nand may have to leave at any time. If you are oncall outside of normal working\nhours, you should sleep as needed and basically live a normal life, other than being\naccessible. If travel is required, such as to go home from work, establish temporary\ncoverage from your secondary oncall person for the duration of the drive.\n",
      "page_number": 316
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 325-333)",
      "start_page": 325,
      "end_page": 333,
      "detection_method": "topic_boundary",
      "content": "14.2\nBeing Oncall\n295\nWhat you should not do during oncall is sit at your computer watching your\nservice dashboards and monitoring systems to see if you can spot problems. This\nis a waste of time and is exactly what monitoring systems are made for. If this kind\nof activity is a requirement of the oncall shift, then your oncall system has been\npoorly designed.\nSome teams have a list of tasks that are done during each shift. Some example\ntasks include verifying the monitoring system is working, checking that backups\nran, and checking for security alerts related to software used in-house. These tasks\nshould be eliminated through automation. However, until they are automated,\nassigning responsibility to the current oncall person is a convenient way to spread\nthe work around the team. These tasks are generally ones that can be done between\nalerts and are assumed to be done sometime during the shift, though it is wise to\ndo them early in the shift so as not to forget them. However, if a shift starts when\nsomeone is normally asleep, expecting these tasks to be done at the very start of\nthe shift is unreasonable. Waking people up for non-emergencies is not healthy.\nTasks may be performed daily, weekly, or monthly. In all cases there should\nbe a way to register that the task was completed. Either maintain a shared spread-\nsheet where people mark things as complete, or automatically open a ticket to be\nclosed when the task is done. All tasks should have an accompanying bug ID that\nrequests the task be eliminated though automation or other means. For example,\nverifying that the monitoring system is running can be automated by having a sys-\ntem that monitors the monitoring system. (See Section 16.5, “Meta-monitoring.”)\nA task such as emptying the water bucket that collects condensation from a tem-\nporary cooling device should be eliminated when the temporary cooling system is\nﬁnally replaced.\nOncall should be relatively stress-free when there is no active alert.\n14.2.3 Alert Responsibilities\nOnce alerted, your responsibilities change. You are now responsible for verifying\nthe problem, ﬁxing it, and ensuring that follow-up work gets completed. You may\nnot be the person who does all of this work, but you are responsible for making\nsure it all happens through delegation and handoffs.\nYou should acknowledge the alert within the SLA described previously.\nAcknowledging the alert tells the alerting system that it should not try to alert\nthe next contact on the escalation list.\nQuick Fixes versus Long-Term Fixes\nNow the issue is worked on. Your priority is to come up with the best solution that\nwill resolve the issue within the SLA. Sometimes we have a choice between a long-\nterm ﬁx and a quick ﬁx. The long-term ﬁx will resolve the fundamental problem\nand prevent the issue in the future. It may involve writing code or releasing new\nsoftware. Rarely can that be done within the SLA. A quick ﬁx ﬁts within the SLA\n\n\n296\nChapter 14\nOncall\nbut may simply push the issue farther down the road. For example, rebooting a\nmachine may ﬁx the problem for now but will require rebooting it again in a few\ndays because the technical problem was not ﬁxed. However, the reboot can be done\nnow and will prevent an SLA violation.\nIn general, encourage a bias toward long-term ﬁxes over quick ﬁxes: “a stitch\nin time saves nine.” However, oncall is different from normal engineering. Oncall\nplaces a higher priority on speed than on long-term perfection. Since solutions that\ndo not ﬁt within the SLA must be eliminated, a quick ﬁx may be the only option.\nAsking for Help\nIt is also the responsibility of the oncall person to ask for help when needed. Esca-\nlate to more experienced or knowledgable people, or if the issue was raised long\nenough ago, ﬁnd someone who is better rested than you are. You don’t have to save\nthe world single-handedly. You are allowed to call other folks for help. Reach out to\nothers especially if the outage is large or if there are multiple alerts at the same time.\nYou don’t have to ﬁx the problem yourself necessarily. Rather, it is your respon-\nsibility to make sure it gets ﬁxed, which sometimes is best done by looping in the\nright people and coordinating rather than trying to handle everything yourself.\nFollow-up Work\nOnce the problem has been resolved, the priority shifts to raising the visibility of\nthe issue so that long-term ﬁxes and optimizations will be done. For simple issues,\nit may be sufﬁcient to ﬁle a bug report or add annotations to an existing one. More\ncomplex issues require writing a postmortem report that captures what happened\nand makes recommendations about how it can be prevented in the future. By doing\nthis we build a feedback loop that assures operations get better over time, not\nworse. If the issue is not given visibility, the core problem will not be ﬁxed. Do\nnot assume that “everyone knows it is broken” means that it will get ﬁxed. Not\neveryone does know it is broken. You can’t expect that managers who prioritize\nwhich projects are given resources will know everything or be able to read your\nmind. Filing bug reports is like picking up litter: you can assume someone else\nwill do it, but if everyone did that nothing would ever be clean.\nOnce the cause is known, the alert should be categorized so that metrics can be\ngenerated. This helps spot trends and the resulting information should be used to\ndetermine future project priorities. It is also useful to record which machines were\ninvolved in a searchable way. Future alerts can then be related to past ones, and\nsimple trends such as the same machine failing repeatedly can be spotted.\nOther follow-up tasks are discussed in Section 14.3.\n14.2.4 Observe, Orient, Decide, Act (OODA)\nThe OODA loop was developed for combat operations by John Boyd. Designed\nfor situations like ﬁghter jet combat, it ﬁts high-stress situations that require quick\n\n\n14.2\nBeing Oncall\n297\nresponses. Kyle Brandt (2014) popularized the idea of applying OODA to system\nadministration.\nSuppose the alert relates to indicators that your web site is slow and\noften timing out. First we Observe: checking logs, reading I/O measurements,\nand so on.\nNext we Orient ourselves to the situation. Orienting is the act of analyzing\nand interpreting the data. For example, logs contain many ﬁelds, but to turn that\ndata into information the logs need to be queried to ﬁnd anomalies or patterns. In\nthis process we come up with a hypothesis based on the data and our experience\nto ﬁnd the real cause.\nNow we Decide to do something. Sometimes we decide that more informa-\ntion is needed and begin to collect it. For example, if there are indications that the\ndatabase is slow, then we collect more speciﬁc diagnostics from the database and\nrestart the loop.\nThe last stage is to Act and make changes that will either ﬁx the problem, test\na hypothesis, or give us more data to analyze. If you decide that certain queries\nare making the database server slow, eventually someone has to take action to ﬁx\nthem.\nThe OODA loop will almost always have many iterations. More experienced\nsystem administrators can iterate through the loop logically, rapidly, and smoothly.\nAlso, over time a good team develops tools to make the loop go faster and gets\nbetter at working together to tighten the loop.\n14.2.5 Oncall Playbook\nIdeally, every alert that the system can generate will be matched by documentation\nthat describes what to do in response. An oncall playbook is this documentation.\nThe general format is a checklist of things to check or do. If the end of the list\nis reached, the issue is escalated to the oncall escalation point (which itself may be\na rotation of people). This creates a self-correcting feedback loop. If people feel that\nthere are too many escalations waking up them late at night, they can correct the\nproblem by improving the documentation to make oncall more self-sufﬁcient.\nIf they feel that writing documentation is unimportant or “someone else’s job,”\nthey can, by virtue of not creating proper checklists, give oncall permission to wake\nthem up at all hours of the night. It is impressive how someone who feels that\nwriting documentation is below them suddenly learns the joy of writing after being\nwoken up in the middle of the night. The result of this feedback loop is that each\nchecklist becomes as detailed as needed to achieve the right balance.\nWhen writing an oncall playbook, it can be a challenge to determine how\ndetailed each checklist should be. A statement like “Check the status of the\ndatabase” might be sufﬁcient for an experienced person. The actual steps required\nto do that, and instructions on what is considered normal, should be included. It\nis usually too much detail to explain very basic information like how to log in.\n\n\n298\nChapter 14\nOncall\nIntentionally select a baseline of knowledge that is assumed. There should be\ndocumentation that will take a new employee and bring him or her up to that level.\nThe oncall documents, then, can assume that level of knowledge. This also helps\nprevent the situation where documentation becomes too verbose and repetitive.\nRequiring authors to include too much detail can become an impediment to writing\ndocumentation at all.\n14.2.6 Third-Party Escalation\nSometimes escalations must include someone outside the operations team, such as\nthe oncall team of a different service or a vendor. Escalating to a third party has\nspecial considerations.\nDuring an outage, one should not have to waste time researching how to con-\ntact the third party. All third-party dependencies should be documented. There\nshould be a single globally accessible list that has the oncall information for each\ninternal team. If the oncall numbers change with each shift, this list should be\ndynamically generated with the current information.\nLists of contact information for vendors usually contain information that\nshould not be shared outside of the team and, therefore, is stored differently. For\neach dependency, this private list should include the vendor name, contact infor-\nmation, and anything needed to open a support issue. For example, there may be\nlicense information or support contract numbers.\n.\nTips for Managing Vendor Escalations\nSometimes support cases opened with a vendor remain open for many days.\nWhen this happens:\n• One person should interface with the vendor for the duration of the issue.\nOtherwise, communication and context can become disorganized.\n• Trust but verify. Do not let a vendor close the case until you have veriﬁed\nthe issue is resolved.\n• Report the issue to your sales contact only if you are not getting results.\nOtherwise, the sales representative may meddle, which is annoying to\ntechnical support people.\n• Take ownership of the issue. Do not assume the vendor will. Be the person\nwho follows up and makes sure the process keeps moving.\n• When possible, make follow-up calls early in the day. This sets up the\nvendor to spend the day working on your issue.\n\n\n14.3\nBetween Oncall Shifts\n299\n14.2.7 End-of-Shift Responsibilities\nEventually your shift will end and it will be time to hand off oncall to the next\nperson. Making sure the transition goes well ensures that context is transferred to\nthe next person and open issues are not forgotten.\nOne strategy is to write an end-of-shift report that is emailed to the entire\noncall roster. Sending it to just the next person oncall is vulnerable to error. You\nmay pick the wrong person by mistake, or may not know about a substitution that\nwas negotiated. Sending the report to everyone keeps the entire team up-to-date\nand gives everyone an opportunity to get involved if needed.\nThe end-of-shift report should include any notable events that happened and\nanything that the next shift needs to know. For example, it should identify an\nongoing outage or behaviors that need manual monitoring.\nAnother strategy is to have an explicit handoff to the next shift. This means the\noutgoing person must explicitly state that he or she is handing off responsibility to\nthe next person, and the next person must positively acknowledge the handoff.\nIf the next person does not or cannot acknowledge the handoff, then responsi-\nbility stays with the original person. This technique is used in situations where\navailability requirements are very strict.\nIn this case the end-of-shift report may be verbal or via email. A written report\nis better because it communicates to the entire team.\nIf shift change happens when one or both parties may be asleep, a simple email\nmay be sufﬁcient if there are no ongoing issues. If there are ongoing issues, the next\noncall person should be alerted. If the outgoing person will be asleep when the\nshift changes, it is common practice to send out a provisional end-of-shift report,\nnoting that unless there are alerts between now and the shift change, this report\ncan be considered ﬁnal.\n14.3 Between Oncall Shifts\nThe normal working hours between shifts should be spent on project work. That\nincludes follow-up tasks related to oncall alerts. While such work can be done\nduring oncall time, sleep is usually more important.\nProjects related to oncall include working on the long-term solutions that\nweren’t possible to implement during oncall and postmortem reports.\n14.3.1 Long-Term Fixes\nEach alert may generate follow-up work that cannot be done during the oncall\nperiod. Large problems may require a causal analysis to determine the root cause\nof the outage.\n\n\n300\nChapter 14\nOncall\nIn our previous example, a problem was solved by rebooting a machine.\nCausal analysis might indicate that the software has a memory leak. Working with\nthe developers to ﬁnd and ﬁx the memory leak is a long-term solution.\nEven if you are not the developer who will ultimately ﬁx the code, there is\nplenty of work that can be done besides hounding the developers who will pro-\nvide the ﬁnal solution. You can set up monitoring to collect information about the\nproblem, so that before and after comparisons can be made. You can work with\nthe developers to understand how the issue is affecting business objectives such as\navailability.\n14.3.2 Postmortems\nA postmortem is a process that analyzes an outage and documents what happened\nand why, and makes recommendations about how to prevent that outage in the\nfuture.\nA good postmortem process communicates up and down the management\nchain. It communicates to users that action is being taken. It communicates to peer\nteams so that interactions (good and bad) are learned. It can also communicate to\nunrelated teams so they can learn from your problems.\nThe postmortem process should not start until after the outage is complete. It\nshould not be a distraction from ﬁxing the outage.\nA postmortem is part of the strategy of continuous improvement. Each user-\nvisible outage or SLA violation should be followed by a postmortem and conclude\nwith implementation of the recommendations in the postmortem report. By doing\nso we turn outages into learning, and learning into action.\nPostmortem Purpose\nA postmortem is not a ﬁnger-pointing exercise. The goal is to identify what went\nwrong so the process can be improved in the future, not to determine who is to\nblame. Nobody should be in fear of getting ﬁred for having their name associated\nwith a technical error. Blame discourages the kind of openness required to have\nthe transparency that enables problems to be identiﬁed so that improvements can\nbe made. If a postmortem exercise is a “name and shame” process, then engineers\nbecome silent on details about actions and observations in the future. “Cover your\nass” (CYA) behavior becomes the norm. Less information ﬂows, so management\nbecomes less informed about how work is performed and other engineers become\nless knowledgeable about pitfalls within the system. As a result, more outages\nhappen, and the cycle begins again.\nThe postmortem process records, for any engineers whose actions have con-\ntributed to the outage, a detailed account of actions they took at the time, effects\nthey observed, expectations they had, assumptions they had made, and their\nunderstanding of the timeline of events as they occurred. They should be able to\ndo this without fear of punishment or retribution.\n\n\n14.3\nBetween Oncall Shifts\n301\nThis is not to say that staff members get off the hook for making mistakes.\nThey are on the hook for many things. They are now the experts responsible for\neducating the organization on how not to make that mistake in the future. They\nshould drive engineering efforts related to improving the situation.\nA culture of accountability, rather than blame, fosters an organization that\nvalues innovation. If blame is used to avoid responsibility, the whole team suf-\nfers. For more information about this topic, we recommend Allspaw’s (2009) article\n“Blameless Postmortems and a Just Culture.”\n.\nA Postmortem Report for Every High-Priority Alert\nAt Google many teams had a policy of writing a postmortem report every time\ntheir monitoring system paged the oncall person. This was done to make sure\nthat no issues were ignored or “swept under the rug.” As a result there was\nno back-sliding in Google’s high standards for high uptime. It also resulted\nin the alerting system being highly tuned so that very few false alarms were\ngenerated.\nPostmortem Report\nPostmortem reports include four main components: a description of the outage,\na timeline of events, a contributing conditions analysis (CCA), and recommenda-\ntions to prevent the outage in the future. The outage description should say who\nwas affected (for example, internal customers or external customers) as well as\nwhich services were disrupted. The timeline of events may be reconstructed after\nthe fact, but should identify the sequence of what actually happened and when\nso that it is clear. The CCA should go into detail as to why the outage occurred\nand include any signiﬁcant context that may have contributed to the outage (e.g.,\npeak service hours, signiﬁcant loads). Finally, the recommendations for prevention\nin the future should include a ﬁled ticket or bug ID for each recommendation in\nthe list.\nYou will ﬁnd a sample postmortem template in Section D.3 of Appendix D. If\nyour organization does not have a postmortem template, you can use this as the\nbasis for yours.\nThe executive summary should include the most basic information of when\nthe incident happened and what the root causes were. It should reiterate any rec-\nommendations that will need budget approval so that executives can connect the\nbudget request to the incident in their mind.\nCausal analysis or contributing conditions analysis ﬁnds the conditions that\nbrought about the outage. It is sometimes called root cause analysis but that\nimplies that outages have only one cause. If tradition or ofﬁce politics requires\n\n\n302\nChapter 14\nOncall\nusing the word “root,” at least call it a root causes analysis to emphasize that there\nare many possible causes.\nWhile emotionally satisfying to be able to point to a single cause, the reality\nis that there are many factors leading up to an outage. The belief that an outage\ncould have a single cause implies that operations is a series of dominos that topple\none by one, leading up to an outage. Reality is much more complex. As Allspaw’s\n(2012a) article “Each Necessary, But Only Jointly Sufﬁcient” points out, ﬁnding the\nroot cause of a failure is like ﬁnding the root cause of a success.\nPostmortem Communication\nOnce the postmortem report is complete, copies should be sent to the appropriate\nteams, including the teams involved in ﬁxing the outage and the people affected\nby the outage. If the users were external to the company, a version with proprietary\ninformation removed should be produced. Be careful to abide by your company’s\npolicy about external communications. The external version may be streamlined\nconsiderably. Publishing postmortems externally builds customer conﬁdence, and\nit is a best practice.\nWhen communicating externally, the postmortem report should be accompa-\nnied by an introduction that is less technical and highlights the important details.\nMost external customers will not be able to understand a technical postmortem.\nInclude speciﬁc details such as start and end times, who or what was impacted,\nwhat went wrong, and what were the lessons learned. Demonstrate that you are\nusing the experience to improve in the future. If possible, include human elements\nsuch as heroic efforts, unfortunate coincidences, and effective teamwork. You may\nalso include what others can learn from this experience.\nIt is important that such communication be authentic, admit failure, and sound\nlike a human, not a press agent. Figure 14.1 is an example of good external commu-\nnication. Notice that it is written in the ﬁrst person, and contains real remorse—no\nhiding here. Avoid the temptation to hide by using the third person or to mini-\nmize the full impact of the outage by saying something like “We regret the impact\nit may have had on our users and customers.” Don’t regret that there may have\nbeen impact. There was impact—otherwise you wouldn’t be sending this mes-\nsage. More advice can be found in the blog post “A Guideline for Postmortem\nCommunication” on the Transparent Uptime blog (Rachitsky 2010).\n14.4 Periodic Review of Alerts\nThe alert log should be reviewed periodically to spot trends and allocate resources\nto create long-term ﬁxes that ultimately reduce the total number of alerts received.\nWhen this strategy is implemented, alerts become more than just a way to be made\naware of problems—they become one of your primary vehicles for improving\nsystem stability.\n\n\n14.4\nPeriodic Review of Alerts\n303\n.\nSubject: CloudCo Incident Report for February 4, 2014; brief interruption in DNS service\nLast week on Tuesday, February 4, CloudCo experienced an outage of our DNS service lasting\napproximately 4 minutes. As a result of this outage, our customers experienced 4 minutes of\ndowntime as we worked to restore full service. I would like to apologize to our customers for the\nimpact to your daily operations as a result of this outage. Unplanned downtime of any length is\nunacceptable to us. In this case we fell short of both our customers’ expectations and our own.\nFor that, I am truly sorry.\nI would like to take a moment and explain what caused the outage, what happened during the\noutage, and what we are doing to help prevent events like this in the future.\nTechnical Summary\nCloudCo experienced a DNS outage lasting 4 minutes. The issue was the result of accidental\ninput into a DNS rate-limiting tool, which resulted in a rate limit of zero queries per second being\nset. The problem was recognized by the Network Operations team immediately, the rate limit\nwas identiﬁed within 2 minutes, and the conﬁguration was reverted the following minute. DNS\nservice was returned to normal status on a network-wide basis within 4 minutes of the beginning\nof the incident.\nIncident Description\nAt 17:10 Paciﬁc Standard Time (01:10 UTC), the CloudCo DNS service was not responding to\nDNS queries. All DNS queries to CloudCo DNS were being dropped during the 4 minutes the\nrate limit was applied. This affected all DNS queries sent to BIND.\nTimeline of Events – February 4, 2014\n17:10 PST (01:10 UTC) - An erroneous rate limit was applied to CloudCo DNS service\n17:11 PST - Internal automated alerts\n17:13 PST - Issue discovered to be with DNS rate limit\n17:14 PST - Erroneous DNS rate limit removed\n17:14 PST - CloudCo DNS conﬁrmed returned to normal status\n17:17 PST - CloudCoStatus tweets status update\nResolution\nThe erroneous change was made, recognized, and corrected within 4 minutes.\nRecommendations\n• Internal process revision for applying global DNS rate limits, with peer review.\n• Implement additional software controls to validate user input for tools.\nFigure 14.1: Example postmortem email sent to customers after an outage\nThere should be a systematic approach to reduce the number of alerts or\nentropy is likely to make alerts more frequent over time until the volume of alerts\nspirals out of control. Alerts should also be analyzed for trends because each alert\nis a potential indicator of a larger issue. If we aim to improve our system continu-\nously, we must take every opportunity to seek out clues to where improvements\nare needed.\nIt’s useful to have a weekly meeting to review alerts and issues and look\nfor trends. At this meeting, you can identify projects that would ﬁx more com-\nmon issues as well as take an overall snapshot of the health of your production\n",
      "page_number": 325
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 334-341)",
      "start_page": 334,
      "end_page": 341,
      "detection_method": "topic_boundary",
      "content": "304\nChapter 14\nOncall\nenvironment. Quarterly reviews can be useful to spot even larger trends and can\nbe folded into quarterly project planning cycles.\nThe alert log should be annotated by the person who received the alert. Most\nsystems permit alerts to be tagged with keywords. The keywords can then be\nanalyzed for trends. Some sample keywords are listed here:\ncause:network\ncause:human\ncause:bug\ncause:hardware\ncause:knownissue\nseverity:small\nseverity:medium\nseverity:large\nbug:BUGID\ntick:TICKETNUMBER\nmachine:HOSTNAME\nThese keywords enable you to annotate the general cause of the alert, its sever-\nity, and related bug and ticket IDs. Multiple tags can be used. For example, a\nsmall outage caused by a hardware failure might be tagged cause:hardware and\nseverity:small. If the problem was caused by a known bug, the alert might be\ntagged cause:knownissue and bug:12345, assuming the known issue has a bug ID\n12345. Items marked as such would be reserved for situations where there has been\na management decision to not ﬁx a bug or the ﬁx is in progress and a workaround\nis in place until the ﬁx is delivered.\nAs part of your analysis, produce a report showing the most common causes\nof alerts. Look for multiple alerts with the same bug ID or ticket numbers. Also\nlook for the most severe outages and give them special scrutiny, examining the\npostmortem reports and recommendations to see if causes or ﬁxes can be clustered\nor applied to more than one outage.\n14.5 Being Paged Too Much\nA small number of alerts is reasonable but if the number grows too much, inter-\nvention may be required. What constitutes too many alerts is different for different\nteams. There should be an agreed-upon threshold that is tolerated.\nIf the threshold is constantly being violated and things are getting worse, here\nare some interventions one may consider:\n• If a known bug results in frequent pages after a certain amount of time (say,\ntwo release cycles), in the future this alert should automatically be directed to\n\n\n14.6\nSummary\n305\nthe developers’ oncall rotation. If there is no developers’ oncall rotation, push\nto start one. This aligns motivations to have problems ﬁxed.\n• Any alerts received by pager that are not directly related to maintaining the\nSLA should be changed from an alert that generates a page to an alert that\ngenerates a ticket in your trouble-ticketing system.\n• Meet with the developers about this speciﬁc problem. Ensure they understand\nthe seriousness of the issue. Create shared goals to ﬁx the most frequent or\nrecurring issues. If it is part of your culture, set up a list of bugs and have bug\nbash parties or a Fix-It Week.\n• Negotiate to temporarily reduce the SLA. Adjust alerts accordingly. If alerting\nthresholds are already not aligned with SLA (i.e., you receive alerts for low-\npriority issues), then work to get them into alignment. Get agreement as to\nthe conditions by which the temporary reduction will end. It might be a ﬁxed\namount of time, such as a month, or a measurable condition, such as when\nthree successive releases have been pushed without failure or rollback.\n• If all else fails, institute a code yellow: allow the team to defer all other work\nuntil the situation has improved. Set up a metric to measure success and work\ntoward that goal.\n14.6 Summary\nAs part of our mission to maintain a service, we must have a way to handle excep-\ntional situations. To assure that they are handled properly, an oncall rotation is\ncreated.\nThe people who are in the oncall rotation should include operations peo-\nple and developers, so as to align operational priorities. There are many ways\nto design an oncall schedule: weekly, daily, or multiple shifts per day. The goal\nshould be to have no more than a few alerts per shift so that follow-up work can be\ncompleted.\nAn oncall person can be notiﬁed many ways. Generally alerts are shared\nby sending a message to a hand-held device such as a phone plus one other\nmechanism for redundancy.\nBefore a shift, an oncall preparedness checklist should be completed. People\nshould be reachable while oncall in case there is an alert; otherwise, they should\nwork and sleep as normal.\nOnce alerted, the oncall person’s top priority is to resolve the situation, even\nif it means implementing a quick ﬁx and reserving the long-term ﬁx for later.\nAn oncall playbook documents actions to be taken in response to various\nalerts. If the documentation is insufﬁcient, the issue should be escalated to the\nservice owner or other escalation rotation.\n\n\n306\nChapter 14\nOncall\nAlerts should be logged. For major alerts, a postmortem report should be writ-\nten to record what happened, what was done to ﬁx the problem, and what can be\ndone in the future to prevent the problem.\nAlert logs and postmortems should be reviewed periodically to determine\ntrends and select projects that will solve systemic problems and reduce the number\nof alerts.\nExercises\n1. What are the primary design elements of an oncall system?\n2. Describe your current oncall policy. Are you part of an oncall rotation?\n3. How do priorities change for an oncall staffer when alerted?\n4. What are the prerequisites for oncall duty at your organization?\n5. Name 10 things that you monitor. For each of them, which type of notiﬁcation\nis appropriate and why?\n6. Which four elements go into a postmortem, and which details are required\nwith each element?\n7. Write a postmortem report for an incident in which you were involved.\n8. How should an oncall system improve operations over time?\n\n\nChapter 15\nDisaster Preparedness\nFailure is not falling down\nbut refusing to get back up.\n—Theodore Roosevelt\nDisasters and major outages happen. Everyone in the company from the top down\nneeds to recognize that fact and adopt a mindset that accepts outages and learns\nfrom them. An operations organization needs to be able to handle outages well and\navoid repeating past mistakes.\nPreviously we’ve examined technology related to being resilient to failures\nand outages as well as organizational strategies like oncall. In this chapter we dis-\ncuss disaster preparedness at the individual, team, procedural, and organizational\nlevels. People must be trained so that they know the procedure well enough that\nthey can execute it with conﬁdence. Teams need to practice together to build team\ncohesion and conﬁdence, and to ﬁnd and ﬁx procedural problems. Organizations\nneed to practice to ﬁnd inter-team gaps and to ensure the organization as a whole\nis ready to handle the unexpected.\nEvery organization should have a strategy to ensure disaster preparedness at\nall these levels. At the personnel level, training should be both formal (books, doc-\numentation, mentoring) and through game play. Teams and organizations should\nuse ﬁre drills and game day exercises to improve processes and ﬁnd gaps in cov-\nerage. Something like the Incident Command System (ICS) model, described later,\nshould be used to coordinate recovery from outages.\nSuccessful companies operating large distributed systems like Google, Face-\nbook, Etsy, Netﬂix, and others realize that the right way to handle outages is to be\nprepared for them, adopt practices that reduce outages in the future, and reduce\nrisk by practicing effective response procedures. In “Built to Win: Deep Inside\nObama’s Campaign Tech” (Gallagher 2012), we learned that even presidential\ncampaigns have found game day exercises critical to success.\n307\n\n\n308\nChapter 15\nDisaster Preparedness\n15.1 Mindset\nThe ﬁrst step on the road to disaster preparedness is to acknowledge that disas-\nters and major outages happen. They are a normal and expected part of business.\nTherefore we prepare for them and respond appropriately when they occur.\nWe want to reduce the number of outages, but eliminating them totally is\nunrealistic. No technology is perfect. No computer system runs without fail. Elim-\ninating the last 0.00001 percent of downtime is more expensive than mitigating\nthe ﬁrst 99.9999 percent. Therefore as a tradeoff we tolerate a certain amount of\ndowntime and balance it with preparedness so that the situation is handled well.\nEqually, no individual is perfect. Everyone makes mistakes. We strive to make\nas few as possible, and never the same one twice. We try to hire people who are\nmeticulous, but also innovative. And we develop processes and procedures to try\nto catch the mistakes before they cause outages, and to handle any outages that\ndo occur as well as possible. As discussed in Section 14.3.2, each outage should\nbe treated as an opportunity to learn from our own and others’ mistakes and to\nimprove the system. An outage exposes a weakness and enables us to identify\nplaces to make the system more resilient, to add preventive checks, and to edu-\ncate the entire team so that they do not make the same mistake. In this way we\nbuild an organization and a service that is antifragile.\nWhile it is common practice at some companies, it is counterproductive to\nlook for someone to blame and ﬁre when a major incident occurs. When people\nfear being ﬁred, they will adopt behaviors that are antithetical to good operations.\nThey will hide their mistakes, reducing transparency. When the real root causes are\nobscured, no one learns from the mistake and additional checks are not put in place,\nmeaning that it is more likely to recur. This is why a part of the DevOps culture is\nto accept and learn from failure, which exposes problems and thus enables them\nto be ﬁxed.\nUnfortunately, we often see that when a large company or government web\nsite has a highly visible outage, its management scrambles to ﬁre someone to\ndemonstrate to all that the matter was taken seriously. Sometimes the media will\ninﬂame the situation, demanding that someone be blamed and ﬁred and question-\ning why it hasn’t happened yet. The media may eventually lay the blame on the\nCEO or president for not ﬁring someone. The best approach is to release a public\nversion of the postmortem report, as discussed in Section 14.3.2, not naming indi-\nviduals, but rather focusing on the lessons learned and the additional checks that\nhave been put in place to prevent it from happening again.\n15.1.1 Antifragile Systems\nWe want our distributed computing systems to be antifragile. Antifragile sys-\ntems become stronger the more they are stressed or exposed to random behavior.\n\n\n15.1\nMindset\n309\nResilient systems survive stress and failure, but only antifragile systems actually\nbecome stronger in response to adversity.\nAntifragile is not the opposite of fragile. Fragile objects break, or change, when\nexposed to stress. Therefore the opposite of fragile is the ability to stay unchanged\nin the face of stress. A tea cup is fragile and breaks if not treated gently. The\nopposite would be a tea cup that stays the same (does not break) when dropped.\nAntifragile objects, by comparison, react to stress by getting stronger. For example,\nthe process of making steel involves using heat that would destroy most objects,\nbut the process instead makes steel stronger. The mythical Hydra is antifragile\nbecause it grows two heads for each one it loses. The way we learn is antifragile:\nstudying for an exam involves working our brain hard, which strengthens it.\nFragile systems break when the unexpected happens. Therefore, if we\nwant our distributed computing systems to be antifragile, we should introduce\nrandomness, frequently and actively triggering the resiliency features. Every time\na malfunction causes a team to failover a database to a replica, the team gets more\nskilled at executing this procedure. They may also get ideas for improvements.\nBetter execution and procedural improvements make the system stronger.\nTo accelerate this kind of improvement, we introduce malfunctions artiﬁcially.\nRather than trying to avoid malfunctions, we instigate them. If a failover process\nis broken, we want to learn this fact in a controlled way, preferably during nor-\nmal business hours when the largest number of people are awake and available\nto respond. If we do not trigger failures in a controlled way, we will learn about\nsuch problems only during a real emergency: usually after hours, usually when\neveryone is asleep.\n15.1.2 Reducing Risk\nPracticing risky procedures in a controlled fashion enables us to reduce risk. Do not\nconfuse risky behavior with risky procedures. Risky behavior is by deﬁnition risky\nand should be avoided. Procedures that are risky can be improved. For example,\nbeing careless is risky behavior that should be avoided. There’s no way to make\ncarelessness less risky other than to stop being careless. The risk is built in. How-\never, a procedure such as a web service failover may or may not be risky. If it is a\nrisky procedure, it can be improved and reengineered to make it less risky.\nWhen we confuse risky behavior with risky procedures, we end up applying\na good practice to the wrong thing. We make the mistake of avoiding procedures\nthat should, instead, be repeated often until the risk is removed. The technical term\nfor improving something through repetition is called “practice.” Practice makes\nperfect.\nThe traditional way of thinking is that computer systems are fragile and must\nbe protected. As a consequence, they were surrounded by protection systems,\nwhether they be management policies or technical features. Instead of focusing on\n\n\n310\nChapter 15\nDisaster Preparedness\nbuilding a moat around our operations, we should commit to antifragile practices\nto improve the strength and conﬁdence of our systems and organizations.\nThis new attitude toward failure aligns management practices with the reality\nof complex systems:\n• New software releases always have new unexpected bugs.\n• Identifying the root cause is not intended to apportion blame, but rather to\nlearn how to improve the system and operational practices.\n• Building reliable software on top of unreliable components means that\nresiliency features are expected, not an undue burden, a “nice to have” feature,\nor an extravagance.\n• At cloud scale, complex failures are inevitable and unpredictable.\n• In production, complex systems often interact in ways that aren’t explicitly\nknown at ﬁrst (timeouts, resource contention, handoffs).\nIdeally we’d like perfect systems that have perfect uptime. Sadly, such systems\ndon’t exist outside of sales presentations. Until such systems do exist, we’d rather\nhave enough failures to ensure conﬁdence in the precautionary measures we put\nin place. Failover mechanisms need to be exercised whether they are automatic or\nmanual. If they are automatic, the more time that passes without the mechanism\nbeing activated, the less conﬁdent we can be that it will work properly. The sys-\ntem may have changed in ways that are unexpectedly incompatible and break the\nfailover mechanism. If the failover mechanism is a manual procedure, we not only\nlose conﬁdence in the procedure, but we also lose conﬁdence in the team’s ability\nto do the procedure. In other words, the team gets out of practice or the knowledge\nbecomes concentrated among a certain few. Ideally, we want services that fail often\nenough to maintain conﬁdence in the failover procedure but not often enough to\nbe detrimental to the service itself. Therefore, if a component is too perfect, it is\nbetter to artiﬁcially cause a failure to reestablish conﬁdence.\n.\nCase Study: Repeating Risky Behavior to Reduce Risk\nAt one company everyone knew that the last time a database needed to be\nfailed over, it didn’t go well. Therefore the team feared doing the procedure\nand avoided it, thinking this was good risk management. Instead, it actually\nincreased risk. If the process was needed in an emergency, it was unlikely\nto work. Realizing this fact, the manager made the team fail the service over\nevery week.\nThe ﬁrst few times were rough, but the entire team watched and com-\nmented as the person performed the database failover. The procedure book\n\n\n15.2\nIndividual Training: Wheel of Misfortune\n311\n.\nwas updated as comments were made. Pre-checks were documented. A team\nmember pointed out that before she did the failover, she veriﬁed that the disks\nhad plenty of free disk space because previously that had been a problem. The\nrest of the team didn’t know about this pre-check, but now it could be added\nto the procedure book and everyone would know to do it.\nMore importantly, this realization raised an important issue: why wasn’t\nthe amount of free disk space always being monitored? What would hap-\npen if an emergency failover was needed and disk space was too low? A\nside project was spawned to monitor the system’s available disk space. Many\nsimilar issues were discovered and ﬁxed.\nEventually the process got more reliable and soon conﬁdence increased.\nThe team had one less source of stress.\n15.2 Individual Training: Wheel of Misfortune\nThere are many different ways a system can break. People oncall should be able to\nhandle the most common ones, but they need conﬁdence in their ability to do so.\nWe build conﬁdence by providing training in many different ways: documentation,\nmentoring, shadowing more experienced people, and practice.\nWheel of Misfortune is a game that operational teams play to prepare peo-\nple for oncall. It is a way to improve an individual’s knowledge of how to handle\noncall tasks and to share best practices. It can also be used to introduce new pro-\ncedures to the entire team. This game enables team members to maintain skills,\nlearn new skills as needed, and learn from each other. The game is played as\nfollows.\nThe entire team meets in a conference room. Each round involves one person\nvolunteering to be the contestant and another volunteering to be the Master of Dis-\naster (MoD). The MoD explains an oncall situation and the contestant explains how\nthey would work the issue. The MoD acts as the system, responding to any actions\ntaken by the contestant. The MoD can give hints about how to ﬁx the problem\nonly after sufﬁcient guessing or if the contestant has reached a dead end. Eventu-\nally either the contestant takes the issue to resolution or the MoD explains how it\nshould be solved.\nThe MoD usually begins by saying an alert has been received from the moni-\ntoring system and identifying what the message is. The contestant might respond\nby saying which dashboard or log should be checked. The MoD explains what is\nseen on the dashboard or in the logs. This back and forth continues as the contes-\ntant ﬁrst tries to determine what the problem is, then talks through the steps to\nﬁx the problem. It can help a lot if the MoD has prepared screenshots (real or fake)\nthat he or she can pull out and show the contestant, if/when the contestant asks the\n",
      "page_number": 334
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 342-350)",
      "start_page": 342,
      "end_page": 350,
      "detection_method": "topic_boundary",
      "content": "312\nChapter 15\nDisaster Preparedness\nright questions. These might be screenshots from an actual incident or something\nprepared from scratch. Either way, they add a lot of realism to the exercise.\nSome teams have a “no laptops” rule when playing Wheel of Misfortune. Con-\ntestants aren’t allowed to use a laptop directly to check dashboards and such, but\nrather must describe what they want to look at or do to one of their teammates,\nwho drives a laptop and projects the result onto the shared screen for everybody\nelse to see. This forces contestants to be explicit about what they’re doing and why.\nIt serves to expose the contestants’ thought processes to the rest of the audience.\nIf the contestant is new, basic scenarios should be used: the most common\nalerts and the most typical solutions. More advanced contestants should get sce-\nnarios that involve more nuanced problems so that the audience can learn. The goal\nshould never be to trick the contestant or to provide a no-win, or Kobayashi Maru,\nsituation. The goal of the MoD is to educate workers, not to “win” by defeating the\nopponent.\nTo keep it fun, teams can add game show theatrics, starting the session by\nplaying a theme song or spinning a wheel to select the next situation or just for\neffect.\nNo score or points are tallied, because Wheel of Misfortune is purely a training\nexercise. The team leader or manager should observe the game as a way to identify\nwhich members should be given additional coaching. There is no need to embar-\nrass individuals; the suggestion that they get more coaching should be made in\nprivate. On the positive side, the game is also a way to identify which team mem-\nbers have strong troubleshooting skills that can be nurtured and perhaps used to\nhelp others develop their own.\nThe game should be played more frequently when the group is adding new\npeople or technology, and less frequently when things are static. By turning the\neducational process into a game, people have an incentive to want to play. The\ngame keeps the process fun and involves the entire team.\n15.3 Team Training: Fire Drills\nFire drills exercise a particular disaster preparedness process. In these situations\nactual failures are triggered to actively test both the technology and the people\ninvolved.\nThe key to building resilient systems is accepting that failure happens and\nmaking a commitment to being prepared to respond quickly and effectively to\nthose failures. An untested disaster recovery plan isn’t really a plan at all. Fire drills\nare processes where we preemptively trigger the failure, observe it, ﬁx it, and then\nrepeat until the process is perfected and the people involved are conﬁdent in their\nskills.\n\n\n15.3\nTeam Training: Fire Drills\n313\nDrills work because they give us practice and ﬁnd bugs in procedures. It’s\nbetter to prepare for failures by causing them in production while you are watching\nthan to rely on a strategy of hoping the system will behave correctly when you\naren’t watching. Doing these drills in production does carry the risk that something\ncatastrophic will happen. However, what better time for a catastrophe than when\nthe entire engineering team is ready to respond?\nDrills build conﬁdence in the disaster recovery technology because bugs are\nfound and ﬁxed. The less often a failover mechanism is used, the less conﬁdence\nwe have in it. Imagine if a failover mechanism has not been triggered in more than a\nyear. We don’t know if seemingly unrelated changes in the environment have made\nthe process obsolete. It is unreasonable to expect it to work seamlessly. Ignorance\nis not bliss. Being unsure if a failover mechanism will work is a cause of stress.\nDrills build conﬁdence within the operations team. If a team is not accustomed\nto dealing with disaster, they are more likely to react too quickly, react poorly,\nor feel undue levels of pressure and anxiety. This reduces their ability to handle\nthe situation well. Drills give the team a chance to practice reacting calmly and\nconﬁdently.\nDrills build executive-level conﬁdence in their operations teams. While some\nexecutives would rather remain ignorant, smarter executives know that failures\nhappen and that a company that is prepared and rehearsed is the best defense.\nDrills can be done to gain conﬁdence in an individual process, in larger tests\ninvolving major systems, or even in larger tests that involve the entire company for\nmultiple days. Start small and work your way up to the biggest drills over time.\nTrying a large-scale exercise is futile and counterproductive, if you haven’t ﬁrst\nbuilt up capability and conﬁdence through a series of smaller but ever-growing\ndrills.\n15.3.1 Service Testing\nA very practical place to start is by picking individual failover mechanisms to\ntest. Identify failover mechanisms that are not exercised often and pick one. These\ntests should be contained such that if an issue is uncovered, only the immediate\nteam members are needed to respond. Typically these tests ﬁnd broken processes,\ndocumentation gaps, knowledge gaps, and conﬁguration problems.\nThe drill involves causing a failure that will trigger the failover mechanism.\nIt may involve powering off a machine (pulling the power cord), shutting down a\nvirtual machine, killing running software, disconnecting a network connection, or\nany other way one can disrupt service.\nOne strategy is to use drills to improve the process. If the process is unreliable\nor untested, it is worthwhile having the same people run the drill over and over,\nimproving the process documentation and software for each iteration. By having\n\n\n314\nChapter 15\nDisaster Preparedness\nthe same people do each iteration, they will learn the system well enough to make\nbig improvements. They may do many iterations in one day, seeking to ﬁne-tune it.\nAlternatively, doing iterations further apart permits deeper problems to be ﬁxed.\nAnother strategy is to use drills to improve conﬁdence of the individual team\nmembers. Each iteration could be done by a different person until everyone has\nsuccessfully done the process, possibly more than once.\nA hybrid approach uses drills with the same people until the process is solid,\nthen uses an individual team member strategy to verify that everyone on the team\ncan do the process. Once this level of improvement is achieved, the drill is needed\nless frequently. One may choose to do the drill anytime the process has not been\ntriggered by a real incident for more than a certain amount of time, if the process\nchanges, or if new people join the team.\nPicking which day to do such a drill requires serious deliberation. It’s best to\ndo it on a day when no major sales demonstrations or new product releases are\nplanned. It should be a day when everyone is available, and probably not the day\npeople return from vacation or the day before people leave for vacation.\n.\nGoogle Chubby Outage Drills\nInside Google is a global lock service called “Chubby.” It has such an excellent\nreputation for reliability that other teams made the mistake of assuming it was\nperfect. A small outage created big problems for teams that had written code\nthat assumed Chubby could not fail.\nNot wanting to encourage bad coding practices, the Chubby team\ndecided that it would be best for the company to create intentional outages.\nIf a month passed without at least a few minutes of downtime, they would\nintentionally take Chubby down for ﬁve minutes. The outage schedule was\nannounced well in advance.\nThe ﬁrst planned outage was cancelled shortly before it was intended\nto begin. Many critical projects had reported that they would not be able to\nsurvive the test.\nTeams were given 30 days to ﬁx their code, but warned that there would\nbe no further delays. Now the Chubby team was taken seriously. The planned\noutages have happened ever since.\n15.3.2 Random Testing\nAnother strategy is to test a wide variety of potential failures. Rather than pick-\ning a speciﬁc failover process to improve, select random machines or other failure\n\n\n15.4\nTraining for Organizations: Game Day/DiRT\n315\ndomains, cause them to fail, and verify the system is still running. This can be done\non a scheduled day or week, or in a continuous fashion.\nFor example, Netﬂix created many autonomous agents, each programmed to\ncreate different kinds of outages. Each agent is called a monkey. Together they form\nthe Netﬂix Simian Army.\nChaos Monkey terminates random virtual machines. It is programmed to\nselect different machines with different probabilities, and some machines opt\nout entirely. Each hour Chaos Monkey wakes up, picks a random machine, and\nterminates it.\nChaos Gorilla picks an entire datacenter and simulates either a network par-\ntition or a total failure. This causes massive damage, such that recovery requires\nsophisticated control systems to rebalance load. Therefore it is run manually as part\nof scheduled tests.\nThe Simian Army is always growing. Newer members include Latency Mon-\nkey, which induces artiﬁcial delays in API calls to simulate service degradation.\nAn extensive description of the Simian Army can be found in the article “The\nAntifragile Organization” (Tseitlin 2013).\nDrills like this carry a larger risk at the start because they span so many fail-\nure domains; consequently, they should be approved by management. Convincing\nmanagement of the value of this kind of test may be difﬁcult. Most managers want\nto avoid problems, not induce them. It is important to approach the topic from the\npoint of view of improving the ability to respond to problems that will inevitably\nhappen, and by highlighting that the best time to ﬁnd problems is in a controlled\nenvironment and not late at night when employees are asleep. Tie the process to\nbusiness goals involving overall service uptime. Doing so also improves the morale\nand conﬁdence of team members.\n15.4 Training for Organizations: Game Day/DiRT\nGame Day exercises are multi-day, organization-wide disaster preparedness tests.\nThey involve many teams, often including non-technical teams such as commu-\nnications, logistics, and ﬁnance. Game Day exercises focus on testing complex\nscenarios, trying out rarely tested interfaces between systems and teams, and\nidentifying unknown organizational dependencies.\nGame Day exercises may involve a multi-day outage of a datacenter, a com-\nplex week of network and system outages, or veriﬁcation that secondary coverage\npersonnel can successfully run a service if the primary team disappeared for an\nextended amount of time. Such exercises can also be used to rehearse for an\nupcoming event where additional load is expected and the team’s ability to handle\nlarge outages would be critical. For example, many weeks before the 2012 election\nday, the Obama campaign performed three all-day sessions where its election-day\n\n\n316\nChapter 15\nDisaster Preparedness\n“Get Out the Vote” operation was put to the test. This is described in detail in the\narticle “When the Nerds Go Marching in” (Madrigal 2012).\nBecause of the larger scale and scope, this kind of testing can have more impact\nand prevent larger outages. Of course, because of the larger scale and scope, this\nkind of testing also requires more planning, more infrastructure, and a higher level\nof management approval, buy-in, and support.\nThe organization needs to believe that the value realized through learning jus-\ntiﬁes the cost. Game Day exercises might be a sizable engineering effort involving\nhundreds of staff-days of effort. There is a potential for real accidental outages that\nresult in revenue loss.\nExecutives need to recognize that all systems will inevitably fail and that con-\nﬁdence is best gained though practice, not avoidance. They should understand\nthat it is best to have these failures happen in a controlled environment when key\npeople are awake. Learning that a failover system does not work at 4 when\nkey people are asleep or on vacation is the risk to avoid.\nGoogle’s Disaster Recovery Testing (DiRT) is this company’s form of Game\nDay exercises. DiRT is done at a very large scale and focuses on testing interactions\nbetween teams. Such interactions are generally less frequently exercised, but when\nthey are required it is due to larger disasters. These exercises are more likely to lead\nto customer-visible outages and loss of revenue than team-based service testing or\nrandom testing. For this reason teams should be doing well at their own particular\nﬁre drills before getting involved in DiRT.\n15.4.1 Getting Started\nWhile DiRT exercises can be very large, it is best to introduce a company to the\nconcept by starting small. A small project is easier to justify to management. A\nworking small example is easier to approve than a hypothetical big project, which,\nin all honesty, sounds pretty terrifying to someone who is unfamiliar with the con-\ncept. An executive should be very concerned if the people who are supposed to\nkeep their systems up start telling them about how much they want to take those\nsystems down.\nStarting small also means simpler tests. Google’s DiRT started with only a\nfew teams. Tests were safe and were engineered so they could not create any\nuser-visible disruptions. This was done even if it meant tests weren’t very useful.\nIt got teams used to the concept, reassured them that lessons learned would be\nused for constructive improvements to the system, and let them know that failures\nwould not become blame-fests or hunts for the guilty. It also permitted the DiRT\ntest coordinators to keep the processes simple and try out their methodology and\ntracking systems.\nGoogle’s original Site Reliability Engineering (SRE) teams were located in the\ncompany’s headquarters in Mountain View, California. Eventually Google added\n\n\n15.4\nTraining for Organizations: Game Day/DiRT\n317\nSRE teams in Dublin, Ireland, to handle oncall at night-time. The ﬁrst DiRT exercise\nsimply tested whether the Dublin SREs could run the service for an entire week\nwithout help. Mountain View pretended to be unavailable. This exposed instances\nwhere Dublin SREs were depending on the fact that if they had questions or issues,\neventually a Mountain View SRE would wake up and be able to assist.\nAnother of Google’s early, simple tests was to try to work for one day without\naccess to the source code repository. This exercise found areas where production\nprocesses were dependent on a system not intended to be always available. In fact,\nmany tests involved verifying that production systems did not rely on systems\nthat were not built to be in the critical path of production systems. The larger an\norganization grows, the more likely that these dependencies will be found only\nthrough active testing.\n15.4.2 Increasing Scope\nOver time the tests can grow to include more teams. One can raise the bar for testing\nobjectives, including riskier tests, live tests, and the removal of low-value tests.\nToday, Google’s DiRT process is possibly the largest such exercise in the world.\nBy 2012 the number of teams involved had multiplied by 20, covering all SRE teams\nand nearly all services.\nGrowing the process to this size depended on creating a culture where identi-\nfying problems is considered a positive way to understand how the system can\nbe improved rather than a cause for alarm, blame, and ﬁnger-pointing. Some\noperations teams could not see the beneﬁt of testing beyond what their service\ndelivery platform’s continuous delivery system already provided. The best predic-\ntor of a team’s willingness to start participating was whether previous failures had\nresulted in a search for a root cause to be ﬁxed or a person to be blamed. Being able\nto point to earlier, smaller successes gave new teams and executive management\nconﬁdence in expanding the program.\nAn example complex test might involve simulating an earthquake or other\ndisaster that makes the company headquarters unavailable. Forbid anyone at head-\nquarters from talking to the rest of the company. Google DiRT did this and learned\nthat its remote sites could continue, but the approval chain for emergency pur-\nchases (such as fuel for backup generators) required the consent of people at\nthe company headquarters. Such key ﬁndings are non-technical. Another non-\ntechnical ﬁnding was that if all the tests leave people at headquarters with nothing\nto do, they will ﬂood the cafeteria, creating a DoS ﬂood of the food kind.\nCorporate emergency communications plans should also be tested. During\nmost outages people can communicate using the usual chat rooms and such. How-\never, an emergency communication mechanism is needed in the event of a total\nnetwork failure. The ﬁrst Google DiRT exercise found that exactly one person was\nable to ﬁnd the emergency communication plan and show up on the correct phone\n\n\n318\nChapter 15\nDisaster Preparedness\nbridge. Now periodic ﬁre drills spot-check whether everyone has the correct infor-\nmation with them. In a follow-up drill, more than 100 people were able to ﬁnd and\nexecute the emergency communication plan. At that point, Google learned that the\nbridge supported only 40 callers. During another drill, one caller put the bridge\non hold, making the bridge unusable due to “hold music” ﬂooding the bridge. A\nrequirement to have the ability to kick someone off the bridge was identiﬁed. All of\nthese issues were discovered during simulated disasters. Had they been discovered\nduring a real emergency, it would have been a true disaster.\n15.4.3 Implementation and Logistics\nThere are two kinds of tests. Global tests involve major events such as taking down\na datacenter and are initiated by the event planners. Team tests are initiated by\nindividual teams.\nAn event may last multiple days or a single terrible day. Google schedules an\nentire week but ends DiRT sessions as soon as the tests’ goals have been satisﬁed.\nTo keep everyone on their toes, DiRT doesn’t start exactly at the announced start\ntime but rather after a small delay. The length of delay is kept secret.\nA large event has many moving parts to coordinate. The coordinator should\nbe someone with both technical and project management experience, who can ded-\nicate a sufﬁcient amount of time to the project. At a very large scale, coordination\nand planning may require a dedicated, full-time position even though the event\nhappens every 12 months. Much of the year will be spent planning and coordi-\nnating the test. The remaining months are spent reviewing outcomes and tracking\norganization-wide improvement.\nPlanning\nThe planning for the event begins many months ahead of time. Teams need time to\ndecide what should be tested, select proctors, and construct test scenarios. Proctors\nare responsible for designing and executing tests. Long before the big day, they\ndesign tests by documenting the goal of the test, a scenario, and a script that will\nbe followed. For example, the script might involve calling the oncall person for\na service and having that individual simulate a situation much like a Wheel of\nMisfortune exercise. Alternatively, the company may plan to actively take down\na system or service and observe the team’s reaction. During the actual test, the\nproctor is responsible for the tests execution.\nKnowing the event date as early as possible enables teams to schedule project\nwork and vacations. Teams may also use this time to do individual drills so that the\nevent can focus on tests that ﬁnd the gaps between teams. If the team’s individual\nprocesses are not well practiced, then DiRT itself will not go well.\nPrior to the ﬁrst Game Day at Amazon, John Allspaw conducted a series of\ncompany-wide brieﬁngs advising everyone of the upcoming test. He indicated it\n\n\n15.4\nTraining for Organizations: Game Day/DiRT\n319\nwould be on the scale of destroying a complete datacenter. People did not know\nwhich datacenter, which inspired more comprehensive preparation (Robbins,\nKrishnan, Allspaw & Limoncelli 2012).\nRisk is mitigated by having all test plans be submitted in advance for review\nand approval by a cross-functional team of experts. This team checks for unrea-\nsonable risks. Tests never done before are riskier and should be done in a sand-\nbox environment or through simulation. Often it is known ahead of time that\ncertain systems are ill prepared and will not survive the outage. Pre-fail these\nsystems, mark them as failing the test, and do not involve them in the event.\nThere is nothing to be learned by involving them. These machines should be\nwhitelisted so they still receive service during the event. For example, if the out-\nage is simulated using network ﬁltering, these machines can be excluded from the\nﬁlter.\nOrganization\nTwo subteams are needed to make the event a success. The tech team designs the\nglobal tests and evaluates team test plans. The tests are graded on quality, impact,\nand risk. During the event this team is responsible for causing the global outages,\nmonitoring them, and making sure things don’t go awry. This team also handles\nunforeseen issues caused by the tests.\nThe coordination team is responsible for planning, scheduling, budgets, and\nexecution. It works with the tech team to prevent conﬂicting tests. Coordination\nteam members verify that all prep work is complete. They also are in charge of\ncommunication both with management and corporate-wide.\nDuring the event, both subteams reside in the command center, which oper-\nates as a kind of “Mission Control.” Due to the nature of the tests, the command\ncenter must be a physical location, as relying on virtual meeting spaces is too risky.\nFor large organizations, this involves a considerable amount of travel.\nIn the command center, individuals call proctors to tell them to start their tests.\nTest controllers deal with unexpected issues as they come up and communicate\nglobally about the status of the event. They monitor the progress of the tests as\nwell as the effects of the tests on the technical systems that should not be affected.\nTest controllers can halt all tests if something goes massively wrong.\nOne of the most valuable roles in the command center is the story teller. One\nperson concocts and narrates the disaster. He or she should create a story that is\njust unrealistic enough that people know that it is a test. For example, the story\nmight involve a zombie attack, an evil genius who has put everyone at head-\nquarters under hypnosis, or an errant fortune-teller with mystical powers. Daily\nupdates that move the story forward should be distributed by email or old-time\nradio “broadcasts.” The last bulletin should resolve the story and let everyone\nknow that the test is over, the zombies have been defeated, or whatever.\n\n\n320\nChapter 15\nDisaster Preparedness\nThe story teller helps counteract the drudgery of the test. If this person does\na good job, people might actually look forward to next year’s event.\n.\nSuggested Reading\nFor many years DiRT and related practices were company secrets. However, in\n2012 a number of companies and practitioners revealed their disaster testing\nprocedures in a series of articles that were curated by Tom and published in\nACM Queue magazine:\n• “Resilience Engineering: Learning to Embrace Failure” is a group inter-\nview Tom conducted with Kripa Krishnan, the coordinator of Google’s\nDiRT program; Jesse Robbins, the architect of Game Day at Amazon; and\nJohn Allspaw, senior vice president of technological operations at Etsy\n(Robbins et al. 2012).\n• Krishnan details Google’s DiRT program in “Weathering the Unex-\npected” (Krishnan 2012).\n• Allspaw explained the theory and practice of Etsy’s program in “Fault\nInjection in Production” (Allspaw 2012b).\n• Tseitlin details the Netﬂix Simian Army and explains how it has improved\nresilience and maximized availability in “The Antifragile Organization”\n(Tseitlin 2013).\nLater Steven Levy was allowed to observe Google’s annual DiRT process ﬁrst-\nhand for an article he wrote for Wired magazine titled “Google Throws Open\nDoors to Its Top-Secret Data Center” (Levy 2012).\nAfter the 2012 U.S. presidential election, an article in The Atlantic maga-\nzine, “When the Nerds Go Marching in,” described the Game Day exercises\nconducted by the Obama for America campaign in preparation for election\nday 2012 (Madrigal 2012). Dylan Richard’s talk “Gamedays on the Obama\nCampaign” provided a ﬁrst-hand account and the disclaimer that technology\ndidn’t win the election, but it certainly could have lost it (Richard 2013).\n15.4.4 Experiencing a DiRT Test\nWhat follows is a ﬁctionalized account of a Google DiRT exercise as seen from the\nperspective of the engineers involved in the test. The names, locations, and situa-\ntions have been changed. This description is adapted from an article Tom wrote for\nACM Queue magazine titled “Google DiRT: The View from Someone Being Tested”\n(Limoncelli 2012).\n",
      "page_number": 342
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 351-360)",
      "start_page": 351,
      "end_page": 360,
      "detection_method": "topic_boundary",
      "content": "15.4\nTraining for Organizations: Game Day/DiRT\n321\n[Phone rings]\nTom: Hello?\nRosanne: Hi, Tom. I’m proctoring a DiRT exercise. You are on call for [name of\nservice], right?\nTom: I am.\nRosanne: In this exercise we pretend the [name of service] database needs to be\nrestored from backups.\nTom: OK. Is this a live exercise?\nRosanne: No, just talk me through it.\nTom: Well, I’d follow the directions in our operational docs.\nRosanne: Can you ﬁnd the doc?\n[A couple of key clicks later]\nTom: Yes, I have it here.\nRosanne: OK, bring up a clone of the service and restore the database to it.\nOver the next few minutes, I make two discoveries. First, one of the commands\nin the document now requires additional parameters. Second, the temporary area\nused to do the restore does not have enough space. It had enough space when the\nprocedure was written, but the database has grown since then.\nRosanne ﬁles a bug report to request that the document be updated. She also\nﬁles a bug report to set up a process to prevent the disk-space situation from\nhappening.\nI check my email and see the notiﬁcations from our bug database. The noti-\nﬁcations are copied to me and the bugs are tagged as being part of DiRT2011.\nEverything with that tag will be watched by various parties to make sure they get\nattention over the next few months. I ﬁx the ﬁrst bug while waiting for the restore\nto complete.\nThe second bug will take more time. We’ll need to add the restore area to our\nquarterly resource estimation and allocation process. Plus, we’ll add some rules to\nour monitoring system to detect whether the database size is nearing the size of\nthe restore area.\nTom: OK, the service’s backup has been read. I’m running a clone of the service on\nit, and I’m sending you an instant message with a URL you can use to access it.\n[A couple of key clicks later]\nRosanne: OK, I can access the data. It looks good. Congrats!\nTom: Thanks!\nRosanne: Well, I’ll leave you to your work. Oh, and maybe I shouldn’t tell you this,\nbut the test controllers say at 2 there will be some fun.\n\n\n322\nChapter 15\nDisaster Preparedness\nTom: You know my oncall shift ends at 3 , right? If you happen to be delayed an\nhour...\nRosanne: No such luck. I’m in California and 3 your time is when I’ll be leaving\nfor lunch.\nA minute after the exercise is over, I receive an email message with a link to a\npost-exercise document. I update it with what happened and provide links to the\nbugs that were ﬁled. I also think of a few other ways of improving the process and\ndocument them, ﬁling feature requests in our bug database for each of them.\nAt 2 my pager doesn’t go off, but I see on my dashboard that there is an\noutage in Georgia. Everyone in our internal chat room is talking about it. I’m not\ntoo concerned. Our service runs out of four datacenters around the world, and the\nsystem has automatically redirected web requests to the other three locations.\nThe transition is ﬂawless, losing only the queries that were “in ﬂight,” which\nis well within our SLA.\nA new email appears in my inbox explaining that zombies have invaded Geor-\ngia and are trying to eat the brains of the datacenter technicians there. The zombies\nhave severed the network connections to the datacenter. No network trafﬁc is going\nin or out. Lastly, the email points out that this is part of a DiRT exercise and no\nactual technicians have had their brains eaten, but the network connections really\nhave been disabled.\n[Again, phone rings]\nRosanne: Hi! Having fun yet?\nTom: I’m always having fun. But I guess you mean the Georgia outage?\nRosanne: Yup. Shame about those technicians.\nTom: Well, I know a lot of them and they have big brains. Those zombies will feed\nfor hours.\nRosanne: Is your service still within SLA?\nI look at my dashboard and see that with three datacenters doing the work\nnormally distributed to four locations the latency has increased slightly, but it is\nwithin SLA. The truth is that I don’t need to look at my dashboard because I would\nhave gotten paged if the latency was unacceptable (or growing at a rate that would\nreach an unacceptable level if left unchecked).\nTom: Everything is ﬁne.\nRosanne: Great, because I’m here to proctor another test.\nTom: Isn’t a horde of zombies enough?\nRosanne: Not in my book. You see, your SLA says that your service is supposed to\nbe able to survive two datacenter outages at the same time.\n\n\n15.5\nIncident Command System\n323\nShe is correct. Our company standard is to be able to survive two outages\nat the same time. The reason is simple. Datacenters and services need to be able\nto be taken down occasionally for planned maintenance. During this window of\ntime, another datacenter might go down for unplanned reasons such as a network\nor power outage. The ability to survive two simultaneous outages is called N + 2\nredundancy.\nTom: So what do you want me to do?\nRosanne: Pretend the datacenter in Europe is going down for scheduled preventive\nmaintenance.\nI follow our procedure and temporarily shut down the service in Europe.\nWeb trafﬁc from our European customers distributes itself over the remaining two\ndatacenters. Since this is an orderly shutdown, no queries are lost.\nTom: Done!\nRosanne: Are you within the SLA?\nI look at the dashboard and see that the latency has increased further. The\nentire service is running on the two smaller datacenters. Each of the two down\ndatacenters is bigger than the combined, smaller, working datacenters, yet there is\nenough capacity to handle this situation.\nTom: We’re just barely within the SLA.\nRosanne: Congrats. You pass. You may bring the service up in the European\ndatacenter.\nI decide to ﬁle a bug anyway. We stayed within the SLA, but it was too close\nfor comfort. Certainly we can do better.\nI look at my clock and see that it is almost 3 . I ﬁnish ﬁlling out the post-\nexercise document just as the next oncall person comes online. I send her an instant\nmessage to explain what she missed.\nI also remind her to keep her ofﬁce door locked. There’s no telling where the\nzombies might strike next.\n15.5 Incident Command System\nThe public safety arena uses the Incident Command System to manage outages. IT\noperations can adapt that process for handling operational outages. This idea was\nﬁrst popularized by Brent Chapman in his talk “Incident Command for IT: What\nWe Can Learn from the Fire Department” (Chapman 2005). Brent has extensive\nexperience in both IT operations and public safety.\n\n\n324\nChapter 15\nDisaster Preparedness\nOutside the system administration arena, teams from different public safety\norganizations such as ﬁre, police, and paramedics come together to respond to\nemergencies and disasters. A system called the Incident Command System (ICS)\nhas been developed to allow this to happen productively in a way that can scale\nup or down as the situation changes.\nICS is designed to create a ﬂexible framework within which people can work\ntogether effectively. The key principles of ICS are as follows:\n• Standardized Organizational Structure: An ICS team is made up of the\nIncident Commander and subcommand systems: Operations, Logistics,\nPlanning, Admin/Finance, and optionally a Uniﬁed Command, a Public\nInformation Ofﬁcer, and a Liaison Ofﬁcer.\n• Unambiguous Definition of Who Is in Charge: There is one and only one\nIncident Commander. Each person on an ICS team reports to only one super-\nvisor for that ICS incident.\n• Explicit Delegations of Authority: The Incident Commander sets up certain\nkey branches, such as Logistics and Operations and Planning, and delegates\nthose functions to their commanders.\n• Management by Objective: Clear objectives and priorities are established for\nthe incident. Tell people what you want to get done, not how to do it; let them\nﬁgure out the best way to get it done in the current circumstances.\n• Limited Span of Control That Can Scale: Under ICS, a supervisor should\nnot have more than seven direct reports. Ideally, three to ﬁve individu-\nals should report to one supervisor. As the number of people involved\ngrows, the organization expands and new supervisors are created. Through\nthis approach, responsibilities stay limited. Remember that this is a tem-\nporary organizational structure created for responding to this speciﬁc\nevent. The same group of people might organize differently for a different\nevent.\n• Common Terminology and Organizational Framework: By using the ICS\nroles and responsibilities as a common framework, different teams from differ-\nent organizations can understand clearly who is doing what and where their\nresponsibilities are.\nA full description of ICS can be found on the U.S. Federal Emergency\nManagement Administration (FEMA) web site (http://www.fema.gov/incident-\ncommand-system). The FEMA Emergency Management Institute publishes free\nself-study and other training materials (http://training.fema.gov/EMI/).\nA more approachable introduction is the Wikipedia article on ICS (http://en.\nwikipedia.org/wiki/Incident_command_system).\n\n\n15.5\nIncident Command System\n325\n15.5.1 How It Works: Public Safety Arena\nWhen a public safety incident begins, the ﬁrst order of business is to get orga-\nnized, starting by ﬁguring out who is going to be in charge. In an incident, the ﬁrst\nqualiﬁed responder to arrive automatically becomes the Incident Commander (IC).\nAn ICS team is made up of the Incident Commander and the subcom-\nmand systems of Operations, Logistics, Planning/Status, and Administration/\nFinance, as shown in Figure 15.1. A Public Information Ofﬁcer is also appointed\nto deal with communication beyond the response team (internal and exter-\nnal) about the incident. A Liaison Ofﬁcer is the primary contact for out-\nside agencies, such as third-party vendors. A Safety Ofﬁcer monitors safety\nconditions.\nAll incidents have a Command section (the management) and most have an\nOperations section, which directly provides emergency services such as putting\nout the ﬁre or providing medical attention. The management structure is a strict\nhierarchy. In an emergency there is no time for the vagaries and inefﬁciency of\nmatrix management.\nWhen the IC arrives, he or she is often alone and assumes all roles. As more\npeople arrive, the IC delegates roles as needed. As new people arrive, they check\nin with Admin/Finance (if it exists) for bookkeeping purposes, but then the IC\nassigns them to a function. The IC has the global view needed to best determine\nhow to allocate new resources.\nThe IC role does not transfer to anyone just because they have a higher rank or\nbecause a different department arrives. For example, a police chief does not auto-\nmatically trump a police ofﬁcer, and a ﬁre ﬁghter does not automatically trump a\nparamedic. Transfer of control is explicit, implemented by the current IC hand-\ning control to someone else who explicitly accepts the role. Role handoffs are\ndisruptive and are done only when needed.\nTeams stay small. A team size of three to ﬁve people is considered optimal. If\na subcommand gains seven direct reports, the team will be split in two when the\nnext person is added.\nFigure 15.1: The basic ICS organizational structure\n\n\n326\nChapter 15\nDisaster Preparedness\n15.5.2 How It Works: IT Operations Arena\nWhen adapting this system to an IT operations organization, the Operations\nteam handles the operational aspects of the incident—the actual ﬁreﬁghting, as\nit were. The Logistics team handles resources, such as people and materials, and\nmakes sure that Operations has what it needs to do its job. The Planning team is\nresponsible for forecasting situation and resource needs, and for collecting and dis-\nplaying information about the incident. The Admin/Finance team handles general\nadministrative support and budgets.\nInitially the IC is whoever is ﬁrst on site. In IT, that is usually the oncall person\nwho responded to the alert. However, since this person has already been trouble-\nshooting the alert, it usually makes sense for him or her to sound the alarm and\nhandoff the IC role to the next qualiﬁed responder, continuing as the Operations\n(Ops) lead. The IC role is transferred through explicit handoff at shift change or\nif the IC becomes tired and needs relief. The key point here is that the IC role is\nalways handed off as part of a thoughtful process, not automatically as new people\nshow up. If a better-qualiﬁed person arrives on scene, the current IC may decide\nit’s worth the disruption of a handoff to switch ICs.\nFor small IT incidents, the IC handles all leadership roles on his or her own.\nHowever, as an incident grows larger, more people get involved, either because\nmore co-workers notice the outage or because the IC reaches out to them. As people\nshow up, the IC assigns them roles, creating subteams as needed. It’s worth noting\nthat the IC and the Ops lead should be made separate individual roles as quickly as\npossible. The IC has the big-picture view and the Ops lead is down in the trenches\ndealing with the incident directly. Trying to handle both roles simultaneously often\nresults in doing neither role well.\nOutages of long duration require frequent status updates to management and\nother stakeholders. By designating a single person to be the Public Information\nOfﬁcer, the IC can keep from being distracted by executives demanding updates\nor users asking, “Is it up yet?” Every status update should end by noting when the\nnext status can be expected. Many organizations standardize on update frequency.\nHourly updates balance executives’ need to know with technical workers’ need\nto focus on the technical issues at hand. Multi-day outages require less frequent\nupdates to avoid repetition.\n15.5.3 Incident Action Plan\nIn ICS, an Incident Action Plan (IAP) is created and continually reﬁned during the\nincident. This one- to two-page document answers four questions:\n• What do we want to do?\n• Who is responsible for doing it?\n\n\n15.5\nIncident Command System\n327\n• How do we communicate with each other?\n• What is the procedure if someone becomes unavailable?\nIn IT operations, we add a ﬁfth question:\n• What is the procedure if additional failures are detected?\nCreate a template that the IC can use to produce this document. Make the template\naccessible in a standard location so that everyone knows where to ﬁnd it. An\nexample can be found in Figure 15.2.\n15.5.4 Best Practices\nTo assure success, everyone should get practice using ICS procedures and being\nthe Incident Commander, new hires should be introduced to ICS procedures as\npart of their onboarding process, and escalation partners should be informed that\nICS procedures are in use.\nTeams should use ICS procedures for minor incidents to stay in practice so\nthat everyone is comfortable with the process when major situations develop. ICS\nprocedures should be used for non-emergencies such as datacenter moves and\nmajor upgrades as well as for security incidents and adversarial terminations.\n.\nWhat do we want to do?\nThe main web server is down. We must restore it to full functionality.\nWho is responsible for doing it?\nThe IC is Mary. The SRE team is responsible, with help from developers\nRosanne, Eddie, and Frances.\nHow do we communicate with each other?\nVideo chat room:\nhttps://plus.google.com/hangouts/_/example.com/outage\nWhat is the procedure if someone becomes unavailable?\nIf possible, they should report to the IC for relief via phone (201-555-1212),\ntext message, instant message, or video chat room. If they are unable to, or\nif someone suspects that someone else has become unavailable, report this\ninformation to the IC.\nWhat is the procedure if additional failures are detected?\nThe IC will be responsible for leading the triage effort, assessing the situation,\nand deciding how to move forward.\nFigure 15.2: A sample Incident Action Plan (IAP)\n\n\n328\nChapter 15\nDisaster Preparedness\nICS training should be part of an operation team’s new-hire onboarding pro-\ncess. It is important that everyone understand the terminology and processes so\nthat the system works well.\nPeople on a team should rotate through the role of Incident Commander to\nget an idea of the scope and capabilities of the team as it works together. Selecting\nthe IC in non-emergencies should be done by picking the person who has gone the\nlongest without being in the role.\nEscalation partners should be informed about your team’s use of ICS pro-\ncedures and how they will interact with the ICS roles, such as Communications\nOfﬁcer. If they use ICS procedures internally as well, so much the better. Partner\nteams may have adapted ICS procedures differently for use in IT. This can lead to\nconfusion and conﬂicts. Therefore multi-team incidents should be practiced to ﬁnd\nand ﬁx problems. Both simulated problems and real, but non-emergency issues can\nbe used. A good opportunity for this type of practice is when triaging a major bug\nthat involves both teams.\n15.5.5 ICS Example\nAs an example use of ICS procedures, suppose a major outage is affecting XYZ\nCompany’s web operations. Bob, the oncall responder to the outage, activates the\nICS process and is the temporary Incident Commander. The ﬁrst thing Bob does is\nnotify his team about the outage and pass on the role of Incident Commander to\nhis team lead, Janet. Janet starts assembling an ICS team immediately, delegating\nOperations to Bob. She notiﬁes her manager Sandy about the outage and asks him\nto be the Public Information Ofﬁcer.\nBob and Janet start an Incident Action Plan by describing what they believe is\ngoing on and what they are doing about it. Bob is taking action and Janet is coor-\ndinating efforts. Bob explains that he will need certain resources, such as failover\nmachines, and that the team is likely to end up working late on this one. Janet,\nas Incident Commander, now reaches out to ﬁnd someone to take on the Logis-\ntics role. She picks Peter and directs him to get the requirements from Bob. The\nnew Logistics Section Chief delegates people to get the machine resources that\nOperations will need, and arranges for pizza to arrive at 6 .\nJanet identiﬁes someone from the Monitoring group to ask about getting\ndetailed information on the outage. Jyoti agrees to be the Planning Section Chief\nand starts working with other members of the Monitoring group to gather the\nrequested information. They are working on forecasting demand, such as know-\ning that web services are currently coming off daily peak demand in Europe and\nclimbing in North America.\nAs the outage continues, Janet’s manager Sandy (Public Information Ofﬁcer)\ninterfaces with Janet (Incident Command), Bob (Operations), Jyoti (Planning), and\n\n\n15.6\nSummary\n329\nPeter (Logistics) to keep track of what is going on and notify people about the\nprogress of the outage. Sandy delegates a person on the team to update status\npages for XYZ Company’s customers as well as an internal status page within the\ncompany.\nMeanwhile, Bob in Operations has determined that a database needs to be\nfailed over and replicas updated. Managing by objective, he asks Logistics for\nanother resource to do the replica and load balancing work. Logistics ﬁnds some-\none else in the company’s IT staff who has experience with the replica system and\ngets permission for that person to help Bob during this outage. Bob begins the\ndatabase failover and his new helper begins work on the load balancing and replica\nwork needed.\nThe new replica completes its initial copy and begins serving requests. Janet\nconﬁrms with all ICS section chiefs that the service’s status has returned to normal.\nThe event is declared resolved and the ICS process is explicitly terminated. Janet\ntakes the action item to lead the postmortem effort.\n15.6 Summary\nTo handle major outages and disasters well, we must prepare and practice. Igno-\nrance may be bliss, but practice makes progress. It is better to learn that a disaster\nrecovery process is broken by testing it in a controlled environment than to be\nsurprised when it breaks during an actual emergency.\nTo be prepared at every level, a strategy of practicing disaster recovery tech-\nniques at the individual, team, and organization levels is required. Each level is\ndependent on the competency achieved in the previous level.\nWheel of Misfortune is a game that trains individuals by talking through com-\nmon, and not so common, disaster scenarios. Fire drills are live tests performed\nto exercise a particular process. Fire drills should ﬁrst be performed on a process\nagain and again by the same people until the process works and can be performed\nsmoothly. Then the process should be done by each member of the team until\neveryone is conﬁdent in his or her ability to perform the task.\nTests involving shutting down randomly selected machines or servers can ﬁnd\nuntested failure scenarios. These tests can be done at designated times as a test, or\ncontinuously as part of production to ensure that systems that should be resilient\nto failure have not regressed.\nGame Day or DiRT exercises are organization-wide tests that ﬁnd gaps in\nprocesses that involve multiple teams. They often last multiple days and involve\ncutting off major systems or datacenters.\nDiRT events require a large amount of planning and coordination to reduce\nrisk. Tests should be approved by a central planning committee based on quality,\n\n\n330\nChapter 15\nDisaster Preparedness\nimpact, and risk. Some tests are initiated by individual teams, whereas tests with\ncompany-wide impact are planned by the central committee.\nFire, police, medical, and other public safety organizations use the Incident\nCommand System (ICS) to coordinate efforts during disasters and other emergency\nsituations. The ICS provides standardized organizational structure, processes, and\nterminology. This system can be adapted for use in coordinating recovery from\nmajor IT-related outages. To stay in practice, teams should use ICS procedures for\nsmaller incidents as well.\nExercises\n1. What is an antifragile system? How is this different from fragile systems and\nresilient systems?\n2. What are some reasons to do ﬁre drills as part of reliability testing?\n3. Draw a graph that represents the ideal frequency of failures as discussed in\nSection 15.1.1.\n4. Describe a critical subsystem in your own environment that could beneﬁt from\nreliability testing, and explain how you would test it.\n5. Compare and contrast the traditional attitude toward failure with the DevOps\nattitude toward failure. Which lessons can you apply to your own environ-\nment?\n6. Why not ﬁre people who make mistakes? Wouldn’t this result in an organiza-\ntion that employs only perfect people?\n7. Which situations have you been part of that might have beneﬁted from using\nthe Incident Command System? Which speciﬁc beneﬁts would have occurred?\n8. Describe how the ICS process works in emergency services and public safety.\n9. Describe how the ICS process could be applied to IT.\n10. List the key concepts that go into an Incident Action Plan.\n11. How would you attempt to convince your management of the necessity of\nconducting ﬁre-drill exercises in your environment?\n",
      "page_number": 351
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 361-368)",
      "start_page": 361,
      "end_page": 368,
      "detection_method": "topic_boundary",
      "content": "Chapter 16\nMonitoring Fundamentals\nYou can observe a lot\nby just watching.\n—Yogi Berra\nMonitoring is the primary way we gain visibility into the systems we run. It is the\nprocess of observing information about the state of things for use in both short-term\nand long-term decision making. The operational goal of monitoring is to detect the\nprecursors of outages so they can be ﬁxed before they become actual outages, to\ncollect information that aids decision making in the future, and to detect actual\noutages. Monitoring is difﬁcult. Organizations often monitor the wrong things and\nsometimes do not monitor the important things.\nThe ideal monitoring system makes the operations team omniscient and\nomnipresent. Considering that having the root password makes us omnipotent,\nwe are quite the omniarchs.\nDistributed systems are complex. Being omniscient, all knowing, means our\nmonitoring system should give us the visibility into the system to ﬁnd out any-\nthing we need to know to do our job. We may not know everything the monitoring\nsystem knows, but we can look it up when we need it. Distributed systems are too\nlarge for any one person to know everything that is happening.\nThe large size of distributed systems means we must be omnipresent, existing\neverywhere at the same time. Monitoring systems permit us to do this even when\nour systems are distributed around the world. In a traditional system one could\nimagine a system administrator who knows enough about the system to keep an\neye on all the critical components. Whether or not this perception is accurate, we\nknow that in distributed systems it is deﬁnitely not true.\nMonitoring in distributed computing is different from monitoring in enter-\nprise computing. Monitoring is not just a system that wakes you up at night when\na service or site is down. Ideally, that should never happen. Choosing a strategy\n331\n\n\n332\nChapter 16\nMonitoring Fundamentals\n.\nTerms to Know\nServer: Software running to provide a function or API. (Not a piece of\nhardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nDiurnal Cycles: Metrics that are high during the day and low at night.\nthat involves reacting to outages means that we have selected an operational strat-\negy with outages “baked in.” We can improve how fast we respond to an outage\nbut the outage still happened. That’s no way to run a reliable system.\nInstead, monitoring should be designed to detect the precursors of an outage\nin time for the problem to be prevented. A system must be instrumented and mon-\nitored so as to enable this strategy. This is more difﬁcult than detecting an outage,\nbut much better.\n16.1 Overview\nTo understanding monitoring you must ﬁrst understand its particular terminology.\nA measurement refers to a single point of data describing an aspect of a sys-\ntem, usually a value on which numerical operations make sense—for example,\n5, −25, 0, or Null. It can also be a string—for example, a version number or a\ncomma-separated list of currently mounted ﬁle systems.\nA metric is a measurement with a name and timestamp. For example:\nspellcheck:server5.demo.com:request-count@20140214T100000Z = 9566\nDifferent monitoring systems use different formats to label the data. In the\npreceding example, we have a metric from the spellcheck server that was running\non server5. The measurement is the request-count, presumably how many requests\nthe server has received since starting. After the @ symbol is the date stamp. The\nmeasured value is 9566.\nExamples of other metrics include the total RAM in use, the number of out-\nstanding requests in a queue, the build version of the code being run, the number\nof open bugs, the number of bugs, and the total cost on Amazon AWS.\nThere is also metadata associated with a metric. In particular, a numeric\nvalue usually has a unit associated with it. Knowing the unit permits automatic\n\n\n16.1\nOverview\n333\nconversions, chart labeling, and so on. Many monitoring systems do not track\nthe units of a metric, requring people to guess based on the metric name or\nother context, and perform conversions manually. This process is notably prone to\nerror.\nMeasurement frequency is the rate at which new measurements are taken.\nDifferent metrics are collected at different frequencies. Many pieces of data are\ncollected every 5 minutes, but some metrics are collected many times a second and\nothers once a day or once a week.\nMonitoring perspective is the location of the monitoring application collect-\ning the measurement. The importance of perspective depends on the metric. The\ntotal number of packets sent on an interface is the same no matter the perspec-\ntive. In contrast, page load times and other timing data depend on the perspec-\ntive. One might collect the same measurement from different places around the\nworld to see how page load time is affected by distance or to detect problems\nwith a particular country or ISP’s connectivity. Alternatively, the measurement\nmay be collected directly on a machine if perspective is less important than\nconsistency.\nOne way to gather information from many perspectives is real user monitor-\ning (RUM). RUM collects actual application performance measurements from the\nweb browser. Code is inserted into the web page that collects metrics and transmits\nit back to the site that sent the original web page.\n16.1.1 Uses of Monitoring\nMonitoring is not just about collecting data. It is also used in various ways, and\nthere is speciﬁc terminology for those uses.\nVisualization is the assimilation of multiple measurements into a visual rep-\nresentation. These charts and graphs make it possible to ﬁnd trends and make\ncomparisons between systems and time ranges.\nA trend is the direction of a series of measurements on a metric. For example,\none might use visualization to see that use of a service is growing or shrinking.\nAlerting means to bring something to the attention of a person or another\nsystem. A sudden drop in QPS might result in alerting the oncall person.\nAlerts generally need to be acknowledged within a certain amount of time.\nIf the deadline is exceeded, someone else is alerted. This process is called\nescalation.\nVisualization is useful for gaining deeper insights into the system. It can help\nwith everything from design to planning to communication. Trends can be used\nfor capacity planning, which is discussed in more detail in Chapter 18. Alerting\nshould be used to warn people about situations that could result in an outage. It is\nalso used as a last resort to warn when an outage has occurred.\n\n\n334\nChapter 16\nMonitoring Fundamentals\n16.1.2 Service Management\nLastly, here are some service management terms we will use occasionally. They\nmostly come from the business world:\n• Service Level Indicator (SLI): An agreement as to how a measurement will be\nmeasured. For example, it might deﬁne what is measured, how it is measured,\nand from what perspective.\n• Service Level Target (SLT): A target quality of service; in other words, an SLI’s\nexpected minimum or maximum. Until ITIL V3, this was called the Service\nLevel Objective (SLO). An example SLT might be a certain level of availability\nor the maximum permitted latency.\n• Service Level Agreement (SLA): The contract that states the SLIs, SLTs, and\nthe penalties if the SLTs are not met. For example, there may be a refund or\npenalty payment for any outage longer than one hour. The term SLA is often\noverused to mean SLT or any service level requirement.\n16.2 Consumers of Monitoring Information\nThere are many consumers of monitoring information, each of which has differ-\nent needs. People who monitor operational health need to know state changes\nimmediately for fast response—they need real-time monitoring. Capacity plan-\nners, product managers, and others need metrics collected over a long period of\ntime to spot trends—they need historical monitoring data.\nA more ﬁne-grained way to differentiate consumers is the Dickson model\n(Dickson 2013), which uses three characteristics: resolution, latency, and diversity.\nThese characteristics can be rated as high or low.\nResolution describes how frequently the metric is collected. High (R+) is many\ntimes a second, minute, or hour. Low (R−) is many times a day.\nLatency describes how long a period of time passes before the information is\nacted upon. Low (L+) is real-time response. High (L−) means data is stored and\nanalyzed later, perhaps used for daily, weekly, or monthly statistics. (Note that the\nuse of + and −are reversed from R and D. Think of + as the case that is more\ndifﬁcult to engineer.)\nDiversity describes how many metrics are being collected. High (D+) means\nmany metrics, perhaps many measurements about many different services. Low\n(D−) means there is a focus on a particular or small set of metrics.\nConsumers can be described by a 3-tuple. For example, (R+, L−, D+) describes\na high-resolution, high-latency, high-diversity consumer.\n\n\n16.2\nConsumers of Monitoring Information\n335\nGiven these axes, we can describe the primary users of monitoring information\nas follows:\nOperational Health/Response (OH)\n(R+, L+, D+)\nHigh resolution, low latency, high diversity.\nSystem health. The things we get paged about.\nQuality Assurance/SLA (QA)\n(R+, L−, D+)\nHigh resolution, high latency, high diversity.\nLonger-term analysis of jitter, latency, and\nother quality-related factors.\nCapacity Planning (CP)\n(R−, L−, D+)\nLow resolution, high latency, high diversity.\nForecasting and purchasing more resources.\nProduct Management (PM)\n(R−, L−, D−)\nLow resolution, high latency, low diversity.\nDetermining the number of users, cost, and other\nresources.\nOperational Health is typical monitoring, where exceptional situations are\ndetected and alerts are generated. It is the most demanding use case. The resolu-\ntion and latency must be sufﬁcient to detect problems and respond to them within\nan SLA. This usually demands up-to-date access to all metrics, real-time computa-\ntion for high-speed analysis, and reliable alerting. The storage system must be high\nspeed and high volume at the same time. In fact, this kind of monitoring stresses\nevery part of the monitoring infrastructure.\nQuality Assurance usually involves medium- or long-term analysis for spe-\nciﬁc quality metrics such as variability. For example, some queries should always\ntake approximately the same amount of time with little variation. Quality assur-\nance detects this kind of variability just as an auto assembly line quality assurance\nteam looks for defects and unacceptable variations in the product being built. For\nthis reason, Quality Assurance needs high-resolution data but latency is not critical\nsince the data is often processed in batches after the fact.\nQuality Assurance also includes information required when ﬁnding and ﬁx-\ning bugs, such as debug logs, process traces, stack traces, coredumps, and proﬁler\noutput.\nCapacityPlanning(CP)istheprocessofpredictingresourceneedsinthefuture.\nThese predictions require coarse metrics such as the current number of machines,\namount of network bandwidth used, cost per user, and machine utilization and efﬁ-\nciency, as well as alerting when resources arerunninglow. CP is also concerned with\nhow resource use changes as the product changes—for example, if a new release\nrequires signiﬁcantly different resources. CP is the topic of Chapter 18.\n\n\n336\nChapter 16\nMonitoring Fundamentals\nProduct Management (PM) requires very low-resolution data for calculating\nkey performance indicators (KPIs) such as conversion rates, counts of users, and\nanalysis of user retention (often called 7-day actives or 30-day actives). PM beneﬁts\nfrom large amounts of historic data for long-term views and visualizations to help\nunderstand trends.\n.\nMissing (R, L, D) Combinations\nA keen observer will notice that not every combination of R, L, and D\nappears in the model. Some do describe other operations-related functions.\n(R+, L+, D−) is what a load balancer requires to determine if it should send\ntrafﬁc to a particular backend. (R+, L−, D−) covers the kind of log anal-\nysis that is done in periodically in batches. This leaves (R−, L+, D−) and\n(R−, L+, D+), which we have not observed in the wild…yet.\n16.3 What to Monitor\nWhat should be monitored is different for every organization. The general strategy\nis to start with the business’s KPIs and collect related measurements.\nThe following are some example KPIs:\n• Availability: Have a web site that is up 99.99 percent of the time (measured\nfrom outside the datacenter).\n• Latency: The 90th percentile latency for the homepage should not exceed\n400 ms (request to render time).\n• Urgent Bug Count: There should be no more than n outstanding Severity-0\nbugs.\n• Urgent Bug Resolution: All Severity-0 bugs should be closed within 48 hours.\n• Major Bug Resolution: All Severity-1 bugs should be closed within 10 days.\n• Backend Server Stability: There should be no more than n percent queries\nreturning HTTP 5xx Server Error.\n• User Satisfaction: There should be no more than n percent abandoned carts.\n• Cart Size: The median number of items in a shopping card per order should\nbe n.\n• Finance: A total of n revenue this month.\nIf you do not know your organization’s KPIs, stop right now and ﬁnd out what\nthey are. If there aren’t any, prepare your resume because your company is a\n\n\n16.3\nWhat to Monitor\n337\nrudderless ship. Alternatively, you can declare yourself the captain and invent the\nKPIs yourself.\nYou need to instrument your system enough so that you can see when things\nare going to fail. To improve on KPIs, we must measure more than just the end\nresult. By measuring things that are deeper in the pipeline, we can make better\ndecisions about how to achieve our KPIs. For example, to improve availability,\nwe need to measure the availability of the component systems that result in the\nﬁnal availability statistic. There may be systems that get overloaded, queues that\ngrow too long, or bottlenecks that start to choke. There may be a search tree that\nworks best when the tree is balanced; by monitoring how balanced it is, we can\ncorrelate performance issues with when it becomes imbalanced. To determine why\nshopping carts are being abandoned, we must know if there are problems with any\nof the web pages during the shopping experience.\nAll of the previously mentioned KPIs can be monitored by selecting the right\nmetrics, sometimes in combination with others. For example, determining the 90th\npercentile requires some calculation. Calculating the average number of items per\norder might require two metrics, the total number of items and the total number\nof orders.\nA diagnostic is a metric collected to aid technical processes such as debug-\nging and performance tuning. These metrics are not necessarily related to a KPI.\nFor example, we might collect a metric that helps us debug an ongoing technical\nissue that is intermittent but difﬁcult to ﬁnd. There is generally a minimal set of\nmetrics one collects from all machines: system metrics related to CPU, network\nbandwidth, disk space, disk access, and so on. Being consistent makes manage-\nment easier. Hand-crafting a bespoke list for each machine is rarely a good use of\nyour time.\nIt is conventional wisdom in our industry to “monitor everything” in hopes\nof preventing the situation where you suddenly realize you wish you had historic\ndata on a particular metric. If this dictate is taken literally, the metrics collection\ncan overwhelm the systems being monitored or the monitoring system itself. Find\na balance by focusing on KPIs ﬁrst and diagnostics as needed.\n.\nThe Minimum Monitor Problem\nA common interview question is “If you could monitor only three aspects of a\nweb server, what would they be?” This is an excellent test of technical knowl-\nedge and logical thinking. It requires you to use your technical knowledge to\nﬁnd one metric that can proxy for many possible problems.\nFor example, much can be learned by performing an HTTPS GET: We\nlearn whether the server is up, if the service is overloaded, and if there is\n\n\n338\nChapter 16\nMonitoring Fundamentals\n.\nnetwork congestion. TCP timings indicate time to ﬁrst byte and time to full\npayload. The SSL transaction can be analyzed to monitor SSL certiﬁcate valid-\nity and expiration. The other two metrics can be used to differentiate between\nthose issues. Knowing CPU utilization can help differentiate between network\ncongestion and an overloaded system. Monitoring the amount of free disk\nspace can indicate runaway processes, logs ﬁlling the disk, and many other\nproblems.\nWe recommend that you blow the interviewer away by offering to do\nall that while measuring one metric and one metric only. Assuming it is an\ne-commerce site, simply measure revenue. If it drops in a way that is unchar-\nacteristic for that time of day, the site is overloaded. If it stops, the site is down\n(and if it isn’t down, there is reason to investigate anyway). If it ramps up,\nwe know we’re going to run out of capacity soon. It is the one KPI that ties\neverything together.\n16.4 Retention\nRetention is how long collected metric data is stored. After the retention time has\nelapsed, the old metric data is expired, downsampled, or deleted from the storage\nsystem.\nHow long monitoring data is retained differs for each organization and ser-\nvice. Generally there is a desire or temptation to store all metrics forever. This\navoids the problem of suddenly realizing the data you want was deleted. It is also\nsimpler than having to decide on a storage time for each metric.\nUnfortunately, storing data forever has a cost—not just in terms of hardware\nfor storage, but also in terms of backups, power, and complexity. Store enough\ndata, and it must be split between multiple storage systems, which is complex.\nThere may also be legal issues around how long the data should be retained.\nCreating your retention policy should start with collecting business require-\nments and goals, and translating them into requirements for the storage system.\nTwo years is considered to be the minimum storage period because it enables\nyear-over-year comparisons. More is better. It is likely that your next monitoring\nsystem will be unable to read the data collected by the previous system, and a\nconversion process is unlikely to be available. In such a case, if you build a new\nmonitoring system every ﬁve or six years, that may be an upper bound. Time-\nseries databases are becoming more standardized and easier to convert, however,\nmaking this upper bound likely to disappear.\nHaving the ability to retain decades of monitoring data at full resolution has\nbeneﬁts we are just beginning to understand. For example, Google’s paper “Failure\nTrends in a Large Disk Drive Population” (Pinheiro, Weber & Barroso 2007) was\n",
      "page_number": 361
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 369-376)",
      "start_page": 369,
      "end_page": 376,
      "detection_method": "topic_boundary",
      "content": "16.5\nMeta-monitoring\n339\nable to bust many myths about hard disk reliability because the authors had\naccess to high-resolution monitoring data from hundreds of thousands of hard\ndrives’ self-monitoring facility (SMART) collected over ﬁve years. Of course, not\neveryone has seemingly inﬁnite data storage facilities. Some kind of consolidation\nor compaction is needed.\nThe easiest consolidation is to simply delete data that is no longer needed.\nWhile originally many metrics might be collected, many of them will turn out to\nbe irrelevant or unnecessary. It is better to collect too much when setting up the\nsystem than to wish you had data that you didn’t collect. After you run the service\nfor a while, certain metrics may be deemed unnecessary or may be useful only in\nthe short term. For example, there may be speciﬁc CPU-related metrics that are\nuseful when debugging current issues but whose utility expires after a year.\nAnother way to reduce storage needs is through summarization, or down-\nsampling. With this technique, recent data is kept at full ﬁdelity but older data is\nreplaced by averages or other form of summarization. For example, metrics might\nbe collected at 1- or 5-minute intervals. When data is more than 13 months old,\nhourly averages, percentiles, maximums, and minimums are calculated and the\nraw data is deleted. When the data is even older, perhaps 25–37 months, 4-hour\nor even daily summaries are calculated, reducing the storage requirements even\nmore. Again, the amount of summarization one can do depends on business needs.\nIf you need to know only the approximate bandwidth utilization, daily values may\nbe sufﬁcient.\n16.5 Meta-monitoring\nMonitoring the monitoring system is called meta-monitoring. How do you know if\nthe reason you haven’t been alerted today is because everything is ﬁne or because\nthe monitoring system has failed? Meta-monitoring detects situations where the\nmonitoring system itself is the problem.\nThe monitoring system needs to be more available and scalable than the\nservices being monitored. Every monitoring system should have some kind of\nmeta-monitoring. Even the smallest system needs a simple check to make sure it is\nstill running. Larger systems should be monitored for the same scale and capacity\nissues as any other service to prevent disk space, CPU, and network capacity from\nbecoming limiting factors. The accuracy and precision of collected data should be\nmonitored; one should monitor how often an attempt to collect a measurement\nfails. The display of information must be faster than the average person’s attention\nspan. The freshness of the data used to calculate a KPI should be monitored; know-\ning the age of data used to calculate a KPI may be as important as the KPI itself.\nKnowing how KPI latency is trending is important to understanding the health of\nthe monitoring system.\n\n\n340\nChapter 16\nMonitoring Fundamentals\nOne meta-monitoring technique is to deploy a second monitoring system that\nmonitors the primary system. It should have as few common dependencies as\npossible. If different software is used, it is less likely that the same bug will take\ndown both systems. This technique, however, adds complexity, requires additional\ntraining, and necessitates maintenance.\nAnother technique is to divide the network into two parts, each with its own\nmonitoring system. The two monitoring systems can also monitor each other. For\nexample, a site with two datacenters might deploy a different monitoring system in\neach one that monitors local machines. This saves on inter-datacenter bandwidth\nand removes the interconnection as a source of failure.\nWith more than one datacenter, a similar arrangement can be used, with\npairs of datacenters monitoring each other, or each monitoring system monitoring\nanother system in a big circle.\n16.6 Logs\nAnother way of gaining visibility into the system is through analysis of logging\ndata. While not directly related to monitoring, we mention this capability here\nbecause of the visibility it brings. There are many kind of logs:\n• Web “Hit” Logs: Web servers generally log each HTTP access along with\nstatistics about performance, where the access came from, and if the access\nwas a success, error, redirect, and so on. This data can be used for a multitude\nof business purposes: determining page generation times, analyzing where\nusers come from, tracking the paths users take through the system, and more.\nTechnical operations can use this data to analyze and improve page load time\nand latency.\n• API Logs: Logging each API call generally involves storing who made the\ncall, the input parameters, and output results (often summarized as a simple\nsuccess or error code). API logs can be useful for billing, security forensics,\nand feature usage patterns. A team that is going to eliminate certain obsolete\nAPI calls can use logs to determine which users will be affected, if any.\n• System Logs: The operating system kernel, devices, and system services con-\ntribute to the system log. This is often useful for tracking hardware problems\nand system changes.\n• Application Logs: Each service or server generates logs of actions. This is use-\nful for studying errors and debugging problems, as well as providing business\ninformation such as which features are used the most.\n• Application Debug Logs: Applications often generate debugging informa-\ntion in a separate log. This type of log is often more verbose but is retained\nfor a shorter amount of time. Such logs are used for debugging problems by\ndevelopers and operations staff.\n\n\n16.6\nLogs\n341\n16.6.1 Approach\nWhile logs do not directly ﬁt the Dickson model, if they did they would be\n(R+, L−, D−) because they are high resolution (generally one log entry per action),\nhigh latency (often processed much later in batches), and low diversity (usually\ncollected about a particular topic, such a web server hits).\nThe architecture of a log processing system is similar to the monitoring archi-\ntecture discussed in the next chapter. Generally logs are collected from machines\nand services and kept in a central place for storage and analysis. Retention is often\na legal issue, as logging data often contains personal information that is regulated.\nThe market space for log analysis tools has been growing over the years as\nnew analysis methods are invented. Web logs can be analyzed to determine the\npath a user takes through the system. As a result one can ﬁnd user interface\n“dead ends” that leave users bafﬂed. Application logs can be used to ﬁnd ﬂaws\nin sales processes or to identify high-value customers who otherwise would not\nhave been discovered. System logs can be analyzed to ﬁnd anomalies and even\npredict hardware failures.\nThe consolidation of logging data can be rather complex, as different systems\ngenerate logs in different formats. It’s best to establish a single log format for all\nsystems. By providing a software library that generates logs that conform to the\nformat, you can make the path of least resistance (using the library) be the path\nthat has the desired behavior (conforms to the standard).\n16.6.2 Timestamps\nRecord timestamps the same way in all systems to make it easier to combine and\ncompare logs. In particular, all machines should use NTP or the equivalent to keep\nclocks synchronized, and timestamps should be stored in UTC rather than local\ntime zones.\nThis is important because often logs from different systems are compared\nwhen debugging or ﬁguring out what went wrong after an outage. For exam-\nple, when writing a postmortem one builds a timeline of events by collecting\nlogs from various machines and services, including chat room transcripts, instant\nmessage sessions, and email discussions. If each of these services records time-\nstamps in its local time zone, just ﬁguring out the order of what happened can\nbe a challenge, especially if such systems do not indicate which time zone was\nbeing used.\nConsolidating logs on a large scale is generally automated and the consolida-\ntion process normalizes all logs to UTC. Unfortunately, conﬁguration mistakes can\nresult in logging data being normalized incorrectly, something that is not noticed\nuntil it is too late. This problem can be avoided by using UTC for everything.\nGoogle famously timestamps logs using the U.S./Paciﬁc time zone, which\ncaused no end of frustration for Tom when he worked there. This time zone has\n\n\n342\nChapter 16\nMonitoring Fundamentals\na different daylight savings time calendar than Europe, making log normalization\nextra complex two weeks each year, depending on the year. It also means that\nsoftware must be written to understand that one day each year is missing an hour,\nand another day each year has an extra hour. Legend has it that the U.S./Paciﬁc\ntime zone is used simply because the ﬁrst Google systems administrator did not\nconsider the ramiﬁcations of his decision. The time zone is embedded so deeply in\nGoogle’s many systems that there is little hope it will ever change.\n16.7 Summary\nMonitoring is the primary way we gain visibility into the systems we run. It\nincludes real-time monitoring, which is used to alert us to exceptional situations\nthat need attention, and long-term or historic data collection, which facilitates trend\nanalysis. Distributed systems are complex and require extensive monitoring. No\none person can watch over the system unaided or be expected to intuit what is\ngoing on.\nThe goal of monitoring is to detect problems before they turn into outages,\nnot to detect outages. If we simply detect outages, then our operating process has\ndowntime “baked in.”\nA measurement is a data point. It refers to a single point of data describing an\naspect of a system, usually a numerical value or a string. A metric is a measurement\nwith a name and a timestamp.\nDeciding what to monitor should begin with a top-down process. Identify the\nbusiness’s key performance indicators (KPIs) and then determine which metrics\ncan be collected to create those KPIs.\nMonitoring is particularly important for distributed systems. By instrument-\ning systems and servers and automatically collecting the exposed metrics, we\ncan become the omniscient, omnipresent, omnipotent system administrators that\nstakeholders assume we are.\nExercises\n1. What is the goal of monitoring?\n2. Why is monitoring important in distributed systems?\n3. Why should operations staff be omniscient, omnipresent, and omnipotent?\nWhy are these important characteristics?\n4. How is a measurement different from a metric?\n5. If you could monitor only three things about a web site, what would they be?\nJustify your answer.\n\n\nExercises\n343\n6. Section 16.2 mentioned that there are no situations where (R−, L+, D−) and\n(R−, L+, D+) had been observed. Why might this be?\n7. Most monitoring systems “ping” devices to determine if they are up. This\nentire chapter never mentions ping. Based on what you learned in this chap-\nter, why is pinging devices an insufﬁcient or failed operational strategy for\nmonitoring?\n\n\nThis page intentionally left blank \n\n\nChapter 17\nMonitoring Architecture and\nPractice\nWhat is a cynic? A man who\nknows the price of everything\nand the value of nothing.\n—Oscar Wilde\nThis chapter discusses the anatomy of a monitoring system. Best practices for each\ncomponent are described along the way.\nA monitoring system has many different parts. Dickson (2013) decomposes\nmonitoring systems into the functional components depicted in Figure 17.1. A mea-\nsurement ﬂows through a pipeline of steps. Each step receives its conﬁguration\nfrom the conﬁguration base and uses the storage system to read and write metrics\nand results.\nThe sensing and measurement system gathers the measurements. The col-\nlection system transports them to the storage system. From there, one or more\nanalysis systems extract meaning from the raw data—for example, detecting prob-\nlems such as a server being down or anomalies such as a metric for one system\nbeing signiﬁcantly dissimilar from all the others. The alerting and escalation sys-\ntem communicates these conditions to interested parties. Visualization systems\ndisplay the data for human interpretation. Each of these components is told how\nto do its job via information from the configuration base.\nOver the years there have been many monitoring products, both commercial\nand open source. It seems like each one has been very good at two or three compo-\nnents, but left much to be desired with the others. Some systems have done many\ncomponents well, but none has done them all well.\nInterfaces between the components are starting to become standardized in\nways that let us mix and match components. This enables faster innovation as new\nsystems can be created without having to reinvent all the parts.\n345\n\n\n346\nChapter 17\nMonitoring Architecture and Practice\nFigure 17.1: The components of a monitoring system\nWhat follows is a deeper discussion of each component, its purpose, and\nfeatures that we’ve found to be the most useful.\n17.1 Sensing and Measurement\nThe sensing and measurement component gathers the measurements. Measure-\nments can be categorized as blackbox or whitebox, depending on the amount of\nknowledge of the internals that is used. Measurements can be direct or synthesized,\ndepending on whether each item is separately counted, or totals are periodically\nretrieved and averaged. We can monitor the rate at which a particular operation is\nhappening, or the capability of the system to allow that operation to happen. Sys-\ntems may be instrumented to provide gauges, such as percentage CPU utilization,\nor counters, such as the number of times that something has occurred.\nLet’s look at each of those in more detail.\n17.1.1 Blackbox versus Whitebox Monitoring\nBlackbox monitoring means that measurements try to emulate a user. They treat\nthe system as a blackbox, whose contents are unknown. Users do not know how\na system’s internals work and can only examine the external properties of the\n",
      "page_number": 369
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 377-384)",
      "start_page": 377,
      "end_page": 384,
      "detection_method": "topic_boundary",
      "content": "17.1\nSensing and Measurement\n347\nsystem. They may guess about the internals, but they cannot be sure. In other\nwords, these measurements are done at a high level of abstraction.\nDoing an HTTPS GET of a web site’s main page is an example of blackbox\nmonitoring. The measurement is unaware of any load balancing infrastructure,\ninternal server architecture, or which technologies are in use. Nevertheless, from\nthis one measurement, we can determine many things: whether the site is up, how\nfast is it responding, if the SSL certiﬁcate has expired, if the system is producing\na valid HTML document, and so on. Blackbox testing includes monitoring that\nattempts to do multi-step processes as a user, such as verifying that the purchasing\nprocess is working.\nIt is tempting to get a particular web page and verify that the HTML received\nis correct. However, then the monitoring system must be updated anytime the web\nsite changes. An alternative is to verify that the page contains a particular message.\nFor example, a search engine might periodically query for the name of a particular\ncelebrity and verify that the name of the person’s fan club is mentioned in the\nHTML. The measurement communicated to the monitoring system would be 1\nor 0, representing whether the string was or wasn’t found.\nBlackbox monitoring is often highly dependent on perspective. Monitoring\nmight be done from the machine itself, from within the datacenter or other failure\ndomain, or from different places around the world. Each of these tests answers\ndifferent questions about the system.\nA whitebox measurement has the beneﬁt of internal knowledge because it\nis a lower level of abstraction. For example, such a measure might monitor raw\ncounters of the number of times a particular API call was made, the number of\noutstanding items waiting in a queue, or latency information of internal processes.\nSuch measurements may disappear or change meaning as the system’s internals\nchange. In whitebox monitoring, perspective is less important. Usually such mea-\nsurements are done as directly as possible to be more efﬁcient and to prevent data\nloss that could occur by adding distance.\n17.1.2 Direct versus Synthesized Measurements\nSome measurements are direct, whereas others are synthesized. For example, if\nevery time a purchase is made the system sends a metric of the total money col-\nlected to the monitoring system, this is a direct measurement. Alternatively, if\nevery 5 minutes the monitoring system tallies the total money collected so far,\nthis is a synthesized metric. If all we have is a synthesized metric, we cannot\nback-calculate the individual amounts that created it.\nWhether direct measurements are available is generally a function of fre-\nquency of change. In an e-commerce site where one or two purchases are made\neach day, direct measurement is possible. By comparison, for a busy e-commerce\nsite such as eBay or Amazon, the monitoring system could not keep up with the\n\n\n348\nChapter 17\nMonitoring Architecture and Practice\nnumber of purchases made if each one resulted in a metric. In this case the synthe-\nsized metric of total revenue collected is all that is possible. Exact counts would be\nthe job of the accounting and billing system.\nA more obvious example is measuring how much disk space is in use versus\nbeing told about every block allocation and deallocation. There can be millions of\ndisk operations each minute. Collecting the details of each operation just to arrive\nat a metric of how much disk space is in use would be a waste of effort.\n17.1.3 Rate versus Capability Monitoring\nEvent frequency determines what to monitor. One wants to know that a customer\ncan make purchases and is making purchases. The rate at which purchases hap-\npen determines which of these to monitor. If one or two purchases are made each\nday, monitoring whether customers can make purchases is important because it\nhappens so infrequently that we need to verify that the purchasing ﬂow hasn’t\nsomehow broken. If thousands of purchases per minute are made, then we gener-\nally know purchases can be made and it is more important to measure the rate and\ntrends.\nIn short, rate metrics are more important when event frequency is high and\nthere are smooth, predictable trends. When there is low event frequency or an\nuneven rate, capability metrics are more important.\nThat said, never collect rates directly. Rates are lossy. Given two counts and\ntheir time delta, we can compute the rate. Given two rates and their time delta, we\ncan only surmise the count—and then only if the two measurements were collected\nat precise intervals that are in sync with the rate’s denominator.\nFor example, if we are measuring the count of how many API calls have been\nreceived, we might collect a metric of 10,000 at 12:10:00 and 16,000 at 12:15:00. We\ncould then calculate the rate by dividing the delta of the counts by the time delta:\n(16, 000−10, 000)/300,or20APIcallspersecond.Supposethatsecondpollingevent\nwas skipped, perhaps due to a brief network problem, the next polling event would\nbe at 12:20:00 and might collect the measurement of 21,000. We can still calculate\nthe rate: (21, 000 −10, 000)/600, or 18.2 API calls per second. However, if we were\ncollecting rates, that network blip would mean we would not be able to estimate\nwhat happened during the missing 5 minutes. We could take the average of the two\nadjacent rates, but if during that time there was a large spike, we would not know.\n17.1.4 Gauges versus Counters\nSome measurements are gauges, while others are counters.\nA gauge value is an amount that varies. It is analogous to a real-world gauge\nlike one that indicates barometric pressure. Examples include an indicator of how\nmuch unused disk space is available, a temperature reading, and the number\n\n\n17.1\nSensing and Measurement\n349\nof active processes on a system. A gauge’s value varies, going up and down as\nwhatever it measures increases or decreases.\n.\nHigh-Speed Network Counters\nThe frequency and potential rate of change should determine the size of the\ncounter. If the counter that stores the number of bits transmitted on a 10 Gbps\ninterface is 32 bits long, it will roll over to zero after 4,294,967,296 bits have\nbeen transmitted. If the interface is running at about 50 percent capacity, the\ncounter will roll over about every 8.5 seconds. It is not possible to correctly\nsynthesize the metric for trafﬁc across this interface unless you collect data\nfaster than the counter rolls over. Therefore a counter that changes this quickly\nrequires a 64-bit counter.\nA counter is a measurement that only increases—for example, a count of the\nnumber of API calls received by a service or the count of the number of packets\ntransmitted on a network interface. A counter does not decrease or run backward.\nYou can’t unsend a packet or deny that an API call was received after the fact.\nHowever, counters do reset to zero in two circumstances. Unless a counter’s value\nis in persistent storage, the value is reset to zero on reboot or restart. Also, like an\nautomobile odometer that rolls over at 99,999, a counter loops back to zero when\nit reaches its maximum value. Typically counters are 16-, 32-, or 64-bit integers\nthat have maximum values of 65,535, 4,294,967,295, and 18,446,744,073,709,551,615,\nrespectively (i.e., 2n −1, where n is the number of bits).\nDoing math on counter readings is rather complex. In our previous example,\nwe simply subtracted adjacent counter values to learn the delta. However, if the\nsecond reading is less than the ﬁrst, special care must be taken.\nIf the counter looped around to zero, the actual delta between two read-\nings includes the count up to the maximum value plus the newer counter value:\n(maxvalue −Rn) + Rn+1. However, if the counter was reset to zero due to a restart,\nthen the exact delta value is unobtainable because we do not know the value of\nthe counter when the restart occurred. We can make estimates using a little high\nschool calculus involving the past rate and how long ago the reset occured.\nDetermining whether the counter reset to zero due to a restart or reaching\nthe maximum value is another heuristic. If the last reading was near the maxi-\nmum value, we can assume the counter looped around to zero. We can improve\nthe accuracy of this guess, again by applying a little calculus.\nSuch great complexity still results in a margin of error and risks introducing\na loss of accuracy if either heuristic produces incorrect results. A much simpler\napproach is to ignore the problem. For 64-bit counters, wrap-arounds are rare or\n\n\n350\nChapter 17\nMonitoring Architecture and Practice\nnonexistent. For counter resets, if the monitoring frequency is fast enough, the mar-\ngin of error will be very small. Usually the desired end result is not the count itself,\nbut a rate. The calculation of the rate for the entire interval will not be dramatically\naffected. For example, if there are 10 counter readings, 9 deltas are produced. If\na delta is negative, it indicates there was a counter reset. Simply throw away any\nnegative deltas and calculate the rate with the remaining data.\n.\nJava Counters\nSystems written in the Java programming language use counters that are\nsigned integers. Signed integers roll over to negative numbers and have a\nmaximum value that is approximately half their signed counterparts.\n17.2 Collection\nOnce we have a measurement, we must transmit it to the storage system. Met-\nrics are collected from many places and brought to a storage system so that they\nmay be analyzed. The metric’s identity must be preserved through any and all\ntransmissions.\nThere are hundreds of ways to collect metrics. Most fall into one of two camps:\npush or pull. Irrespective of the choice of a push or pull mechanism, there is also\na choice of which protocols are used for data collection. Another aspect of data\ncollection is whether the server itself that communicates directly with the collector,\nor whether an external agent acts as an intermediary between the server and the\ncollector. A monitoring system may have a central collector or regional collectors\nthat consolidate data before passing it back to a central system.\n17.2.1 Push versus Pull\nPush means the sensor that took the measurement transmits it to the collection\nmechanism. Pull means an agent polls the object being monitored and requests\nthe data and stores it.\nFour myths in monitoring are that pull doesn’t scale, that push is horrible and\nshouldn’t be used, that pull is horrible and shouldn’t be used, and that push doesn’t\nscale. Dispelling these myths is easy.\nThe ability to scale is a matter of implementation, not push versus pull. Moni-\ntoring 10,000 items at least once every 5 minutes requires 33 connections per second\nif spread evenly. A web server can handle millions of inbound TCP connections.\nOutbound connections are no different. A monitoring system has the beneﬁt that\n\n\n17.2\nCollection\n351\nit can make certain optimizations. It can transmit multiple measurements over\neach TCP connection. It can simply create a long-lived TCP connection for every\nmachine. Scaling is a concern, but not an insurmountable obstacle.\nWhether push or pull is better is a matter of application. Pull is better for syn-\nthesized measurements, such as counters. It is also better for direct measurements\nwhen change is infrequent. For example, if a change happens a few times per hour,\nbeing polled every 5 minutes may be sufﬁcient resolution. Pull is also better if it\nwould be useful to have a variable measurement rate, such as a system that polls\nat a higher frequency when diagnosing a problem. This approach is analogous to\nincreasing the magniﬁcation on a microscope.\nPush is better when direct observation is required, such as when we need to\nknow when something happens rather than how many times it has happened. In\nthe event of a failure, it is easier for a push system to store measurements and\ntransmit the backlog when possible. If it is useful to observe each discrete or speciﬁc\naction, then push is more appropriate.\n17.2.2 Protocol Selection\nMany different protocols are used in monitoring. The Simple Network Manage-\nment Protocol (SNMP) is horrible and should be avoided. Sadly, if you deal with\nnetwork equipment, it is likely your only choice. If you manage services and\nmachines, there are many alternatives.\nMost of the alternatives are migrating to using JSON transmitted over HTTP.\nJSON is a text-based standard for human-readable data interchange. HTTP PUT\nis used for push and HTTP GET is used for pull. Because the same JSON data\nstructure is sent in either case, it simpliﬁes the processing of the data.\n.\nSNMP: Simply Not a Management Protocol\nWe dislike SNMP. Most implementations transmit passwords in clear text and\nare inefﬁcient because they transmit one measurement per transaction. Ver-\nsion 3 of the protocol can be secure and fast but few vendors implement it.\nThe protocol is complex enough that implementations are a constant source\nof security-related bugs. The adjective “simple” was intended to describe the\nprotocol, which makes us concerned what the complex protocol, if it had been\ninvented, would look like. Because it scales so badly, we wonder whether\n“simple” actually describes the kind of network it is good for monitoring.\nThe use of the word “management” in its name has hurt the industry by cre-\nating the misperception that managing a network is a matter of collecting\nmeasurements.\n\n\n352\nChapter 17\nMonitoring Architecture and Practice\n17.2.3 Server Component versus Agent versus Poller\nCollectors can work in several different ways. The cleanest way to implement\nwhitebox monitoring is to have the running server gather its own measure-\nments and handle pushing or pulling the results to the collector. The soft-\nware that provides these functions can be put into a library for convenient\nuse by any program. For example, every time an API call is received, the\nsoftware might call metric.increment('api-calls-count') to increment the\ncount called api-calls-count. To collect how much bandwidth is being used\nin a video server, after each write to the network the software might call\nmetric.add('bandwidth-out', x), where x is the amount of data sent. The library\nwould maintain a running total under the name bandwidth-out. Functions like this\nare called throughout the program to count, total, or record measurements.\nIf the collection is done by push, the library spawns a thread that wakes up\nperiodically to push the metrics to the monitoring system. Pull is implemented by\nthe library spawning a thread that listens to a speciﬁc TCP port for polling events\nand replies with some or all of the metrics. If the server already processes HTTP\nrequests, it can simply attach the library to a particular path or route. For example,\nHTTP requests for /monitor would call the library, which would then reply to the\nrequest with a JSON representation of the metrics.\nSadly, we can’t always modify software to collect metrics this way. In that case\nwe run software on the machine that collects data by reading logs, status APIs, or\nsystem parameters and pulls or pushes the information. Such software is called an\nagent. For example, scollector is a software agent that runs on a Linux machine\nand calls any number of plug-ins to gather metrics, which are then pushed to the\nBosun monitoring system. Plug-ins are available that collect performance infor-\nmation about the operating system, disk systems, and many popular open source\nsystems such as MySQL and HBase.\nSometimes we cannot modify software or install software on the machine\nitself. For example, hardware devices such as network switches, storage area\nnetwork (SAN) hardware, and uninterruptible power supplies (UPSs) do not\npermit user-installed software. In these cases software is run on one machine\nthat polls the other devices, collects the information, and transmits it via push\nor pull to the monitoring system. This type of software is called a poller or a\nmonitoring proxy.\n17.2.4 Central versus Regional Collectors\nSome monitoring systems scale to global size by having a collector run in each\nregion and relay the data collected to the main monitoring system. This type of\ncollector is called a remote monitoring station or aggregator. An aggregator might\nbe placed in each datacenter or geographical region. It may receive metrics by push\nor pull, and it generally consolidates the information and transmits it to the main\n\n\n17.3\nAnalysis and Computation\n353\nsystem in a more efﬁcient manner. This approach may be used to scale a system\nglobally, saving bandwidth between each datacenter. Alternatively, it may be done\nto scale a system up; each aggregator may be able to handle a certain number of\ndevices.\n17.3 Analysis and Computation\nOnce the data has been collected, it can be used and interpreted. Analysis extracts\nmeaning from raw data. Analysis is the most important component because it\nproduces the results that justify having a monitoring system in the ﬁrst place.\nReal-time analysis examines the data as it is collected. It is generally the\nmost computationally expensive analysis and is reserved for critical tasks such as\ndetermining conditions where someone should be alerted. To do this efﬁciently,\nmonitoring systems tee the data as it is collected and send one copy to the storage\nsystem and another to the real-time analysis system. Alternatively, storage systems\nmay hold copies of recently collected metrics in RAM. For example, by keeping the\nlast hour’s worth of metrics in RAM and constructing all alerting rules to refer to\nonly the last hour of history, hundreds or thousands of alert rules can be processed\nefﬁciently.\nTypically real-time analysis involves dozens or hundreds of alert rules that are\nsimultaneously processed to ﬁnd exceptional situations, called triggers. Sample\ntriggers include if a service is down, if HTTP responses from a server exceed n\nms for x minutes, or if the amount of free disk space drops below m gigabytes.\nThe real-time analysis includes a language for writing formulas that describe these\nsituations.\nThis analysis may also detect situations that are not so critical as to require\nimmediate attention, but if left unattended could create a more signiﬁcant prob-\nlem. Such situations should generate tickets rather than alerts. See Section 14.1.7\nfor problem classiﬁcations. Alerts should be reserved for problems that do require\nimmediate attention. When an alert is triggered, it prompts the alerting and\nescalation manager, described later in this chapter, to take action.\nShort-term analysis examines data that was collected in the last day, week, or\nmonth. Generally dashboards ﬁt into this category. They are updated infrequently,\noften every few minutes or on demand when someone calls up the speciﬁc web\npage. Short-term analysis usually queries the on-disk copy of the stored metrics.\nNear-term analysis is also used to generate tickets for problems that are not so\nurgent as to require immediate attention. See Section 14.1.7.\nDashboard systems generally include a template language that generates\nHTML pages and a language for describing data graphs. The data graph descrip-\ntions are encoded in URLs so they may be included as embedded images in the\nHTML pages. For example, one URL might specify a graph that compares the ratio\nof two metrics for the last month for a particular service. It may specify a histogram\n\n\n354\nChapter 17\nMonitoring Architecture and Practice\nof latency for the 10 slowest web servers, after calculating latency for hundreds of\nweb servers.\nLong-term analysis generally examines data collected over large spans of time,\noften the entire history of a metric, to produce trend data. In many cases, this\ninvolves generating and storing summaries of data (averages, aggregates, and so\non) so that navigating the data can be done quickly, although at low resolution.\nBecause this type of analysis requires a large amount of processing, the results are\nusually stored permanently rather than regenerated as needed. Some systems also\nhandle situations where old data is stored on different media—for example, tape.\nAnomaly detection is the determination that a speciﬁc measurement is not\nwithin expectations. For example, one might examine all web servers of the same\ntype and detect if one is generating metrics that are signiﬁcantly different from the\nothers. This could imply that the one server is having difﬁculties that others are\nnot. Anomaly detection ﬁnds problems that you didn’t think to monitor for.\nAnomaly detection can also be predictive. Mathematical models can be cre-\nated that use last year’s data to predict what should be happening this year. One\ncan then detect when this year’s data deviates signiﬁcantly from the prediction. For\nexample, if you can predict how many QPS are expected from each country, iden-\ntifying a deviation of more than 10 percent from the prediction might be a good\nway to detect regional outages or just that an entire South American country stops\nto watch a particular sporting event.\nDoing anomaly detection in real time and across many systems can be com-\nputationally difﬁcult but systems for doing this are becoming more commonplace.\n17.4 Alerting and Escalation Manager\nThe alerting and escalation component manages the process of communicating to\noncall and other people when exceptional situations are detected. If the person can-\nnot be reached in a certain amount of time, this system attempts to contact others.\nSection 14.1.7 discusses alerting strategy and various communication technologies.\nThe ﬁrst job of the alerting component is to get the attention of the person\noncall, or his or her substitute. The next job is to communicate speciﬁc information.\nThe former is usually done by pager or text message. Since these systems permit\nonly short messages to be sent, there is usually a second method, such as email, to\ncommunicate the complete message.\nThe message should communicate the following information:\n• Failure Condition: A description of what is wrong in technical terms but in\nplain English. For example, “QPS too high on service XYZ” is clear. “Error 42”\nis not.\n",
      "page_number": 377
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 385-392)",
      "start_page": 385,
      "end_page": 392,
      "detection_method": "topic_boundary",
      "content": "17.4\nAlerting and Escalation Manager\n355\n• Business Impact: The size and scope of the issue—for example, how many\nmachines or users this affects, and whether service is reduced or completely\nunavailable.\n• Escalation Chain: The escalation chain is who to contact, and who to contact\nif that person does not respond. Generally, one or two chains are deﬁned for\neach service or group of services.\n• Suggested Resolution: Concise instructions of what to do to resolve this issue.\nThis is best done with a link to the playbook entry related to this alert, as\ndescribed in Section 14.2.5.\nThe last two items may be difﬁcult to write at the time the alert rule is created. The\nspeciﬁc business impact may not be known, but at least you’ll know which service\nis affected, so that information can be used as a placeholder. When the alert rule is\ntriggered, the speciﬁcs will become clear. Take time to record your thoughts so as\nto not lose this critical information.\nUpdate the impact and resolution as part of the postmortem exercise. Bring all\nthe stakeholders together. Ask the affected stakeholders to explain how their busi-\nness was impacted in their own business terms. Ask the operational stakeholders\nto evaluate the steps taken, including what went well and what could have been\nimproved. Compare this information with what is in the playbook and update it\nas necessary.\n17.4.1 Alerting, Escalation, and Acknowledgments\nThe alert system is responsible for delivering the alert to to the right person and\nescalating to others if they do not respond. As described in Section 14.1.5, this\ninformation is encoded in the oncall calendar.\nIn most cases, the workﬂow involves communicating to the primary oncall\nperson or people. They acknowledge the alert by replying to the text message with\nthe word “ACK” or “YES,” clicking on a link, or other means. If there is no acknowl-\nedgment after a certain amount of time, the next person on the escalation list is\ntried.\nHaving the ability to negatively acknowledge (“NAK”) the alert saves time\nduring escalations. A NAK immediately escalates to the next person on the list. For\nexample, if the oncall person receives the alert but is unable to attend to the issue\nbecause his or her Internet connection has died, the individual could simply do\nnothing and in a few minutes the escalation will happen automatically. However,\nif the escalation schedule involves paging the oncall person every 5 minutes and\nnot escalating until three attempts have been made, this means delaying action\nfor 15 minutes. In this case, the person can NAK and the escalation will happen\nimmediately.\n\n\n356\nChapter 17\nMonitoring Architecture and Practice\nInevitably, there are alert ﬂoods or “pager storms”—situations where dozens\nor hundreds of alerts are sent at the same time. This is usually due to one network\noutage that causes many alert rules to trigger. In most cases, there is a mechanism\nto suppress dependent alerts automatically, but ﬂoods may still happen despite the\norganization’s best efforts. For this reason, an alert system should have the ability\nto acknowledge all alerts at the same time. For example, by replying to the text\nmessage with the word “ALL” or “STFU,” all pending alerts for that particular\nperson are acknowledged as well as any alerts received in the next 5 minutes. The\nactual alerts can be seen at the alerting dashboard.\nSome alert managers have a two-stage acknowledgment. First the oncall per-\nson must acknowledge receiving the alert. This establishes that the person is\nworking on the issue. This disables alerts for that particular issue while person-\nnel are working on it. When the issue is resolved, the oncall person must “resolve”\nthe alert to indicate that the issue is ﬁxed. The beneﬁt of this system is that it makes\nit easier to generate metrics about how long it took to resolve the issue. But what if\nthe person forgets to mark the issue resolved? The system would need to send out\nreminder alerts periodically, which defeats the purpose of having two stages.\nFor this reason, we feel the two-stage acknowledgment provides little actual\nvalue. If there is a desire to record how long it takes to resolve an issue, have the\nsystem detect when the alert is no longer triggering. It will be more accurate and\nless annoying than requiring the operator to manually indicate that the issue is\nresolved.\n17.4.2 Silence versus Inhibit\nOperationally, there is a need to be able to silence an alert at will. For example,\nduring scheduled maintenance the person doing the maintenance does not need to\nreceive alerts that a system is down. Systems that depend on that system should not\ngenerate alerts because, in theory, their operations teams have been made aware of\nthe scheduled maintenance.\nThe mechanism for handling this is called a silence or sometimes a mainte-\nnance. A silence is speciﬁed as a start time, an end time, and a speciﬁcation of what\nto silence. When specifying what to silence, it can be useful to accept wildcards or\nregular expressions.\nBy implementing silences in the alert and escalation system, the alerts still\ntrigger but no action is taken. This is an important distinction. The alert is still trig-\ngering; we’re just not receiving notiﬁcations. The problem or outage is still really\nhappening and any dashboards or displays will reﬂect that fact.\nAlternatively, one can implement silences in the real-time analysis system.\nThis approach is called an inhibit. In this case the alert does not trigger and as\n\n\n17.4\nAlerting and Escalation Manager\n357\na result no alerts are sent. An inhibit can be implemented by a mechanism where\nan alert rule speciﬁes it should not be evaluated (calculated) if one or more pre-\nrequisite alert rules are currently triggering. Alternatively, the formula language\ncould include a Boolean function that returns true or false depending on whether\nthe alert is triggering. This function would be used to short-circuit the evaluation\nof the rule.\nFor example, an alert rule to warn of a high error rate might be inhibited if the\ndatabase in use is ofﬂine. There is no sense in getting alerted for something you\ncan’t do anything about. Monitoring the database is handled by another rule, or\nby another team. The alert rule might look like:\nIF http-500-rate > 1% THEN\nALERT(error-rate-too-high)\nUNLESS ACTIVE_ALERT(database-offline)\n.\nSilence Creation UI Advice\nWe learned the hard way that people are bad at doing math related to dates,\ntimes, and time zones, especially when stress is high and alerts are blaring.\nIt is also easy to make mistakes when wildcards or regular expressions can\nbe used to specify what to silence. Therefore we recommend the following UI\nfeatures:\n• The default start time should be “now.”\n• It should be possible to enter the end time as a duration in minutes, hours,\nor days, as well as a speciﬁc time and date in any time zone.\n• So that the user may check his or her work, the UI should display what\nwill be silenced and require conﬁrmation before it is activated.\nInhibits can cause confusion if an outage is happening and, due to inhibits, the\nmonitoring system says that everything is ﬁne. Teams that depend on the service\nwill be confused when their dashboards show it being up, yet their service is mal-\nfunctioning due to the outage. Therefore we recommend using inhibits sparingly\nand carefully.\nThe difference between a silence and an inhibit is very subtle. You silence an\nalert when you know it would be erroneous to page someone based on some condi-\ntion such as a planned outage or upgrade. You inhibit alerts to conditionally cause\none alert to not ﬁre when another condition is active.\n\n\n358\nChapter 17\nMonitoring Architecture and Practice\n17.5 Visualization\nVisualization is the creation of a visual representation of one or more metrics.\nIt is more than just creating the pretty graphs that will impress management.\nVisualization helps ﬁnd meaning in large amounts of data.\nA well-designed monitoring system does not collect data twice, once for real-\ntime alerting and yet again for visualization. If you collect it for visualization, be\nable to alert on it. If you alert on it, store it so that you can visualize it.\nSimple graphs can display raw data, summarized data, or a comparison of two\nmore metrics. Visualization also involves synthesizing new metrics from others.\nFor example, a rate can be calculated as the function of a counter and time.\nRaw data graphed over time, either as a single point or as an aggregate of\nsimilar metrics, is useful for metrics that change slowly. For example, if the memory\nuse of a server over time, which usually remains nearly constant, changes after a\nparticular software release, it is something to investigate. If the usage went up and\ndevelopers did not expect it to, there may be a memory leak. If usage went down\nand developers did not expect it to, there may be a bug.\nCounters are best viewed as rates. A counter, visualized in its raw form, looks\nlike a line going up and to the right. If the rate over time is calculated, we will see\neither acceleration, deceleration, or an equilibrium.\nSome visualizations are less useful and, in fact, can be misleading. A pie chart\nof disk space used and unused is a common dashboard ﬁxture that we ﬁnd rather\npretty but pointless. Knowing that a disk has 30 percent space remaining isn’t use-\nful without knowing the size of the disk. Seeing two such pie charts, side by side,\nof two different-sized disks, is rarely a useful comparison.\nIn contrast, calculating the rate at which the disk is ﬁlling and displaying the\nderivative of that rate becomes an actionable metric. It helps predict when the\ndisk space will be exhausted. Graph this derivative against the derivative of other\nresource consumption rates, such as the number of new accounts created, and you\ncan see how resources are consumed in proportion to each other.\nThe only thing we dislike more than pie charts are averages, or the mathemat-\nical mean. Averages can be misleading in ways that encourage you to make bad\ndecisions. If half of your customers like hot tea and the other half like cold tea, none\nof them will be happy if you serve them all lukewarm tea even though you have\nserved the same average temperature. A network link that is overloaded during\nthe day and barely used at night will be, on average, half used. It would be a bad\nidea to make a decision based on the average utilization. Averages lie. If billionaire\nBill Gates walks into a homeless shelter, the average person in the building is a\nmultimillionaire but it doesn’t take a genius to realize that the homeless problem\nhasn’t been eliminated.\nThat said, averages aren’t inherently misleading and have a place in your\nstatistical toolbox. Like other statistical functions they need to be used wisely.\n\n\n17.5\nVisualization\n359\n17.5.1 Percentiles\nPercentiles and medians are useful when analyzing utilization or quality. These\nterms are often misunderstood. Here’s a simple way to understand them.\nImagine a school has 400 students. Stand those students up along a 100-meter\nline in order of lowest GPA to highest. Stretch out the line evenly so that the student\nwith the lowest GPA is at 0 and the one with the highest GPA is at 100. The student\nstanding at the 90-meter mark is the 90th percentile. This means that 90 percent of\nthe student body did worse than this person. The student at the 50-meter mark,\nexactly in the middle, is known as the median.\nIf there are more students, if there are fewer students, or if the grading stan-\ndard changed, there will always be a student at (or closest to) the 90-meter mark.\nThe GPA of the 90th percentile student could be a C- or an A+. It all depends on the\nmakeup of the entire student body’s GPAs. The entire school could have excellent\ngrades but someone is still going to be at the 90th percentile. Trends can be ana-\nlyzed by, for example, graphing the GPA of the 50th and 90th percentile students\neach year.\nIt is common for colocation facilities to charge based on the 90th percentile\nof bandwidth used. This means they take measurements every 5 minutes of\nhow much bandwidth is being used at that moment. All these measurements\nare sorted, but duplicates are not removed. If you wrote all these measure-\nments, distributed evenly, along a 100-meter football ﬁeld, the measurement\nat the 90-meter mark would be the bandwidth rate you will be billed for.\nThis is a more fair billing method than charging for the maximum or average\nrate.\nIt is very smart of your colocation facility to charge you this way. If it charged\nbased on the average bandwidth used, the company would take the total amount of\nbytes transmitted in the day, divide by the number of seconds in a day, and use that\nrate. That rate would be misleadingly low for customers that consume very little\nbandwidth at night and a lot during the day, what is known as a diurnal usage\npattern. In contrast, the 90th percentile represents how heavily you use bandwidth\nwithout penalizing you for huge bursts that happen from time to time. In fact,\nit forgives you for your 30 most extreme 5-minute periods of utilization, nearly\n2.5 hours each day. More importantly for the colocation facility, it best represents\nthe cost of the bandwidth used.\nPercentiles are also commonly used when discussing latency or delay per unit.\nFor example, a site that is trying to improve its page-load time might set a goal of\nhaving the 80th percentile page load time be 300 ms or less. A particular API call\nmight have a lot of variation in how long it takes to execute due to caching, type of\nrequest, and so on. One might set a goal of having its 99th percentile be less than\nsome amount, such as 100 ms.\n\n\n360\nChapter 17\nMonitoring Architecture and Practice\n17.5.2 Stack Ranking\nStack ranking makes it easy to compare data by sorting the elements by value.\nFigure 17.2 is an example of stack ranking. If the cities had been sorted alphabeti-\ncally, it would not be clear how, for example, Atlanta compares to New York. With\nthe stack rank we can clearly see the difference.\nStack ranking is best used when data is apples-to-apples comparable. One way\nto achieve this is to normalize the data to be per unit (e.g., per capita, per machine,\nper service, per thousand queries). Figure 17.2 would mean something very dif-\nferent if the y-axis was thousands of tacos eaten in total by all people in that city\nversus being normalized to be a per-person metric. The latter is a more comparable\nnumber.\nIf you do not have a base-line, a stack rank can help establish one. You might\nnot know if “55” is a lot or a little, a bad or good amount, but you know only\ntwo cities were bigger. Managers who are unskilled at determining whether an\nemployee is effective can instead rely on peer stack ranking rather than making\nthe decision themselves.\nFigure 17.2: Cities stack ranked by number of tacos consumed\n\n\n17.5\nVisualization\n361\n17.5.3 Histograms\nHistograms can reveal a lot about datasets and are particularly useful when ana-\nlyzing latency. A histogram is a graph of the count of values in a dataset, possibly\nafter rounding. For example, if we take a set of data and round all the values to the\nnearest multiple of 5, we would graph the number of 0s, 5s, 10s, 15s, and so on. We\nmight simply round to the nearest integer. These ranges are often called buckets.\nEach data point is put in its most appropriate bucket and the graph visualizes how\nmany items are in each bucket.\nFigure 17.3a shows the page load measurements collected on a ﬁctional web\nsite. Graphing them as a line plot does not reveal much except that there is a lot of\nvariation. The average is 27, which is also not very actionable.\nHowever, if we take each measurement, round it to the nearest multiple of\n5, and graph the number of 0s, 5s, 10s, and so on, we end up with the graph in\nFigure 17.3b. This reveals that the majority of the data points are in the 10, 15,\n35, and 40 buckets. The graph has two humps, like a camel. This pattern is called\nbimodal. Knowing that the data has this shape gives us a basis for investigation.\nWe can separate the data points out into the two humps and look for similarities.\nWe might ﬁnd that most of the site’s pages load very quickly, but most of the data\npoints in the ﬁrst hump are from a particular page. Investigation shows that every\ntime this page is generated, a large amount of data must be sorted. By pre-sorting\nthat data, we can reduce the load time of that page. The other hump might be\nfrom a web page that requires a database lookup that is larger than the cache and\ntherefore performs badly. We can adjust the cache so as to improve performance of\nthat page.\nThere are many other ways to visualize data. More sophisticated visualiza-\ntions can draw out different trends or consolidate more information into a picture.\nColor can be used to add an additional dimension. There are many ﬁne books on\nFigure 17.3: Page load time recorded from a ﬁctional web site\n\n\n362\nChapter 17\nMonitoring Architecture and Practice\nthis subject. We highly recommend The Visual Display of Quantitative Information by\nTufte (1986).\n17.6 Storage\nThe storage system holds the metrics collected and makes them accessible by the\nother modules.\nStorage is one of the most architecturally demanding parts of the monitoring\nsystem. New metrics arrive in a constant ﬂood, at high speed. Alerting requires\nfast, real-time, read access for recent data. At the same time, other analysis requires\niterations over large ranges, which makes caching difﬁcult. Typical SQL databases\nare bad at all of these things.\nMedium-sized systems often collect 25–200 metrics for each server. There are\nmultiple servers on each machine. A medium-sized system may need to store 400\nnew metrics each second. Larger systems typically store thousands of metrics per\nsecond. Globally distributed systems may store tens or hundreds of thousands of\nmetrics every second.\nAs a result, many storage systems handle either real-time data or long-term\nstorage but not both, or can’t do both at large scale. Recently a number of time-\nseries databases such as OpenTSDB have sprung up that are speciﬁcally designed\nto be good at both real-time and long-term storage. They achieve this by keeping\nrecent data in RAM, often up to an hour’s worth, as well as by using a highly tuned\nstorage format.\nOn-disk storage of time-series data is usually done one of two ways. One\nmethod achieves fast high-speed random access by using ﬁxed-size records. For\nexample, each metric might be stored in a 20-byte record. The system can efﬁciently\nﬁnd a metric at a particular time by using a modiﬁed binary search. This approach\nis most effective when real-time visualization is required. Another method is to\ncompress the data, taking advantage of the fact that deltas can be stored in very few\nbits. The result is often variable-length records, which means the time-series data\nmust be read from the start to ﬁnd the metric at a particular time. These systems\npermit much greater storage density. Some systems achieve a balance by storing\nﬁxed-size records on a ﬁle system with built-in compression.\n17.7 Configuration\nThe six monitoring components discussed so far all need conﬁguration information\nto direct their work. The sensing system needs to know which data to measure and\nhow often. The collection system needs to know what to collect and where to send\nit. The analysis system has a base of formulas to process. The alerting system needs\n",
      "page_number": 385
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 393-403)",
      "start_page": 393,
      "end_page": 403,
      "detection_method": "topic_boundary",
      "content": "17.8\nSummary\n363\nto know who to alert, how, and who to escalate to. The visualization system needs\nto know which graphs to generate and how to do so. The storage system needs to\nknow how to store and access the data.\nThese conﬁgurations should be treated like any other software source code:\nkept under revision control, tested using both unit tests and system tests, and so\non. Revision control tracks changes of a ﬁle over time, enabling one to see what\na ﬁle looked like at any point in its history. A unit test framework would take as\ninput time-series data for one or more metrics and output whether the alert would\ntrigger. This permits one to validate alert formulas.\nIn distributed monitoring systems, each component may be separated out and\nperhaps replicated or sharded. Each piece needs a way to access the conﬁguration.\nA system like ZooKeeper, discussed in Section 11.7, can be used to distribute a\npointer to where the full conﬁguration can be found, which is often a source code\nor package repository.\nSome monitoring systems are multitenant. This is where a monitoring sys-\ntem permits many teams to independently control metric collection, alert rules,\nand so on. By centralizing the service but decentralizing the ability to use it, we\nempower service owners, developers, and others to do their own monitoring and\nbeneﬁt from the ability to automatically collect and analyze data. Other monitor-\ning systems achieve the same goal by making it easy for individuals to install their\nown instance of the monitoring system, or just their own sensing and collection\ncomponents while centralizing storage and other components.\n17.8 Summary\nMonitoring systems are complex, with many components working together.\nThe sensing and measurement component takes measurements. Whitebox\nmonitoring monitors the systems internals. Blackbox monitoring collects data from\nthe perspective of a user. Gauges measure an amount that varies. Counters are\nnon-decreasing indicators of how many times something has happened.\nThe collection system gathers the metrics. Metrics may be pushed (sent) to the\ncollection system or the collection system may pull (query) systems to gather the\nmetrics.\nThe storage system stores the metrics. Usually custom databases are used\nto handle the large volume of incoming data and take advantage of the unique\nqualities of time-series data.\nThe analysis system extracts meaning from the data. There may be many\ndifferent analysis systems, each providing services such as anomaly detection, fore-\ncasting, or data mining. Some analysis occurs in real time, happening as the data is\ngathered. Short-term analysis focuses on recent data or provides the random access\n\n\n364\nChapter 17\nMonitoring Architecture and Practice\nneeded by speciﬁc applications such as dashboards. Long-term analysis examines\nlarge spans of data to detect trends over many years. It is often done in batch mode,\nstoring intermediate results for later use.\nAlerting and escalation systems reach out to ﬁnd people when manual inter-\nvention is needed, ﬁnding a substitute when a person doesn’t respond within a\ncertain amount of time.\nVisualization systems provide graphs and dashboards. They can combine\nand transform data and do operations such as calculating percentiles, building\nhistograms, and determining stack ranks.\nAll of this is tied together by a conﬁguration manager that directs all of the\nother components in their work. Changes to conﬁgurations can be distributed in\nmany ways, often by distributing conﬁguration ﬁles or more dynamic systems such\nas ZooKeeper.\nWhen monitoring systems are multitenant, we empower individual service\nteams to control their own monitoring conﬁgurations. They beneﬁt from central-\nized components, freeing them from having to worry about capacity planning and\nother operational duties.\nWhen all the components work together, we have a monitoring system that is\nscalable, reliable, and functional.\nExercises\n1. What are the components of the monitoring system?\n2. Pick three components of the monitoring system and describe them in detail.\n3. Do all monitoring systems have all the components described in this chapter?\nGive examples of why components may be optional.\n4. What is a pager storm, and what are the ways to deal with one?\n5. Research the JSON format for representing data. Design a JSON format for the\ncollection of metrics.\n6. Describe the monitoring system in use in your organization or one you’ve had\nexperience with in the past. How is it used? What does it monitor? Which\nproblems does it solve?\n7. Create one or more methods of calculating a rate for a counter metric. The\nmethod should work even if there is a counter reset. Can your method also\ncalculate a margin of error?\n8. Design a better monitoring system for your current environment.\n9. Why are averages discouraged? Present a metric misleadingly as an average,\nand non-misleadingly some other way.\n\n\nChapter 18\nCapacity Planning\nPlans are nothing,\nplanning is everything.\n—Philip Kotler\nCapacity planning means ensuring that there will be enough resources when they\nare needed. Optimally this is done such that the system is neither under capac-\nity nor over capacity. Resources include CPUs, memory, storage, server instances,\nnetwork bandwidth, switch ports, console connections, power, cooling, datacenter\nspace, and any other infrastructure components that are required to run a service.\nThere are two major objectives of capacity planning. First, we want to prevent\nservice interruptions due to lack of capacity. Second, we want to preserve capital\ninvestment by adding only the capacity required at any given time. Good capac-\nity planning provides a demonstrable return on investment (ROI) by showing the\nneed for resources and capping resource usage at a level that ensures good service.\nCapacity planning should be a data-driven process, using data collected about\nthe running system to forecast trends. It also should be informed by future business\nplans for growth. This chapter explains which data you need to collect and how to\nuse it to forecast your future capacity requirements.\nCapacity planning in large, fast-growing services becomes highly complex\nand relies on sophisticated mathematical models. Organizations often hire a full-\ntime statistician to develop and maintain these models. A statistician with a\ntechnical background may be difﬁcult to ﬁnd, but such a person is worth his or\nher weight in gold. This chapter introduces some mathematical models that were\ndeveloped for trading on the ﬁnancial markets, and shows how you can apply\nthem to capacity planning in a rapidly changing environment.\nThis chapter does not look at the enterprise-style capacity planning required\nto meet the day-to-day needs of the people who work for the service provider.\nInstead, this chapter focuses on capacity planning for the service itself.\n365\n\n\n366\nChapter 18\nCapacity Planning\n.\nTerms to Know\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nActive Users: The number of users who have accessed the service in the\nspeciﬁed timeframe.\nMAU: Monthly active users. The number of users who have accessed the\nservice in the last month.\nEngagement: How many times on average an active user performs a par-\nticular transaction.\nPrimary Resource: The one system-level resource that is the main limiting\nfactor for the service.\nCapacity Limit: The point at which performance starts to degrade rapidly\nor become unpredictable.\nCore Driver: A factor that strongly drives demand for a primary resource.\nTime Series: A sequence of data points measured at equally spaced time\nintervals. For example, data from monitoring systems.\n18.1 Standard Capacity Planning\nCapacity planning needs to provide answers to two questions: What are you going\nto need to buy in the coming year? and When are you going to need to buy it? To\nanswer those questions, you need to know the following information:\n• Current Usage: Which components can inﬂuence service capacity? How much\nof each do you use at the moment?\n• Normal Growth: What is the expected growth rate of the service, without\nthe inﬂuence of any speciﬁc business or marketing events? Sometimes this\nis called organic growth.\n• Planned Growth: Which business or marketing events are planned, when will\nthey occur, and what is the anticipated growth due to each of these events?\n• Headroom: Which kind of short-term usage spikes does your service\nencounter? Are there any particular events in the coming year, such as the\nOlympics or an election, that are expected to cause a usage spike? How much\nspare capacity do you need to handle these spikes gracefully? Headroom is\nusually speciﬁed as a percentage of current capacity.\n• Timetable: For each component, what is the lead time from ordering to deliv-\nery, and from delivery until it is in service? Are there speciﬁc constraints for\nbringing new capacity into service, such as change windows?\n\n\n18.1\nStandard Capacity Planning\n367\n.\nMath Terms\nCorrelation Coefficient: Describes how strongly measurements for differ-\nent data sources resemble each other.\nMoving Average: A series of averages, each of which is taken across a short\ntime interval (window), rather than across the whole data set.\nRegression Analysis: A statistical method for analyzing relationships\nbetween different data sources to determine how well they correlate,\nand to predict changes in one based on changes in another.\nEMA: Exponential moving average. It applies a weight to each data point\nin the window, with the weight decreasing exponentially for older data\npoints.\nMACD: Moving average convergence/divergence. An indicator used to\nspot changes in strength, direction, and momentum of a metric. It mea-\nsures the difference between an EMA with a short window and an EMA\nwith a long window.\nZero Line Crossover: A crossing of the MACD line through zero happens\nwhen there is no difference between the short and long EMAs. A move\nfrom positive to negative shows a downward trend in the data, and a\nmove from negative to positive shows an upward trend.\nMACD Signal Line: An EMA of the MACD measurement.\nSignal Line Crossover: The MACD line crossing over the signal line indi-\ncates that the trend in the data is about to accelerate in the direction of\nthe crossover. It is an indicator of momentum.\nFrom that information, you can calculate the amount of capacity you expect to need\nfor each resource by the end of the following year with a simple formula:\nFuture Resources = Current Usage × (1 + Normal Growth\n+ Planned Growth) + Headroom\nYou can then calculate for each resource the additional capacity that you need\nto purchase:\nAdditional Resources = Future Resources −Current Resources\nPerform this calculation for each resource, whether or not you think you will\nneed more capacity. It is okay to reach the conclusion that you don’t need any\nmore network bandwidth in the coming year. It is not okay to be taken by sur-\nprise and run out of network bandwidth because you didn’t consider it in your\n\n\n368\nChapter 18\nCapacity Planning\ncapacity planning. For shared resources, the data from many teams will need to be\ncombined to determine whether more capacity is needed.\n18.1.1 Current Usage\nBefore you can consider buying additional equipment, you need to understand\nwhat you currently have available and how much of it you are using. Before you\ncan assess what you have, you need a complete list of all the things that are required\nto provide the service. If you forget something, it won’t be included in your capac-\nity planning, and you may run out of that one thing later, and as a result be unable\nto grow the service as quickly as you need.\nWhat to Track\nThe two most obvious things that the provider of an Internet-based service needs\nare some machines to provide the service and a connection to the Internet. Some\nmachines may be generic machines that are later customized to perform given\ntasks, whereas others may be specialized appliances. Going deeper into these\nitems, machines have CPUs, caches, RAM, storage, and network. Connecting to\nthe Internet requires a local network, routers, switches, and a connection to at least\none ISP. Going deeper still, network cards, routers, switches, cables, and storage\ndevices all have bandwidth limitations. Some appliances may have higher-end\nnetwork cards that need special cabling and interfaces on the network gear. All net-\nworked devices need IP addresses. These are all resources that need to be tracked.\nTaking one step back, all devices run some sort of operating system, and some\nrun additional software. The operating systems and software may require licenses\nand maintenance contracts. Data and conﬁguration information on the devices\nmay need backing up to yet more systems. Stepping even farther back, machines\nneed to be installed in a datacenter that meets their power and environment needs.\nThe number and type of racks in the datacenter, the power and cooling capac-\nity, and the available ﬂoor space all need to be tracked. Datacenters may provide\nadditional per-machine services, such as console service. For companies that have\nmultiple datacenters and points of presence, there may be links between those sites\nthat also have capacity limits. These are all additional resources to track.\nOutside vendors may provide some services. The contracts covering those ser-\nvices specify cost or capacity limits. To make sure that you have covered avery\npossible aspect, talk to people in every department, and ﬁnd out what they do and\nhow it relates to the service. For everything that relates to the services, you need to\nunderstand what the limits are, how you can track them, and how you can measure\nhow much of the available capacity is used.\nHow Much Do You Have\nThere is no substitute for a good up-to-date inventory database for keeping track\nof your assets. The inventory database should be kept up-to-date by making it a\n\n\n18.1\nStandard Capacity Planning\n369\ncore component in the ordering, provisioning, and decommissioning processes. An\nup-to-date inventory system gives you the data you need to ﬁnd out how much of\neach resource you have. It should also be used to track the software license and\nmaintenance contract inventory, and the contracted amount of resources that are\navailable from third parties.\nUsing a limited number of standard machine conﬁgurations and having a set\nof standard appliances, storage systems, routers, and switches makes it easier to\nmap the number of devices to the lower-level resources, such as CPU and RAM,\nthat they provide.\nHow Much Are You Using Now\nIdentify the limiting resources for each service. Your monitoring system is likely\nalready collecting resource use data for CPU, RAM, storage, and bandwidth. Typi-\ncally it collects this data at a higher frequency than required for capacity planning.\nA summarization or statistical sample may be sufﬁcient for planning purposes and\nwill generally simplify calculations. Combining this data with the data from the\ninventory system will show how much spare capacity you currently have.\nTracking everything in the inventory database and using a limited set of stan-\ndard hardware conﬁgurations also makes it easy to specify how much space,\npower, cooling, and other datacenter resources are used per device. With all of\nthat data entered into the inventory system, you can automatically generate the\ndatacenter utilization rate.\n18.1.2 Normal Growth\nThe monitoring system directly provides data on current usage and current capac-\nity. It can also supply the normal growth rate for the preceding years. Look for any\nnoticeable step changes in usage, and see if these correspond to a particular event,\nsuch as the roll-out of a new product or a special marketing drive. If the offset due\nto that event persists for the rest of the year, calculate the change and subtract it\nfrom subsequent data to avoid including this event-driven change in the normal\ngrowth calculation. Plot the data from as many years as possible on a graph, to\ndetermine if the normal growth rate is linear or follows some other trend.\n18.1.3 Planned Growth\nThe second step is estimating additional growth due to marketing and business\nevents, such as new product launches or new features. For example, the marketing\ndepartment may be planning a major campaign in May that it predicts will increase\nthe customer base by 20 to 25 percent. Or perhaps a new product is scheduled to\nlaunch in August that relies on three existing services and is expected to increase\nthe load on each of those by 10 percent at launch, increasing to 30 percent by the\n\n\n370\nChapter 18\nCapacity Planning\nend of the year. Use the data from any changes detected in the ﬁrst step to validate\nthe assumptions about expected growth.\n18.1.4 Headroom\nHeadroom is the amount of excess capacity that is considered routine. Any service\nwill have usage spikes or edge conditions that require extended resource usage\noccasionally. To prevent these edge conditions from triggering outages, spare\nresources must be routinely available. How much headroom is needed for any\ngiven service is a business decision. Since excess capacity is largely unused capac-\nity, by its very nature it represents potentially wasted investment. Thus a ﬁnan-\ncially responsible company wants to balance the potential for service interruption\nwith the desire to conserve ﬁnancial resources.\nYour monitoring data should be picking up these resource spikes and provid-\ning hard statistical data on when, where, and how often they occur. Data on outages\nand postmortem reports are also key in determining reasonable headroom.\nAnother component in determining how much headroom is needed is the\namount of time it takes to have additional resources deployed into production from\nthe moment that someone realizes that additional resources are required. If it takes\nthree months to make new resources available, then you need to have more head-\nroom available than if it takes two weeks or one month. At a minimum, you need\nsufﬁcient headroom to allow for the expected growth during that time period.\n18.1.5 Resiliency\nReliable services also need additional capacity to meet their SLAs. The additional\ncapacity allows for some components to fail, without the end users experiencing an\noutage or service degradation. As discussed in Chapter 6, the additional capacity\nneeds to be in a different failure domain; otherwise, a single outage could take\ndown both the primary machines and the spare capacity that should be available\nto take over the load.\nFailure domains also should be considered at a large scale, typically at the\ndatacenter level. For example, facility-wide maintenance work on the power sys-\ntems requires the entire building to be shut down. If an entire datacenter is ofﬂine,\nthe service must be able to smoothly run from the other datacenters with no capac-\nity problems. Spreading the service capacity across many failure domains reduces\nthe additional capacity required for handling the resiliency requirements, which\nis the most cost-effective way to provide this extra capacity. For example, if a\nservice runs in one datacenter, a second datacenter is required to provide the addi-\ntional capacity, about 50 percent. If a service runs in nine datacenters, a tenth\nis required to provide the additional capacity; this conﬁguration requires only\n10 percent additional capacity.\n\n\n18.2\nAdvanced Capacity Planning\n371\nAs discussed in Section 6.6.5, the gold standard is to provide enough capacity\nfor two datacenters to be down at the same time. This permits one to be down\nfor planned maintenance while the organization remains prepared for another\ndatacenter going down unexpectedly. Appendix B discusses the history of such\nresilient architectures.\n18.1.6 Timetable\nMost companies plan their budgets annually, with expenditures split into quarters.\nBased on your expected normal growth and planned growth bursts, you can map\nout when you need the resources to be available. Working backward from that date,\nyou need to ﬁgure out how long it takes from “go” until the resources are available.\nHow long does it take for purchase orders to be approved and sent to the\nvendor? How long does it take from receipt of a purchase order until the vendor has\ndelivered the goods? How long does it take from delivery until the resources are\navailable? Are there speciﬁc tests that need to be performed before the equipment\ncan be installed? Are there speciﬁc change windows that you need to aim for to\nturn on the extra capacity? Once the additional capacity is turned on, how long\ndoes it take to reconﬁgure the services to make use of it? Using this information,\nyou can provide an expenditures timetable.\nPhysical services generally have a longer lead time than virtual services. Part\nof the popularity of IaaS and PaaS offerings such as Amazon’s EC2 and Elas-\ntic Storage are that newly requested resources have virtually instant delivery\ntime.\nIt is always cost-effective to reduce resource delivery time because it means we\nare paying for less excess capacity to cover resource delivery time. This is a place\nwhere automation that prepares newly acquired resources for use has immediate\nvalue.\n18.2 Advanced Capacity Planning\nLarge, high-growth environments such as popular internet services require a dif-\nferent approach to capacity planning. Standard enterprise-style capacity planning\ntechniques are often insufﬁcient. The customer base may change rapidly in ways\nthat are hard to predict, requiring deeper and more frequent statistical analy-\nsis of the service monitoring data to detect signiﬁcant changes in usage trends\nmore quickly. This kind of capacity planning requires deeper technical knowledge.\nCapacity planners will need to be familiar with concepts such as QPS, active users,\nengagement, primary resources, capacity limit, and core drivers. The techniques\ndescribed in this section of the book were covered by Yan and Kejariwal (2013),\nwhose work inspired this section.\n\n\n372\nChapter 18\nCapacity Planning\n18.2.1 Identifying Your Primary Resources\nEach service has one primary resource, such as CPU utilization, memory footprint\nor bandwidth, storage footprint or bandwidth, or network bandwidth, that is the\ndominant resource consumed by the service. For example, a service that does a\nlot of computation is usually limited by the available CPU resources—that is, it is\nCPU-bound. Capacity planning focuses on the primary resource.\nServices also have secondary resource needs. For example, a service that is\nCPU-bound may, to a lesser extent, use memory, storage, and network bandwidth.\nThe secondary resources are not interesting from a capacity planning point of view,\nwith the current software version and hardware models, but one of them may\nbecome the primary resource later as the code or hardware changes. Consequently,\nthese secondary resources should also be monitored and the usage trends tracked.\nDetecting a change in which resource is the primary resource for a given service\nis done by tracking the constraint ratio (disk:CPU:memory:network) between the\nprimary and secondary resources. Whenever a new software version is released or\na new hardware model is introduced, this ratio should be recalculated.\nPrimary and secondary resources are low-level resource units that drive the\nneed for ancillary resources such as server instances, switch ports, load balancers,\npower, and other datacenter infrastructure.\nThe ﬁrst step in capacity planning is to identify your primary resources, as\nthese are the resources that you will focus on for capacity planning. You must also\ndeﬁne a relationship between your primary and ancillary usage. When your capac-\nity planning indicates that you need more of your primary resources, you need to\nbe able to determine how much of the ancillary resources are required as a result.\nAs the hardware that you use changes, these mappings need to be updated.\n18.2.2 Knowing Your Capacity Limits\nThe capacity limit of any resource is the point at which performance starts to\ndegrade rapidly or become unpredictable, as shown in Figure 18.1. Capacity limits\nshould be determined by load testing. Load testing is normally performed in an\nisolated lab environment. It can be performed by synthetically generating trafﬁc\nor by replaying production trafﬁc.\nFor each primary and secondary resource, you need to know the capacity\nlimit. To generate such data for each resource in isolation, your lab setup should\nbe equipped with lots of excess capacity for all but one low-level resource. That\nresource will then be the limiting factor for the service, and response times can be\ngraphed against the percentage utilization of that resource and separately against\nthe absolute amount of that resource that is available. For completeness, it is best\nto repeat this test several times, each time scaling up the whole environment\nuniformly. This approach will enable you to determine if the limit is more closely\nrelated to percentage utilization or to quantity of remaining resources.\n\n\n18.2\nAdvanced Capacity Planning\n373\nFigure 18.1: Response time starts to degrade beyond a certain percent\nutilization.\n18.2.3 Identifying Your Core Drivers\nCore drivers are factors that strongly drive demand for a primary resource. They\ntypically include values such as MAU, QPS, the size of the corpus, or other high-\nlevel metrics that represent well the factors that generate trafﬁc or load on the\nservice. These metrics also often have meaningful business implications, with links\nto sources of revenue, for example.\nA site may have millions of registered users, but it will typically have many\nfewer users who are active. For example, many people sign up for accounts and\nuse the service a few times, but never return. Counting these users in your planning\ncan be misleading. Many people register with social networking sites, but rarely\nuse their accounts. Some people who are registered with online shopping sites\nmay use the service only before birthdays and gift-buying holidays. A\nmore accurate representation of users may be how many were active in the last\n7 or 30 days. The number of users in the last 7 days is often called 7-day actives\n(7DA), while the term weekly active users (WAU) is used to indicate how many\nwere active in a speciﬁc calendar week. Likewise, 30-day actives (30DA) measures\nthe number of users in the last 30 days, with the term monthly active users (MAU)\nused if the measurement was bounded by a speciﬁc calendar month. These mea-\nsurements often reﬂect usage much more accurately than the number of registered\nusers.\n",
      "page_number": 393
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 404-412)",
      "start_page": 404,
      "end_page": 412,
      "detection_method": "topic_boundary",
      "content": "374\nChapter 18\nCapacity Planning\nFor metrics such as active users that have a time component, different val-\nues of that time component may be appropriate to use in capacity planning for\ndifferent services. For example, for some services monthly active users may be the\nappropriate core driver, whereas for another service minutely active users may be a\nbetter indicator to use in capacity planning. For highly transactional systems that\nare driven by active users, smaller time scales like minutely active users may be\nappropriate. For storage-bound services that are driven by users, the total number\nof registered users (or total user corpus number) may be more appropriate.\nThe capacity model depicts the relationship between the core driver and the\nprimary resource. For a given service, the capacity model expresses how changes\nin the core driver affect that service’s need for its core driver.\nOnce you have identiﬁed the core drivers, and have determined the effect that\neach one has on each of the primary and secondary resources, you can quantify\nthe effect that each will have on your requirements for ancillary resources, such as\nservers. You can also analyze whether your ancillary resources are well balanced.\nIf servers run out of CPU cycles long before they run out of RAM, then it may\nbe more cost-effective to order servers with less RAM or more or faster CPUs, for\nexample. Or it may be possible to rework the service by taking advantage of the\nextra RAM or making it less CPU-intensive. Similarly, if your switches run out of\nports long before backplane bandwidth, perhaps a different switch model would\nbe more appropriate.\n18.2.4 Measuring Engagement\nIf the number of active users is growing rapidly, it might seem as if this number is\nthe only core driver that needs to be taken into consideration. However, that works\non the assumption that this growth will continue. Another factor can, and should,\nbe tracked separately—user engagement. Engagement is the number of times that\nthe average active user accesses the particular service in a given period of time.\nIn other words, it is a measure of the popularity of a particular feature. For exam-\nple, a site might ﬁnd that when it ﬁrst introduces a video-sharing functionality, not\nmany people are interested, and only about 1 percent of the monthly active users\naccess that feature. However, as time goes on, this feature may become more pop-\nular, and six months later 25 percent of the monthly active users may be accessing\nthat feature.\nIf the number of active users stays static but engagement increases, then the\nload on your service will also increase. Taken in combination, these two numbers\nindicate how often a service is accessed.\nThe engagement graph for each service will be different, and an increase in\nengagement for one service will have a different effect on resource requirements\nthan a change in engagement for another service. For example, an increase in\n\n\n18.2\nAdvanced Capacity Planning\n375\nengagement for video uploads will have more of an impact on disk space and I/O\nbandwidth than an increase in engagement for the chat service will.\n18.2.5 Analyzing the Data\nOnce you have decided which metrics to collect and started gathering data, you\nneed to be able to process that data and generate useful inputs for your capacity\nplanning process. Standard capacity planning looks at year-to-year statistics and\nmakes projections based on those data. This approach is still necessary, but in a\nlarge-scale, rapidly changing environment, we need to augment it so that we are\nable to react more quickly to changes in demand.\nIn a large-scale environment, one of the goals is to be able to simply but accu-\nrately specify how many resources you need to have based on a measured core\ndriver. One way to meet this goal is to simplify the metrics down to blocks of\nusers based on the serving capacity of one rack or cluster of machines. With this\napproach, capacity planning is simpliﬁed into a more cookie-cutter approach. One\napplication might serve 100,000 active users per rack. Another application might\nserve 1000 active users per cluster, where each cluster is a standardized combi-\nnation of machines that act as an independent unit. Engineering can produce new\nratios as newer, faster hardware becomes available or new software designs are cre-\nated. Now capacity planning is simpliﬁed and resources can be managed based on\nblocks of active users. While this approach is less granular, it is sufﬁcient because\nit matches the deployment granularity. You can’t buy half a machine, so capacity\nplanning doesn’t need to be super precise.\nTo identify such a relationship between a core driver and resource consump-\ntion, you ﬁrst need to understand which core drivers inﬂuence which resources\nand how strongly. The way to do so is to correlate the resource usage metrics with\nthe core driver metrics.\nCorrelation\nCorrelation measures how closely data sources resemble each other. Visually, you\nmight see on a monitoring graph that an increase in CPU usage on a server matches\nup with a corresponding increase in network trafﬁc to the same server, which also\nmatches up with a spike in QPS. From these observations you might conclude that\nthese three measurements are related, although you cannot necessarily say that the\nchanges in one caused the changes in another.\nRegression analysis mathematically calculates how well time-series data\nsources match up. Regression analysis of your metrics can indicate how strongly\nchanges in a core driver affect the usage of a primary resource. It can also indicate\nhow strongly two core drivers are related.\n\n\n376\nChapter 18\nCapacity Planning\nTo perform a regression analysis on time-series data, you ﬁrst need to deﬁne\na time interval, such as 1 day or 4 weeks. The number of data samples in that time\nperiod is n. If your core driver metric is x and your primary resource metric is y,\nyou ﬁrst calculate the sum of the last n values for x, x2, y, y2, and x times y, giving\n∑x, ∑x2, ∑y, ∑y2, and ∑xy. Then calculate SSxy, SSxx, SSyy, and R as follows:\nSSxy =\n∑\nxy −(∑x)(∑y)\nn\nSSxx =\n∑\nx2 −(∑x)2\nn\nSSyy =\n∑\ny2 −(∑y)2\nn\nr =\nSSxy\n√SSxxSSyy\nRegression analysis results in a correlation coefﬁcient R, which is a number\nbetween −1 and 1. Squaring this number and then multiplying by 100 gives the\npercentage match between the two data sources. For example, for the MAU and\nnetwork utilization ﬁgures shown in Figure 18.2, this calculation gives a very high\ncorrelation, between 96 percent and 100 percent, as shown in Figure 18.3, where R2\nis graphed.\nWhen changes in a core driver correlate well with changes in usage of a pri-\nmary resource, you can derive an equation of the form y = a + bx, which describes\nthe relationship between the two, known as the regression line. This equation\nenables you to calculate your primary resource requirements based on core driver\nFigure 18.2: The number of users correlates well with network trafﬁc.\n\n\n18.2\nAdvanced Capacity Planning\n377\nFigure 18.3: The upper line shows a high correlation of over 96% between two data\nsources. The lower line shows low correlation—less than 60%.\nmeasurements. In other words, given a value for your core driver x, you can\ncalculate how much of your primary resource y you think you will need, with a\nconﬁdence of R2. To calculate a and b, ﬁrst calculate the moving average of the last\nn data points for x and y, giving ¯x and ¯y. Then:\nb = SSxx\nSSyy\na = ¯y −b¯x\nCorrelation between metrics changes over time, and should therefore be\ngraphed and tracked with a rolling correlation analysis, rather than assessed\nonce and assumed to be constant. Changes in the service can have a signiﬁcant\nimpact on correlation. Changes in the end-user demographic are usually slower,\nbut can also affect correlation by changing how the average customer uses the\nservice.\nFigure 18.4a shows a sharp drop in correlation corresponding to a change in\nresource usage patterns between one software release and the next. The change in\ncorrelation in the graph actually corresponds to a step-change in resource usage\npatterns from one release to the next. After the time interval chosen for the rolling\ncorrelation measurement elapses, correlation returns to normal.\n\n\n378\nChapter 18\nCapacity Planning\nFigure 18.4: Change in correlation between MAU and bandwidth\nFigure 18.4b shows b for the same time interval. Notice that after the upgrade\nb changes signiﬁcantly during the time period chosen for the correlation analysis\nand then becomes stable again but at a higher value. The large ﬂuctuations in b for\nthe length of the correlation window are due to signiﬁcant changes in the moving\naverages from day to day, as the moving average has both pre- and post-upgrade\ndata. When sufﬁcient time has passed so that only post-upgrade data is used in\nthe moving average, b becomes stable and the correlation coefﬁcient returns to its\nprevious high levels.\nThe value of b corresponds to the slope of the line, or the multiplier in the\nequation linking the core driver and the usage of the primary resource. When corre-\nlation returns to normal, b is at a higher level. This result indicates that the primary\nresource will be consumed more rapidly with this software release than with the\nprevious one. Any marked change in correlation should trigger a reevaluation of\nthe multiplier b and corresponding resource usage predictions.\nForecasting\nForecasting attempts to predict future needs based on current and past measure-\nments. The most basic forecasting technique is to graph the 90th percentile of the\nhistorical usage and then ﬁnd an equation that best ﬁts this data. You can then\nuse that equation to predict future usage. Calculating percentiles was discussed in\nSection 17.5.1.\nOf course, growth rates change, usage patterns change, and application\nresource needs change. We need to be able to detect these changes and alter our\nresource planning accordingly. To detect changes in a trend, we use the moving\naverage convergence/divergence (MACD) metric. MACD measures the difference\nbetween a long-period (e.g., 3 months) and a short-period (e.g., 1 month) moving\naverage. However, a standard moving average tends to mask recent changes in\nmetrics. Since forecasting aims to provide early detection for such changes, MACD\n\n\n18.2\nAdvanced Capacity Planning\n379\nuses an exponential moving average (EMA), which gives an average with a much\nheavier weighting for recent data and a very low weighting for old data. An EMA\nis calculated as follows, where n is the number of samples:\nk =\n2\nn + 1\nEMAx = Valuex × k + EMAx−1 × (1 −k)\nTo get started, the ﬁrst EMAx−1 value (actually EMAn) is just a straight average\nof the ﬁrst n data points.\nTo use MACD to give early warning of changes in behavior, you need to cal-\nculate and plot some additional data, called the MACD signal line, on the same\ngraph. The MACD signal is an EMA of the MACD. When the MACD line crosses\nthe MACD signal line, that is an indication that the trend is changing. When the\nMACD line crosses from below the signal line to above the signal line, it indi-\ncates an increase. For example, for an engagement metric, it would indicate an\nunexpected increase in engagement for a particular feature. When the MACD line\ncrosses from above the signal line to below it, it indicates a downward trend. For\nexample, in a memory usage graph, it might indicate that the most recent release is\nmore memory efﬁcient than the previous one. This may cause you to reassess the\nnumber of users a cluster can support.\nThe challenge in measuring and graphing MACD is to deﬁne the time scales\nto use for the long and short periods. If the periods are too short, the MACD will\nindicate changes in trends too frequently to be useful, as in Figure 18.5. The bar\nchart in the background of this ﬁgure is the 90th percentile data. The smoother\ngraph line is the MACD signal line, and the other line is the MACD.\nHowever, increasing the short period, in particular, will tend to delay trig-\ngering a change in trend event. In Figure 18.6, the downward trend is triggered\n2 days earlier with a 2-week short period (Figure 18.6a) than with the 4-week short\nperiod (Figure 18.6b), keeping the long period constant at 3 months. However, the\n2-week short period also has a little extra noise, with a downward trigger followed\n2 days later by an upward trigger, followed a week later by the ﬁnal downward\ntrigger.\nWhen choosing your long and short periods, you can validate the model by\nseeing if older data predicts more recent data. If it does, it is reasonable to conclude\nthat the model will be a good predictor in the near-term future. Start with existing\ndata, as far back as you have available. Try several different combinations of short\nand long periods, and see which combination best predicts trends observed in your\nhistorical datasets.\n\n\n380\nChapter 18\nCapacity Planning\nFigure 18.5: Noisy data from a 1-week short period and a 4-week long period\nFigure 18.6: Effect of changing only the short period\n18.2.6 Monitoring the Key Indicators\nThe correlation coefﬁcients between core drivers and resources should be graphed\nand monitored. If there is a signiﬁcant change in correlation, the relationship\nbetween the core driver and resource consumption should be reassessed, and any\nchanges fed back into the capacity planning process.\nSimilarly, the MACD and MACD signal for each core driver and resource\nshould be monitored. Since these metrics can be used for early detection in changes\n\n\n18.3\nResource Regression\n381\nin trends, they are key elements in capacity management. Alerts generated by the\nMACD line and MACD signal line crossing should go straight to the team respon-\nsible for capacity planning. If the core drivers and relationships to the primary\nresources are well deﬁned, theoretically it should be necessary to monitor only\nthe core drivers and the correlation coefﬁcients. In reality, it is prudent to monitor\neverything to minimize the chance of surprises.\n18.2.7 Delegating Capacity Planning\nCapacity planning is often done by the technical staff. However, with good metrics\nand a clear understanding of how core drivers affect your resource requirements,\nyou can decouple the capacity planning from the deployment. A program man-\nager can do the capacity planning and ordering, while technical staff take care of\ndeployment.\nYou can enable non-technical staff to do the capacity planning by building\na capacity planning dashboard as part of your monitoring system. Create one or\nmore web pages with capacity data in a specialized view, ideally with the ability\nto create graphs automatically. Make this dashboard accessible within the organi-\nzation separately from the main monitoring dashboard. This way anyone in the\norganization can access the data in a reasonable form to justify capital expenditure\non additional capacity.\nSometimes the decision is not to buy more resources but rather to make more\nefﬁcient use of existing resources. Examples might include compressing data rather\nthan buying more storage or bandwidth, or using better algorithms rather than\nincreasing memory or CPU. Testing such scenarios relies on resource regression,\nwhich we will examine next.\n18.3 Resource Regression\nA resource regression is a calculation of the difference in resource usage between\none release or version and another. It is expected that each software release will\nhave slightly different resource needs. If a new software release uses signiﬁcantly\nmore resources, that discrepancy is often unintended; in other words, it is a bug\nthat should be reported. If it is intended, it means capacity planners need to adjust\ntheir models and purchasing plans.\nTo perform a resource regression, do workﬂow analysis based on the user\ntransactions associated with a release. This is simply a fancy way of saying that\nyou make lists of which capabilities are possible with the new release and which\nkind of resource usage goes with each capability. Then, for each capability, you\nmultiply by the number of projected transactions per customer and the number of\ncustomers active. This will give you a resource projection for the new release.\n\n\n382\nChapter 18\nCapacity Planning\nFor example, suppose you have a photo sharing site. There are three main\ntransactions that your site has with a customer. First, the user can log in and edit\ntheir proﬁle data. Second, the user can upload a picture. Third, the user can view a\npicture. Each of these transactions has a resource cost that can be measured. How\nwould you measure it? The easiest way would be by analyzing data from a staging\nsystem with sample users whom you control.\nBy using automated testing scripts to simulate user login and transactions,\nyou can have a known number of users, each involved in a particular transaction.\nThe monitoring data you gather from that system can then be compared with data\nfrom the baseline (no simulated users) system. It is reasonable to assume that the\ndifferences in memory, disk, CPU, and network usage are due to the user trans-\nactions. Subtract the baseline footprint of each of those resources from the loaded\nfootprint. The difference can then be divided by the number of user transactions,\nallowing you to get a sense of the per-transaction resource usage. Be sure to load\ntest your predictions to see whether the resource usage scales linearly or whether\ninﬂection points appear as you add more transactions.\nWhen calculating workﬂow analysis, be sure to include infrastructure resource\ncosts as well. How many DNS lookups are required for this end-to-end transaction?\nHow many database calls? A single transaction may touch many services within\nyour system infrastructure, and those service resources must be assessed as well\nand scaled appropriately along with the transaction server scaling.\n18.4 Launching New Services\nNow that we understand how to plan for capacity on an existing system, let’s\ntalk about launching new services. Launching a new service is difﬁcult and risky\nbecause you have no prior experience or service metrics to plan the initial capacity\nrequired. For large services, testing may be unreliable, as there may be insufﬁcient\ncapacity on a staging environment for a true test.\nAdding to this risk, launch day is often when the service will have the most\nmedia scrutiny. If the system runs out of capacity and becomes unreliable, the\nmedia will have nothing to write about except what a terrible launch it was. This\nmakes a bad ﬁrst impression on customers.\nOne way to mitigate this risk is to ﬁnd a way to have a slow ramp-up in the\nnumber of active users. For example, enabling a product but not announcing it (a\nsoft launch) gives engineers time to ﬁnd and ﬁx problems as they happen. How-\never, if additional resources are needed and will require weeks of lead time, only\nthe slowest of ramp-ups will help.\nRelying on a slow ramp-up is not an option for well-known companies.\nGoogle and Facebook haven’t had a slow ramp-up on a new service, even with\na soft launch, for years. Any new service is immediately ﬂooded with new users.\n",
      "page_number": 404
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 413-420)",
      "start_page": 413,
      "end_page": 420,
      "detection_method": "topic_boundary",
      "content": "18.4\nLaunching New Services\n383\nTherefore the ability to do accurate capacity planning on new services has become\na highly desired skill.\nFortunately, there is a technique that can be used to bridge the knowledge\ngap, and that technique is the dark launch. Facebook used a dark launch of its\nchat messaging system to ensure that the company would bring a reliable system\nto its user base.\nIn a dark launch, the new feature is released into production with simulated\ntrafﬁc, in effect treating the production environment as a well-controlled testbed for\nthe new feature’s resource needs. No user-visible information is created—service\nagents exercise a feature silently from the user perspective, but employ real user\nactivity to trigger simulated activity for the new feature.\nFor example, suppose we wanted to add a photo editing capability to our\nphoto sharing site from the earlier example. To do a dark launch, we might do\nthe following:\n• Create a software toggle for the photo editing feature: on or off. (Software\ntoggles were described in Section 2.1.9.)\n• Create a dark launch toggle for the photo editing feature: on or off.\n• Create a sample photo editing transaction that is saved to a dummy area. The\ncustomer’s photo is not changed, but behind the scenes the photo is edited\nand a modiﬁed version saved elsewhere.\n• Modify an existing transaction to behave differently when the dark launch\ntoggle is “on.” In this case, photo upload will be modiﬁed to run the sample\nphoto editing transaction 25 percent of the time that uploads occur, feeding\nthe uploaded photo into the editing transaction.\nThe 25 percent value is only an example—it could be any percentage. It simply\nrepresents a known quantity that can be calculated at a later time based on trans-\naction data. Starting with a more conservative number such as 5 percent is likely\nto be a good idea with an actual complex service. Use the resource regression anal-\nysis technique to see which kind of resource costs are associated with the new\nlaunch feature. This will give you a ﬁrst pass at capacity planning for the new fea-\nture launch, based on actual production environment usage. Adjust your capacity\naccordingly, and then continue the dark launch, ﬁxing any bugs that are found and\nadjusting capacity as needed. Gradually increase the percentage of sample dark\nlaunch transactions until you reach a level where real usage is likely to occur. Be\nsure to go beyond that by some percentage to give yourself headroom.\nFinally the dark launch is turned off and the feature is turned on for the cus-\ntomers. With the appropriate toggles, this can all be done without rolling a new\nrelease. In practice, enough bug ﬁxing happens during a dark launch that you will\nalmost certainly upgrade new releases during the dark launch period. However,\n\n\n384\nChapter 18\nCapacity Planning\nall of these bugs should be invisible to your customer community, and your actual\nlaunch will be the better for having done a dark launch.\n.\nCase Study: Facebook Chat’s Dark Launch\nThe term “dark launch” was coined in 2008 when Facebook revealed the\ntechnique was used to launch Facebook Chat. The launch raised an impor-\ntant issue: how to go from zero to 70 million users overnight without scaling\nissues. An outage would be highly visible. Long before the feature was visible\nto users, Facebook pages were programmed to make connections to the chat\nservers, query for presence information, and simulate message sends without\na single UI element drawn on the page (Letuchy 2008). This gave Facebook an\nopportunity to ﬁnd and ﬁx any issues ahead of time. If you were a Facebook\nuser back then, you had no idea your web browser was sending simulated chat\nmessages but the testing you provided was greatly appreciated. (Section 11.7\nhas a related case study.)\n18.5 Reduce Provisioning Time\nAs discussed in Section 18.1.4, one of the factors inﬂuencing your headroom\nrequirement is resource acquisition and provisioning time. If you can reduce the\nacquisition and provisioning time, you can reduce your headroom, which in turn\nreduces the amount of capital investment that is idle. In addition, faster acquisi-\ntion and provisioning enables faster response to changing demands and can be a\nsigniﬁcant competitive advantage.\nHowever, idle excess capacity also can be a result of being unable to reduce\nthe available resources quickly. Being able to jettison increased capacity, and its\nassociated costs, at will is also a competitive advantage. There are a number of\napproaches to reducing idle excess capacity, and they can often be combined to\nhave greater effect:\n• Lease computers rather than purchase them. There may be a shorter acqui-\nsition time depending on what is in stock, and the ability to terminate a lease\nand return the hardware may be useful. Leasing can be used to shift capacity\ncosts from capital expenditures into operational costs.\n• Use virtual resources that are allocated quickly and have little or no startup\ncosts associated with them. These resources can also be quickly terminated,\nand billing reﬂects only actual usage of the resources.\n\n\n18.6\nSummary\n385\n• Improve the ordering process for new hardware resources. Preapprove\nbudget decisions and create standardized ordering.\n• Improve installation time. Part of the provisioning time is the time from when\nhardware hits the loading dock to when it is actually in use. Find ways to\nstreamline the actual rack and burn-in.\n• Manage your time. Make it a priority to install new equipment the moment it\narrives. Have no idle hardware. Alternatively, dedicate staff to doing this. Hire\nnon-system administrators (“technicians”) to unbox and rack mount systems.\n• Work with vendors (supply chain management) to reduce ordering time.\n• Place many smaller orders rather than one huge order. This improves par-\nallelism of the system. Vendors may be able to chop up one big order into\nperiodic deliveries and monthly billing. A transition from one huge order\nevery 6 months to 6 monthly orders and deliveries may have billing, capital\ncost, and labor beneﬁts.\n• Automate configuration so that once new hardware is racked, it is soon\navailable for use.\n18.6 Summary\nCapacity planning is the process that ensures services have enough resources\nwhen they are needed. It is challenging to prevent service interruptions due to\nlack of capacity and, simultaneously, preserve capital by adding only the capacity\nrequired at any given time.\nStandard capacity planning is based on current usage and simple rates of\nchange. It assumes future resource needs will be similar to current usage plus two\nkinds of growth. Normal or organic growth is what is expected based on current\ntrends. Planned growth is what is expected due to new initiatives such as mar-\nketing plans. Additional capacity, called headroom, is added to handle short-term\nspikes. Based on the timetable showing lead time (i.e., how long it takes to acquire\nand conﬁgure new resources), capacity schedules can be determined. By reducing\nlead time, capacity planning can be more agile.\nStandard capacity planing is sufﬁcient for small sites, sites that grow slowly,\nand sites with simple needs. It is insufﬁcient for large, rapidly growing sites. They\nrequire more advanced techniques.\nAdvanced capacity planning is based on core drivers, capacity limits of indi-\nvidual resources, and sophisticated data analysis such as correlation, regression\nanalysis, and statistical models for forecasting. Regression analysis ﬁnds corre-\nlations between core drivers and resources. Forecasting uses past data to predict\nfuture needs.\n\n\n386\nChapter 18\nCapacity Planning\nWith sufﬁciently large sites, capacity planning is a full-time job, often done\nby project managers with technical backgrounds. Some organizations employ\nfull-time statisticians to build complex models and dashboards that provide the\ninformation required by a project manager.\nGood capacity planning models can also detect unexpected changes in\nresource needs—for example, a new software release that unexpectedly requires\nmore resources.\nCapacity planning is highly data-driven and uses past data to predict future\nneeds. Launching a brand-new service, therefore, poses a special challenge. Dark\nlaunches and other techniques permit services to gather accurate data before the\nservice is visible to users.\nTo improve cost-effectiveness, reduce the time it takes to provision\nnew resources. Provisioning involves acquiring, conﬁguring, and putting new\nresources into production. Total provisioning time is called lead time. Long lead\ntimes tie up capital. Reducing lead time reduces idle capacity and improves\nﬁnancial efﬁciency.\nCapacity planning is a complex and important part of reliable operations.\nExercises\n1. Describe how standard capacity planning works.\n2. Describe how advanced capacity planning works.\n3. Compare and contrast standard and advanced capacity planning. When is\neach best used?\n4. What are the challenges of launching a new service?\n5. List the resources used by your main application.\n6. Use regression analysis to determine the correlation between two resources.\n7. Create a forecasting model for a service’s capacity needs.\n8. Describe how you would implement a dark launch of a new feature in your\nmain application.\n9. What is a resource regression and why is it important?\n10. Perform a resource regression between your current application release and a\nprevious release. Discuss what you ﬁnd and its implications for your capacity\nplanning.\n11. There are many ways to reduce provisioning time. Which three ways would\nhave the most impact in your environment? (Alternatively, group the methods\nlisted in this chapter by categories of your choosing.)\n12. Why is it desirable to reduce provisioning time?\n\n\nChapter 19\nCreating KPIs\nThe only man I know who\nbehaves sensibly is my tailor;\nhe takes my measurements anew\neach time he sees me. The rest go on\nwith their old measurements and\nexpect me to ﬁt them.\n—George Bernard Shaw\nA startup decides that web site speed is important to the success of its business.\nManagement decides that page load time will be the key performance indicator\n(KPI) that determines employee pay raises at the end of the year. Soon services that\nshared machines are given dedicated resources to avoid any possibility of inter-\nprocess interference. Only the fastest machines are purchased. Many features are\ndelayed as time is dedicated to code optimization. By the end of the year, the KPI\nmeasurements conﬁrm the web pages have extremely fast load times. The goal has\nbeen reached. Sadly, the company has no money for raises because it has spent its\nway out of business.\nMeasurement affects behavior. People change their behavior when they know\nthey are being measured. People tend to ﬁnd the shortest path to meeting a goal.\nThis creates unintended side effects.\nIn this chapter we talk about smart ways to set goals and create KPIs. Managers\nneed to set goals that drive desired behavior to achieve desired results while min-\nimizing the unintended consequences. Done correctly, this enables us to manage\noperations in a way that is more efﬁcient, is fairer, and produces better results.\nSetting KPIs is quite possibly the most important thing that a manager does.\nIt is often said that a manager has two responsibilities: setting priorities and pro-\nviding the resources to get those priorities done. Setting KPIs is an important way\nto verify that those priorities are being met.\n387\n\n\n388\nChapter 19\nCreating KPIs\nThe effectiveness of the KPI itself must be evaluated by making measurements\nbefore and after introducing it and then observing the differences. This changes\nmanagement from a loose set of guesses into a set of scientiﬁc methods. We measure\nthe quality of our system, set or change policies, and then measure again to see their\neffect. This is more difﬁcult than it sounds.\n19.1 What Is a KPI?\nA key performance indicator is a type of performance measurement used to evalu-\nate the success of an organization or a particular activity. Generally KPIs are used\nto encourage an organization or team to reach a particular goal.\nKPIs should be directly tied to the organization’s strategy, vision, or mission.\nGenerally they come from executive management but often other levels of man-\nagement create KPIs for their own purposes, usually as a way of furthering the\nKPIs relevant to them.\nA well-deﬁned KPI follows the SMART criteria: Speciﬁc, Measurable,\nAchievable, Relevant, and Time-phrased. It is speciﬁc, unambiguously deﬁned\nand not overly broad. It is measurable so that success or failure can be objectively\nquantiﬁed. It is achievable under reasonable circumstances. It is relevant to the suc-\ncess of the organization as a whole, or the project as a whole. It is time-phrased,\nwhich means the relevant time period is speciﬁed.\nGoals should be measurable so that one can unambiguously determine what\nfraction of the goal was achieved, or if it was achieved at all. Things you can count,\nsuch as uptime, disk space, and the number of major features released this month,\nare all measurable. Example measurable goals include the following:\n• Provide 10T of disk space to each user.\n• Page load time less than 300 ms.\n• Fewer than 10 “severity 1” open bugs.\n• Launch 10 major features this month.\n• 99.99 percent service uptime.\nNon-measurable goals cannot be quantiﬁed or do not include a speciﬁc numerical\ngoal. Some examples are shown here:\n• Get better at writing Python code.\n• Get better at failing over to DR systems.\n• Provide more free disk space.\n• Make pages load faster.\n\n\n19.2\nCreating KPIs\n389\nThese are all good things, of course. They could all be turned into measurable goals.\nHowever, as stated, they are not measurable.\nKPIs go by many different names. Sometimes they are informally called “met-\nrics,” which is true in the sense that a KPI is a kind of metric—the kind used to drive\norganizational behavior. However, that would be like calling oranges “fruit” and\nexpecting people to know you mean a speciﬁc kind of fruit.\n.\nIntel’s OKR System\nIntel uses a related term called OKRs, which stands for “objectives and key\nresults.” OKRs are often used to set goals at the personal, team, division, and\ncompany levels. The key results are often measured via KPIs. The acronym\nOKR was popularized by venture capitalist John Doerr, who brought the\nconcept to Google. Rick Klau’s video “How Google Sets Goals: OKRs” is\nan excellent explanation of OKRs and serves as a tutorial on how to adopt\nGoogle’s OKR system to your team or business (Klau 2012).\n19.2 Creating KPIs\nCreating good KPIs requires serious time and effort. This process has many steps.\nFirst we envision what the world would look like if the goal was met. Next we\ndetermine ways to quantify how close we are to that ideal. This leads to one or\nmore potential KPIs. Then we consider all the ways that people could behave but\nstill match the incentive. Based on that information, we revise the potential KPIs.\nNow we repeat these steps until we have our ﬁnal KPI.\nWhen deﬁned correctly, KPIs can improve a team’s performance by 10 to\n30 percent. The total cost of a typical engineer, including salary, beneﬁts, and other\nexpenses, can be $200,000 or more per year. For an organization with 50 engineers,\nsuch an improvement is worth $1 million to $3 million per year. Most managers\nfacing a $3 million project would dedicate days or weeks of planning to assure its\nproper execution. In terms of return on investment, spending 10 hours to create\nsuch an improvement has a 1:1000 or 1:3000 payoff. Who would turn down such a\nreturn? Yet KPIs are often created with little forethought and the unintended side\neffects negate any beneﬁt.\nThese numbers should be your personal incentive to develop the skills\nrequired to create effective KPIs. Better KPIs are, quite possibly, more important\nthan anything else you say or do.\n\n\n390\nChapter 19\nCreating KPIs\n19.2.1 Step 1: Envision the Ideal\nPause to imagine what the world would be like if this goal was met perfectly. How\nwould it be different from what it is today? Think in terms of the end result, not\nhow we get there. Exercise your creativity. How would you describe the company,\nproject, or service? How would resources be used?\nOne place to look for inspiration is subteams that are doing well in this area.\nGenerally they won’t have a KPI you can use, but they often have qualities you\nwant to reproduce. Sometimes the quality may be their culture, not a particular\ntangible outcome.\nThe most common mistake we see is managers skipping this ﬁrst step and\ngoing directly to creating a KPI formula. Taking this shortcut is a bad idea. If you\ndon’t ﬁrst decide on the destination, it isn’t likely you’ll get there.\n19.2.2 Step 2: Quantify Distance to the Ideal\nAsk yourself, how can we measure how far we are now from this ideal? We create\nmany candidate KPIs that might measure this distance. The measurement might be\na time duration, a count, the number of times something happens or doesn’t hap-\npen, a quantity, or any other numerical quantiﬁcation. The measurement should\nbe real and repeatable.\nFor example, if the ideal is users having a web experience that is faster than\nwith native software applications on a PC, the way to measure our distance to the\nideal would involve measuring page load time. If the ideal envisions a world with\nno security breaches, the way to measure how far we are from that ideal would be\nthe count of known security breaches detected each month. The ideal might also be\nthat all detected intrusions are investigated within a certain time frame, in which\ncase we would quantify both the time from detection to start of investigation and\nthe duration of the investigation.\n19.2.3 Step 3: Imagine How Behavior Will Change\nFor each potential KPI, try to defeat it. What are all the ways that people could\nbehave but still match the incentive? How could a person maximize his or her\npersonal gain?\nSet aside hope that employees will “just behave.” If they are following your\nformula, they’re behaving. You can’t expect them to read your mind and under-\nstand your intention.\nSalespeople make a good example. If they are selling many products, they\nwill immediately analyze the commission structure and ﬁgure out which product\nto focus on to maximize the use of their time. If they sell two products with the\nsame price and same commission rate but one is easier to sell, the other product\nwill be starved for attention. If they sell 10 products, each with a different price,\n",
      "page_number": 413
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 421-429)",
      "start_page": 421,
      "end_page": 429,
      "detection_method": "topic_boundary",
      "content": "19.2\nCreating KPIs\n391\ncommission, and likelihood of a successful sale, they will do the math and ﬁgure\nout which products to actively sell; the others will be ignored. This is not cheating,\nbut rather following the rules they’ve been given. This is why some companies\nhave separate sales teams for big customers and small customers. Who would\nspend time trying to make $1,000 deals with small customers when they can make\nmillion-dollar deals with big companies? The sales cycle would have to be 1000\ntimes longer for the big deals to be less efﬁcient. Without dedicated sales teams,\nthe company would miss out on any sales to small companies.\nSimilarly, engineers will examine the KPIs given and follow them as stated.\nAgain, this is not “gaming the system,” but simply following the rules. This is\ndoing what the engineers were told to do. Gaming the system would be falsifying\nlogs or going around the KPIs to beneﬁt themselves. Do not be upset or surprised\nwhen people conform to what you wrote, rather than what you intended.\nReturning to our earlier security example, the easiest way to achieve the goals\nas stated is to turn off any mechanisms that detect such breaches. This is probably\nnot the intended reaction. Thus, it should be revised.\n19.2.4 Step 4: Revise and Select\nBased on what we’ve learned in Step 3, we revise the KPI. We may select one KPI\nover another or loop back to Step 1 and start over.\nSeek conﬁrming and non-conﬁrming data. For example, calculate the KPI\nbased on past or current metrics. If such metrics are not available, test against\nsimulated data.\nSeek the opinions of others. Ask how they would behave under such KPIs. You\nwill get different answers depending on whether the person has a vested interest\nin the KPI. If the person’s work will be judged by the KPI, ﬁlter what he or she says\nbased on the individual’s potential bias toward personal gain or preferring to be\njudged less harshly. If the person would beneﬁt from the KPI’s goal succeeding, he\nor she will have the opposite bias but may not be in tune with the internal processes\nthat can be used to subvert the KPI.\nAsking for feedback on KPIs that have not been announced creates the\npotential that rumors will spread about the new KPI. Like the children’s game\n“Telephone,” where the message becomes more and more misconstrued as it gets\npassed along, the rumor of the new KPI will invariably portray it as harsher as\nit moves from person to person. This will hurt morale and will result in mis-\ninformation that confuses people when the real KPI is announced. The worst\nsituation is where the draft KPI is tossed out but the rumors persist. Therefore\ndiscussing potential KPIs should be done with an understanding of who they can\nbe shared with.\nIt is sometimes possible to test the KPI on one team or one project before apply-\ning it to all the others. This gives us real-world experience with how people react.\n\n\n392\nChapter 19\nCreating KPIs\nAlternatively, it can be employed during a trial period so that people expect some\nrevision before it becomes policy.\nWe don’t have many chances to revise the KPI after it becomes policy. If a KPI\nis announced and has unintended negative side effects, you should modify the KPI\nto ﬁx any bugs. However, if the KPI changes again and again, management looks\nlike they don’t know what they are doing. Employees lose trust. Morale will suffer\ngreatly if the rules keep changing. People will feel like the rug is being pulled out\nfrom under their feet if they have just ﬁnished adjusting their behavior to align\nwith one KPI when it changes and requires opposite adjustments.\nBecause of this, great effort should be put into getting it right the ﬁrst time.\nBecause of the beneﬁts of a well-structured KPI, and the potential damage of a\nbadly engineered KPI, this effort should be given high priority and be taken seri-\nously. The ﬁnal KPI doesn’t have to be simple, but it has to be easy to understand.\nIf it is easy to understand, it will be easy to follow.\nReturning to our security breach example, our last draft contained a bug in\nthat it did not include both the coverage of the intrusion detection system and the\nnumber of intrusions detected. Thus, we revise it to be three KPIs: one that reﬂects\nthe percentage of subnets that are covered by the intrusion detection system, one\nthat reﬂects the total number of intrusions, and one that reﬂects the duration of\ninvestigations.\n19.2.5 Step 5: Deploy the KPI\nThe next step is to deploy the KPI, or institute it as policy. This is mostly a com-\nmunication function. The new KPI must be communicated to the team that is\nresponsible for the KPI, plus key stakeholders, management, and so on.\nDeploying the KPI means making people aware of it as well as putting into\nplace mechanisms to measure the KPI. The KPI should be deployed with the\nassumption that it may require revision in the future, but this should not be an\nexcuse to do a bad job in creating it.\nIf at all possible, the metrics that are used to calculate KPIs should be collected\nautomatically. This might occur via the monitoring system, or it might be done by\nextracting the information from logs. Either way, the process should not require\nhuman intervention. Even if you forget about them, the metrics you need should\nautomatically accumulate for you.\nIf the metrics cannot be collected automatically, there are two actions to take.\nOne is to ﬁnd a way to ensure that the metrics are collected manually. This could be\nan automated email reminder or a repeating calendar entry. The other is to develop\na mechanism so that the collection will be automated. There are no metrics that\ncan’t be collected automatically, only metrics whose collection has not yet been\nautomated.\n\n\n19.3\nExample KPI: Machine Allocation\n393\nOnce the data is collected, a dashboard should be created. A dashboard is a\nweb page that shows a visualization of the KPI. It may have related metrics and\ninformation useful for drilling down into the data.\nImportant KPIs should have a second dashboard, one that is appropriate for\nlarge displays. Install a large monitor in a hallway or other common area and dis-\nplay this dashboard continuously. Such displays should use large fonts and have\ngraphics that are easy to view from a distance. Such displays serve as an impor-\ntant reminder of the KPIs’ status and can become a point of pride when the KPIs\nare met.\n19.3 Example KPI: Machine Allocation\nSuppose we are developing a KPI to assess the quality of the process by which\nvirtual machines (VMs) are created. This KPI may apply to a public cloud service\nprovider, it may be geared toward a team that creates VMs for an internal cloud\nservice within a large company, or perhaps it is intended just to assess the process\nthat a team uses for creating its own VMs.\n19.3.1 The First Pass\nWe begin with Step 1, Envision the Ideal. In an ideal world, people would get the\nVMs they want as soon as they request them. There would be no delay.\nIn Step 2, Quantify Distance to the Ideal, we simply measure the duration of\ntime from the request to when the VM is created.\nIn Step 3, Imagine How Behavior Will Change, we brainstorm all the ways that\npeople could behave but still match the incentive. We foresee many challenges.\nOne challenge is that people could get very creative about the deﬁnition of\n“start time.” It could be the time when the request is received from the user, or it\ncould be when the creation process actually begins. If requests are made by creating\ntickets in a helpdesk request system, the delay before someone processes the ticket\ncould be very large. If the requests come in via a web portal or API, they may\nbe queued up and processed sequentially. If that wait time is not included in the\nmetric, it would make the team look good, but would not truly reﬂect the service\nusers receive.\nA more realistic indication comes from measuring the end-to-end result from\nthe customer’s perspective. Doing so might inspire the team to move from a ticket-\nbased request system to a self-service portal or API. This would not only replace\nthe human process of re-entering data from a ticket, but also ensure that all the\ninformation needed to complete the process is collected at the very beginning.\nThus it avoids the back-and-forth communication that might be required to collect\ninformation the user forgot or didn’t know to include in the ticket.\n\n\n394\nChapter 19\nCreating KPIs\nPeople could also demonstrate creativity in how they deﬁne when the request\nis “complete.” The incentive is to interpret completion time as soon as possible. Is\nthe VM created when the VM Manager allocates RAM, disk, and other resources to\ncreate the empty virtual machine, or when that virtual machine has loaded its oper-\nating system? If a different team is responsible for the OS installation mechanism,\ncan we be “complete” once the OS installation starts, whether or not it ﬁnishes suc-\ncessfully? To the user, a failed, half-installed VM is worthless, but a mistated KPI\nmight permit it.\nThe team could rationalize any of these end points to be the end time. Thus,\nwe go back to Step 1 and use what we have learned to do better.\n19.3.2 The Second Pass\nOur original deﬁnition was a very narrow deﬁnition of the end result. Let’s broaden\nthe deﬁnition and focus on what the user sees as the desired result.\nTo the user, the end result is that the VM is ready for use for the purpose the\nuser intended.\nIdeally, the user always receives the VM that has been requested: it has the\nright name, size (amount of RAM, disk, and vCPU), operating system, and so on.\nThus, our ideal world implies that the right information is gathered from the user\nand the user receives a VM that works as requested.\nPerhaps some roadblocks might prevent the customer from using the machine\nright away. These should be included in the metric. For example, to use the machine\nrequires DNS changes to be propagated, access controls to be implemented, and\nso on.\nAlso consider the use of company resources. Can users request an inﬁnite\nnumber of VMs? Who pays for them? Billing for resources is often an effective\nway to lead users to restrain their use of a resource. We’ll use that.\nTherefore we revise our ideal-world deﬁnition as follows: Users get a usable\nVM, as they requested, as quickly as possible, with billing arrangements estab-\nlished.\nContinuing our example into Step 2, we deﬁne the start time as when the\nrequest is initially received from the user. The end time is when the VM is usable by\nthe requester. We can deﬁne “usable” as the user being able to log into the machine.\nThis automatically includes DNS propagation and access control as well as issues\nwe are unaware of.\nThe draft KPI becomes:\nThe 90th percentile creation time, which starts when the request is created, and ends\nwhen the user is able to log into the machine.\nWe use a percentile instead of an average because, as discussed in Section 17.5,\naverages can be misleading. A single long-delayed creation would unfairly ruin\nan average.\n\n\n19.3\nExample KPI: Machine Allocation\n395\nAn ideal world has no resource shortages. Anytime a new VM is needed, there\nwould be capacity to allocate one. New capacity would come online just in time to\nfulﬁll any request. We could add a metric related to capacity planning but it is bet-\nter to keep the KPI at a high level of abstraction, rather than micro-managing how\nthe service is run. We are concerned with the end result. If capacity planning is not\ndone right, that fact will surface due to the KPI we have constructed as requests\nare held while waiting for new capacity to come on line. That said, if capacity plan-\nning becomes an issue, we can later establish KPIs speciﬁcally related to capacity\nplanning efﬁciency.\nIn Step 3, we imagine how behavior will change. If a cancelled request does\nnot count toward the KPI, we could simply cancel any request that takes too long.\nFor example, if requests are made via a helpdesk ticket, if the user doesn’t supply\nall the information required to complete the task, we could cancel the request and\nlet the user know what information we need when the user creates the new ticket.\nThis would be terrible behavior but would improve the KPI.\nIf the OS installation step is unreliable and requires multiple restarts, this\nwould delay the total creation time. To improve our KPI numbers, the operators\ncould simply cancel jobs that fail rather than retry them. Again, this would beneﬁt\nthe KPI but not the user.\nWe could work around this loophole by adding signiﬁcant complexity to the\nKPI. However, often it is easier to prevent bad behavior by letting people know\nthey are being watched. First, we can publish the number of customer-initiated\ncancellation requests. This permits management to quietly watch for shenanigans.\nSecond, we can privately agree with the team’s manager that such behavior will\nbe discouraged. Executive management can assist by creating an environment with\nhigh standards, including only hiring managers who wouldn’t tolerate such behav-\nior. Such managers would, for example, notice that the system doesn’t log who\ncancelled a request, and require that this be changed so that the KPI can properly\nmeasure customer-initiated cancellations versus cancellations from operational\nstaff.\nStep 4, Revise and Select, results in the following KPI:\nThe 90th percentile creation time, measured from when the request is received from\nthe user until the user is able to log into the machine. Manual and automatic retries\nafter a failure are counted as part of the original request, not as separate requests.\nRequests that are outright canceled will be logged and investigated if within a week\nthere are more than 5 operator-initiated cancellations or if more than 1 percent of all\nrequests end in customer-initiated cancellation.\nStep 5 deploys the KPI. The KPI is communicated to key stakeholders. Mea-\nsurements required to calculate the KPI plus the computation itself should be\nautomated and presented in a dashboard. In our example, the system might gener-\nate the metric we require or it might timestamp the request and completion times,\n\n\n396\nChapter 19\nCreating KPIs\nrequiring some post-processing to correlate the two and calculate the duration.\nHowever the metric is gathered, it should then be stored and made available to\nwhatever data visualization system is in use so that a graphical dashboard can be\ncreated.\n19.3.3 Evaluating the KPI\nA few weeks after deployment, the initial results should be audited for unintended\nnegative side effects.\nIn our example, depending on how often VMs are created, the resulting KPI\nmeasurements might be reviewed daily, weekly, or monthly. These results might\ninspire additional metrics to isolate problem areas. The VM creation process is\nmade up of many steps. By measuring the wait time before each step, as well as\nthe duration of each step, opportunities for improvement can be easily found.\nFor example, measuring lead time might reveal a long span of time between\nwhen a ticket is ﬁled and when the actual creation begins, indicating that a self-\nservice request system would have a large beneﬁt. Lead time might reveal that\nrequests are often delayed for weeks waiting for new resources to be installed,\nindicating that capacity planning needs improvement. Collecting data on how\nmany times a task fails and is retried might indicate that the OS installation pro-\ncess is unstable and should be improved. Perhaps DNS propagation delays can\nbe tightened or the OS installation process can be made faster using image-based\ninstallations. Monitoring network utilization might ﬁnd an overloaded link that, if\nimproved, could result in faster installation time in general. By analyzing the types\nof requests, it may be determined that a few standard-size VMs can be pre-created\nand simply handed out as needed.\nAll these changes are viable. By taking measurements, we can predict how\neach one might improve the KPI. By collecting KPI measurements before and after\nchanges, we can measure the actual improvement.\n19.4 Case Study: Error Budget\nBenjamin Treynor Sloss, Vice President of Engineering at Google, revealed a highly\nsuccessful KPI called Google Error Budget. The goal was to encourage high uptime\nwithout stiﬂing innovation, and to encourage innovation without encouraging\nundue risk.\n19.4.1 Conflicting Goals\nThere is a historic conﬂict between developers and operations teams. Developers\nwant to launch new features; operations teams want stability.\n\n\n19.4\nCase Study: Error Budget\n397\nDevelopers are in the business of making change. They are rewarded for new\nfeatures, especially ones that are highly visible to the end customers. They would\nprefer to have each feature they create pushed into production as fast as possible\nso as not to delay gratiﬁcation. The question they get the most from management\nis likely to be, “When will it ship?”\nOperations people are in the business of stability. They want nothing to break\nso they don’t get paged or otherwise have a bad day. This makes them risk averse.\nIf they could, they would reject a developer’s request to push new releases into\nproduction. If it ain’t broke, don’t ﬁx it. The question they get the most from\nmanagement is likely to be, “Why was the system down?”\nOnce a system is stable, operations would prefer to reject new software\nreleases. However, it is culturally unacceptable to do so. Instead, rules are cre-\nated to prevent problems. They start as simple rules: no upgrades on Friday; if\nsomething goes wrong, we shouldn’t have to spend the weekend debugging it.\nThen Mondays are eliminated because human errors are perceived to increase then.\nThen early mornings are eliminated, as are late nights. More and more safeguards\nare added prior to release: 1 percent tests go from being optional to required. Basi-\ncally operations never says “no” directly but enough rules accumulate that “no” is\nvirtually enforced.\nNot to be locked out of shipping code, developers work around these rules.\nThey hide large amounts of untested code releases behind ﬂag ﬂips; they encode\nmajor features in conﬁguration ﬁles so that software upgrades aren’t required, just\nnew conﬁgurations. Workarounds like these circumvent operations’ approvals and\ndo so at great risk.\nThis situation is not the fault of the developers or the operations teams. It is\nthe fault of the manager who decreed that any outage is bad. One hundred percent\nuptime is for pacemakers, not web sites. The typical user is connecting to the web\nsite via WiFi, which has an availability of 99 percent, possibly less. This dwarfs any\ngoal of perfection demanded from on high.\n19.4.2 A Unified Goal\nTypically four 9s (99.99 percent) availability is sufﬁcient for a web site. That leaves\na “budget” of 0.01 percent downtime, a bit less than an hour each year (52.56\nminutes). Thus the Google Error Budget was created. Rather than seeking perfect\nuptime, a certain amount of imperfection is budgeted for each quarter. Without\npermission to fail, innovation is stiﬂed. The Error Budget encourages risk taking\nwithout encouraging carelessness.\nAt the start of each quarter, the budget is reset to 13 minutes, which is about\n0.01 percent of 90 days. Any unavailability subtracts from the budget. If the bud-\nget has not been exhausted, developers may release as often as they want. When\n\n\n398\nChapter 19\nCreating KPIs\nthe budget is exhausted, all launches stop. An exception is made for high-priority\nsecurity ﬁxes. The releases begin again when the counter resets and there is once\nagain a 13-minute budget in place.\nAs a result, operations is no longer put into the position of having to decide\nwhether to permit a launch. Being in such a position makes them “the bad guys”\nevery time they say no, and leads developers to think of them as “the enemy to be\ndefeated.” More importantly, it is unfair to put operations in this position because\nof the information asymmetry inherent in this relationship. Developers know the\ncode better than operations and therefore are in a better position to perform testing\nand judge the quality of the release. Operations staff, though they are unlikely to\nadmit it, are not mind readers.\n19.4.3 Everyone Benefits\nFor developers, the Error Budget creates incentives to improve reliability by offer-\ning them something they value highly: the opportunity to do more releases. This\nencourages them to test releases more thoroughly, to adopt better release prac-\ntices, and to invest effort in building frameworks that improve operations and\nreliability. Previously these tasks might have been considered distractions from\ncreating new features. Now these tasks create the ability to push more features.\nFor example, developers may create a framework that permits new code to\nbe tested better, or to perform 1 percent experiments with less effort. They are\nencouraged to take advantage of existing frameworks they may not have con-\nsidered before. For example, implementation of lame-duck mode, as described in\nSection 2.1.3, may be built into the web framework they use, but they have simply\nnot taken advantage of it.\nMore importantly, the budget creates peer pressure between developer teams\nto have high standards. Development for a given service is usually the result of\nmany subteams. Each team wants to launch frequently. Yet one team can blow the\nbudget for all teams if they are not careful. Nobody wants to be the last team to\nadopt a technology or framework that improves launch success. Also, there is less\ninformation asymmetry between developer teams. Therefore teams can set high\nstandards for code reviews and other such processes. (Code reviews are discussed\nin Section 12.7.6.)\nThis does not mean that Google considers it okay to be down for an hour\neach year. If you recall from Section 1.3, user-visible services are often composed\nof the output of many other services. If one of those services is not responding,\nthe composition can still succeed by replacing the missing part with generic ﬁller,\nby showing blank space, or by using other graceful degradation techniques as\ndescribed in Section 2.1.10.\n\n\nExercises\n399\nThis one KPI has succeeded in improving availability at Google and at the\nsame time has aligned developer and operations priorities, helping them work\ntogether. It removes operations from the “bad guy” role of having to refuse\nreleases, and it gives developers an incentive to balance time between adding new\nfeatures and improving operational processes. It is simple to explain and, since\navailability is already tightly monitored, easy to implement. As a result, all of\nGoogle’s services beneﬁt.\n19.5 Summary\nManaging by using KPIs is a radical departure from traditional IT management.\nIt is effective because it sets goals and permits the smart people whom you hired\nto work out how to achieve them. Those people are closer to the task and more\nknowledgable about the technical details at hand, making them better suited to\ninventing ways to achieve the goal.\nCreating effective KPIs is difﬁcult—and it should be. It yields a huge return on\ninvestment when done right. Something that has a 1:1000 payoff is not going to be\neasy. KPIs should be speciﬁc, measurable, achievable, relevant, and time-phrased.\nPoorly written KPIs have unintended consequences when people follow the rules\nas written rather than what you intended. It is important to think through all the\nways that the KPI may be interpreted, and the actions that people might take to\nimprove their performance on this metric.\nSpend time up front examining and amending the KPI to make sure that the\nKPI will most likely give the result you want. Do not over-complicate the KPI. If\nyou suspect that the KPI might trigger an adverse behavior, measure and publish\nthat behavior as well, and make sure that your managers know to watch out for\nand discourage it.\nRemember that the KPI should be achievable. One hundred percent\navailability is not achievable, unless the deﬁnition of “availability” is adjusted\nto cover more than you intended. However four 9s (99.99 percent) is achievable.\nGoogle’s Error Budget KPI successfully uses that target to achieve the desired\nresult: a stable service with innovative new features deployed frequently. It is an\nexcellent example of how a good KPI can beneﬁt everyone.\nExercises\n1. What is a KPI?\n2. What are the SMART criteria? Brieﬂy describe each one.\n3. Give examples of unintended side effects of KPIs.\n",
      "page_number": 421
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 430-446)",
      "start_page": 430,
      "end_page": 446,
      "detection_method": "topic_boundary",
      "content": "400\nChapter 19\nCreating KPIs\n4. What are the steps for creating KPIs? Evaluate which steps are the most\ndifﬁcult and provide justiﬁcation as to why they are difﬁcult.\n5. Which KPIs do you track in your environment and why?\n6. Create an effective KPI for assessing a service in your environment. After\ncreating it, have three other people tell you how they would maximize their\npersonal gain. Revise the KPI. Report on the ideal (Step 1), the initial KPI(s),\npeople’s reactions, and the ﬁnal KPI.\n7. How does the KPI you created address each of the SMART criteria?\n8. When managed through KPIs, why do people follow what is written rather\nthan what was intended? Is this good or bad?\n9. The example in Section 19.3 always discusses measuring the end-to-end time\nbased on the customer’s perspective. Would there be value in measuring the\ntime from when the VM creation starts to when it is usable?\n10. How would you modify the KPI created in Section 19.3 if after the request was\nmade, the requester’s manager had to approve the request?\n\n\nChapter 20\nOperational Excellence\nA company can seize\nextraordinary opportunities\nonly if it is very good at\nthe ordinary operations.\n—Marcel Telles\nThis chapter is about measuring or assessing the quality of service operations. It\nproposes an assessment tool and gives examples of how to use it to evaluate an\nindividual service, a team, or multiple teams.\nIn Chapter 19, we discussed how to create KPIs that drive desired behavior to\nachieve speciﬁc goals. This chapter describes an assessment system that evaluates\nthe degree of formality and optimization of processes—that is, whether processes\nare ad hoc, or formal, or actively optimized. This assessment is different than KPIs\nin that it gauges teams on a more generic level, one that is more comparable across\nteams or across services within a team.\nSuch assessments help identify areas of improvement. We can then make\nchanges, reassess, and measure the improvement. If we do this periodically, we\ncreate an environment of continuous improvement.\n20.1 What Does Operational Excellence Look Like?\nWhat does great system administration look like? Like art and literature, it is difﬁ-\ncult to deﬁne other than to say you know it when you see it. This ambiguity makes\nit difﬁcult to quantitatively measure how well or poorly a system administration\nteam is performing.\nHigh-performing organizations have smooth operations, well-designed poli-\ncies and practices, and discipline in what they do. They meet or exceed the needs\nof their customers and delight them with innovations that meet future needs often\nbefore such needs ever surface. The organization is transparent about how it plans,\n401\n\n\n402\nChapter 20\nOperational Excellence\noperates, provides services, and handles costs or charge-backs to customers. The\nvast majority of customers are happy customers. Even dissatisﬁed customers feel\nthey have a voice, are heard, and have a channel to escalate their issues. Everyone\nfeels the operations organization moves the company forward. Its funding reﬂects\na reasonable budget for the work the organization does. The organization makes\nits successes visible and, more importantly, is honest and forthright when it comes\nto discussing its own faults. The organization is constantly improving. Outages\nand escalated issues result in action plans that reduce future occurrences of that\nproblem. The world is constantly changing, and the organization incorporates new\ntechnologies and techniques to improve its inner workings as well as the services\nit provides.\nOperations organizations seem to fall into three broad categories or strata: the\ngreat ones, the ones that want to be great, and the ones that don’t even know what\ngreat is.\nWe estimate that 5 to 10 percent of all operations teams fall into this ﬁrst cate-\ngory. They know and use the best practices of our industry. Some even invent new\nones. The next 25 to 30 percent know that the best practices exist but are strug-\ngling to adopt them. The remaining super-majority do not even know these best\npractices exist.\nScience ﬁction writer William Gibson famously said, “The future is already\nhere—it’s just not very evenly distributed.” Likewise, the knowledge of how to be\na great system administration team is here—it’s just not very evenly distributed.\n20.2 How to Measure Greatness\nMeasuring the quality of an operations team is extremely difﬁcult. Other aspects of\noperations are easy to measure. For example, size can be measured by counting the\nnumber of team members, the number of services provided, or the dollars spent\nper year. Scale can be measured by counting the number of machines, the amount\nof storage, the total bandwidth used, and so on. We can measure efﬁciency using\ncost ratios.\nAlas, quality is not so easy to measure.\nImagine for a moment that we could measure quality. Imagine we had a stan-\ndard way to rate the quality of an operations team with a simple value on a scale of\n0 to 1000. Also imagine that we could, possibly by some feat of magic, rank every\noperations organization in the world.\nIf we could do that, we could line all of these organizations up from “best”\nto “worst.” The potential for learning would be incredible. We could observe what\nthe top 50 percent do differently from the bottom 50 percent. Alternatively, a single\norganization could study the organizations ranked higher for inspiration on how\nto improve.\n\n\n20.3\nAssessment Methodology\n403\nAlas, there is no such single measurement. Operations is just too complex.\nTherefore the measurement, or assessment, must reﬂect that complexity.\nAssessments sound a lot like the grades we received in school, but the concept\nis very different. A student assessment evaluates an individual student’s learning\nand performance. Grades assess learning, but they also incorporate attendance,\nparticipation, and effort. An assessment is more focused.\nAn assessment of a service is an evaluation based on speciﬁc criteria related\nto process maturity. It is not an evaluation of whether the service is popular, has\nhigh availability, or is fast. Not all services need to be popular, highly available, or\nfast. In contrast, all services need good processes to achieve whatever goals they do\nhave. Therefore we assess process because good processes are a roadmap to success.\n20.3 Assessment Methodology\nThis assessment methodology is a bottom-up assessment. A service is evaluated\non eight attributes, called operational responsibilities (OR). Each OR is assessed\nto be at one of ﬁve levels, with 5 being best. If assessment is done periodically, one\ncan see progress over time. A weighted average can be used to roll up the eight\nindividual assessments to arrive at a single number representing the service.\nA team performs this assessment on each service. A team can be assessed using\nthe weighted average of the services it provides. Teams can then be compared by\nstack rank. Teams can seek to improve their rank by identifying problem areas to\nwork on. Best practices of high-ranking teams can be identiﬁed and shared.\nThe eight core ORs are geared toward service management and do not ﬁt well\nfor transactional IT services such as a helpdesk or other front-of-house, tier 1, or\nother customer-facing service center.\n20.3.1 Operational Responsibilities\nWe have identiﬁed eight broad categories of operational responsibilities that most\nservices have. Some may be more or less important for a particular service. Your\nteam may emphasize or de-emphasize certain ORs by using a weighted average\nwhen performing roll-ups. Teams may also choose to add more ORs if need be.\nThe eight common operational responsibilities are as follows:\n• Regular Tasks (RT): How normal, non-emergency, operational duties are\nhandled; that is, how work is received, queued, distributed, processed, and\nveriﬁed, plus how periodic tasks are scheduled and performed.\n• Emergency Response (ER): How outages and disasters are handled. This\nincludes technical and non-technical processes performed during and after\noutages (response and remediation). This is the stuff of Chapters 6, 14, and 15.\n\n\n404\nChapter 20\nOperational Excellence\n• Monitoring and Metrics (MM): Collecting and using data to make decisions.\nMonitoring collects data about a system. A metric uses that data to measure\na quantiﬁable component of performance. This is the stuff of Chapters 16, 17,\nand 19.\n• Capacity Planning (CP): Determining future resource needs. Capacity plan-\nning involves the technical work of understanding how many resources are\nneeded per unit of growth, plus non-technical aspects such as budgeting,\nforecasting, and supply chain management. This is the stuff of Chapter 18.\n• Change Management (CM): Managing how services are purposefully\nchanged over time. This includes the service delivery platform and how it is\nused to create, deliver, and push into production new application and infra-\nstructure software. This includes ﬁrmware upgrades, network changes, and\nOS conﬁguration management.\n• New Product Introduction and Removal (NPI/NPR): Determining how new\nproducts and services are introduced into the environment and how they\nare deprecated and removed. This is a coordination function. Introducing\nand removing a service or product from an environment touches on multiple\nteams. Removing it involves tracking down the current users and managing\nthe migration away from the service so it can be eliminated. For example, it\nmay involve coordinating all teams that are touched by introducing a new\nbrand of hardware into an ecosystem, launching an entirely new service, or\ndecommissioning all instances of an old hardware platform.\n• Service Deploy and Decommission (SDD): Determining how instances of an\nexisting service are created and how they are turned off (decommissioned).\nAfter a service is introduced to an environment, it is deployed many times.\nAfter serving their purpose, these deployments are decommissioned. Exam-\nples include turning up a new datacenter, adding a replica of a service, or\nadding a replica of a database.\n• Performance and Efficiency (PE): Measuring how well a service performs\nand how cost-effectively resources are used. A running service needs to have\ngood performance without wasting resources. Examples include managing\nutilization or power efﬁciency and related costs.\nThe difference between NPI, SDD, and CM is subtle. NPI is how something is\nlaunched for the ﬁrst time. It is the non-technical coordinating function for all\nrelated processes, many of which are technical. SDD is the technical process of\ndeploying new instances of an existing item, whether it is a machine, a server, or a\nservice. It may include non-technical processes such as budget approval, but these\nare in support of the technical goal. CM is how upgrades and changes are man-\naged, either using a software deployment platform or an enterprise-style change\nmanagement review board.\n\n\n20.3\nAssessment Methodology\n405\nSee Appendix A for more detailed descriptions of these, plus additional ORs\nspeciﬁc to certain chapters of this book.\n20.3.2 Assessment Levels\nThe rating, or assessment, uses a scale of 1 to 5, based on the Capability Maturity\nModel (CMM). The CMM is a tool designed to measure the maturity of a capability\nor responsibility.\nThe term “maturity” indicates how formally or informally an operational\naspect is practiced. The formality ranges from ad hoc and improvised, to having a\ndocumented process, to measuring the results of the process, to actively improving\nthe system based on those measurements.\nThe CMM deﬁnes ﬁve levels:\n• Level 1, Initial: Sometimes called Chaotic. This is the starting point for a new\nor undocumented process. Processes are ad hoc and rely on individual heroics.\n• Level 2, Repeatable: The process is at least documented sufﬁciently such that\nit can be repeated with the same results.\n• Level 3, Defined: Roles and responsibilities of the process are deﬁned and\nconﬁrmed.\n• Level 4, Managed: The process is quantitatively managed in accordance with\nagreed-upon metrics.\n• Level 5, Optimizing: Process management includes deliberate process opti-\nmization/improvement.\nLevel 1: Initial\nAt this level, the process is ad hoc. Results are inconsistent. Different people do\ntasks in different ways, usually with slightly different results. Processes are not\ndocumented. Work is untracked and requests are often lost. The team is unable\nto accurately estimate how long a task will take. Customers and partners may be\nhappy with the service they receive, but there is no evidence-based determination\nat this time.\nLevel 2: Repeatable\nAt this level, the process has gone from being ad hoc to repeatable. The steps are\ndeﬁned in such a way that two different people can follow them and get the same\nresults. The process is documented with no missing steps. The end results are rel-\natively consistent. This is not to say that errors don’t happen; after all, nothing is\nperfect. Because there is little measurement at this level, we may not know how\noften work is defective.\n\n\n406\nChapter 20\nOperational Excellence\nLevel 3: Defined\nAt this level, the roles and responsibilities of the process are deﬁned and conﬁrmed.\nAt the previous level, we learned what needed to be done. At this level, we know\nwho is responsible for doing it, and we have deﬁnitions of how to measure cor-\nrectness. The correctness measurements might not be collected, but at least the\npeople involved know what they are. Each step has a series of checks to ﬁnd errors\nas they happen rather than waiting until the end of the process to ﬁnd out what\nwent wrong. Duplication of effort is minimized. If we need to increase the rate of\nproduction, we can duplicate the process or add more capacity.\nLevel 4: Managed\nAt this level, aspects of the process are measured. How long each step takes is mea-\nsured, including what portion of that time involved waiting to begin the process\nitself. Measurements include how often the process is done each month, how often\nit is performed without error, and how many times an exceptional case arises that\nrequires special treatment and ad hoc processes. Variation is measured. These mea-\nsurements are collected automatically. There is a dashboard that shows all of these\nmeasurements. It is easy to spot bottlenecks. Postmortems are published within a\nspeciﬁc amount of time after an exception. Exceptions are cataloged and periodi-\ncally reviewed. Requests to change a process are justiﬁed using measurement data\nto show that there is a problem. Capacity needs are predicted ahead of time.\nLevel 5: Optimizing\nAt this level, the measurements and metrics are being used to optimize the entire\nsystem. Improvements have been made at all the previous levels but the improve-\nments made here are prompted by measurements and, after the change is made, the\nsuccess is evaluated based on new measurements. Analyzing the duration of each\nstep exposes bottlenecks and delays. Measurements indicate month-over-month\nimprovements. We can stress-test the system to see what breaks, ﬁx it, and then\nrun the system at the new level.\n.\nDefined versus Managed\nPeople often have a difﬁcult time conceptualizing the subtle difference\nbetween Levels 3 and 4. Consider the example of assessing the load bal-\nancer aspect of a service. Load balancers can be used to improve capacity or\nresiliency, or both. Resiliency requires the system to run with enough spare\ncapacity to withstand a failed backend. (See Section 6.3.)\nIf there is a written policy that the load balancer is used for resiliency, this\nis a Level 3 behavior. If there exists monitoring that determines the current\n\n\n20.4\nService Assessments\n407\n.\nlevel of redundancy (N + 0, N + 1, and so on), this is Level 4 behavior. In this\ncase, Level 4 is signiﬁcantly more difﬁcult to achieve because it requires a lot\nof effort to accurately determine the maximum capacity of a backend, which\nis required to know how much spare capacity is available.\nSomewhat surprisingly, we have found many situations where there is\nno clear sense of how a load balancer is being used, or there is disagreement\namong team members about whether the role of a load balancer is to improve\ncapacity or to improve resiliency. This demonstrates Level 1 behavior.\n20.3.3 Assessment Questions and Look-For’s\nTo perform an assessment for an operational responsibility, describe the current\npractices in that area. Based on this description, evaluate which level describes the\ncurrent practices.\nTo help this process, we have developed a standard set of questions to ask to\nhelp form your description.\nWe’ve also developed a set of look-for’s for each level. A look-for is a behavior,\nindicator, or outcome common to a service or organization at a particular level. In\nother words, it is “what a level looks like.” If you read a description and it “sounds\nlike you’re talking about where I work,” then that’s a good indication that your\norganization is at that level for that service.\nFor example, in Regular Tasks, at Level 1 there is no playbook of common\noperational duties, nor is there a list of what those duties are. At Level 3 those\nduties have been deﬁned and documented.\nLook-for’s are not checklists. One does not have to demonstrate every look-\nfor to be assessed at that level. Some look-for’s are appropriate only for certain\nsituations or services. Look-for’s are simply signals and indicators, not steps to\nfollow or achievements to seek out.\nAppendix A lists operational responsibilities, questions, and look-for’s for\neach level. Take a moment to ﬂip there and review some of them. We’ll wait.\n20.4 Service Assessments\nTo put this evaluation process into action, staff must periodically perform an\nassessment on each major service or group of related services they provide.\nThe assessment is used to expose areas for improvement. The team brainstorms\nways to ﬁx these problems and chooses a certain number of projects to ﬁx the\nhighest-priority issues. These become projects for the new quarter.\nThe team then repeats the process. Services are assessed. The assessment\ninspires new projects. The projects are worked on. The services are assessed. The\n\n\n408\nChapter 20\nOperational Excellence\nprocess then begins again. Teams that work this way beneﬁt from having a struc-\nture in place that determines projects. They always know what they should be\nworking on.\n20.4.1 Identifying What to Assess\nFirst, identify the major services that your organization provides. A web-based\nservice might identify the major components of the service (major feature groups\nserved by software components) and infrastructure services such as networks,\npower, cooling, and Internet access. An enterprise IT organization might iden-\ntify the major applications provided to the company (e.g., email, ﬁle storage,\ncentralized compute farm, desktop/laptop ﬂeet management), plus infrastructure\nservices such as DNS, DHCP, ActiveDirectory/LDAP, NTP, and so on. Smaller\nsites may group services together (DNS, DHCP, and ActiveDirectory/LDAP might\nbe “name services”), and larger sites may consider each individual component its\nown service. Either way, construct your list of major services.\nFor each service, assess the service’s eight operational responsibilities. Each\nsection in Appendix A lists questions that will help in this assessment. These\nquestions are generic and should apply to most services. You may wish to add\nadditional questions that are appropriate for your organization or for a particular\nservice. It is important to use the same questions for each assessment so that the\nnumbers are comparable. Make a reference document that lists which questions\nare used.\n20.4.2 Assessing Each Service\nDuring each assessment period, record the assessment number (1 through 5) along\nwith notes that justify the assessment. Generally these notes are in the form of\nanswers to the questions.\nFigure 20.1 shows an example spreadsheet that could be used to track the\nassessment of a service. The ﬁrst column lists the eight operational responsibilities.\nThe other columns each represent an assessment period. Use a different sub-sheet\nfor each service. Use the spreadsheet’s “insert comment” feature to record notes\nthat justify the assessment value.\nIt is a good idea to list the responsibilities in an order that indicates their impor-\ntance to that service. For example, a service that does not grow frequently is less\nconcerned with capacity planning, so that responsibility might be listed last.\nColor the squares red, orange, yellow, green, and blue for the values\n1 through 5, respectively. This gives a visual indication and creates a “heat map”\nshowing progress over time as the colors change.\nFor the person responsible for a service, this tool is a good way to help evaluate\nthe service as well as track progress.\n\n\n20.4\nService Assessments\n409\n.\nFigure 20.1: Assessment of a service\n.\nFigure 20.2: Roll-up to assess a team\n20.4.3 Comparing Results across Services\nFigure 20.2 shows an example spreadsheet being used to roll up the assessments\nto compare the services. The services should be listed by order of importance.\nEach number represents all eight assessments for that service. The number may\n\n\n410\nChapter 20\nOperational Excellence\nbe a weighted average or simply the mathematical “mode” (the most common\nnumber). Whatever method you use, be consistent.\nBoth of these spreadsheets should be ﬁlled out by the team as a group exercise.\nThe manager’s role is to hold everyone to high standards for accuracy. The man-\nager should not do this assessment on his or her own and surprise the team with\nthe result. A self-assessment has an inherent motivational factor; being graded by\na manager is demotivating at best.\n20.4.4 Acting on the Results\nWe can now determine which services need the most improvement and, within\neach service, identify the problem areas.\nDetermine which projects will ﬁx the areas that are both low assessment and\nfor which improvements would have a large impact. That is, focus on the most\nneeded improvements for the most important services.\nLevel 3 is often good enough for most services. Unless it is directly revenue\ngenerating or has highly demanding requirements, generally one should achieve\n“a solid 3” across most responsibilities before expending effort on projects that\nwould result in achieving Level 4 or 5.\nWork on the selected projects during the next quarter. At the start of the\nnext quarter, repeat the assessment, update the spreadsheet, and repeat the pro-\ncess.\n20.4.5 Assessment and Project Planning Frequencies\nThe frequencies of assessments and project planning cycles do not have to be the\nsame. Plan for monthly assessments and quarterly project cycles. Monthly assess-\nment is a good frequency to track progress. More-frequent assessments become a\nwaste of time if very little measurable progress will have happened in that interval.\nLess-frequent assessments may make it difﬁcult to spot new problems quickly.\nThe project cycle may be quarterly to sync up with performance review cycles\nor simply because three months is a reasonable amount of time for a project to be\ncompleted and show results. If the project selection occurs too infrequently, it may\nslow the cadence of change. If the project cycle is too frequent, it may discourage\nlarge, meaningful projects.\nThe longer the gap between assessments, the larger a burden they become. If\nthe cycle is monthly, a meeting that lasts an hour or two can generally complete the\nassessment. A yearly cycle makes assessment “a big deal” and there is a temptation\nto stop all work for a week to prepare for day-long assessment meetings that ana-\nlyze every aspect of the team. Such annual assessments become scary, unhelpful,\nand boring, and they usually turn into a bureaucratic waste of time. This is another\ncase of small batches being better.\n\n\n20.5\nOrganizational Assessments\n411\n20.5 Organizational Assessments\nAs an executive or manager responsible for many services, this assessment system\nis a good way to guide the team’s direction and track progress. These assessments\nalso clearly communicate expectations to team members.\nThe people responsible for the service should do the assessments as a group.\nWhen people are allowed to do their own assessments, it motivates them to take on\nrelated improvement projects. You will be surprised at how often someone, seeing\na low assessment, jumps at the chance to start a project that ﬁxes related problems,\neven before you suggest it.\nYour role as manager is to hold the team to high standards for accuracy and\nconsistency as they complete the assessment. The rubric used should be consistent\nacross all services and across all teams. If you use the assessment questions and\nlook-for’s in Appendix A, revise them to be better suited to your organization.\nLarger organizations have many operational teams. Use this assessment tool\nwith all teams to spot problem areas needing resources and attention, track\nprogress, and motivate healthy competition. Senior managers responsible for\nmultiple teams should work together to assure that rubrics are used consistently.\nFigure 20.3 shows an example spreadsheet that could be used to roll up the\nassessments of many teams so they can be compared. Notice that teams 1, 4, and 5\nhave consistently improved over time; team 4’s progress has been slower. Team 3\nhad a bad year and is just starting to make progress. This team may need extra\nattention to assure the progress continues. Team 2’s progress is erratic, ﬂipping\nbetween Levels 3 and 4 with a small trend upward.\nFigure 20.3: Roll-up to compare teams\n\n\n412\nChapter 20\nOperational Excellence\nHigh-performing employees will see such comparisons as a source of pride\nor inspiration to do better. Teams with high assessments should be encouraged to\nshare their knowledge and best practices.\n20.6 Levels of Improvement\nEach CMM level builds on the next. Level 2 contains the seeds that enable Level 3;\nLevel 3 contains the seeds that enable Level 4. For this reason organizations pass\nthrough the levels in order and levels cannot be skipped.\nIt is not important to get to Level 5 for all operational responsibilities of all\nservices. It would be a waste of resources to achieve Level 5 for a service that is little\nused and has low priority. In fact, it would be professionally negligent to expend\nthe resources required to achieve a Level 5 assessment on a low-priority service\nwhen more important services need improvement.\nLevel 3 assessment is designed to be good enough for most services. Going\nabove that level should be reserved for high-priority services such as revenue-\ngenerating services or services that are particularly demanding.\nWhen setting organizational goals, focus on ﬁxing a problem, rather than\nachieving a particular assessment level. That is, never set a goal of improving the\nassessment of an operational responsibility for the sake of improving the assess-\nment. That is putting the cart before the horse. Instead, identify a problem, engineer\na solution, and measure the success or failure by whether the assessment improves.\nThis is a subtle but important difference.\nSetting the goal of an improved assessment drives the wrong behavior. It\nencourages people to create spot ﬁxes that do not solve the larger problem or to\nﬁx unimportant-but-easy issues just to get a better assessment. It would be like\npaying teachers based on the grades their students receive; such a system would\nsimply lead to all students receiving A+’s as exams become easier and easier.\nFor this reason it is equally wrong to set a goal of raising the assessment level\nof all services to a certain level. An engineer’s time is scarce. A push to have a\nhigh assessment for every organizational responsibility results in expending those\nscarce resources on low-priority services in the name of raising an average. Such\na pursuit creates bureaucratic handcuffs that strangle an organization. It is some-\nthing you should never do; indeed, it is a trap that you should only hope your\ncompetition falls into.\nThis point cannot be overstated. If your management ever sets a corporate,\ndivision, or organizational goal of achieving a certain assessment level on all orga-\nnizational responsibilities of all services, you are to show them this section of the\nbook and tell them that such a plan is a recipe for disaster.\nLikewise, one should never make raises and bonuses contingent on the results\nof an assessment. This discourages people from joining the projects that need the\n\n\n20.7\nGetting Started\n413\nmost help. Instead, encourage your best people to join projects where they can\ndo the most good. Engineers are highly motivated by the opportunity to do good\nwork. The best reward for an improved assessment is not money, but the oppor-\ntunity to work on the most interesting project, or to receive notoriety by sharing\ntheir knowledge with other teams. Rewards based on rising assessment levels are\nalso not recommended because often it is a major achievement to simply retain a\nparticular level in the face of challenging times.\n20.7 Getting Started\nIntroducing this assessment system to a team can be a challenge. Most teams are\nnot used to working in such a data-driven assessment environment. It should be\nintroduced and used as a self-improvement tool—a way to help teams aspire to do\ntheir very best.\nTo begin, the team should enumerate the major services it provides. For exam-\nple, a team that is responsible for a large web site might determine that each web\nproperty is a service, each internal API is a service, and the common platform used\nto provide the services is a service. There may be multiple platforms, in which case\neach is considered a service. Finally, the infrastructure itself is often counted as a\nservice as far as assessment is concerned.\nAssessments should be done on a periodic and repeatable schedule. The\nﬁrst Monday of each month is a common choice for frequency. The team meets\nand conducts a self-assessment of each service. Management’s role is to maintain\nhigh standards and to ensure consistency across the services and teams. Manage-\nment may set global standards for how certain ORs are evaluated. For example,\nthere may be a corporate change management policy; compliance with that policy\nshould be evaluated the same way for all services by all teams.\nEight core ORs should be used to assess all services. Appendix A includes\ndetails about these eight operational responsibilities, along with questions to ask\nduring assessment to aid the team’s understanding of the OR. The questions are\nfollowed by look-for’s describing behaviors typically seen at various levels. Look-\nfor’s are indicators that the service is operating at a particular level. They are not\nchecklists of behaviors to emulate. Do not attempt to achieve every look-for. They\nare indicators, not requirements or checklists. Not every look-for is appropriate for\nevery service.\nIn addition to the eight core ORs, Appendix A lists other ORs that are particu-\nlar to speciﬁc services. They are optional and serve as examples of service-speciﬁc\nORs that an organization may choose to add to the core eight. Organizations may\nalso choose to invent ORs for their own special needs.\nAs the monthly assessments progress, the changes over time should be appar-\nent. The results of the assessments will help teams determine project priorities.\n\n\n414\nChapter 20\nOperational Excellence\nOver time, the roll-ups described in this chapter can be used to compare services\nor teams.\n20.8 Summary\nIn this chapter we discussed how to assess the quality of operations and how to\nuse this assessment to drive improvements.\nMeasuring the quality of system administration is complex. Therefore for each\nservice we assess eight different qualities, called operational responsibilities: reg-\nular tasks (how normal, nonemergency tasks are performed), emergency response\n(how outages and other emergencies are handled), monitoring and metrics (collect-\ning data used to make decisions), capacity planning (determining future resource\nneeds), change management (how services are purposefully changed from birth\nto end), new service introduction and removal (how new products, hardware, or\nservices are introduced into the environment and how they are removed), service\ndeployment and decommissioning (how instances of an existing service are cre-\nated and decommissioned), and performance and efﬁciency (how cost-effectively\nresources are used).\nEach operational responsibility is assessed as being at one of ﬁve levels, reﬂect-\ning the Capability Maturity Model (CMM) levels used in software engineering. The\nCMM is a set of maturity levels for assessing processes: Initial (ad hoc), Repeatable\n(documented, automated), Deﬁned (roles and responsibilities are agreed upon),\nManaged (decisions are data-driven), and Optimizing (improvements are made\nand the results measured). The ﬁrst three levels of the CMM are the most important;\nthe other levels are often attempted only for high-value services.\nThe eight responsibilities of a service are individually assessed. These num-\nbers are rolled up to create a single assessment of the service. Teams are typically\nresponsible for many services. The individual service assessments are rolled up to\nassess the team, usually via a weighted average since not all services are equally\nimportant. These assessments can be used to rank teams.\nBy doing assessments periodically, progress can be tracked on the service,\nteam, or organization level. High-ranking teams should be encouraged to share\ntheir best practices so others may adopt them.\nThe goal is to make improvements and measure their effectiveness by seeing\nif the assessment changes. The goal should not be to improve the assessment or to\nachieve an average assessment across a set of services.\nUsing assessments to drive decisions in IT brings us closer to a system of\nscientiﬁc management for system administration and moves us away from “gut\nfeelings” and intuition. The importance of the ﬁrst three levels of the CMM is\nthat they take us away from ad hoc processes and individual heroics and create\nrepeatable processes that are more efﬁcient and of higher quality.\n\n\nExercises\n415\nExercises\n1. Why is it difﬁcult to assess the quality of operations work, or to compare two\ndifferent operations teams?\n2. Describe the ﬁve Capability Maturity Model (CMM) levels.\n3. Rank the eight “operational responsibilities” from most important to least\nimportant. Justify your ranking.\n4. In what ways could assessments become too bureaucratic and how might one\nprevent this?\n5. Why is the mathematical mode recommended when rolling up a service’s\nassessments, and a weighted average used when rolling up a team’s assess-\nments?\n6. In which other ways could one roll up assessments? Discuss their pros\nand cons.\n7. Select a service you are responsible for. Assess it based on the CMM levels for\neach of the operational responsibilities.\n8. Perform an assessment on a service you are not responsible for but for which\nyou are familiar with its IT staff. Interview those personnel to assess the service\nbased on the CMM levels for each of the operational responsibilities.\n9. Compare your experience doing Exercises 7 and 8.\n10. The eight operational responsibilities may not be appropriate for all services\nin all companies. Which modiﬁcations, additions, or subtractions would you\npropose for a service you are involved in? Justify your answer.\n11. There are disadvantages to repeating the assessment cycle too frequently or\nnot frequently enough. What would be the advantages and disadvantages of\nusing a weekly cycle? A yearly cycle?\n12. This chapter advises against setting goals that specify achieving a particu-\nlar CMM level. Relate the reason given to a personal experience (inside or\noutside of IT).\n\n\nThis page intentionally left blank \n",
      "page_number": 430
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 447-454)",
      "start_page": 447,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "Epilogue\nWe look for things.\nThings that make us go.\n—Pakled Captain Grebnedlog\nWe want a better world.\nWe want a world where food is safer and tastes better.\nWe want a world where deliveries happen on time.\nWe want a world where visiting the Department of Motor Vehicles is a fast\nand efﬁcient process.\nWe want a world where relationships are stronger, more meaningful, and more\nloving.\nWe want a world where we are happier in our jobs.\nWe want a world where the gloriously fanciful ideas of visionary science\nﬁction become the real-life products and experiences available to everyone.\nWe want a world without war, hunger, poverty, and hate.\nWe want a world where optimism, science, and truth win out over pessimism,\nignorance, and lies.\nWe want a world where everyone works together for the betterment of all.\nWe, the authors of this book, were born when computers could not ﬁt on a\ndesktop, let alone in a pocket. As youths, we saw the rise of computers and felt\ndeep in our hearts that technology would save the world. As adults, we’ve seen\ntechnology transform the world, make it smaller, and enable us to do things that\nour parents would never have dreamed of.\nNow we realize that computers and software are just a small piece of the pic-\nture. To make the world a better place requires operational practices that bring all\nthe parts together, that bring all the people together, to run it and maintain it and\nkeep it going.\n417\n\n\n418\nEpilogue\nPeople say that software “eats” one industry at a time, disrupting and\nrevolutionizing it. When that happens, it is the operational practices that determine\nsuccess or failure.\nThe logistics of producing and distributing food is now a software function.\nFrom the farm to the dinner table, better software makes it possible to grow more\nwith fewer resources, harvest it at the right time, and transport it. Because the oper-\national practices are successful, we eat a wider variety of food that is fresher and\nless expensive than ever before.\nWhat looks like the simple act of buying something online requires a chain\nof organizations working together: raw material suppliers, factories, distribution,\nsales, marketing, purchasing, logistics, and delivery. Each of these is embodied in\nsoftware and works well, or not, because of the operational practices involved.\nThe creation of new products is accelerated by the improved cycle time that\nbrings new ideas and technologies to market faster. New ideas breed new ideas.\nWho would have imagined that someday we’d use our phone to run apps that\nenable us to deposit checks by taking their picture, reserve a car by pressing a\nbutton, or toss disgruntled birds at unstable structures?\nWhen operations is done right, we are happier in our jobs. We eliminate the\nterror and uncertainty of major upgrades. We end the practice of requiring certain\ntasks to be done at odd hours that steal our sleep and distract us from pleasure.\nThe stress from our jobs that hurts our nonwork relationships goes away. We live\nhappier lives. We have more time for living and loving. We have more freedom to\nuse our time to help others. Happiness multiplies love and it overﬂows us, leading\nus to share it with others.\nThese are the early years of all this stuff whose names and deﬁnitions are\nstill evolving: cloud computing, distributed computing, DevOps, SRE, the web,\nthe internet of things. We are standing at the base of the mountain, looking up,\nwondering what the future holds.\nIf you follow every bit of advice in this book, it will not cure all the world’s\nills. It will not end poverty or make food taste better. The advice in this book is\nobsolete as we write it.\nBut it is a start.\nWe hope this book gave you good things to think about. Interesting ideas to\ntry. A starting place. We are merely collectors of the good ideas we’ve learned from\nothers, read and heard about, and often been lucky enough to have experienced. We\nare scribes. We hope we have translated these ideas and experiences into words that\nconvey their essence without misrepresenting them or leaving out the important\nbits. Where we haven’t, we apologize in advance.\nThese are the early years. This book is just one voice. The rest is up to you.\nTake what you’ve learned here and build a better world.\n\n\nPart III\nAppendices\n\n\nThis page intentionally left blank \n\n\nAppendix A\nAssessments\nThis\nappendix\ncontains\nassessment\nquestions\nand\nlook-for’s\nfor\nvarious\noperational responsibilities (ORs). Chapter 20, “Operational Excellence,” is the\ninstruction manual for this appendix. Advice on getting started is in Section 20.7.\nAssessment Levels\n.\nLevel 1\nInitial/Chaotic\nAd hoc and relying on individual heroics\nLevel 2\nRepeatable\nRepeatable results\nLevel 3\nDeﬁned\nResponsibilities deﬁned/conﬁrmed\nLevel 4\nManaged\nQuantitatively managed metrics\nLevel 5\nOptimizing\nDeliberate optimization/improvement\nCore Operational Responsibilities\n.\nPage\nOperational Responsibility\nChapters\n423\nRegular Tasks (RT)\n12, 14\n426\nEmergency Response (ER)\n6, 14, 15\n428\nMonitoring and Metrics (MM)\n16, 17, 19\n431\nCapacity Planning (CP)\n18\n433\nChange Management (CM)\n435\nNew Product Introduction and Removal (NPI/NPR)\n437\nService Deployment and Decommissioning (SDD)\n439\nPerformance and Efﬁciency (PE)\n421\n\n\n422\nAppendix A\nAssessments\nAdditional Operational Responsibilities\n.\nPage\nOperational Responsibility\nChapters\n442\nService Delivery: The Build Phase\n9\n444\nService Delivery: The Deployment Phase\n10, 11\n446\nToil Reduction\n12\n448\nDisaster Preparedness\n15\n\n\nA.1\nRegular Tasks (RT)\n423\nA.1 Regular Tasks (RT)\nRegular Tasks include how normal, non-emergency, operational duties are\nhandled—that is, how work is received, queued, distributed, processed, and veri-\nﬁed, plus how periodic tasks are scheduled and performed. All services have some\nkind of normal, scheduled or unscheduled work that needs to be done. Often web\noperations teams do not perform direct customer support but there are interteam\nrequests, requests from stakeholders, and escalations from direct customer support\nteams. These topics are covered in Chapters 12 and 14.\nSample Assessment Questions\n• What are the common and periodic operational tasks and duties?\n• Is there a playbook for common operational duties?\n• What is the SLA for regular requests?\n• How is the need for new playbook entries identiﬁed? Who may write new\nentries? Edit existing ones?\n• How are requests from users received and tracked?\n• Is there a playbook for common user requests?\n• How often are user requests not covered by the playbook?\n• How do users engage us for support? (online and physical locations)\n• How do users know how to engage us for support?\n• How do users know what is supported and what isn’t?\n• How do we respond to requests for support of the unsupported?\n• What are the limits of regular support (hours of operation, remote or on-site)?\nHow do users know these limits?\n• Are different size categories handled differently? How is size determined?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• There is no playbook, or it is out of date and unused.\n• Results are inconsistent.\n• Different people do tasks differently.\n• Two users requesting the same thing usually get different results.\n• Processes aren’t documented.\n• The team can’t enumerate all the processes a team does (even at a high level).\n• Requests get lost or stalled indeﬁnitely.\n\n\n424\nAppendix A\nAssessments\n• The organization cannot predict how long common tasks will take to\ncomplete.\n• Operational problems, if reported, don’t get attention.\nLevel 2: Repeatable\n• There is a ﬁnite list of which services are supported by the team.\n• Each end-to-end process has each step enumerated, with dependencies.\n• Each end-to-end process has each step’s process documented.\n• Different people do the tasks the same way.\n• Sadly, there is some duplication of effort seen in the ﬂow.\n• Sadly, some information needed by multiple tasks may be re-created by each\nstep that needs it.\nLevel 3: Defined\n• The team has an SLA deﬁned for most requests, though it may not be\nadhered to.\n• Each step has a QA checklist to be completed before handing off to next step.\n• Teams learn of process changes by other teams ahead of time.\n• Information or processing needed by multiple steps is created once.\n• There is no (or minimal) duplication of effort.\n• The ability to turn up new capacity is a repeatable process.\nLevel 4: Managed\n• The deﬁned SLA is measured.\n• There are feedback mechanisms for all steps.\n• There is periodic (weekly?) review of defects and reworks.\n• Postmortems are published for all to see, with a draft report available within\nx hours and a ﬁnal report completed within y days.\n• There is periodic review of alerts by the affected team. There is periodic review\nof alerts by a cross-functional team.\n• Process change requests require data to measure the problem being ﬁxed.\n• Dashboards report data in business terms (i.e., not just technical terms).\n• Every “failover procedure” has a “date of last use” dashboard.\n• Capacity needs are predicted ahead of need.\n",
      "page_number": 447
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 455-463)",
      "start_page": 455,
      "end_page": 463,
      "detection_method": "topic_boundary",
      "content": "A.1\nRegular Tasks (RT)\n425\nLevel 5: Optimizing\n• After process changes are made, before/after data are compared to determine\nsuccess.\n• Process changes are reverted if before/after data shows no improvement.\n• Process changes that have been acted on come from a variety of sources.\n• At least one process change has come from every step (in recent history).\n• Cycle time enjoys month-over-month improvements.\n• Decisions are supported by modeling “what if” scenarios using extracted\nactual data.\n\n\n426\nAppendix A\nAssessments\nA.2 Emergency Response (ER)\nEmergency Response covers how outages and disasters are handled. This includes\nengineering resilient systems that prevent outages plus technical and non-technical\nprocesses performed during and after outages (response and remediation). These\ntopics are covered in Chapters 6, 14, and 15.\nSample Assessment Questions\n• How are outages detected? (automatic monitoring? user complaints?)\n• Is there a playbook for common failover scenarios and outage-related duties?\n• Is there an oncall calendar?\n• How is the oncall calendar created?\n• Can the system withstand failures on the local level (component failure)?\n• Can the system withstand failures on the geographic level (alternative data-\ncenters)?\n• Are staff geographically distributed (i.e., can other regions cover for each other\nfor extended periods of time)?\n• Do you write postmortems? Is there a deadline for when a postmortem must\nbe completed?\n• Is there a standard template for postmortems?\n• Are postmortems reviewed to assure action items are completed?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Outages are reported by users rather than a monitoring system.\n• No one is ever oncall, a single person is always oncall, or everyone is always\noncall.\n• There is no oncall schedule.\n• There is no oncall calendar.\n• There is no playbook of what to do for various alerts.\nLevel 2: Repeatable\n• A monitoring system contacts the oncall person.\n• There is an oncall schedule with escalation plan.\n• There is a repeatable process for creating the next month’s oncall calendar.\n• A playbook item exists for any possible alert.\n• A postmortem template exists.\n\n\nA.2\nEmergency Response (ER)\n427\n• Postmortems are written occasionally but not consistently.\n• Oncall coverage is geographically diverse (multiple time zones).\nLevel 3: Defined\n• Outages are classiﬁed by size (i.e., minor, major, catastrophic).\n• Limits (and minimums) for how often people should be oncall are deﬁned.\n• Postmortems are written for all major outages.\n• There is an SLA deﬁned for alert response: initial, hands-on-keyboard, issue\nresolved, postmortem complete.\nLevel 4: Managed\n• The oncall pain is shared by the people most able to ﬁx problems.\n• How often people are oncall is veriﬁed against the policy.\n• Postmortems are reviewed.\n• There is a mechanism to triage recommendations in postmortems and assure\nthey are completed.\n• The SLA is actively measured.\nLevel 5: Optimizing\n• Stress testing and failover testing are done frequently (quarterly or monthly).\n• “Game Day” exercises (intensive, system-wide tests) are done periodically.\n• The monitoring system alerts before outages occur (indications of “sick”\nsystems rather than “down” systems).\n• Mechanisms exist so that any failover procedure not utilized in recent history\nis activated artiﬁcially.\n• Experiments are performed to improve SLA compliance.\n\n\n428\nAppendix A\nAssessments\nA.3 Monitoring and Metrics (MM)\nMonitoring and Metrics covers collecting and using data to make decisions. Moni-\ntoring collects data about a system. Metrics uses that data to measure a quantiﬁable\ncomponent of performance. This includes technical metrics such as bandwidth,\nspeed, or latency; derived metrics such as ratios, sums, averages, and percentiles;\nand business goals such as the efﬁcient use of resources or compliance with\na service level agreement (SLA). These topics are covered in Chapters 16, 17,\nand 19.\nSample Assessment Questions\n• Is the service level objective (SLO) documented? How do you know your SLO\nmatches customer needs?\n• Do you have a dashboard? Is it in technical or business terms?\n• How accurate are the collected data and the predictions? How do you know?\n• How efﬁcient is the service? Are machines over- or under-utilized? How is\nutilization measured?\n• How is latency measured?\n• How is availability measured?\n• How do you know if the monitoring system itself is down?\n• How do you know if the data used to calculate key performance indicators\n(KPIs) is fresh? Is there a dashboard that shows measurement freshness and\naccuracy?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• No SLOs are documented.\n• If there is monitoring, not everything is monitored, and there is no way to\ncheck completeness.\n• Systems and services are manually added to the monitoring system, if at all:\nthere is no process.\n• There are no dashboards.\n• Little or no measurement or metrics.\n• You think customers are happy but they aren’t.\n• It is common (and rewarded) to enact optimizations that beneﬁt a person or\nsmall group to the detriment of the larger organization or system.\n• Departmental goals emphasize departmental performance to the detriment of\norganizational performance.\n\n\nA.3\nMonitoring and Metrics (MM)\n429\nLevel 2: Repeatable\n• The process for creating machines/server instances assures they will be\nmonitored.\nLevel 3: Defined\n• SLOs are documented.\n• Business KPIs are deﬁned.\n• The freshness of business KPI data is deﬁned.\n• A system exists to verify that all services are monitored.\n• The monitoring system itself is monitored (meta-monitoring).\nLevel 4: Managed\n• SLOs are documented and monitored.\n• Deﬁned KPIs are measured.\n• Dashboards exist showing each step’s completion time; the lag time of each\nstep is identiﬁed.\n• Dashboards exist showing current bottlenecks, backlogs, and idle steps.\n• Dashboards show defect and rework counts.\n• Capacity planning is performed for the monitoring system and all analysis\nsystems.\n• The freshness of the data used to calculate KPIs is measured.\nLevel 5: Optimizing\n• The accuracy of collected data is veriﬁed through active testing.\n• KPIs are calculated using data that is less than a minute old.\n• Dashboards and other analysis displays are based on fresh data.\n• Dashboards and other displays load quickly.\n• Capacity planning for storage, CPU, and network of the monitoring system is\ndone with the same sophistication as any major service.\n.\nThe Unexpectedly Slow Cache\nStack Exchange purchased a product that would accelerate web page delivery\nto customers using a globally distributed cache. Most customers deploy this\nproduct and assume it has a “can’t lose” beneﬁt.\n\n\n430\nAppendix A\nAssessments\n.\nBefore deploying it, Stack Exchange engineer Nick Craver created a\nframework for measuring end-to-end page load times. The goal was to pre-\ncisely know how much improvement was gained both globally and for\ncustomers in various geographic regions.\nWe were quite surprised to discover that the product degraded per-\nformance. It improved certain aspects but only at the detriment of others,\nresulting in a net performance loss.\nStack Exchange worked with the vendor to identify the problem. As a\nresult, a major design error was found and ﬁxed.\nIf care hadn’t been taken to measure performance before and after the\nchange, Stack Exchange’s efforts would have unknowingly made its service\nslower. One wonders how many other customers of this product did no\nsuch measurements and simply assumed performance was improved while\nin reality it was made worse.\n\n\nA.4\nCapacity Planning (CP)\n431\nA.4 Capacity Planning (CP)\nCapacity Planning covers determining future resource needs. All services require\nsome kind of planning for future resources. Services tend to grow. Capacity plan-\nning involves the technical work of understanding how many resources are needed\nper unit of growth, plus non-technical aspects such as budgeting, forecasting, and\nsupply chain management. These topics are covered in Chapter 18.\nSample Assessment Questions\n• How much capacity do you have now?\n• How much capacity do you expect to need three months from now? Twelve\nmonths from now?\n• Which statistical models do you use for determining future needs?\n• How do you load-test?\n• How much time does capacity planning take? What could be done to make it\neasier?\n• Are metrics collected automatically?\n• Are metrics available always or does their need initiate a process that collects\nthem?\n• Is capacity planning the job of no one, everyone, a speciﬁc person, or a team\nof capacity planners?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• No inventory is kept.\n• The system runs out of capacity from time to time.\n• Determining how much capacity to add is done by tradition, guessing, or luck.\n• Operations is reactive about capacity planning, often not being able to fulﬁll\nthe demand for capacity in time.\n• Capacity planning is everyone’s job, and therefore no one’s job.\n• No one is speciﬁcally assigned to handle CP duties.\n• A large amount of headroom exists rather than knowing precisely how much\nslack is needed.\nLevel 2: Repeatable\n• CP metrics are collected on demand, or only when needed.\n• The process for collecting CP metrics is written and repeatable.\n\n\n432\nAppendix A\nAssessments\n• Load testing is done occasionally, perhaps when a service is new.\n• Inventory of all systems is accurate, possibly due to manual effort.\nLevel 3: Defined\n• CP metrics are automatically collected.\n• Capacity required for a certain amount of growth is well deﬁned.\n• There is a dedicated CP person on the team.\n• CP requirements are deﬁned at a subsystem level.\n• Load testing is triggered by major software and hardware changes.\n• Inventory is updated as part of capacity changes.\n• The amount of headroom needed to survive typical surges is deﬁned.\nLevel 4: Managed\n• CP metrics are collected continuously (daily/weekly instead of monthly or\nquarterly).\n• Additional capacity is gained automatically, with human approval.\n• Performance regressions are detected during testing, involving CP if perfor-\nmance regression will survive into production (i.e., it is not a bug).\n• Dashboards include CP information.\n• Changes in correlation are automatically detected and raise a ticket for CP to\nverify and adjust relationships between core drivers and resource units.\n• Unexpected increases in demand are automatically detected using MACD\nmetrics or similar technique, which generates a ticket for the CP person or\nteam.\n• The amount of headroom in the system is monitored.\nLevel 5: Optimizing\n• Past CP projections are compared with actual results.\n• Load testing is done as part of a continuous test environment.\n• The team employs a statistician.\n• Additional capacity is gained automatically.\n• The amount of headroom is systematically optimized to reduce waste.\n\n\nA.5\nChange Management (CM)\n433\nA.5 Change Management (CM)\nChange Management covers how services are deliberately changed over time.\nThis includes the software delivery platform—the steps involved in a software\nrelease: develop, build, test, and push into production. For hardware, this includes\nﬁrmware upgrades and minor hardware revisions. These topics are covered in\nChapters 9, 10, and 11.\nSample Assessment Questions\n• How often are deployments (releases pushed into production)?\n• How much human labor does it take?\n• When a release is received, does the operations team need to change anything\nin it before it is pushed?\n• How does operations know if a release is major or minor, a big or small\nchange? How are these types of releases handled differently?\n• How does operations know if a release is successful?\n• How often have releases failed?\n• How does operations know that new releases are available?\n• Are there change-freeze windows?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Deployments are done sparingly, as they are very risky.\n• The deployment process is ad hoc and laborious.\n• Developers notify operations of new releases when a release is ready for\ndeployment.\n• Releases are not deployed until weeks or months after they are available.\n• Operations and developers bicker over when to deploy releases.\nLevel 2: Repeatable\n• The deployment is no longer ad hoc.\n• Deployment is manual but consistent.\n• Releases are deployed as delivered.\n• Deployments fail often.\n",
      "page_number": 455
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 464-471)",
      "start_page": 464,
      "end_page": 471,
      "detection_method": "topic_boundary",
      "content": "434\nAppendix A\nAssessments\nLevel 3: Defined\n• What constitutes a successful deployment is deﬁned.\n• Minor and major releases are handled differently.\n• The expected time gap between release availability and deployment is\ndeﬁned.\nLevel 4: Managed\n• Deployment success/failure is measured against deﬁnitions.\n• Deployments fail rarely.\n• The expected time gap between release availability and deployment is\nmeasured.\nLevel 5: Optimizing\n• Continuous deployment is in use.\n• Failed deployments are extremely rare.\n• New releases are deployed with little delay.\n\n\nA.6\nNew Product Introduction and Removal (NPI/NPR)\n435\nA.6 New Product Introduction and Removal\n(NPI/NPR)\nNew Product Introduction and Removal covers how new products and services are\nintroduced into the environment and how they are removed. This is a coordination\nfunction: introducing a new product or service requires a support infrastructure\nthat may touch multiple teams.\nFor example, before a new model of computer hardware is introduced into\nthe datacenter environment, certain teams must have access to sample hardware\nfor testing and qualiﬁcation, the purchasing department must have a process\nto purchase the machines, and datacenter technicians need documentation. For\nintroducing software and services, there should be tasks such as requirements\ngathering, evaluation and procurement, licensing, and creation of playbooks for\nthe helpdesk and operations.\nProduct removal might involve ﬁnding all machines with a particularly old\nrelease of an operating system and seeing that all of them get upgraded. Prod-\nuct removal requires identifying current users, agreeing on timelines for migrating\nthem away, updating documentation, and eventually decommissioning the prod-\nuct, any associated licenses, maintenance contracts, monitoring, and playbooks.\nThe majority of the work consists of communication and coordination between\nteams.\nSample Assessment Questions\n• How is new hardware introduced into the environment? Which teams are\ninvolved and how do they communicate? How long does the process take?\n• How is old hardware or software eliminated from the system?\n• What is the process for disposing of old hardware?\n• Which steps are taken to ensure disks and other storage are erased when\ndisposed?\n• How is new software or a new service brought into being? Which teams are\ninvolved and how do they communicate? How long does the process take?\n• What is the process for handoff between teams?\n• Which tools are used?\n• Is documentation current?\n• Which steps involve human interaction? How could it be eliminated?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\n\n\n436\nAppendix A\nAssessments\nLevel 1: Initial\n• New products are introduced through ad hoc measures and individual\nheroics.\n• Teams are surprised by NPI, often learning they must deploy something into\nproduction with little notice.\n• NPI is delayed due to lack of capacity, miscommunication, or errors.\n• Deprecating old products is rarely done, resulting in operations having to\nsupport an “inﬁnite” number of hardware or software versions.\nLevel 2: Repeatable\n• The process used for NPI/NPR is repeatable.\n• The handoff between teams is written and agreed upon.\n• Each team has a playbook for tasks related to its involvement with NPR/NPI.\n• Equipment erasure and disposal is documented and veriﬁed.\nLevel 3: Defined\n• Expectations for how long NPI/NPR will take are deﬁned.\n• The handoff between teams is encoded in a machine-readable format.\n• Members of all teams understand their role as it ﬁts into the larger, overall\nprocess.\n• The maximum number of products supported by each team is deﬁned.\n• The list of each team’s currently supported products is available to all teams.\nLevel 4: Managed\n• There are dashboards for observing NPI and NPR progress.\n• The handoff between teams is actively revised and improved.\n• The number of no-longer-supported products is tracked.\n• Decommissioning no-longer-supported products is a high priority.\nLevel 5: Optimizing\n• NPI/NPR tasks have become API calls between teams.\n• NPI/NPR processes are self-service by the team responsible.\n• The handoff between teams is a linear ﬂow (or for very complex systems,\njoining multiple linear ﬂows).\n\n\nA.7\nService Deployment and Decommissioning (SDD)\n437\nA.7 Service Deployment and Decommissioning\n(SDD)\nService Deployment and Decommissioning covers how instances of an existing\nservice are created and how they are turned off (decommissioned). After a service\nis designed, it is usually deployed repeatedly. Deployment may involve turning\nup satellite replicas in new datacenters or creating a development environment of\nan existing service. Decommissioning could be part of turning down a datacenter,\nreducing excess capacity, or turning down a particular service instance such as a\ndemo environment.\nSample Assessment Questions\n• What is the process for turning up a service instance?\n• What is the process for turning down a service instance?\n• How is new capacity added? How is unused capacity turned down?\n• Which steps involve human interaction? How could it be eliminated?\n• How many teams touch these processes?\n• Do all teams know how they ﬁt into the over all picture?\n• What is the workﬂow from team to team?\n• Which tools are used?\n• Is documentation current?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• The process is undocumented and haphazard. Results are inconsistent.\n• The process is deﬁned by who does something, not what is done.\n• Requests get delayed due to miscommunication, lack of resources, or other\navoidable reasons.\n• Different people do the tasks differently.\nLevel 2: Repeatable\n• The processes required to deploy or decommission a service are understood\nand documented.\n• The process for each step is documented and veriﬁed.\n• Each step has a QA checklist to be completed before handing off to the next\nstep.\n• Teams learn of process changes by other teams ahead of time.\n\n\n438\nAppendix A\nAssessments\n• Information or processing needed by multiple steps is created once.\n• There is no (or minimal) duplication of effort.\n• The ability to turn up new capacity is a repeatable process.\n• Equipment erasure and disposal is documented and veriﬁed.\nLevel 3: Defined\n• The SLA for how long each step should take is deﬁned.\n• For physical deployments, standards for removal of waste material (boxes,\nwrappers, containers) are based on local environmental standards.\n• For physical decommissions, standards for disposing of old hardware are\nbased on local environmental standards as well as the organization’s own\nstandards for data erasure.\n• Tools exist to implement many of the steps and processes.\nLevel 4: Managed\n• The deﬁned SLA for each step is measured.\n• There are feedback mechanisms for all steps.\n• There is periodic review of defects and reworks.\n• Capacity needs are predicted ahead of need.\n• Equipment disposal compliance is measured against organization standards\nas well as local environmental law.\n• Waste material (boxes, wrappers, containers) involved in deployment is\nmeasured.\n• Quantity of equipment disposal is measured.\nLevel 5: Optimizing\n• After process changes are made, before/after data is compared to determine\nsuccess.\n• Process changes are reverted if before/after data shows no improvement.\n• Process changes that have been acted on come from a variety of sources.\n• Cycle time enjoys month-over-month improvements.\n• Decisions are supported by modeling “what if” scenarios using extracts from\nactual data.\n• Equipment disposal is optimized by the reduction of equipment deployment.\n\n\nA.8\nPerformance and Efﬁciency (PE)\n439\nA.8 Performance and Efficiency (PE)\nPerformance and Efﬁciency covers how cost-effectively resources are used and\nhow well the service performs. A running service needs to have good performance\nwithout wasting resources. We can generally improve performance by using more\nresources, or we may be able to improve efﬁciency to the detriment of performance.\nAchieving both requires a large effort to bring about equilibrium. Cost-efﬁciency\nis cost of resources divided by quantity of use. Resource efﬁciency is quantity of\nresources divided by quantity of use. To calculate these statistics, one must know\nhow many resources exist; thus some kind of inventory is required.\nSample Assessment Questions\n• What is the formula used to measure performance?\n• What is the formula used to determine utilization?\n• What is the formula used to determine resource efﬁciency?\n• What is the formula used to determine cost efﬁciency?\n• How is performance variation measured?\n• Are performance, utilization, and resource efﬁciency monitored automati-\ncally? Is there a dashboard for each?\n• Is there an inventory of the machines and servers used in this service?\n• How is the inventory kept up-to-date?\n• How would you know if something was missing from the inventory?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Performance and utilization are not consistently measured.\n• What is measured depends on who set up the systems and services.\n• Resource efﬁciency is not measured.\n• Performance problems often come as a surprise and are hard to diagnose and\nresolve because there is insufﬁcient data.\n• Inventory is not up-to-date.\n• Inventory may or may not be updated, depending on who is involved in\nreceiving or disposing of items.\n\n\n440\nAppendix A\nAssessments\nLevel 2: Repeatable\n• All metrics relevant to performance and utilization are collected across all\nsystems and services.\n• The process for bringing up new systems and services is documented and\neveryone follows the process.\n• Systems are associated with services when conﬁgured for use by a service, and\ndisassociated when released.\n• Inventory is up-to-date. The inventory process is well documented and every-\none follows the process.\nLevel 3: Defined\n• Performance and utilization monitoring is automatically conﬁgured for all\nsystems and services during installation and removed during decommission.\n• Performance targets for each service are deﬁned.\n• Resource usage targets for each service are deﬁned.\n• Formulas for service-oriented performance and utilization metrics are\ndeﬁned.\n• Performance of each service is monitored continuously.\n• Resource utilization of each service is monitored continuously.\n• Idle capacity that is not currently used by any service is monitored.\n• The desired amount of headroom is deﬁned.\n• The roles and responsibilities for keeping the inventory up-to-date are deﬁned.\n• Systems for tracking the devices that are connected to the network and their\nhardware conﬁgurations are in place.\nLevel 4: Managed\n• Dashboards track performance, utilization, and resource efﬁciency.\n• Minimum, maximum, and 90th percentile headroom are tracked and com-\npared to the desired headroom and are visible on a dashboard.\n• Goals for performance and efﬁciency are set and tracked.\n• There are periodic reviews of performance and efﬁciency goals and status for\neach service.\n• KPIs are used to set performance, utilization, and resource efﬁciency goals that\ndrive optimal behavior.\n• Automated systems track the devices that are on the network and their con-\nﬁgurations and compare them with the inventory system, ﬂagging problems\nwhen they are found.\n\n\nA.8\nPerformance and Efﬁciency (PE)\n441\nLevel 5: Optimizing\n• Bottlenecks are identiﬁed using the performance dashboard. Changes are\nmade as a result.\n• Services that use large amounts of resources are identiﬁed and changes are\nmade.\n• Changes are reverted if the changes do not have a positive effect.\n• Computer hardware models are regularly evaluated to ﬁnd models where\nutilization of the different resources is better balanced.\n• Other sources of hardware and other hardware models are regularly evaluated\nto determine if cost efﬁciency can improved.\n",
      "page_number": 464
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 472-480)",
      "start_page": 472,
      "end_page": 480,
      "detection_method": "topic_boundary",
      "content": "442\nAppendix A\nAssessments\nA.9 Service Delivery: The Build Phase\nService delivery is the technical process of how a service is created. It starts with\nsource code created by developers and ends with a service running in production.\nSample Assessment Questions\n• How is software built from source code to packages?\n• Is the ﬁnal package built from source or do developers deliver precompiled\nelements?\n• What percentage of code is covered by unit tests?\n• Which tests are fully automated?\n• Are metrics collected about bug lead time, code lead time, and patch lead time?\n• To build the software, do all raw source ﬁles come from version control\nrepositories?\n• To build the software, how many places (repositories or other sources) are\naccessed to attain all raw source ﬁles?\n• Is the resulting software delivered as a package or a set of ﬁles?\n• Is everything required for deployment delivered in the package?\n• Which package repository is used to hand off the results to the deployment\nphase?\n• Is there a single build console for status and control of all steps?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Each person builds in his or her own environment.\n• People check in code without checking that it builds.\n• Developers deliver precompiled elements to be packaged.\n• Little or no unit testing is performed.\n• No metrics are collected.\n• Version control systems are not used to store source ﬁles.\n• Building the software is a manual process or has manual steps.\n• The master copies of some source ﬁles are kept in personal home directories\nor computers.\nLevel 2: Repeatable\n• The build environment is deﬁned; everyone uses the same system for consis-\ntent results.\n• Building the software is still done manually.\n\n\nA.9\nService Delivery: The Build Phase\n443\n• Testing is done manually.\n• Some unit tests exist.\n• Source ﬁles are kept in version-controlled repositories.\n• Software packages are used as the means of delivering the end result.\n• If multiple platforms are supported, each is repeatable, though possibly\nindependently.\nLevel 3: Defined\n• Building the software is automated.\n• Triggers for automated builds are deﬁned.\n• Expectations around unit test coverage are deﬁned; they are less than\n100 percent.\n• Metrics for bug lead time, code lead time, and patch lead time are deﬁned.\n• Inputs and outputs of each step are deﬁned.\nLevel 4: Managed\n• Success/fail build ratios are measured and tracked on a dashboard.\n• Metrics for bug lead time, code lead time, and patch lead time are collected\nautomatically.\n• Metrics are presented on a dashboard.\n• Unit test coverage is measured and tracked.\nLevel 5: Optimizing\n• Metrics are used to select optimization projects.\n• Attempts to optimize the process involve collecting before and after metrics.\n• Each developer can perform the end-to-end build process in his or her own\nsandbox before committing changes to a centralized repository.\n• Insufﬁcient unit test code coverage stops production.\n• If multiple platforms are supported, building for one is as easy as building for\nthem all.\n• The software delivery platform is used for building infrastructure as well as\napplications.\n\n\n444\nAppendix A\nAssessments\nA.10 Service Delivery: The Deployment Phase\nThe goal of the deployment phase is to create a running environment. The deploy-\nment phase creates the service in one or more testing and production environ-\nments. This environment will then be used for testing or for live production\nservices.\nSample Assessment Questions\n• How are packages deployed in production?\n• How much downtime is required to deploy the service in production?\n• Are metrics collected about frequency of deployment, mean time to restore\nservice, and change success rate?\n• How is the decision made to promote a package from testing to production?\n• Which kind of testing is done (system, performance, load, user acceptance)?\n• How is deployment handled differently for small, medium, and large releases?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply?\nLevel 1: Initial\n• Deployment involves or requires manual steps.\n• Deployments into the testing and production environments are different\nprocesses, each with its own tools and procedures.\n• Different people on the team perform deployments differently.\n• Deployment requires downtime, and sometimes signiﬁcant downtime.\n• How a release is promoted to production is ad hoc or ill deﬁned.\n• Testing is manual, ill deﬁned, or not done.\nLevel 2: Repeatable\n• Deployment is performed in a documented, repeatable process.\n• If deployment requires downtime, it is a predictable.\n• Testing procedures are documented and repeatable.\nLevel 3: Defined\n• Metrics for frequency of deployment, mean time to restore service, and change\nsuccess rate are deﬁned.\n• How downtime due to deployments is to be measured is deﬁned; limits and\nexpectations are deﬁned.\n\n\nA.10\nService Delivery: The Deployment Phase\n445\n• How a release is promoted to production is deﬁned.\n• Testing results are clearly communicated to all stakeholders.\nLevel 4: Managed\n• Metrics for frequency of deployment, mean time to restore service, and change\nsuccess rate are collected automatically.\n• Metrics are presented on a dashboard.\n• Downtime due to deployments is measured automatically.\n• Reduced production capacity during deployment is measured.\n• Tests are fully automated.\nLevel 5: Optimizing\n• Metrics are used to select optimization projects.\n• Attempts to optimize the process involve collecting before and after metrics.\n• Deployment is fully automated.\n• Promotion decisions are fully automated, perhaps with a few speciﬁc excep-\ntions.\n• Deployment requires no downtime.\n\n\n446\nAppendix A\nAssessments\nA.11 Toil Reduction\nToil Reduction is the process by which we improve the use of people within our\nsystem. When we reduce toil (i.e., exhausting physical labor), we create a more\nsustainable working environment for operational staff. While reducing toil is not\na service per se, this OR can be used to assess the amount of toil and determine\nwhether practices are in place to limit the amount of toil.\nSample Assessment Questions\n• How many hours each week are spent on coding versus non-coding projects?\n• What percent of time is spent on project work versus manual labor that could\nbe automated?\n• What percentage of time spent on manual labor should raise a red ﬂag?\n• What is the process for detecting that the percentage of manual labor has\nexceeded the red ﬂag threshold?\n• What is the process for raising a red ﬂag? Whose responsibility is it?\n• What happens after a red ﬂag is raised? When is it lowered?\n• How are projects for reducing toil identiﬁed? How are they prioritized?\n• How is the effectiveness of those projects measured?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Toil is not measured and grows until no project work, or almost no project\nwork, can be accomplished.\n• There is no process for raising a red ﬂag.\n• Some individuals recognize when toil is becoming a problem and look for\nsolutions, but others are unaware of the problem.\n• Individuals choose to work on the projects that are the most interesting to\nthem, without looking at which projects will have the biggest impact.\nLevel 2: Repeatable\n• The amount of time spent on toil versus on projects is measured.\n• The percentage of time spent on toil that constitutes a problem is deﬁned and\ncommunicated.\n• The process for raising a red ﬂag is documented and communicated.\n• Individuals track their own toil to project work ratio, and are individually\nresponsible for raising a red ﬂag.\n• Red ﬂags may not always be raised when they should be.\n\n\nA.11\nToil Reduction\n447\n• The process for identifying which projects will have the greatest impact on toil\nreduction is deﬁned.\n• The method for prioritizing projects is documented.\nLevel 3: Defined\n• For each team, the person responsible for tracking toil and raising a red ﬂag is\nidentiﬁed.\n• The people involved in identifying and prioritizing toil-reduction projects are\nknown.\n• Both a red ﬂag level of toil and a target level are deﬁned. The red ﬂag is lowered\nwhen toil reaches the target level.\n• During the red ﬂag period, the team works on only the highest-impact toil-\nreduction projects.\n• During the red ﬂag period, the team has management support for putting\nother projects on hold until toil is reduced to a target level.\n• After each step in a project, statistics on toil are closely monitored, providing\nfeedback on any positive or negative changes.\nLevel 4: Managed\n• Project time versus toil is tracked on a dashboard, and the amount of time\nspent on each individual project or manual task is also tracked.\n• Red ﬂags are raised automatically, and the dashboard gives an overview of\nwhere the problems lie.\n• The time-tracking data is monitored for trends that give an early alert for teams\nthat are showing an increase in toil in one or more areas.\n• KPIs are deﬁned and tracked to keep toil within the desired range and\nminimize the red ﬂag periods.\nLevel 5: Optimizing\n• The target and red ﬂag levels are adjusted and the results are monitored to the\neffect on overall ﬂow, performance, and innovation.\n• Changes to the main project prioritization process are introduced and evalu-\nated for positive or negative impact, including the impact on toil.\n• Changes to the red ﬂag toil-reduction task prioritization process are intro-\nduced and evaluated.\n\n\n448\nAppendix A\nAssessments\nA.12 Disaster Preparedness\nAn operations organization needs to be able to handle outages well, and it must\nhave practices that reduce the chance of repeating past mistakes. Disasters and\nmajor outages happen. Everyone in the company from the top down needs to rec-\nognize that fact, and adopt a mind-set that accepts outages and learns from them.\nSystems should be designed to be resilient to failure.\nSample Assessment Questions\n• What is the SLA? Which tools and processes are in place to ensure that the\nSLA is met?\n• How complete are the playbooks?\n• When was each scenario in the playbooks last exercised?\n• What is the mechanism for exercising different failure modes?\n• How are new team members trained to be prepared to handle disasters?\n• Which roles and responsibilities apply during a disaster?\n• How do you prepare for disasters?\n• How are disasters used to improve future operations and disaster response?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Disasters are handled in an ad hoc manner, requiring individual heroics.\n• Playbooks do not exist, or do not cover all scenarios.\n• Little or no training exists.\n• Service resiliency and different failure scenarios are never tested.\nLevel 2: Repeatable\n• Playbooks exist for all failure modes, including large-scale disasters.\n• New team members receive on-the-job training.\n• Disasters are handled consistently, independent of who is responding.\n• If multiple team members respond, their roles, responsibilities, and handoffs\nare not clearly deﬁned, leading to some duplication of effort.\nLevel 3: Defined\n• The SLA is deﬁned, including dates for postmortem reports.\n• Handoff procedures are deﬁned, including checks to be performed and docu-\nmented.\n\n\nA.12\nDisaster Preparedness\n449\n• How to scale the responding team to make efﬁcient use of more team members\nis deﬁned.\n• The roles and responsibilities of team members in a disaster are deﬁned.\n• Speciﬁc disaster preparedness training for new team members is deﬁned and\nimplemented.\n• The team has regular disaster preparedness exercises.\n• The exercises include ﬁre drills performed on the live service.\n• After every disaster, a postmortem report is produced and circulated.\nLevel 4: Managed\n• The SLA is tracked using dashboards.\n• The timing for every step in the process from the moment the event occurred\nis tracked on the dashboard.\n• A program for disaster preparedness training ensures that all aspects are\ncovered.\n• The disaster preparedness program measures the results of disaster prepared-\nness training.\n• As teams become better at handling disasters, the training expands to cover\nmore complex scenarios.\n• Teams are involved in cross-functional ﬁre drills that involve multiple teams\nand services.\n• Dates for publishing initial and ﬁnal postmortem reports are tracked and\nmeasured against the SLA.\nLevel 5: Optimizing\n• Areas for improvement are identiﬁed from the dashboards.\n• New techniques and processes are tested and the results measured and used\nfor further decision making.\n• Automated systems ensure that every failure mode is exercised within a\ncertain period, by artiﬁcially causing a failure if one has not occurred naturally.\n\n\nThis page intentionally left blank \n",
      "page_number": 472
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 481-488)",
      "start_page": 481,
      "end_page": 488,
      "detection_method": "topic_boundary",
      "content": "Appendix B\nThe Origins and Future of\nDistributed Computing\nand Clouds\nTo me it seems quite clear\nthat it’s all just a little bit of\nhistory repeating\n—Propellerheads\nModern, large datacenters typically consist of rack after rack of pizza box–sized\ncomputers. This is a typical design pattern for any large cloud-scale, distributed\ncomputing environment. Why is this? How did so many companies arrive at the\nsame design pattern?\nThe answer is that this design pattern was inevitable. To manage enormous\ndatacenters, hardware and processes must be highly organized. Repeating the\nsame rack structure within a datacenter reduces complexity, which reduces man-\nagement and administrative overhead. Many other factors led to the choice of\nhardware to ﬁt into those racks.\nHowever, nothing is truly inevitable. Thus the appearance of massive data-\ncenters full of machines built from commodity hardware, distributed computing\nreplacing large computers, and the popularity of cloud computing were inevitable\nonly in the sense that necessity is the mother of invention.\nEarlier approaches to large web and e-commerce sites did not have the right\neconomies of scale. In fact, scaling became more expensive per user. This caused the\nﬁrst dot-com bubble to be unsustainable. What drove the distributed computing\nrevolution was the need to create the right economies of scale, where addtional\nusers become cheaper to support.\nEvery order of magnitude improvement in the cost of computing enables a\nnew era of applications, each of which was unimaginable just a few years before.\n451\n\n\n452\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nEach requires new supporting infrastructure technology and operational method-\nologies. These technologies and methodologies do not always arrive in time, and\nsometimes they are ahead of their time.\nUnderstanding the historical context of all these changes gives context to the\ntools and techniques described in this book. If this doesn’t interest you, feel free to\nskip this appendix.\nThe history of the computer industry is quite long, starting with the abacus.\nThis appendix will skip ahead a few years and focus on a few speciﬁc periods:\n• The pre-web era: The years immediately prior to the web, from 1985 to 1994\n• The ﬁrst web era: “The dot-com bubble” from 1995 to 2000\n• The dot-bomb era: The economic downturn from 2000 to 2003\n• The second web era: The resurgence from 2003 to 2010\n• The cloud computing era: Where we are today\nEach period had different needs that drove the technology decisions of the day,\npushing evolution forward step by step. These needs changed over time due to\nthe ebb and ﬂow of economic prosperity, the march of technological improvement,\nand increasing expectations of reliability and speed. This appendix provides our\ninterpretation of how it happened.\nB.1 The Pre-Web Era (1985–1994)\nComputing was different prior to the web. Reliability and scale were important but\nnot in the same way they are today. The Internet was used only by a small group\nof technologists, and most people were unaware of its existence.\nAvailability Requirements\nFor most businesses, their internal operations were reliant on computers, but\noutages were largely invisible to external customers. The exceptions were com-\npanies whose customers included a large segment of the population and accessed\nthe services from outside the business premises—for example, telephone compa-\nnies whose customers were making phone calls and banks that provided ATMs.\nCustomers expect those services to be available on a 24 × 7 basis.\nIn this era most large businesses were already heavily reliant on computers\nfor the bulk of their work. Some employees had remote access from home, over a\ntelephony-based modem, sometimes with a dumb terminal, sometimes with a PC\nor Mac.\nMost businesses could schedule outages for maintenance—by looking at prod-\nuct release schedules and avoiding the end-of-quarter period. The customers of\n\n\nB.1\nThe Pre-Web Era (1985–1994)\n453\nthe computer systems were internal to the company, and there were easy, deﬁned\nways of contacting them to schedule downtime.\nThe Internet was largely text-based, with email, Internet news, bulletin boards,\nand ﬁle transfer programs. Outages of an Internet service might cause a backlog\nof email or news, but went largely unnoticed by most people. For most people,\nInternet access was a perk, but it was not business-critical. Some companies offered\nanonymous-ftp drop-boxes for third parties, such as customers needing support.\nLittle other commercial business was carried out over the Internet. It was far from\nclear in the early days whether it was a potential acceptable use policy violation to\nuse the Internet for commercial purposes.\nDuring this era, 24 × 7 operation was important for some internal corporate\nsystems, but not for Internet services. Maintenance outages of internal systems\ncould be scheduled, because the user population was known and could be easily\ninformed.\n.\nDowntime Used to Be Normal\nTom’s ﬁrst computer experience was in 1980. Tom was in sixth grade and his\nschool gave a small group of students access to a terminal connected to a main-\nframe. When someone logged into the system, it would display a message that\n“the system will be down every day after 6 for taping.” Neither Tom nor\nhis teacher knew what that meant, but years later Tom realized that “taping”\nmust have meant “doing backups.” The system was down for hours each day\nto do backups and that was normal.\nTechnology\nBusiness applications and services ran on mainframes and minicomputers. These\ndevices often had megabytes of storage and could execute less than 1 million\ninstructions per second (MIPS). They were server-class machines, which were\nbuilt with high-quality components, high-speed technologies, and expansion capa-\nbilities.\nHome computers were a new thing. There was no Internet access and common\napplications (stand-alone games, word processing, spreadsheets, and tax pack-\nages) came on ﬂoppy disk. The hardware components were “consumer grade,”\nwhich meant cheap, underpowered, and unreliable. The computers often relied\non 8-bit and later 16-bit processors, with RAM and disk storage measured in\nkilobytes and megabytes, not gigabytes and terabytes. CPU speed was measured\nin MHz, and GHz was science ﬁction. Data was stored on slow ﬂoppy disks.\n\n\n454\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nA 10MB hard drive was a luxury item, even though it was fragile and being\ndropped would destroy it.\nScaling\nA company’s computer systems served internal customers, applications, and busi-\nness processes. They needed to scale with the growth of the company. As the\ncompany’s business grew, so would the number of employees and the computing\nrequirements. Even for fast-growing companies, this growth was relatively pre-\ndictable and often bounded by the much slower rate at which new employees were\nhired.\nBusiness-critical applications ran on a small number of large, high-end com-\nputers. If a computer system ran out of capacity, it could be upgraded with\nadditional disks, memory, and CPUs. Upgrading further meant buying a big-\nger machine, so machines that were to be servers were typically purchased with\nplenty of spare upgrade capacity. Sometimes services were also scaled by deploy-\ning servers for the application into several geographic regions, or business units,\neach of which would then use its local server. For example, when Tom ﬁrst worked\nat AT&T, there was a different payroll processing center for each division of the\ncompany.\nHigh Availability\nApplications requiring high availability required “fault-tolerant” computers.\nThese computers had multiple CPUs, error-correcting RAM, and other technolo-\ngies that were extremely expensive at the time. Fault-tolerant systems were niche\nproducts. Generally only the military and Wall Street needed such systems. As a\nresult they were usually priced out of the reach of typical companies.\nCosts\nDuring this era the Internet was not business-critical, and outages for internal\nbusiness-critical systems could be scheduled because the customer base was a lim-\nited, known set of people. When necessary, advanced reliability and scaling needs\nwere addressed with very expensive specialized hardware. However, businesses\ncould easily calculate the costs of any outages, do a risk analysis to understand\nwhat the budget for providing higher reliability was, and make an informed\ndecision about how to proceed. These costs were under control.\nScaling was handled through hardware upgrades, but compute requirements\nscaled in predictable ways with the company’s business. As the business grew, the\nbudget for the compute infrastructure grew, and upgrades could be planned and\nscheduled.\n\n\nB.2\nThe First Web Era: The Bubble (1995–2000)\n455\nB.2 The First Web Era: The Bubble (1995–2000)\nIn the beginning of the ﬁrst web era, web sites were relatively static reposito-\nries of linked documents. The ﬁrst corporate web sites were largely marketing\nliterature—information about the company and its products, press releases, job\nlistings, and contact information for various customer-facing groups, such as sales\nand customer support. But many businesses quickly realized the potential for\nconducting business over the Internet, and e-commerce was born.\nAvailability Requirements\nFor most companies, their Internet presence was initially treated in much the same\nway as their key internal systems. They used server-grade hardware with some\nhigh-availability options and additional capacity for scaling. For most companies,\ntheir Internet presence was just another part of the infrastructure, and not a par-\nticularly business-critical part at that. Under normal conditions the site should\nremain up, but system administrators could schedule outages for maintenance\nas needed. Usually a scheduled outage for the web service involved conﬁguring\nanother machine to respond with a static page letting people know that the site\nwas down for maintenance and to try again later.\nThen new startup companies appeared with business models that revolved\naround conducting business entirely on the web. These companies did not have\nexisting products, channels, and customers. They were not adapting existing busi-\nness processes to include the web. For these companies, their Internet presence was\nthe entire business. If their web sales channel failed, everything came to a standstill.\nThey did not have the luxury of contacting their customers to schedule a mainte-\nnance window, as they had no way to know who their customers might be during\nthat time period. Anyone with Internet access was a potential customer. These com-\npanies needed a highly reliable Internet presence in a way that no other companies\nhad before. Companies wanted 24 × 7 operations with no maintenance windows.\nTechnology\nDuring this era, home computers became more common, as did faster connections\nto the home with xDSL and Internet service from cable TV companies. Better graph-\nics and more powerful computers resulted in better games, many of which were\nnetworked, multi-user games. Voice over IP (VoIP) emerged, with associated new\nproducts and services. Disks became larger and cheaper, so people started digitiz-\ning and storing new types of data, such as music and videos. Inevitably companies\nbuilt products and Internet services around that data as well.\nOn the server side, companies looking to provide a more reliable Internet pres-\nence started buying RAID subsystems instead of plain disks, multiple high-end\n\n\n456\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nprocessors, and so on. While these technologies are common today, they were\nextremely expensive then. Vendors that previously sold to the very small set of\ncustomers who needed either very large or very reliable computers had an entirely\nnew group of customers who wanted machines that were both large and reliable.\nSun Microsystems’ E4500 server was the normative hardware of the dot-com era.\nLoad balancers also appeared on the market during this era. A load balancer\nsits in front of a group of machines that are all providing the same service. It contin-\nually tests to see if the service is available on each of the machines. Load balancers\ncan be conﬁgured in a primary/secondary setup—sending all trafﬁc to the primary\ndevice until it fails, and then automatically switching to the secondary device. They\ncan also be conﬁgured to load-share between machines that all provide the same\nservice. In this latter mode, when a machine stops responding correctly, the load\nbalancer stops directing queries to it until it resumes responding correctly. Load\nbalancers provide automatic failover in case a machine is down, making them a\nuseful tool for a service that needs high availability.\nScaling\nBusinesses needed servers to power their web sites. System administrators applied\ntheir old methods to the new requirements: one machine for each web site. As the\nsite got more popular, larger machines were used to meet the demand. To achieve\nbetter performance, custom CPUs and new internal architectures were developed,\nbut these machines were expensive. Software was also expensive. A typical web\nserver required an OS license, a web server license, and a database server license.\nEach was priced proportionately to the hardware.\nWith the web the requirements for scaling are not bound to the number of\nemployees in the company. The web introduces an environment where the users\nof the service can be anyone with an Internet connection. That is a very large and\nrapidly growing number. When a site introduces or successfully markets a web-\nbased service, the number of people accessing the servers can increase very rapidly\nover a short period of time. Scaling for sudden, unpredictable ﬂuctuations in ser-\nvice usage was a new challenge that was not well understood in this era, although\nit was something for which Internet startups tried to prepare. Internet startups\nplanned for success and purchased the biggest, most reliable systems available.\nThese were expensive times.\nHowever, the practice of growing web sites through bigger, more reliable, more\nexpensive hardware was not economically viable. Normally as a company grows,\neconomies of scale result in lower cost per unit. The dot-coms, however, required\ncomputers that were more expensive per unit of capacity as the company grew\nlarger. A computer that is 10 times more powerful than an average computer is\n\n\nB.2\nThe First Web Era: The Bubble (1995–2000)\n457\nmore than 10 times as expensive. These larger, more reliable systems used custom\nhardware and had a smaller market—two factors that drove up prices. For linear\nincreases in performance, the cost per unit of capacity was growing super-linearly.\nThe more users the web site gained, the more expensive it was to provide the ser-\nvice. This model is the opposite of what you want. Additional costs came from the\ntechniques used to ensure high availability, which are covered in the next section.\nOddly enough, these high costs were acceptable at the time. The dot-coms\nwere ﬂush with cash. Spending it on expensive machines was common, because\nit showed how optimistic you were about your company’s success. Also, the eco-\nnomics of scaling sites in this way was not well understood—the traditional model\nof increased sales yielding a lower cost per unit was assumed to still hold true. In\naddition, startup valuations were made in a rather strange manner in those days.\nIf a startup was achieving a small proﬁt, it got a low valuation. However, if it was\nshowing a loss, it got a high valuation. Moreover, the larger the loss, the higher\nthe valuation. When these startups were ﬂoated on the stock market, their stock\nprices went through the roof, and the investors made substantial proﬁts, even if\nthe business model made no sense.\nThe saying at the time was “We may lose money on every sale, but we’ll make\nit up in volume.” Behind the scenes was the thought that if the company could\ncorner the market, it could raise prices later. However, that assumption failed to\ntake into account that the next startup would immediately undercut the higher\nprices with its own loss-making corner-the-market scheme.\nThis gold rush mentality kept the economy buzzing for quite a while. Then\nin 2000 the bubble burst. The house of cards collapsed. This approach to scaling\nweb-based services was unsustainable. If the bubble had not burst due to issues\nof investment and cash ﬂow, it would have failed due to the bad economics of the\ntechnology being used.\nHigh Availability\nWith the advent of the web, the user base changed from known internal company\nemployees with predictable, cyclic availability (the 9-to-5 business day) require-\nments and access schedules, to unknown external Internet users who required\nconstant access. This change created the need for higher reliability. The ﬁrst\napproach to meeting these availability goals was to buy more expensive hardware\nwith built-in higher availability—for example, RAID and multiple CPUs.\nBut even the most reliable system fails occasionally. Traditional options for\ncritical systems were to have a service contract with a four-hour turnaround for\nreplacement parts, or to purchase spare parts to be stored near the machine. Either\nway, some downtime would be required in the event of a hardware failure.\n\n\n458\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nThere was also the issue of downtime to perform software upgrades. Apply-\ning the internal corporate approach of notifying the user base, it became common\npractice for web sites to pre-announce such downtime. There would be a warn-\ning that “This site will be down Saturday from noon to 5 PST for an upgrade.”\nRegular users could plan around this, but new or occasional customers would be\ncaught unaware, as there was no other way to notify them. Advertising upgrades\nin advance could also lead to adverse headlines if the upgrade went poorly, as\nsome people would watch to see how the upgrade went and to report on the new\nservice.\nN + 1 Configurations\nUsing two machines became the best practice. One would run the service and the\nother was idle but conﬁgured and ready to take over if the ﬁrst machine failed.\nUnless the site had a load balancer the “failover” usually required manual inter-\nvention but a good systems administrator could do the switch fast enough that the\nweb site would be down for less than an hour. This is called an N+1 conﬁguration\nsince there is one more device than required to provide the service. This technique\nis very expensive considering that at any given time 50 percent of your investment\nis sitting idle.\nSoftware upgrades could be done by upgrading the spare server and switching\nto it when it was time to unveil the new features. The downtime would only be\nminutes or seconds to perform the failover. Users might not even notice!\nN + 2 Configurations\nWhat if the primary machine failed while the spare was being upgraded? The half-\nconﬁgured machine would not be in a usable state. As software releases increased\nin frequency, the likelihood that the spare would not be in a usable state also\nincreased.\nThus, the best practice became having three machines, or an N + 2 conﬁgura-\ntion. Now systems administrators could safely perform upgrades but 66 percent\nof the hardware investment was idle at any given time. Imagine paying for three\nhouses but only living in one. Imagine being the person who had to tell the CEO\nhow much money was used on idle equipment!\nSome companies tried to optimize by load sharing between the machines.\nExtra software development or a load balancer was required to make this work\nbut it was possible. In an N + 1 conﬁguration, systems administrators could per-\nform software upgrades by taking one machine out of service and upgrading it\nwhile the other remained running. However, if both machines were at 80 percent\nutilization, the site now had a single machine that was 160 percent utilized, which\nwould make it unacceptably slow for the end users. The web site might as well be\ndown. The easy solution to that problem is to never let either machine get more\n",
      "page_number": 481
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 489-496)",
      "start_page": 489,
      "end_page": 496,
      "detection_method": "topic_boundary",
      "content": "B.3\nThe Dot-Bomb Era (2000–2003)\n459\nthan 50 percent utilized—but that simply returns us to the situation where half the\ncapacity we paid for is idle. The idle capacity is just split between two machines!\nSome companies tried to do such upgrades only late at night when fewer users\nmeant that utilization had dipped below 50 percent. That left very little time to\ndo the upgrade, making large or complex upgrades extremely risky. Changes that\nrequired an operating system upgrade or extensive testing were not possible. If\nthe site became popular internationally and was busy during every time zone, this\noption disappeared. Also, no one can schedule hardware failures to happen only\nat night! Neither of these approaches was a viable option for better utilization of\nthe available resources.\nFor high availability, sites still needed three machines and the resulting 66\npercent idle capacity.\nCosts\nThe cost of providing a highly available and popular web service during this\nera was very high. It was run on expensive high-end hardware, with expensive\nreliability features such as RAID and multiple CPUs, in an N + 1 or N + 2 conﬁgu-\nration. That architecture meant that 50 to 66 percent of this expensive hardware was\nalways idle. To reduce downtime to next to nothing, sites might also use expensive\nload balancers.\nThe OS costs and support costs for this high-end hardware were also very\nhigh, as were the costs for the application software, such as the web server software\nand database software.\nScaling a site built in this way meant that the cost per unit of performance\nincreased as the site got bigger. Rather than more customers resulting in economies\nof scale, more customers meant a higher cost per unit. The cost model was the\nreverse of what a business would normally expect.\nB.3 The Dot-Bomb Era (2000–2003)\nThe dot-com bubble collapse started in 2000. By 2001, the bubble was deﬂating as\nfast as it could. Most of the dot-coms burned through their venture capital and\ntheir stock ceased trading, often never having reached proﬁtability.\nAfter the collapse came a period of calm. Things slowed down a bit. It was\npossible to pause and consider what had been learned from the past. Without all\nthe hype, marketing, and easy money, the better technologies survived. Without\npressure from investors wanting their cash spent quickly so that they would get\na quick return on their investment, people could take enough time to think and\ninvent new solutions. Silicon Valley is, for the most part, a meritocracy. Working\ntechnology rules; hype and self-promotion are ﬁltered out.\n\n\n460\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nAvailability Requirements\nDuring the dot-bomb era, there were no signiﬁcant changes in availability require-\nments. The Internet-based companies that had survived the crash developed a\nbetter understanding of their availability requirements and ﬁgured out how to\nmeet them without breaking the bank.\nTechnology\nThree trends enabled the next phase: surplus capacity left over from the previous\nboom years, the commoditization of hardware, and the maturation of open source\nsoftware.\nThe ﬁrst trend was short-lived but signiﬁcant. A lot of capacity had been built\nup in the previous boom years and suppliers were slashing prices. Millions of\nmiles of ﬁber had been laid in the ground and in the oceans to meet the predicted\nbandwidth needs of the world. With relatively few customers, telecommunications\nproviders were desperate to make deals. Similarly, huge datacenter “colocation\nfacilities” had been built. A colocation facility is a highly reliable datacenter facil-\nity that rents space to other companies. Many colocation providers went bankrupt\nafter building some of the world’s largest facilities. That space could now be rented\nvery inexpensively. While these surpluses would eventually be exhausted, the\ntemporarily depressed prices helped kickstart the era.\nThe second trend was the commoditization of hardware components used in\nhome computers, such as Intel x86 CPUs, low-end hard drives, and RAM. Before\nthe advent of the web, the average home did not have a computer. The popularity\nof the Internet created more demand for home computers, resulting in components\nbeing manufactured at a scale never before seen. In addition, the popularity of\ngames that required high-end graphics, lots of memory, and fast CPUs was one\nof the major drivers toward making increasingly higher-end devices available in\nthe consumer market. This mass production led to commoditization and, in turn,\nlower prices. The price of home PCs came down, but servers still used different\ncomponents and remained expensive.\nThe third trend was the maturity of open source projects such as Linux,\nApache, MySQL, and Perl. The rise of Linux brought a UNIX-like server operating\nsystem to the Intel x86 platform. Previously systems that used the Intel x86 chips\ncould not run server-class, UNIX and UNIX-like operating systems. SGI, IBM, Sun,\nand others did not make their operating systems available for the x86. Intel x86\ncomputers ran Windows 95 and variants that were not designed as server operat-\ning systems. Even Windows NT, which was designed as a server operating system,\ndid not achieve success as a web service platform.\nThere were also free versions of BSD UNIX available for x86-based comput-\ners at the same time. Eventually, however, Linux became the dominant x86 UNIX\n\n\nB.3\nThe Dot-Bomb Era (2000–2003)\n461\nbecause various companies like RedHat offered versions of Linux with support.\nCorporations had typically shied away from free open source software, because it\nwas not supported. These companies were slowly persuaded that commercial ver-\nsions of Linux were acceptable for servers, because they could still buy support.\nEven though they were still paying for the OS and support for the OS, they real-\nized signiﬁcant cost savings through the use of cheaper x86 hardware and reduced\nOS costs.\nAlthough Linux was available during the ﬁrst web era, it was not mature\nenough for production use. In fact, tools like Linux, Apache, MySQL, and Perl were\nconsidered toys compared to a Solaris OS, Netscape web server, Oracle database,\nJava “stack” or a Microsoft NT Server, IIS (web server), and SQL Server database\nand .NET environment. Even so, those open source projects were now creating soft-\nware that was reliable enough for production use. Linux matured. Apache proved\nto be faster, more stable, more feature rich, and easier to conﬁgure than commer-\ncial platforms. MySQL was easier to install and manage than Oracle. Oracle’s price\ntag was so high that the moment a free SQL database was available, the ﬂood-\ngates opened. Perl matured, added object-oriented features, and gained acceptance\noutside of its initial niche as a system administration language. All of this was\nunimaginable just a few years earlier. One combination of open source software\nwas so common that the acronym “LAMP” was coined, referring to the quartet of\nLinux, Apache, MySQL, and Perl. The ability to use commodity servers running a\nfree operating system was revolutionary.\nHigh Availability\nWhile a LAMP system was less expensive, it was slower and less reliable. It would\ntake many such servers to equal the aggregate processing capacity of the larger\nmachine being replaced. Reliability was more complex.\nResearchers started experimenting with using low-cost components to build\nservers. Research such as Recovery-Oriented Computing (ROC) at University of\nCalifornia–Berkeley (Patterson et al. 2002) discovered that many small servers\ncould work better and be cheaper than individual large servers. This was pre-\nviously unheard of. How could that underpowered CPU designed for desktops\ncompete with a custom-designed Sun SPARC chip? How could a cheap consumer-\ngrade hard disk compete with a fancy SCSI drive?\nThe conventional engineering wisdom is that more components means more\nfailures. If one computer is likely to fail every 100 days, then it is likely that a\nsystem with two computers will have a failure every 50 days. If you had enough\ncomputers, you were likely to experience failures every day.\nResearch projects like ROC at UC-Berkeley and others reversed that logic.\nMore computers could be more reliable when “distributed computing” techniques\n\n\n462\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nwere employed. Rather than design a system where any one machine’s failure\nprevented the system from providing service, these researchers developed dis-\ntributed computing techniques that let many machines share the workload and\nhave enough spare capacity that a certain percentage of the machines could be\ndown and service would be unaffected.\nROC also observed that companies were paying for reliability at every level.\nThe load balancer provided reliability, sending trafﬁc to only the “up” server(s).\nThe servers provided reliability by using expensive custom CPUs. The storage sys-\ntems provided reliability by using expensive RAID controllers and high-end hard\ndrives. Yet, to avoid downtime due to applications that required manual failover,\ncompanies still had to write software to survive outages. Why pay for many layers\nof reliability and also pay to develop software that survives outages? ROC rea-\nsoned that if the software already survived outages, why pay for extra quality at\nevery level? Using the cheaper commodity components might result in less reliable\nservers, but software techniques would result in a system that was more reliable\nas a whole. Since the software had to be developed anyway, this made a lot of\nsense.\nDigital electronics either work or they don’t. The ability to provide service is\nbinary: the computer is on or off. This is called the “run run run dead” problem.\nDigital electronics typically run and run and run until they fail. At that point, they\nfail completely. The system stops working. Compare this to an analog system such\nas an old tube-based radio. When it is new, it works ﬁne. Over time components\nstart to wear out, reducing audio quality. The user hears more static, but the radio\nis usable. The amount of static increases slowly, giving the user months of warn-\ning before the radio is unusable. Instead of “run run run dead,” analog systems\ndegrade slowly.\nWith distributed computing techniques, each individual machine is still digi-\ntal: it is either running or not. However, the collection of machines is more like the\nanalog radio: the system works but performance drops as more and more machines\nfail. A single machine being down is not a cause for alarm but rather a signal that\nit must be repaired before too many other machines have also failed.\nScaling\nROC researchers demonstrated that distributed computing could be reliable\nenough to provide a service requiring high availability. But could it be fast enough?\nThe answer to this question was also “yes.” The computing power of a fully loaded\nSun E10K could be achieved with enough small, pizza box–sized machines based\non commodity hardware.\nWeb applications were particularly well suited for distributed computing.\nImagine a simple case where the contents of a web site can be stored on one\n\n\nB.3\nThe Dot-Bomb Era (2000–2003)\n463\nmachine. A large server might be able to deliver 4000 queries per second (QPS) of\nservice. Suppose a commodity server could provide only 100 QPS. It would take\n40 such servers to equal the aggregate capacity of the larger machine. Distributed\ncomputing algorithms for load balancing can easily scale to 40 machines.\nData Scaling\nFor some applications, all the data for the service might not ﬁt on a single commod-\nity server. These commodity servers were too small to store a very large dataset.\nApplications such as web search have a dataset, or “corpus,” that could be quite\nlarge. Researchers found that they could resolve this issue by dividing the cor-\npus into many “fractions,” each stored on a different machine, or “leaf.” Other\nmachines (called “the root”) would receive requests and forward each one to the\nappropriate leaf.\nTo make the system more resilient to failures, each fraction could be stored on\ntwo different leaves. If there were 10 fractions, there would be 20 leaves. The root\nwould divide the trafﬁc for a particular fraction among the two leaves as long as\nboth were up. If one failed, the root would send all requests related to that fraction\nto the remaining leaf. The chance of a simultaneous failure by two leaves hold-\ning the same data was unlikely. Even if it did happen, users might not notice that\ntheir web searches returned slightly fewer results until the replacement algorithms\nloaded the missing data onto a spare machine.\nScaling was also achieved through replication. If the system did not process\nrequests fast enough, it could be scaled by adding leaves. A particular fraction\nmight be stored in three or more places.\nThe algorithms got more sophisticated over time. For example, rather than\nsplitting the corpus into 10 fractions, one for each machine, the corpus could be\nsplit into 100 fractions and each machine would store 10. If a particular fraction\nwas receiving a particularly large number of hits (it was “hot”), that fraction could\nbe placed on more machines, bumping out less popular fractions. Better algo-\nrithms resulted in better placement, diversity, and dynamically updatable corpus\ndata.\nApplicability\nThese algorithms were particularly well suited for web search and similar appli-\ncations where the data was mostly static (did not change) except for wholesale\nreplacements when a new corpus was produced. In contrast, they were inappropri-\nate for traditional applications. After all, you wouldn’t want your payroll system\nbuilt on a database that dealt with machine failures by returning partial results.\nAlso, these systems lacked many of the features of traditional databases related to\nconsistency and availability.\n\n\n464\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nNew distributed computing algorithms enabled new applications one by one.\nFor example, the desire to provide email as a massive web-based service led\nto better storage systems. Over time more edge cases were conquered so that\ndistributed computing techniques could be applied to more applications.\nCosts\nIf distributed computing could be more reliable and faster, could it be more cost\neffective, too? The cost of one highly reliable and expensive machine compared\nvery well to the cost of enough commodity machines to provide equivalent capa-\ncity. In fact, the cost per unit of capacity was often 3 to 50 times less expensive\nusing distributed computing. Remember that previously we saw 50 to 66 percent\nidle capacity with the large servers when used in N + 1 and N + 2 conﬁgura-\ntions. Suppose an entire rack of machines was required to provide the equivalent\ncapacity. That would typically be 40 machines, with two held as spares. (N + 2\nredundancy). The “waste” now drops from 66 percent to 5 percent. That shrink-\ning of idle capacity gives the distributed computing design a head start. Factoring\nin the power of a commodity market to drive down the cost of components\nimproves the situation further. Moreover, one gets a volume discount when pur-\nchasing many computers—something you can’t do when buying one or two large\nmachines.\nIn the 2003 article “Web Search for a Planet: The Google Cluster Architecture,”\nBarroso et al. wrote:\nThe cost advantages of using inexpensive, PC-based clusters over high-end multipro-\ncessor servers can be quite substantial, at least for a highly parallelizable application\nlike ours. The example $278,000 rack contains 176 2-GHz Xeon CPUs, 176 Gbytes of\nRAM, and 7 Tbytes of disk space. In comparison, a typical x86-based server contains\neight 2-GHz Xeon CPUs, 64 Gbytes of RAM, and 8 Tbytes of disk space; it costs about\n$758,000. In other words, the multiprocessor server is about three times more expen-\nsive but has 22 times fewer CPUs, three times less RAM, and slightly more disk space.\n(Barroso, Dean & Hölzle 2003)\nFurther cost reductions came by stripping the machines down to the exact\ncomponents needed. These machines did not need video cards, audio cards, speak-\ners, keyboards, USB ports, or a fancy plastic bezel with a cool logo on the front.\nEven if eliminating an item saved just a few dollars, when buying thousands of\nmachines it added up. Some companies (Yahoo!) worked with vendors to build\ncustom computers to their exact speciﬁcations, while others grew large enough to\ndesign their own computers from scratch (Google).\nAll these changes altered the economics of computing. In fact, they are what\nenabled the second web era.\n\n\nB.4\nThe Second Web Era (2003–2010)\n465\nUnder the old economics, the larger the scale, the more disproportionately\nexpensive the system became. In the new economics, the larger the scale, the more\nopportunity for the economics to improve. Instead of super-linear cost curves, cost\ngrowth was closer to linear.\nB.4 The Second Web Era (2003–2010)\nIn the history of computing, every jump in improved cost led to a wave of new\napplications. Distributed computing’s better economics was one such leap. Web\nservices that had previously failed, unable to make a proﬁt, were now cheaper to\noperate and got a second chance.\nAvailability Requirements\nAs before, companies that conducted business entirely over the Internet typi-\ncally aimed at a global audience of Internet users. That meant that they required\n24 × 7 availability with no scheduled downtime, and as close to no unscheduled\ndowntime as possible. Companies started applying the newly developed dis-\ntributed computing techniques to their service offerings to meet these availability\nrequirements in a more cost-effective way.\nTechnology\nMany companies embraced distributed computing techniques. Google adopted\nthe techniques to provide web search that was faster than ever seen before. Previ-\nously web searches took many seconds and sometimes minutes or hours to show\nsearch results. Google was so proud of its ability to return results in less than half\na second that it listed the number of milliseconds your request took to process at\nthe bottom of every page.\nNew advertising systems like Google AdSense, which collects pennies per\nclick, could be proﬁtable now that the cost of computing was signiﬁcantly cheaper.\nEarlier business models that involved selling advertising on a web site required\nhuge sales forces to get advertisers to buy ads on their web site. AdSense and simi-\nlar systems were fully automated. Potential advertisers would bid in a huge, online\nauction for ad placement. This reduced and often eliminated the need for a sales\nforce. All the company needed to do was add some JavaScript code to its site. Soon\nthe advertisements would start appearing and money would start rolling in. Such\nadvertising systems created a new business model that enabled the development\nof entire new industries.\nWeb hosting became much cheaper. The software got much easier to use.\nThis led to the invention of “blogs” (originally from the term “web-log”), which\n\n\n466\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nrequired very little technical knowledge to operate. Rather than needing a sales\nforce and a large IT department, a single person could run a successful blog.\nBloggers could focus on creating content and the advertising network would send\nthem checks.\nHigh Availability\nWith distributed computing techniques, high availability and scaling became\nclosely coupled. Techniques used to ensure high availability also aided scaling,\nand vice versa. Google published many of the distributed computing techniques\nit invented. Some of the earliest of these were the Google File System (GFS) and\nMapReduce.\nGFS was a ﬁle system that scaled to multiple terabytes of data. Files were\nstored as 64-megabyte chunks. Each chunk was stored on three or more commod-\nity servers. Accessing the data was done by talking directly to the machine with the\ndesired data rather than going through a mediator machine that could be a bottle-\nneck. GFS operations often happened in parallel. For example, copying a ﬁle was\nnot done one block at a time. Instead, each machine that stored a part of the source\nﬁle would be paired with another machine and all blocks would be transmitted\nin parallel. Resiliency could be improved by conﬁguring GFS to keep more than\nthree replicas of each block, decreasing the chance that all copies of the ﬁle would\nbe on a down machine at any given time. If one machine did fail, the GFS master\nwould use the remaining copies to populate the data elsewhere until the replication\nfactor was achieved. Increasing the replication level also improved performance.\nData access was load balanced among all the replicas. More replicas meant more\naggregate read performance.\n.\nCase Study: MapReduce\nMapReduce is a system for processing large batches of data in parallel. Sup-\npose you needed to process 100 terabytes of data. Reading that much data\nfrom start to ﬁnish would take a long time. Instead, dozens (or thousands) of\nmachines could be set up to process a portion of the data each in parallel. The\ndata they read would be processed (mapped) to create an intermediate result.\nThese intermediates would be processed and then summarized (reduced) to\ngain the ﬁnal result. MapReduce did all the difﬁcult work of dividing up the\ndata and delegating work to various machines. As a software developer using\nMapReduce, you just needed to write the “map” function and the “reduce”\nfunction. Everything else was taken care of.\n",
      "page_number": 489
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 497-504)",
      "start_page": 497,
      "end_page": 504,
      "detection_method": "topic_boundary",
      "content": "B.4\nThe Second Web Era (2003–2010)\n467\n.\nBecause MapReduce was a centralized service, it could be improved and\nall users would beneﬁt from the changes. For example, there was a good prob-\nability that a machine would fail during a MapReduce run that takes days and\ninvolves thousands of machines. The logic for detecting a failed machine and\nsending its portion of data to another working machine is incorporated in the\nMapReduce system. The developer does not need to worry about it. Suppose a\nbetter way to detect and replace a failed machine is developed. This improve-\nment would be made to the central MapReduce system and all users would\nbeneﬁt.\nOne such improvement involved predicting which machines would fail.\nIf one machine was taking considerably longer to process its share of the data,\nthat slow performance could be a sign that it had a hard disk that was start-\ning to fail or that some other problem was present. MapReduce could direct\nanother machine to process the same portion of data. Whichever machine ﬁn-\nished ﬁrst would “win” and its results would be kept. The other machine’s\nprocessing would be aborted. This kind of preemptive redundancy could\nsave hours of processing, especially since a MapReduce run is only as fast\nas the slowest machine. Many such optimizations were developed over the\nyears.\nOpen source versions of MapReduce soon appeared. Now it was easy for\nother companies to adopt MapReduce’s techniques. The most popular imple-\nmentation is Hadoop. It includes a data storage system similar to GFS called\nHBase.\nScaling\nIf you’ve spent money to invent ways to create highly available distributed com-\nputing systems that scale well, the best way to improve your return on investment\n(ROI) is to use that technology on as many datacenters as possible.\nOnce you are maintaining hundreds of racks of similarly conﬁgured com-\nputers, it makes sense to manage them centrally. Centralized operations have\neconomic beneﬁts. For example, standardization makes it possible to automate\noperational tasks and therefore requires less labor. When labor is manual, larger\nsystems require more labor. When operations are automated, the cost of develop-\ning the software is spread across all the machines, so the cost per machine decreases\nas the number of machines increases.\nTo gain further efﬁciencies of scale, companies like Google and Facebook take\nthis one step further. Why not treat the entire datacenter as one giant computer?\nA computer has memory, disks, and CPUs that are connected by a communi-\ncation “bus.” A datacenter has a network that acts like a bus connecting units\n\n\n468\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nof compute and storage. Why not design and operate datacenters as one large,\nwarehouse-sized computer? Barroso, Clidaras, and Hölzle (2013) coined the phrase\n“warehouse-scale computing” to capture this idea of treating the entire datacenter\nas one giant computer.\nCosts\nIn this era companies also competed to develop better and less expensive data-\ncenters. The open source movement was reducing the cost of software. Moore’s\nlaw was reducing the cost of hardware. The remaining costs were electricity and\noperations itself. Companies saw that improvements in these areas would give\nthem a competitive edge.\nPrior to the second web era, datacenters wasted electricity at unbelievable\nrates. According to the Uptime Institute, the typical datacenter has an average\npower usage effectiveness (PUE) of 2.5. This means that for every 2.5 watts in at\nthe utility meter, only 1 watt is used for the IT. By using the most efﬁcient equip-\nment and best practices, most facilities could achieve 1.6 PUE. The lack of efﬁciency\nprimarily comes from two places. First, some efﬁciency is lost every time there is\na power conversion. A UPS converts the power from alternating current (A/C) to\ndirect current (D/C), and back to A/C—two conversions. Power is converted from\nhigh-voltage lines to 110 VAC used in power outlets to 12 V and other voltage levels\nrequired by the components and chips in computers—four or ﬁve more conver-\nsions. In addition, cooling is a major factor. If 1 watt of power makes a computer\ngenerate a certain amount of heat, it takes at least 1 watt of power to remove that\nheat. You don’t actually “cool” a datacenter; you extract the heat. Companies seek-\ning to reduce their operational costs like Google, Microsoft, and Facebook achieved\ncloser to 1.2 PUE. In 2011, Google reported one datacenter achieved 1.08 PUE dur-\ning winter months (Google 2012). Every decimal improvement in PUE means a big\ncompetitive edge.\nOther things are even more important than power efﬁciency. Using less\npower—for example, by shutting off unused machines—is an improvement no\nmatter your PUE rating. Smart companies work toward achieving the best price\nper unit of computation. For example, for internet search, true efﬁciency comes\nfrom getting the most QPS for your money. This may mean skipping a certain gen-\neration of Intel CPU because, while faster, the CPU uses a disproportionate amount\nof power.\nThe new economics also made it more feasible to give away a service because\nadvertising revenue was sufﬁcient to make the business viable. Many sites were\nfree for most features but offered “premium” services at an additional cost. This\n“freemium” (free + premium) business model was a great way to attract large num-\nbers of customers. Being free meant there was very little barrier to entry. Once the\nuser was accustomed to using the service, selling the premium features was easier.\nIf advertising revenue was sufﬁcient, the free users were no burden.\n\n\nB.5\nThe Cloud Computing Era (2010–present)\n469\nSoftware, hardware, and power costs were all signiﬁcantly lower with\nthis new approach, enabling the emergence of new business models. Could\nthe price drop to the point that computing becomes cost free? The answer will\nsurprise you.\nB.5 The Cloud Computing Era (2010–present)\nThe trend so far as been that each era has computing capacity that is more econom-\nical than the previous era. Increases in capacity and reliability went from having\nsuper-linear cost growth to linear and sub-linear cost growth. Can it get any better?\nCould it become free?\nMany landlords live “rent free” by simply having enough proﬁtable tenants\nto cover the cost of the apartment they live in. This is the economic thinking that\nenables cloud computing: build more computing resources than you need and rent\nthe surplus.\nAvailability Requirements\nAround this time, mobile computing became more affordable and, in turn, more\naccessible. Cell phones became smartphones running operating systems as sophis-\nticated as those found on PCs. Mobile applications created demand for even better\nlatency and reliability.\nMobile applications create demand for lower-latency services—that is, ser-\nvices that respond faster. A mobile map application would not be useful if it\ntook an hour to calculate the best driving route to a location. It has to be fast\nenough to be usable in a real-world context, responding in a second or two. If such\ncalculations can be done hundreds of times a second, it opens the door to new\napplications: the ability to drag points in the map and see the route recalculated\nin real time. Now instead of one request, dozens are sent. Because the applica-\ntion is so much more usable this way, the number of users increases. Thus capacity\ndemands increase by multiple orders of magnitude, which requires quantum leaps\nin support infrastructure.\nMobile applications demand new levels of reliability. A map application is not\nvery useful if the map service it relies on is down when you need it. Yes, map tiles\ncan be cached but real-time trafﬁc reports less so. As mobile apps become more and\nmore life-critical, the reliability of their supporting services becomes more impor-\ntant. As reliability improvements leap forward, more life-critical applications\nbecome possible.\nCosts\nAs computing was done at even larger scales, new economics present themselves.\nIf computing becomes cheaper at larger scales, then it becomes advantageous to\n\n\n470\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nbuild a larger infrastructure. If you build an infrastructure larger than you need,\nyou simply have to develop the technology that lets you “rent” the spare capacity\nto others. The part you use is cheaper than it would have been otherwise; the part\nyou don’t use turns a proﬁt. If the proﬁt is small, it offsets the cost of your infra-\nstructure. If the proﬁt is large enough, it could pay for all of your infrastructure. At\nthat point your computing infrastructure becomes “free” for you. Do things right\nand you could have an infrastructure with a negative cost. Imagine running all of\nAmazon’s infrastructure and having someone else pay for it. Now imagine trying\nto start a new web site that sells books when your competition gets its infrastruc-\nture “for free.” These are the economic aspirations that drive the supplier side of\ncloud computing.\nIn the cloud computing era, the scale provides economics that make the cost\na new order less expensive. This frees up enough headroom to price the service\nat less than customers could do it themselves and delivers additional proﬁt that\nsubsidizes the provider’s infrastructure. Anything done to improve the efﬁciency\nof operations either adds to the service provider’s proﬁtability or enables it to offer\nservices at a lower cost than the competition.\nTo understand the consumer demand for cloud computing, we need to look\nat the costs associated with small-scale computing systems. At a small scale one\ncannot take advantage of the economics of distributed computing. Instead, one\nmust achieve reliability through more expensive hardware. Automation makes less\nsense when doing things at a small scale, which drives up the operational cost.\nWhen automation is created, it is more expensive because the cost is not amortized\nover as many uses. Many distributed computing technologies require people with\nspecialized knowledge that a small company does not possess, since at a small scale\none must hire generalists and can’t afford a full-time person (or team) to oversee\njust one aspect of the system. The use of external consultants when such expertise\nis needed can be expensive.\nMany of these problems are mitigated when small-scale computing is done\nby renting space on a cloud infrastructure. Customers get to take advantage of\nthe lower cost and greater power efﬁciency. Difﬁcult-to-maintain services such as\nspecialized storage and networking technology can be offered in a way that hides\nthe difﬁcult behind-the-scenes management such systems require.\nThere are also non-cost advantages for the customers. Elasticity is the ability\nto increase and decrease the amount of resources consumed dynamically. With\nmany applications, being able to spin up many servers for a short amount of\ntime is valuable. Suppose you have an advertising campaign that will last for just\none week. While the ads are running, you may need hundreds of web servers\nto handle the trafﬁc. Being able to acquire so many servers in hours is invalu-\nable. Most companies would need months or a year to set up so many servers.\n\n\nB.5\nThe Cloud Computing Era (2010–present)\n471\nAn advertising agency previously would not have the ability to do so. Now,\nwithout even the knowledge of how to build a datacenter, an ad agency can have\nall the systems it needs. Possibly more important is that at the end of the campaign,\nthe servers can be “given back” to the cloud provider. Doing that the old way with\nphysical hardware would be impractical.\nAnother non-cost advantage for many companies is that cloud computing\nenabled other departments to make an end-run around IT departments that had\nbecome recalcitrant or difﬁcult to deal with. The ability to get the computing\nresources they need by clicking a mouse, instead of spending months of argu-\ning with an uncooperative and underfunded IT department, is appealing. We are\nashamed to admit that this is true but it is often cited as a reason people adopt\ncloud computing services.\nScaling and High Availability\nMeeting the new requirements of scaling and high availability in the cloud com-\nputing era requires new paradigms. Lower latency is achieved primarily through\nfaster storage technology and faster ways to move information around.\nIn this era SSDs have replaced disks. SSDs are faster because there are no mov-\ning parts. There is no wait for a read head to move to the right part of a disk platter,\nno wait for the platter to rotate to the right position. SSDs are more expensive per\ngigabyte but the total cost of ownership is lower. Suppose you require 10 database\nserver replicas to provide enough horsepower to provide a service at the latency\nrequired. While using SSDs would be more expensive, the same latency can be\nprovided with fewer machines, often just two or three machines in total. The SSDs\nare more expensive, it is true—but not as expensive as needing seven additional\nmachines.\nService latency is also reduced by reducing the latency of internal communi-\ncation. In the past, information sent between two machines went through many\nlayers of technology. The information was converted to a “wire format,” which\nmeant making a copy read for transmission and putting it in a packet. The packet\nthen went through the operating system’s TCP/IP layer and device layer, through\nthe network, and then reached the other machine, where the process was reversed.\nEach of these steps added latency. Most or all of this latency has now been removed\nthrough technologies that permit direct memory access between machines. Some-\ntimes these technologies even bypass the CPU of the source or destination machine.\nThe result is the ability to pass information between machines nearly as fast as\nreading local RAM. The latency is so low that it has caused underlying RPC\nmechanisms to be redesigned from scratch to fully take advantage of the new\ncapabilities.\n\n\n472\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nTechnology\nThe technology that drives cloud computing itself is the ability to segment infra-\nstructure such that one “tenant” cannot interfere with another. This requires net-\nworking technologies that allow segmenting computers from each other at a very\nﬁne granularity such as programmable VLANs and software-deﬁned networks. It\nrequires the ability to partition one large machine into many smaller ones—that\nis, virtualization technology. It requires control panels, APIs, and various self-\nservice administration capabilities so that customers support themselves and do\nnot add considerable labor cost to datacenter operations. It requires storage and\nother services to attract customers.\nAmazon was the ﬁrst company to develop such a system. Amazon Elastic\nCompute Cloud (Amazon EC2) was the ﬁrst major company to rent virtual\nmachines on its infrastructure to subsidize its cost. The term “elastic” comes from\nthe fact that this approach makes it so fast to expand and contract the amount\nof resources being used. One of the most attractive features is how quickly one\ncan spin up new machines. Even more attractive is how quickly one can dispose\nof machines when they aren’t needed. Suppose you needed 1000 machines for a\nmonth-long web advertising campaign. It would be an arduous task to set up that\nmany machines yourself and difﬁcult to get rid of them a month later. With EC2,\nyou can spin them all up with a mouse click or a few API calls. When the campaign\nis over, releasing the machines is just as simple.\nSuch “tenant” systems are just getting started. We are just now beginning to\nunderstand these new economics and their ramiﬁcations.\nB.6 Conclusion\nThe goal of this appendix was to explain the history of technologies used for pro-\nviding web services. The techniques described here form the basis for the platform\noptions introduced in Chapter 3, the web architectures presented in Chapter 4,\nand the scaling and resiliency techniques described in Chapters 5 and 6. The other\nchapters in this book reﬂect the operational practices that make all of the above\nwork.\nThe economics of computing change over time. Faster and more reliable com-\nputing technology had a super-linear cost curve in the pre-web and ﬁrst web eras.\nThe second web era was enabled by linear cost curves. Cloud computing gives\nus sub-linear cost curves. These changes happened by taking advantage of com-\nmoditization and standardization, shifting to open source software, building more\nreliability through software instead of hardware, and replacing labor-intensive\noperations with more software.\nEvery order-of-magnitude improvement in the cost of computing enables a\nnew era of applications, each of which was unimaginable just a few years before.\n\n\nExercises\n473\nCould the person who used an 8-bit computer to balance his or her checkbook in\n1983 ever have imagined Facebook or Google Glass?\nWhat will follow cloud computing is anyone’s guess. The applications of\ntomorrow will demand computing resources that are orders-of-magnitude faster,\nhave lower latency, and are less expensive. It will be very exciting to see what\ndevelops.\nExercises\n1. This appendix provides a history of ﬁve eras of computing technology: pre-\nweb era, ﬁrst web era, dot-bomb era, second web era, and cloud computing\nera. For each era, describe the level of economic prosperity, the technological\nimprovements, and the expectations for reliability and computing power.\n2. The practice of owning the entire process instead of using external providers\nfor certain steps or parts is called “vertical integration.” Which examples of\nvertical integration were described in this appendix? What were the beneﬁts?\n3. What role did open source software play in the maturation of the second\nweb era?\n4. Describe the redundant reliability levels replaced by ROC-style distributed\ncomputing.\n5. The history discussed in this appendix is described as inevitable. Do you agree\nor disagree? Support your case.\n6. What era will follow cloud computing? Identify the trends described in\nthis appendix and extrapolate them to predict the future based on past\nperformance.\n\n\nThis page intentionally left blank \n",
      "page_number": 497
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 505-513)",
      "start_page": 505,
      "end_page": 513,
      "detection_method": "topic_boundary",
      "content": "Appendix C\nScaling Terminology and\nConcepts\nThe essence of life is statistical\nimprobability on a colossal scale.\n—Richard Dawkins\nYou’ve probably experienced software that works well with a small amount of data\nbut gets slower and slower as more data is added. Some systems get a little slower\nwith more data. With others, the slow-down is much bigger, often dramatically so.\nWhen discussing scalability, there is terminology that describes this phe-\nnomenon. This enables us to communicate with each other with great speciﬁcity.\nThis appendix describes some basic terminology, a more mathematical way of\ndescribing the same concepts, and ﬁnally some caveats for how modern systems\ndo or do not act as one would expect.\nC.1 Constant, Linear, and Exponential Scaling\nThere is a lot of terminology related to describing how systems perform and scale.\nThree terms are used so commonly that to be a professional system administrator\nyou should be able to use these terms conversationally. They describe how a system\nperforms as data size grows: the system is unaffected, gets slower, or gets much\nslower.\n• Constant Scaling: No matter the scale of the input, performance does not\nchange. For example, a hash-table lookup in RAM can always be done in con-\nstant time whether the table contains 100 items or 100 million items. It would\nbe nice if all systems were so fast, but such algorithms are rare. In fact, even a\nhash-table lookup is limited to situations where the data ﬁts in RAM.\n475\n\n\n476\nAppendix C\nScaling Terminology and Concepts\n• Linear Scaling: As input size grows, the system slows down proportionately.\nFor example, twice as much data will require twice as much processing time.\nSuppose an authentication system for a service with 10,000 users takes 10 ms to\nauthenticate each request. When there are 20,000 users, the system might take\n60 ms. With 30,000 users, the system might take 110 ms. Each unit of growth\nresulted in a slowdown that was the same size (50 ms slower per 10,000 users\nadded). Therefore we can classify this as a system with linear performance\nwith respect to the size of the user database.\n• Exponential Scaling: As input size grows, the system slows down dispro-\nportionately. Continuing our authentication system example, if adding more\nusers resulted in response times of 10 ms, 100 ms, and 1000 ms, this would\nbe exponential scaling. A system that slows down at such a rate, if it inter-\nacts with all parts of our system, would be a disaster! It might be ﬁne if the\ninput size is not expected to change and, at the current size, the system is fast\nenough. This assumption carries a high risk.\nC.2 Big O Notation\nO\n()\nor “Big O notation” is used to classify a system based on how it responds\nto changes in input size. It is the more mathematical way of describing a sys-\ntem’s behavior. O\n()\nnotation comes from computer science. The letter “O” is used\nbecause the growth rate of an algorithm’s run-time is known as its “order.”\nHere is a list of common Big O terms that a system administrator should be\nfamiliar with. The ﬁrst three we’ve already described:\n• O\n(\n1\n)\n: Constant Scaling: No change in performance no matter the size of the\ninput.\n• O\n(\nn\n)\n: Linear Scaling: Gets slower in proportion to the size of the input.\n• O\n(\nnm)\n: Exponential Scaling: Performance worsens exponentially.\n• O\n(\nn2)\n: Quadratic Scaling: Performance worsens relative to the square of the\nsize of input. Quadratic scaling is a special case of exponential scaling, where m\nis 2. People tend to refer to systems as scaling “exponentially” when they actu-\nally mean “quadratically.” This is so common that one rarely hears the term\n“quadratic” anymore except when someone is being very speciﬁc (or pedantic)\nabout the performance curve.\n• O\n(\nlog n\n)\n: Logarithmic Scaling: Performance worsens proportional to the log2\nof the size of input. Performance asymptotically levels off as input size grows.\nFor example, a binary search grows slower as the size of the corpus being\nsearched grows, but less than linearly.\n• O\n(\nn log n\n)\n: Loglinear Scaling: Performance worsens more than linearly, with\nthe “more than” component being proportional to log2 of the size of input.\nThink of this as linear scaling with a small but ever-growing performance\n\n\nC.2\nBig O Notation\n477\nsurcharge added as the input size grows. People often incorrectly use the term\n“logarithmic scaling” when they actually mean “loglinear scaling.” You can\nuse the term “loglinear” when you want to sound like a true expert.\n• O\n(\nn!\n)\n: Factorial Scaling: Performance worsens proportional to the factorial of\nthe size of input. In other words, performance gets bad so quickly that each\nadditional unit of size worsens performance by a leap as big as all previous\nleaps put together plus some more! O\n(\nn!\n)\nalgorithms are usually a worst-case\nscenario. For example, the breakthrough that permits Google PageRank and\nFacebook SocialRank to work so well came from computer science research\nthat invented replacements for O\n(\nn!\n)\nalgorithms.\nThe term sub-linear refers to anything that grows less than linearly, such as con-\nstant and logarithmic scaling. Super-linear refers to anything that grows faster,\nsuch as exponential and factorial scaling.\nIn addition to being used to describe scaling, these terms are often used to\ndescribe growth. One might describe the increase in customers being attracted to\nyour business as growing linearly or exponentially. The run-time of a system might\nbe described as growing in similar terms.\nSuper-linear systems sound awful compared to sub-linear systems. Why not\nalways use algorithms that are constant or linear? The simplest reason is that often\nalgorithms of that order don’t exist. Sorting algorithms have to touch every item\nat least once, eliminating the possibility of O\n(\n1\n)\nalgorithms. There is one O\n(\nn\n)\nsort\nalgorithm but it works on only certain kinds of data.\nAnother reason is that faster algorithms often require additional work ahead\nof time: for example, building an index makes future searches faster but requires\nthe overhead and complexity of building and maintaining the index. That effort of\ndeveloping, testing, and maintaining such indexing code may not be worth it if the\nsystem’s performance is sufﬁcient as is.\nAt small values, systems of different order may be equivalent. What’s big-\nger, x or x2? Your gut reaction may be that the square of something will be bigger\nthan the original number. However, if x is 0.5, then this is not true. 0.52 = 0.25,\nwhich is smaller than 0.5. Likewise, an O\n(\nn\n)\nalgorithm may be slower than an\nO\n(\nn2)\nalgorithm for very small inputs. Also an O\n(\nn2)\nalgorithm may be the easiest\nto implement, even though it wouldn’t be the most efﬁcient for larger-sized input.\nThus the more complex algorithm would be a waste of time to develop. It would\nalso be riskier. More complex code is more likely to have bugs.\nIt is important to remember that the concept of O\n()\nis a gross generaliza-\ntion that focuses on the trend as its input grows, not the speciﬁc run-time. For\nexample, two systems that are both O\n(\nn2)\nwill not to have the exact same per-\nformance. The shared order simply indicates that both scale worse than, say, an\nO\n(\nn\n)\nsystem.\nO\n()\nnotation is an indicator of the biggest factor, not all factors. A real sys-\ntem may be dominated by an O\n(\nn\n)\nlookup, but might also involve many O\n(\n1\n)\n\n\n478\nAppendix C\nScaling Terminology and Concepts\noperations and possibly even some O\n(\nn2)\noperations, albeit ones that are inconse-\nquential for other reasons.\nFor example, an API call might perform an O\n(\nn\n)\nlookup on a small list of\nauthorized users and then spend the majority of its time doing an O\n(\n1\n)\nlookup in\na large database. If the list of authorized users is tiny (say, it veriﬁes the user is one\nof three people permitted to use the system) and done in RAM, it is inconsequen-\ntial compared to the database lookup. Therefore this API call would be considered\nO\n(\n1\n)\n, not O\n(\nn\n)\n. Such a system may run at acceptable speed for years but then\nthe number of authorized users surges, possibly due to a management change\nthat authorizes thousands of additional users. Suddenly this O\n(\nn\n)\nlookup starts\nto dominate the run-time of the API call. Surprises like this happen all the time in\nproduction systems.\nWhile the “O” in Big O notation stands for “order,” conversationally you may\nhear the word “order” used as shorthand for order of magnitude. Order of mag-\nnitude is related but different. Order of magnitude means, essentially, how many\ndigits are in a number that describes a size (the magnitude of a digit literally means\nif it is in the 1s, 10s, 100s, or some other place). If you work for a company with\n100 employees and your friend works at a company with 1000 employees, then\nyour friend works at a company that is an order of magnitude larger. You may hear\nphrases like an algorithm working for situations “for 1 million users but breaks\ndown on larger order of magnitude.” You might hear references to a system pro-\ncessing “on the order of 2000 queries per second,” which is a fancy way of saying\n“approximately 2000” with the implication that this is a very coarse estimate.\nC.3 Limitations of Big O Notation\nO\n()\nnotation relates to the number of operations an algorithm will perform. How-\never, not all operations take the same amount of time. Reading two adjacent records\nfrom a database, if both ﬁt into the same disk block, is essentially one disk read fol-\nlowed by two reads from the RAM that caches the entire block. Two algorithms\nwith the same O\n()\norder, one that takes advantage of this fact and one that does\nnot, will have extremely different performance.\nIn Kamp’s (2010) description of the Varnish HTTP accelerator, he explains that\nby taking advantage of a deep understanding of how a modern OS handles disk\nI/O, signiﬁcantly better performance can be achieved than with naive O\n()\ncompar-\nisons. Varnish was able to replace 12 overloaded machines running a competing\nsoftware package with three machines that were 90 percent idle. He writes:\nWhat good is an O\n(\nlog2 n\n)\nalgorithm if those operations cause page faults and slow\ndisk operations? For most relevant datasets an O\n(\nn\n)\nor even an O\n(\nn2)\nalgorithm,\nwhich avoids page faults, will run circles around it.\n\n\nC.3\nLimitations of Big O Notation\n479\nThe improvement came from structuring his heap in such a way that nodes\nwould avoid generating virtual memory page faults. Kamp arranged data struc-\ntures such that parent and child nodes tended to be on the same page in virtual\nmemory.\nA page fault occurs when a program accesses virtual memory that is not cur-\nrently in the computer’s RAM. The operating system pauses the process, reads the\ndata from disk, and unpauses the process when the read is complete. Since mem-\nory is handled in 4KB blocks, the ﬁrst access to a block may cause a page fault but\nlater accesses within that same 4KB area do not.\nA page fault takes about as long as 10 million instructions could run. In other\nwords, spending 10,000 instructions to avoid a page fault has a 1000-to-1 payback.\nBy carefully organizing where in memory objects are stored, Kamp was able to\neliminate many page faults.\nHis message to the industry, titled “You’re Doing It Wrong,” details in very\ntechnical terms how most computer science education is still teaching algorithm\ndesign as appropriate for computer systems that have not existed for 30 years.\nModern CPUs have many complexities that make performance non-intuitive.\nLinear access is signiﬁcantly faster than random access. Instructions execute in par-\nallel when they are provably independent. The performance of a CPU drops when\nmultiple CPUs are accessing the same part of memory.\nFor example, reading every element in an array in row order is signiﬁcantly\nfaster than reading the elements in column order. If the rows are bigger than the\nCPU’s memory cache, the latter is essentially going to be a cache “miss” for every\nread. The former would be linear memory reads, which CPUs are optimized to do\nquickly. Even though either way reads the same number of bytes from memory,\nthe former is faster.\nMore surprisingly, a loop that reads every element of an array takes approxi-\nmately the same amount of time to execute as a loop that reads every 16th element\nof the same array. Even though the former does 1/16th of the number of opera-\ntions, the number of RAM cache misses is the same for both loops. Reading blocks\nof memory from RAM to the CPU’s L1 (Level 1) cache is slow and dominates the\nrun-time of either algorithm. The fact that the former runs faster is due to the fact\nthat there are special instructions for sequentially reading memory.\nAbove and beyond RAM, virtual memory, and disk issues, we also have to\nconsider the effect of threads, multiple CPUs trying to access the same data, locks,\nand mutexes. All of these cloud the issue. You can’t really tell how fast an algo-\nrithm will be without benchmarking. O\n()\nnotation becomes a general guide, not\nan absolute.\nThat said, O\n()\nnotation is still useful when conversationally describing\nsystems and communicating with other system administrators. Therefore under-\nstanding the nomenclature is essential to being a system administrator today.\n\n\nThis page intentionally left blank \n\n\nAppendix D\nTemplates and Examples\nD.1 Design Document Template\nBelow is a simple design document template as described in Section 13.2.\n.\nTitle:\nDate:\nAuthor(s): (add authors, please link to their email addresses)\nReviewers(s): (add reviewers, please link to their email addresses)\nApprovers(s): (add approvers, please link to their email addresses)\nRevision Number:\nStatus: (draft, in review, approved, in progress)\nExecutive Summary:\n(2–4 sentences explaining the project)\nGoals:\n(bullet list describing the problem being solved)\nNon-goals:\n(bullet list describing the limits of the project)\nBackground:\n(terminology and history one needs to know to understand the design)\nHigh-Level Design:\n(a brief, high-level description of the design; diagrams are helpful)\n481\n\n\n482\nAppendix D\nTemplates and Examples\n.\nDetailed Design:\n(the design in detail; usually a top-down description)\nAlternatives Considered:\n(alternatives considered and rejected; include pros and cons of each)\nSecurity Concerns:\n(security/privacy ramiﬁcations and how those concerns are mitigated)\nD.2 Design Document Example\nBelow is an example design document as described in Section 13.2.\n.\nTo illustrate the principle of design documents, we have created a sample\ndesign document with most of the parts included:\nTitle: New Monitoring System\nDate: 2014-07-19\nAuthor(s): Strata Chalup <src@example.com>\nand Tom Limoncelli <tal@example.com>\nReviewers: Joe, Mary, Jane\nApprovers: Security Operations 2014-08-03, Chris (Build System Tech Lead)\n2014-08-04, Sara (Director of Ops) 2014-08-10\nRevision Number: 1.0\nStatus: Approved\nExecutive Summary:\nCreate a monitoring system for devices that will support real-time alerting via\npagers and deploy to production cluster.\nGoals:\nA monitoring system for our network:\n• Be able to monitor at least 10,000 devices, 500 attributes each, collected\neach minute.\n\n\nD.2\nDesign Document Example\n483\n.\n• Must support real-time alerting via SMS.\n• Must retain data for 30 years.\nNon-goals:\n• Monitoring external systems.\n• Building chat functionality into the system.\nBackground:\nHistorically we have used home-brew monitoring scripts, but we keep having\nto adjust them based on changing platforms. We want to have a standardized\nway of monitoring that doesn’t require development time on our part as we\nadd platforms.\nHigh-Level Design:\nThe plan is to use Bosun with remote collectors. The server will be the current\ncorporate standard conﬁguration for server hardware and Linux. The server\nwill be named bosun01.example.com. It will be in the Phoenix and Toronto\ndatacenters. Monitoring conﬁguration will be kept in the Git “sysadmin con-\nﬁgs” repo, in the top-level directory /monitor.\nDetailed Design:\nServer conﬁguration:\nDell 720XD with 64G RAM, 8 disks in RAID6 conﬁguration\nDebian Linux (standard corporate “server conﬁguration”)\nBosun package name is “bosun-server.”\n• Any system being monitored will have the “bosun-client” package\ninstalled.\n• Backups will be performed nightly using the standard corporate backup\nmechanism.\n• (The full document would include a description of operational tasks such\nas adding new monitoring collectors, and alerting rules.)\nCost Projections:\n• Initial cost [cost of server].\n• Software is open source.\n• One half-time FTE for 3 weeks to set up monitoring and launch.\n",
      "page_number": 505
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 514-521)",
      "start_page": 514,
      "end_page": 521,
      "detection_method": "topic_boundary",
      "content": "484\nAppendix D\nTemplates and Examples\n.\nSchedule after approval:\n1. 1 week to procure hardware.\n2. 2 days of installation and testing.\n3. “Go live” on week 3.\nAlternatives Considered:\nZappix and SkunkStorm were both considered, but the feature set of Bosun is\nmore closely aligned with the project goals. [link to comparison charts]\nSpecial Constraints:\nBosun stores SNMP “community strings,” which are essentially passwords,\nin a password vault. The storage method has been reviewed and approved by\nSecurity Operations.\nD.3 Sample Postmortem Template\nBelow is a simple postmortem template as described in Section 14.3.2.\n.\nTitle:\nReport Status:\nExecutive Summary:\nList what happened, who was affected, and what are the key recommendations for\nprevention in the future (especially any that will require budget or executive approval).\nOutage Description:\nA general description of the outage, from a technical perspective of what hap-\npened.\nAffected users:\nWho was affected.\nStart Date/Time:\nEnd Date/Time:\nDuration:\nTimeline:\nA minute-by-minute timeline assembled from system logs, chat logs, emails, and\nwhatever other resources are available.\nContributing Conditions Analysis:\nWhat were the contributing causes that led to the outage?\n\n\nD.3\nSample Postmortem Template\n485\n.\nWhat went well?\nA bullet list of what went well. This is a good opportunity to thank anyone who\nwent above and beyond expectations to help out.\nWhat could have gone better?\nA bullet list of which actions could have been taken that would have improved\nhow fast we were back in service, the techniques used, and so on.\nRecommendations:\nA bullet list of recommendations that would prevent this outage in the future.\nEach should be actionable and measurable. Good example: “Monitor disk space for\ndatabase server and alert if less than 20 percent is available.” Bad example: “Improve\nmonitoring.” File a bug/feature request for each recommendation; list bug IDs here.\nNames of people involved:\nList of the people involved in the resolution of the outage.\n\n\nThis page intentionally left blank \n\n\nAppendix E\nRecommended Reading\nTo achieve great things, two things\nare needed; a plan, and\nnot quite enough time.\n—Leonard Bernstein\nDevOps:\n• The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win\n(Kim et al. 2013)\nA ﬁctional story that teaches the Three Ways of DevOps.\n• Building a DevOps Culture (Walls 2013)\nStrategies for building a DevOps culture, including aligning incentives,\ndeﬁning meaningful and achievable goals, and creating a supportive team\nenvironment.\n• Continuous Delivery: Reliable Software Releases through Build, Test, and Deploy-\nment Automation (Humble & Farley 2010)\nThe canonical book about service delivery platforms.\n• Release It!: Design and Deploy Production-Ready Software (Nygard 2007)\nDetailed coverage and examples of how to implement many of the ideas in\nChapter 11.\n• Blameless PostMortems and a Just Culture (Allspaw 2009)\nTheory and practice of postmortems.\n• A Mature Role for Automation (Allspaw 2012c)\nWhy “Automate Everything!” is bad, and what to do instead.\n• Each Necessary, But Only Jointly Sufﬁcient (Allspaw 2012a)\nMyths and limits of “root cause analysis” and “The Five Why’s.”\n487\n\n\n488\nAppendix E\nRecommended Reading\nITIL:\n• Owning ITIL: A Skeptical Guide for Decision-Makers (England 2009)\nA gentle introduction to ITIL with honest advice about how to make it work.\nTheory:\n• In Search of Certainty: The Science of Our Information Infrastructure (Burgess 2013)\nThe physics of distributed systems.\n• Promise Theory: Principles and Applications (Burgess & Bergstra 2014)\nThe theory of conﬁguration management and change management.\nClassic Google Papers:\n• “The Anatomy of a Large-Scale Hypertextual Web Search Engine” (Brin &\nPage 1998)\nThe ﬁrst paper describing the Google search engine.\n• “Web Search for a Planet: The Google Cluster Architecture” (Barroso, Dean &\nHölzle 2003)\nGoogle’s ﬁrst paper revealing how clusters are designed using commodity\nclass PCs with fault-tolerant software.\n• “The Google File System” (Ghemawat, Gobioff & Leung 2003)\n• ”MapReduce: Simpliﬁed Data Processing on Large Clusters” (Dean &\nGhemawat 2004)\n• “Bigtable: A Distributed Storage System for Structured Data” (Chang et al.\n2006)\n• “The Chubby Lock Service for Loosely-Coupled Distributed Systems”\n(Burrows 2006)\n• “Spanner: Google’s Globally-Distributed Database” (Corbett et al. 2012)\n• “The Tail at Scale” (Dean & Barroso 2013)\nImproving latency means ﬁxing the last percentile.\n• “Failure Trends in a Large Disk Drive Population” (Pinheiro, Weber & Barroso\n2007)\nLongitudinal study of hard drive failures.\n• “DRAM Errors in the Wild: A Large-Scale Field Study” (Schroeder, Pinheiro &\nWeber 2009)\nLongitudinal study of DRAM failures.\nClassic Facebook Papers:\n• “Cassandra: A Decentralized Structured Storage System” (Lakshman & Malik\n2010)\n\n\nRecommended Reading\n489\nScalability:\n• The Art of Scalability: Scalable Web Architecture, Processes, and Organizations for\nthe Modern Enterprise (Abbott & Fisher 2009)\nAn extensive catalog of techniques and discussion of scalability of people,\nprocess, and technology.\n• Scalability Rules: 50 Principles for Scaling Web Sites (Abbott & Fisher 2011)\nA slimmer volume, focused on technical strategy and techniques.\nUNIX Internals:\n• The Design and Implementation of the FreeBSD Operating System (McKusick,\nNeville-Neil & Watson 2014)\nThis is the best deep-dive into how UNIX-like operating systems work.\nWhile the examples are all FreeBSD, Linux users will beneﬁt from the theory\nand technical details the book provides.\nUNIX Systems Programming:\n• The UNIX Programming Environment (Kernighan & Pike 1984)\n• Advanced Programming in the UNIX Environment (Stevens & Rago 2013)\n• UNIX Network Programming (Stevens 1998)\nNetwork Protocols:\n• TCP/IP Illustrated, Volume 1: The Protocols (Stevens & Fall 2011)\n• TCP/IP Illustrated, Volume 2: The Implementation (Wright & Stevens 1995)\n• TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP, and the UNIX\nDomain Protocols (Stevens & Wright 1996)\n\n\nThis page intentionally left blank \n\n\nBibliography\nAbbott, M., & Fisher, M. (2009). The Art of Scalability: Scalable Web Architecture, Processes, and\nOrganizations for the Modern Enterprise, Pearson Education.\nAbbott, M., & Fisher, M. (2011). Scalability Rules: 50 Principles for Scaling Web Sites, Pearson\nEducation.\nAbts, D., & Felderman, B. (2012). A guided tour through data-center networking, Queue\n10(5): 10:10–10:23.\nhttp://queue.acm.org/detail.cfm?id=2208919\nAdya, A., Cooper, G., Myers, D., & Piatek, M. (2011). Thialﬁ: A client notiﬁcation service\nfor internet-scale applications, Proceedings of the 23rd ACM Symposium on Operating\nSystems Principles (SOSP), pp. 129–142.\nhttp://research.google.com/pubs/pub37474.html\nAllspaw, J. (2009). Blameless postmortems and a just culture.\nhttp://codeascraft.com/2012/05/22/blameless-postmortems.\nAllspaw, J. (2012a). Each necessary, but only jointly sufﬁcient.\nhttp://www.kitchensoap.com/2012/02/10/each-necessary-but-only-jointly-\nsufficient.\nAllspaw, J. (2012b). Fault injection in production, Queue 10(8): 30:30–30:35.\nhttp://queue.acm.org/detail.cfm?id=2353017\nAllspaw, J. (2012c). A mature role for automation.\nhttp://www.kitchensoap.com/2012/09/21/a-mature-role-for-automation-\npart-i\nAnderson, C. (2012). Idea ﬁve: Software will eat the world, Wired.\nhttp://www.wired.com/business/2012/04/ff_andreessen/5\nArmbrust, M., Fox, A., Grifﬁth, R., Joseph, A. D., Katz, R., Konwinski, A., Lee, G.,\nPatterson, D., Rabkin, A., Stoica, I., & Zaharia, M. (2010). A view of cloud computing,\nCommunications of the ACM 53(4): 50–58.\nhttp://cacm.acm.org/magazines/2010/4/81493-a-view-of-cloud-computing\nBarroso, L. A., Dean, J., & Hölzle, U. (2003). Web search for a planet: The Google cluster\narchitecture, IEEE Micro 23(2): 22–28.\nhttp://research.google.com/archive/googlecluster.html\n491\n",
      "page_number": 514
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 522-529)",
      "start_page": 522,
      "end_page": 529,
      "detection_method": "topic_boundary",
      "content": "492\nBibliography\nBarroso, L. A., Clidaras, J., & Hölzle, U. (2013). The Datacenter as a Computer: An Introduction\nto the Design of Warehouse-Scale Machines, 2nd ed., Morgan and Claypool Publishers.\nhttp://research.google.com/pubs/pub41606.html\nBeck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M.,\nGrenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R. C.,\nMellor, S., Schwaber, K., Sutherland, J., & Thomas, D. (2001). Manifesto for agile\nsoftware development.\nhttp://agilemanifesto.org\nBlack, B. (2009). EC2 origins.\nhttp://blog.b3k.us/2009/01/25/ec2-origins.html\nBrandt, K. (2014). OODA for sysadmins.\nhttp://blog.serverfault.com/2012/07/18/ooda-for-sysadmins\nBrin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine,\nProceedings of the Seventh International Conference on World Wide Web 7, WWW7,\nElsevier Science Publishers, Amsterdam, Netherlands, pp. 107–117.\nhttp://dl.acm.org/citation.cfm?id=297805.297827\nBurgess, M. (2013). In Search of Certainty: The Science of Our Information Infrastructure,\nCreatespace Independent Publishing.\nBurgess, M., & Bergstra, J. (2014). Promise Theory: Principles and Applications, On Demand\nPublishing, Create Space.\nBurrows, M. (2006). The Chubby lock service for loosely-coupled distributed systems,\nProceedings of the 7th Symposium on Operating Systems Design and Implementation,\nOSDI ’06, USENIX Association, Berkeley, CA, pp. 335–350.\nhttp://research.google.com/archive/chubby.html\nCandea, G., & Fox, A. (2003). Crash-only software, HotOS, pp. 67–72.\nhttps://www.usenix.org/conference/hotos-ix/crash-only-software\nChang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T.,\nFikes, A., & Gruber, R. E. (2006). Bigtable: A distributed storage system for structured\ndata, Proceedings of the 7th USENIX Symposium on Operating Systems Design and\nImplementation, pp. 205–218.\nChapman, B. (2005). Incident command for IT: What we can learn from the ﬁre\ndepartment., LISA, USENIX.\nhttp://www.greatcircle.com/presentations\nCheswick, W. R., Bellovin, S. M., & Rubin, A. D. (2003). Firewalls and Internet Security:\nRepelling the Wiley Hacker, Addison-Wesley.\nhttp://books.google.com/books?id=_ZqIh0IbcrgC\nClos, C. (1953). A study of non-blocking switching networks, The Bell System Technical\nJournal 32(2): 406–424.\nhttp://www.alcatel-lucent.com/bstj/vol32-1953/articles/bstj32-2-406.pdf\nCorbett, J. C., Dean, J., Epstein, M., Fikes, A., Frost, C., Furman, J. J., Ghemawat, S.,\nGubarev, A., Heiser, C., Hochschild, P., Hsieh, W., Kanthak, S., Kogan, E., Li, H.,\nLloyd, A., Melnik, S., Mwaura, D., Nagle, D., Quinlan, S., Rao, R., Rolig, L., Saito, Y.,\n\n\nBibliography\n493\nSzymaniak, M., Taylor, C., Wang, R., & Woodford, D. (2012). Spanner: Google’s\nglobally-distributed database, Proceedings of the 10th USENIX Conference on Operating\nSystems Design and Implementation, OSDI’12, USENIX Association, Berkeley, CA,\npp. 251–264.\nhttp://research.google.com/archive/spanner.html\nDean, J. (2009). Designs, lessons and advice from building large distributed systems.\nhttp://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-\nladis2009.pdf\nDean, J., & Barroso, L. A. (2013). The tail at scale, Communications of the ACM 56(2): 74–80.\nhttp://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale\nDean, J., & Ghemawat, S. (2004). MapReduce: Simpliﬁed data processing on large clusters,\nOSDI’04: Proceedings of the 6th USENIX Symposium on Operating Systems Design and\nImplementation, USENIX Association.\nDebois, P. (2010a). Jedi4Ever blog.\nhttp://www.jedi.be/blog\nDebois, P. (2010b). DevOps is a verb. Slideshare of talk.\nhttp://www.slideshare.net/jedi4ever/devops-is-a-verb-its-all-about-\nfeedback-13174519\nDeming, W. (2000). Out of the Crisis, Massachusetts Institute of Technology, Center for\nAdvanced Engineering Study.\nDevOps-Toolchain. (2010). A set of best practices useful to those practicing DevOps\nmethodology.\nhttp://code.google.com/p/devops-toolchain/wiki/BestPractices\nDickson, C. (2013). A working theory of monitoring, presented as part of the 27th Large\nInstallation System Administration Conference, USENIX, Berkeley, CA.\nhttps://www.usenix.org/conference/lisa13/working-theory-monitoring\nEdwards, D. (2010). DevOps is not a technology problem.\nhttp://dev2ops.org/2010/11/devops-is-not-a-technology-problem-devops-\nis-a-business-problem\nEdwards, D. (2012). Use DevOps to turn IT into a strategic weapon.\nhttp://dev2ops.org/2012/09/use-devops-to-turn-it-into-a-strategic-\nweapon\nEdwards, D., & Shortland, A. (2012). Integrating DevOps tools into a service delivery\nplatform.\nhttp://dev2ops.org/2012/07/integrating-devops-tools-into-a-service-\ndelivery-platform-video\nEngland, R. (2009). Owning ITIL: A Skeptical Guide for Decision-Makers, Two Hills.\nFitts, P. (1951). Human engineering for an effective air navigation and trafﬁc-control\nsystem, Technical Report ATI-133954, Ohio State Research Foundation.\nhttp://www.skybrary.aero/bookshelf/books/355.pdf\nFlack, M., & Wiese, K. (1977). The Story about Ping, Picture Pufﬁn Books, Pufﬁn\nBooks.\n\n\n494\nBibliography\nGallagher, S. (2012). Built to win: Deep inside Obama’s campaign tech.\nhttp://arstechnica.com/information-technology/2012/11/built-to-win-\ndeep-inside-obamas-campaign-tech\nGhemawat, S., Gobioff, H., & Leung, S.-T. (2003). The Google ﬁle system, Proceedings of the\nNineteenth ACM Symposium on Operating Systems Principles, SOSP ’03, ACM,\nNew York, NY, pp. 29–43.\nhttp://doi.acm.org/10.1145/945445.945450\nGilbert, S., & Lynch, N. (2002). Brewer’s conjecture and the feasibility of consistent,\navailable, partition-tolerant web services, SIGACT News 33(2): 51–59.\nhttp://doi.acm.org/10.1145/564585.564601\nGoogle. (2012). Efﬁciency: How we do it.\nhttp://www.google.com/about/datacenters/efficiency/internal\nGruver, G., Young, M., & Fulghum, P. (2012). A Practical Approach to Large-Scale Agile\nDevelopment: How HP Transformed HP LaserJet FutureSmart Firmware, Addison-Wesley.\nHaynes, D. (2013). Understanding CPU steal time: When should you be worried.\nhttp://blog.scoutapp.com/articles/2013/07/25/understanding-cpu-steal-\ntime-when-should-you-be-worried.\nHickstein, J. (2007). Sysadmin slogans.\nhttp://www.jxh.com/slogans.html\nHohpe, G., & Woolf, B. (2003). Enterprise Integration Patterns: Designing, Building, and\nDeploying Messaging Solutions, Addison-Wesley Longman, Boston, MA.\nHumble, J., & Farley, D. (2010). Continuous Delivery: Reliable Software Releases through Build,\nTest, and Deployment Automation, Addison-Wesley Professional.\nJacob, A. (2010). Choose your own adventure: Adam Jacob on DevOps.\nhttp://www.youtube.com/watch?v=Fx8OBeNmaWw\nKamp, P.-H. (2010). You’re doing it wrong, Communications of the ACM 53(7): 55–59.\nhttp://queue.acm.org/detail.cfm?id=1814327\nKartar, J. (2010). What DevOps means to me.\nhttp://www.kartar.net/2010/02/what-devops-means-to-me\nKernighan, B., & Pike, R. (1984). The UNIX Programming Environment, Prentice Hall.\nKernighan, B., & Plauger, P. (1978). The Elements of Programming Style, McGraw-Hill.\nKim, G., Behr, K., & Spafford, G. (2013). The Phoenix Project: A Novel about IT, DevOps, and\nHelping Your Business Win, IT Revolution Press.\nhttp://books.google.com/books?id=mqXomAEACAAJ\nKlau, R. (2012). How Google sets goals: OKRs.\nhttp://gv.com/1322\nKrishnan, K. (2012). Weathering the unexpected, Queue 10(9): 30:30–30:37.\nhttp://queue.acm.org/detail.cfm?id=2371516\nLakshman, A., & Malik, P. (2010). Cassandra: A decentralized structured storage system,\nSIGOPS Operating Systems Review 44(2): 35–40.\nhttp://doi.acm.org/10.1145/1773912.1773922\n\n\nBibliography\n495\nLamport, L. (2001). Paxos made simple, SIGACT News 32(4): 51–58.\nhttp://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf\nLamport, L., & Marzullo, K. (1998). The part-time parliament, ACM Transactions on\nComputer Systems 16: 133–169.\nLee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance,\nHuman Factors 46(1): 50–80.\nLetuchy, E. (2008). Facebook Chat.\nhttps://www.facebook.com/note.php?note_id=14218138919\nLevinson, M. (2008). The Box: How the Shipping Container Made the World Smaller and the\nWorld Economy Bigger, Princeton University Press.\nLevy, S. (2012). Google throws open doors to its top-secret data center, Wired 20(11).\nhttp://www.wired.com/wiredenterprise/2012/10/ff-inside-google-data-\ncenter/all\nLimoncelli, T. A. (2005). Time Management for System Administrators, O’Reilly and\nAssociates.\nLimoncelli, T. (2012). Google DiRT: The view from someone being tested, Queue\n10(9): 35:35–35:37.\nhttp://queue.acm.org/detail.cfm?id=2371516#sidebar\nLimoncelli, T. A., Hogan, C., & Chalup, S. R. (2015). The Practice of System and Network\nAdministration, 3rd ed., Pearson Education.\nLink, D. (2013). Netﬂix and stolen time.\nhttp://blog.sciencelogic.com/netflix-steals-time-in-the-cloud-and-from-\nusers/03/2011\nMadrigal, A. C. (2012). When the nerds go marching in: How a dream team of engineers\nfrom Facebook, Twitter, and Google built the software that drove Barack Obama’s\nreelection.\nhttp://www.theatlantic.com/technology/archive/2012/11/when-the-nerds-\ngo-marching-in/265325\nMcKinley, D. (2012). Why MongoDB never worked out at Etsy.\nhttp://mcfunley.com/why-mongodb-never-worked-out-at-etsy\nMcKusick, M. K., Neville-Neil, G., & Watson, R. N. (2014). The Design and Implementation of\nthe FreeBSD Operating System, Prentice Hall.\nMegiddo, N., & Modha, D. S. (2003). ARC: A self-tuning, low overhead replacement cache,\nProceedings of the 2nd USENIX Conference on File and Storage Technologies, FAST ’03,\nUSENIX Association, Berkeley, CA, pp. 115–130.\nhttps://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-\nreplacement-cache\nMetz, C. (2013). Return of the Borg: How Twitter rebuilt Google’s secret weapon, Wired.\nhttp://www.wired.com/wiredenterprise/2013/03/google-borg-twitter-\nmesos/all\nNygard, M. T. (2007). Release It!: Design and Deploy Production-Ready Software, Pragmatic\nBookshelf.\n\n\n496\nBibliography\nOppenheimer, D. L., Ganapathi, A., & Patterson, D. A. (2003). Why do Internet services\nfail, and what can be done about it?, USENIX Symposium on Internet Technologies and\nSystems.\nParasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of\nhuman interaction with automation, Transactions on Systems, Man, and Cybernetics\nPart A 30(3): 286–297.\nhttp://dx.doi.org/10.1109/3468.844354\nPatterson, D., Brown, A., Broadwell, P., Candea, G., Chen, M., Cutler, J., Enriquez, P., Fox,\nA., Kiciman, E., Merzbacher, M., Oppenheimer, D., Sastry, N., Tetzlaff, W.,\nTraupman, J., & Treuhaft, N. (2002). Recovery Oriented Computing (ROC):\nMotivation, deﬁnition, techniques, and case studies, Technical Report\nUCB/CSD-02-1175, EECS Department, University of California, Berkeley, CA.\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2002/5574.html\nPfeiffer, T. (2012). Why waterfall was a big misunderstanding from the beginning: Reading\nthe original paper.\nhttp://pragtob.wordpress.com/2012/03/02/why-waterfall-was-a-big-\nmisunderstanding-from-the-beginning-reading-the-original-paper\nPinheiro, E., Weber, W.-D., & Barroso, L. A. (2007). Failure trends in a large disk drive\npopulation, 5th USENIX Conference on File and Storage Technologies (FAST 2007),\npp. 17–29.\nRachitsky, L. (2010). A guideline for postmortem communication.\nhttp://www.transparentuptime.com/2010/03/guideline-for-postmortem-\ncommunication.html\nRichard, D. (2013). Gamedays on the Obama campaign.\nhttp://velocityconf.com/velocity2013/public/schedule/detail/28444;\nhttp://www.youtube.com/watch?v=LCZT_Q3z520\nRobbins, J., Krishnan, K., Allspaw, J., & Limoncelli, T. (2012). Resilience engineering:\nLearning to embrace failure, Queue 10(9): 20:20–20:28.\nhttp://queue.acm.org/detail.cfm?id=2371297\nRockwood, B. (2013). Why SysAdmin’s can’t code.\nhttp://cuddletech.com/?p=817\nRossi, C. (2011). Facebook: Pushing millions of lines of code ﬁve days a week.\nhttps://www.facebook.com/video/video.php?v=10100259101684977\nRoyce, D. W. W. (1970). Managing the development of large software systems: Concepts\nand techniques.\nSchlossnagle, T. (2011). Career development.\nhttp://www.youtube.com/watch?v=y0mHo7SMCQk\nSchroeder, B., Pinheiro, E., & Weber, W.-D. (2009). DRAM errors in the wild: A large-scale\nﬁeld study, SIGMETRICS.\nSchwarzkopf, M., Konwinski, A., Abd-El-Malek, M., & Wilkes, J. (2013). Omega: Flexible,\nscalable schedulers for large compute clusters, SIGOPS European Conference on\nComputer Systems (EuroSys), Prague, Czech Republic, pp. 351–364.\nhttp://research.google.com/pubs/pub41684.html\n\n\nBibliography\n497\nSeven, D. (2014). Knightmare: A DevOps cautionary tale.\nhttp://dougseven.com/2014/04/17/knightmare-a-devops-cautionary-tale\nSiegler, M. (2011). The next 6 months worth of features are in Facebook’s code right now\n(but we can’t see).\nhttp://techcrunch.com/2011/05/30/facebook-source-code\nSpear, S., & Bowen, H. K. (1999). Decoding the DNA of the Toyota production system,\nHarvard Business Review.\nSpolsky, J. (2004). Things you should never do, Part I, Joel on Software, Apress.\nhttp://www.joelonsoftware.com/articles/fog0000000069.html\nStevens, W. (1998). UNIX Network Programming: Interprocess Communications, UNIX\nNetworking Reference Series, Vol. 2, Prentice Hall.\nStevens, W. R., & Fall, K. (2011). TCP/IP Illustrated, Volume 1: The Protocols, Pearson\nEducation.\nhttp://books.google.com/books?id=a23OAn5i8R0C\nStevens, W., & Rago, S. (2013). Advanced Programming in the UNIX Environment, Pearson\nEducation.\nStevens, W., & Wright, G. (1996). TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP,\nNNTP, and the UNIX Domain Protocols, Addison-Wesley.\nTseitlin, A. (2013). The antifragile organization, Queue 11(6): 20:20–20:26.\nhttp://queue.acm.org/detail.cfm?id=2499552\nTufte, E. R. (1986). The Visual Display of Quantitative Information, Graphics Press,\nCheshire, CT.\nWalls, M. (2013). Building a DevOps Culture, O’Reilly Media.\nWillis, J., & Edwards, D. (2011). Interview with Jesse Robbins.\nhttp://www.opscode.com/blog/2011/09/22/jesse-robbins-interview-on-\ndevops-cafe-19-w-full-transcript\nWillis, J., Edwards, D., & Humble, J. (2012). DevOps Cafe Episode 33: Finally it’s Jez.\nhttp://devopscafe.org/show/2012/9/26/devops-cafe-episode-33.html\nWright, G., & Stevens, W. (1995). TCP/IP Illustrated, Volume 2: The Implementations, Pearson\nEducation.\nYan, B., & Kejariwal, A. (2013). A systematic approach to capacity planning in the real\nworld.\nhttp://www.slideshare.net/arunkejariwal/a-systematic-approach-to-\ncapacity-planning-in-the-real-world\n\n\nThis page intentionally left blank \n\n\nIndex\nA-B testing, 232–233\nAAA (authentication, authorization, and\naccounting), 222\nAbbott, M., 99–100\nAbstracted administration in\nDevOps, 185\nAbstraction in loosely coupled\nsystems, 24\nAbts, D., 137\nAccess Control List (ACL) mechanisms\ndescription, 40\nGoogle, 41\nAccess controls in design for operations,\n40–41\nAccount creation automation example,\n251–252\nAccuracy, automation for, 253\nACID databases, 24\nAcknowledgments for alert messages,\n355–356\nACL (Access Control List) mechanisms\ndescription, 40\nGoogle, 41\nActive-active pairs, 126\nActive users, 366\nAdaptive Replacement Cache (ARC)\nalgorithm, 107\nAdopting design documents, 282–283\nAdvertising systems in second web\nera, 465\nAfter-hours oncall maintenance\ncoordination, 294\nAgents in collections, 352\nAggregators, 352\nAgile Alliance, 189\nAgile Manifesto, 189\nAgile techniques, 180\ncontinuous delivery, 188–189\nfeature requests, 264\nAKF Scaling Cube, 99\ncombinations, 104\nfunctional and service splits,\n101–102\nhorizontal duplication, 99–101\nlookup-oriented splits, 102–104\nAlerts, 163, 285\nalerting and escalation systems, 345,\n354–357\nmonitoring, 333\noncall for. See Oncall\nrules, 353\nthresholds, 49\nAlexander, Christopher, 69\nAllen, Woody, 285\nAllspaw, John\nautomation, 249\ndisaster preparedness tests, 318–320\noutage factors, 302\npostmortems, 301\nAlternatives in design documents, 278\nAmazon\ndesign process, 276\nGame Day, 318\nSimple Queue Service, 85\nAmazon Elastic Compute Cloud\n(Amazon EC2), 472\nAmazon Web Services (AWS), 59\nAnalysis\nin capacity planning, 375–376\ncausal, 301–302\n499\n",
      "page_number": 522
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 530-537)",
      "start_page": 530,
      "end_page": 537,
      "detection_method": "topic_boundary",
      "content": "500\nIndex\nAnalysis (continued)\ncrash reports, 129\nin monitoring, 345, 353–354\nAncillary resources in capacity\nplanning, 372\nAndreessen, Marc, 181\nAnomaly detection, 354\n“Antifragile Organization” article,\n315, 320\nAntifragile systems, 308–310\nApache systems\nHadoop, 132, 467\nMesos, 34\nweb server forking, 114\nZookeeper, 231, 363\nAPI (Application Programming\nInterface)\ndeﬁned, 10\nlogs, 340\nApplicability in dot-bomb era,\n463–464\nApplication architectures, 69\ncloud-scale service, 80–85\nexercises, 93\nfour-tier web service, 77–80\nmessage bus, 85–90\nreverse proxy service, 80\nservice-oriented, 90–92\nsingle-machine web servers, 70–71\nsummary, 92–93\nthree-tier web service, 71–77\nApplication debug logs, 340\nApplication logs, 340\nApplication Programming Interface\n(API)\ndeﬁned, 10\nlogs, 340\nApplication servers in four-tier web\nservice, 79\nApprovals\ncode, 47–48\ndeployment phase, 214, 216–217\ndesign documents, 277, 281\nservice launches, 159\nArbitrary groups, segmentation\nby, 104\nARC (Adaptive Replacement Cache)\nalgorithm, 107\nArchitecture factors in service launches,\n157\nArchives\ndesign documents, 279–280\nemail, 277\nArt of Scalability, Scalable Web\nArchitecture, Processes, and\nOrganizations for the Modern\nEnterprise, 100\nArtifacts\nartifact-scripted database changes,\n185\ndeﬁned, 196\nAssessments, 421–422\nCapacity Planning, 431–432\nChange Management, 433–434\nDisaster Preparedness, 448–450\nEmergency Response, 426–428\nlevels, 405–407\nmethodology, 403–407\nMonitoring and Metrics, 428–430\nNew Product Introduction and\nRemoval, 435–436\noperational excellence, 405–407\norganizational, 411–412\nPerformance and Efﬁciency, 439–441\nquestions, 407\nRegular Tasks, 423–425\nService Delivery: The Build Phase,\n442–443\nService Delivery: The Deployment\nPhase, 444–445\nService Deployment and\nDecommissioning, 437–438\nservices, 407–410\nToil Reduction, 446–447\nAsynchronous design, 29\nAtlassian Bamboo tool, 205\nAtomicity\nACID term, 24\nrelease, 240–241\nAttack surface area, 79\nAuditing operations design, 42–43\nAugmentation ﬁles, 41–42\nAuthentication, authorization, and\naccounting (AAA), 222\nAuthentication in deployment phase,\n222\nAuthors in design documents, 277, 282\nAuto manufacturing automation\nexample, 251\n\n\nIndex\n501\nAutomation, 243–244\napproaches, 244–245\nbaking, 219\nbeneﬁts, 154\ncode amount, 269–270\ncode reviews, 268–269\ncompensatory principle, 246–247\ncomplementarity principle, 247–248\ncontinuous delivery, 190\ncrash data collection and analysis,\n129\ncreating, 255–258\nDevOps, 182, 185–186\nexercises, 272–273\ngoals, 252–254\nhidden costs, 250\ninfrastructure strategies, 217–220\nissue tracking systems, 263–265\nlanguage tools, 258–262\nleft-over principle, 245–246\nlessons learned, 249–250\nmultitenant systems, 270–271\nprioritizing, 257–258\nrepair life cycle, 254–255\nsoftware engineering tools and\ntechniques, 262–270\nsoftware packaging, 266\nsoftware restarts and escalation,\n128–129\nsteps, 258\nstyle guides, 266–267, 270\nsummary, 271–272\nsystem administration, 248–249\ntasks, 153–155\ntest-driven development, 267–268\ntoil reduction, 257\nvs. tool building, 250–252\nversion control systems, 265–266\nAvailability\nCAP Principle, 21–22\nmonitoring, 336\nAvailability and partition tolerance\n(AP), 24\nAvailability requirements\ncloud computing era, 469\ndot-bomb era, 460\nﬁrst web era, 455\npre-web era, 452–453\nsecond web era, 465\nAverages in monitoring, 358\nAWS (Amazon Web Services), 59\nBackend replicas, load balancers with,\n12–13\nBackends\nmultiple, 14–15\nserver stability, 336\nBackground in design documents,\n277–278\nBackground processes for containers, 61\nBackups in design for operations, 36\nBaked images for OS installation,\n219–220\nBanned query lists, 130\nBare metal clouds, 68\nBarroso, L. A.\ncanary requests, 131\ncost comparisons, 464\ndisk drive failures, 133, 338\nBASE (Basically Available Soft-state\nservices) databases, 24\nBaseboard Management Controller\n(BMC), 218\nBasecamp application, 55\nbash (Bourne Again Shell), 259\nBatch size in DevOps, 178–179\nBathtub failure curve, 133\nBeck, K., 189\nBehaviors in KPIs, 390–391\nBehr, K., 172\nBellovin, S. M., 79\nBenchmarks in service platform\nselection, 53\nBernstein, Leonard, 487\nBerra, Yogi, 331\nBibliography, 491–497\nBidirectional learning in code review\nsystems, 269\nBig O notation, 476–479\nBigtable storage system, 24\nBimodal patterns in histograms, 361\nBIOS settings in deployment phase,\n218\nBlackbox monitoring, 346–347\nBlacklists, 40–42\nBlade servers, 217–218\n“Blameless Postmortems and a Just\nCulture” article, 301\n\n\n502\nIndex\nBlog Search, upgrading, 226\nBlue-green deployment, 230\nBMC (Baseboard Management\nController), 218\nBotnets, 140\nBots in virtual ofﬁces, 166–167\nBottlenecks\nautomation for, 257\nDevOps, 179\nidentifying, 96\nBourne Again Shell (bash), 259\nBowen, H. K., 172\nBoyd, John, 296\nBSD UNIX, 460\nBuckets in histograms, 361\nBuffer thrashing, 71\nBugs\ncode review systems, 269\nvs. feature requests, 263\nﬂag ﬂips for, 232\nlead time, 201\nmonitoring, 336\nnew releases, 178\npriority for, 150\nunused code, 270\nBuilds\nassessments, 442–443\nbuild console, 205\nbuild step, 203–204\ncommit step, 202–203\ncontinuous deployment, 237\ncontinuous integration, 205–207\ndevelop step, 202\nDevOps, 185–186\nexercises, 209\noverview, 195–196\npackage step, 204\npackages as handoff interface, 207–208\nregister step, 204\nservice delivery strategies, 197–200\nsteps overview, 202–204\nsummary, 208–209\nversion-controlled, 191\nvirtuous cycle of quality, 200–201\nwaterfall methodology, 199\n“Built to Win: Deep Inside Obama’s\nCampaign Tech” article, 307\nBusiness impact in alert messages, 355\nBusiness listings in Google Maps, 42\nc-SOX requirements, 43\nCache hit ratio, 105, 109\nCache hits, 104\nCache misses, 104\nCaches, 104–105\neffectiveness, 105\npersistence, 106\nplacement, 106\nreplacement algorithms, 107\nsize, 108–110\nCalendar documents for oncall\nschedules, 290–291\nCanary process for upgrading services,\n227–228\nCanary requests, 131\nCandea, G., 35\nCAP Principle, 21–24\nCapability Maturity Model (CMM),\n405–407\nCapability monitoring, 348\nCapacity factors in service launches, 158\nCapacity models, 374\nCapacity planning (CP), 365\nadvanced, 371–381\nassessments, 431–432\ncapacity limits, 366, 372–373\ncore drivers, 373–374\ncurrent usage, 368–369\ndata analysis, 375–380\ndelegating, 381\nengagement measuring, 374–375\nexercises, 386\nheadroom, 370\nkey indicators, 380–381\nlaunching new services, 382–384\nmonitoring, 335\nnormal growth, 369\noperational responsibility, 404\nplanned growth, 369–370\nprimary resources, 372\nprovisioning time, 384–385\nresiliency, 370–371\nresource regression, 381–382\nstandard, 366–371\nsummary, 385–386\ntimetables, 371\nCart size monitoring, 336\nCascade load balancing, 74\nCassandra system, 24\n\n\nIndex\n503\nCausal analysis, 301–302\nCCA (contributing conditions analysis),\n301\nCDNs (content delivery networks),\n114–116\nCentral collectors, 352–353\nCertiﬁcate management, 79\nCFEngine system\nconﬁguration management, 261\ndeployment phase, 213\nChalup, S. R., 204\nChange\ndocumenting, 276\nlimits, 41\nvs. stability, 149–151\nsuccess rate, 201\nversion control systems, 265\nChange-instability cycles, 150\nChange Management (CM)\nassessments, 433–434\noperational responsibility, 404\nChannels in message bus architectures,\n86\nChaos Gorilla, 315\nChaos Monkey, 315\nChapman, Brent, 323\nChat room bots for alerts, 293\nChat rooms for virtual ofﬁces, 166–167\nChecklists\noncall pre-shift responsibilities, 294\nservice launches, 157, 159\nChef software framework, 213\nCheswick, W. R., 79\n“Choose Your Own Adventure” talk, 173\nChubby system, 231, 314\nChurchill, Winston, 119\nClassiﬁcation systems for oncall, 292\nClos networking, 137\nCloud computing era (2010-present),\n469–472\nCloud-scale service, 80–81\nglobal load balancing methods, 82,\n83–85\ninternal backbones, 83–84\npoints of presence, 83–85\nCM (conﬁguration management)\nlanguages, 260–262\nCMDB (Conﬁguration Management\nDatabase), 222\nCMM (Capability Maturity Model),\n405–407\nCNN.com web site, 13–14\nCode\napproval process, 47–48\nautomated reviews, 268–269\nlead time, 201\nlive changes, 236\nsufﬁcient amount, 269–270\nCode latency in DevOps, 178–179\nCode pushes\ndescription, 225, 226\nfailed, 239–240\nCode review system (CRS), 268–269\nCognitive systems engineering (CSE)\napproach, 248\nCold caches, 106\nCold storage factor in service platform\nselection, 54\nCollaboration in DevOps, 183\nCollection systems, 345\ncentral vs. regional collectors,\n352–353\nmonitoring, 349–353\nprotocol selection, 351\npush and pull, 350–351\nserver component vs. agents vs.\npollers, 352\nColocation\nCDNs, 114\nservice platform selection, 65–66\nCommand-line ﬂags, 231\nComments in style guides, 267\nCommit step in build phase, 202–203\nCommodity servers, 463\nCommunication\nemergency plans, 317–318\npostmortems, 302\nvirtual ofﬁces, 166–167\nCompensation in oncall schedules, 290\nCompensatory automation principle,\n244, 246–247\nCompiled languages, 260\nComplementarity principle, 244, 247–248\nCompliance in platform selection, 63\nComprehensiveness in continuous\ndeployment, 237\nComputation, monitoring, 353–354\nConﬁdence in service delivery, 200\n\n\n504\nIndex\nConﬁguration\nautomating, 254\ndeployment phase, 213–214\nin designing for operations, 33–34\nDevOps, 185\nfour-tier web service, 80\nmonitoring, 345–346, 362–363\nConﬁguration management (CM)\nlanguages, 260–262\nConﬁguration Management Database\n(CMDB), 222\nConﬁguration management strategy in\nOS installation, 219\nConﬁguration packages, 220\nConﬂicting goals, 396–397\nCongestion problems, 15\nConsistency\nACID term, 24\nCAP Principle, 21\nConsistency and partition tolerance\n(CP), 24\nConstant scaling, 475–476\nContainers, 60–62\nContent delivery networks (CDNs),\n114–116\nContent distribution servers, 83\nContinuous builds in DevOps, 186\nContinuous Delivery, 223\nContinuous delivery (CD)\ndeployment phase, 221\nDevOps, 189–192\npractices, 191\nprinciples, 190–191\nContinuous deployment\nDevOps, 186\nupgrading live services, 236–239\nContinuous improvement technique\nDevOps, 153, 183\nservice delivery, 201\nContinuous integration (CI) in build\nphase, 205–207\nContinuous tests, 186\nContract questions for hosting\nproviders, 64–65\nContributing conditions analysis (CCA),\n301\nControl in platform selection, 64\nConvergent orchestration, 213–214\nCookies, 76–78\nCoordination for oncall schedules, 290\nCore drivers\ncapacity planning, 373–374\ndeﬁned, 366\nCoredumps, 129\nCorporate emergency communications\nplans, 317–318\nCorpus, 16–17\nCorrelation coefﬁcient, 367\nCorrelation in capacity planning,\n375–378\nCosts\ncaches, 105\ncloud computing era, 469–470\ndot-bomb era, 464–465\nﬁrst web era, 459\nplatform selection, 63–64\npre-web era, 454\nsecond web era, 468–469\nservice platform selection, 66–67\nTCO, 172\nCounters in monitoring, 348–350, 358\nCPU core sharing, 59\nCrash-only software, 35\nCrashes\nautomated data collection and\nanalysis, 129\nsoftware, 128–129\nCraver, Nick, 430\nCRS (code review system), 268–269\nCSE (cognitive systems engineering)\napproach, 248\nCurrent usage in capacity planning,\n368–369\nCustomer functionality, segmentation\nby, 103\nCustomers in DevOps, 177\nCycle time, 196\nDaemons for containers, 61\nDaily oncall schedules, 289\nDark launches, 233, 383–384\nDashboards for alerts, 293\nData analysis in capacity planning,\n375–380\nData import controls, 41–42\nData scaling in dot-bomb era, 463\nData sharding, 110–112\nDatabase-driven dynamic content, 70\n\n\nIndex\n505\nDatabase views in live schema\nchanges, 234\nDatacenter failures, 137–138\nDates in design documents, 277, 282\nDawkins, Richard, 475\nDDoS (distributed denial-of-service)\nattacks, 140\nDeallocation of resources, 160\nDean, Jeff\ncanary requests, 131\nscaling information, 27\nDebois, Patrick, 180\nDebug instrumentation, 43\nDecommissioning services, 404\nassessments, 437–438\ndescription, 156\noverview, 160\nDedicated wide area network\nconnections, 83\nDefault policies, 40\nDefense in depth, 119\nDeﬁned level in CMM, 406–407\nDegradation, graceful, 39–40, 119\nDelays in continuous deployment,\n238\nDelegating capacity planning, 381\nDelegations of authority in Incident\nCommand System, 324\nDeming, W. Edwards, 172\nDenial-of-service (DoS) attacks, 140\nDependencies\ncontainers, 60–61\nservice launches, 158\nDeployment and deployment phase,\n195, 197, 211\napprovals, 216–217\nassessments, 444–445\nconﬁguration step, 213–214\ncontinuous delivery, 221\ndeﬁned, 196\nDevOps, 185\nexercises, 223\nfrequency in service delivery, 201\ninfrastructure as code, 221–222\ninfrastructure automation strategies,\n217–220\ninstallation step, 212–213\ninstalling OS and services, 219–220\nKPIs, 392–393\noperations console, 217\nphysical machines, 217–218\nplatform services, 222\npromotion step, 212\nsummary, 222–223\ntesting, 215–216\nvirtual machines, 218\nDescriptions of outages, 301\nDescriptive failure domains, 127\nDesign documents, 275\nadopting, 282–283\nanatomy, 277–278\narchive, 279–280\nchanges and rationale, 276\nexercises, 284\noverview, 275–276\npast decisions, 276–277\nreview workﬂows, 280–282\nsummary, 283\ntemplates, 279, 282, 481–484\nDesign for operations, 31\naccess controls and rate limits,\n40–41\nauditing, 42–43\nbackups and restores, 36\nconﬁguration, 33–34\ndata import controls, 41–42\ndebug instrumentation, 43\ndocumentation, 43–44\nexception collection, 43–44\nexercises, 50\nfeatures, 45–48\ngraceful degradation, 39–40\nhot swaps, 38–39\nimplementing, 45–48\nimproving models, 48–49\nmonitoring, 42\noperational requirements, 31–32\nqueue draining, 35–36\nredundancy, 37\nreplicated databases, 37–38\nsoftware upgrades, 36\nstartup and shutdown, 34–35\nsummary, 49–50\nthird-party vendors, 48\ntoggles for features, 39\nDesign patterns for resiliency. See\nResiliency\nDesign patterns for scaling. See Scaling\n\n\n506\nIndex\nDetails\ndesign documents, 278\npostmortems, 302\nDevelop step in build phase, 202\nDevelopers for oncall, 287\nDevOps, 171–172\nAgile, 188–189\napproach, 175–176\nautomation, 182, 185–186\nbatch size, 178–179\nbuild phase, 197–198\nbusiness level, 187–188\ncontinuous delivery, 189–192\ncontinuous improvement, 183\nconverting to, 186–188\ndescription, 172–173\nexercises, 193\nexperimentation and learning,\n178\nfeedback, 177–178\nhistory, 180–181\nintegration, 182\nnontechnical practices, 183–184\nrecommended reading, 487\nrelationships, 182\nrelease engineering practices, 186\nSRE, 181\nstarting, 187\nstrategy adoption, 179–180\nsummary, 192\nvs. traditional approach, 173–175\nvalues and principles, 181–186\nworkﬂow, 176–177\nDevOps Cafe Podcast, 180, 200\nDevOps culture, 171\n“DevOps Days” conferences, 180\nDiagnostics, monitoring, 337\nDickson, C., 345\nDickson model, 334\ndiff tool, 33\nDifferentiated services, 233\nDirect measurements, 347–348\nDirect orchestration, 213–214\nDiRT (Disaster Recovery Testing), 316,\n318, 320–323\nDisaster preparedness, 307, 448–450\nantifragile systems, 308–309\nDiRT tests, 320–323\nexercises, 330\nﬁre drills, 312–313\nimplementation and logistics, 318–320\nincident Command System, 323–329\nmindset, 308–310\nrandom testing, 314–315\nrisk reduction, 309–311\nscope, 317–318\nservice launches, 158\nservice testing, 313–314\nstarting, 316–317\nsummary, 329–330\ntraining for individuals, 311–312\ntraining for organizations, 315–317\nDisaster Recovery Testing (DiRT), 316,\n318, 320–323\nDisks\naccess time, 26\ncaches, 106–107\nfailures, 132–133\nDistributed computing and clouds\ncloud computing era, 469–472\nconclusion, 472–473\ndot-bomb era, 459–465\nexercises, 473\nﬁrst web era, 455–459\norigins overview, 451–452\npre-web era, 452–455\nsecond web era, 465–469\nDistributed computing overview, 9–10\nCAP Principle, 21–24\ndistributed state, 17–20\nexercises, 30\nload balancer with multiple backend\nreplicas, 12–13\nloosely coupled systems, 24–25\nserver trees, 16–17\nservers with multiple backends, 14–15\nsimplicity importance, 11\nspeed issues, 26–29\nsummary, 29–30\nvisibility at scale, 10–11\nDistributed denial-of-service (DDoS)\nattacks, 140\nDistributed state, 17–20\nDistributed version control systems\n(DVCSs), 265\nDiurnal cycles, 332\nDiurnal usage patterns, 359\nDiversity, monitoring, 334\n\n\nIndex\n507\nDNS\ndeployment phase, 222\nround robin, 72–73\nDocker system, 61, 219\nDocumentation\ndesign documents. See Design\ndocuments\ndesign for operations, 43–44\nservice launches, 158\nstakeholder interactions, 154\nDoerr, John, 389\nDomains, failure, 126–128\nDomain-speciﬁc languages (DSLs), 244\nDoS (denial-of-service) attacks, 140\nDot-bomb era (2000–2003), 459–465\nDownsampling, monitoring, 339\nDowntime\ncontainers, 61\npre-web era, 453\nin upgrading live services, 225\nDrain tool, 254\nDraining process, 112\nDrains, queue, 35–36\n“DRAM Errors in the Wild: A\nLarge-Scale Field Study” article, 134\nDSLs (domain-speciﬁc languages), 244\nDual load balancers, 76\nDurability in ACID term, 24\nDVCSs (distributed version control\nsystems), 265\nDynamic content with web servers, 70\nDynamic resource allocation, 138\nDynamic roll backs, 232\nDynamo system, 24\n“Each Necessary, But Only Jointly\nSufﬁcient” article, 302\nECC (error-correcting code) memory,\n131–132\nEdge cases, 153\nEdwards, Damon\nDevOps beneﬁts, 172-173\nDevOps Cafe podcast, 180, 188, 200\nEffectiveness of caches, 105\n80/20 rule for operational features, 47\nElements of Programming Style, 11\nEliminating tasks, 155\nEMA (exponential moving average),\n367, 379\nEmail\nalerts, 292–293\narchives, 277\nEmbedded knowledge in DevOps,\n177–178, 187\nEmergency hotﬁxes, 240\nEmergency issues, 160\nEmergency Response (ER), 403, 426–428\nEmergency tasks, 156\nEmployee human resources data\nupdates example, 89–90\nEmpowering users, automation for, 253\nEmptying queues, 35\nEncryption in four-tier web service, 79\nEnd-of-shift oncall responsibilities, 299\nEnd-to-end process in service delivery,\n200\nEngagement\ndeﬁned, 366\nmeasuring, 374–375\nEnterprise Integration Practices: Designing,\nBuilding, and Deploying Messaging\nSolutions, 87\nEnvironment-related ﬁles, 220\nEphemeral computing, 67\nEphemeral machines, 58\nErlang language, 236\nError Budgets, 152\ncase study, 396–399\nDevOps, 184\nError-correcting code (ECC) memory,\n131–132\nEscalation\nalert messages, 345, 354–357\nautomated, 128–129\nmonitoring, 333\nthird-party, 298\nEtsy blog, 256\nEU Data Protection Directive\nplatform selection factor, 63\nrequirements, 43\nEventual consistency, 21\nException collection, 43–44\nExceptional situations. See Oncall\nExecution in service delivery, 201\nExecutive summaries in design\ndocuments, 277, 282\nExpand/contract technique, 234–235\nExperimentation in DevOps, 178\n",
      "page_number": 530
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 538-545)",
      "start_page": 538,
      "end_page": 545,
      "detection_method": "topic_boundary",
      "content": "508\nIndex\nExpertise of cloud providers factor in\nservice platform selection, 66\nExplicit oncall handoffs, 299\nExponential moving average (EMA),\n367, 379\nExponential scaling, 476\nFace-to-face discussions in DevOps, 187\nFacebook\nchat dark launch, 384\nrecommended reading, 488\nFactorial scaling, 477\nFail closed actions, 40\nFail open actions, 40\nFailed code pushes, 239–240\nFailed RAM chips, 123\nFailure condition in alert messages, 354\n“Failure Trends in a Large Disk Drive\nPopulation,” 133, 338\nFailures, 10, 120. See also Resiliency\noverload, 138–141\nphysical. See Physical failures\nFair queueing, 113\nFan in, 15\nFan out, 15\nFarley, D., 190, 223\n“Fault Injection in Production,” 320\nFeature requests vs. bugs, 263\nFeatures in design for operations, 46\nbuilding, 45\ntoggles, 39, 230–232\nwriting, 47–48\nFederal Emergency Management\nAdministration (FEMA) web site,\n324\nFeedback\ndesign for operations, 47–48\nDevOps, 177–178, 186–187\nFeeding from queues, 113\nFelderman, B., 137\nFEMA (Federal Emergency Management\nAdministration) web site, 324\nFiles, environment-related, 220\nFinance monitoring, 336\nFire drills, 312–313\nFirst web era: bubble (1995-2000),\n455–459\nFisher, M., 99–100\nFitts, P., 246\nFitts’s list, 246\nFix-it days, 166\nFIXME comments in style guides, 267\nFlag ﬂipping, 39, 230–232\nFlexibility in service-oriented\narchitectures, 91\nFlows\nbuild phase, 197\ndeﬁned, 196\nFocus in operational teams, 165–166\nFollow-up oncall work, 296\nForecasting in capacity planning,\n378–380\nFormal workﬂows, 280\nFour-tier web service, 77–78\napplication servers, 79\nconﬁguration options, 80\nencryption and certiﬁcate\nmanagement, 79\nfrontends, 78–79\nsecurity, 79\nFox, A., 35\nFreeBSD containers, 60\nFrequency\ndeployment, 201\nmeasurement, 333\noncall, 291–292\nFrontends in four-tier web service, 78–79\nFrying images, 219–220\nFulghum, P., 188, 215\nFunctional splits in AKF Scaling Cube,\n101–102\nFuture capacity planning, 49\nGallagher, S., 307\nGame Day exercises\nAmazon, 318\nDevOps, 184\ndisaster preparedness, 315, 318–320\n“Gamedays on the Obama Campaign”\narticle, 320\nGanapathi, A., 141\nGaneti system, 240, 254\nGatekeeper tool, 233\nGates, 196\nGauges in monitoring, 348–350\nGeographic diversity factor in service\nplatform selection, 54\nGeolocation in cloud-scale service, 81\n\n\nIndex\n509\nGFS (Google File System), 466\nGibson, William, 402\nGilbert, S., 23\nGlobal load balancing (GLB), 82–83,\n83–85\nGo Continuous Delivery tool, 205\nGoals\nautomation, 252–254\nconﬂicting, 396–397\ndesign documents, 277, 282\nuniﬁed, 397–398\nGoogle\nACLs, 41\nAdSense, 465\nAppEngine, 54\nBlog Search upgrading, 226\nbots, 167\nChubby system, 231, 314\nDisaster Recovery Testing, 316, 318\ngraceful degradation of apps, 40\nmessage bus architectures, 86\noncall, 291\noutages, 119\npostmortem reports, 301\nrecommended reading, 488\nself-service launches at, 159\nSRE model, 181\ntimestamps, 341–342\n“Google DiRT: The View from Someone\nBeing Tested” article, 320–323\nGoogle Error Budget KPI, 396–399\nGoogle File System (GFS), 466\nGoogle Maps\nload balancing, 83–84\nlocal business listings, 42\nGoogle Omega system, 34\n“Google Throws Open Doors to Its\nTop-Secret Data Center” article, 320\nGraceful degradation, 39–40, 119\nGraphical user interfaces (GUIs) for\nconﬁguration, 34\nGraphs, monitoring, 358\nGreatness, measuring, 402–403\nGruver, G., 188, 215\n“Guided Tour through Data-center\nNetworking” article, 137\n“Guideline for Postmortem\nCommunication” blog post, 302\nGUIs (graphical user interfaces) for\nconﬁguration, 34\nHadoop system, 132, 467\nHandoff interface, packages as, 207–208\nHandoffs, oncall, 299\nHangs, software, 129–130\nHardware components\ndot-bomb era, 460\nload balancers, 136\nresiliency, 120–121\nHardware output factor in service\nplatform selection, 67–68\nHardware qualiﬁcation, 156\nHardware virtual machines (HVMs), 58\nHash functions, 110\nHash preﬁx, segmentation by, 103\nHbase storage system, 23–24\nHead of line blocking, 112\nHeadroom in capacity planning, 370\nHealth checks\ndeployment phase, 213\nqueries, 12\nHeartbeat requests, 129\nHelp\noncall. See Oncall\nscaling, 252\nHidden costs of automation, 250\nHierarchy, segmentation by, 104\nHigh availability\ncloud computing era, 471\ndot-bomb era, 461–462\nﬁrst web era, 457–458\npre-web era, 454\nsecond web era, 466–467\nHigh-level design, 278\nHigh-speed network counters, 349\nHistograms, 361–362\n“Hit” logs, 340\nHits, cache, 104\nHoare, C. A. R., 9\nHogan, C., 204\nHohpe, G., 87\nHome computers in dot-bomb era, 460\nHorizontal duplication in AKF Scaling\nCube, 99–101\nHosting providers, contract questions\nfor, 64–65\nHot-pluggable devices, 38–39\nHot spares vs. load sharing, 126\nHot-swappable devices, 38–39\nHotﬁxes, 240\n\n\n510\nIndex\n“How Google Sets Goals: OKRs” video,\n389\nHTTP (Hyper-Text Transfer Protocol)\nload balancing, 75\noverview, 69\nHudson tool, 205\nHuman error, 141–142\nHuman processes, automating, 154\nHuman resources data updates example,\n89–90\nHumble, J.\ncontinuous delivery, 190, 223\nDevOps Cafe Podcast, 188, 200\nHVMs (hardware virtual machines), 58\nHybrid load balancing strategy, 75\nHyper-Text Transfer Protocol (HTTP)\nload balancing, 75\noverview, 69\nIaaS (Infrastructure as a Service), 51–54\nIAPs (Incident Action Plans), 326–327\nIdeals for KPIs, 390\nImage method of OS installation,\n219–220\nImpact focus for feature requests, 46\nImplementation of disaster\npreparedness, 318–320\nImport controls, 41–42\nImprovement levels in operational\nexcellence, 412–413\nImproving models in design for\noperations, 48–49\nIn-house service provider factor in\nservice platform selection, 67\nIncident Action Plans (IAPs), 326–327\n“Incident Command for IT: What We\nCan Learn from the Fire\nDepartment” talk, 323\nIncident Command System, 323–324\nbest practices, 327–328\nexample use, 328–329\nIncident Action Plan, 326–327\nIT operations arena, 326\npublic safety arena, 325\nIncident Commanders, 324–325, 328\nIndex lookup speed, 28\nIndividual training for disaster\npreparedness, 311–312\nInformal review workﬂows, 280\nInfrastructure\nautomation strategies, 217–220\nDevOps, 185\nservice platform selection, 67\nInfrastructure as a Service (IaaS), 51–54\nInfrastructure as code, 221–222\nInhibiting alert messages, 356–357\nInitial level in CMM, 405\nInnovating, 148\nInput/output (I/O)\noverload, 13\nvirtual environments, 58–59\nInstallation\nin deployment phase, 212–213\nOS and services, 219–220\nIntegration in DevOps, 182\nIntel OKR system, 389\nIntentional delays in continuous\ndeployment, 238\nIntermodal shipping, 62\nInternal backbones in cloud-scale\nservice, 83–85\nInternet Protocol (IP) addresses\ndeployment phase, 222\nload balancers, 72–73\nrestrictions on, 40\nIntroducing new features, ﬂag ﬂips for,\n232\nIntrospection, 10\nInvalidation of cache entry, 108\nInvolvement in DevOps, 183\nIP (Internet Protocol) addresses\ndeployment phase, 222\nload balancers, 72–73\nrestrictions on, 40\nIsolation in ACID term, 24\nISPs for cloud-scale service, 83\nIssues\nnaming standards, 264\ntracking systems, 263–265\nIT operations arena in Incident\nCommand System, 326\nITIL recommended reading, 488\nj-SOX requirements, 43\nJacob, Adam, 173\nJails\ncontainers, 60\nprocesses, 55\n\n\nIndex\n511\nJava counters, 350\nJCS (joint cognitive system), 248\nJenkins CI tool, 205\nJob satisfaction in service delivery, 201\nJoint cognitive system (JCS), 248\nJSON transmitted over HTTP, 351\nKamp, P.-H., 478–479\nKartar, J., 183\nKeeven, T., 99\nKejariwal, A., 371\nKernighan, B., 11\nKey indicators in capacity planning,\n380–381\nKey performance indicators (KPIs),\n387–388\ncreating, 389–390\nError Budget case study, 396–399\nevaluating, 396\nexercises, 399–400\nmachine allocation example,\n393–396\nmonitoring example, 336–337\noverview, 388–389\nsummary, 399\nKeywords in alerts, 304\nKim, Gene, 171–172\nKlau, Rick, 389\nKotler, Philip, 365\nKPIs. See Key performance indicators\n(KPIs)\nKrishnan, Kripa, 319–320\nLabor laws, 43\nLame-duck mode, 35\n“LAMP” acronym, 461\nLanguage tools for automation, 258–262\nLatency\ncloud computing era, 471\ncloud-scale service, 81–82\ncode, 178–179\nmonitoring, 334, 336\nservice platform selection, 54\nSRE vs. traditional enterprise IT, 149\nLatency load balancing, 74\nLatency Monkey, 315\nLaunch leads, 159\nLaunch Readiness Engineers (LREs),\n157–158\nLaunch Readiness Reviews (LRRs), 159\nLaunches\ndark, 233\nservices, 156–160, 382–384\nLayer 3 and 4 load balancers, 73\nLayer 7 load balancers, 73\nLead times in service delivery, 201\nLeaf servers, 17\n“Lean Manufacturing,” 172\nLearning DevOps, 178\nLeast Frequently Used (LFU) algorithm,\n107\nLeast Loaded (LL) algorithm, 13\nload balancing, 74\nproblems, 13–14\nLeast Loaded with Slow Start load\nbalancing, 74\nLeast Recently Used (LRU) algorithm,\n107\nLee, J. D., 249\nLeft-over automation principle, 244–246\nLegacy system backups and restores, 36\nLessons learned in automation, 249–250\nLevel of service abstraction in service\nplatform selection, 52–56\nLevels of improvement in operational\nexcellence, 412–413\nLevy, Steven, 320\nLFU (Least Frequently Used) algorithm,\n107\nLimoncelli, T. A.\nDiRT tests, 320\nmeta-work, 162\npackages, 204\ntest event planning, 319\ntime management, 256\nLinear scaling, 476–477\nLink-shortening site example, 87–89\nLinking tickets to subsystems, 263–264\nLinux in dot-bomb era, 460–461\nLive code changes, 236\nLive restores, 36\nLive schema changes, 234–236\nLive service upgrades. See Upgrading\nlive services\nLoad balancers\nfailures, 134–136\nﬁrst web era, 456\nwith multiple backend replicas, 12–13\n\n\n512\nIndex\nLoad balancers (continued)\nwith shared state, 75\nthree-tier web service, 72–74\nLoad sharing vs. hot spares, 126\nLoad shedding, 139\nLoad testing, 215\nLocal business listings in Google Maps,\n42\nLocal labor laws, 43\nLogarithmic scaling, 476\nLogistics in disaster preparedness,\n318–320\nLogistics team in Incident Command\nSystem, 326\nLoglinear scaling, 476–477\nLogs, 11, 340\napproach, 341\ndesign documents, 282\ntimestamps, 341–342\nLong-term analysis, 354\nLong-term ﬁxes\noncall, 299–300\nvs. quick, 295–296\nLongitudinal hardware failure study,\n133–134\nLook-for’s, 407\nLookup-oriented splits in AKF Scaling\nCube, 102–104\nLoosely coupled systems, 24–25\nLower-latency services in cloud\ncomputing era, 469\nLREs (Launch Readiness Engineers),\n157–158\nLRRs (Launch Readiness Reviews), 159\nLRU (Least Recently Used) algorithm,\n107\nLynch, N., 23\nMACD (moving average\nconvergence/divergence) metric,\n367, 378–379\nMACD signal line, 367\nMachine Learning service, 55\nMachines\nautomated conﬁguration example, 251\ndeﬁned, 10\nfailures, 134\nKPI example, 393–396\nMadrigal, A. C., 316\nMain threads, 112\nMaintenance alert messages, 356\nMajor bug resolution monitoring, 336\nMajor outages, 307\nMalfunctions, 121\ndeﬁned, 120\ndistributed computing approach, 123\nMTBF, 121–122\ntraditional approach, 122–123\nManaged level in CMM, 406–407\nManagement support in design\ndocuments, 282\nManual scaling, 153\nManual stop lists, 238\nMany-to-many communication in\nmessage bus architectures, 85–86\nMany-to-one communication in message\nbus architectures, 85–86\nMapReduce system, 466–467\nMaster-master pairs, 126\nMaster of Disaster (MoD) in Wheel of\nMisfortune game, 311–312\nMaster servers, 20\nMaster-slave split-brain relationship, 22\nMath terms, 367\n“Mature Role for Automation” blog\npost, 249\nMAUs (monthly active users), 366, 373\nMcHenry, Stephen, 234\nMcHenry Technique, 234–235\nMcKinley, D., 256\nMcLean, Malcom, 62\nMCollective service, 86\nMD5 algorithm, 110\nMean time between failures (MTBF),\n120–122, 125\nMean time to repair (MTTR), 125\nMean time to restore service in service\ndelivery, 201\nMeasurements, 332\nengagement, 374–375\nfrequency, 333\ngreatness, 402–403\nmonitoring. See Monitoring\nscaling, 97\nMedians, monitoring, 359\nMemory\ncaches, 104–106\nECC, 131–132\nfailures, 123, 131–132\n\n\nIndex\n513\nvirtual machines, 59\nweb servers, 71\nMesos system, 34\nMessage bus architectures, 85–86\ndesigns, 86\nemployee human resources data\nupdates example, 89–90\nlink-shortening site example, 87–89\nreliability, 87\nMessages, alert. See Alerts\nMeta-monitoring, 339–340\nMeta-processes, automating, 155\nMeta-work, 162\nMetrics, 11, 332\nMindset in disaster preparedness,\n308–310\nMinimum monitor problem, 337–338\nMisses, cache, 104\n“Model for Types and Levels of Human\nInteraction with Automation”\narticle, 248\nMonitoring, 331–332\nalerting and escalation management,\n354–357\nanalysis and computation,\n353–354\nassessments, 428–430\nblackbox vs. whitebox, 346–347\ncollections, 350–353\ncomponents, 345–346\nconﬁguration, 362–363\nconsumers, 334–336\ndesign for operations, 42\nexercises, 342–343, 364\nhistograms, 361–362\nkey indicators, 336–337, 380–381\nlogs, 340–342\nmeta-monitoring, 339–340\noverview, 332–333\npercentiles, 359\nprotocol selection, 351\npush vs. pull, 350–351\nretention, 338–339\nsensing and measurement, 345–350\nservice launches, 158\nstack ranking, 360\nstorage, 362\nsummary, 342, 363–364\nsystem integration, 250\nuses, 333\nvisualization, 358–362\nMonitoring and Metrics (MM), 404,\n428–430\nMonthly active users (MAUs), 366, 373\nMoving average\nconvergence/divergence (MACD)\nmetric, 367, 378–379\nMoving averages\ncapacity planning, 377\ndeﬁned, 367\nMTBF (mean time between failures),\n120–122, 125\nMTTR (mean time to repair), 125\nMultiple backend replicas, load\nbalancers with, 12–13\nMultiple backends, servers with, 14–15\nMultiple data stores in three-tier web\nservice, 77\nMultitenant systems, automation,\n270–271\nMultithreaded code, 112\nN + 1 conﬁgurations, 458\nN + 2 conﬁgurations, 458–459\nN + M redundancy, 124–125\n“NAK” (negatively acknowledge) alerts,\n355\nNaming standards, 264\nNative URLs, 115\nNatural disasters factor in service\nplatform selection, 53\nNearest load balancing, 81\nNearest by other metric load balancing,\n81\nNearest with limits load balancing, 81\nNegatively acknowledge (“NAK”)\nalerts, 355\nNetﬂix\ndisaster preparedness, 315\nvirtual machines, 59\nNetﬂix Aminator framework, 219\nNetﬂix Simian Army, 315\nNetworks\naccess speed, 26–27\ncounters, 349\ninterface failures, 133\nprotocols, 489\nNew feature reviews in DevOps, 183\n\n\n514\nIndex\nNew Product Introduction and Removal\n(NPI/NPR)\nassessments, 435–436\noperational responsibility, 404\nNew services, launching, 382–384\nNielsen, Jerri, 225\nNon-blocking bandwidth, 137\nNon-functional requirements term, 32\nNon-goals in design documents, 277\nNonemergency tasks, 156\nNontechnical DevOps practices, 183–184\nNormal growth in capacity planning, 369\nNormal requests, 161\nNoSQL databases, 24\nNotiﬁcation types in oncall, 292–293\nObjectives in Incident Command\nSystem, 324\nObserve, Orient, Decide, Act (OODA)\nloop, 296–297\nO’Dell’s Axiom, 95\nOKR system, 389\nOn-premises, externally run services\nfactor in service platform\nselection, 67\nOncall, 285\nafter-hours maintenance coordination,\n294\nalert responsibilities, 295–296\nalert reviews, 302–304\nbeneﬁts, 152\ncalendar, 290–291, 355\ncontinuous deployment, 238\ndeﬁned, 148\ndesigning, 285–286\nDevOps, 183\nend-of-shift responsibilities, 299\nexcessive paging, 304–305\nexercises, 306\nfrequency, 291–292\nlong-term ﬁxes, 299–300\nnotiﬁcation types, 292–293\nonduty, 288\nOODA, 296–297\noperational rotation, 161–162\noverview, 163–164\nplaybooks, 297–298\npostmortems, 300–302\npre-shift responsibilities, 294\nregular responsibilities, 294–295\nrosters, 287\nschedule design, 288–290\nservice launches, 158\nSLAs, 286–287\nsummary, 305–306\nthird-party escalation, 298\nOnduty, 288\nOne percent testing, 233\nOne-to-many communication in\nmessage bus architectures, 85–86\nOne-way pagers, 293\nOODA (Observe, Orient, Decide, Act)\nloop, 296–297\nOpen source projects in dot-bomb era,\n460\nOpenStack system, 240\nOperating system installation, 219–220\nOperational excellence, 401\nassessment levels, 405–407\nassessment methodology, 403–407\nassessment questions and look-for’s,\n407\nexercises, 415\ngreatness measurement, 402–403\nimprovement levels, 412–413\norganizational assessments, 411–412\noverview, 401–402\nservice assessments, 407–410\nstarting, 413–414\nsummary, 414\nOperational Health, monitoring, 335\nOperational hygiene in service launches,\n158–159\nOperational requirements in designing\nfor operations, 31–32\nOperational responsibilities (OR),\n403–404\nOperational teams, 160–162\nﬁx-it days, 166\nfocus, 165–166\nin Incident Command System, 326\noncall days, 163–164\norganizing strategies, 160–166\nproject-focused days, 162–163\nticket duty days, 164–165\ntoil reduction, 166\nOperations, 147–148\nchange vs. stability, 149–151\n\n\nIndex\n515\ndesign for. See Design for operations\nexercises, 168–169\norganizing strategies for operational\nteams, 160–166\nat scale, 152–155\nservice life cycle, 155–160\nSRE overview, 151–152\nSRE vs. traditional enterprise IT,\n148–149\nsummary, 167–168\nvirtual ofﬁces, 166–167\nOperations console in deployment\nphase, 217\nOperator errors, 171\nOppenheimer, D. L., 141, 171\nOpportunity costs in service platform\nselection, 66–67\nOptimizing level in CMM, 406\nOrders of magnitude, 478\nOrganizational assessments in\noperational excellence, 411–412\nOrganizational divisions, segmentation\nby, 103\nOrganizational memory, 157\nOrganizational training for disaster\npreparedness, 315–317\nOS installation, 219–220\nOutages\ncode review systems, 269\ndeﬁned, 120\ndisasters. See Disaster preparedness\nOverﬂow capacity factor in service\nplatform selection, 67\nOverload failures\nDoS and DDoS attacks, 139\nload shedding, 139\nscraping attacks, 140–141\ntrafﬁc surges, 138–139\nOversubscribed systems\ndeﬁned, 53\nspare capacity, 125\nPaaS (Platform as a Service), 51, 54–55\nPackages\nbuild phase, 204\nconﬁguration, 220\ncontinuous delivery, 190\ndeployment phase, 213\ndistributing, 266\nas handoff interface, 207–208\npinning, 212\nregistering, 206\nPager storms, 356\nPagers for alerts, 293, 304–305\nPanics, 128\nParasuraman, R., 248\nParavirtualization (PV), 58–59\nParity bits, 131–132\nPartition tolerance in CAP Principle,\n22–24\nParts and components failures, 131–134\nPast decisions, documenting, 276–277\nPatch lead time in service delivery, 201\nPatterson, D. A., 141\nPCI DSS requirements, 43\nPercentiles in monitoring, 359\nPerformance\ncaches, 105\ntesting, 215\nPerformance and Efﬁciency (PE)\nassessments, 439–441\noperational responsibility, 404\nPerformance regressions, 156, 215\nPerformant systems, 10\nPerl language, 259–260, 262\nPersistence in caches, 106\nPerspective in monitoring, 333\nPhased roll-outs, 229\nPhoenix Project, 172\nPhysical failures, 131\ndatacenters, 137–138\nload balancers, 134–136\nmachines, 134\nparts and components, 131–134\nracks, 136–137\nPhysical machines\ndeployment phase, 217–218\nfailures, 134\nservice platform selection, 57\nPie charts, 358\nPinheiro, E.\ndrive failures, 133, 338\nmemory errors, 134\nPinning packages, 212\nPKI (public key infrastructure), 40\nPlanned growth in capacity planning,\n369–370\nPlanning in disaster preparedness,\n318–319\n",
      "page_number": 538
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 546-553)",
      "start_page": 546,
      "end_page": 553,
      "detection_method": "topic_boundary",
      "content": "516\nIndex\nPlanning team in Incident Command\nSystem, 326\nPlatform as a Service (PaaS), 51, 54–55\nPlatform selection. See Service platform\nselection\nPlauger, P., 11\nPlaybooks\noncall, 297–298\nprocess, 153\nPods, 137\nPoints of presence (POPs), 83–85\nPollers, 352\nPost-crash recovery, 35\nPostmortems, 152\ncommunication, 302\nDevOps, 184\noncall, 291, 300–302\npurpose, 300–301\nreports, 301–302\ntemplates, 484–485\nPower failures, 34, 133\nPower of 2 mapping process, 110–111\nPractical Approach to Large-Scale Agile\nDevelopment: How HP Transformed\nHP LaserJet FutureSmart Firmware,\n188\nPractice of System and Network\nAdministration, 132, 204\nPre-checks, 141\nPre-shift oncall responsibilities, 294\nPre-submit checks in build phase,\n202–203\nPre-submit tests, 267\nPre-web era (1985-1994), 452–455\nPrefork processing module, 114\nPremature optimization, 96\nPrescriptive failure domains, 127\nPrimary resources\ncapacity planning, 372\ndeﬁned, 366\nPrioritizing\nautomation, 257–258\nfeature requests, 46\nfor stability, 150\nPrivacy in platform selection, 63\nPrivate cloud factor in platform\nselection, 62\nPrivate sandbox environments, 197\nProactive scaling solutions, 97–98\nProblems to solve in DevOps, 187\nProcess watchers, 128\nProcesses\nautomation beneﬁts, 253\ncontainers, 60\ninstead of threads, 114\nProctors for Game Day, 318\nProduct Management (PM) monitoring,\n336\nProduction candidates, 216\nProduction health in continuous\ndeployment, 237\nProject-focused days, 162–163\nProject planning frequencies, 410\nProject work, 161–162\nPromotion step in deployment phase,\n212\nPropellerheads, 451\nProportional shedding, 230\nProtocols\ncollections, 351\nnetwork, 489\nPrototyping, 258\nProvider comparisons in service\nplatform selection, 53\nProvisional end-of-shift reports, 299\nProvisioning\nin capacity planning, 384–385\nin DevOps, 185–186\nProxies\nmonitoring, 352\nreverse proxy service, 80\nPublic cloud factor in platform\nselection, 62\nPublic Information Ofﬁcers in Incident\nCommand System, 325–326\nPublic key infrastructure (PKI), 40\nPublic safety arena in Incident\nCommand System, 325\nPublishers in message bus architectures,\n85\nPublishing postmortems, 302\nPubSub2 system, 86\nPull monitoring, 350–351\nPuppet systems\nconﬁguration management, 261\ndeployment phase, 213\nmultitenant, 271\nPush conﬂicts in continuous\ndeployment, 238\n\n\nIndex\n517\nPush monitoring, 350–351\n“Pushing Millions of Lines of Code Five\nDays a Week” presentation, 233\nPV (paravirtualization), 58–59\nPython language\nlibraries, 55\noverview, 259–261\nQPS (queries per second)\ndeﬁned, 10\nlimiting, 40–41\nQuadratic scaling, 476\nQuality Assurance monitoring, 335\nQuality assurance (QA) engineers, 199\nQuality measurements, 402\nQueries in HTTP, 69\nQueries of death, 130–131\nQueries per second (QPS)\ndeﬁned, 10\nlimiting, 40–41\nQueues, 113\nbeneﬁts, 113\ndraining, 35–36\nissue tracking systems, 263\nmessages, 86\nvariations, 113–114\nQuick ﬁxes vs. long-term, 295–296\nRabbitMQ service, 86\nRachitsky, L., 302\nRack diversity, 136\nRacks\nfailures, 136\nlocality, 137\nRAID systems, 132\nRAM\nfor caching, 104–106\nfailures, 123, 131–132\nRandom testing for disaster\npreparedness, 314–315\nRapid development, 231–232\nRate limits in design for operations,\n40–41\nRate monitoring, 348\nRationale, documenting, 276\nRe-assimilate tool, 255\nRead-only replica support, 37\nReal-time analysis, 353\nReal user monitoring (RUM), 333\nReboots, 34\nRecommendations in postmortem\nreports, 301\nRecommended reading, 487–489\nRecovery-Oriented Computing (ROC),\n461\nRecovery tool, 255\nRedis storage system, 24, 106\nReduced risk factor in service delivery,\n200\nReducing risk, 309–311\nReducing toil, automation for, 257\nRedundancy\ndesign for operations, 37\nﬁle chunks, 20\nfor resiliency, 124–125\nservers, 17\nReengineering components, 97\nRefactoring, 97\nRegional collectors, 352–353\nRegistering packages, 204, 206\nRegression analysis, 375–376\nRegression lines, 376\nRegression tests for performance, 156,\n215\nRegular meetings in DevOps, 187\nRegular oncall responsibilities, 294–295\nRegular software crashes, 128\nRegular Tasks (RT)\nassessments, 423–425\noperational responsibility, 403\nRegulating system integration, 250\nRelationships in DevOps, 182\nRelease atomicity, 240–241\nRelease candidates, 197\nRelease engineering practice in DevOps,\n186\nRelease vehicle packaging in DevOps,\n185\nReleases\ndeﬁned, 196\nDevOps, 185\nReliability\nautomation for, 253\nmessage bus architectures, 87\nReliability zones in service platform\nselection, 53–54\nRemote hands, 163\nRemote monitoring stations, 352\n\n\n518\nIndex\nRemote Procedure Call (RPC) protocol,\n41\nRepair life cycle, 254–255\nRepeatability\nautomation for, 253\ncontinuous delivery, 190\nRepeatable level in CMM, 405\nReplacement algorithms for caches, 107\nReplicas, 124\nin design for operations, 37–38\nload balancers with, 12–13\nthree-tier web service, 76\nupdating, 18\nReports for postmortems, 301–302\nRepositories in build phase, 197\nReproducibility in continuous\ndeployment, 237\nRequests in updating state, 18\n“Resilience Engineering: Learning to\nEmbrace Failure” article, 320\nResiliency, 119–120\ncapacity planning, 370–371\nDevOps, 178\nexercises, 143\nfailure domains, 126–128\nhuman error, 141–142\nmalfunctions, 121–123\noverload failures, 138–141\nphysical failures. See Physical failures\nsoftware failures, 128–131\nsoftware vs. hardware, 120–121\nspare capacity for, 124–126\nsummary, 142\nResolution\nalert messages, 355\nmonitoring, 334\nResource pools, 99\nResource regression in capacity\nplanning, 381–382\nResource sharing\nservice platform selection, 62–65\nvirtual machines, 59\nResources\ncontention, 59, 238\ndeallocation, 160\ndynamic resource allocation, 138\nResponsibilities for oncall, 294–296\nRestarts, automated, 128–129\nRestores in design for operations, 36\nRetention monitoring, 338–339\nReverse proxy service, 80\nReview workﬂows in design documents,\n280–282\nReviewers in design documents, 277, 281\nRevising KPIs, 391–392\nRevision numbers in design documents,\n277, 282\nRework time factor in service delivery,\n201\nRich, Amy, 51\nRichard, Dylan, 320\nRisk reduction, 309–311\nRisk system, 24\nRisk taking in DevOps, 178\nRituals in DevOps, 178\nRobbins, Jesse\ncommunication beneﬁts, 186\nDiRT tests, 320\ntest planning, 319–320\nROC (Recovery-Oriented Computing),\n461\nRoll back, 239\nRoll forward, 239\nRolling upgrades, 226\nRoosevelt, Franklin D., 275\nRoosevelt, Theodore, 307\nRoot cause analysis, 301–302\nRoot servers, 17\nRossi, Chuck, 233\nRosters, oncall, 287\nRound-robin for backends, 12\nRound robin (RR) load balancing, 72–74\nRoyce, D. W. W., 175\nRPC (Remote Procedure Call) protocol,\n41\nRSS feeds of build status, 205\nRubin, A. D., 79\nRuby language, 259–260\nRUM (real user monitoring), 333\n“Run run run dead” problem, 462\nSaaS (Software as a Service), 51, 55–56\nSafeguards, automation for, 253\nSafety for automation, 249\nSalesforce.com, 55–56\nSample launch readiness review survey,\n157–158\nSandbox environments, 197\n\n\nIndex\n519\nSatellites in cloud-scale service, 83\nScalability Rules: 50 Principles for Scaling\nWeb Sites, 100\nScale\noperations at, 152–155\nvisibility at, 10–11\nScaling, 95, 475\nAKF Scaling Cube, 99–104\nautomation for, 252\nBig O notation, 476–479\ncaching, 104–110\ncloud computing era, 471\nconstant, linear, and exponential,\n475–476\ncontent delivery networks, 114–116\ndata sharding, 110–112\ndatabase access, 37\ndot-bomb era, 462–463\nexercises, 116–117\nﬁrst web era, 456–457\ngeneral strategy, 96–98\nmonitoring, 350\nPaaS services, 54\npre-web era, 454\nqueueing, 113–114\nrecommended reading, 489\nin resiliency, 135\nscaling out, 99–101\nscaling up, 98–99\nsecond web era, 467–468\nsmall-scale computing systems, 470\nsummary, 116\nthreading, 112–113\nthree-tier web service, 76\nSchedules\ncontinuous deployment, 238\noncall, 288–291\nSchema changes, 234–236\nSchlossnagle, Theo, 31, 172\nSchroeder, B., 134\nScope in disaster preparedness, 317–318\nScraping attacks, 140–141\nScripting languages, 259–260\nSDLC (Software Development Life\nCycle), 184–185\nSeaLand company, 62\nSecond web era (2003-2010), 465–469\nSecondary resources in capacity\nplanning, 372\nSecurity in four-tier web service, 79\nSee, K. A., 249\nSegments in lookup-oriented splits,\n102–103\nSelenium WebDriver project, 215\nSelf-service launches at Google, 159\nSelf-service requests, 154\nSend to Repairs tool, 255\nSenge, Peter, 147\nSensing and measurement systems,\n345–350\nServer trees, 16–17, 80\nServerFault.com, 102\nServers\ncollections, 352\ndeﬁned, 10\nwith multiple backends, 14–15\nService assessments, operational\nexcellence, 407–410\nService delivery\nassessments, 442–445\nbuild phase. See Builds\ndeployment phase. See Deployment\nand deployment phase\nﬂow, 196\nService Deployment and\nDecommissioning (SDD), 404,\n437–438\nService latency in cloud computing era,\n471\nService level agreements (SLAs)\nError Budgets, 152\nload shedding, 139\nmonitoring, 334\noncall, 286–287\nService Level Indicators (SLIs), 334\nService Level Objectives (SLOs), 334\nService Level Targets (SLTs), 334\nService life cycle, 155\ndecommissioning services, 160\nstages, 156–160\nService management monitoring, 334\nService-oriented architecture (SOA),\n90–91\nbest practices, 91–92\nﬂexibility, 91\nsupport, 91\nService platform selection, 51–52\ncolocation, 65–66\ncontainers, 60–61\n\n\n520\nIndex\nService platform selection (continued)\nexercises, 68\nlevel of service abstraction, 52–56\nmachine overview, 56\nphysical machines, 57\nresource sharing levels, 62–65\nstrategies, 66–68\nsummary, 68\nvirtual machines, 57–60\nService splits in AKF Scaling Cube,\n101–102\nService testing in disaster preparedness,\n313–314\nServices\nassessing, 407–410\ndecommissioning, 160\ndeﬁned, 10\ninstalling, 219–220\nrestart, 34\nSRE vs. traditional enterprise IT, 148–149\nSession IDs, 76\n7-day actives (7DA), 373\nSharding, 110–112\nShards, 16, 18–19\nShared oncall responsibilities, 183\nShared pools, 138\nShared state in load balancing, 75\nShaw, George Bernard, 387\nShedding, proportional, 230\nShell scripting languages, 259\nSheridan, T. B., 248\n“Shewhart cycle,” 172\nShifts, oncall, 164–165, 291–292\nShipping containers, 62\nShort-lived machines, 58\nShort-term analysis, 353\nShutdown in design for operations,\n34–35\nSign-off for design documents, 281–282\nSignal line crossover, 367\nSilencing alert messages, 356–357\nSilos, 174\nSimian Army, 315\nSimple Network Management Protocol\n(SNMP), 351\nSimple Queue Service (SQS), 86\nSimplicity\nimportance, 11\nreview workﬂows, 280\nSingapore MAS requirements, 43\nSingle-machine web servers, 70–71\nSite Reliability Engineering (SRE),\n147–148\nDevOps, 181\noverview, 151–152\nvs. traditional enterprise IT, 148–149\nSite reliability practices, 151–152\nSize\nbatches, 178–179\ncaches, 108–110\nSLAs (service level agreements)\nError Budgets, 152\nload shedding, 139\nmonitoring, 334\noncall, 286–287\nSLIs (Service Level Indicators), 334\nSLOs (Service Level Objectives), 334\nSloss, Benjamin Treynor\nGoogle Error Budget, 396\nsite reliability practices, 151\nSlow start algorithm, 13\nSLTs (Service Level Targets), 334\nSmall-scale computing systems, scaling,\n470\nSMART (Speciﬁc, Measurable,\nAchievable, Relevant, and\nTime-phrased) criteria, 388\nSmart phone apps for alerts, 293\nSmoke tests, 192\nSMS messages for alerts, 293\nSNMP (Simple Network Management\nProtocol), 351\nSOA (service-oriented architecture),\n90–91\nbest practices, 91–92\nﬂexibility, 91\nsupport, 91\nSoft launches, 148, 382\nSoftware as a Service (SaaS), 51, 55–56\nSoftware Development Life Cycle\n(SDLC), 184–185\nSoftware engineering tools and\ntechniques in automation, 262–263\ncode reviews, 268–269\nissue tracking systems, 263–265\npackages, 266\nstyle guides, 266–267, 270\nsufﬁcient code, 269–270\ntest-driven development, 267–268\nversion control systems, 265–266\n\n\nIndex\n521\nSoftware engineers (SWEs), 199\nSoftware failures, 128\ncrashes, 128–129\nhangs, 129–130\nqueries of death, 130–131\nSoftware load balancers, 136\nSoftware packages. See Packages\nSoftware resiliency, 120–121\nSoftware upgrades in design for\noperations, 36\nSolaris containers, 60\nSolid-state drives (SSDs)\nfailures, 132\nspeed, 26\nSource control systems, 206\nSOX requirements, 43\nSpafford, G., 172\nSpare capacity, 124–125\nload sharing vs. hot spares, 126\nneed for, 125–126\nSpear, S., 172\nSpecial constraints in design documents,\n278\nSpecial notations in style guides, 267\nSpeciﬁc, Measurable, Achievable,\nRelevant, and Time-phrased\n(SMART) criteria, 388\nSpeed\nimportance, 10\nissues, 26–29\nSpell check services, abstraction in, 24\nSpindles, 26\nSplit brain, 23\nSplit days oncall schedules, 289\nSpolsky, J., 121\nSprints, 189\nSQS (Simple Queue Service), 86\nSRE (Site Reliability Engineering),\n147–148\nDevOps, 181\noverview, 151–152\nvs. traditional enterprise IT, 148–149\nSSDs (solid-state drives)\nfailures, 132\nspeed, 26\nStability vs. change, 149–151\nStack Exchange, 167\nStack ranking, 360\nStakeholders, 148\nStandard capacity planning, 366–368\nStandardized shipping containers, 62\nStartup in design for operations, 34–35\nStates, distributed, 17–20\nStatic content on web servers, 70\nStatus of design documents, 277, 282\nSteal time, 59\nStickiness in load balancing, 75\nStorage systems, monitoring, 345, 362\nStranded capacity, 57\nStranded resources in containers, 61\nStyle guides\nautomation, 266–267, 270\ncode review systems, 269\nSub-linear scaling, 477\nSubscribers in message bus\narchitectures, 86\nSubsystems, linking tickets to, 263–264\nSuggested resolution in alert messages,\n355\nSummarization, monitoring, 339\nSuper-linear scaling, 477\nSurvivable systems, 120\nSWEs (software engineers), 199\nSynthesized measurements, 347–348\nSystem administration, automating,\n248–249, 253\nSystem logs, 340\nSystem testing\nin build phase, 203\nvs. canarying, 228–229\noverview, 215\nT-bird database system, 103\nTags in repositories, 208\nTaking down services for upgrading,\n225–226\nTargeting in system integration, 250\nTasks\nassessments, 423–425\nautomating, 153–155\nTCO (total cost of ownership), 172\nTDD (test-driven development), 267–268\nTeam managers in operational rotation,\n162\nTeamCity tool, 205\nTeams, 160–162\nautomating processes, 155\nﬁx-it days, 166\nfocus, 165–166\n\n\n522\nIndex\nTeams (continued)\nin Incident Command System, 326\noncall days, 163–164\norganizing strategies, 160–166\nproject-focused days, 162–163\nticket duty days, 164–165\ntoil reduction, 166\nvirtual ofﬁces, 166–167\nTechnical debt, 166\nTechnical practices in DevOps, 184–185\nTechnology\ncloud computing era, 472\ndot-bomb era, 460–461\nﬁrst web era, 455–456\npre-web era, 453–454\nsecond web era, 465–466\nTelles, Marcel, 401\nTemplates\ndesign documents, 279, 282, 481–484\npostmortem, 484–485\nTerminology for Incident Command\nSystem, 324\nTest-driven development (TDD),\n267–268\nTests\nvs. canarying, 228–229\ncontinuous deployment, 237\ndeployment phase, 215–216\nDevOps, 186\ndisaster preparedness. See Disaster\npreparedness\nearly and fast, 195\nenvironments, 197\nﬂag ﬂips for, 232–233\nText-chat, 167\nText ﬁles for conﬁguration, 33\nText messages for alerts, 293\nTheme for operational teams, 165–166\nTheory, recommended reading for, 488\nThialﬁsystem, 86\n“Things You Should Never Do” essay,\n121\nThird-party vendors\ndesign for operations, 48\noncall escalation, 298\n30-day actives (30DA), 373\nThompson, Ken, 245\nThreading, 112–113\nThree-tier web service, 71–72\nload balancer methods, 74\nload balancer types, 72–73\nload balancing with shared state,\n75\nscaling, 76–77\nuser identity, 76\nTicket duty\ndescription, 161–162\nticket duty days, 164–165\nTime Management for System\nAdministrators, 162, 256\nTime savings, automation for, 253\nTime series, 366\nTime to live (TTL) value for caches,\n108\nTime zones in oncall schedules, 289\nTimed release dates, 232\nTimelines of events, 301\nTimestamps in logs, 341–342\nTimetables in capacity planning, 371\nTitles in design documents, 277, 282\nTODO comments in style guides, 267,\n270\nToggling features, 39, 230–233\nToil\ndeﬁned, 244\nreducing, 166, 257, 446–447\nTool building vs. automation, 250–252\nTorvalds, Linus, 276\nTotal cost of ownership (TCO), 172\nTracebacks, 129\nTracking system integration, 250\nTrafﬁc\ndeﬁned, 10\nsurges, 138–139\nTrailing averages, 13\nTraining\ndisaster preparedness, 311–312,\n315–317\noncall, 287\nTransit ISPs, 83\nTrends, monitoring, 333\nTriggers, 353\nTrustworthiness of automation, 249\nTseitlin, A., 315, 320\nTTL (time to live) value for caches, 108\nTufte, E. R., 362\nTumblr Invisible Touch system, 218\nTwain, Mark, 243\nTwitter, 103\nTwo-way pagers, 293\n\n\nIndex\n523\nUAT (User Acceptance Testing), 216\nUbuntu Upstart system, 34\nUnaligned failure domains, 127\nUndersubscribed systems, 53\nUniﬁed goals, 397–398\nUninterrupted time, 164\nUninterruptible power supply (UPS)\nsystems, 34\nUnit tests in build phase, 203\nUNIX, recommended reading, 489\nUnused code, bugs in, 270\nUpdating state, 18\nUpgrades\nBlog Search, 226\noverview, 156\nsoftware, 36\nUpgrading live services, 225–226\nblue-green deployment, 230\ncanary process, 227–228\ncontinuous deployment, 236–239\nexercises, 241–242\nfailed code pushes, 239–240\nlive code changes, 236\nlive schema changes, 234–236\nphased roll-outs, 229\nproportional shedding, 230\nrelease atomicity, 240–241\nrolling upgrades, 226–227\nsummary, 241\ntaking down services for, 225–226\ntoggling features, 230–233\nUploading to CDNs, 115\nUPS (uninterruptible power supply)\nsystems, 34\nUptime in SRE vs. traditional enterprise\nIT, 149\nUrgent bug count, monitoring, 336\nUrgent bug resolution, monitoring, 336\nURLs for CDNs, 115\nU.S. Federal Emergency Management\nAdministration web site, 324\nUser Acceptance Testing (UAT), 216\nUser identity in three-tier web service,\n76\nUser satisfaction, monitoring, 336\nUser-speciﬁc data, global load balancing\nwith, 82–83\nUser stories, 189\nUser wait time, automation for, 253\nUtilization, segmentation by, 103\nUtilization Limit load balancing, 74\nVagrant framework, 219\nValue streams in DevOps, 176\nVarnish HTTP accelerator, 478\nVCSs (version control systems), 265–266\nVelocity in DevOps, 179\nVendor lock-in, 56\nVendors\ndesign for operations, 48\noncall escalations, 298\nVersion conﬂicts in containers, 60–61\nVersion control systems (VCSs), 265–266\nVersion-controlled builds, 191\nVertical integration, 64\nViews in live schema changes, 234\nVirtual machine monitor (VMM), 58–59\nVirtual machines\nbeneﬁts, 58\ndeployment phase, 218\ndisadvantages, 59–60\nIaaS, 52\nI/O, 58–59\noverview, 57\nservice platform selection, 66\nVirtual ofﬁces, 166–167\nVirtuous cycle of quality, 200–201\nVisibility at scale, 10–11\nVisual Display of Quantitative Information,\n362\nVisualization, monitoring, 333, 358–362\nVMM (virtual machine monitor), 58–59\nVoice calls for alerts, 293\nVolatile data in OS installation, 219–220\nWait time\nautomation for, 253\nservice delivery, 201\nWAN (wide area network) connections,\n83\nWarmed caches, 106\nWatchdog timers, 130\nWaterfall methodology\noverview, 173–175\nphases, 199\nWAUs (weekly active users), 373\n“Weathering the Unexpected” article,\n320\n",
      "page_number": 546
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 554-559)",
      "start_page": 554,
      "end_page": 559,
      "detection_method": "topic_boundary",
      "content": "524\nIndex\nWeb “Hit” logs, 340\n“Web Search for a Planet: The Google\nCluster Architecture” article, 464\nWeb servers, single-machine, 70–71\nWeb services\nfour-tier, 77–80\nthree-tier, 71–77\nWeber, W.-D.\ndrive errors, 133, 338\nmemory errors, 134\nWeekly active users (WAUs), 373\nWeekly oncall schedules, 288–289\nWeighted RR load balancing, 74\nWheel of Misfortune game, 311–312\n“When the Nerds Go Marching in”\narticle, 316, 320\nWhitebox monitoring, 346–347\nWhitelists, 40–42\n“Why Do Internet Services Fail, and\nWhat Can Be Done about It?”\npaper, 141, 171\nWickens, C. D., 248\nWide area network (WAN) connections,\n83\nWilde, Oscar, 345\nWillis, John, 180, 200\nWilly Wonka, 195\nWoolf, B., 87\nWorker threads, 112\nWorkﬂows\ndesign documents, 280–282\nDevOps, 176–177\nWorking from home, 166–167\nWrites in updating state, 18\nX-axes in AKF Scaling Cube, 99–101\nX-Forwarded-For headers, 73\nY-axes in AKF Scaling Cube, 99,\n101–102\nYan, B., 371\n“You’re Doing It Wrong” article, 479\nYoung, M., 188, 215\nZ-axes in AKF Scaling Cube, 99, 102–104\nZero line crossover, 367\nZones, Solaris, 60\nZooKeeper system, 231, 363\n\n\nThis page intentionally left blank \n\n\nEssential Practices Previously Handed\nDown Only from Mentor to Protégé\ninformit.com/tposa\nWhether you use Linux, Unix, or Windows, this \nwonderfully lucid, often funny cornucopia of \ninformation introduces beginners to advanced \nframeworks valuable for their entire career, yet is \nstructured to help even the most advanced experts \nthrough difﬁcult projects.\nWith 28 new chapters, the third edition has been \nrevised with thousands of updates and clariﬁca-\ntions based on reader feedback. This new edition \nalso incorporates DevOps strategies even for \nnon-DevOps environments.\nFor more information and sample content visit informit.com/tposa.\neBook and print formats available.\n\n\nRegister the Addison-Wesley, Exam \nCram, Prentice Hall, Que, and \nSams products you own to unlock \ngreat beneﬁ ts. \nTo begin the registration process, \nsimply go to informit.com/register \nto sign in or create an account. \nYou will then be prompted to enter \nthe 10- or 13-digit ISBN that appears \non the back cover of your product.\ninformIT.com \nTHE TRUSTED TECHNOLOGY LEARNING SOURCE\nAddison-Wesley  |  Cisco Press  |  Exam Cram  \nIBM Press   |   Que   |   Prentice Hall   |   Sams \nSAFARI BOOKS ONLINE\nAbout InformIT — THE TRUSTED TECHNOLOGY LEARNING SOURCE\nINFORMIT IS HOME TO THE LEADING TECHNOLOGY PUBLISHING IMPRINTS \nAddison-Wesley Professional, Cisco Press, Exam Cram, IBM Press, Prentice Hall \nProfessional, Que, and Sams. Here you will gain access to quality and trusted content and \nresources from the authors, creators, innovators, and leaders of technology. Whether you’re \nlooking for a book on a new technology, a helpful article, timely newsletters, or access to \nthe Safari Books Online digital library, InformIT has a solution for you.\nRegistering your products can unlock \nthe following beneﬁ ts:\n•  Access to supplemental content, \nincluding bonus chapters, \nsource code, or project ﬁ les. \n•  A coupon to be used on your \nnext purchase.\nRegistration beneﬁ ts vary by product.  \nBeneﬁ ts will be listed on your Account \npage under Registered Products.\ninformit.com/register\nTHIS PRODUCT\n\n\n InformIT is a brand of Pearson and the online presence \nfor the world’s leading technology publishers. It’s your source \nfor reliable and qualified content and knowledge, providing \naccess to the top brands, authors, and contributors from \nthe tech community.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nLearnIT at InformIT\nLooking for a book, eBook, or training video on a new technology? Seek-\ning timely and relevant information and tutorials? Looking for expert opin-\nions, advice, and tips?  InformIT has the solution.\n•  Learn about new releases and special promotions by \nsubscribing to a wide variety of newsletters. \nVisit informit.com/newsletters.\n•   Access FREE podcasts from experts at informit.com/podcasts.\n•   Read the latest author articles and sample chapters at \ninformit.com/articles.\n•  Access thousands of books and videos in the Safari Books \nOnline digital library at safari.informit.com.\n• Get tips from expert blogs at informit.com/blogs.\nVisit informit.com/learn to discover all the ways you can access the \nhottest technology content.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nAre You Part of the IT Crowd?\nConnect with Pearson authors and editors via RSS feeds, Facebook, \nTwitter, YouTube, and more! Visit informit.com/socialconnect.\n",
      "page_number": 554
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "The Practice of Cloud\nSystem Administration\nVolume 2\n",
      "content_length": 53,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "The Practice of Cloud\nSystem Administration\nVolume 2\nThomas A. Limoncelli\nStrata R. Chalup\nChristina J. Hogan\nUpper Saddle River, NJ • Boston • Indianapolis • San Francisco\nNew York • Toronto • Montreal • London • Munich • Paris • Madrid\nCapetown • Sydney • Tokyo • Singapore • Mexico City\nDevOps and SRE Practices for\nWeb Services\n",
      "content_length": 332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed\nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nThe authors and publisher have taken care in the preparation of this book, but make no expressed\nor implied warranty of any kind and assume no responsibility for errors or omissions. No liability is\nassumed for incidental or consequential damages in connection with or arising out of the use of the\ninformation or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales opportunities (which may\ninclude electronic versions; custom cover designs; and content particular to your business, training\ngoals, marketing focus, or branding interests), please contact our corporate sales department at corp-\nsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the United States, please contact intlcs@pearsoned.com.\nVisit us on the Web: informit.com/aw\nLibrary of Congress Cataloging-in-Publication Data\nLimoncelli, Tom.\nThe practice of cloud system administration : designing and operating large distributed systems /\nThomas A. Limoncelli, Strata R. Chalup, Christina J. Hogan.\nvolumes\ncm\nIncludes bibliographical references and index.\nISBN-13: 978-0-321-94318-7 (volume 2 : paperback)\nISBN-10: 0-321-94318-X (volume 2 : paperback)\n1. Computer networks—Management. 2. Computer systems. 3. Cloud computing. 4. Electronic data\nprocessing—Distributed processing. I. Chalup, Strata R. II. Hogan, Christina J. III. Title.\nTK5105.5.L529 2015\n004.67’82068—dc23\n2014024033\nCopyright © 2015 Thomas A. Limoncelli, Virtual.NET Inc., Christina J. Lear née Hogan\nAll rights reserved. Printed in the United States of America. This publication is protected by copyright,\nand permission must be obtained from the publisher prior to any prohibited reproduction, storage in a\nretrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying,\nrecording, or likewise. To obtain permission to use material from this work, please submit a written\nrequest to Pearson Education, Inc., Permissions Department, 200 Old Tappan Road, Old Tappan, New\nJersey 07675, or you may fax your request to (201) 236-3290.\nISBN-13: 978-0-321-94318-7\nISBN-10: 0-321-94318-X\n3\n17\n",
      "content_length": 2514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Contents at a Glance\nContents\nvii\nPreface\nxxiii\nAbout the Authors\nxxix\nIntroduction\n1\nPart I\nDesign: Building It\n7\nChapter 1\nDesigning in a Distributed World\n9\nChapter 2\nDesigning for Operations\n31\nChapter 3\nSelecting a Service Platform\n51\nChapter 4\nApplication Architectures\n69\nChapter 5\nDesign Patterns for Scaling\n95\nChapter 6\nDesign Patterns for Resiliency\n119\nPart II\nOperations: Running It\n145\nChapter 7\nOperations in a Distributed World\n147\nChapter 8\nDevOps Culture\n171\nChapter 9\nService Delivery: The Build Phase\n195\nChapter 10 Service Delivery: The Deployment Phase\n211\nChapter 11 Upgrading Live Services\n225\nChapter 12 Automation\n243\nChapter 13 Design Documents\n275\nChapter 14 Oncall\n285\nChapter 15 Disaster Preparedness\n307\nChapter 16 Monitoring Fundamentals\n331\nv\n",
      "content_length": 776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "vi\nContents at a Glance\nChapter 17 Monitoring Architecture and Practice\n345\nChapter 18 Capacity Planning\n365\nChapter 19 Creating KPIs\n387\nChapter 20 Operational Excellence\n401\nEpilogue\n417\nPart III\nAppendices\n419\nAppendix A Assessments\n421\nAppendix B The Origins and Future of Distributed Computing\nand Clouds\n451\nAppendix C Scaling Terminology and Concepts\n475\nAppendix D Templates and Examples\n481\nAppendix E Recommended Reading\n487\nBibliography\n491\nIndex\n499\n",
      "content_length": 462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Contents\nPreface\nxxiii\nAbout the Authors\nxxix\nIntroduction\n1\nPart I\nDesign: Building It\n7\n1\nDesigning in a Distributed World\n9\n1.1\nVisibility at Scale\n10\n1.2\nThe Importance of Simplicity\n11\n1.3\nComposition\n12\n1.3.1\nLoad Balancer with Multiple Backend Replicas\n12\n1.3.2\nServer with Multiple Backends\n14\n1.3.3\nServer Tree\n16\n1.4\nDistributed State\n17\n1.5\nThe CAP Principle\n21\n1.5.1\nConsistency\n21\n1.5.2\nAvailability\n21\n1.5.3\nPartition Tolerance\n22\n1.6\nLoosely Coupled Systems\n24\n1.7\nSpeed\n26\n1.8\nSummary\n29\nExercises\n30\nvii\n",
      "content_length": 521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "viii\nContents\n2\nDesigning for Operations\n31\n2.1\nOperational Requirements\n31\n2.1.1\nConﬁguration\n33\n2.1.2\nStartup and Shutdown\n34\n2.1.3\nQueue Draining\n35\n2.1.4\nSoftware Upgrades\n36\n2.1.5\nBackups and Restores\n36\n2.1.6\nRedundancy\n37\n2.1.7\nReplicated Databases\n37\n2.1.8\nHot Swaps\n38\n2.1.9\nToggles for Individual Features\n39\n2.1.10\nGraceful Degradation\n39\n2.1.11\nAccess Controls and Rate Limits\n40\n2.1.12\nData Import Controls\n41\n2.1.13\nMonitoring\n42\n2.1.14\nAuditing\n42\n2.1.15\nDebug Instrumentation\n43\n2.1.16\nException Collection\n43\n2.1.17\nDocumentation for Operations\n44\n2.2\nImplementing Design for Operations\n45\n2.2.1\nBuild Features in from the Beginning\n45\n2.2.2\nRequest Features as They Are Identiﬁed\n46\n2.2.3\nWrite the Features Yourself\n47\n2.2.4\nWork with a Third-Party Vendor\n48\n2.3\nImproving the Model\n48\n2.4\nSummary\n49\nExercises\n50\n3\nSelecting a Service Platform\n51\n3.1\nLevel of Service Abstraction\n52\n3.1.1\nInfrastructure as a Service\n52\n3.1.2\nPlatform as a Service\n54\n3.1.3\nSoftware as a Service\n55\n3.2\nType of Machine\n56\n3.2.1\nPhysical Machines\n57\n3.2.2\nVirtual Machines\n57\n3.2.3\nContainers\n60\n",
      "content_length": 1098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Contents\nix\n3.3\nLevel of Resource Sharing\n62\n3.3.1\nCompliance\n63\n3.3.2\nPrivacy\n63\n3.3.3\nCost\n63\n3.3.4\nControl\n64\n3.4\nColocation\n65\n3.5\nSelection Strategies\n66\n3.6\nSummary\n68\nExercises\n68\n4\nApplication Architectures\n69\n4.1\nSingle-Machine Web Server\n70\n4.2\nThree-Tier Web Service\n71\n4.2.1\nLoad Balancer Types\n72\n4.2.2\nLoad Balancing Methods\n74\n4.2.3\nLoad Balancing with Shared State\n75\n4.2.4\nUser Identity\n76\n4.2.5\nScaling\n76\n4.3\nFour-Tier Web Service\n77\n4.3.1\nFrontends\n78\n4.3.2\nApplication Servers\n79\n4.3.3\nConﬁguration Options\n80\n4.4\nReverse Proxy Service\n80\n4.5\nCloud-Scale Service\n80\n4.5.1\nGlobal Load Balancer\n81\n4.5.2\nGlobal Load Balancing Methods\n82\n4.5.3\nGlobal Load Balancing with User-Speciﬁc Data\n82\n4.5.4\nInternal Backbone\n83\n4.6\nMessage Bus Architectures\n85\n4.6.1\nMessage Bus Designs\n86\n4.6.2\nMessage Bus Reliability\n87\n4.6.3\nExample 1: Link-Shortening Site\n87\n4.6.4\nExample 2: Employee Human Resources Data Updates\n89\n4.7\nService-Oriented Architecture\n90\n4.7.1\nFlexibility\n91\n4.7.2\nSupport\n91\n4.7.3\nBest Practices\n91\n",
      "content_length": 1030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "x\nContents\n4.8\nSummary\n92\nExercises\n93\n5\nDesign Patterns for Scaling\n95\n5.1\nGeneral Strategy\n96\n5.1.1\nIdentify Bottlenecks\n96\n5.1.2\nReengineer Components\n97\n5.1.3\nMeasure Results\n97\n5.1.4\nBe Proactive\n97\n5.2\nScaling Up\n98\n5.3\nThe AKF Scaling Cube\n99\n5.3.1\nx: Horizontal Duplication\n99\n5.3.2\ny: Functional or Service Splits\n101\n5.3.3\nz: Lookup-Oriented Split\n102\n5.3.4\nCombinations\n104\n5.4\nCaching\n104\n5.4.1\nCache Effectiveness\n105\n5.4.2\nCache Placement\n106\n5.4.3\nCache Persistence\n106\n5.4.4\nCache Replacement Algorithms\n107\n5.4.5\nCache Entry Invalidation\n108\n5.4.6\nCache Size\n109\n5.5\nData Sharding\n110\n5.6\nThreading\n112\n5.7\nQueueing\n113\n5.7.1\nBeneﬁts\n113\n5.7.2\nVariations\n113\n5.8\nContent Delivery Networks\n114\n5.9\nSummary\n116\nExercises\n116\n6\nDesign Patterns for Resiliency\n119\n6.1\nSoftware Resiliency Beats Hardware Reliability\n120\n6.2\nEverything Malfunctions Eventually\n121\n6.2.1\nMTBF in Distributed Systems\n121\n6.2.2\nThe Traditional Approach\n122\n6.2.3\nThe Distributed Computing Approach\n123\n",
      "content_length": 993,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Contents\nxi\n6.3\nResiliency through Spare Capacity\n124\n6.3.1\nHow Much Spare Capacity\n125\n6.3.2\nLoad Sharing versus Hot Spares\n126\n6.4\nFailure Domains\n126\n6.5\nSoftware Failures\n128\n6.5.1\nSoftware Crashes\n128\n6.5.2\nSoftware Hangs\n129\n6.5.3\nQuery of Death\n130\n6.6\nPhysical Failures\n131\n6.6.1\nParts and Components\n131\n6.6.2\nMachines\n134\n6.6.3\nLoad Balancers\n134\n6.6.4\nRacks\n136\n6.6.5\nDatacenters\n137\n6.7\nOverload Failures\n138\n6.7.1\nTrafﬁc Surges\n138\n6.7.2\nDoS and DDoS Attacks\n140\n6.7.3\nScraping Attacks\n140\n6.8\nHuman Error\n141\n6.9\nSummary\n142\nExercises\n143\nPart II\nOperations: Running It\n145\n7\nOperations in a Distributed World\n147\n7.1\nDistributed Systems Operations\n148\n7.1.1\nSRE versus Traditional Enterprise IT\n148\n7.1.2\nChange versus Stability\n149\n7.1.3\nDeﬁning SRE\n151\n7.1.4\nOperations at Scale\n152\n7.2\nService Life Cycle\n155\n7.2.1\nService Launches\n156\n7.2.2\nService Decommissioning\n160\n7.3\nOrganizing Strategy for Operational Teams\n160\n7.3.1\nTeam Member Day Types\n162\n7.3.2\nOther Strategies\n165\n",
      "content_length": 997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xii\nContents\n7.4\nVirtual Ofﬁce\n166\n7.4.1\nCommunication Mechanisms\n166\n7.4.2\nCommunication Policies\n167\n7.5\nSummary\n167\nExercises\n168\n8\nDevOps Culture\n171\n8.1\nWhat Is DevOps?\n172\n8.1.1\nThe Traditional Approach\n173\n8.1.2\nThe DevOps Approach\n175\n8.2\nThe Three Ways of DevOps\n176\n8.2.1\nThe First Way: Workﬂow\n176\n8.2.2\nThe Second Way: Improve Feedback\n177\n8.2.3\nThe Third Way: Continual Experimentation and\nLearning\n178\n8.2.4\nSmall Batches Are Better\n178\n8.2.5\nAdopting the Strategies\n179\n8.3\nHistory of DevOps\n180\n8.3.1\nEvolution\n180\n8.3.2\nSite Reliability Engineering\n181\n8.4\nDevOps Values and Principles\n181\n8.4.1\nRelationships\n182\n8.4.2\nIntegration\n182\n8.4.3\nAutomation\n182\n8.4.4\nContinuous Improvement\n183\n8.4.5\nCommon Nontechnical DevOps Practices\n183\n8.4.6\nCommon Technical DevOps Practices\n184\n8.4.7\nRelease Engineering DevOps Practices\n186\n8.5\nConverting to DevOps\n186\n8.5.1\nGetting Started\n187\n8.5.2\nDevOps at the Business Level\n187\n8.6\nAgile and Continuous Delivery\n188\n8.6.1\nWhat Is Agile?\n188\n8.6.2\nWhat Is Continuous Delivery?\n189\n8.7\nSummary\n192\nExercises\n193\n",
      "content_length": 1071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Contents\nxiii\n9\nService Delivery: The Build Phase\n195\n9.1\nService Delivery Strategies\n197\n9.1.1\nPattern: Modern DevOps Methodology\n197\n9.1.2\nAnti-pattern: Waterfall Methodology\n199\n9.2\nThe Virtuous Cycle of Quality\n200\n9.3\nBuild-Phase Steps\n202\n9.3.1\nDevelop\n202\n9.3.2\nCommit\n202\n9.3.3\nBuild\n203\n9.3.4\nPackage\n204\n9.3.5\nRegister\n204\n9.4\nBuild Console\n205\n9.5\nContinuous Integration\n205\n9.6\nPackages as Handoff Interface\n207\n9.7\nSummary\n208\nExercises\n209\n10\nService Delivery: The Deployment Phase\n211\n10.1\nDeployment-Phase Steps\n211\n10.1.1\nPromotion\n212\n10.1.2\nInstallation\n212\n10.1.3\nConﬁguration\n213\n10.2\nTesting and Approval\n214\n10.2.1\nTesting\n215\n10.2.2\nApproval\n216\n10.3\nOperations Console\n217\n10.4\nInfrastructure Automation Strategies\n217\n10.4.1\nPreparing Physical Machines\n217\n10.4.2\nPreparing Virtual Machines\n218\n10.4.3\nInstalling OS and Services\n219\n10.5\nContinuous Delivery\n221\n10.6\nInfrastructure as Code\n221\n10.7\nOther Platform Services\n222\n10.8\nSummary\n222\nExercises\n223\n",
      "content_length": 984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "xiv\nContents\n11\nUpgrading Live Services\n225\n11.1\nTaking the Service Down for Upgrading\n225\n11.2\nRolling Upgrades\n226\n11.3\nCanary\n227\n11.4\nPhased Roll-outs\n229\n11.5\nProportional Shedding\n230\n11.6\nBlue-Green Deployment\n230\n11.7\nToggling Features\n230\n11.8\nLive Schema Changes\n234\n11.9\nLive Code Changes\n236\n11.10\nContinuous Deployment\n236\n11.11\nDealing with Failed Code Pushes\n239\n11.12\nRelease Atomicity\n240\n11.13\nSummary\n241\nExercises\n241\n12\nAutomation\n243\n12.1\nApproaches to Automation\n244\n12.1.1\nThe Left-Over Principle\n245\n12.1.2\nThe Compensatory Principle\n246\n12.1.3\nThe Complementarity Principle\n247\n12.1.4\nAutomation for System Administration\n248\n12.1.5\nLessons Learned\n249\n12.2\nTool Building versus Automation\n250\n12.2.1\nExample: Auto Manufacturing\n251\n12.2.2\nExample: Machine Conﬁguration\n251\n12.2.3\nExample: Account Creation\n251\n12.2.4\nTools Are Good, But Automation Is Better\n252\n12.3\nGoals of Automation\n252\n12.4\nCreating Automation\n255\n12.4.1\nMaking Time to Automate\n256\n12.4.2\nReducing Toil\n257\n12.4.3\nDetermining What to Automate First\n257\n12.5\nHow to Automate\n258\n12.6\nLanguage Tools\n258\n12.6.1\nShell Scripting Languages\n259\n12.6.2\nScripting Languages\n259\n",
      "content_length": 1170,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Contents\nxv\n12.6.3\nCompiled Languages\n260\n12.6.4\nConﬁguration Management Languages\n260\n12.7\nSoftware Engineering Tools and Techniques\n262\n12.7.1\nIssue Tracking Systems\n263\n12.7.2\nVersion Control Systems\n265\n12.7.3\nSoftware Packaging\n266\n12.7.4\nStyle Guides\n266\n12.7.5\nTest-Driven Development\n267\n12.7.6\nCode Reviews\n268\n12.7.7\nWriting Just Enough Code\n269\n12.8\nMultitenant Systems\n270\n12.9\nSummary\n271\nExercises\n272\n13\nDesign Documents\n275\n13.1\nDesign Documents Overview\n275\n13.1.1\nDocumenting Changes and Rationale\n276\n13.1.2\nDocumentation as a Repository of Past\nDecisions\n276\n13.2\nDesign Document Anatomy\n277\n13.3\nTemplate\n279\n13.4\nDocument Archive\n279\n13.5\nReview Workﬂows\n280\n13.5.1\nReviewers and Approvers\n281\n13.5.2\nAchieving Sign-off\n281\n13.6\nAdopting Design Documents\n282\n13.7\nSummary\n283\nExercises\n284\n14\nOncall\n285\n14.1\nDesigning Oncall\n285\n14.1.1\nStart with the SLA\n286\n14.1.2\nOncall Roster\n287\n14.1.3\nOnduty\n288\n14.1.4\nOncall Schedule Design\n288\n14.1.5\nThe Oncall Calendar\n290\n14.1.6\nOncall Frequency\n291\n",
      "content_length": 1018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xvi\nContents\n14.1.7\nTypes of Notiﬁcations\n292\n14.1.8\nAfter-Hours Maintenance Coordination\n294\n14.2\nBeing Oncall\n294\n14.2.1\nPre-shift Responsibilities\n294\n14.2.2\nRegular Oncall Responsibilities\n294\n14.2.3\nAlert Responsibilities\n295\n14.2.4\nObserve, Orient, Decide, Act (OODA)\n296\n14.2.5\nOncall Playbook\n297\n14.2.6\nThird-Party Escalation\n298\n14.2.7\nEnd-of-Shift Responsibilities\n299\n14.3\nBetween Oncall Shifts\n299\n14.3.1\nLong-Term Fixes\n299\n14.3.2\nPostmortems\n300\n14.4\nPeriodic Review of Alerts\n302\n14.5\nBeing Paged Too Much\n304\n14.6\nSummary\n305\nExercises\n306\n15\nDisaster Preparedness\n307\n15.1\nMindset\n308\n15.1.1\nAntifragile Systems\n308\n15.1.2\nReducing Risk\n309\n15.2\nIndividual Training: Wheel of Misfortune\n311\n15.3\nTeam Training: Fire Drills\n312\n15.3.1\nService Testing\n313\n15.3.2\nRandom Testing\n314\n15.4\nTraining for Organizations: Game Day/DiRT\n315\n15.4.1\nGetting Started\n316\n15.4.2\nIncreasing Scope\n317\n15.4.3\nImplementation and Logistics\n318\n15.4.4\nExperiencing a DiRT Test\n320\n15.5\nIncident Command System\n323\n15.5.1\nHow It Works: Public Safety Arena\n325\n15.5.2\nHow It Works: IT Operations Arena\n326\n15.5.3\nIncident Action Plan\n326\n15.5.4\nBest Practices\n327\n15.5.5\nICS Example\n328\n",
      "content_length": 1184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Contents\nxvii\n15.6\nSummary\n329\nExercises\n330\n16\nMonitoring Fundamentals\n331\n16.1\nOverview\n332\n16.1.1\nUses of Monitoring\n333\n16.1.2\nService Management\n334\n16.2\nConsumers of Monitoring Information\n334\n16.3\nWhat to Monitor\n336\n16.4\nRetention\n338\n16.5\nMeta-monitoring\n339\n16.6\nLogs\n340\n16.6.1\nApproach\n341\n16.6.2\nTimestamps\n341\n16.7\nSummary\n342\nExercises\n342\n17\nMonitoring Architecture and Practice\n345\n17.1\nSensing and Measurement\n346\n17.1.1\nBlackbox versus Whitebox Monitoring\n346\n17.1.2\nDirect versus Synthesized Measurements\n347\n17.1.3\nRate versus Capability Monitoring\n348\n17.1.4\nGauges versus Counters\n348\n17.2\nCollection\n350\n17.2.1\nPush versus Pull\n350\n17.2.2\nProtocol Selection\n351\n17.2.3\nServer Component versus Agent versus Poller\n352\n17.2.4\nCentral versus Regional Collectors\n352\n17.3\nAnalysis and Computation\n353\n17.4\nAlerting and Escalation Manager\n354\n17.4.1\nAlerting, Escalation, and Acknowledgments\n355\n17.4.2\nSilence versus Inhibit\n356\n17.5\nVisualization\n358\n17.5.1\nPercentiles\n359\n17.5.2\nStack Ranking\n360\n17.5.3\nHistograms\n361\n17.6\nStorage\n362\n",
      "content_length": 1059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "xviii\nContents\n17.7\nConﬁguration\n362\n17.8\nSummary\n363\nExercises\n364\n18\nCapacity Planning\n365\n18.1\nStandard Capacity Planning\n366\n18.1.1\nCurrent Usage\n368\n18.1.2\nNormal Growth\n369\n18.1.3\nPlanned Growth\n369\n18.1.4\nHeadroom\n370\n18.1.5\nResiliency\n370\n18.1.6\nTimetable\n371\n18.2\nAdvanced Capacity Planning\n371\n18.2.1\nIdentifying Your Primary Resources\n372\n18.2.2\nKnowing Your Capacity Limits\n372\n18.2.3\nIdentifying Your Core Drivers\n373\n18.2.4\nMeasuring Engagement\n374\n18.2.5\nAnalyzing the Data\n375\n18.2.6\nMonitoring the Key Indicators\n380\n18.2.7\nDelegating Capacity Planning\n381\n18.3\nResource Regression\n381\n18.4\nLaunching New Services\n382\n18.5\nReduce Provisioning Time\n384\n18.6\nSummary\n385\nExercises\n386\n19\nCreating KPIs\n387\n19.1\nWhat Is a KPI?\n388\n19.2\nCreating KPIs\n389\n19.2.1\nStep 1: Envision the Ideal\n390\n19.2.2\nStep 2: Quantify Distance to the Ideal\n390\n19.2.3\nStep 3: Imagine How Behavior Will Change\n390\n19.2.4\nStep 4: Revise and Select\n391\n19.2.5\nStep 5: Deploy the KPI\n392\n19.3\nExample KPI: Machine Allocation\n393\n19.3.1\nThe First Pass\n393\n",
      "content_length": 1046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "Contents\nxix\n19.3.2\nThe Second Pass\n394\n19.3.3\nEvaluating the KPI\n396\n19.4\nCase Study: Error Budget\n396\n19.4.1\nConﬂicting Goals\n396\n19.4.2\nA Uniﬁed Goal\n397\n19.4.3\nEveryone Beneﬁts\n398\n19.5\nSummary\n399\nExercises\n399\n20\nOperational Excellence\n401\n20.1\nWhat Does Operational Excellence Look Like?\n401\n20.2\nHow to Measure Greatness\n402\n20.3\nAssessment Methodology\n403\n20.3.1\nOperational Responsibilities\n403\n20.3.2\nAssessment Levels\n405\n20.3.3\nAssessment Questions and Look-For’s\n407\n20.4\nService Assessments\n407\n20.4.1\nIdentifying What to Assess\n408\n20.4.2\nAssessing Each Service\n408\n20.4.3\nComparing Results across Services\n409\n20.4.4\nActing on the Results\n410\n20.4.5\nAssessment and Project Planning Frequencies\n410\n20.5\nOrganizational Assessments\n411\n20.6\nLevels of Improvement\n412\n20.7\nGetting Started\n413\n20.8\nSummary\n414\nExercises\n415\nEpilogue\n416\nPart III\nAppendices\n419\nA\nAssessments\n421\nA.1\nRegular Tasks (RT)\n423\nA.2\nEmergency Response (ER)\n426\nA.3\nMonitoring and Metrics (MM)\n428\n",
      "content_length": 988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xx\nContents\nA.4\nCapacity Planning (CP)\n431\nA.5\nChange Management (CM)\n433\nA.6\nNew Product Introduction and Removal (NPI/NPR)\n435\nA.7\nService Deployment and Decommissioning (SDD)\n437\nA.8\nPerformance and Efﬁciency (PE)\n439\nA.9\nService Delivery: The Build Phase\n442\nA.10\nService Delivery: The Deployment Phase\n444\nA.11\nToil Reduction\n446\nA.12\nDisaster Preparedness\n448\nB\nThe Origins and Future of Distributed Computing and Clouds\n451\nB.1\nThe Pre-Web Era (1985–1994)\n452\nAvailability Requirements\n452\nTechnology\n453\nScaling\n454\nHigh Availability\n454\nCosts\n454\nB.2\nThe First Web Era: The Bubble (1995–2000)\n455\nAvailability Requirements\n455\nTechnology\n455\nScaling\n456\nHigh Availability\n457\nCosts\n459\nB.3\nThe Dot-Bomb Era (2000–2003)\n459\nAvailability Requirements\n460\nTechnology\n460\nHigh Availability\n461\nScaling\n462\nCosts\n464\nB.4\nThe Second Web Era (2003–2010)\n465\nAvailability Requirements\n465\nTechnology\n465\nHigh Availability\n466\nScaling\n467\nCosts\n468\n",
      "content_length": 949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "Contents\nxxi\nB.5\nThe Cloud Computing Era (2010–present)\n469\nAvailability Requirements\n469\nCosts\n469\nScaling and High Availability\n471\nTechnology\n472\nB.6\nConclusion\n472\nExercises\n473\nC\nScaling Terminology and Concepts\n475\nC.1\nConstant, Linear, and Exponential Scaling\n475\nC.2\nBig O Notation\n476\nC.3\nLimitations of Big O Notation\n478\nD\nTemplates and Examples\n481\nD.1\nDesign Document Template\n481\nD.2\nDesign Document Example\n482\nD.3\nSample Postmortem Template\n484\nE\nRecommended Reading\n487\nBibliography\n491\nIndex\n499\n",
      "content_length": 514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "Preface\nWhich of the following statements are true?\n1. The most reliable systems are built using cheap, unreliable components.\n2. The techniques that Google uses to scale to billions of users follow the same\npatterns you can use to scale a system that handles hundreds of users.\n3. The more risky a procedure is, the more you should do it.\n4. Some of the most important software features are the ones that users never see.\n5. You should pick random machines and power them off.\n6. The code for every feature Facebook will announce in the next six months is\nprobably in your browser already.\n7. Updating software multiple times a day requires little human effort.\n8. Being oncall doesn’t have to be a stressful, painful experience.\n9. You shouldn’t monitor whether machines are up.\n10. Operations and management can be conducted using the scientiﬁc principles\nof experimentation and evidence.\n11. Google has rehearsed what it would do in case of a zombie attack.\nAll of these statements are true. By the time you ﬁnish reading this book, you’ll\nknow why.\nThis is a book about building and running cloud-based services on a large\nscale: internet-based services for millions or billions of users. That said, every day\nmore and more enterprises are adopting these techniques. Therefore, this is a book\nfor everyone.\nThe intended audience is system administrators and their managers. We do\nnot assume a background in computer science, but we do assume experience with\nUNIX/Linux system administration, networking, and operating system concepts.\nOur focus is on building and operating the services that make up the cloud,\nnot a guide to using cloud-based services.\nxxiii\n",
      "content_length": 1665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "xxiv\nPreface\nCloud services must be available, fast, and secure. At cloud scale, this is a\nunique engineering feat. Therefore cloud-scale services are engineered differently\nthan your typical enterprise service. Being available is important because the\nInternet is open 24 × 7 and has users in every time zone. Being fast is important\nbecause users are frustrated by slow services, so slow services lose out to faster\nrivals. Being secure is important because, as caretakers of other people’s data, we\nare duty-bound (and legally responsible) to protect people’s data.\nThese requirements are intermixed. If a site is not secure, by deﬁnition, it\ncannot be made reliable. If a site is not fast, it is not sufﬁciently available. If a site\nis down, by deﬁnition, it is not fast.\nThe most visible cloud-scale services are web sites. However, there is a\nhuge ecosystem of invisible internet-accessible services that are not accessed with\na browser. For example, smartphone apps use API calls to access cloud-based\nservices.\nFor the remainder of this book we will tend to use the term “distributed com-\nputing” rather than “cloud computing.” Cloud computing is a marketing term that\nmeans different things to different people. Distributed computing describes an archi-\ntecture where applications and services are provided using many machines rather\nthan one.\nThis is a book of fundamental principles and practices that are timeless.\nTherefore we don’t make recommendations about which speciﬁc products or tech-\nnologies to use. We could provide a comparison of the top ﬁve most popular web\nservers or NoSQL databases or continuous build systems. If we did, then this book\nwould be out of date the moment it is published. Instead, we discuss the quali-\nties one should look for when selecting such things. We provide a model to work\nfrom. This approach is intended to prepare you for a long career where technology\nchanges over time but you are always prepared. We will, of course, illustrate our\npoints with speciﬁc technologies and products, but not as an endorsement of those\nproducts and services.\nThis book is, at times, idealistic. This is deliberate. We set out to give the reader\na vision of how things can be, what to strive for. We are here to raise the bar.\nAbout This Book\nThe book is structured in two parts, Design and Operations.\nPart I captures our thinking on the design of large, complex, cloud-based dis-\ntributed computing systems. After the Introduction, we tackle each element of\ndesign from the bottom layers to the top. We cover distributed systems from the\npoint of view of a system administrator, not a computer scientist. To operate a\nsystem, one must be able to understand its internals.\n",
      "content_length": 2709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Preface\nxxv\nPart II describes how to run such systems. The ﬁrst chapters cover the most\nfundamental issues. Later chapters delve into more esoteric technical activities,\nthen high-level planning and strategy that tie together all of the above.\nAt the end is extra material including an assessment system for operations\nteams, a highly biased history of distributed computing, templates for forms\nmentioned in the text, recommended reading, and other reference material.\nWe’re excited to present a new feature of our book series: our operational\nassessment system. This system consists of a series of assessments you can use\nto evaluate your operations and ﬁnd areas of improvement. The assessment ques-\ntions and “Look For” recommendations are found in Appendix A. Chapter 20 is\nthe instruction manual.\nAcknowledgments\nThis book wouldn’t have been possible without the help and feedback we received\nfrom our community and people all over the world. The DevOps community was\ngenerous in its assistance.\nFirst, we’d like to thank our spouses and families: Christine Polk, Mike\nChalup, and Eliot and Joanna Lear. Your love and patience make all this possible.\nIf we have seen further, it is by standing on the shoulders of giants. Certain\nchapters relied heavily on support and advice from particular people: John Looney\nand Cian Synnott (Chapter 1); Marty Abbott and Michael Fisher (Chapter 5);\nDamon Edwards, Alex Honor, and Jez Humble (Chapters 9 and 10); John Allspaw\n(Chapter 12); Brent Chapman (Chapter 15); Caskey Dickson and Theo Schlossna-\ngle (Chapters 16 and 17); Arun Kejariwal and Bruce Yan (Chapter 18); Benjamin\nTreynor Sloss (Chapter 19); and Geoff Halprin (Chapter 20 and Appendix A).\nThanks to Gene Kim for the “strategic” inspiration and encouragement.\nDozens of people helped us—some by supplying anecdotes, some by review-\ning parts of or the entire book. The only fair way to thank them all is alphabetically\nand to apologize in advance to anyone we left out: Thomas Baden, George Beech,\nRaymond Blum, Kyle Brandt, Mark Burgess, Nick Craver, Geoff Dalgas, Robert\nP. J. Day, Patrick Debois, Bill Duane, Paul Evans, David Fullerton, Tom Geller, Peter\nGrace, Elizabeth Hamon Reid, Jim Hickstein, Zachary Hueras, Matt Jones, Jennifer\nJoy, Jimmy Kaplowitz, Daniel V. Klein, Steven Levine, Cory Lueninghoener, Shane\nMadden, Jim Maurer, Stephen McHenry, Dinah McNutt, Scott Hazen Mueller,\nSteve Murawski, Mohit Muthanna, Lenny Rachitsky, Amy Rich, Adele Shakal,\nBart Silverstrim, Josh Simon, Joel Spolsky, Desiree Sylvester, Win Treese, Todd\nUnderwood, Nicole Forsgren Velasquez, and Dave Zwieback.\nLast but not least, thanks to everyone from Addison-Wesley. In particular,\nthanks to Debra Williams Cauley, for guiding us to Addison-Wesley and steering\n",
      "content_length": 2765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "xxvi\nPreface\nus the entire way; Michael Thurston, for editing our earliest drafts and reshaping\nthem to be much, much better; Kim Boedigheimer, who coordinated and assisted\nus calmly even when we were panicking; Lori Hughes, our LaTeX wizard; Julie\nNahil, our production manager; Jill Hobbs, our copyeditor; and John Fuller and\nMark Taub, for putting up with all our special requests!\nPart I\nDesign: Building It\nChapter 1: Designing in a Distributed World\nOverview of how distributed systems are designed.\nChapter 2: Designing for Operations\nFeatures software should have to enable smooth operations.\nChapter 3: Selecting a Service Platform\nPhysical and virtual machines, private and public clouds.\nChapter 4: Application Architectures\nBuilding blocks for creating web and other applications.\nChapter 5: Design Patterns for Scaling\nBuilding blocks for growing a service.\nChapter 6: Design Patterns for Resiliency\nBuilding blocks for creating systems that survive failure.\nPart II\nOperations: Running It\nChapter 7: Operations in a Distributed World\nOverview of how distributed systems are run.\nChapter 8: DevOps Culture\nIntroduction to DevOps culture, its history and practices.\nChapter 9: Service Delivery: The Build Phase\nHow a service gets built and prepared for production.\nChapter 10: Service Delivery: The Deployment Phase\nHow a service is tested, approved, and put into production.\nChapter 11: Upgrading Live Services\nHow to upgrade services without downtime.\nChapter 12: Automation\nCreating tools and automating operational work.\nChapter 13: Design Documents\nCommunicating designs and intentions in writing.\nChapter 14: Oncall\nHandling exceptions.\nChapter 15: Disaster Preparedness\nMaking systems stronger through planning and practice.\n",
      "content_length": 1744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "Preface\nxxvii\nChapter 16: Monitoring Fundamentals\nMonitoring terminology and strategy.\nChapter 17: Monitoring Architecture and Practice\nThe components and practice of monitoring.\nChapter 18: Capacity Planning\nPlanning for and providing additional resources before we need them.\nChapter 19: Creating KPIs\nDriving behavior scientiﬁcally through measurement and reﬂection.\nChapter 20: Operational Excellence\nStrategies for constant improvement.\nEpilogue\nSome ﬁnal thoughts.\nPart III\nAppendices\nAppendix A: Assessments\nAppendix B: The Origins and Future of Distributed Computing and Clouds\nAppendix C: Scaling Terminology and Concepts\nAppendix D: Templates and Examples\nAppendix E: Recommended Reading\nBibliography\nIndex\n",
      "content_length": 717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "About the Authors\nThomas A. Limoncelli is an internationally recognized author, speaker, and\nsystem administrator. During his seven years at Google NYC, he was an SRE for\nprojects such as Blog Search, Ganeti, and various internal enterprise IT services. He\nnow works as an SRE at Stack Exchange, Inc., home of ServerFault.com and Stack-\nOverﬂow.com. His ﬁrst paid system administration job was as a student at Drew\nUniversity in 1987, and he has since worked at small and large companies, includ-\ning AT&T/Lucent Bell Labs. His best-known books include Time Management for\nSystem Administrators (O’Reilly) and The Practice of System and Network Adminis-\ntration, Second Edition (Addison-Wesley). His hobbies include grassroots activism,\nfor which his work has been recognized at state and national levels. He lives in\nNew Jersey.\nStrata R. Chalup has been leading and managing complex IT projects for many\nyears, serving in roles ranging from project manager to director of operations.\nStrata has authored numerous articles on management and working with teams\nand has applied her management skills on various volunteer boards, including\nBayLISA and SAGE. She started administering VAX Ultrix and Unisys UNIX in\n1983 at MIT in Boston, and spent the dot-com years in Silicon Valley building inter-\nnet services for clients like iPlanet and Palm. In 2007, she joined Tom and Christina\nto create the second edition of The Practice of System and Network Administration\n(Addison-Wesley). Her hobbies include working with new technologies, including\nArduino and various 2D CAD/CAM devices, as well as being a master gardener.\nShe lives in Santa Clara County, California.\nChristina J. Hogan has twenty years of experience in system administration\nand network engineering, from Silicon Valley to Italy and Switzerland. She has\ngained experience in small startups, mid-sized tech companies, and large global\ncorporations. She worked as a security consultant for many years and her cus-\ntomers included eBay, Silicon Graphics, and SystemExperts. In 2005 she and Tom\nxxix\n",
      "content_length": 2061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "xxx\nAbout the Authors\nshared the SAGE Outstanding Achievement Award for their book The Practice of\nSystem and Network Administration (Addison-Wesley). She has a bachelor’s degree\nin mathematics, a master’s degree in computer science, a doctorate in aeronautical\nengineering, and a diploma in law. She also worked for six years as an aero-\ndynamicist in a Formula 1 racing team and represented Ireland in the 1988 Chess\nOlympiad. She lives in Switzerland.\n",
      "content_length": 455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "Introduction\nThe goal of this book is to help you build and run the best cloud-scale service\npossible. What is the ideal environment that we seek to create?\nBusiness Objectives\nSimply stated, the end result of our ideal environment is that business objectives\nare met. That may sound a little boring but actually it is quite exciting to work\nwhere the entire company is focused and working together on the same goals.\nTo achieve this, we must understand the business objectives and work back-\nward to arrive at the system we should build.\nMeeting business objectives means knowing what those objectives are, having\na plan to achieve them, and working through the roadblocks along the way.\nWell-deﬁned business objectives are measurable, and such measurements can\nbe collected in an automated fashion. A dashboard is automatically generated so\neveryone is aware of progress. This transparency enhances trust.\nHere are some sample business objectives:\n• Sell our products via a web site\n• Provide service 99.99 percent of the time\n• Process x million purchases per month, growing 10 percent monthly\n• Introduce new features twice a week\n• Fix major bugs within 24 hours\nIn our ideal environment, business and technical teams meet their objectives and\nproject goals predictably and reliably. Because of this, both types of teams trust\nthat other teams will meet their future objectives. As a result, teams can plan\nbetter. They can make more aggressive plans because there is conﬁdence that exter-\nnal dependencies will not fail. This permits even more aggressive planning. Such\nan approach creates an upward spiral that accelerates progress throughout the\ncompany, beneﬁting everyone.\n1\n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "2\nIntroduction\nIdeal System Architecture\nThe ideal service is built on a solid architecture. It meets the requirements of the\nservice today and provides an obvious path for growth as the system becomes\nmore popular and receives more trafﬁc. The system is resilient to failure. Rather\nthan being surprised by failures and treating them as exceptions, the architecture\naccepts that hardware and software failures are a part of the physics of information\ntechnology (IT). As a result, the architecture includes redundancy and resiliency\nfeatures that work around failures. Components fail but the system survives.\nEach subsystem that makes up our service is itself a service. All subsys-\ntems are programmable via an application programming interface (API). Thus,\nthe entire system is an ecosystem of interconnected subservices. This is called a\nservice-oriented architecture (SOA). Because all these systems communicate over\nthe same underlying protocol, there is uniformity in how they are managed.\nBecause each subservice is loosely coupled to the others, all of these services can\nbe independently scaled, upgraded, or replaced.\nThe geometry of the infrastructure is described electronically. This electronic\ndescription is read by IT automation systems, which then build the production\nenvironment without human intervention. Because of this automation, the entire\ninfrastructure can be re-created elsewhere. Software engineers use the automation\nto make micro-versions of the environment for their personal use. Quality and test\nengineers use the automation to create environments for system tests.\nThis “infrastructure as code” can be achieved whether we use physical\nmachines or virtual machines, and whether they are in datacenters we run or are\nhosted by a cloud provider. With virtual machines there is an obvious API available\nfor spinning up a new machine. However, even with physical machines, the entire\nﬂow from bare metal to working system can be automated. In our ideal world the\nautomation makes it possible to create environments using combinations of phys-\nical and virtual machines. Developers may build the environment out of virtual\nmachines. The production environment might consist of a mixture of physical and\nvirtual machines. The temporary and unexpected need for additional capacity may\nrequire extending the production environment into one or more cloud providers\nfor some period of time.\nIdeal Release Process\nOur ideal environment has a smooth ﬂow of code from development to operations.\nTraditionally (not in our ideal environment) the sequence looks like this:\n1. Developers check code into a repository.\n2. Test engineers put the code through a number of tests.\n",
      "content_length": 2695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "Introduction\n3\n3. If all the tests pass, the a release engineer builds the packages that will be used\nto deploy the software. Most of the ﬁles come from the source code repos-\nitory, but some ﬁles may be needed from other sources such as a graphics\ndepartment or documentation writers.\n4. A test environment is created; without an “infrastructure as code” model, this\nmay take weeks.\n5. The packages are deployed into a test environment.\n6. Test engineers perform further tests, focusing on the interaction between\nsubsystems.\n7. If all these tests succeed, the code is put into production.\n8. System administrators upgrade systems while looking for failures.\n9. If there are failures, the software is rolled back.\nDoing these steps manually incurs a lot of risk, owing to the assumptions that the\nright people are available, that the steps are done the same way every time, that\nnobody makes mistakes, and that all the tasks are completed in time.\nMistakes, bugs, and errors happen, of course—and as a result defects are\npassed down the line to the next stage. When a mistake is discovered the ﬂow of\nprogress is reversed as the team members who were responsible for the previous\nstage are told to ﬁx their problem. This means progress is halted and time is lost.\nA typical response to a risky process is to do it as rarely as possible. Thus\nthere is a temptation to do as few releases as possible. The result is “mega-releases”\nlaunched only a few times a year.\nHowever, by batching up so many changes at once, we actually create more\nrisk. How can we be sure thousands of changes, released simultaneously, will\nall work on the ﬁrst try? We can’t. Therefore we become even more recalcitrant\ntoward and fearful of making changes. Soon change becomes nearly impossible\nand innovation comes to a halt.\nNot so in our ideal environment.\nIn our ideal environment, we ﬁnd automation that eliminates all manual steps\nin the software build, test, release, and deployment processes. The automation\naccurately and consistently performs tests that prevent defects from being passed\nto the next step. As a consequence, the ﬂow of progress is in one direction: forward.\nRather than mega-releases, our ideal environment creates micro-releases. We\nreduce risk by doing many deployments, each with a few small changes. In fact,\nwe might do 100 deployments per day.\n1. When the developers check in code, a system detects this fact and triggers a\nseries of automated tests. These tests verify basic code functionality.\n2. If these tests pass, the process of building the packages is kicked off and runs\nin a completely automated fashion.\n",
      "content_length": 2621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "4\nIntroduction\n3. The successful creation of new packages triggers the creation of a test envi-\nronment. Building a test environment used to be a long week of connecting\ncables and installing machines. But with infrastructure as code, the entire\nenvironment is created quickly with no human intervention.\n4. When the test environment is complete, a series of automated tests are run.\n5. On successful completion the new packages are rolled out to production. The\nroll-out is also automated but orderly and cautious.\n6. Certain systems are upgraded ﬁrst and the system watches for failures. Since\nthe test environment was built with the same automation that built the\nproduction environment, there should be very few differences.\n7. Seeing no failures, the new packages are rolled out to more and more systems\nuntil the entire production environment is upgraded.\nIn our ideal environment all problems are caught before they reach production.\nThat is, roll-out is not a form of testing. Failure during a roll-out to production is\nessentially eliminated. However, if a failure does happen, it would be considered\na serious issue warranting pausing new releases from going into production until\na root causes analysis is completed. Tests are added to detect and prevent future\noccurrences of this failure. Thus, the system gets stronger over time.\nBecause of this automation, the traditional roles of release engineering, qual-\nity assurance, and deployment are practically unrecognizable from their roles at a\ntraditional company. Hours of laborious manual toil are eliminated, leaving more\ntime for improving the packaging system, improving the software quality, and\nreﬁning the deployment process. In other words, people spend more time making\nimprovements in how work is done rather than doing work itself.\nA similar process is used for third-party software. Not all systems are home-\ngrown or come with source code. Deploying third-party services and products\nfollows a similar pattern of release, testing, deployment. However, because these\nproducts and services are developed externally, they require a slightly different\nprocess. New releases are likely to occur less frequently and we have less control\nover what is in each new release. The kind of testing these components require is\nusually related to features, compatibility, and integration.\nIdeal Operations\nOnce the code is in production, operational objectives take precedence. The soft-\nware is instrumented so that it can be monitored. Data is collected about how long\nit takes to process transactions from external users as well as from internal APIs.\nOther indicators such as memory usage are also monitored. This data is collected\nso that operational decisions can be made based on data, not guesses, luck, or\n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "Introduction\n5\nhope. The data is stored for many years so it may be used to predict the future\ncapacity needs.\nMeasurements are used to detect internal problems while they are small, long\nbefore they result in a user-visible outage. We ﬁx problems before they become\noutages. An actual outage is rare and would be investigated with great diligence.\nWhen problems are detected there is a process in place to make sure they are\nidentiﬁed, worked on, and resolved quickly.\nAn automated system detects problems and alerts whoever is oncall. Our\noncall schedule is a rotation constructed so that each shift typically receives a man-\nageable number of alerts. At any given time one person is the primary oncall person\nand is ﬁrst to receive any alerts. If that individual does not respond in time, a sec-\nondary person is alerted. The oncall schedule is prepared far enough in advance\nthat people can plan vacations, recreational activities, and personal time.\nThere is a “playbook” of instructions on how to handle every alert that can be\ngenerated. Each type of alert is documented with a technical description of what\nis wrong, what the business impact is, and how to ﬁx the issue. The playbook is\ncontinually improved. Whoever is oncall uses the playbook to ﬁx the problem. If\nit proves insufﬁcient, there is a well-deﬁned escalation path, usually to the oncall\nperson for the related subsystem. Developers also participate in the oncall rotation\nso they understand the operational pain points of the system they are building.\nAll failures have a corresponding countermeasure, whether it is manually or\nautomatically activated. Countermeasures that are activated frequently are always\nautomated. Our monitoring system detects overuse, as this may indicate a larger\nproblem. The monitoring system collects internal indicator data used by engineers\nto reduce the failure rate as well as improve the countermeasure.\nThe less frequently a countermeasure is activated, the less conﬁdent we are\nthat it will work the next time it is needed. Therefore infrequently activated coun-\ntermeasures are periodically and automatically exercised by intentionally causing\nfailures. Just as we require school children to practice ﬁre drills so that everyone\nknows what to do in an emergency, so we practice ﬁre drills with our operational\npractices. This way our team becomes experienced at implementing the counter-\nmeasures and is conﬁdent that they work. If a database failover process doesn’t\nwork due to an unexpected dependency, it is better to learn this during a live drill\non Monday at 10 rather than during an outage at 4 on a Sunday morning.\nAgain, we reduce risk by increasing repetition rather than shying away from it. The\ntechnical term for improving something through repetition is called “practice.” We\nstrongly believe that practice makes perfect.\nOur ideal environment scales automatically. As more capacity is needed, addi-\ntional capacity comes from internal or external cloud providers. Our dashboards\nindicate when re-architecting will be a better solution than simply allocating more\nRAM, disk, or CPU.\n",
      "content_length": 3112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "6\nIntroduction\nScaling down is also automatic. When the system is overloaded or degraded,\nwe never turn users away with a “503—Service Unavailable” error. Instead, the\nsystem automatically switches to algorithms that use less resources. Bandwidth\nfully utilized? Low-bandwidth versions of the service kick in, displaying fewer\ngraphics or a more simpliﬁed user interface. Databases become corrupted? A read-\nonly version of the service keeps most users satisﬁed.\nEach feature of our service can be individually enabled or disabled. If a feature\nturns out to have negative consequences, such as security holes or unexpectedly\nbad performance, it can be disabled without deploying a different software release.\nWhen a feature is revised, the new code does not eliminate the old\nfunctionality. The new behavior can be disabled to reveal the old behavior. This is\nparticularly useful when rolling out a new user interface. If a release can produce\nboth the old and new user interface, it can be enabled on a per-user basis. This\nenables us to get feedback from “early access” users. On the ofﬁcial release date,\nthe new feature is enabled for successively larger and larger groups. If performance\nproblems are found, the feature can easily be reverted or switched off entirely.\nIn our ideal environment there is excellent operational hygiene. Like brush-\ning our teeth, we regularly do the things that preserve good operational health.\nWe maintain clear and updated documentation for how to handle every counter-\nmeasure, process, and alert. Overactive alerts are ﬁne-tuned, not ignored. Open\nbug counts are kept to a minimum. Outages are followed by the publication\nof a postmortem report with recommendations on how to improve the system\nin the future. Any “quick ﬁx” is followed by a root causes analysis and the\nimplementation of a long-term ﬁx.\nMost importantly, the developers and operations people do not think of them-\nselves as two distinct teams. They are simply specializations within one large\nteam. Some people write more code than others; some people do more operational\nprojects than others. All share responsibility for maintaining high uptime. To that\nend, all members participate in the oncall (pager) rotation. Developers are most\nmotivated to improve code that affects operations when they feel the pain of oper-\nations, too. Operations must understand the development process if they are to be\nable to constructively collaborate.\nNow you know our vision of an ideal environment. The remainder of this book\nwill explain how to create and run it.\n",
      "content_length": 2562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "Part I\nDesign: Building It\n",
      "content_length": 27,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\nDesigning in a Distributed\nWorld\nThere are two ways of constructing\na software design: One way is to\nmake it so simple that there are\nobviously no deﬁciencies and the\nother way is to make it so\ncomplicated that there are no\nobvious deﬁciencies.\n—C.A.R. Hoare, The 1980 ACM\nTuring Award Lecture\nHow does Google Search work? How does your Facebook Timeline stay updated\naround the clock? How does Amazon scan an ever-growing catalog of items to tell\nyou that people who bought this item also bought socks?\nIs it magic? No, it’s distributed computing.\nThis chapter is an overview of what is involved in designing services that use\ndistributed computing techniques. These are the techniques all large web sites use\nto achieve their size, scale, speed, and reliability.\nDistributed computing is the art of building large systems that divide the work\nover many machines. Contrast this with traditional computing systems where a\nsingle computer runs software that provides a service, or client–server computing\nwhere many machines remotely access a centralized service. In distributed com-\nputing there are typically hundreds or thousands of machines working together to\nprovide a large service.\nDistributed computing is different from traditional computing in many ways.\nMost of these differences are due to the sheer size of the system itself. Hundreds or\nthousands of computers may be involved. Millions of users may be served. Billions\nand sometimes trillions of queries may be processed.\n9\n",
      "content_length": 1498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "10\nChapter 1\nDesigning in a Distributed World\n.\nTerms to Know\nServer: Software that provides a function or application program interface\n(API). (Not a piece of hardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nTraffic: A generic term for queries, API calls, or other requests sent to a\nserver.\nPerformant: A system whose performance conforms to (meets or exceeds)\nthe design requirements. A neologism from merging “performance”\nand “conformant.”\nApplication Programming Interface (API): A protocol that governs how\none server talks to another.\nSpeed is important. It is a competitive advantage for a service to be fast and\nresponsive. Users consider a web site sluggish if replies do not come back in 200 ms\nor less. Network latency eats up most of that time, leaving little time for the service\nto compose the page itself.\nIn distributed systems, failure is normal. Hardware failures that are rare, when\nmultiplied by thousands of machines, become common. Therefore failures are\nassumed, designs work around them, and software anticipates them. Failure is an\nexpected part of the landscape.\nDue to the sheer size of distributed systems, operations must be automated.\nIt is inconceivable to manually do tasks that involve hundreds or thousands\nof machines. Automation becomes critical for preparation and deployment of\nsoftware, regular operations, and handling failures.\n1.1 Visibility at Scale\nTo manage a large distributed system, one must have visibility into the system.\nThe ability to examine internal state—called introspection—is required to operate,\ndebug, tune, and repair large systems.\nIn a traditional system, one could imagine an engineer who knows enough\nabout the system to keep an eye on all the critical components or “just knows”\nwhat is wrong based on experience. In a large system, that level of visibility must\nbe actively created by designing systems that draw out the information and make\nit visible. No person or team can manually keep tabs on all the parts.\n",
      "content_length": 2138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "1.2\nThe Importance of Simplicity\n11\nDistributed systems, therefore, require components to generate copious logs\nthat detail what happened in the system. These logs are then aggregated to a central\nlocation for collection, storage, and analysis. Systems may log information that is\nvery high level, such as whenever a user makes a purchase, for each web query,\nor for every API call. Systems may log low-level information as well, such as the\nparameters of every function call in a critical piece of code.\nSystems should export metrics. They should count interesting events, such as\nhow many times a particular API was called, and make these counters accessible.\nIn many cases, special URLs can be used to view this internal state.\nFor example, the Apache HTTP Web Server has a “server-status” page\n(http://www.example.com/server-status/).\nIn addition, components of distributed systems often appraise their own\nhealth and make this information visible. For example, a component may have\na URL that outputs whether the system is ready (OK) to receive new requests.\nReceiving as output anything other than the byte “O” followed by the byte “K”\n(including no response at all) indicates that the system does not want to receive\nnew requests. This information is used by load balancers to determine if the\nserver is healthy and ready to receive trafﬁc. The server sends negative replies\nwhen the server is starting up and is still initializing, and when it is shutting\ndown and is no longer accepting new requests but is processing any requests\nthat are still in ﬂight.\n1.2 The Importance of Simplicity\nIt is important that a design remain as simple as possible while still being able\nto meet the needs of the service. Systems grow and become more complex\nover time. Starting with a system that is already complex means starting at a\ndisadvantage.\nProviding competent operations requires holding a mental model of the sys-\ntem in one’s head. As we work we imagine the system operating and use this\nmental model to track how it works and to debug it when it doesn’t. The more\ncomplex the system, the more difﬁcult it is to have an accurate mental model. An\noverly complex system results in a situation where no single person understands\nit all at any one time.\nIn The Elements of Programming Style, Kernighan and Plauger (1978) wrote:\nDebugging is twice as hard as writing the code in the ﬁrst place. Therefore, if you write\nthe code as cleverly as possible, you are, by deﬁnition, not smart enough to debug it.\nThe same is true for distributed systems. Every minute spent simplifying a design\npays off time and time again when the system is in operation.\n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "12\nChapter 1\nDesigning in a Distributed World\n1.3 Composition\nDistributed systems are composed of many smaller systems. In this section, we\nexplore three fundamental composition patterns in detail:\n• Load balancer with multiple backend replicas\n• Server with multiple backends\n• Server tree\n1.3.1 Load Balancer with Multiple Backend Replicas\nThe ﬁrst composition pattern is the load balancer with multiple backend replicas.\nAs depicted in Figure 1.1, requests are sent to the load balancer server. For each\nrequest, it selects one backend and forwards the request there. The response comes\nback to the load balancer server, which in turn relays it to the original requester.\nThe backends are called replicas because they are all clones or replications of\neach other. A request sent to any replica should produce the same response.\nThe load balancer must always know which backends are alive and ready to\naccept requests. Load balancers send health check queries dozens of times each\nsecond and stop sending trafﬁc to that backend if the health check fails. A health\ncheck is a simple query that should execute quickly and return whether the system\nshould receive trafﬁc.\nPicking which backend to send a query to can be simple or complex. A\nsimple method would be to alternate among the backends in a loop—a practice\ncalled round-robin. Some backends may be more powerful than others, however,\nFigure 1.1: A load balancer with many replicas\n",
      "content_length": 1440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "1.3\nComposition\n13\nand may be selected more often using a proportional round-robin scheme.\nMore complex solutions include the least loaded scheme. In this approach, a\nload balancer tracks how loaded each backend is and always selects the least\nloaded one.\nSelecting the least loaded backend sounds reasonable but a naive implemen-\ntation can be a disaster. A backend may not show signs of being overloaded until\nlong after it has actually become overloaded. This problem arises because it can be\ndifﬁcult to accurately measure how loaded a system is. If the load is a measure-\nment of the number of connections recently sent to the server, this deﬁnition is\nblind to the fact that some connections may be long lasting while others may be\nquick. If the measurement is based on CPU utilization, this deﬁnition is blind to\ninput/output (I/O) overload. Often a trailing average of the last 5 minutes of load\nis used. Trailing averages have a problem in that, as an average, they reﬂect the\npast, not the present. As a consequence, a sharp, sudden increase in load will not\nbe reﬂected in the average for a while.\nImagine a load balancer with 10 backends. Each one is running at 80 percent\nload. A new backend is added. Because it is new, it has no load and, therefore,\nis the least loaded backend. A naive least loaded algorithm would send all trafﬁc\nto this new backend; no trafﬁc would be sent to the other 10 backends. All too\nquickly, the new backend would become absolutely swamped. There is no way a\nsingle backend could process the trafﬁc previously handled by 10 backends. The\nuse of trailing averages would mean the older backends would continue reporting\nartiﬁcially high loads for a few minutes while the new backend would be reporting\nan artiﬁcially low load.\nWith this scheme, the load balancer will believe that the new machine is less\nloaded than all the other machines for quite some time. In such a situation the\nmachine may become so overloaded that it would crash and reboot, or a system\nadministrator trying to rectify the situation might reboot it. When it returns to\nservice, the cycle would start over again.\nSuch situations make the round-robin approach look pretty good. A less naive\nleast loaded implementation would have some kind of control in place that would\nnever send more than a certain number of requests to the same machine in a row.\nThis is called a slow start algorithm.\n.\nTrouble with a Naive Least Loaded Algorithm\nWithout slow start, load balancers have been known to cause many prob-\nlems. One famous example is what happened to the CNN.com web site on\nthe day of the September 11, 2001, terrorist attacks. So many people tried to\naccess CNN.com that the backends became overloaded. One crashed, and then\ncrashed again after it came back up, because the naive least loaded algorithm\n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "14\nChapter 1\nDesigning in a Distributed World\n.\nsent all trafﬁc to it. When it was down, the other backends became overloaded\nand crashed. One at a time, each backend would get overloaded, crash, and\nbecome overloaded from again receiving all the trafﬁc and crash again.\nAs a result the service was essentially unavailable as the system adminis-\ntrators rushed to ﬁgure out what was going on. In their defense, the web was\nnew enough that no one had experience with handling sudden trafﬁc surges\nlike the one encountered on September 11.\nThe solution CNN used was to halt all the backends and boot them at\nthe same time so they would all show zero load and receive equal amounts of\ntrafﬁc.\nThe CNN team later discovered that a few days prior, a software upgrade\nfor their load balancer had arrived but had not yet been installed. The upgrade\nadded a slow start mechanism.\n1.3.2 Server with Multiple Backends\nThe next composition pattern is a server with multiple backends. The server\nreceives a request, sends queries to many backend servers, and composes the ﬁnal\nreply by combining those answers. This approach is typically used when the orig-\ninal query can easily be deconstructed into a number of independent queries that\ncan be combined to form the ﬁnal answer.\nFigure 1.2a illustrates how a simple search engine processes a query with the\nhelp of multiple backends. The frontend receives the request. It relays the query\nto many backend servers. The spell checker replies with information so the search\nengine may suggest alternate spellings. The web and image search backends reply\nwith a list of web sites and images related to the query. The advertisement server\nFigure 1.2: This service is composed of a server and many backends.\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "1.3\nComposition\n15\nreplies with advertisements relevant to the query. Once the replies are received,\nthe frontend uses this information to construct the HTML that makes up the search\nresults page for the user, which is then sent as the reply.\nFigure 1.2b illustrates the same architecture with replicated, load-balanced,\nbackends. The same principle applies but the system is able to scale and survive\nfailures better.\nThis kind of composition has many advantages. The backends do their work\nin parallel. The reply does not have to wait for one backend process to complete\nbefore the next begins. The system is loosely coupled. One backend can fail and the\npage can still be constructed by ﬁlling in some default information or by leaving\nthat area blank.\nThis pattern also permits some rather sophisticated latency management. Sup-\npose this system is expected to return a result in 200 ms or less. If one of the\nbackends is slow for some reason, the frontend doesn’t have to wait for it. If it takes\n10 ms to compose and send the resulting HTML, at 190 ms the frontend can give\nup on the slow backends and generate the page with the information it has. The\nability to manage a latency time budget like that can be very powerful. For exam-\nple, if the advertisement system is slow, search results can be displayed without\nany ads.\nTo be clear, the terms “frontend” and “backend” are a matter of perspective.\nThe frontend sends requests to backends, which reply with a result. A server can be\nboth a frontend and a backend. In the previous example, the server is the backend\nto the web browser but a frontend to the spell check server.\nThere are many variations on this pattern. Each backend can be replicated for\nincreased capacity or resiliency. Caching may be done at various levels.\nThe term fan out refers to the fact that one query results in many new queries,\none to each backend. The queries “fan out” to the individual backends and the\nreplies fan in as they are set up to the frontend and combined into the ﬁnal result.\nAny fan in situation is at risk of having congestion problems. Often small\nqueries may result in large responses. Therefore a small amount of bandwidth is\nused to fan out but there may not be enough bandwidth to sustain the fan in. This\nmay result in congested network links and overloaded servers. It is easy to engineer\nthe system to have the right amount of network and server capacity if the sizes of\nthe queries and replies are consistent, or if there is an occasional large reply. The\ndifﬁcult situation is engineering the system when there are sudden, unpredictable\nbursts of large replies. Some network equipment is engineered speciﬁcally to deal\nwith this situation by dynamically provisioning more buffer space to such bursts.\nLikewise, the backends can rate-limit themselves to avoid creating the situation in\nthe ﬁrst place. Lastly, the frontends can manage the congestion themselves by con-\ntrolling the new queries they send out, by notifying the backends to slow down, or\nby implementing emergency measures to handle the ﬂood better. The last option\nis discussed in Chapter 5.\n",
      "content_length": 3122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "16\nChapter 1\nDesigning in a Distributed World\n1.3.3 Server Tree\nThe other fundamental composition pattern is the server tree. As Figure 1.3 illus-\ntrates, in this scheme a number of servers work cooperatively with one as the root\nof the tree, parent servers below it, and leaf servers at the bottom of the tree. (In\ncomputer science, trees are drawn upside-down.) Typically this pattern is used to\naccess a large dataset or corpus. The corpus is larger than any one machine can\nhold; thus each leaf stores one fraction or shard of the whole.\nTo query the entire dataset, the root receives the original query and forwards it\nto the parents. The parents forward the query to the leaf servers, which search their\nparts of the corpus. Each leaf sends its ﬁndings to the parents, which sort and ﬁlter\nthe results before forwarding them up to the root. The root then takes the response\nfrom all the parents, combines the results, and replies with the full answer.\nImagine you wanted to ﬁnd out how many times George Washington was\nmentioned in an encyclopedia. You could read each volume in sequence and arrive\nat the answer. Alternatively, you could give each volume to a different person and\nhave the various individuals search their volumes in parallel. The latter approach\nwould complete the task much faster.\nFigure 1.3: A server tree\n",
      "content_length": 1334,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "1.4\nDistributed State\n17\nThe primary beneﬁt of this pattern is that it permits parallel searching of a\nlarge corpus. Not only are the leaves searching their share of the corpus in parallel,\nbut the sorting and ranking performed by the parents are also done in parallel.\nFor example, imagine a corpus of the text extracted from every book in the\nU.S. Library of Congress. This cannot ﬁt in one computer, so instead the informa-\ntion is spread over hundreds or thousands of leaf machines. In addition to the leaf\nmachines are the parents and the root. A search query would go to a root server,\nwhich in turn relays the query to all parents. Each parent repeats the query to all\nleaf nodes below it. Once the leaves have replied, the parent ranks and sorts the\nresults by relevancy.\nFor example, a leaf may reply that all the words of the query exist in the same\nparagraph in one book, but for another book only some of the words exist (less\nrelevant), or they exist but not in the same paragraph or page (even less relevant).\nIf the query is for the best 50 answers, the parent can send the top 50 results to the\nroot and drop the rest. The root then receives results from each parent and selects\nthe best 50 of those to construct the reply.\nThis scheme also permits developers to work within a latency budget. If fast\nanswers are more important than perfect answers, parents and roots do not have\nto wait for slow replies if the latency deadline is near.\nMany variations of this pattern are possible. Redundant servers may exist with\na load-balancing scheme to divide the work among them and route around failed\nservers. Expanding the number of leaf servers can give each leaf a smaller por-\ntion of the corpus to search, or each shard of corpus can be placed on multiple\nleaf servers to improve availability. Expanding the number of parents at each level\nincreases the capacity to sort and rank results. There may be additional levels of\nparent servers, making the tree taller. The additional levels permit a wider fan-\nout, which is important for an extremely large corpus. The parents may provide a\ncaching function to relieve pressure on the leaf servers; in this case more levels of\nparents may improve cache effectiveness. These techniques can also help mitigate\ncongestion problems related to fan-in, as discussed in the previous section.\n1.4 Distributed State\nLarge systems often store or process large amounts of state. State consists of data,\nsuch as a database, that is frequently updated. Contrast this with a corpus, which\nis relatively static or is updated only periodically when a new edition is published.\nFor example, a system that searches the U.S. Library of Congress may receive a\nnew corpus each week. By comparison, an email system is in constant churn with\nnew data arriving constantly, current data being updated (email messages being\nmarked as “read” or moved between folders), and data being deleted.\n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "18\nChapter 1\nDesigning in a Distributed World\nDistributed computing systems have many ways to deal with state. How-\never, they all involve some kind of replication and sharding, which brings about\nproblems of consistency, availability, and partitioning.\nThe easiest way to store state is to put it on one machine, as depicted in\nFigure 1.4. Unfortunately, that method reaches its limit quite quickly: an individ-\nual machine can store only a limited amount of state and if the one machine dies\nwe lose access to 100 percent of the state. The machine has only a certain amount\nof processing power, which means the number of simultaneous reads and writes\nit can process is limited.\nIn distributed computing we store state by storing fractions or shards of the\nwhole on individual machines. This way the amount of state we can store is lim-\nited only by the number of machines we can acquire. In addition, each shard is\nstored on multiple machines; thus a single machine failure does not lose access\nto any state. Each replica can process a certain number of queries per second, so\nwe can design the system to process any number of simultaneous read and write\nrequests by increasing the number of replicas. This is illustrated in Figure 1.5,\nwhere N QPS are received and distributed among three shards, each replicated\nthree ways. As a result, on average one ninth of all queries reach a particular\nreplica server.\nWrites or requests that update state require all replicas to be updated. While\nthis update process is happening, it is possible that some clients will read from\nstale replicas that have not yet been updated. Figure 1.6 illustrates how a write can\nbe confounded by reads to an out-of-date cache. This will be discussed further in\nthe next section.\nIn the most simple pattern, a root server receives requests to store or retrieve\nstate. It determines which shard contains that part of the state and forwards the\nrequest to the appropriate leaf server. The reply then ﬂows up the tree. This looks\nsimilar to the server tree pattern described in the previous section but there are two\nFigure 1.4: State kept in one location; not distributed computing\n",
      "content_length": 2159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "1.4\nDistributed State\n19\nFigure 1.5: This distributed state is sharded and replicated.\nFigure 1.6: State updates using cached data lead to an inconsistent view.\n",
      "content_length": 161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "20\nChapter 1\nDesigning in a Distributed World\ndifferences. First, queries go to a single leaf instead of all leaves. Second, requests\ncan be update (write) requests, not just read requests. Updates are more complex\nwhen a shard is stored on many replicas. When one shard is updated, all of the\nreplicas must be updated, too. This may be done by having the root update all\nleaves or by the leaves communicating updates among themselves.\nA variation of that pattern is more appropriate when large amounts of data\nare being transferred. In this case, the root replies with instructions on how to get\nthe data rather than the data itself. The requestor then requests the data from the\nsource directly.\nFor example, imagine a distributed ﬁle system with petabytes of data spread\nout over thousands of machines. Each ﬁle is split into gigabyte-sized chunks. Each\nchunk is stored on multiple machines for redundancy. This scheme also permits the\ncreation of ﬁles larger than those that would ﬁt on one machine. A master server\ntracks the list of ﬁles and identiﬁes where their chunks are. If you are familiar with\nthe UNIX ﬁle system, the master can be thought of as storing the inodes, or per-ﬁle\nlists of data blocks, and the other machine as storing the actual blocks of data. File\nsystem operations go through a master server that uses the inode-like information\nto determine which machines to involve in the operation.\nImagine that a large read request comes in. The master determines that the ﬁle\nhas a few terabytes stored on one machine and a few terabytes stored on another\nmachine. It could request the data from each machine and relay it to the system\nthat made the request, but the master would quickly become overloaded while\nreceiving and relaying huge chunks of data. Instead, it replies with a list of which\nmachines have the data, and the requestor contacts those machines directly for the\ndata. This way the master is not the middle man for those large data transfers. This\nsituation is illustrated in Figure 1.7.\n.\nFigure 1.7: This master server delegates replies to other servers.\n",
      "content_length": 2094,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "1.5\nThe CAP Principle\n21\n1.5 The CAP Principle\nCAP stands for consistency, availability, and partition resistance. The CAP Prin-\nciple states that it is not possible to build a distributed system that guarantees\nconsistency, availability, and resistance to partitioning. Any one or two can be\nachieved but not all three simultaneously. When using such systems you must be\naware of which are guaranteed.\n1.5.1 Consistency\nConsistency means that all nodes see the same data at the same time. If there are\nmultiple replicas and there is an update being processed, all users see the update\ngo live at the same time even if they are reading from different replicas. Systems\nthat do not guarantee consistency may provide eventual consistency. For exam-\nple, they may guarantee that any update will propagate to all replicas in a certain\namount of time. Until that deadline is reached, some queries may receive the new\ndata while others will receive older, out-of-date answers.\nPerfect consistency is not always important. Imagine a social network that\nawards reputation points to users for positive actions. Your reputation point total\nis displayed anywhere your name is shown. The reputation database is replicated\nin the United States, Europe, and Asia. A user in Europe is awarded points and that\nchange might take minutes to propagate to the United States and Asia replicas. This\nmay be sufﬁcient for such a system because an absolutely accurate reputation score\nis not essential. If a user in the United States and one in Asia were talking on the\nphoneas onewasawardedpoints,theother userwouldseetheupdatesecondslater\nand that would be okay. If the update took minutes due to network congestion or\nhours due to a network outage, the delay would still not be a terrible thing.\nNow imagine a banking application built on this system. A person in the\nUnited States and another in Europe could coordinate their actions to withdraw\nmoney from the same account at the same time. The ATM that each person uses\nwould query its nearest database replica, which would claim the money is avail-\nable and may be withdrawn. If the updates propagated slowly enough, both people\nwould have the cash before the bank realized the money was already gone.1\n1.5.2 Availability\nAvailability is a guarantee that every request receives a response about whether\nit was successful or failed. In other words, it means that the system is up. For\n1.\nThe truth is that the global ATM system does not require database consistency. It can be defeated by\nleveraging network delays and outages. It is less expensive for banks to give out a limited amount of\nmoney when the ATM network is down than to have an unhappy customer stranded without cash.\nFraudulent transactions are dealt with after the fact. Daily withdrawal limits prevent major fraud.\nAssessing overage fees is easier than implementing a globally consistent database.\n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "22\nChapter 1\nDesigning in a Distributed World\nexample, using many replicas to store data such that clients always have access\nto at least one working replica guarantees availability.\nThe CAP Principle states that availability also guarantees that the system is\nable to report failure. For example, a system may detect that it is overloaded and\nreply to requests with an error code that means “try again later.” Being told this\nimmediately is more favorable than having to wait minutes or hours before one\ngives up.\n1.5.3 Partition Tolerance\nPartition tolerance means the system continues to operate despite arbitrary mes-\nsage loss or failure of part of the system. The simplest example of partition\ntolerance is when the system continues to operate even if the machines involved\nin providing the service lose the ability to communicate with each other due to a\nnetwork link going down (see Figure 1.8).\nReturning to our example of replicas, if the system is read-only it is easy to\nmake the system partition tolerant, as the replicas do not need to communicate with\neach other. But consider the example of replicas containing state that is updated\non one replica ﬁrst, then copied to other replicas. If the replicas are unable to com-\nmunicate with each other, the system fails to be able to guarantee updates will\npropagate within a certain amount of time, thus becoming a failed system.\nNow consider a situation where two servers cooperate in a master–slave rela-\ntionship. Both maintain a complete copy of the state and the slave takes over the\nmaster’s role if the master fails, which is determined by a loss of heartbeat—that is,\nFigure 1.8: Nodes partitioned from each other\n",
      "content_length": 1682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "1.5\nThe CAP Principle\n23\na periodic health check between two servers often done via a dedicated network.\nIf the heartbeat network between the two is partitioned, the slave will promote\nitself to being the master, not knowing that the original master is up but unable\nto communicate on the heartbeat network. At this point there are two masters and\nthe system breaks. This situation is called split brain.\nSome special cases of partitioning exist. Packet loss is considered a temporary\npartitioning of the system as it applies to the CAP Principle. Another special case\nis the complete network outage. Even the most partition-tolerant system is unable\nto work in that situation.\nThe CAP Principle says that any one or two of the attributes are achievable in\ncombination, but not all three. In 2002, Gilbert and Lynch published a formal proof\nof the original conjecture, rendering it a theorem. One can think of this as the third\nattribute being sacriﬁced to achieve the other two.\nThe CAP Principle is illustrated by the triangle in Figure 1.9. Traditional rela-\ntional databases like Oracle, MySQL, and PostgreSQL are consistent and available\n(CA). They use transactions and other database techniques to assure that updates\nare atomic; they propagate completely or not at all. Thus they guarantee all users\nwill see the same state at the same time. Newer storage systems such as Hbase,\nFigure 1.9: The CAP Principle\n",
      "content_length": 1416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "24\nChapter 1\nDesigning in a Distributed World\nRedis, and Bigtable focus on consistency and partition tolerance (CP). When par-\ntitioned, they become read-only or refuse to respond to any requests rather than\nbe inconsistent and permit some users to see old data while others see fresh data.\nFinally, systems such as Cassandra, Riak, and Dynamo focus on availability and\npartition tolerance (AP). They emphasize always being able to serve requests even\nif it means some clients receive outdated results. Such systems are often used in\nglobally distributed networks where each replica talks to the others by less reliable\nmedia such as the Internet.\nSQL and other relational databases use the term ACID to describe their side\nof the CAP triangle. ACID stands for Atomicity (transactions are “all or nothing”),\nConsistency (after each transaction the database is in a valid state), Isolation (con-\ncurrent transactions give the same results as if they were executed serially), and\nDurability (a committed transaction’s data will not be lost in the event of a crash\nor other problem). Databases that provide weaker consistency models often refer\nto themselves as NoSQL and describe themselves as BASE: Basically Available\nSoft-state services with Eventual consistency.\n1.6 Loosely Coupled Systems\nDistributed systems are expected to be highly available, to last a long time, and to\nevolve and change without disruption. Entire subsystems are often replaced while\nthe system is up and running.\nTo achieve this a distributed system uses abstraction to build a loosely cou-\npled system. Abstraction means that each component provides an interface that\nis deﬁned in a way that hides the implementation details. The system is loosely\ncoupled if each component has little or no knowledge of the internals of the other\ncomponents. As a result a subsystem can be replaced by one that provides the same\nabstract interface even if its implementation is completely different.\nTake, for example, a spell check service. A good level of abstraction would be\nto take in text and return a description of which words are misspelled and a list of\npossible corrections for each one. A bad level of abstraction would simply provide\naccess to a lexicon of words that the frontends could query for similar words. The\nreason the latter is not a good abstraction is that if an entirely new way to check\nspelling was invented, every frontend using the spell check service would need\nto be rewritten. Suppose this new version does not rely on a lexicon but instead\napplies an artiﬁcial intelligence technique called machine learning. With the good\nabstraction, no frontend would need to change; it would simply send the same\nkind of request to the new server. Users of the bad abstraction would not be so\nlucky.\nFor this and many other reasons, loosely coupled systems are easier to evolve\nand change over time.\n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "1.6\nLoosely Coupled Systems\n25\nContinuing our example, in preparation for the launch of the new spell check\nservice both versions could be run in parallel. The load balancer that sits in front\nof the spell check system could be programmed to send all requests to both the\nold and new systems. Results from the old system would be sent to the users, but\nresults from the new system would be collected and compared for quality control.\nAt ﬁrst the new system might not produce results that were as good, but over time\nit would be enhanced until its results were quantiﬁably better. At that point the\nnew system would be put into production. To be cautious, perhaps only 1 percent\nof all queries would come through the new system—if no users complained, the\nnew system would take a larger fraction. Eventually all responses would come\nfrom the new system and the old system could be decommissioned.\nOther systems require more precision and accuracy than a spell check system.\nFor example, there may be requirements that the new system be bug-for-bug com-\npatible with the old system before it can offer new functionality. That is, the new\nsystem must reproduce not only the features but also the bugs from the old sys-\ntem. In this case the ability to send requests to both systems and compare results\nbecomes critical to the operational task of deploying it.\n.\nCase Study: Emulation before Improvements\nWhen Tom was at Cibernet, he was involved in a project to replace an older\nsystem. Because it was a ﬁnancial system, the new system had to prove it was\nbug-for-bug compatible before it could be deployed.\nThe old system was built on obsolete, pre-web technology and had\nbecome so complex and calciﬁed that it was impossible to add new features.\nThe new system was built on newer, better technology and, being a cleaner\ndesign, was more easily able to accommodate new functionality. The systems\nwere run in parallel and results were compared.\nAt that point engineers found a bug in the old system. Currency conver-\nsion was being done in a way that was non-standard and the results were\nslightly off. To make the results between the two systems comparable, the\ndevelopers reverse-engineered the bug and emulated it in the new system.\nNow the results in the old and new systems matched down to the penny.\nWith the company having gained conﬁdence in the new system’s ability to be\nbug-for-bug compatible, it was activated as the primary system and the old\nsystem was disabled.\nAt this point, new features and improvements could be made to the sys-\ntem. The ﬁrst improvement, unsurprisingly, was to remove the code that\nemulated the currency conversion bug.\n",
      "content_length": 2654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "26\nChapter 1\nDesigning in a Distributed World\n1.7 Speed\nSo far we have elaborated on many of the considerations involved in designing\nlarge distributed systems. For web and other interactive services, one item may\nbe the most important: speed. It takes time to get information, store information,\ncompute and transform information, and transmit information. Nothing happens\ninstantly.\nAn interactive system requires fast response times. Users tend to perceive any-\nthing faster than 200 ms to be instant. They also prefer fast over slow. Studies have\ndocumented sharp drops in revenue when delays as little as 50 ms were artiﬁcially\nadded to web sites. Time is also important in batch and non-interactive systems\nwhere the total throughput must meet or exceed the incoming ﬂow of work.\nThe general strategy for designing a system that is performant is to design a\nsystem using our best estimates of how quickly it will be able to process a request\nand then to build prototypes to test our assumptions. If we are wrong, we go back\nto step one; at least the next iteration will be informed by what we have learned. As\nwe build the system, we are able to remeasure and adjust the design if we discover\nour estimates and prototypes have not guided us as well as we had hoped.\nAt the start of the design process we often create many designs, estimate how\nfast each will be, and eliminate the ones that are not fast enough. We do not auto-\nmatically select the fastest design. The fastest design may be considerably more\nexpensive than one that is sufﬁcient.\nHow do we determine if a design is worth pursuing? Building a prototype is\nvery time consuming. Much can be deduced with some simple estimating exer-\ncises. Pick a few common transactions and break them down into smaller steps,\nand then estimate how long each step will take.\nTwo of the biggest consumers of time are disk access and network delays.\nDisk accesses are slow because they involve mechanical operations. To read a\nblock of data from a disk requires the read arm to move to the right track; the platter\nmust then spin until the desired block is under the read head. This process typically\ntakes 10 ms. Compare this to reading the same amount of information from RAM,\nwhich takes 0.002 ms, which is 5,000 times faster. The arm and platters (known as\na spindle) can process only one request at a time. However, once the head is on\nthe right track, it can read many sequential blocks. Therefore reading two blocks\nis often nearly as fast as reading one block if the two blocks are adjacent. Solid-\nstate drives (SSDs) do not have mechanical spinning platters and are much faster,\nthough more expensive.\nNetwork access is slow because it is limited by the speed of light. It takes\napproximately 75 ms for a packet to get from California to the Netherlands. About\nhalf of that journey time is due to the speed of light. Additional delays may be\nattributable to processing time on each router, the electronics that convert from\n",
      "content_length": 2986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "1.7\nSpeed\n27\nwired to ﬁber-optic communication and back, the time it takes to assemble and\ndisassemble the packet on each end, and so on.\nTwo computers on the same network segment might seem as if they commu-\nnicate instantly, but that is not really the case. Here the time scale is so small that\nother delays have a bigger factor. For example, when transmitting data over a local\nnetwork, the ﬁrst byte arrives quickly but the program receiving the data usually\ndoes not process it until the entire packet is received.\nIn many systems computation takes little time compared to the delays from\nnetwork and disk operation. As a result you can often estimate how long a trans-\naction will take if you simply know the distance from the user to the datacenter\nand the number of disk seeks required. Your estimate will often be good enough\nto throw away obviously bad designs.\nTo illustrate this, imagine you are building an email system that needs to\nbe able to retrieve a message from the message storage system and display it\nwithin 300 ms. We will use the time approximations listed in Figure 1.10 to help us\nengineer the solution.\n.\nJeff Dean, a Google Fellow, has popularized this chart of common numbers\nto aid in architectural and scaling decisions. As you can see, there are many\norders of magnitude difference between certain options. These numbers\nimprove every year. Updates can be found online.\nAction\nTypical Time\nL1 cache reference\n0.5 ns\nBranch mispredict\n5 ns\nL2 cache reference\n7 ns\nMutex lock/unlock\n100 ns\nMain memory reference\n100 ns\nCompress 1K bytes with Zippy\n10,000 ns\n(0.01 ms)\nSend 2K bytes over 1 Gbps network\n20,000 ns\n(0.02 ms)\nRead 1 MB sequentially from memory\n250,000 ns\n(0.25 ms)\nRound trip within same datacenter\n500,000 ns\n(0.5 ms)\nRead 1 MB from SSD\n1,000,000 ns\n(3 ms)\nDisk seek\n10,000,000 ns\n(10 ms)\nRead 1 MB sequentially from network\n10,000,000 ns\n(10 ms)\nRead 1 MB sequentially from disk\n30,000,000 ns\n(30 ms)\nSend packet from California to\nNetherlands to California\n150,000,000 ns\n(150 ms)\nFigure 1.10: Numbers every engineer should know\n",
      "content_length": 2076,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "28\nChapter 1\nDesigning in a Distributed World\nFirst we follow the transaction from beginning to end. The request comes from\na web browser that may be on another continent. The request must be authenti-\ncated, the database index is consulted to determine where to get the message text,\nthe message text is retrieved, and ﬁnally the response is formatted and transmitted\nback to the user.\nNow let’s budget for the items we can’t control. To send a packet between\nCalifornia and Europe typically takes 75 ms, and until physics lets us change the\nspeed of light that won’t change. Our 300 ms budget is reduced by 150 ms since we\nhave to account for not only the time it takes for the request to be transmitted but\nalso the reply. That’s half our budget consumed by something we don’t control.\nWe talk with the team that operates our authentication system and they\nrecommend budgeting 3 ms for authentication.\nFormatting the data takes very little time—less than the slop in our other\nestimates—so we can ignore it.\nThis leaves 147 ms for the message to be retrieved from storage. If a typical\nindex lookup requires 3 disk seeks (10 ms each) and reads about 1 megabyte of\ninformation (30 ms), that is 60 ms. Reading the message itself might require 4 disk\nseeks and reading about 2 megabytes of information (100 ms). The total is 160 ms,\nwhich is more than our 147 ms remaining budget.\n.\nHow Did We Know That?\nHow did we know that it will take 3 disk seeks to read the index? It requires\nknowledge of the inner workings of the UNIX ﬁle system: how ﬁles are looked\nup in a directory to ﬁnd an inode and how inodes are used to look up the data\nblocks. This is why understanding the internals of the operating system you\nuse is key to being able to design and operate distributed systems. The inter-\nnals of UNIX and UNIX-like operating systems are well documented, thus\ngiving them an advantage over other systems.\nWhile disappointed that our design did not meet the design parameters, we\nare happy that disaster has been averted. Better to know now than to ﬁnd out when\nit is too late.\nIt seems like 60 ms for an index lookup is a long time. We could improve that\nconsiderably. What if the index was held in RAM? Is this possible? Some quick\ncalculations estimate that the lookup tree would have to be 3 levels deep to fan\nout to enough machines to span this much data. To go up and down the tree is\n5 packets, or about 2.5 ms if they are all within the same datacenter. The new total\n(150 ms+3 ms+2.5 ms+100 ms = 255.5 ms) is less than our total 300 ms budget.\n",
      "content_length": 2556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "1.8\nSummary\n29\nWe would repeat this process for other requests that are time sensitive. For\nexample, we send email messages less frequently than we read them, so the time\nto send an email message may not be considered time critical. In contrast, delet-\ning a message happens almost as often reading messages. We might repeat this\ncalculation for a few deletion methods to compare their efﬁciency.\nOne design might contact the server and delete the message from the stor-\nage system and the index. Another design might have the storage system simply\nmark the message as deleted in the index. This would be considerably faster but\nwould require a new element that would reap messages marked for deletion and\noccasionally compact the index, removing any items marked as deleted.\nEven faster response time can be achieved with an asynchronous design. That\nmeans the client sends requests to the server and quickly returns control to the user\nwithout waiting for the request to complete. The user perceives this system as faster\neven though the actual work is lagging. Asynchronous designs are more complex\nto implement. The server might queue the request rather than actually performing\nthe action. Another process reads requests from the queue and performs them in\nthe background. Alternatively, the client could simply send the request and check\nfor the reply later, or allocate a thread or subprocess to wait for the reply.\nAll of these designs are viable but each offers different speed and complexity of\nimplementation. With speed and cost estimates, backed by prototypes, the business\ndecision of which to implement can be made.\n1.8 Summary\nDistributed computing is different from traditional computing in many ways. The\nscale is larger; there are many machines, each doing specialized tasks. Services are\nreplicated to increase capacity. Hardware failure is not treated as an emergency or\nexception but as an expected part of the system. Thus the system works around\nfailure.\nLarge systems are built through composition of smaller parts. We discussed\nthree ways this composition is typically done: load balancer for many backend\nreplicas, frontend with many different backends, and a server tree.\nThe load balancer divides trafﬁc among many duplicate systems. The front-\nend with many different backends uses different backends in parallel, with each\nperforming different processes. The server tree uses a tree conﬁguration, with each\ntree level serving a different purpose.\nMaintaining state in a distributed system is complex, whether it is a large\ndatabase of constantly updated information or a few key bits to which many sys-\ntems need constant access. The CAP Principle states that it is not possible to build\na distributed system that guarantees consistency, availability, and resistance to\npartitioning simultaneously. At most two of the three can be achieved.\n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "30\nChapter 1\nDesigning in a Distributed World\nSystems are expected to evolve over time. To make this easier, the components\nare loosely coupled. Each embodies an abstraction of the service it provides, such\nthat the internals can be replaced or improved without changing the abstraction.\nThus, dependencies on the service do not need to change other than to beneﬁt from\nnew features.\nDesigning distributed systems requires an understanding of the time it takes\nvarious operations to run so that time-sensitive processes can be designed to meet\ntheir latency budget.\nExercises\n1. What is distributed computing?\n2. Describe the three major composition patterns in distributed computing.\n3. What are the three patterns discussed for storing state?\n4. Sometimes a master server does not reply with an answer but instead replies\nwith where the answer can be found. What are the beneﬁts of this method?\n5. Section 1.4 describes a distributed ﬁle system, including an example of how\nreading terabytes of data would work. How would writing terabytes of data\nwork?\n6. Explain the CAP Principle. (If you think the CAP Principle is awesome, read\n“The Part-Time Parliament” (Lamport & Marzullo 1998) and “Paxos Made\nSimple” (Lamport 2001).)\n7. What does it mean when a system is loosely coupled? What is the advantage\nof these systems?\n8. Give examples of loosely and tightly coupled systems you have experience\nwith. What makes them loosely or tightly coupled?\n9. How do we estimate how fast a system will be able to process a request such\nas retrieving an email message?\n10. In Section 1.7 three design ideas are presented for how to process email dele-\ntion requests. Estimate how long the request will take for deleting an email\nmessage for each of the three designs. First outline the steps each would take,\nthen break each one into individual operations until estimates can be created.\n",
      "content_length": 1880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Chapter 2\nDesigning for Operations\nYour job is to design\nsystems that operate.\n—Theo Schlossnagle\nThis chapter catalogs the most common operations tasks and discusses how to\ndesign for them. It also discusses how to rework an existing architecture that was\nnot designed with operations in mind.\nDesigning for operations means making sure all the normal operational func-\ntions can be done well. Normal operational functions include tasks such as periodic\nmaintenance, updates, and monitoring. These issues must be kept in mind in early\nstages of planning.\nWhen you consider the full life cycle of a given service, only a small portion\nof that life cycle is spent building the features of the service. The vast majority of\nthe life cycle is spent operating the service. Yet traditionally the operational func-\ntions of software are considered lower priority than features, if they are considered\nat all.\nThe best strategy for providing a highly available service is to build features\ninto the software that enhance one’s ability to perform and automate operational\ntasks. This is in contrast to strategies where operations is an after-thought and oper-\nations engineers are forced into a position of “running what other people build.”\nThat’s the outdated way.\n2.1 Operational Requirements\nSoftware is usually designed based on requirements related to what the ultimate\nuser will see and do. The functionality required for smooth operations is rarely\nconsidered. As a consequence, systems administrators ﬁnd themselves lacking\ncontrol points for key interactions. When we design for operations, we take into\n31\n",
      "content_length": 1609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "32\nChapter 2\nDesigning for Operations\naccount the normal functions of an infrastructure life cycle. They include, but are\nnot necessarily limited to, the following:\n• Conﬁguration\n• Startup and shutdown\n• Queue draining\n• Software upgrades\n• Backups and restores\n• Redundancy\n• Replicated databases\n• Hot swaps\n• Toggles for individual features\n• Graceful degradation\n• Access controls and rate limits\n• Data import controls\n• Monitoring\n• Auditing\n• Debug instrumentation\n• Exception collection\nFeatures like conﬁguration and backups/restores make typical operational tasks\npossible. Features like queue draining and toggles for individual features allow\nsuch tasks to be done seamlessly. Many of these features create the introspec-\ntion required to debug, tune, and repair large systems, as previously discussed\nin Section 1.1.\nTypically, developers and managers don’t think of these issues and they are\noften left off of lists of requirements. At best, they are after-thoughts or it is assumed\nthat the operations team will “ﬁgure something out” through improvisation. The\ntruth is that these tasks are important and cannot be done well or at all without\nspeciﬁc features. In the worst case, systems are designed in ways that work against\nthe ability to “improvise” a solution.\nRather than the term “operational requirements,” some organizations use the\nterm“non-functionalrequirements.”Weconsiderthistermmisleading.Whilethese\nfeatures are not directly responsible for the function of the application or service, the\nterm “non-functional” implies that these features do not have a function. A service\ncannot exist without the support of these features; they are essential.\nThe remainder of this chapter discusses these operational aspects and features\nthat enable them. Many of these will seem obvious to someone with operational\nexperience. Yet, each of them appears on this list because we have observed at least\none system that suffered from its omission.\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "2.1\nOperational Requirements\n33\n2.1.1 Configuration\nThe system must be conﬁgurable by automated means. This includes initial conﬁg-\nuration as well as changes made later. It must be possible to perform the following\ntasks:\n• Make a backup of a conﬁguration and restore it\n• View the difference between one archived copy and another revision\n• Archive the running conﬁguration without taking the system down\nA typical way to achieve all of these goals is for the conﬁguration to take the form of\na text ﬁle with a well-deﬁned format. Automated systems can easily generate such\na ﬁle. Text ﬁles are easy to parse and therefore auditable. They can be archived eas-\nily. They can also be stored in a source code repository and analyzed with standard\ntext comparison tools such as UNIX diff.\nIn some systems the conﬁguration is dynamically updated as the system runs.\nThis “state” may be reﬂected back into the primary conﬁguration ﬁle or may be a\nseparate entity. In this case there are additional requirements.\nThere must be a way for automation to read and update the state. This step\nmay be carried out through an API or by reading and updating a conﬁguration ﬁle.\nIf a ﬁle is used, a locking protocol must exist to prevent both the service and exter-\nnal automation from reading the ﬁle in an incomplete state and to prevent update\ncollisions. Tools should be available for doing sanity checks on conﬁgurations that\ndo not involve activating the conﬁguration on a live system.\nAn inferior option would be a conﬁguration ﬁle that is an opaque binary blob.\nSuch a ﬁle is not human readable. In these types of systems it is impossible to keep\na history of the conﬁguration and see change over time. Often strange problems\nare debugged by analyzing changes to a conﬁguration ﬁle, where a change may be\ntoo small to be remembered but just big enough to cause a problem. This type of\nanalysis is not possible if the ﬁle is not plain text.\nWe have been burned by systems that provide an API for extracting the entire\nconﬁguration but where the result turns out not to actually represent the entire\nconﬁguration. All too often, the omission is found only during a disaster recovery\nexercise or emergency. For that reason each new release should be tested to verify\nthat the conﬁguration data does not omit anything.\nFrom an operational perspective, the ideal is for the conﬁguration to consist of\none or more plain text ﬁles that can be easily examined, archived, and compared.\nSome systems read their conﬁguration directly from a source code repository,\nwhich is convenient and highly recommended. However, it must also be possible\nto disable this feature and provide conﬁgurations directly. Such an approach may\nbe used in an emergency, when the source code repository is down, and for exper-\nimentation. The use of this feature must be exposed in a way that the monitoring\n",
      "content_length": 2868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "34\nChapter 2\nDesigning for Operations\nsystem can detect it. Other users can then be made aware that it is happening—for\nexample, by showing this status in dashboards. It may also be an alertable event,\nin which case alerts can be generated if this feature is used on production systems.\nAlerting if this feature is disabled for more than a certain amount of time assures\nthat temporary ﬁxes are not forgotten.\n.\nEasy Configuration Does Not Require a GUI\nA product manager from IBM once told Tom that the company had spent a lot\nof money adding a graphical user interface (GUI) to a system administration\ntool. This was done to make it easier to conﬁgure. To the team’s dismay, the\nmajority of their customers did not use the GUI because they had written Perl\nscripts to generate the conﬁguration ﬁles.\n2.1.2 Startup and Shutdown\nThe service should restart automatically when a machine boots up. If the machine is\nshut down properly, the system should include the proper operating system (OS)\nhooks to shut the service down properly. If the machine crashes suddenly, the\nnext restart of the system should automatically perform data validations or repairs\nbefore providing service.\nEnsuring that a service restarts after a reboot can be as simple as installing a\nboot-time script, or using a system that monitors processes and restarts them (such\nas Ubuntu Upstart). Alternatively, it can be an entire process management sys-\ntem like Apache Mesos (Metz 2013) or Google Omega (Schwarzkopf, Konwinski,\nAbd-El-Malek & Wilkes 2013), which not only restarts a process when a machine\nreboots, but also is able to restart the process on an entirely different machine in\nthe event of machine death.\nThe amount of time required to start up or shut down a system should be doc-\numented. This is needed for preparing for disaster recovery situations. One needs\nto know how quickly a system can be safely shut down to plan the battery capac-\nity of uninterruptible power supply (UPS) systems. Most UPS batteries can sustain\na system for about ﬁve minutes. After a power outage, starting up thousands of\nservers can be very complex. Knowing expected startup times and procedures can\ndramatically reduce recovery time.\nTesting for how a system behaves when all systems lose power concurrently\nis important. It’s a common datacenter stressor. Thousands of hard disk motors\nspinning up at the same time create a huge power draw that can overload power\n",
      "content_length": 2435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "2.1\nOperational Requirements\n35\nsystems. In general, one can expect 1 to 5 percent of machines to not boot on the\nﬁrst try. In a system with 1000 machines, a large team of people might be required\nto resuscitate them all.\nRelated to this is the concept of “crash-only” software. Candea & Fox (2003)\nobserve that the post-crash recovery procedure in most systems is critical to sys-\ntem reliability, yet receives a disproportionately small amount of quality assurance\n(QA) testing. A service that is expected to have high availability should rarely use\nthe orderly shutdown process. To align the importance of the recovery procedure\nwith the amount of testing it should receive, these authors propose not implement-\ning the orderly shutdown procedure or the orderly startup procedures. Thus, the\nonly way to stop the software is to crash it, and the only way to start it is to exer-\ncise the crash recovery system. In this way, the crash recovery process is exercised\nfrequently and test processes are less likely to ignore it.\n2.1.3 Queue Draining\nThere must be an orderly shutdown process that can be triggered to take the sys-\ntem out of service for maintenance. A drain occurs when the service is told to\nstop accepting new requests but complete any requests that are “in ﬂight.” This\nis sometimes called lame-duck mode.\nThis mechanism is particularly important when using a load balancer with\nmultiple backend replicas, as described in Section 1.3.1. Software upgrades are\nimplemented by removing one replica at a time, upgrading it, and returning it\nto service. If each replica is simply “killed,” any in-ﬂight requests will be lost. It\nis better to have a draining mode, where the replica continues to process requests\nbut intentionally fails the load balancer’s health check requests. If it sees the health\ncheck requests fail, the load balancer stops sending new requests to the replica.\nOnce no new requests have been received for a while and the existing requests are\ncompleted, it is safe to kill the replica and perform the upgrade.\n.\nEmptying the Queue\nWhile developing the pioneering Palm VII wireless messaging service, the\nteam realized the main application did not have the ability to be drained. Any\nattempt to shut it down would lose any messages that were in ﬂight. Strata\nnegotiated to add this feature. This “drain and exit” feature enabled the oper-\nations team to be able to take servers down for maintenance or swap them out\nfor service reliability without losing messages.\n",
      "content_length": 2499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "36\nChapter 2\nDesigning for Operations\nSimilarly, it is useful for the service to be able to start in drained mode. In this\ncase, the load balancer will not send new trafﬁc to the replica, but operations can\nsend messages directly to it for testing. Once conﬁdence is achieved, undraining\nthe replica signals the load balancer to send trafﬁc.\n2.1.4 Software Upgrades\nIt must be possible for software upgrades to be implemented without taking down\nthe service. Usually the software is located behind a load balancer and upgraded\nby swapping out replicas as they are upgraded.\nSome systems can be upgraded while running, which is riskier and requires\ncareful design and extensive testing.\nNonreplicated systems are difﬁcult to upgrade without downtime. Often the\nonly alternative is to clone the system, upgrade the clone, and swap the newly\nupgraded system into place faster than customers will notice. This is usually a\nrisky—and sometimes improvised—solution. Clones of production systems tend\nto be imperfect copies because it is difﬁcult to assure that the clone was made\nprecisely when no changes are in progress.\n2.1.5 Backups and Restores\nIt must be possible to back up and restore the service’s data while the system is\nrunning.\nOften legacy systems must be taken down to do backups or restores. This\napproach may sufﬁce for a small ofﬁce with 10 understanding users, but it is not\nreasonable for a web site with hundreds or millions of users. The design of a system\nthat permits live backups and restores is very different\nOne way to achieve live backups without interfering with the service is to\nperform the backup on a read-only replica of the database. If the system can\ndynamically add and remove replicas, a replica is removed from service, frozen,\nand used to make the backup. This replica is then added back to the system later.\nIt is common to have a particular replica dedicated to this process.\nLive restores are often done by providing a special API for inserting data dur-\ning a restore operation. The architecture should allow for the restoration of a single\naccount, preferably without locking that user or group out of the service.\nFor example, an email system should be able to restore a single user’s account\nwithout having to restore all accounts. It should be able to do this live so that the\nuser may continue using the service while the restoring messages appear.\nBoth backups and restores create additional load on a system. This burden\nmust be accounted for in capacity planning (headroom) and latency calculations.\n",
      "content_length": 2547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "2.1\nOperational Requirements\n37\n2.1.6 Redundancy\nMany reliability and scaling techniques are predicated on the ability to run mul-\ntiple, redundant replicas of a service. Therefore services should be designed to\nsupport such conﬁgurations. Service replicas are discussed in Section 1.3.1. The\nchallenge of replicating state between replicas is discussed in Section 1.5.\nIf a service wasn’t designed to work behind a load balancer, it may work\nthrough “luck,” which is not a recommended way to do system administration.\nOnly the most rudimentary services will work in such a situation. It is more likely\nthat the system will not work or, worse, will seem to work but develop problems\nlater that are difﬁcult to trace.\nA common issue is that a user’s login state is stored locally by a web server\nbut not communicated to replicas. When the load balancer receives future requests\nfrom the same user, if they are sent to a different replica the user will be asked to\nlog in again. This will repeat until the user has logged into every replica. Solutions\nto this problem are discussed in Section 4.2.3.\n2.1.7 Replicated Databases\nSystems that access databases should do so in a way that supports database scaling.\nThe most common way to scale database access in a distributed system is to cre-\nate one or more read-only replicas. The master database does all transactions that\nmutate (make changes to) the database. Updates are then passed to the read-only\nreplicas via bulk transfers. Services can access the master database as normal, or if a\nquery does not make any mutations it is sent to a read-only replica. Most database\naccess is read-only, so the majority of work is off-loaded to the replicas. The replicas\noffer fast, though slightly out of date, access. The master offers full-service access\nto the “freshest” data, though it might be slower.\nSoftware that uses a database must be speciﬁcally engineered to support read-\nonly replicas. Rather than opening a connection to the database, two connections\nare created: a connection to the database master and a connection to one of the\nread-only replicas. As developers code each query, they give serious consideration\nto which connection it should be sent over, trading off speed for freshness and\nrealizing that every query sent directly to the master “just in case” is consuming\nthe master database’s very precious resources.\nIt is good practice to segregate these kinds of queries even if the database does\nnot have any replicas and both connections go to the same server. Someday you\nwill want to add read-only replicas. Deciding which connection a query should use\nis best done when the query is originally being invented, not months or years later.\nThat said, if you ﬁnd yourself retroﬁtting a system after the fact, it may be a better\nuse of your time to identify a few heavy hitters that can be moved to the read-only\n",
      "content_length": 2880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "38\nChapter 2\nDesigning for Operations\nreplica, rather than examine every single query in the source code. Alternatively,\nyou may create a read-only replica for a speciﬁc purpose, such as backups.\n.\nUnlucky Read-Only Replicas\nAt Google Tom experienced a race condition between a database master and its\nreplicas. The system carefully sent writes to the master and did all reads from\nthe replicas. However, one component read data soon after it was updated and\noften became confused because it saw outdated information from the replica.\nAs the team had no time to recode the component, it was simply reconﬁgured\nso that both the write and read connections went to the master. Even though\nonly one read query out of many had to go to the master, all were sent to the\nmaster. Since the component was used just once or twice a day, this did not\ncreate an undue burden on the master.\nAs time went on, usage patterns changed and this component was used\nmore frequently, until eventually it was used all day long. One day there was\nan outage because the master became overloaded due to the load from this\ncomponent. At that point, the component had to be re-engineered to properly\nsegregate queries.\nOne more warning against relying on luck rather than ofﬁcial support from\nthe developers: luck runs out. Relying on luck today may result in disaster at the\nnext software release. Even though it may be a long-standing policy for the oper-\nations staff, it will appear to the developers as a “surprise request” to support this\nconﬁguration. The developers could rightfully refuse to ﬁx the problem because\nit had not been a supported conﬁguration in the ﬁrst place. You can imagine the\nconfusion and the resulting conﬂict.\n2.1.8 Hot Swaps\nService components should be able to be swapped in or out of their service roles\nwithout triggering an overall service outage. Software components may be self-\nsufﬁcient for completing a hot swap or may simply be compatible with a load\nbalancer or other redirection service that controls the process.\nSome physical components, such as power supplies or disk drives, can be\nswapped while still electrically powered on, or “hot.” Hot-swappable devices can\nbe changed without affecting the rest of the machine. For example, power sup-\nplies can be replaced without stopping operations. Hot-pluggable devices can be\ninstalled or removed while the machine is running. Administrative tasks may be\nrequired before or after this operation is performed. For example, a hard drive may\n",
      "content_length": 2505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "2.1\nOperational Requirements\n39\nnot be recognized unless the operating system is told to scan for new disks. A new\nnetwork interface card may be recognized, but the application server software may\nnot see it without being restarted unless it has been speciﬁcally programmed to\nperiodically scan for new interfaces.\nIt is often unclear what vendors mean by “hot-pluggable” and “hot-\nswappable.” Directly test the system to understand the ramiﬁcations of the process\nand to see how application-level software responds.\n2.1.9 Toggles for Individual Features\nA conﬁguration setting (a toggle) should be present to enable or disable each new\nfeature. This allows roll-outs of new software releases to be independent of when\nthe new feature is made available to users. For example, if a new feature is to appear\non the site precisely at noon on Wednesday, it is very difﬁcult to coordinate a new\nbinary “push” exactly at that time. However, if each new feature can be individu-\nally enabled, the software can be deployed early and the feature can be enabled via\nchanging a conﬁguration setting at the desired time. This is often called a flag flip.\nThis approach is also useful for dealing with new features that cause problems.\nIt is easier to disable the individual feature via a ﬂag ﬂip than to roll back to the\nprevious binary.\nMore sophisticated toggles can be enabled for particular groups of users.\nA feature may be enabled for a small group of trusted testers who receive early\naccess. Once it is validated, the toggle can enable the feature for all users, perhaps\nby enabling it for successively larger groups.\nSee Section 11.7 for more details.\n2.1.10 Graceful Degradation\nGraceful degradation means software acts differently when it is becoming over-\nloaded or when systems it depends on are down. For example, a web site might\nhave two user interfaces: one is rich and full of images, while the other is\nlightweight and all text. Normally users receive the rich interface. However, if\nthe system is overloaded or at risk of hitting bandwidth limits, it switches to the\nlightweight mode.\nGraceful degradation also requires the software to act smartly during outages.\nA service may become read-only if the database stops accepting writes (a com-\nmon administrative defense when corruption is detected). If a database becomes\ncompletely inaccessible, the software works from its cache so that users see partial\nresults rather than an error message.\nWhen a service can no longer access services it depends on, related features\nmay disappear rather than just displaying a broken web page or a “404 page not\nfound” error.\n",
      "content_length": 2622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "40\nChapter 2\nDesigning for Operations\nEven small sites have learned that it is better to put up a temporary web server\nthat displays the same “under construction” page no matter what the query, than\nto have the users receive no service at all. There are simple web server software\npackages made just for this situation.\n.\nCase Study: Graceful Degradation in Google Apps\nGoogle Docs deploys many graceful degradation techniques. Google’s word\nprocessor can switch into read-only mode when only a read-only database\nreplica is available. The client-side JavaScript can work with the cached data\nin the browser if the server is inaccessible. Gmail provides a rich, JavaScript-\nbased user interface as well as a slimmer HTML-only interface that appears\nautomatically as needed. If the entire system is unavailable, the user is sent to a\ngeneric front page that displays the system status rather than simply receiving\nno response.\n2.1.11 Access Controls and Rate Limits\nIf a service provides an API, that API should include an Access Control List (ACL)\nmechanism that determines which users are permitted or denied access, and also\ndetermines rate-limiting settings.\nAn ACL is a list of users, along with an indication of whether they are\nauthorized to access the system. For example, access could be restricted to cer-\ntain Internet Protocol (IP) addresses or blocks, to certain users or processes, or by\nother identiﬁcation mechanisms. IP addresses are the weakest form of identiﬁca-\ntion because they can be easily forged. Something better should be used, such as a\npublic key infrastructure (PKI) that uses digital certiﬁcates to prove identity.\nThe most simple ACL is a list of users that are permitted access; everyone else\nis banned. This is called a default closed policy; the list is called the whitelist. The\nreverse would be a default open policy, where the default is to give access to all\nusers unless they appear on a blacklist.\nA more sophisticated ACL is an ordered list of users and/or groups annotated\nas either being “permitted” or “denied.” If a user is not mentioned in the ACL, the\ndefault action might be to permit the user (fail open) or, alternatively, to deny the\nuser (fail closed).\nIn addition to indicating permission, ACLs can indicate rate limits. Different\nusers might be permitted different queries per second (QPS) rates, with requests\nthat go over that rate being denied. For example, the service may give premium\n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "2.1\nOperational Requirements\n41\ncustomers an unlimited QPS rate, regular paid customers a moderate QPS rate,\nand unpaid customers a low QPS rate or no access at all.\n.\nCase Study: ACLs at Google\nGoogle’s Remote Procedure Call (RPC) protocol, used by all internal APIs, has\na powerful ACL system. Connections are authenticated via a PKI so that the\nservice is assured of the client’s identity and knows which groups that client is\na member of. Identity and groups are globally deﬁned and represent products\nand services as opposed to individual external customers. The ACLs specify\nthe access allowed for that individual or group: permit, deny, or permit with\na rate limit. Teams negotiate QPS rates for accessing a service as part of the\nservice’s capacity planning. Teams that have not negotiated rates get access\nbut at a very low rate limit. This enables all teams to try out new services and\neliminates the need for the service team to expend effort negotiating hundreds\nof lightweight or casual use requests.\n2.1.12 Data Import Controls\nIf a service periodically imports data, mechanisms should be established that\npermit operations staff to control which data is accepted, rejected, or replaced.\nThe quality of incoming data varies, and the system importing the data needs\na way to restrict what is actually imported so that known bad data can be disre-\ngarded. If a bad record causes a problem with the system, one must be able to block\nit via conﬁguration rather than waiting for a software update.\nSuch a system uses the same whitelist/blacklist terminology we saw earlier.\nA blacklist is a way of specifying input that is to be rejected, with the assump-\ntion that all other data is accepted. A whitelist is used to specify data that is to be\naccepted; all other data is rejected.\nIn addition to control the incoming data stream, we need a way to augment\nan imported data source with locally provided data. This is accomplished using an\naugmentation ﬁle of data to import.\nEstablishing a change limit can also prevent problems. For example, if a\nweekly data import typically changes less than 20 percent of all records, one might\nwant to require manual approval if the change will affect 30 or more percent of all\nrecords. This can prevent a disaster caused by a software bug or a bad batch of new\ndata.\n",
      "content_length": 2314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "42\nChapter 2\nDesigning for Operations\n.\nCase Study: Google Maps Local Business Listing\nGoogle subscribes to listings of businesses from various “local business direc-\ntory” services for use in its map-related products. These information providers\nperiodically send data that must be processed and imported into the map\nsystem. The quality of the data is disheartening: listings are often incorrect,\nmangled, or somehow useless. Corrections are reported to the provider but\ncould take months to appear in the data feed. Some sources provide data that\nhas good quality for certain states and countries but categorically bad data for\nothers. Therefore the system that imports this data has a whitelist, a blacklist,\nand an augmentation ﬁle.\nThe whitelist indicates which regions to include. It is used when the infor-\nmation from a particular provider might be of high quality only for certain\nregions. Once the quality of data for a particular region is veriﬁed, it is added\nto the whitelist. If the quality drops, the data for that region is removed.\nThe blacklist identiﬁes known bad records. It includes records that have\nbeen identiﬁed as bad or incorrect in past batches.\nThe data from the business directories is augmented by data that Google\nproduces independently of the source. This includes information sourced by\nGoogle itself, overrides for known bad data that Google has independently\ncorrected, and “Easter eggs” (jokes to be included in the listings).\n2.1.13 Monitoring\nOperations requires visibility into how the system is working. Therefore each com-\nponent of a system must expose metrics to the monitoring system. These metrics\nare used to monitor availability and performance, for capacity planning, and as\npart of troubleshooting.\nChapters 16 and 17 cover this topic in greater detail.\n2.1.14 Auditing\nLogging, permissions, and role accounts are set up to enable the service to be exam-\nined for, and pass, security and compliance audits. This area is changing rapidly,\nso it is always best to consult your legal department for information about the lat-\nest laws. The biggest concerns for corporations that are considering using public\ncloud services revolve around compliance with the relevant laws governing their\nbusiness: if they choose to use a public cloud service, will they fail their next audit,\nand subsequently face massive ﬁnes or be prevented from conducting business\nuntil they pass another audit?\n",
      "content_length": 2432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "2.1\nOperational Requirements\n43\nAlthough local labor laws usually do not directly affect compliance or gov-\nernance issues, some countries strongly believe that people from other countries\ndoing IT administration can be a compliance issue. Consider the system admin-\nistrator who is sitting in Orange, New Jersey, and doing some administration on\na server in Frankfurt, Germany. The system administrator has no knowledge of\nthe local labor laws or the European Union (EU) Data Protection Directive and\nmoves a virtual server across the company intranet as a scheduled and approved\nchange. That system administrator may have violated at least two EU mandates,\ninadvertently making her employer non compliant and subject to sanctions, ﬁnes,\nor both.\nExamples of regulations with speciﬁc IT audit requirements are SOX, j-SOX,\nc-SOX, PCI DSS, the EU Data Protection Directive, and Singapore MAS. More\nthan 150 such regulatory mandates can be found across the world. In addition,\nsome global standards apply to various governance scenarios, such as CobiT 5 and\nISO/IEC 27001, 27002, and 27005. IT governance and compliance are covered more\nfully in Volume 1 of this series (Limoncelli, Hogan & Chalup, forthcoming 2015).\n2.1.15 Debug Instrumentation\nSoftware needs to generate logs that are useful when debugging. Such logs should\nbe both human-readable and machine-parseable. The kind of logging that is appro-\npriate for debugging differs from the kind of logging that is needed for audit-\ning. A debug log usually records the parameters sent to and returned from any\nimportant function call. What constitutes “important” varies.\nLarge systems should permit debug logging to be enabled on individual\nmodules. Otherwise, the volume of information can be overwhelming.\nIn some software methodologies, any logged information must be matched\nwith documentation that indicates what the message means and how to use it.\nThe message and the documentation must be translated into all (human) lan-\nguages supported by the system and must be approved by marketing, product\nmanagement, and legal personnel. Such a policy is a fast path to ending any and\nall productivity through bureaucratic paralysis. Debugging logs should be exempt\nfrom such rules because these messages are not visible to external users. Every\ndeveloper should feel empowered to add a debug logging statement for any infor-\nmation he or she sees ﬁt. The documentation on how to consume such information\nis the source code itself, which should be available to operations personnel.\n2.1.16 Exception Collection\nWhen software generates an exception, it should be collected centrally for anal-\nysis. A software exception is an error so severe that the program intentionally\nexits. For example, the software author may decide that handling a particular\n",
      "content_length": 2805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "44\nChapter 2\nDesigning for Operations\nsituation is unlikely to happen and will be difﬁcult to recover from; therefore the\nprogram declares an exception and exits in this situation. Certain data corruption\nscenarios are better handled by a human than by the software itself. If you’ve ever\nseen an operating system “panic” or present a “blue screen of death,” that is an\nexception.\nWhen designing software for operability, it is common to use a software\nframework that detects exceptions, gathers the error message and other informa-\ntion, and submits it to a centralized database. Such a framework is referred to as\nan exception collector.\nException collection systems offer three beneﬁts. First, since most software\nsystems have some kind of automatic restart capability, certain exceptions may\ngo unnoticed. If you never see that the exceptions are occurring, of course, you\ncan’t deal with the underlying causes. An exception collector, however, makes the\ninvisible visible.\nSecond, exception collection helps determine the health of a system. If there\nare many exceptions, maintenance such as rolling out new software releases should\nbe cancelled. If a sharp increase in exceptions is seen during a roll-out, it may be\nan indication that the release is bad and the roll-out should stop.\nThe third beneﬁt from using an exception collector is that the history of excep-\ntions can be studied for trends. A simple trend to study is whether the sheer volume\nof exceptions is going up or down. Usually exception levels can be correlated to a\nparticular software release. The other trend to look for is repetition. If a particular\ntype of exception is recorded, the fact that it is happening more or less frequently\nis telling. If it occurs less frequently, that means the software quality is improving.\nIf it is increasing in frequency, then there is the opportunity to detect it and ﬁx the\nroot cause before it becomes a bigger problem.\n2.1.17 Documentation for Operations\nDevelopers and operational staff should work together to create a playbook of\noperating procedures for the service. A playbook augments the developer-written\ndocumentation by adding operations steps that are informed by the larger busi-\nness view. For example, the developers might write the precise steps required to\nfail over a system to a hot spare. The playbook would document when such a\nfailover is to be done, who should be notiﬁed, which additional checks must be\ndone before and after failover, and so on. It is critical that every procedure include\na test suite that veriﬁes success or failure. Following is an example database failover\nprocedure:\n1. Announce the impending failover to the db-team and manager-team mailing\nlists.\n2. Verify the hot spare has at least 10 terabytes of free disk space.\n",
      "content_length": 2782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "2.2\nImplementing Design for Operations\n45\n3. Verify these dependencies are all operating within parameters: (link to server\ncontrol panel) (link to data feed control panel).\n4. Perform the failover using the system failover procedure (link).\n5. Verify that these dependencies have successfully switched to the hot spare and\nare operating correctly.\n6. Reply-all to the previously sent email regarding the operation’s success or\nfailure.\nDocumenting the basic operational procedures for a service cannot be the sole\nresponsibility of either the development or operations team. Instead, it needs to\nbe a collaborative effort, with the operations team ensuring that all the operational\nscenarios they can foresee are addressed, and the developers ensuring that the doc-\numentation covers all error situations that can arise in their code, and how and\nwhen to use the supporting tools and procedures.\nDocumentation is a stepping stone to automation. Processes may change fre-\nquently when they are new. As they solidify, you can identify good candidates for\nautomation (see Chapter 12). Writing documentation also helps you understand\nwhat can be automated easily and what will be more challenging to automate,\nbecause documenting things means explaining the steps in detail.\n2.2 Implementing Design for Operations\nFeatures designed for operations need to be implemented by someone; they are\nnot magically present in software. For any given project, you will ﬁnd that the\nsoftware may have none, some, or all of the features listed in this chapter. There\nare four main ways that you can get these features into software:\n• Build them in from the beginning.\n• Request features as they are identiﬁed.\n• Write the features yourself.\n• Work with a third-party vendor.\n2.2.1 Build Features in from the Beginning\nIn this instance, a savvy development and operations team has built the features\ninto the product you are using to run your service. It is extremely rare that you will\nencounter this scenario outside of large, experienced shops like Google, Facebook,\nor Yahoo.\nIf you are fortunate enough to be involved in early development of a system,\nwork with developers and help them set priorities so that these features are “baked\nin” from the beginning. It also helps if the business team driving the requirements\ncan recognize the needs of operations.\n",
      "content_length": 2348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "46\nChapter 2\nDesigning for Operations\n2.2.2 Request Features as They Are Identified\nMore likely a service does not contain all of the operational features desired\nbecause it is impossible to know what will be needed before the system is in oper-\nation. If you have access to the developers, these features can be requested over\ntime. First and foremost, speak up about your operations needs—ﬁle a feature\nrequest for every missing feature. The feature request should identify the prob-\nlem that needs to be solved rather than the speciﬁc implementation. List the risks\nand impact to the business so that your request can be prioritized. Work collabora-\ntively with the developers as they implement the features. Make yourself available\nfor consultation with the developers, and offer encouragement.\nDeveloper time and resources are limited, so it is important to prioritize your\nrequests. One strategy for prioritization is to select the item that will have the\nbiggest impact for the smallest amount of effort. Figure 2.1 shows a graph where\nthe x-axis is the expected impact of a change, ranging from low- to high-impact. The\ny-axis represents the amount of effort required to create the change, also ranging\nfrom easy (low effort) to hard (high effort). It is tempting to focus on the easiest\ntasks or “low-hanging fruit.” However, this often ends up wasting resources on\neasy tasks that have very little impact. That outcome may be emotionally satisfying\nbut does not solve operational problems. Instead, you should focus on the high-\nimpact items exclusively, starting with the low-effort projects while selectively\nchoosing the ones that require larger effort.\nFixing the biggest bottleneck usually has the biggest impact. This point will\nbe discussed in greater detail in Section 12.4.3.\nOne of the differences we have found between high-performing teams and\nlow-performing teams is that the high-performing teams focus on impact.\n.\nFigure 2.1: Implementation priorities for design for operations\n",
      "content_length": 2004,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "2.2\nImplementing Design for Operations\n47\n.\nCase Study: An 80/20 Rule for Operational Features\nWhen Tom was at Lumeta, a disagreement arose over how much developer\ntime should be spent on operational issues versus new features. The prod-\nuct manager came up with a very creative solution. The product alternated\nbig releases and small releases. Big releases were expected to have major new\nfeatures. Small releases were expected to ﬁx bugs from the previous major\nrelease.\nIt was negotiated that big releases would have 20 percent of developer\ntime spent on issues requested by the operations team. The small releases\nwere not intended to add major new features, but it was useful to have one or\ntwo high-priority features included. Therefore, for small releases, 80 percent\nof developer time was spent on operational requests. Since the releases were\nsmaller, the same number of hours was spent on operational requests for both\nbig and small releases.\n2.2.3 Write the Features Yourself\nWhen developers are unwilling to add operational features, one option is to write\nthe features yourself. This is a bad option for two reasons.\nFirst, the developers might not accept your code. As an outsider, you do not\nknow their coding standards, the internal infrastructure, and their overall vision\nfor the future software architecture. Any bugs in your code will receive magniﬁed\nblame.\nSecond, it sets a bad precedent. It sends a message that developers do not need\nto care about operational features because if they delay long enough you’ll write\nthem yourself.\nOperational staff should spend time coding operational services that create\nthe ecosystem in which services run. Write frameworks that developers can use to\nimprove operations. For example, write a library that can be linked to that makes\nit easy to report status to the monitoring system. Write tools that let developers be\nself-sufﬁcient rather than dependent on operations. For example, write a tool that\ngathers exceptions and core dumps for analysis rather than emailing requests to\noperations anytime that step is needed.\nThere are exceptions to this rule. Code submissions from outsiders are easier\nto accept when they are small. In a highly collaborative organization, people may\nsimply be more accustomed to receiving code contributions from many sources.\nThis is typical on open source projects. At Google there is a code approval pro-\ncess that makes it easy for outsiders to contribute to a project and assure the code\nmeets team standards. This system permits feedback and revisions until the code\n",
      "content_length": 2567,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "48\nChapter 2\nDesigning for Operations\nchange is deemed acceptable by both parties; only then is the code accepted into\nthe system. It also helps that there is a corporate-wide high standard for code qual-\nity and style. In such a system, the code quality and style you are used to writing\nwill probably be compatible with that of other teams.\nWhen possible, operational staff should be embedded with developers so they\ncan learn the code base, become familiar with the release and testing process, and\nbuild a relationship with the code. However, even that is no replacement for getting\noperational features added by the developers themselves. Therefore it is better to\nembed developers with the operations staff, possibly in six-month rotations, so that\nthey understand the operational need for such features.\nWhat works best may be dictated by the size of the organization and the scale\nof the system being operated. For example, a high degree of collaboration may be\neasier to achieve in small organizations.\n2.2.4 Work with a Third-Party Vendor\nWorking with a third-party vendor is quite similar to working with your own\ndevelopment team. Many of the same processes need to be followed, such as ﬁling\nbugs and having periodic meetings to discuss feature requests.\nAlways raise the visibility of your issues in a constructive way, as vendors are\nsensitive to criticism of their products. For example, write a postmortem report that\nincludes the feature request so that the vendor can see the context of the request.\n(See Section 14.3.2 for more details on writing good postmortem reports.)\nIf the vendor is unresponsive to your requests, you may be able to write code\nthat builds frameworks around the vendor’s software. For example, you might\ncreate a wrapper that provides startup and shutdown services in a clean manner\naround vendor software that handles those tasks ungracefully. We highly recom-\nmend publishing such systems externally as open source products. If you need\nthem, someone else will, too. Developing a community around your code will\nmake its support less dependent on your own efforts.\n2.3 Improving the Model\nGood design for operations makes operations easy. Great design for operations\nhelps eliminate some operational duties entirely. It’s a force multiplier often equiv-\nalent to hiring an extra person. When possible, strive to create systems that embed\nknowledge or capability into the process, replacing the need for operational inter-\nvention. The job of the operations staff then changes from performing repetitive\noperational tasks to building, maintaining, and improving the automation that\nhandles those tasks.\n",
      "content_length": 2647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "2.4\nSummary\n49\nTom once worked in an environment where resource allocations were\nrequested via email and processed manually. Once an API was made available,\nthe entire process became self-service; users could manage their own resources.\nSome provisioning systems let you specify how much RAM, disk, and CPU\neach “job” will need. A better system does not require you to specify any resources\nat all: it monitors use and allocates what is needed, maintaining an effective balance\namong all the jobs on a cluster, and reallocating them and shifting resources around\nover time.\nA common operational task is future capacity planning—that is, predicting\nhow many resources will be needed 3 to 12 months out. It can be a lot of work.\nAlternatively, a thoughtfully constructed data collection and analysis system can\nmake these predictions for you. For more information on capacity planning, see\nChapter 18.\nCreating alert thresholds and ﬁne-tuning them can be an endless task. That\nwork can be eliminated if the monitoring system sets its own thresholds. For exam-\nple, one web site developed an accurate prediction model for how many QPS it\nshould receive every hour of the year. The system administrators could then set\nan alert if the actual QPS was more than 10 percent above or below the prediction.\nMonitoring hundreds of replicas around the world can’t be done manually without\nhuge investments in staff. By eliminating this operational duty, the system scaled\nbetter and required less operational support.\n2.4 Summary\nServices should include features that beneﬁt operations, not just the end users.\nFeatures requested by operations staff are aimed at building a stable, reliable, high-\nperforming service, which scales well and can be run in a cost-effective manner.\nEven though these features are not directly requested by customers, the better\noperational effectiveness ultimately beneﬁts the customer.\nOperations staff need many features to support day-to-day operations. They\nalso need full documentation of all the operational processes, failure scenarios, and\nfeature controls. They need authentication, authorization, and access control mech-\nanisms, as well as rate-limiting functionality. Operations staff need to be able to\nenable and disable new features with toggles, globally for roll-out and roll-back,\nand on a per-user basis for beta testing and premium services.\nServices should degrade gracefully when there are problems, rather than\nbecome completely unusable. Services that import data from other sources must\nallow the operations staff to apply controls to those data sources, based on their\ndata quality or other criteria.\n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "50\nChapter 2\nDesigning for Operations\nSystems are easier to maintain when this functionality is designed into them\nfrom inception. Operations staff can work with developers to ensure that opera-\ntional features are included in a system, particularly if the software is developed\nin-house.\nExercises\n1. Why is design for operations so important?\n2. How is automated conﬁguration typically supported?\n3. List the important factors for redundancy through replication.\n4. Give an example of a partially implemented process in your current environ-\nment. What would you do to fully implement it?\n5. Why might you not want to solve an issue by coding the solution yourself?\n6. Which type of problems should appear ﬁrst on your priority list?\n7. Which factors can you bring to an outside vendor to get the vendor to take\nyour issue seriously?\n",
      "content_length": 836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Chapter 3\nSelecting a Service Platform\nWhen I hear someone touting the\ncloud as a magic-bullet for all\ncomputing problems, I silently\nreplace “cloud” with “clown” and\ncarry on with a zen-like smile.\n—Amy Rich\nA service runs on a computing infrastructure called a platform. This chapter pro-\nvides an overview of the various types of platforms available in cloud computing,\nwhat each of them provides, and their strengths and weaknesses. It does not offer\nan examination of speciﬁc products but rather a categorization that will help you\nunderstand the variety of offerings. Strategies for choosing between these different\nservices are summarized at the end of the chapter.\nThe term “cloud” is ambiguous; it means different things to different people\nand has been made meaningless by marketing hype. Instead, we use the following\nterms to be speciﬁc:\n• Infrastructure as a Service (IaaS): Computer and network hardware, real or\nvirtual, ready for you to use.\n• Platform as a Service (PaaS): Your software running in a vendor-provided\nframework or stack.\n• Software as a Service (SaaS): An application provided as a web site.\nFigure 3.1 depicts the typical consumer of each service. SaaS applications are\nfor end users and fulﬁll a particular market niche. PaaS provides platforms for\ndevelopers. IaaS is for operators looking to build their own platforms on which\napplications will be built, thus providing the most customizability.\nIn this chapter, we will discuss these services in terms of being provided by a\nthird-party vendor since that is the general case.\n51\n",
      "content_length": 1566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "52\nChapter 3\nSelecting a Service Platform\nFigure 3.1: The consumers of SaaS, PaaS, and IaaS\nA platform may be described along three axes:\n• Level of service abstraction: IaaS, PaaS, SaaS\n• Type of machine: Physical, virtual, or process container\n• Level of resource sharing: Shared or private\n3.1 Level of Service Abstraction\nAbstraction is, essentially, how far users are kept from the details of the raw\nmachine itself. That is, are you offered a raw machine (low abstraction) or are ser-\nvices provided as a high-level API that encapsulates what you need done rather\nthan how to do it (high abstraction)? The closer you are to the raw machine, the\nmore control you have. The higher the level of abstraction, the less you have to\nconcern yourself with technical details of building infrastructure and the more you\ncan focus on the application.\n3.1.1 Infrastructure as a Service\nIaaS provides bare machines, networked and ready for you to install the operating\nsystem and your own software. The service provider provides the infrastructure\nso that the customer can focus on the application itself.\nThe machines provided by the vendor are usually virtual machines but may\nbe physical machines. The provider takes care of the infrastructure: the machines\nthemselves, power, cooling, and networking, providing internet access, and all\ndatacenter operations.\n",
      "content_length": 1356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "3.1\nLevel of Service Abstraction\n53\n.\nTerms to Know\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nOversubscribed: A system that provides capacity X is used in a place where\nY capacity is needed, when X < Y. Used to describe a potential or actual\nneed.\nUndersubscribed: The opposite of oversubscribed.\nAlthough the service provider manages its layers of the infrastructure, IaaS\ndoes not relieve you from all work. A lot of work must be done to coordinate all\nthe pieces, understand and tune the various parts so they work well together, and\nmanage the operating system (since you have total control of the OS).\nProviders charge for compute time, storage, and network trafﬁc. These costs\nwill affect how your application is architected. Keeping information locally ver-\nsus retrieving it over a network may have different costs, affecting your design\nchoices. If information is accessed frequently over the network, the network\ncharges can be reduced by caching or storing more information locally. However,\nthe additional local storage may have its own cost. Such engineering details are\nimportant, because otherwise you may ﬁnd yourself with a startlingly large bill\nat the end of the month. These are important points to work out with the devel-\nopment and business teams. The software and operational choices have real costs\nand tradeoffs.\nThe performance characteristics of providers may vary wildly. When compar-\ning providers, it is important to benchmark local storage, remote storage, CPU, and\nnetwork performance. Some providers’ remote storage is signiﬁcantly faster than\nothers. Repeat any such benchmarks at different times of the day—some service\nproviders may experience high packet loss at daily peak times. Design decisions\nmade for one provider may not be the right choice for other providers.\nWithin an IaaS offering, partitions or “reliability zones” segment the service\ngeographically to provide regional uptime guarantees. While all attempts are made\nto ensure reliable service, it is inevitable that some downtime will be required for\nmaintenance or due to unavoidable circumstances such as a natural disaster. The\nservice provider should segment its service into multiple zones and provide guar-\nantees that planned downtime will not occur in multiple zones at the same time.\nEach zone should be far enough apart from the others that natural disasters are\nunlikely to strike more than one zone at a time. This permits customers to keep\ntheir service in one zone and fail over to another zone if necessary.\n",
      "content_length": 2668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "54\nChapter 3\nSelecting a Service Platform\nFor example, a service provider may offer four zones: U.S. East Coast, U.S.\nWest Coast, Western Europe, and Eastern Europe. Each zone is built and man-\naged to have limited dependencies on the others. At a minimum, customers should\nlocate a service in one zone with plans for failover in another zone. A more sophis-\nticated plan would be to have the service run in each zone with load balancing\nbetween all locations, automatically shifting trafﬁc away from any zone that is\ndown. We will cover this in more detail in Chapter 6.\nSuch geographic diversity also permits customers to better manage the latency\nof their service. Information takes time to travel, so it is generally faster to provide\nservice to someone from a nearby datacenter. For example, a service may be archi-\ntected such that a user’s data is stored in one zone with a backup kept in one other\nzone. Users from New York would have their data stored in the U.S. East Coast\nzone, with backup copies stored in the Western Europe zone. During an outage,\nthe user is served from the backup zone; the service would not be as fast in such a\ncase, but at least the data would be accessible.\nIaaS providers have expanded beyond offering just simple machines and net-\nworks. Some provide a variety of storage options, including relational (SQL) and\nnon-relational (NoSQL or key/value) databases, high-speed storage options, and\ncold storage (bulk data storage that is inexpensive but has latency on the order\nof hours or days). More advanced networking options include virtual private\nnetwork (VPN)–accessible private networks and load balancing services. Many\nprovide both local load balancing and global load balancing, as will be described\nin Chapter 4. Some provide elastic scaling services, which automatically allocate\nand conﬁgure additional machines on demand as capacity is needed.\nProviders that offer both IaaS and PaaS often blur the line between the two by\nproviding high-level managed services that are available to both.\n3.1.2 Platform as a Service\nPaaS enables you to run your applications from a vendor-provided framework.\nThese services offer you a high level of value, as they manage all aspects of the\ninfrastructure, even much of the application stack. They offer very elastic scaling\nservices, handling additional load without any input required from you. Generally\nyou are not even aware of the speciﬁc resources dedicated to your application.\nFor example, in Google AppEngine, you upload application-level software\nand Google takes care of the rest. The framework (platform) automatically pro-\nvides load balancing and scaling. The more active your users are, the more\nmachines Google allocates to your application. Internally the system is managing\nbandwidth, CPU allocations, and even authentication. Your application and hun-\ndreds of others might be sharing the same machine or your application may require\nthe resources of hundreds of dedicated machines. You do not have to manage such\ndecisions except to limit resource use to control costs.\n",
      "content_length": 3074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "3.1\nLevel of Service Abstraction\n55\nPaaS providers charge for their services based on how much CPU, band-\nwidth, and storage are used. This is similar to IaaS except the charges are higher to\ncompensate for the more extensive framework that is provided.\nThe downside of PaaS is that you are restricted to using what the vendor’s\nplatform provides. The platform is generally programmable but not necessarily\nextensible. You do not have direct access to the operating system. For instance,\nyou may not be able to add binaries or use popular libraries until the vendor makes\nthem part of its service. Generally processes run in a secure “jail” (similar to UNIX’s\n“chroot” restricted environment), which aims to prevent them from breaking out of\nthe service’s framework. For example, one PaaS offered the Python language but\nnot the Python Imaging Library (PIL). It could not be installed by users because\nthe framework does not permit Python libraries that include portions written in\ncompiled languages.\nPaaS provides many high-level services including storage services, database\nservices, and many of the same services available in IaaS offerings. Some offer\nmore esoteric services such as Google’s Machine Learning service, which can\nbe used to build a recommendation engine. Additional services are announced\nperiodically.\n3.1.3 Software as a Service\nSaaS is what we used to call a web site before the marketing department decided\nadding “as a service” made it more appealing. SaaS is a web-accessible application.\nThe application is the service, and you interact with it as you would any web site.\nThe provider handles all the details of hardware, operating system, and platform.\nSome common examples include Salesforce.com, which replaces locally run\nsales team management software; Google Apps, which eliminates the need for\nlocally run email and calendaring software; and Basecamp, which replaces locally\nrun project management software. Nearly any business process that is common\namong many companies is offered as SaaS: human resources (HR) functions such\nas hiring and performance management; accounting functions such as payroll,\nexpense tracking, and general ledgers; IT incident, request, and change manage-\nment systems; and many aspects of marketing and sales management.\nThe major selling point of SaaS is that customers do not have to concern them-\nselves with software installation, upgrades, and operations. There is no client soft-\nware to download. The service is fully managed, upgraded, and maintained by the\nprovider.Becausetheserviceisaccessedviatheweb,itcanbeusedfromanylocation.\nAs a SaaS provider, you need to design the service to obscure upgrades and\nother operational details. Developers must avoid features that require client soft-\nware or browser plug-ins. In designing the service you need to recognize that since\nit can be accessed from anywhere, it will be accessed from anywhere, including\nmobile devices. This affects architecture and security decisions.\n",
      "content_length": 2992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "56\nChapter 3\nSelecting a Service Platform\nMake it easy for customers to get started using your service. Rather than hav-\ning to speak with a salesperson to sign up, signing up should be possible via the\nweb site, possibly requiring submission of a credit card or other payment informa-\ntion. Facebook would not have gotten to where it is today if each user had to ﬁrst\nspeak to a customer service representative and arrange for an account to be cre-\nated. Importing data and enabling features should also be self-service. A product\nlike Salesforce.com would not have been able to grow at the rate it has if importing\ndata or other operations required working with customer support personnel.\nIt is also important that people can leave the service in a self-service manner.\nThis means users should be able to export or retrieve their data and close their\naccounts, even if this makes it easier to leave and move to a competitor. Customers\nwill be concerned about their ability to migrate out of the application at the end of\nthe contract or if they are dissatisﬁed. We believe it is unethical to lock people into\na product by making it impossible or difﬁcult to export their data. This practice,\ncalled vendor lock-in, should be considered a “red ﬂag” that the product is not\ntrustworthy. The best way to demonstrate conﬁdence in your product is to make it\neasy to leave. It also makes it easy for users to back up their data.\nMany SaaS offerings are upgraded frequently, often without warning, pro-\nviding little opportunity for training. Users should be able to access major new\nreleases for the purpose of planning, training, and user acceptance testing. Pro-\nvide a mechanism for users to select the day they will be moved to major releases,\nor provide two tracks: a “rapid release” track for customers that want new features\nwithout delay and a “scheduled release” track for customers that would like new\nfeatures to appear on a published schedule, perhaps two to three weeks after the\nrapid release track.\nFinally, conﬂicts may exist between your data privacy and application host-\ning policies and those of your customers. Your privacy policy will need to be\na superset of all your customers’ privacy policies. You may need to provide\nheightened security for certain customers, possibly segmenting them from other\ncustomers.\n3.2 Type of Machine\nThere are three options for the type of machine that a service runs on: physical\nmachine, virtual machine, and process container. The decision between physi-\ncal, virtual, and container is a technical decision. Each has different performance,\nresource efﬁciency, and isolation capabilities. The desired technical attributes\nshould guide your decision on which to use.\nIaaS generally provides the widest variety of options. PaaS generally obscures\nwhat is used, as the user works in a framework that hides the distinction. That said,\nmost PaaS providers use containers.\n",
      "content_length": 2920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "3.2\nType of Machine\n57\n3.2.1 Physical Machines\nA physical machine is a traditional computer with one or more CPUs, and sub-\nsystems for memory, disk, and network. These resources are controlled by the\noperating system, whose job it is to act as the trafﬁc cop coordinating all the pro-\ncesses that want to share these resources. The resources allocated to a running\nprocess (a program running on the system) are actual hardware resources. As a\nresult their performance is relatively predictable.\n3.2.2 Virtual Machines\nVirtual machines are created when a physical machine is partitioned to run a sep-\narate operating system for each partition. Processes running on a virtual machine\nhave little or no awareness that they are not on a physical machine. They cannot\naccess the resources, such as disk or memory, of other virtual machines running\non the same physical machine.\nVirtual machines can make computing more efﬁcient. Physical machines\ntoday are so fast and powerful that some applications do not need the full resources\nof a single machine. The excess capacity is called stranded capacity because it is\nunusable in its current form. Sharing a large physical machine’s power among\nmany smaller virtual machines helps reduce stranded capacity by permitting the\ncreation of virtual machines that are the right size for their requirements.\nStranded capacity can also be mitigated by running multiple servers on\nthe same machine. However, virtualization provides better isolation than simple\nmultitasking.\nFor example, when two applications share a machine, when one application\ngets overloaded or has a problem that causes it to consume large amounts of CPU,\ndisk space, or memory, it will affect the performance of the other application.\nNow suppose those two programs each ran on their own virtual machines, each\nwith a certain amount of CPU, disk space, and memory allocated. This arrange-\nment provides better isolation for each application from problems caused by the\nother.\nSometimes the reason for using virtual machines is organizational. Differ-\nent departments within an organization may not trust each other enough or have\nsufﬁcient cross-department billing options to run software on the same machine.\nNevertheless, they can share a pool of physical machines if each is able to create its\nown virtual machine.\nSometimes the reason for using virtual machines is logistical. Running ﬁve\nservices on one machine requires that any OS patches or upgrades be approved by\nall ﬁve services. If each service runs in its own virtual machine, then upgrades and\npatches can be done on different schedules for different services. In all these cases,\nvirtual machines permit isolation at the OS level.\n",
      "content_length": 2704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "58\nChapter 3\nSelecting a Service Platform\nBenefits of Virtual Machines\nVirtual machines are fast to create and destroy. There is very little lead time\nbetween when one is requested and when the virtual machine is usable. Some sys-\ntems can spin up a new virtual machine in less than a minute. Consequently, it is\neasy to create a virtual machine for a speciﬁc task and delete it when the task is\ncompleted. Such a virtual machine is called an ephemeral machine or short-lived\nmachine. Some systems create hundreds of ephemeral machines across many phys-\nical machines, run a parallel compute job, and then destroy the machines when the\njob is complete.\nBecause virtual machines are controlled through software, virtualization sys-\ntems are programmable. An API can be used to create, start, stop, modify, and\ndestroy virtual machines. Software can be written to orchestrate these functions on\na large scale. This is not possible with physical machines, which have to be racked,\ncabled, and conﬁgured via manual labor.\nVirtual machine functionality is provided by a combination of virtualization\nsupport in modern CPUs and virtualization control software called the virtual\nmachine monitor (VMM). Modern CPUs have special features for partitioning\nmemory and CPU time to create virtual machines. Access to disk, network, and\nother I/O devices is handled by emulation at the chip or device level.\nSome virtualization systems permit a virtual machine to be moved between\nphysical machines. Like a laptop that is put to sleep and woken up later, the VMM\nputs the machine to sleep, copies its memory and all other state to a different phys-\nical machine, and continues its activities there. The process can be coordinated so\nthat most of the copying happens ahead of time, so that the machine freeze lasts for\nless than a second. This permits a virtual machine to be moved to a different phys-\nical machine when the current one needs to be upgraded or repaired, or moved to\na different failure domain in advance of a planned maintenance outage.\nI/O in a Virtual Environment\nA hardware virtual machine (HVM) performs I/O emulation at the chip level.\nWith HVMs, the virtual machine’s operating system thinks there is, for example,\nan actual SATA hard drive controller installed. This lets the virtual machine use\nan unmodiﬁed operating system. However, such emulation is rather slow. On an\nHVM, every time the OS tries to access the SATA controller, the CPU’s virtual-\nization feature detects this access, stops the virtual machine, and gives control to\nthe VMM. The VMM performs the disk request, emulating a real SATA controller\nand placing the result where the actual chip would have. The virtual machine\nis then allowed to continue where it left off. It sees the result and is none the\nwiser.\nParavirtualization (PV) performs I/O emulation at the device level. PV\nrequires the operating system to be modiﬁed so that the I/O calls it would normally\n",
      "content_length": 2945,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "3.2\nType of Machine\n59\nperform are instead done by requests to the VMM. The VMM handles the I/O and\nreturns the result. The modiﬁcations usually take the form of device drivers that\nlook like standard hard disks, video displays, keyboards, and so on, but are actu-\nally talking to the VMM. PV is able to perform the requests more efﬁciently since\nit captures the request at a higher level of abstraction.\nVirtual machines are allocated a ﬁxed amount of disk space, memory, and CPU\nfrom the physical machine. The hard drive a VM sees actually may be a single large\nﬁle on the physical machine.\nDisadvantages of Virtual Machines\nSome resources, such as CPU cores, are shared. Suppose a physical machine has\na four-core CPU. Three virtual cores may be allocated to one virtual machine and\nthree virtual cores may be allocated to another virtual machine. The VMM will\nload-share the six virtual cores on the four physical cores. There may be times when\none virtual machine is relatively idle and does not need all three virtual cores it was\nallocated. However, if both virtual machines are running hard and require all six\nvirtual cores, each will receive a fraction of the CPU’s attention and so will run\nmore slowly.\nA virtual machine can detect CPU contention. In Linux and the Xen hyper-\nvisor, this is called “steal time”: it is the amount of CPU time that your virtual\nmachine is missing because it was allocated to other virtual machines (Haynes\n2013). IaaS providers usually cannot provide guarantees of how much steal time\nwill exist, nor can they provide mechanisms to control it. Netﬂix found the only\nway it could deal with this issue was to be reactionary. If high steal time was\ndetected on a virtual machine in Amazon Web Services (AWS), Netﬂix would\ndelete the virtual machine and have it re-created. If the company was lucky,\nthe new virtual machine would be created on a physical machine that was less\noversubscribed. This is a sorry state of affairs (Link 2013).\nSome resources are shared in an unbounded manner. For example, if one vir-\ntual machine is generating a huge amount of network trafﬁc, the other virtual\nmachines may suffer. This is also typical of disk I/O. A hard drive can perform\nonly so much disk I/O per second, with the amount being limited by the band-\nwidth from the computer to the disk. Where there is a resource shortage such as\ndisk I/O bandwidth, the situation is called resource contention.\nVirtual machines are very heavy-weight. They run a full operating system,\nwhich requires a lot of disk space. They hold on to all the memory allocated to\nthem, even if it isn’t being used. The underlying OS cannot reallocate this memory\nto other machines. Because virtual machines run a complete operating system, the\noperational burden is similar to a full machine that needs to be monitored, patched,\nupgraded, and so on. Also, because a complete operating system is running, each\nOS is running many background service processes such as maintenance tasks and\n",
      "content_length": 2994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "60\nChapter 3\nSelecting a Service Platform\nservice daemons. Those take up resources and add to the operational burden on\nthe system administration team.\n3.2.3 Containers\nA container is a group of processes running on an operating system that are iso-\nlated from other such groups of processes. Each container has an environment with\nits own process name space, network conﬁguration, and other resources. The ﬁle\nsystem to which the processes have access consists of a subdirectory on the host\nmachine. The processes in a particular container see that subdirectory as their root\ndirectory, and cannot access ﬁles outside that subdirectory (and its subdirectories)\nwithout special accommodation from the host machine. The processes all run on\nthe same operating system or kernel. As a consequence, you cannot, for example,\nhave some processes running under Linux and others running under Windows as\nyou can with virtual machines.\nUnlike a virtual machine, which is allocated a large chunk of RAM and disk,\ncontainers consume resources at the same ﬁne-grained level as processes. Thus\nthey are less wasteful.\nProcesses in a container are controlled as a group. If the container is conﬁg-\nured to have a memory limit, the sum total of memory used by all processes in that\ncontainer cannot exceed that limit. If the container is allocated a certain amount of\ndisk bandwidth, that limit is enforced on the processes in the container as a whole.\nSolaris containers, called Zones, can be allocated network interfaces and have their\nnetwork bandwidth regulated to control bandwidth resource contention. Contain-\ners on Linux can assign a different amount of disk cache to each container so that\none container’s buffer thrashing will not affect the buffers of another container.\nProcesses in a container are isolated in other ways. A container can kill or\notherwise interact with only processes in its container. In contrast, processes that\nare not in containers can kill or interact with all processes, even ones in individ-\nual containers. For example, the shell command ps, when running in a FreeBSD\ncontainer (called a “jail”), displays only processes running in that container. This\nis not a parlor trick; the container has no visibility to other processes. However,\nwhen the same command is run on the host from outside any container, it shows\nall processes, including those inside the each container. Thus, if you are logged into\nthe main host (no particular container), you have global visibility and can serve as\nadministrator for all containers.\nEach container has its own copy of the packages, shared libraries, and other\nsupporting ﬁles that it requires. Two containers running on the same machine\ncannot have dependency or version conﬂicts. For example, without containers\none program might require a particular version of a library while another requires\na very different version and cannot operate with the other version installed.\nThis “dependency hell” is common. When each program is put in a different\n",
      "content_length": 3010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "3.2\nType of Machine\n61\ncontainer, however, each can have its own copy of the library and thus the conﬂict\nis avoided.\nContainers are very lightweight because they do not require an entire OS. Only\nthe speciﬁc system ﬁles needed by the software are copied into the container. The\nsystem allocates disk space as ﬁles are needed, as opposed to allocating a large\nvirtual disk ahead of time. A container runs fewer processes because it needs to\nrun only the ones related to the software. System background processes such as\nSSH and other daemons do not run in the container since they are available in the\noutside operating system. When using virtual machines, each machine has a full\ncomplement of such daemons.\nContainers are different from virtual machines. Each virtual machine is a\nblackbox. An administrator logged into the physical machine cannot (without\ntricks) peer into the individual virtual machines. A virtual machine can run a dif-\nferent operating system than its host physical machine because it is emulating a\nfull machine. A virtual machine is a larger, less granular allocation of resources.\nWhen the virtual machine starts, a certain amount of RAM and disk space is allo-\ncated and dedicated to that virtual machine. If it does not use all of the RAM, the\nRAM can’t be used by anything else. Virtual disks are often difﬁcult to resize,\nso you create them larger than needed to reduce the chance that the container\nwill need to be enlarged. The extra capacity cannot be used by other virtual\nmachines—a situation called having stranded resources.\nContainers do share some of the downsides of virtual machines. Downtime\nof the host machine affects all containers. This means that planned downtime for\npatching the host as well as unplanned outages affect all containers. Nevertheless,\nthe host machine doesn’t have to do much, so it can run a stripped-down version\nof the operating system. Thus there is less to patch and maintain.\nSo far we have discussed the technical aspects of containers. What can be done\nwith them, however, is much more exciting.\nContainers are usually the underlying technology in PaaS. They enable\ncustomers to be isolated from each other while still sharing physical machines.\nBecause they consume the exact amount of resources they need at the time,\ncontainers are also much more efﬁcient means of providing such shared services.\nSystems like Docker deﬁne a standardized container for software. Rather than\ndistributing software as a package, one can distribute a container that includes the\nsoftware and everything needed for it to run. This container can be created once\nand run on many systems.\nBeing self-contained, containers eliminate dependencies and conﬂicts. Rather\nthan shipping a software package plus a list of other dependent packages and\nsystem requirements, all that is needed is the standardized container and a sys-\ntem that supports the standard. This greatly simpliﬁes the creation, storage, and\ndelivery and distribution of software. Since many containers can coexist on the\nsame machine, the resulting machine works much like a large hotel that is able to\n",
      "content_length": 3120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "62\nChapter 3\nSelecting a Service Platform\nprovide for many customers, treating them all the same way, even though they are\nall unique.\n.\nStandardized Shipping Containers\nA common way for industries to dramatically improve processes is to stan-\ndardize their delivery mechanism. The introduction of standardized shipping\ncontainers revolutionized the freight industry.\nPreviously individual items were loaded and unloaded from ships, usu-\nally by hand. Each item was a different size and shape, so each had to be\nhandled differently.\nStandardized shipping containers resulted in an entirely different way to\nship products. Because each shipping container was the same shape and size,\nloading and unloading could be done much faster. A single container might\nhold many individual items, but since they were transported as a group, trans-\nferring the items between modes of transport was quick work. Customs could\napprove all the items in a particular container and seal it, eliminating the need\nfor customs checks at remaining hops on the container’s journey as long as the\nseal remained unbroken.\nAsothermodesoftransportationadoptedthestandardshippingcontainer,\nthe concept of intermodal shipping was born. A container would be loaded at\na factory and remain as a unit whether it was on a truck, train, or ship.\nAll of this started in April 1956, when Malcom McLean’s company\nSeaLand organized the ﬁrst shipment using standardized containers from\nNew Jersey (where Tom lives) to Texas. (Levinson 2008).\n3.3 Level of Resource Sharing\nIn a “public cloud,” a third party owns the infrastructure and uses it to provide\nservice for many customers. The sharing may be ﬁne-grained, mixing processing\nand data of different customers on the same machine. Alternatively, the sharing\nmay be more segmented, like tenants in an apartment building with well-deﬁned\npartitions between them. In a “private cloud,” a company runs its own computing\ninfrastructure on its own premises. This infrastructure may be set up for a ded-\nicated internal project or, more commonly, done as an internal service provider\nthat makes the offering available to projects and departments within the company.\nHybrids may also be created, such as private clouds run in rented datacenter space.\nThe choice between private or public use of a platform is a business decision\nbased on four factors: compliance, privacy, cost, and control.\n",
      "content_length": 2398,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "3.3\nLevel of Resource Sharing\n63\n3.3.1 Compliance\nCompanies are governed by varying amounts of regulation depending on their\nbusiness, size, locations, and public or private status. Their compliance with all\napplicable regulations can be audited, and failing an audit can have signiﬁcant con-\nsequences, such as the company being unable to conduct business until it passes a\nsubsequent audit.\nUsing a public cloud for certain data or services may cause a company to fail a\ncompliance audit. For example, the EU Data Protection Directive dictates that cer-\ntain data about EU citizens may not leave the EU. Unless the public cloud provider\nhas sufﬁcient controls in place to ensure that will not happen, even in a failover\nscenario, a company that moves the data into the public cloud would fail an audit.\n3.3.2 Privacy\nUsing a public cloud means your data and code reside on someone else’s equip-\nment, in someone else’s facility. They may not have direct access to your data, but\nthey could potentially gain access without your knowledge. Curious employees,\nwith or without malicious intent, could poke around using diagnostic tools that\nwould enable them to view your data. Data might be accidentally leaked by a ser-\nvice provider that disposed of old equipment without properly erasing storage\nsystems.\nBecause of these risks, service providers spell out how they will take care of\nyour data in their contracts. Contracts aside, vendors know that they must earn\ntheir users’ trust if they are to retain them as customers. They maintain that trust\nby being transparent about their policies, and they submit to external audits to\nverify that they are abiding by the rules they set out.\nAnother issue with the public cloud is how law enforcement requests are han-\ndled. If law enforcement ofﬁcials have a warrant to access the data, they can make\na third party provide access without telling you. In contrast, in a private cloud,\ntheir only avenue to access your data involves making you aware of their request\n(although clandestine techniques can be hidden even at your own site).\nThere is also the possibility of accidental exposure of your data. Due to soft-\nware bugs, employee mistakes, or other issues, your data could be exposed to other\ncustomers or the entire world. In a private cloud, the other customers are all from\nthe same company, which may be considered an acceptable risk; the incident can\nbe contained and not become public knowledge. In a public cloud, the exposure\ncould be to anyone, possibly your competitors, and could be front-page news.\n3.3.3 Cost\nThe cost of using a public cloud may or may not be less than the cost of build-\ning the necessary infrastructure yourself. Building such infrastructure requires\n",
      "content_length": 2736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "64\nChapter 3\nSelecting a Service Platform\nlarge amounts of engineering talent, from physical engineering of a datacenter’s\ncooling system, electric service, and design, to technical expertise in running a\ndatacenter and providing the services themselves. All of this can be very expen-\nsive. Amortizing the expense over many customers reduces cost. By comparison,\ndoing it yourself saves money due to the beneﬁts of vertical integration. Vertical\nintegration means saving money by handling all levels of service delivery your-\nself, eliminating the uplift in cost due to “middle men” and service provider proﬁt\nmargins. There is a break-even point where vertical integration becomes more eco-\nnomical. Calculating the total cost of ownership (TCO) and return on investment\n(ROI) will help determine which is the best option.\nDeciding whether the cost of private versus public clouds is appropriate is\nsimilar to making a rent versus own decision about where you live. In the long\nterm, it is less expensive to own a house than to rent one. In the short term, however,\nit may be less expensive to rent. Renting a hotel room for a night makes more sense\nthan buying a building in a city and selling it the next day.\nSimilarly, building a private cloud makes sense if you will use all of it and\nneed it for as long as it takes to pay for itself. Using a third-party provider makes\nsense if the need is small or short term.\n3.3.4 Control\nA private cloud affords you more control. You can specify exactly what kind of\nhardware will be used, which network topology will be set up, and so on. Any fea-\nture you need is a matter of creating it yourself, acquiring it, or licensing it. Changes\ncan happen as fast as your organization can make them. In a public cloud you have\nless control. You must choose from a ﬁnite set of features. While most providers\nare responsive to feature requests, you are merely one customer among many.\nProviders need to focus on those features that will be used by many customers\nand may not be able to provide the specialized features you need.\nLetting the vendor take care of all hardware selection means losing the abil-\nity to specify low-level hardware requirements (speciﬁc CPU types or storage\nproducts).\n.\nContract Questions for Hosting Providers\nThe contract you sign is the baseline of what to expect and what obligations\nthe provider has toward you, the customer. Here are some key questions to\nask your potential providers:\n1. If you want to exit the contract, will you be able to take all your data with\nyou?\n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "3.4\nColocation\n65\n.\n2. In the case of physical machines, if you wish to leave, can you buy the\nmachine itself?\n3. What happens to your servers and data if the vendor goes bankrupt? Will\nthey be tied up in bankruptcy proceedings?\n4. Is internet bandwidth provided by the vendor or do you have to arrange\nfor it yourself? If provided, which Internet service providers (ISPs) do you\nconnect to and how much oversubscription is done? What’s the hardware\nand peering transit redundancy?\n5. Are backups performed? If so, with what frequency and retention policy?\nCan you access the backups by request or are they solely for use by the\nvendor in case of a vendor emergency? How often are restores tested?\n6. How does the vendor guarantee its service level agreement (SLA) num-\nbers for capacity and bandwidth? Are refunds given in the event of an\nSLA violation?\n3.4 Colocation\nWhile not particularly considered a “cloud environment,” colocation is a use-\nful way to provide services. Colocation occurs when a datacenter owner rents\nspace to other people, called tenants. Renting datacenter space is very economi-\ncal for small, medium, and even large businesses. Building your own datacenter\nis a huge investment and requires specialized knowledge about cooling, design,\nnetworking, location selection, real-estate management, and so on.\nThe term “colocation” comes from the telecommunications world. In the past,\ntelecommunication companies were among the rare businesses that built data-\ncenters, which they used to house their equipment and systems. Some third-party\ncompanies offered services to the customers of the telecommunication companies,\nand it was easier to do so if they could put their own equipment in the telecom-\nmunication company’s datacenters. Thus, they colocated their equipment with\nthe telecommunication company’s equipment. In more recent years, any rental of\ndatacenter space has been called colocation service.\nThis service is ideal when you need a small or medium amount of datacenter\nspace. Such datacenters tend to be well designed and well run. While it can take\nyears to procure space and build a datacenter, using a colocation facility can get\nyou up and running quickly.\nColocation is also useful when you need many small spaces. For example,\na company might want to have a single rack of equipment in each of a dozen\ncountries to provide better access for its customers in those countries.\nISPs often extend their network into colocation provider spaces so that tenants\ncan easily connect to them directly. Being directly connected to an ISP improves\n",
      "content_length": 2580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "66\nChapter 3\nSelecting a Service Platform\naccess time for the ISP’s users. Alternatively, colocation providers may provide\ninternet access as a service to tenants, often by blending connections from two or\nmore ISPs. Tenants can take advantage of this capability rather than manage their\nown ISP connections and relationships.\n3.5 Selection Strategies\nThere are many strategies one may use to choose between IaaS, PaaS, and SaaS.\nHere are a few we have used:\n• Default to Virtual: Use containers or virtual machines wherever possible, opt-\ning for physical machines only when performance goals cannot be achieved\nany other way. For example, physical machines are generally better at disk\nI/O and network I/O. It is common to see a fully virtualized environment\nwith the exception of a particularly demanding database with high I/O\nrequirements. A web load balancer may also be a candidate for physical hard-\nware because of the availability of high-bandwidth network interface cards\nor the beneﬁt of predictable performance gained from dedicated network\nconnections.\n• Make a Cost-Based Decision: Select a public cloud solution or a private cloud\nsolution based on cost. Create a business plan that describes the total cost of a\nproject if it is done in-house, on a private infrastructure, versus using a public\ncloud provider. Choose the option with the lower cost. For a multiyear project,\nit may be less expensive to do it yourself. For a short-term project, a public\ncloud is probably the cheaper option.\n• Leverage Provider Expertise: Use the expertise of cloud providers in creating\nan infrastructure so your employees can focus on the application. This strat-\negy is especially appealing for small companies and startups. With only one\nor two developers, it is difﬁcult to imagine building large amounts of infra-\nstructure when public cloud services are available. Public clouds are generally\nrun more professionally than private clouds because of the dynamics of com-\npeting against other providers for customers. Of course, private clouds can be\nrun professionally, but it is difﬁcult to do at a small scale, especially without\na dedicated staff.\n• Get Started Quickly: Leverage cloud providers to get up and running as fast\nas possible. Often the biggest cost is “opportunity cost”: if you miss an oppor-\ntunity because you moved too slowly, it doesn’t matter how much money\nyou were going to save. Contracting with a public cloud provider may be\nmore expensive than doing it yourself, but doing it yourself may take months\nor years to build the infrastructure and by then the opportunity may have\ndisappeared. It is also true that building infrastructure is a waste of time when\n",
      "content_length": 2691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "3.5\nSelection Strategies\n67\nthe product’s success is uncertain. Early in a product’s life the entire idea may\nbe experimental, involving trying new things rapidly and iterating over and\nover to try new features. Getting a minimal product available to early testers\nto see if the product is viable can be quickly done using public cloud services.\nIf the product is viable, private infrastructure can be considered. If the prod-\nuct is not viable, no time was wasted building infrastructure that is no longer\nneeded.\n• Implement Ephemeral Computing: Use ephemeral computing for short-term\nprojects. Ephemeral computing entails setting up a large computing infrastruc-\nture for a short amount of time. For example, imagine a company is doing\nan advertising campaign that will draw millions of users to its web site for a\nfew weeks, with a sharp drop-off occurring after that. Using a public provider\nenables the company to expand to thousands of machines brieﬂy and then dis-\npose of them when they are no longer needed. This is commonly done for large\ndata processingand analysisprojects,proof-of-conceptprojects,biotechnology\ndata mining, and handling unexpected bursts of web site trafﬁc. It is unreason-\nable to build a large infrastructure to be used for such a short span time, but a\ncloud service provider may specialize in providing such computing facilities.\nIn the aggregate the utilization will smooth out and be a constant load.\n• Use the Cloud for Overflow Capacity: Establish baseline capacity require-\nments and build those in-house. Then use the cloud to burst beyond your\nnormal capacity. This is often more cost-effective than either building in-house\ncapacity or exclusively using a public provider.\n• Leverage Superior Infrastructure: Gain an edge through superior infra-\nstructure. In this strategy the ability to create customized infrastructure is\nleveraged to gain competitive advantage. This may include building your own\ndatacenters to control cost and resource utilization, or selecting IaaS over PaaS\nto take advantage of the ability to customize at the OS and software level.\n• Develop an In-House Service Provider: Create an in-house service provider\nto control costs and maintain privacy. Often computing infrastructures are\nonly cost-effective at very large scale, which is why public cloud providers can\noffer services so economically. However, a large company can achieve similar\neconomies of scale by building a large infrastructure that is shared by many in-\nhouse customers. Because it is in-house, it is private—a criterion often required\nby industries that are highly regulated.\n• Contract for an On-Premises, Externally Run Service: Some companies will\nrun an in-house cloud service for you. They differ in how much control and\ncustomization they provide. You beneﬁt from their expertise and mitigate\nprivacy issues by owning and controlling the equipment.\n• Maximize Hardware Output: Pursue a strategy of squeezing every bit of\nproductivity and efﬁciency out of computers by eschewing virtualization.\n",
      "content_length": 3039,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "68\nChapter 3\nSelecting a Service Platform\nWhen an infrastructure has hundreds of thousands of computers or millions\nof cores, improving efﬁciency by 1 percent can be the equivalent of gaining\nthousands of new computers. The loss of efﬁciency from virtualization can\nbe a huge expense. In this strategy physical machines, rather than virtual\nmachines, are used and services are tightly packed on them to maximize\nutilization.\n• Implement a Bare Metal Cloud: Manage physical machine infrastructure\nlike a virtual machine cloud. Provide physical machines via the same API\nused for provisioning virtual machines. The beneﬁts of being able to spin\nup virtual machines can be applied to physical machines with some plan-\nning. Rather than selecting the exact custom conﬁguration needed for each\nphysical machine, some companies opt to purchase hundreds or thousands of\nmachines all conﬁgured the same way and manage them as a pool that can be\nreserved by departments or individuals. They do this by providing an API for\nallocating machines, wiping and reinstalling their OS, rebooting them, con-\ntrolling access, and returning them to the pool. The allocations may not be as\nfast or as dynamic as virtual machines, but many of the same beneﬁts can be\nachieved.\n3.6 Summary\nThis chapter examined a number of platforms. Infrastructure as a Service (IaaS)\nprovides a physical or virtual machine for your OS and application installs. Plat-\nform as a Service (PaaS) provides the OS and application stack or framework for\nyou. Software as a Service (SaaS) is a web-based application.\nYou can create your own cloud with physical or virtual servers, hosting them\nyourself or using an IaaS provider for the machines. Your speciﬁc business needs\nwill guide you in determining the best course of action for your organization.\nThere is a wide palette of choices from which to construct a robust and reliable\narchitecture for any given service or application. In the next chapter we examine\nthe various architectures themselves.\nExercises\n1. Compare IaaS, PaaS, and SaaS on the basis of cost, conﬁgurability, and control.\n2. What are the caveats to consider in adopting Software as a Service?\n3. List the key advantages of virtual machines.\n4. Why might you choose physical over virtual machines?\n5. Which factors might make you choose private over public cloud services?\n6. Which selection strategy does your current organization use? What are the\nbeneﬁts and caveats of using this strategy?\n",
      "content_length": 2474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Chapter 4\nApplication Architectures\nPatterns are solutions to recurring\nproblems in a context.\n—Christopher Alexander\nThis chapter examines the building blocks used when designing applications and\nother services. The previous chapter discussed cloud platform options. Now we\nmove up one layer to the application architecture.\nWe start with an examination of common web service architectures beginning\nwith a single web server, to multi-machine designs, growing larger and larger until\nwe have a design that is appropriate for a large global service. Then we exam-\nine architectures that are common behind the scenes of web applications: message\nbuses and service-oriented architectures.\nMost examples in this chapter will assume that the service is a web-based\napplication using the Hyper-Text Transfer Protocol (HTTP). The user runs a web\nbrowser such as Firefox, Chrome, or Internet Explorer. In HTTP terminology, this is\ncalled the client. Each request for a web page involves speaking the HTTP protocol\nto a web server, usually running on a machine elsewhere on the internet. The web\nserver speaks the server side of the HTTP protocol, receives the HTTP connection,\nparses the request, and processes it to generate the reply. The reply is an HTML\npage or other ﬁle that is sent to the client. The client then displays the web page or\nﬁle to the user. Generally each HTTP request, or query, is a separate TCP/IP con-\nnection, although there are extensions to the protocol that let one session process\nmany HTTP requests.\nSome applications use protocols other than HTTP. For example, they may\nimplement their own protocol. Some non-web applications use HTTP. For exam-\nple, mobile phone apps may use HTTP to talk to APIs to make requests or gather\ninformation. While most of our examples will assume web browsers speaking the\nHTTP protocol, the principles apply to any client/server application and protocol.\n69\n",
      "content_length": 1914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "70\nChapter 4\nApplication Architectures\n4.1 Single-Machine Web Server\nThe ﬁrst design pattern we examine is a single self-sufﬁcient machine used to\nprovide web service (Figure 4.1). The machine runs software that speaks the\nHTTP protocol, receiving requests, processing them, generating a result, and send-\ning the reply. Many typical small web sites and web-based applications use this\narchitecture.\nThe web server generates web pages from three different sources:\n• Static Content: Files are read from local storage and sent to the user\nunchanged. These may be HTML pages, images, and other content like music,\nvideo, or downloadable software.\n• Dynamic Content: Programs running on the web server generate HTML and\npossibly other output that is sent to the user. They may do so independently\nor based on input received from the user.\n• Database-Driven Dynamic Content: This is a special case of dynamic con-\ntent where the programs running on the web server consult a database for\ninformation and use that to generate the web page. In this architecture, the\ndatabase software and its data are on the same machine as the web server.\nFigure 4.1: Single-machine web service architecture\n",
      "content_length": 1186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "4.2\nThree-Tier Web Service\n71\nNot all web servers have all three kinds of trafﬁc. A static web server has no\ndynamic content. Dynamic content servers may or may not need a database.\nThe single-machine web server is a very common conﬁguration for web sites\nand applications. It is sufﬁcient for many small applications, but it does have limits.\nFor example, it cannot store or access more data than can ﬁt on a single machine.\nThe number of simultaneous users it can service is limited by the capacity of the\nmachine’s CPU, memory, and I/O capacity.\nThe system is also only as reliable as one machine can be. If the machine\ncrashes or dies, web service stops until the machine is repaired. Doing mainte-\nnance on the machine is also difﬁcult. Software upgrades, content changes, and so\non all are disruptions that may require downtime.\nAs the amount of trafﬁc received by the machine grows, the single-machine\nweb server may become overloaded. We can add more memory, more disk, and\nmore CPUs, but eventually we will hit the limits of the machine. These might be\ndesign limits that dictate the hardware’s physical connections (number of physical\nslots and ports) or internal limits such as the amount of bandwidth on the internal\ninterconnections between disk and memory. We can purchase a larger, more pow-\nerful machine, but we will eventually reach limits with that, too. As trafﬁc grows,\nthe system will inevitably reach a limit and the only solution will be to implement\na different architecture.\nAnother problem has to do with buffer thrashing. Modern operating systems\nuse all otherwise unused memory as a disk cache. This improves disk I/O per-\nformance. An operating system can use many different algorithms to tune the\ndisk cache, all having to do with deciding which blocks to discard when mem-\nory is needed for other processes or newer disk blocks. For example, if a machine\nis running a web server, the OS will self-tune for the memory footprint of the web\nserver. If a machine is running a database server, the OS will tune itself differently,\npossibly even selecting an entirely different block replacement algorithm.\nThe problem is that if a machine is running a web server and a database server,\nthe memory footprint may be complex enough that the operating system cannot\nself-tune for the situation. It may pick an algorithm that is optimal for only one\napplication, or it may simply give up and pick a default scheme that is non-optimal\nfor all. Attempts at improving this include Linux “containers” systems like LXC\nand Lmctfy.\n4.2 Three-Tier Web Service\nThe three-tier web service is a pattern built from three layers: the load balancer\nlayer, the web server layer, and the data service layer (see Figure 4.2). The web\nservers all rely on a common backend data server, often an SQL database. Requests\nenter the system by going to the load balancer. The load balancer picks one of the\n",
      "content_length": 2905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "72\nChapter 4\nApplication Architectures\nFigure 4.2: Three-tier web service architecture\nmachines in the middle layer and relays the request to that web server. The web\nserver processes the request, possibly querying the database to aid it in doing so.\nThe reply is generated and sent back via the load balancer.\nA load balancer works by receiving requests and forwarding them to one of\nmany replicas—that is, web servers that are conﬁgured such that they can all ser-\nvice the same URLs. Users talk to the load balancer as if it is a web server; they do\nnot realize it is a frontend for many replicas.\n4.2.1 Load Balancer Types\nThere are many ways to create a load balancer. In general, they fall into three\ncategories:\n• DNS Round Robin: This works by listing the IP addresses of all replicas in\nthe DNS entry for the name of the web server. Web browsers will receive all\nthe IP addresses but will randomly pick one of them to try ﬁrst. Thus, when\na multitude of web browsers visit the site, the load will be distributed almost\n",
      "content_length": 1028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "4.2\nThree-Tier Web Service\n73\nevenly among the replicas. The beneﬁt to this technique is that it is easy to\nimplement and free. There is no actual hardware involved other than the DNS\nserver, which already exists. However, this technique is rarely used because it\ndoesn’t work very well and is difﬁcult to control. It is not very responsive. If\none replica dies unexpectedly, clients will continue to try to access it as they\ncache DNS heavily. The site will appear to be down until those DNS caches\nexpire. There is very little control over which servers receive which requests.\nThere is no simple way to reduce the trafﬁc sent to one particular replica if it\nis becoming unusually overloaded.\n• Layer 3 and 4 Load Balancers: L3 and L4 load balancers receive each TCP ses-\nsion and redirect it to one of the replicas. Every packet of the session goes ﬁrst\nthrough the load balancer and then to the replica. The reply packets from the\nreplica go back through the load balancer. The names come from the ISO pro-\ntocol stack deﬁnitions: Layer 3 is the network (IP) layer; Layer 4 is the session\n(TCP) layer. L3 load balancers track TCP sessions based on source and des-\ntination IP addresses (i.e., the network layer). All trafﬁc from a given source\naddress will be sent to the same server regardless of the number of TCP ses-\nsions it has generated. L4 load balancers track source and destination ports\nin addition to IP addresses (i.e., the session layer). This permits a ﬁner granu-\nlarity. The beneﬁt of these load balancers is that they are simple and fast. Also,\nif a replica goes down, the load balancer will route trafﬁc to the remaining\nreplicas.\n• Layer 7 Load Balancer: L7 load balancers work similarly to L3/L4 load bal-\nancers but make decisions based on what can be seen by peering into the\napplication layer (Layer 7) of the protocol stack. They can examine what is\ninside the HTTP protocol itself (cookies, headers, URLs, and so on) and make\ndecisions based on what was found. As a result they offer a richer mix of fea-\ntures than the previous load balancers. For example, the L7 load balancer can\ncheck whether a cookie has been set and send trafﬁc to a different set of servers\nbased on that criterion. This is how some companies handle logged-in users\ndifferently. Some companies set a special cookie when their most important\ncustomers log in and conﬁgure the load balancer to detect that cookie and send\ntheir trafﬁc to especially fast servers.\nSome load balancers are transparent: the source IP address of the request is unal-\ntered. Most, however, are not: the source IP address of each request the backend\nsees is the IP address of the load balancer itself. That is, from the backend’s perspec-\ntive, it looks as if all requests are coming from a single source, the load balancer.\nThe actual source IP of the requests is obscured.\nWhen all requests appear to come from the same IP address, debugging and\nlog analysis may be impossible at worst and confusing at best. The usual way to\n",
      "content_length": 3007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "74\nChapter 4\nApplication Architectures\ndeal with this issue is for the load balancer to inject an additional header that indi-\ncates the IP address of the original requester. Backends can access this information\nas needed. This header is called X-Forwarded-For:. It contains a list of IP addresses\nstarting with the client’s and includes all the previous proxies or load balancers that\nthe request has passed through. Note that the client and intermediate devices can\nadd invalid or forged addresses to the list. Therefore you can only trust the address\nadded by your own load balancer. Using the rest is insecure and risky.\n4.2.2 Load Balancing Methods\nFor each request, an L3, L4, or L7 load balancer has to decide which backend to\nsend it to. There are different algorithms for making this decision:\n• Round Robin (RR): The machines are rotated in a loop. If there are three repli-\ncas, the rotation would look something like A-B-C-A-B-C. Down machines are\nskipped.\n• Weighted RR: This scheme is similar to RR but gives more queries to the back-\nends with more capacity. Usually a manually conﬁgured weight is assigned\nto each backend. For example, if there are three backends, two of equal capac-\nity but a third that is huge and can handle twice as much trafﬁc, the rotation\nwould be A-C-B-C.\n• Least Loaded (LL): The load balancer receives information from each backend\nindicating how loaded it is. Incoming requests always go to the least loaded\nbackend.\n• Least Loaded with Slow Start: This scheme is similar to LL, but when a new\nbackend comes online it is not immediately ﬂooded with queries. Instead, it\nstarts receiving a low rate of trafﬁc that slowly builds until it is receiving an\nappropriate amount of trafﬁc. This ﬁxes the problems with LL as described in\nSection 1.3.1.\n• Utilization Limit: Each server estimates how many more QPS it can handle\nand communicates this to the load balancer. The estimates may be based on\ncurrent throughput or data gathered from synthetic load tests.\n• Latency: The load balancer stops forwarding requests to a backend based\non the latency of recent requests. For example, when requests are taking\nmore than 100 ms, the load balancer assumes this backend is overloaded.\nThis technique manages bursts of slow requests or pathologically overloaded\nsituations.\n• Cascade: The ﬁrst replica receives all requests until it is at capacity. Any over-\nﬂow is directed to the next replica, and so on. In this case the load balancer\nmust know precisely how much trafﬁc each replica can handle, usually by\nstatic conﬁguration based on synthetic load tests.\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "4.2\nThree-Tier Web Service\n75\n4.2.3 Load Balancing with Shared State\nAnother issue with load balancing among many replicas is shared state. Suppose\none HTTP request generates some information that is needed by the next HTTP\nrequest. A single web server can store that information locally so that it is available\nwhen the second HTTP request arrives. But what if the load balancer sends the next\nHTTP request to a different backend? It doesn’t have that information (state). This\ncan cause confusion.\nConsider the commonly encountered case in which one HTTP request takes a\nuser’s name and password and validates it, letting the user log in. The server stores\nthe fact that the user is logged in and reads his or her proﬁle from the database. This\nis stored locally for fast access. Future HTTP requests to the same machine know\nthat the user is logged in and have the user proﬁle on hand, so there’s no need to\naccess the database.\nWhat if the load balancer sends the next HTTP request to a different backend?\nThis backend will not know that the user is logged in and will ask the user to log\nin again. This is annoying to the user and creates extra work for the database.\nThere are a few strategies for dealing with this situation:\n• Sticky Connections: Load balancers have a feature called stickiness, which\nmeans if a user’s previous HTTP request went to a particular backend, the next\none should go there as well. That solves the problem discussed earlier, at least\ninitially. However, if that backend dies, the load balancer will send requests\nfrom that user to another backend; it has no choice. This new backend will not\nknow the user is logged in. The user will be asked to log in again. Thus, this\nis only a partial solution.\n• Shared State: In this case the fact that the user has logged in and the user’s pro-\nﬁle information are stored somewhere that all backends can access. For each\nHTTP connection, the user’s state is fetched from this shared area. With this\napproach, it doesn’t matter if each HTTP request goes to a different machine.\nThe user is not asked to log in every time the backends are switched.\n• Hybrid: When a user’s state moves from one backend to another, it generally\ncreates a little extra work for the web server. Sometimes this burden is small\nand tolerable. For some applications, however, it is extremely inconvenient\nand requires a lot more processing. In that case using both sticky connections\nand shared state is the best solution.\nThere are many schemes for storing and retrieving the sharing state. A simple\napproach is to use a database table on a database server to which all backends have\naccess. Unfortunately, databases may respond slowly, as they are not optimized\nfor this type of operation. Other systems are speciﬁcally designed for shared state\n",
      "content_length": 2799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "76\nChapter 4\nApplication Architectures\nstorage, often holding all the information in RAM for fastest access. Memcached\nand Redis are two examples. In any event, you should avoid using a directory on\nan NFS server, as that does not provide failover in the event of a server outage or\nreliable ﬁle locking.\n4.2.4 User Identity\nAlthough the interaction with a web site appears to the user to be one seamless\nsession, in truth it is made up of many distinct HTTP requests. The backends need\nto know that many HTTP requests are from the same user. They cannot use the\nsource IP address of the HTTP request: due to the use of network address transla-\ntion (NAT), many different machines are often seen as using the same IP address.\nEven if that weren’t the case, the IP address of a particular machine changes from\ntime to time: when a laptop moves from one WiFi network to another, when a\nmobile device moves from WiFi to cellular and back, or if any machine is turned\noff and turned on again on a different (or sometimes even the same) network. Using\nthe IP address as an identity wouldn’t even work for one user running two web\nbrowsers on the same machine.\nInstead, when a user logs into a web application, the web application generates\na secret and includes it with the reply. The secret is something generated randomly\nand given to only that user on that web browser. In the future, whenever that web\nbrowser sends an HTTP request to that same web app, it also sends the secret.\nBecause this secret was not sent to any other user, and because the secret is difﬁcult\nto guess, the web app can trust that this is the same user. This scheme is known as\na cookie and the secret is often referred to as a session ID.\n4.2.5 Scaling\nThe three-tier web service has many advantages over the single web server. It is\nmore expandable; replicas can be added. If each replica can process 500 queries\nper second, they can continue to be added until the total required capacity is\nachieved. By splitting the database service onto its own platform, it can be grown\nindependently.\nThis pattern is also very ﬂexible. This leads to many variations:\n• Replica Groups: The load balancer can serve many groups of replicas, not just\none. Each group serves a different web site. The number of replicas in each\ngroup can be grown independently as that web site requires.\n• Dual Load Balancers: There can be multiple load balancers, each a replica of\nthe other. If one fails, the other can take over. This topic is discussed further in\nSection 6.6.3.\n",
      "content_length": 2525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "4.3\nFour-Tier Web Service\n77\n• Multiple Data Stores: There may be many different data stores. Each replica\ngroup may have its own data store, or all of the data stores may be shared.\nThe data stores may each use the same or different technology. For example,\nthere may be one data store dedicated to shared state and another that stores\na product catalog or other information needed for the service.\n4.3 Four-Tier Web Service\nA four-tier web service is used when there are many individual applications with a\ncommon frontend infrastructure (Figure 4.3). In this pattern, web requests come in\nas usual to the load balancer, which divides the trafﬁc among the various frontends.\nFigure 4.3: Four-tier web service architecture\n",
      "content_length": 724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "78\nChapter 4\nApplication Architectures\nThe frontends handle interactions with the users, and communicate to the applica-\ntion servers for content. The application servers access shared data sources in the\nﬁnal layer.\nThe difference between the three-tier and four-tier designs is that the applica-\ntion and the web servers run on different machines. The beneﬁts of using the latter\ndesign pattern are that we decouple the customer-facing interaction, protocols, and\nsecurity issues from the applications. The downside is that it takes a certain amount\nof trust for application service teams to rely on a centralized frontend platform\nteam. It also takes management discipline to not allow exceptions.\n4.3.1 Frontends\nThe frontends are responsible for tasks that are common to all applications, thus\nreducing the complexity of the applications. Because the user interactions and\napplication services are decoupled, each can focus on doing one thing well.\nThe frontends handle all cookie processing, session pipelining (handling mul-\ntiple HTTP requests over the same TCP connection), compression, and encryption.\nThe frontends can implement HTTP 2.0 even though the application servers may\nstill be stuck on HTTP 1.1. In fact, application servers often implement a simpler\nsubset of the HTTP protocol, knowing they will be behind frontends that imple-\nment HTTP fully. End users still beneﬁt from these features because the frontend\nsupports them and acts as a gateway.\nFrontend software can track the ever-changing HTTP protocol deﬁnition so\nthat the application servers do not have to. HTTP started out as a simple protocol,\nhandling simple requests. Over time, it has evolved to have a much richer feature\nset, which makes it more complex and difﬁcult to implement. Being able to inde-\npendently upgrade the frontends means not having to wait for each application\nserver to do so.\nFrontends process everything related to the users logging in and logging out.\nHandling these tasks at the frontend tier makes it easy to have a uniﬁed username\nand password infrastructure for all applications. The requests from the frontends\nto the application layer include the username that has been pre-authenticated. The\napplications can trust this information implicitly.\nFrontends often ﬁx problems so that the application servers don’t have to. For\nexample, HTTP headers are case insensitive, but some application servers have\nbroken implementations that assume they are case sensitive. Frontends can auto-\nmatically downcase all headers so that the application servers only see the clean\nprotocol headers that they naively expect. If an application server doesn’t support\nIPv6, the frontend can receive requests via IPv6 but speak IPv4 to the backends.\nSome application servers don’t even use HTTP. Some companies have\ninvented their own protocols for communication between the frontends and the\napplication servers that are much faster and more efﬁcient.\n",
      "content_length": 2947,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "4.3\nFour-Tier Web Service\n79\nEncryption and Certificate Management\nEncryption is particularly important to centralize at one layer because certiﬁcate\nmanagement is quite complex. It is easy for the non-expert to make mistakes that\nweaken the security of the system at best and make the service stop working at\nworst. By centralizing the encryption function, you can assure that the people\nmanaging it are experts in their ﬁeld. Typically each application server is run by\na different team. Expecting each team to be highly qualiﬁed at crypto certiﬁcate\nmanagement as well as their application is unreasonable.\nEven if they all had a high degree of security expertise, there is still another\nissue: trust. Each person who manages the crypto certiﬁcates has to be trusted not\nto expose or steal the keys. Giving that trust to one specialized team is more secure\nthan giving that trust to members of many individual teams. In some cases all ser-\nvices use the same key, which means an accidental leak of the key by any one\nteam would weaken security for all applications. As Cheswick, Bellovin, and Rubin\n(2003) suggest, often the best security policy is to put all your eggs in one basket\nand then make sure it is a really strong basket.\nSecurity Benefits\nThe frontends are the one part of the system that is directly exposed to the Inter-\nnet. This reduces the number of places that have to be secured against attacks.\nIn the security ﬁeld this approach is called reducing the attack surface area. By\ndecoupling these functions from the applications, bugs and security holes can be\nﬁxed more rapidly.\nHTTP is a complex protocol and becomes more complex with every revision.\nThe more complex something is, the more likely it is to contain bugs or security\nholes. Being able to upgrade the frontends rapidly and independently of any appli-\ncation server upgrade schedule is important. Application teams have their own\npriorities and may not be willing or able to do a software upgrade at the drop of a\nhat. There are often dozens or hundreds of individual applications and application\nteams, and tracking all of their upgrades would be impossible.\n4.3.2 Application Servers\nThe frontends send queries to the application servers. Because all HTTP processing\nis handled by the frontends, this permits the frontend-to-application protocol to be\nsomething other than HTTP. HTTP is a general protocol, so it is slow and not as\ngood at serving API requests as a purpose-built protocol can be.\nSplitting application servers from the frontends also means that different\napplications can run on different servers. Having dedicated servers for frontends\nand for each application means that each component can be scaled independently.\nAlso, it brings even greater reliability because problems in one application do not\naffect other applications or the frontends.\n",
      "content_length": 2849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "80\nChapter 4\nApplication Architectures\n4.3.3 Configuration Options\nSplitting functionality into frontend, application server, and data service layers\nalso permits one to pick hardware speciﬁcally appropriate for each layer. The front-\nends generally need high network throughput and very little storage. They are also\nlocated in a special place in the network so they have direct internet access. The\napplication servers may be on machines that are conﬁgured differently for each\napplication based on their needs. The database service most likely has a need for\nlarge amounts of disk storage or may be a server tree (as described in Section 1.3.3).\n4.4 Reverse Proxy Service\nA reverse proxy enables one web server to provide content from another web\nserver transparently. The user sees one cohesive web site, even though it is actually\nmade up of a patchwork of applications.\nFor example, suppose there is a web site that provides users with sports news,\nweather reports, ﬁnancial news, an email service, plus a main page:\n• www.company.com/ (the main page)\n• www.company.com/sports\n• www.company.com/weather\n• www.company.com/finance\n• www.company.com/email\nEach of those web features is provided by a very different web service, but all\nof them can be combined into a seamlessly uniﬁed user experience by a reverse\nproxy. Requests go to the reverse proxy, which interprets the URL and collects the\nrequired pages from the appropriate server or service. This result is then relayed\nto the original requester.\nLike the frontend four-tier web service, this scheme permits centralizing secu-\nrity and other services. Having many applications behind one domain simpliﬁes\nmany administrative tasks, such as maintaining per-domain crypto certiﬁcates.\nThe difference between a reverse proxy and the frontend of the four-tier web\nservice is that a reverse proxy is simpler and usually just connects the web browsers\nand the patchwork of HTTP servers that sit behind it. Sometimes reverse proxies\nare stand-alone software, but the functionality is so simple that it is often a feature\nbuilt into general web server software.\n4.5 Cloud-Scale Service\nCloud-scale services are globally distributed. The service infrastructure uses one\nof the previously discussed architectures, such as the four-tier web service, which\n",
      "content_length": 2308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "4.5\nCloud-Scale Service\n81\nis then replicated in many places around the world. A global load balancer is used\nto direct trafﬁc to the nearest location (Figure 4.4).\nIt takes time for packets to travel across the internet. The farther the distance,\nthe longer it takes. Anyone who has spent signiﬁcant time in Australia accessing\ncomputers in the United States will tell you that even though data can travel at the\nspeed of light, that’s just not fast enough. The latency over such a great distance\ngives such terrible performance for interactive applications that they are unusable.\nWe ﬁx this problem by bringing the data and computation closer to the users.\nWe build multiple datacenters around the world, or we rent space in other people’s\ndatacenters, and replicate our services in each of them.\n4.5.1 Global Load Balancer\nA global load balancer (GLB) is a DNS server that directs trafﬁc to the nearest data-\ncenter. Normally a DNS server returns the same answer to a query no matter where\nthe query originated. A GLB examines the source IP address of the query and\nreturns a different result depending on the geolocation of the source IP address.\nGeolocation is the process of determining the physical location of a machine on\ntheinternet.Thisisaverydifﬁculttask.Unlikeaphonenumber,whosecountrycode\nandareacodegiveafairlyaccurateindicationofwheretheuseris(althoughthis,too,\nis less accurate in the age of cell phones), an IP address has no concrete geographic\nmeaning. There is a small industry consisting of companies that use various means\n(and a lot of guessing) to determine where each IP subnet is physically located.\nThey sell databases of this information for the purpose of geolocation.\nFigure 4.4: Cloud-scale web service architecture\n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "82\nChapter 4\nApplication Architectures\n4.5.2 Global Load Balancing Methods\nA GLB maintains a list of replicas, their locations, and their IP addresses. When\na GLB is asked to translate a domain name to an IP address, it takes into account\nthe geolocation of the requester when determining which IP address to send in the\nreply.\nGLBs use many different techniques:\n• Nearest: Strictly selects the nearest datacenter to the requester.\n• Nearest with Limits: The nearest datacenter is selected until that site is full.\nAt that point, the next nearest datacenter is selected. Slow start, as described\npreviously, is included for the same reasons as on local load balancers.\n• Nearest by Other Metric: The best location may be determined not by distance\nbut rather by another metric such as latency or cost. Latency and distance\nare usually the same but not always. For example, for a long time the only\nroute between most South American countries was via the United States. In\nSection 4.5.4 we’ll see that cost is not always a function of distance, either.\n4.5.3 Global Load Balancing with User-Specific Data\nNow that the HTTP request is directed to a particular datacenter, it will be handled\nby local load balancers, frontends, and whatever else makes up the service. This\nbrings up another architectural issue. Suppose the service stores information for a\nuser. What happens if that user’s data is stored at some other datacenter?\nFor example, a global email service provider might have datacenters around\nthe world. It may store a person’s email in the datacenter nearest to the user\nwhen he or she creates an account. But what if the person moves? Or what if that\ndatacenter is decommissioned and the person’s account is moved to some other\ndatacenter?\nA global load balancer works on the DNS level, which has no idea who the user\nis. It cannot determine that Mary sent the DNS query and return the IP address of\nthe service replica with her data.\nThere are a few solutions to this problem. First, usually each user’s email is\nstored in two different datacenters. That way, if one datacenter goes down, the data\nis still available. Now there are twice as many chances that Mary will be directed\nto a datacenter with her email, but there is still a chance that her HTTP requests\nwill reach the wrong datacenter.\nTo resolve this dilemma, the frontend communicates with the email service to\nﬁnd out where Mary’s email is and, from then on, accesses the application servers\nin the correct datacenter. The web frontends are generic, but they pull email from\nthe speciﬁc datacenter.\n",
      "content_length": 2578,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "4.5\nCloud-Scale Service\n83\nTo do this the company must have connections between each datacenter so\nthat the frontends can talk to any application server. They could communicate\nbetween datacenters over the internet, but typically a company in this situation\nowns private, dedicated wide area network (WAN) connections between data-\ncenters. A dedicated WAN gives the company more control and more reliable\nperformance.\n4.5.4 Internal Backbone\nThe private WAN links that connect datacenters form an internal backbone. An\ninternal backbone is not visible to the internet at large. It is a private network.\nThis internal network connects to the internet in many places. Wherever there\nis a datacenter, the datacenter will generally connect to many ISPs in the area. Con-\nnecting to an ISP directly has speed and cost beneﬁts. If you do not have a direct\nconnection to a particular ISP, then sending data to users of that ISP involves send-\ning the data through another ISP that connects your ISP and theirs. This ISP in the\nmiddle is called a transit ISP. The transit ISP charges the other ISPs for the privilege\nof permitting packets to travel over its network. There are often multiple transit\nISPs between you and your customers. The more transits, the slower, less reliable,\nand more expensive the connections become.\nPOPs\nA point of presence (POP) is a small, remote facility used for connection to local\nISPs. It is advantageous to connect to many ISPs but they cannot always connect to\nyour datacenter. For example, your datacenter may not be in the state or country\nthey operate in. Since they cannot go to you, you must extend your network to\nsomeplace near them. For example, you might create a POP in Berlin to connect to\nmany different German ISPs.\nA POP is usually a rack of equipment in a colocation center or a small space\nthat resembles a closet. It contains network equipment and connections from\nvarious telecom providers, but no general-purpose computers.\nA POP plus a small number of computers is called a satellite. The computers\nare used for frontend and content distribution services. The frontends terminate\nHTTP connections and proxy to application servers in other datacenters. Con-\ntent distribution servers are machines that cache large amounts of content. For\nexample, they may store the 1000 most popular videos being viewed at the time\n(Figure 4.5).\nThus an internal network connects datacenters, POPs, and satellites.\nGlobal Load Balancing and POPs\nThis brings up an interesting question: should the GLB direct trafﬁc to the near-\nest frontend or to the nearest datacenter that has application servers for that\n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "84\nChapter 4\nApplication Architectures\nFigure 4.5: A private network backbone connecting many datacenters (DCs) and\npoints of presence (POPs) on the Internet\nparticular service? For example, imagine (and these numbers are completely ﬁcti-\ntious) Google had 20 POPs, satellites, and datacenters but Google Maps was served\nout of only 5 of the datacenters. There are two paths to get to a server that provides\nthe Google Maps service.\nThe ﬁrst path is to go to the nearest frontend. Whether that frontend is in a\ndatacenter or satellite doesn’t matter. There’s a good chance it won’t be in one of\nthe ﬁve datacenters that host Google Maps, so the frontend will then talk across\nthe private backbone to the nearest datacenter that does host Maps. This solution\nwill be very fast because Google has total control over its private backbone, which\ncan be engineered to provide the exact latency and bandwidth required. There are\nno other customers on the backbone that could hog bandwidth or overload the\nlink. However, every packet sent on the backbone has a cost associated with it, and\nGoogle pays that expense.\nThe second path is to go to a frontend at the nearest datacenter that hosts\nGoogle Maps. If that datacenter is very far away, the query may traverse multi-\nple ISPs to get there, possibly going over oceans. This solution will be rather slow.\nThese transcontinental links tend to be overloaded and there is no incentive for\nthe ISPs involved to provide stellar performance for someone else’s trafﬁc. Even\nso, the cost of the transmission is a burden on the ISP, not Google. It may be slow,\nbut someone else is bearing the cost.\n",
      "content_length": 1636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "4.6\nMessage Bus Architectures\n85\nWhich path does Google Maps trafﬁc take? The fast/expensive one or the\ncheap/slow one? The answer is: both!\nGoogle wants its service to be fast and responsive to the user. Therefore trafﬁc\nrelated to the user interface (UI) is sent over the ﬁrst path. This data is HTML and\nis generally very small.\nThe other part of Google Maps is delivering the map tiles—that is, the graph-\nics that make up the maps. Even though they are compressed, they are big and\nbulky and use a lot of bandwidth. However, they are loaded “off screen” by very\ncrafty JavaScript code, so responsiveness isn’t required. The map tiles can load\nvery slowly and it would not hinder the users’ experience. Therefore the map tile\nrequests take the slow but inexpensive path.\nIf you look at the HTML code of Google Maps, you will see that the URLs\nof the UI and map tiles refer to different hostnames. This way the global DNS\nload balancer can assign different paths to the different types of trafﬁc. The GLB\nis conﬁgured so that maps.google.com, which is used for all the elements related\nto the UI, returns the IP address of the nearest frontend. The map tiles are loaded\nusing URLs that contain a different hostname. The GLB is conﬁgured so that this\nhostname returns the IP address of a frontend in the nearest datacenter that serves\nGoogle Maps. Thus users get fast interaction and Google pays less for bandwidth.\n4.6 Message Bus Architectures\nA message bus is a many-to-many communication mechanism between servers.\nIt is a convenient way to distribute information among different services. Mes-\nsage buses are becoming a popular architecture used behind the scenes in sys-\ntem administration systems, web-based services, and enterprise systems. This\napproach is more efﬁcient than repeatedly polling a database to see if new infor-\nmation has arrived.\nA message bus is a mechanism whereby servers send messages to “channels”\n(like a radio channel) and other servers listen to the channels they need. A server\nthat sends messages is a publisher and the receivers are subscribers. A server can\nbe a publisher or subscriber of a given channel, or it can simply ignore the channel.\nThis permits one-to-many, many-to-many, and many-to-one communication. One-\nto-many communication enables one server to quickly send information to a large\nset of machines. Many-to-many communication resembles a chat room applica-\ntion, where many people all hear what is being said. Many-to-one communication\nenables a funnel-like conﬁguration where many machines can produce informa-\ntion and one machine takes it in. A central authority, or master, manages which\nservers are connected to which channels.\nThe messages being sent may contain any kind of data. They may be real-time\nupdates such as chat room messages, database updates, or notiﬁcations that update\n",
      "content_length": 2849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "86\nChapter 4\nApplication Architectures\na user’s display to indicate there are messages waiting in the inbox. They may be\nlow-priority or batch updates communicating status changes, service requests, or\nlogging information.\nMessage bus technology goes by many names, including message queue,\nqueue service, or pubsub service. For example, Amazon provides the Simple\nQueue Service (SQS), MCollective is described as publish subscribe middleware,\nand RabbitMQ calls itself a message broker.\nA message bus system is efﬁcient in that clients receive a message only if\nthey are subscribed. There may be hundreds or thousands of machines involved,\nbut different subsets of machines will typically be subscribed to different chan-\nnels. Messages are transmitted only to the subscribed machines. Thus network\nbandwidth and processing are conserved. This approach is more efﬁcient than a\nbroadcast system that sends all messages to all machines and lets the receiving\nmachines ﬁlter out the messages they aren’t interested in.\nMessage bus systems are operations-friendly. It’s trivial to connect a\ncommand-line client to listen to messages and see what is being emitted. This\ncapability is very handy when debugging requires peering into information\nﬂows.\n4.6.1 Message Bus Designs\nThe message bus master learns the underlying network topology and for each\nchannel determines the shortest path a message needs to follow. IP multicast is\noften used to send a message to many machines on the same subnet at the same\ntime. IP unicast is used to transmit a message between subnets or when IP multicast\nisn’t available. Determining and optimizing the unique topology of each channel\nseparately requires a lot of calculations, especially when there are thousands of\nchannels.\nSome message bus systems require all messages to ﬁrst go to the master for\ndistribution. Consequently, the master may become a bottleneck. Other systems\nare more sophisticated and either have multiple masters, one master per channel,\nor have a master that controls topology but does not require all messages to go\nthrough it.\nChannels may be open to anyone, or they may be tightly controlled with ACLs\ndetermining who can publish and who can subscribe. On some systems the listen-\ners can send a reply to the message sender. Other systems do not have this feature\nand instead create a second channel for publishing replies.\nThere may be one channel, or thousands. Different message bus systems\nare optimized for different sizes. Google’s PubSub2 system (Publish Subscribe\nVersion 2) can handle hundreds of channels and tens of thousands of hosts.\nGoogle’s Thialﬁsystem can handle 2.3 million subscribers (Adya, Cooper,\nMyers & Piatek 2011).\n",
      "content_length": 2701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "4.6\nMessage Bus Architectures\n87\n4.6.2 Message Bus Reliability\nMessage bus systems usually guarantee that every message will be received. For\nexample, some message bus systems require subscribers to acknowledge each mes-\nsage received. If the subscriber crashes, when it returns to service it is guaranteed\nto receive all the unacknowledged messages again; if the acknowledgment did not\nmake it back to the message bus system, the subscriber may receive a second copy\nof the message. It is up to the subscriber to detect and skip duplicates.\nHowever, message bus systems do vary in how they handle long subscriber\noutages. In some systems, subscribers miss messages when they are down. In oth-\ners, messages are stored up for a certain amount of time and are lost only if the\nsubscriber is down for more than a deﬁned length of time. Some message bus\nsystems might hold things in RAM for just a few minutes before giving up and\nqueueing messages to disk. When a subscriber comes back online, it will get the\nbacklog of messages in a ﬂood.\nMessage bus systems do not guarantee that messages will be received in the\nsame order as they were sent. Doing so would mean that if one message was being\nretried, all other messages would have to wait for that message to succeed. In the\nmeantime, millions of messages might be delayed.\nFor this reason, the subscriber must be able to handle messages arriving out of\norder. The messages usually include timestamps, which help the subscriber detect\nwhen messages do arrive out of order. However, reordering them is difﬁcult, if\nnot impossible. You could hold messages until any late ones have arrived, but\nhow would you know how long to wait? If you waited an hour, a slow-poke mes-\nsage might arrive 61 minutes late. It is best to write code that does not depend on\nperfectly ordered messages than to try to reorder them.\nBecause messages may be missed or lost, usually services have a mechanism\nthat is not based on the message bus but that enables clients to catch up on anything\nthey missed. For example, if the message bus is used to keep a database in sync,\nthere may be a way to receive a list of all database keys and the date they were last\nupdated. Receiving this list once a day enables the subscriber to notice any missing\ndata and request it.\nAs systems get larger, message bus architectures become more appealing\nbecause they are fast, they are efﬁcient, and they push control and operational\nresponsibility to the listeners. A good resource for using message bus architec-\ntures is Enterprise Integration Patterns: Designing, Building, and Deploying Messaging\nSolutions by Hohpe and Woolf (2003).\n4.6.3 Example 1: Link-Shortening Site\nA link-shortening site very much like bit.ly had a message bus architecture used by\nits components to communicate. The application had two user-visible components:\nthe control panel (a web UI for registering new URLs to be shortened) and the\n",
      "content_length": 2930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "88\nChapter 4\nApplication Architectures\nweb service that took in short links and responded with the redirect code to the\nexpanded URL.\nThe company wanted fast updates between the user interface and the redirect\nservers. It was common for users to create a short link via the control panel and\nthen immediately try to use the link to make sure that it worked. Initially there was\na multi-minute delay between when the link was created and when the redirec-\ntion service was able to redirect it. The new link had to be indexed and processed,\nthen added to the database, and the database changes had to propagate to all the\nredirection servers.\nTo ﬁx this problem the company set up a message bus system that connected\nall machines. There were two channels: one called “new shortlinks” and one called\n“shortlinks used.”\nAs depicted in Figure 4.6, the architecture had four elements:\n• Control Panel: A web frontend that was the portal people used to create new\nshortlinks.\n• Main Database: A database server that stored all the shortlink information.\n• Trend Server: A server that kept track of “trending links” statistics.\nFigure 4.6: Link-shortening architecture\n",
      "content_length": 1161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "4.6\nMessage Bus Architectures\n89\n• Link Redirect Servers: Web servers that received requests with short URLs\nand replied with a redirect to the expanded URL.\nWhen any control panel server received a new shortlink from a user, it would pub-\nlish it on the “new shortlink” channel. Subscribers to that channel included the\ndatabase (which stored the new shortlink information) as well as all the redirect\nservers (which stored the new information in their lookup cache). When perform-\ning a redirect, each redirect server relied on information from its local cache and\nconsulted the database only in case of a cache miss. Since it could take minutes or\nhours for the main database to ingest a new shortlink (due to indexing and other\nissues), the message bus architecture enabled the redirect servers to always be up\nto date.\nWhen a redirect server used a shortlink, it would publish this fact on the\n“shortlinks used” channel. There were only two subscribers to this channel: the\nmain database and the trend server. The main database updated the usage statis-\ntics for that database entry. The trend server kept an in-memory database of which\nURLs had been used recently so that the company could always display an accurate\nlist of trending URLs.\nImagine how difﬁcult creating this functionality would have been without a\nmessage bus. The control panel machines would need an “always up-to-date” list\nof all the redirect servers. It would have to handle down machines, new machines,\nand so on. The communication would not be optimized for the network topology.\nIt would be very difﬁcult to achieve the same quality of service as that provided\nby message bus.\nWithout a message bus system, adding a new service would mean changing all\nthe data providers to also send a copy to it. Suppose an entirely new mechanism for\ncalculating trending links was devised. Without a message bus, the redirect servers\nwould have to be updated to send information to them. With the message bus\nsystem, the new trend server could be added and run in parallel without requiring\nchanges to the other servers. If it proved successful, the old trend server could be\ndisconnected just as easily.\n4.6.4 Example 2: Employee Human Resources Data Updates\nAn enterprise had a large employee base. Each day many new employees were\nhired, left the company, changed names, or changed other database attributes.\nMeanwhile, there were numerous consumers of this information: the login/\nauthentication system, the payroll system, the door-lock systems, and many\nothers.\nIn the ﬁrst incarnation of the system, a single database was maintained. All\nsystems queried the database for changes. As the company grew, however, the\ndatabase got more and more overloaded.\n",
      "content_length": 2727,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "90\nChapter 4\nApplication Architectures\nIn the second incarnation, any department that needed to be informed of\nuser change requests had to write a plug-in that would be called for each change.\nThe plug-in had to handle subcommands including add, update, remove, name-\nchange, and others. The plug-ins all ran under the same role account, which meant\nthis one account had privileged access to every critical system in the company. It\nwas a nightmare. If one department’s plug-in had a bug, it would take down the\nentire system. This system remained in operation for years even though it was very\nbrittle and required a lot of work to keep it running.\nThe newest version of the system implemented a message bus. All changes\nwent through one system that was the publisher on a channel called “user\nupdates.” Any department that needed to be informed of changes would subscribe\nto that channel. The department could run its system on its own role account, which\nisolated each department into its own security domain. Each department also had\nits own failure domain, as the failure of one listener did not affect any of the others.\nA synchronization mechanism talked to each department once a day so that any\nmissed updates could be processed.\nNew departments could join the system at will without affecting any others.\nBest of all, this system was very easy to maintain. Responsibility was distributed\nto each department. The main group simply had to make sure the publisher was\nworking.\n4.7 Service-Oriented Architecture\nService-oriented architecture (SOA) enables large services to be managed more\neasily. With this architecture, each subsystem is a self-contained service provid-\ning its functionality as a consumable service via an API. The various services\ncommunicate with one another by making API calls.\nA goal of SOAs is to have the services be loosely coupled. That is, each API\npresents its service at a high level of abstraction. This makes it easier to improve\nand even replace a service. The replacement must simply provide the same abstrac-\ntion. Loosely coupled systems do not know the internal workings of the other\nsystems that are part of the architecture. If they did, they would be tightly bound\nto each other.\nAs an example, imagine a job scheduler service. It accepts requests to perform\nvarious actions, schedules them, coordinates them, and reports back progress as it\nexecutes. In a tightly coupled system, the API would be tightly linked to the inner\nworkings of the job scheduler. Users of the API could specify details related to how\nthe jobs work rather than what is needed. For example, the API might provide\ndirect access to the status of the lock mechanism used to prevent the same job from\nbeing executed twice.\nSuppose a new internal design was proposed that prevented duplicate job exe-\ncution but did locking some other way. This change could not be made without\n",
      "content_length": 2896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "4.7\nService-Oriented Architecture\n91\nchanging the code of all the services that used the API. In a loosely coupled sys-\ntem, the API would provide job status at a higher level of abstraction: is the job\nwaiting, is it running, where is it running, can it be stopped, and so on. No matter\nwhat the internal implementation was, these requests could be processed.\n4.7.1 Flexibility\nAn SOA makes it easier for services to cooperate. Services can be combined in\nmany, often unexpected, ways to create new applications. New applications can\nbe designed without consultation of all the other services as long as the API meets\nthe new application’s needs. Each subsystem can be managed as a discrete sys-\ntem. It is easier to manage a few, small, easy-to-understand services than one large\nsystem with ill-deﬁned internal interconnections. When the touch-points between\nservices are well-deﬁned APIs that implement high-level abstractions, it makes it\neasier to evolve and/or replace the service.\n4.7.2 Support\nSOAs have beneﬁts at the people management level, too. As a system grows, the\nteam that develops and operates it tends to grow as well. Large teams are more\ndifﬁcult to manage than smaller, focused teams. With an SOA it is easy to split up\na team every time it grows beyond a manageable limit. Each team can focus on a\nrelated set of subsystems. They can even trade subsystems between teams as skills\nand demands require. Contrast this to a large, tightly coupled system that cannot\nbe easily divided and managed by separate teams.\n.\nSplitting Teams by Functionality\nAt Google, Gmail was originally maintained by one group of Google site reli-\nability engineers (SREs). As the system grew, subteams split off to focus on\nsubsystems such as the storage layer, the anti-spam layer, the message receiv-\ning system, the message delivery system, and so on. This was possible because\nof the SOA design of the system.\n4.7.3 Best Practices\nFollowing are some best practices for running an SOA:\n• Use the same underlying RPC protocol to implement the APIs on all services.\nThis way any tool related to the RPC mechanism is leveraged for all services.\n",
      "content_length": 2145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "92\nChapter 4\nApplication Architectures\n• Have a consistent monitoring mechanism. All services should expose mea-\nsurements to the monitoring system the same way.\n• Use the same techniques with each service as much as possible. Use the same\nload balancing system, management techniques, coding standards, and so on.\nAs services move between teams, it will be easier for people to get up to speed\nif these things are consistent.\n• Adopt some form of API governance. When so many APIs are being designed,\nit becomes important to maintain standards for how they work. These\nstandards often impart knowledge learned through painful failures in the past\nthat the organization does not want to see repeated.\nWhen a tightly coupled system becomes difﬁcult to maintain, one option is to\nevolve it into a loosely coupled system. Often when systems are new, they start\nout tightly coupled with the justiﬁcation, real or not, of being more resource efﬁ-\ncient or faster to develop. Decoupling the components can be a long and difﬁcult\njourney. Start by identifying pieces that can be spilt out as services one at a time.\nDo not pick the easiest pieces but rather the pieces most in need of the beneﬁts of\nSOA: ﬂexibility, ease of upgrade and replacement, and so on.\n4.8 Summary\nWeb-based applications need to grow and scale. A small service can run on a sin-\ngle machine, serving content that is static, dynamic, or database driven. When the\nlimits of one machine are reached, a three-tier architecture is used. It moves each\nfunction to a different machine: a load balancer to direct trafﬁc among web servers,\nand one or more data servers providing static or database content.\nLocal load balancers distribute trafﬁc between replicas within a datacenter.\nThey work by intercepting trafﬁc and redirecting it to web replicas. There are many\nload balancing technologies, and many algorithms for deciding how to distribute\ntrafﬁc. When an application is divided among replicas, synchronizing user identity\nand other state information becomes complex. This goal is usually achieved by\nhaving some kind of server that stores state common to all replicas.\nA four-tier architecture creates a frontend to many three-tier systems. The new\ntier handles common services, often related to user sessions, security, and logging.\nA reverse proxy ensures that many application servers appear to be a single large\napplication.\nCloud-scale services take this architecture and replicate it to many datacenters\naround the world. They use global load balancers to direct trafﬁc to particular data-\ncenters. A private network between datacenters may be used if one would beneﬁt\nfrom being able to control inter-datacenter network quality. This private network\nmay connect to the internet at many points of presence to improve connectivity at\n",
      "content_length": 2807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Exercises\n93\nmany ISPs. POPs may have servers that terminate HTTP sessions and relay data\nto datacenters using more efﬁcient protocols.\nOther architecture patterns are appropriate for non-web applications. Message\nbus architectures create a message-passing system that decouples communication\nfrom the services that need the information. Service-oriented architectures involve\nmany small services that cooperate to create larger services. Each service in an SOA\ncan be independently scaled, upgraded, and even replaced.\nChapter 1 discussed composition, server trees, and other patterns. These\nrudimentary elements make up many of the patterns discussed in this chapter.\nChapter 5 will discuss patterns for data storage; in particular, Section 5.5 will\ndescribe the distributed hash table.\nAll of these architecture patterns have tradeoffs on cost, scalability, and\nresiliency to failure. Understanding these tradeoffs is key to knowing when to use\neach pattern.\nExercises\n1. Describe the single-machine, three-tier, and four-tier web application architec-\ntures.\n2. Describe how a single-machine web server, which uses a database to generate\ncontent, might evolve to a three-tier web server. How would this be done with\nminimal downtime?\n3. Describe the common web service architectures, in order from smallest to\nlargest.\n4. Describe how different local load balancer types work and what their pros and\ncons are. You may choose to make a comparison chart.\n5. What is “shared state” and how is it maintained between replicas?\n6. What are the services that a four-tier architecture provides in the ﬁrst tier?\n7. What does a reverse proxy do? When is it needed?\n8. Suppose you wanted to build a simple image-sharing web site. How would\nyou design it if the site was intended to serve people in one region of the\nworld? How would you then expand it to work globally?\n9. What is a message bus architecture and how might one be used?\n10. What is an SOA?\n11. Why are SOAs loosely coupled?\n12. How would you design an email system as an SOA?\n13. Who was Christopher Alexander and what was his contribution to\narchitecture?\n",
      "content_length": 2116,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Chapter 5\nDesign Patterns for Scaling\nThe only real problem is scaling.\nEverything else is a sub-problem.\n—O’Dell’s Axiom\nA system’s ability to scale is its ability to process a growing workload, usually\nmeasured in transactions per second, amount of data, or number of users. There\nis a limit to how far a system can scale before reengineering is required to permit\nadditional growth.\nDistributed systems must be built to be scalable from the start because growth\nis expected. Whether you are building a web-based service or a batch-processing\ndata analytics platform, the goal is always to be successful, which usually means\nattracting more users, uses, or data.\nMaking sure a service is fast and stays fast is critical. If your service does not\nscale, or if it gets too slow as it becomes more popular, users will go elsewhere.\nGoogle found that an artiﬁcial 400-ms delay inserted into its search results would\nmake users conduct 0.2 to 0.6 percent fewer searches. This could translate into\nmillions or billions of dollars of lost revenue.\nIronically, a slow web service is more frustrating than one that is down. If a\nsite is down, users understand that fact immediately and can go to a competing\nsite or ﬁnd something else to do until it comes back up. If it is slow, the experience\nis just painful and frustrating.\nBuilding a scalable system does not happen by accident. A distributed system\nis not automatically scalable. The initial design must be engineered to scale to meet\nthe requirements of the service, but it also must include features that create options\nfor future growth. Once the system is in operation, we will always be optimizing\nthe system to help it scale better.\n95\n",
      "content_length": 1691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "96\nChapter 5\nDesign Patterns for Scaling\nIn previous chapters we’ve discussed many techniques that enable distributed\nsystems to grow to extreme sizes. In this chapter we will revisit these techniques in\ngreater detail. We will review terminology related to scaling, examine the theory\nbehind scaling techniques, and describe speciﬁc techniques used to scale. Math-\nematical terms used to describe how systems perform and scale can be found in\nAppendix C.\n5.1 General Strategy\nThe basic strategy for building a scalable system is to design it with scalability\nin mind from the start and to avoid design elements that will prevent additional\nscaling in the future.\nThe initial requirements should include approximations of the desired scale:\nthe size of data being stored, the throughput of the systems that process it, the\namount of trafﬁc the service currently receives, and expected growth rates. All\nof these factors then guide the design. This process was described previously in\nSection 1.7.\nOnce the system is running, performance limits will be discovered. This is\nwhere the design features that enable further scaling come into play.\nWhile every effort is made to foresee potential scaling issues, not all of them\ncan receive engineering attention. The additional design and coding effort that will\nhelp deal with future potential scaling issues is lower priority than writing code\nto ﬁx the immediate issues of the day. Spending too much time preventing scal-\ning problems that may or may not happen is called premature optimization and\nshould be avoided.\n5.1.1 Identify Bottlenecks\nA bottleneck is a point in the system where congestion occurs. It is a point that is\nresource starved in a way that limits performance. Every system has a bottleneck.\nIf a system is underperforming, the bottleneck can be ﬁxed to permit the system to\nperform better. If the system is performing well, knowing the location of the bot-\ntleneck can be useful because it enables us to predict and prevent future problems.\nIn this case the bottleneck can be found by generating additional load, possibly in\na test environment, to see at which point performance suffers.\nDeciding what to scale is a matter of ﬁnding the bottleneck in the system and\neliminating it. The bottleneck is where a backlog of work accumulates. Optimiza-\ntions done to the process upstream of the bottleneck simply make the backlog grow\nfaster. Optimizations made downstream of the bottleneck may improve the efﬁ-\nciency of that part but do not improve the total throughput of the system. Therefore\nany effort not spent focused on the bottleneck is wasteful.\n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "5.1\nGeneral Strategy\n97\n5.1.2 Reengineer Components\nSome scaling issues can be resolved through adjustments to the current system.\nFor example, enlarging a cache might be as simple as adjusting a conﬁguration\nﬁle. Other scaling issues require engineering effort.\nRewriting parts of a system is called reengineering and is usually done to\nimprove speed, functionality, or resource consumption. Sometimes reengineering\nis difﬁcult because of earlier design decisions that led to particular code or design\nstructures. It is often best to ﬁrst replace such code with code that is functionally\nequivalent but has an internal organization that makes other reengineering efforts\neasier to accomplish. Restructuring an existing body of code—namely, altering its\ninternal structure without changing its external behavior—is called refactoring.\n5.1.3 Measure Results\nScaling solutions must be evaluated using evidence, meaning data collected from a\nreal system. Take measurements, try a solution, and repeat the same measurements\nto see the effect. If the effect is minimal or negative, the solution is not deployed.\nFor example, if performance is slow and measurements indicate that a cache\nhit rate is very low, the cache is probably too small. In such a case, we would mea-\nsure performance, resize the cache, and then measure performance again. While\nthe cache may be performing better with such an adjustment, the overall effect on\nthe system may not be a signiﬁcant improvement or may not be big enough to\njustify the cost of the additional RAM required.\nIf we do not measure before and after a change is made, we cannot be sure\nwhether our changes were actually effective. Making changes without measure-\nment would be system administration by luck at best, and by ego at worst. It is\noften tempting to rush ahead with a solution and measure only after the change\nis made. This is as bad as not measuring at all, because there is no baseline for\ncomparison.\nWhile past experience should inform and guide us, we must resist the temp-\ntation to skip the scientiﬁc process of using measurement and analysis to guide\nour decisions. Distributed systems are always too large for any one person to “just\nknow” the right thing to do. A hunch or guess by an experienced system admin-\nistrator should be trumped by the recommendation of a more junior person who\nhas taken the time to perform scientiﬁc analysis: set up data collection mechanisms,\ntake measurements, verify a hypothesis about what is wrong, test a theory of what\nmight ﬁx it, and analyze the results.\n5.1.4 Be Proactive\nThe best time to ﬁx a bottleneck is before it becomes a problem. Ideally, the ﬁx will\narrive just in time, immediately before this point is reached. If we ﬁx a problem\n",
      "content_length": 2741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "98\nChapter 5\nDesign Patterns for Scaling\ntoo soon, we may be wasting effort on a problem that never arises—effort that\ncould have been better spent elsewhere. If we begin to design and implement a\nsolution too late, the problem will arise before the solution is deployed. If we wait\nmuch too long, the problem will surprise us, catching us off guard, and we will be\n“ﬁreﬁghting” to ﬁnd a solution. Engineering takes time and doing it in a rushed\nfashion leads to mistakes and more problems. We want the Goldilocks solution:\nnot too early, not too late, just right.\nEvery system has a bottleneck or constraint. This is not a bad thing. Constraints\nare inherent in all systems. A constraint dictates the rate at which processing can\nhappen, or how much work can ﬂow through the process. If the current rate is\nsufﬁcient, the bottleneck is not a problem. In other words, the constraint becomes\na problem only if it actually hampers the system’s ability to achieve its goal.\nThe best strategy for scaling a running system, then, is to predict problems far\nenough in advance that there is enough time to engineer a proper solution. This\nmeans one should always be collecting enough measurements to be aware of where\nthe bottlenecks are. These measurements should be analyzed so that the point at\nwhich the bottleneck will become a problem can be predicted. For example, simply\nmeasuring how much internet bandwidth is being consumed and graphing it can\nmake it easy to predict when the capacity of your internet link will be exhausted.\nIf it takes 6 weeks to order more bandwidth, then you need to be able to order that\nbandwidth at least 6 weeks ahead of time, and preferably 12 weeks ahead of time\nto permit one failed attempt and do-over.\nSome solutions can be implemented quickly, by adjusting a conﬁguration set-\nting. Others require weeks or months of engineering effort to solve, often involving\nrewriting or replacing major systems.\n5.2 Scaling Up\nThe simplest methodology for scaling a system is to use bigger, faster equipment.\nA system that runs too slowly can be moved to a machine with a faster CPU, more\nCPUs, more RAM, faster disks, faster network interfaces, and so on. Often an exist-\ning computer can have one of those attributes improved without replacing the\nentire machine. This is called scaling up because the system is increasing in size.\nWhen this solution works well, it is often the easiest solution because it does\nnot require a redesign of the software. However, there are many problems with\nscaling this way.\nFirst, there are limits to system size. The fastest, largest, most powerful com-\nputer available may not be sufﬁcient for the task at hand. No one computer can\nstore the entire corpus of a web search engine or has the CPU power to process\npetabyte-scale datasets or respond to millions of HTTP queries per second. There\nare limits as to what is available on the market today.\n",
      "content_length": 2910,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "5.3\nThe AKF Scaling Cube\n99\nSecond, this approach is not economical. A machine that is twice as fast costs\nmore than twice as much. Such machines are not sold very often and, therefore, are\nnot mass produced. You pay a premium when buying the latest CPU, disk drives,\nand other components.\nFinally, and most importantly, scaling up simply won’t work in all situations.\nBuying a faster, more powerful machine without changing the design of the soft-\nware being used usually won’t result in proportionally faster throughput. Software\nthat is single-threaded will not run faster on a machine with multiple processors.\nSoftware that is written to spread across all processors may not see much perfor-\nmance improvement beyond a certain number of CPUs due to bottlenecks such as\nlock contention.\nLikewise, improving the performance of any one component is not guaran-\nteed to improve overall system performance. A faster network connection will not\nimprove throughput, for example, if the protocol performs badly when latency\nis high.\nAppendix B goes into more detail about these issues and the history of how\nsuch problems led to the invention of distributed computing.\n5.3 The AKF Scaling Cube\nMethodologies for scaling to massive proportions boil down to three basic options:\nreplicate the entire system (horizontal duplication); split the system into individual\nfunctions, services, or resources (functional or service splits); and split the system\ninto individual chunks (lookup or formulaic splits).\nFigure 5.1, the AKF Scaling Cube, was developed by Abbott, Keeven, and\nFisher and conceptualizes these categories as x-, y-, and z-axes (Abbott & Fisher\n2009).\n5.3.1 x: Horizontal Duplication\nHorizontal duplication increases throughput by replicating the service. It is also\nknown as horizontal scaling or scaling out.\nThis kind of replication has been discussed in past chapters. For example, the\ntechnique of using many replicas of a web server behind a load balancer is an\nexample of horizontal scaling.\nA group of shared resources is called a resource pool. When adding resources\nto a pool, it is necessary for each replica to be able to handle the same transactions,\nresulting in the same or equivalent results.\nThe x-axis does not scale well with increases in data or with complex transac-\ntions that require special handling. If each transaction can be completed indepen-\ndently on all replicas, then the performance improvement can be proportional to\nthe number of replicas. There is no loss of efﬁciency at scale.\n",
      "content_length": 2524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "100\nChapter 5\nDesign Patterns for Scaling\nFigure 5.1: The AKF Scaling Cube. Trademark AKF Partners. Reprinted with\npermission from Scalability Rules: 50 Principles for Scaling Web Sites.\n.\nRecommended Books on Scalability\nThe Art of Scalability: Scalable Web Architecture, Processes, and Organizations\nfor the Modern Enterprise by Abbott and Fisher (2009) is an extensive cat-\nalog of techniques and discussion of scalability of people, processes, and\ntechnologies.\nScalability Rules: 50 Principles for Scaling Web Sites, also by Abbott and\nFisher (2011), is a slimmer volume, focused on technical strategy and tech-\nniques.\nWhen the transactions require replicas to communicate, the scaling is less efﬁ-\ncient. For example, transactions that write new data that must be communicated\nto all replicas may require all replicas to hold off on any future transactions related\nto the update until all replicas have received the change. This is related to the CAP\nPrinciple (discussed in Section 1.5).\n",
      "content_length": 996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "5.3\nThe AKF Scaling Cube\n101\nTechniques that involve x-axis scaling include the following:\n• Adding more machines or replicas\n• Adding more disk spindles\n• Adding more network connections\n5.3.2 y: Functional or Service Splits\nA functional or service split means scaling a system by splitting out each individ-\nual function so that it can be allocated additional resources.\nAn example of this was discussed in Section 4.1, where we had a single\nmachine that was used for a web server, a database, and an application server\n(dynamic content generation). The three functions all compete for resources such\nas disk buffer cache, CPU, and the bandwidth to the disk, memory, and memory\nsubsystems. By moving the three major functions to separate machines, each is able\nto perform better because it has dedicated resources.\nSeparating the functions requires making them less tightly coupled to each\nother. When they are loosely coupled, it becomes easier to scale each one inde-\npendently. For example, we could apply x-axis scaling techniques to a single\nsubsystem. Scaling individual parts has advantages. It is less complicated to repli-\ncate a small part rather than an entire system. It is also often less expensive to\nreplicate one part that needs more capacity than the entire system, much of which\nmay be performing adequately at the current scale.\nIn addition to splitting along subsystem boundaries, y-axis scaling may\ninvolve splitting workﬂows or transaction types.\nPerhaps some category of transactions might be better handled as a special\ncase rather than being treated the same way as all the other requests. This type of\ntransaction might be split off to be processed by a dedicated pool of machines.\nFor example, it is expensive to engineer a system that has very low latency for\nall requests. If all trafﬁc is placed in the same bucket, you need far more hardware\nto keep latencies low for the few requests that care. A better alternative might be\nto separate requests that come from batch processing systems versus interactive\nservices. The latter can be processed by machines that are less oversubscribed or\nare on networks with different quality of service (QoS) settings.\nOne can also mark special customers for special treatment. One ﬁnancial ser-\nvices web site sets a cookie if a user invests multiple millions of dollars with the\nﬁrm. The web load balancer detects the cookie and sends its trafﬁc to a pool of\nservers that are dedicated to very important customers.\nAlternatively, there may be an infrequent type of transaction that is partic-\nularly burdensome, such that moving it to its own pool would prevent it from\n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "102\nChapter 5\nDesign Patterns for Scaling\noverloading the general transactions. For example, a particular query might nega-\ntively affect the cache infrastructure. Such queries might be directed to a separate\nset of machines that uses a different cache algorithm.\nTechniques that involve y-axis scaling include the following:\n• Splitting by function, with each function on its own machine\n• Splitting by function, with each function on its own pool of machines\n• Splitting by transaction type\n• Splitting by type of user\n.\nCase Study: Separating Traffic Types at ServerFault.com\nServerFault.com is a question-and-answer web site for system administrators.\nWhen displaying a question (and its answers) to a logged-in user, the page\nis augmented and customized for the particular person. Anonymous users\n(users who are not logged in) all see the same generic page.\nTo scale the system on the y-axis, the two ways to generate the same page\nwere split out. The anonymous pages are now handled by a different system\nthat generates the HTML once and caches it for future requests. Since the vast\nmajority of queries are from anonymous users, this division greatly improved\nperformance.\nAnonymous page views are subdivided one additional way. Search\nengines such as Google and Bing crawl every page at Serverfault.com look-\ning for new content. Since this crawling hits every page, it might potentially\noverload the service, due to both the volume of requests and the fact that hit-\nting every page in order exhausts the cache. Both factors make performance\nsuffer for other users. Therefore requests from web crawlers are sent to a ded-\nicated pool of replicas. These replicas are conﬁgured not to cache the HTML\npages that are generated. Because the pool is separate, if the crawlers overload\nit, regular users will not be affected.\n5.3.3 z: Lookup-Oriented Split\nA lookup-oriented split scales a system by splitting the data into identiﬁable seg-\nments, each of which is given dedicated resources. z-axis scaling is similar to y-axis\nscaling except that it divides the data instead of the processing.\nA simple example of this is to divide, or segment, a database by date. If the\ndatabase is an accumulation of data, such as log data, one can start a new database\n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "5.3\nThe AKF Scaling Cube\n103\nserver every time the current one ﬁlls up. There may be a database for 2013 data,\n2014 data, and so on. Queries that involve a single year go to the appropriate\ndatabase server. Queries that span years are sent to all the appropriate database\nservers and the responses are combined. If a particular year’s database is accessed\nso often that it becomes overloaded, it can be scaled using the x-axis technique of\nreplication. Since no new data is written to past years’ servers, most servers can be\nsimple read-only replicas.\n.\nCase Study: Twitter’s Early Database Architecture\nWhen Twitter was very new, the history of all Tweets ﬁt on a single database\nserver running MySQL. When that server ﬁlled up, Twitter started a new\ndatabase server and modiﬁed its software to handle the fact that its data was\nnow segmented by date.\nAs Twitter became more popular, the amount of time between a new\nsegment being started and that new database ﬁlling up decreased rapidly. It\nbecame a race for the operations team to keep up with demand. This solu-\ntion was not sustainable. Load was unbalanced, as older machines didn’t get\nmuch trafﬁc. This solution was also expensive, as each machine required many\nreplicas. It was logistically complex as well.\nEventually, Twitter moved to a home-grown database system called\nT-bird, based on Gizzard, which smoothly scales automatically.\nAnother way to segment data is by geography. In a global service it is common\npractice to set up many individual data stores around the world. Each user’s data\nis kept on the nearest store. This approach also gives users faster access to their\ndata because it is stored closer to them.\nGoing from an unsegmented database to a segmented one may require con-\nsiderable refactoring of application code. Thus scaling on the z-axis is often\nundertaken only when scaling using the x- and y-axes is exhausted.\nAdditional ways to segment data include the following:\n• By Hash Prefix: This is known as sharding and is discussed later.\n• By Customer Functionality: For example, eBay segments by product—cars,\nelectronics, and so on.\n• By Utilization: Putting high-use users in dedicated segments.\n• By Organizational Division: For example, sales, engineering, business devel-\nopment.\n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "104\nChapter 5\nDesign Patterns for Scaling\n• Hierarchically: The segments are kept in a hierarchy. DNS uses this pattern,\nlooking up an address like www.everythingsysadmin.com ﬁrst in the root\nservers, then in the servers for com, and ﬁnally in the servers for the domain\neverythingsysadmin.\n• By Arbitrary Group: If a cluster of machines can reliably scale to 50,000 users,\nthen start a new cluster for each 50,000 users. Email services often use this\nstrategy.\n5.3.4 Combinations\nMany scaling techniques combine multiple axes of the AKF Scaling Cube. Some\nexamples include the following:\n• Segment plus Replicas: Segments that are being accessed more frequently can\nbe replicated at a greater depth. This enables scaling to larger datasets (more\nsegments) and better performance (more replicas of a segment).\n• Dynamic Replicas: Replicas are added and removed dynamically to achieve\nrequired performance. If latency is too high, add replicas. If utilization is too\nlow, remove replicas.\n• Architectural Change: Replicas are moved to faster or slower technology\nbased on need. Infrequently accessed shards are moved to slower, less expen-\nsive technology such as disk. Shards in higher demand are moved to faster\ntechnology such as solid-state drives (SSD). Extremely old segments might be\narchived to tape or optical disk.\n5.4 Caching\nA cache is a small data store using fast/expensive media, intended to improve a\nslow/cheap bigger data store. For example, recent database queries may be stored\nin RAM so that if the same query is repeated, the disk access can be avoided.\nCaching is a distinct pattern all its own, considered an optimization of the z-axis\nof the AKF Scaling Cube.\nConsider lookups in a very large data table. If the table was stored in RAM,\nlookups could be very fast. Assume the data table is larger than will ﬁt in RAM,\nso it is stored on disk. Lookups on the disk are slow. To improve performance, we\nallocate a certain amount of RAM and use it as a cache. Now when we do a lookup,\nﬁrst we check whether the result can be found in the cache. If it is, the result is used.\nThis is called a cache hit. If it is not found, the normal lookup is done from the disk.\nThis is called a cache miss. The result is returned as normal and in addition is stored\nin the cache so that future duplicate requests will be faster.\n",
      "content_length": 2335,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "5.4\nCaching\n105\nFigure 1.10 lists performance comparisons useful for estimating the speed of\na cache hit and miss. For example, if your database is in Netherlands and you are\nin California, a disk-based cache is faster if it requires fewer than 10 seeks and two\nor three 1MB disk reads. In contrast, if your database queries are within the same\ndatacenter, your cache needs to be signiﬁcantly faster, such as RAM or a cache\nserver on the same subnet.\n5.4.1 Cache Effectiveness\nThe effectiveness of a cache is measured by the cache hit ratio, sometimes called a\ncache hit rate. It is the ratio of the number of cache hits over the total number of\nlookups. For example, if 500 lookups are performed and 100 were serviced from\nthe cache, the cache hit ratio would be 1/5 or 20 percent.\nPerformance\nA cache is a net beneﬁt in performance if the time saved during cache hits exceeds\nthe time lost from the additional overhead. We can estimate this using weighted\naverages. If the typical time for a regular lookup is L, a cache hit is H, a cache\nmiss is M, and the cache hit ratio is R, then using the cache is more effective if\nH × R + M × (1 −R) < L.\nWhen doing engineering estimates we can simplify the formula if cache\nlookups and updates are extremely fast or nearly zero. In that case we can assume\nthe performance beneﬁt will be a function of the cache hit ratio. For example, sup-\npose a typical lookup took 6 seconds and we predict a cache hit rate of 33 percent.\nWe know that in a perfect world with an instant, zero-overhead cache, the average\nlookup would fall by 33 percent to 4 seconds. This gives us a best-case scenario,\nwhich is useful for planning purposes. Realistically, performance will improve\nslightly less than this ideal.\nCost-Effectiveness\nA cache is cost-effective only if the beneﬁt from the cache is greater than the cost of\nimplementing the cache. Recall that accessing RAM is faster than accessing disk,\nbut much more expensive. The performance and costs of the items in Figure 1.10\nwill inevitably change over time. We recommend building your own chart based\non the performance and costs in your speciﬁc environment.\nSuppose a system without caching required 20 replicas, but with caching\nrequired only 15. If each replica is a machine, this means the cache is more\ncost-effective if it costs less than purchasing 5 machines.\nPurchase price is not always the only consideration. If being able to provide a\nfaster service will improve sales by 20 percent, then cost should be weighed against\nthat improvement.\n",
      "content_length": 2536,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "106\nChapter 5\nDesign Patterns for Scaling\n5.4.2 Cache Placement\nCaching can be local, in which case the software is performing its own caching,\nsaving itself the trouble of requesting a lookup for each cache hit (Figure 5.2a).\nCaching can be external to the application, with a cache placed between the\nserver and the external resources. For example, a web cache sits between a web\nbrowser and the web server, intercepting requests and caching them when possible\n(Figure 5.2b). Caches can also be at the server side. For example, a server that pro-\nvides an API for looking up certain information might maintain its own cache that\nservices requests when possible (Figure 5.2c). If there are multiple caches, some\nmight potentially be outdated. The CAP Principle, described in Section 1.5, then\napplies.\nNot all caches are found in RAM. The cache medium simply must be faster\nthan the main medium. A disk can cache for data that has to be gathered from a\nremote server because disks are generally faster than remote retrieval. For example,\nYouTube videos are cached in many servers around the world to conserve internet\nbandwidth. Very fast RAM can cache for normal RAM. For example, the L1 cache\nof a CPU caches the computer’s main memory. Caches are not just used to improve\ndata lookups. For example, calculations can be cached. A function that does a dif-\nﬁcult mathematical calculation might cache recent calculation results if they are\nlikely to be requested again.\n5.4.3 Cache Persistence\nWhen a system starts, the cache is usually empty, or cold. The cache hit ratio will\nbe very low and performance will remain slow until enough queries have warmed\nFigure 5.2: Cache placement\n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "5.4\nCaching\n107\nthe cache. Some caches are persistent, meaning they survive restarts. For example,\na cache stored on disk is persistent across restarts. RAM is not persistent and is lost\nbetween restarts. If a system has a cache that is slow to warm up and is stored in\nRAM, it may be beneﬁcial to save the cache to disk before a shutdown and read it\nback in on startup.\n.\nCase Study: Saving a RAM Cache to Disk\nStack Exchange’s web sites depend on a database that is heavily cached in\nRAM by Redis. If Redis is restarted, its performance will be unacceptably slow\nfor 10 to 15 minutes while the cache warms. Redis has many features that pre-\nvent this problem, including the ability to snapshot the data held in RAM to\ndisk, or to use a second Redis server to store a copy of the cache.\n5.4.4 Cache Replacement Algorithms\nWhen a cache miss is processed, the data gathered by the regular lookup is added\nto the cache. If the cache is full some data must be thrown away to make room.\nThere are many different replacement algorithms available to handle the cache\nmanipulation.\nIn general, better algorithms keep track of more usage information to improve\nthe cache hit ratio. Different algorithms work best for different data access patterns.\nThe Least Recently Used (LRU) algorithm tracks when each cache entry was\nused and discards the least recently accessed entry. It works well for access patterns\nwhere queries are repeated often within a small time period. For example, a DNS\nserver might use this algorithm: if a domain has not been accessed in a long time,\nchances are it won’t be accessed again. Typos, for example, rarely repeat and will\neventually expire from the cache.\nThe Least Frequently Used (LFU) algorithm counts the number of times a\ncache entry is accessed and discards the least active entries. It may track total\naccesses, or keep an hourly or daily count. This algorithm is a good choice when\nmore popular data tends to be accessed the most. For example, a video service\nmight cache certain popular videos that are viewed often while other videos are\nviewed once and rarely ever rewatched.\nNew algorithms are being invented all the time. Tom’s favorite algorithm,\nAdaptive Replacement Cache (ARC), was invented in 2003 (Megiddo & Modha\n2003). Most algorithms do not perform well with a sudden inﬂux of otherwise\nlittle-used data. For example, backing up a database involves reading every record\none at a time and leaves the cache ﬁlled with otherwise little-used data. At that\npoint, the cache is cold, so performance suffers. ARC solves this problem by putting\nnewly cached data in a probationary state. If it is accessed a second time, it gets out\n",
      "content_length": 2672,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "108\nChapter 5\nDesign Patterns for Scaling\nof probation and is put into the main cache. A single pass through the database\nﬂushes the probationary cache, not the main cache.\nMany other algorithms exist, but most are variations on the LRU, LFU, and\nARC options.\n5.4.5 Cache Entry Invalidation\nWhen data in the primary storage location changes, any related cache entries\nbecome obsolete. There are many ways to deal with this situation.\nOne method is to ignore it. If the primary storage does not change, the cache\nentries do not become obsolete. In other cases, the cache is very small and obsolete\nentries will be eventually be replaced via the cache replacement algorithm. If the\nsystem can tolerate occasional outdated information, this may be sufﬁcient. Such\nsituations are rather rare.\nAnother method is to invalidate the entire cache anytime the database\nchanges. The beneﬁt of this method is that it is very simple to implement. It leaves\nthe cache cold, however, and performance suffers until it warms again. This is\nacceptable in applications where the cache warms quickly.\nThe main storage may communicate to the cache the need to invalidate an\nentry whenever the data in the cache has changed. This may create a lot of work if\nthere are many caches and updates occur frequently. This is also an example of how\nthe CAP Principle, described in Section 1.5, comes into play: for perfect consistency,\nprocessing of queries involving the updated entry must pause until all caches have\nbeen notiﬁed.\nSome methods eliminate the need for the main storage to communicate\ndirectly to the caches. The cache can, for example, assume a cache entry is valid\nonly for a certain number of seconds. The cache can record a timestamp on each\nentry when it is created and expire entries after a certain amount of time.\nAlternatively, the server can help by including how long an entry may be\ncached when it answers a query. For example, suppose a DNS server responds\nto all queries with not just the answer, but also how many seconds each entry in\nthe answer may be cached. This is called the time to live (TTL) value. Likewise, an\nHTTP server can annotate a response with an expiration date, an indication of how\nlong the item can be cached. If you do not control the client software, you cannot be\nassured that clients will abide by such instructions. Many ISPs cache DNS entries\nfor 24 hours at a minimum, much to the frustration of those using DNS as part of\na global load balancer. Many web browsers are guilty of ignoring expiration dates\nsent by HTTP servers.\nLastly, the cache can poll the server to see if a local cache should be invali-\ndated. For example, an HTTP client can query the server about whether an item\nhas changed to ﬁnd out if an item in its cache is fresh. This method works when the\nquery for freshness can be processed signiﬁcantly faster than the full answer is sent.\n",
      "content_length": 2885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "5.4\nCaching\n109\n5.4.6 Cache Size\nPicking the right cache size is difﬁcult but important. If it is too small, perfor-\nmance will suffer, which may potentially be worse than having no cache at all. If it\nis too large, we have wasted money when a smaller cache would do. If we have\nthousands of machines, a few dollars wasted on each machine adds up quickly.\nCaches are usually a ﬁxed size. This size usually consists of a ﬁxed number\nof cache entries or a ﬁxed amount of total storage, in which case the number of\npotential entries is adjusted accordingly.\nThere are several approaches to selecting a cache size. The most accurate way\nto determine the correct cache size for a given situation is to take measurements\non a running system. Caches are complex, especially due to interactions between\nreplacement algorithms and cache size. Measuring a running system involves all\nthese factors, but may not always be possible.\nOne approach is to run the system with a variety of cache sizes, measuring\nperformance achieved by each one. Benchmarks like this can be done with real or\nsimulated data on a separate system set up for benchmark tests. Such operations\ncan be done on live systems for the most accurate measurements, though this may\nbe risky. One way to limit such risks is to adjust cache sizes on just one replica of\nmany. Negative effects are then less likely to be noticed.\nAnother approach is to conﬁgure a system with a larger cache size than the\napplication could possibly need. If the cache entries expire fast enough, the cache\nwill grow to a certain size and then stop growing. This gives an upper bound of\nhow big the cache needs to be. If the cache size does not stabilize, eventually the\nreplacement algorithm will kick in. Take a snapshot of the age of all the entries.\nReview the snapshot to see which entires are hot (frequently being used) and cold\n(less frequently used). Often you will see a pattern surface. For example, you may\nsee that 80 percent of the cache entries have been used recently and the rest are\nsigniﬁcantly older. The 80 percent size represents approximately the amount of\ncache that is contributing to improving performance.\nAnother approach is to estimate the cache hit ratio. If we are considering\nadding a cache to a running system, we can collect logs of data and estimate what\nthe cache hit ratio will be. For example, we could collect 24 hours’ worth of query\nlogs and count duplicate requests. The ratio of duplicates to total queries is a good\npredictor of what the cache hit ratio will be be if caching is added. This assumes\nan inﬁnite cache with no expiration. If even under these theoretically perfect con-\nditions the cache hit ratio is low, we know that a cache will not help. However, if\nthere are duplicate queries, the cumulative size of the responses to those queries\nwill give a good estimate for sizing the cache.\nThe problem with measurements from live or benchmark systems is that they\nrequire the system to exist. When designing a system it is important to be able to\nmake a reasonable prediction of what cache size will be required.\n",
      "content_length": 3098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "110\nChapter 5\nDesign Patterns for Scaling\nWe can improve our estimate by using a cache simulator. These tools can be\nused to provide “what if” analysis to determine the minimum cache size.\nOnce the cache is in place, the cache hit ratio should be monitored. The\ncache size can be reevaluated periodically, increasing it to improve performance\nas needed.\n5.5 Data Sharding\nSharding is a way to segment a database (z-axis) that is ﬂexible, scalable, and\nresilient. It divides the database based on the hash value of the database keys.\nA hash function is an algorithm that maps data of varying lengths to a\nﬁxed-length value. The result is considered probabilistically unique. For exam-\nple, the MD5 algorithm returns a 128-bit number for any input. Because there\nare 340,282,366,920,938,463,463,374,607,431,768,211,456 possible combinations, the\nchance of two inputs producing the same hash is very small. Even a small\nchange in the data creates a big change in the hash. The MD5 hash of\n“Jennifer” is e1f6a14cd07069692017b53a8ae881f6 but the MD5 hash of “Gennifer”\nis 1e49bbe95b90646dca5c46a8d8368dab.\nTo divide a database into two shards, generate the hash of the key and store\nkeys with even hashes in one database and keys with odd hashes in the other\ndatabase. To divide a database into four shards, split the database based on the\nremainder of the key’s hash divided by 4 (i.e., the hash mod4). Since the remain-\nder will be 0, 1, 2, or 3, this will indicate which of the four shards will store that\nkey. Because the hash values are randomly distributed between the shards, each\nshard will store approximately the same number of keys automatically. This pat-\ntern is called a distributed hash table (DHT) since it distributes the data over many\nmachines, and uses hashes to determine where the data is stored.\n.\nThe Power of 2\nWe use a power of 2 to optimize the hash-to-shard mapping process. When\nyou want the remainder of the hash when divided by 2n, you just need to look\nat the last n bits of the hash. This is a very fast operation, much faster than\ngetting the modulus using a number that is not a power of 2.\nShards can be replicated on multiple machines to improve performance. With\nsuch an approach, each replica processes a share of the queries destined for that\nshard. Replication can also provide better availability. If multiple machines store\nany shard, then any machine can crash or be taken down for maintenance and the\nother replicas will continue to service the requests.\n",
      "content_length": 2495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "5.5\nData Sharding\n111\nAs more data are stored, the shards may outgrow the machine. The database\ncan then be split over twice as many shards. The number of shards doubles, and the\nold data is divided among the new shards. Each key’s hash is evaluated to deter-\nmine if the key should stay in the current shard or if it belongs to the new shard.\nSystems that perform this step while live have complex algorithms to manage\nqueries received during the expansion.\nIt is rather inﬂexible to require that the number of segments be a power of 2.\nFor example, going from one shard to two requires adding just one machine, which\nis easy to purchase. However, as the system grows, you may ﬁnd yourself needing\nto go from 32 machines to 64 machines, which is quite a large purchase. The next\njump is twice as big. If the new machines are more powerful, this extra capacity will\nbe wasted until all the smaller machines are eliminated. Also, while the number of\nkeys is evenly divided between shards, each key may not store exactly the same\namount of data. Thus a machine may have one fourth of the keys, but more data\nthan can ﬁt on the machine. One must increase the number of shards based on the\nlargest shard, which could be considerably bigger than the smallest.\nThe solution to these problems is to create more, smaller shards and store\nmultiple shards on each machine. Now you can vary the number of shards on a\nmachine to compensate for uneven shard sizes and different machine sizes. For\nexample, you could divide the hash value by a larger power of 2 and produce, for\nexample, 8192 buckets. Divide those buckets across as many machines as needed.\nFor example, if there are three machines, one twice as large as the other two, the\nlarger machine might store keys that fall into bucket 0...4095 (4096 total) in the\nlarger server and store buckets 4096...6143 and 6144...8191 (2048 keys each) in\nthe second and third machines, respectively.\nAs new, more powerful hardware becomes available, one can pack more\nshards on a machine. More shards mean more queries will be directed to that\nmachine. Thus, more CPU and network bandwidth is required. It is possible that\nwhen the machine stores the maximum number of shards, the CPU or network\nbandwidth will be exhausted and performance suffer. New hardware models\nshould be benchmarked to determine the usable capacity before being put into\nservice. Ideally CPU, shard storage, and network bandwidth will all top out at the\nsame time.\nIf shards are used for a read/write database, each write updates the appro-\npriate shards. Replicas must be kept up-to-date, abiding by the CAP Principle\ndiscussed in Chapter 1.\nIn reality, shards are often used to distribute a read-only corpus of information.\nFor example, a search engine collects data and indexes it, producing shards of a\nread-only database. These then need to be distributed to each search cluster replica.\nDistribution can be rather complex. Transmitting an updated shard can take a long\ntime. If it is copied over the old data, the server cannot respond to requests on that\n",
      "content_length": 3071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "112\nChapter 5\nDesign Patterns for Scaling\nshard until the update is complete. If the transfer fails for some reason, the shard\nwill be unusable. Alternatively, one could set aside storage space for temporary use\nby shards as they are transmitted. Of course, this means there is unused space when\nupdates are not being done. To eliminate this waste, one can stop advertising that\nthe shard is on this machine so that the replicas will process any requests instead.\nNow it can be upgraded. The process of unadvertising until no new requests are\nreceived is called draining. One must be aware that while a shard is being drained,\nthere is one fewer replica—so performance may suffer. It is important to globally\ncoordinate shard upgrades so that enough replicas exist at any given moment to\nmaintain reliability and meet performance goals.\n5.6 Threading\nData can be processed in different ways to achieve better scale. Simply processing\none request at a time has its limits. Threading is a technique that can be used to\nimprove system throughput by processing many requests at the same time.\nThreading is a technique used by modern operating systems to allow\nsequences of instructions to execute independently. Threads are subsets of pro-\ncesses; it’s typically faster to switch operations among threads than among pro-\ncesses. We use threading to get a ﬁne granularity of control over processing for use\nin complex algorithms.\nIn a single-thread process, we receive a query, process it, send the result, and\nget the next query. This is simple and direct. A disadvantage is that a single long\nrequest will stall the requests behind it. It is like wanting to buy a pack of gum but\nbeing in line behind a person with a full shopping cart. In this so-called head of\nline blocking, the head of the line is blocked by a big request. The result is high\nlatency for requests that otherwise could be serviced quickly.\nA second disadvantage to single-threading is that in a ﬂood of requests, some\nrequests will be dropped. The kernel will queue up incoming connections while\nwaiting for the program to take the next one and process it. The kernel limits how\nmany waiting connections are permitted, so if there is a ﬂood of new connections,\nsome will be dropped.\nFinally, in a multi-core machine, the single thread will be bound to a single\nCPU, leaving the other cores idle. Multithreaded code can take advantage of all\ncores, thereby making maximum use of a machine.\nIn multithreading, a main thread receives new requests. For each request, it\ncreates a new thread, called a worker thread, to do the actual work and send the\nreply. Since thread creation is fast, the main thread can keep up with a ﬂood of\nnew requests and none will be dropped. Throughput is improved because requests\nare processed in parallel, multiple CPUs are utilized, and head of line blocking is\nreduced or eliminated.\n",
      "content_length": 2881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "5.7\nQueueing\n113\nThat said, multithreading is more difﬁcult to implement. If multiple threads\nhave to access the same resources in memory, locks or signaling ﬂags (semaphores)\nare needed to prevent resource collision. Locking is complex and error prone.\nThere are limits to the number of threads a machine can handle, based on\nRAM and CPU core limits. If any one core becomes overloaded, performance will\nquickly drop for that core. Connection ﬂoods can still cause dropped requests, but\nthe number of connections being handled is increased.\n5.7 Queueing\nAnother way that data can be processed differently to achieve better scale is called\nqueuing. A queue is a data structure that holds requests until the software is ready\nto process them. Most queues release elements in the order that they were received,\ncalled ﬁrst in, ﬁrst out (FIFO) processing.\nQueueing is similar to multithreading in that there is a master thread and\nworker threads. The master thread collects requests and places them in the queue.\nThere is usually a ﬁxed number of worker threads. Each one takes a request from\nthe queue, processes it, sends a reply, and then takes another item from the queue.\nThis workﬂow is called feeding from a queue.\n5.7.1 Benefits\nQueueing shares many of the advantages and disadvantages of multithreading. At\nthe same time, it has several advantages over basic multithreading.\nWith queueing, you are less likely to overload the machine since the num-\nber of worker threads is ﬁxed and remains constant. There is also an advantage\nin retaining the same threads to service multiple requests. This avoids the over-\nhead associated with new thread creation. Thread creation is lightweight, but on a\nmassive scale the overhead can add up.\nAnother beneﬁt of the queuing model is that it is easier to implement a prior-\nity scheme. High-priority requests can go to the head of the queue. A plethora of\nqueueing algorithms may be available depending on the type of priority scheme\nyou want to implement. In fair queueing, the algorithm prevents a low-priority\nitem from being “starved” by a ﬂood of high-priority items. Other algorithms\ndynamically shift priorities so that bursty or intermittent trafﬁc does not overload\nthe system or starve other priorities from being processed.\n5.7.2 Variations\nVariations of the queueing model can optimize performance. It is common to have\nthe ability to shrink or grow the number of threads. This may be automatic based on\n",
      "content_length": 2459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "114\nChapter 5\nDesign Patterns for Scaling\ndemand, or it may be manually controlled. Another variation is for threads to kill\nand re-create themselves periodically so that they remain “fresh.” This mitigates\nmemory leaks and other problems, but in doing so hides them and makes them\nmore difﬁcult to ﬁnd.\nFinally, it is common practice to use processes instead of threads. Process\ncreation can be expensive, but the ﬁxed population of worker processes means\nthat you pay that overhead once and then get the beneﬁt of using processes. Pro-\ncesses can do things that threads cannot, because they have their own address\nspace, memory, and open ﬁle tables. Processes are self-isolating, in that a process\nthat is corrupted cannot hurt other processes, whereas one ill-behaved thread can\nadversely affect other threads.\nAn example of queueing implemented with processes is the Prefork process-\ning module for the Apache web server. On startup, Apache forks off a certain\nnumber of subprocesses. Requests are distributed to subprocesses by a master pro-\ncess. Requests are processed faster because the subprocess already exists, which\nhides the long process creation time. Processes are conﬁgured to die and be\nrefreshed to every n requests so that memory leaks are averted. The number of\nsubprocesses used can be adjusted dynamically.\n5.8 Content Delivery Networks\nA content delivery network (CDN) is a web-acceleration service that delivers con-\ntent (web pages, images, video) more efﬁciently on behalf of your service. CDNs\ncache content on servers all over the world. Requests for content are serviced from\nthe cache nearest the user. Geolocation techniques are used to identify the network\nlocation of the requesting web browser.\nCDNs do not copy all content to all caches. Instead, they notice usage trends\nand determine where to cache certain content. For example, seeing a surge of use\nfrom Germany for a particular image, the CDN might copy all images for that cus-\ntomer to its servers in Germany. These images may displace cached images that\nhave not been accessed as recently.\nCDNs have extremely large, fast connections to the internet. They have more\nbandwidth to the internet than most web sites.\nCDNs often place their cache servers in the datacenters of ISPs, in arrange-\nments called colocation. As a result, the ISP-to-ISP trafﬁc is reduced. Considering\nthat ISPs charge each other for this trafﬁc, such a reduction can pay for itself\nquickly.\nTypically, an image in the middle of a web page might come from a URL on\nthe same server. However, this image rarely, if ever, changes. A web site that uses\na CDN would upload a copy of this image to the CDN, which then serves it from\n",
      "content_length": 2692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "5.8\nContent Delivery Networks\n115\na URL that points to the CDN’s servers. The web site then uses the CDN’s URL\nto refer to the image. When users load the web page, the image comes from the\nCDN’s servers. The CDN uses various techniques to deliver the image faster than\nthe web site could.\nUploading content to a CDN is automatic. Your web site serves the content\nas it normally does. A link to this content is called a native URL. To activate the\nCDN, you replace the native URLs in the HTML being generated with URLs that\npoint to the CDN’s servers. The URL encodes the native URL. If the CDN has the\ncontent cached already, it serves the content as one expects. If this is the ﬁrst time\nthat particular content is accessed, the CDN loads the content from the native URL,\ncaches it, and serves the content to the requester. The idea to encode the native URL\nin the CDN URL is quite smart; it means that there is no special step for uploading\nto the CDN that must be performed.\nBest practice is to use a ﬂag or software switch to determine whether native\nURLs or CDN URLs are output as your system generates web pages. Sometimes\nCDNs have problems and you will want to be able to switch back to native URLs\neasily. Sometimes the problem is not the CDN but rather a conﬁguration error that\nyou have caused. No amount of marketing material expounding the reliability of a\nCDN product will save you from this situation. Also, while a new web site is in the\ntesting phase, you may not want to use a CDN, especially if you are testing new,\nsecret features that should not be exposed to the world yet. Lastly, having such a\nswitch enables you to switch between CDN vendors easily.\nCDNs are great choices for small sites. Once the site becomes extremely large,\nhowever, it may be more cost-effective to run your own private CDN. Google ini-\ntially used a third-party CDN to improve performance and achieved an order of\nmagnitude better uptime than it could achieve when it was a young company. As\nGoogle grew, it established its own datacenter space all over the world. At that\npoint Google built its own private CDN, which permitted it to achieve another\norder of magnitude better uptime.\nCDNs didn’t appear until the late 1990s. At that time they focused on static\ncontent delivery for images and HTML ﬁles. The next generation of CDN products\nadded video hosting. In the past, ﬁle sizes were limited, but video hosting requires\nthe CDN to be able to handle larger content plus deal with protocols related to skip-\nping around within a video. The current generation of CDN products, therefore,\ncan act as a proxy. All requests go through the CDN, which acts as a middle man,\nperforming caching services, rewriting HTML to be more efﬁcient, and supporting\nother features.\nCDNs now compete on price, geographic coverage, and an ever-growing list\nof new features. Some CDNs specialize in a particular part of the world, either\noffering lower prices for web sites that have users only in their part of the world,\nor offering special services such as being licensed to serve content in China from\n",
      "content_length": 3087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "116\nChapter 5\nDesign Patterns for Scaling\ninside the Great Firewall of China (that is, assisting clients with the difﬁcult and\ncomplex censorship requirements placed on such trafﬁc).\nNew features include the ability to serve dynamic content, serve different con-\ntent to mobile devices, and provide security services. HTTPS (encrypted HTTP)\nservice can be complex and difﬁcult to administer. Some CDNs can process the\nHTTPS connections on your behalf, relieving you of managing such complex-\nity. The connection between your web servers and the CDN can then use an\neasier-to-manage transport mechanism, or no encryption at all.\n5.9 Summary\nMost approaches to scaling fall under one of the axes of the AKF Scaling Cube. The\nx-axis (horizontal scaling) is a power multiplier, cloning systems or increasing their\ncapacities to achieve greater performance. The y-axis (vertical scaling) scales by\nisolating transactions by their type or scope, such as using read-only database repli-\ncas for read queries and sequestering writes to the master database only. Finally,\nthe z-axis (lookup-based scaling) is about splitting data across servers so that the\nworkload is distributed according to data usage or physical geography.\nSharding scales large databases by putting horizontal partitions of your\ndatabase (rows) on multiple servers, gaining the advantage of smaller indices and\ndistributed queries. Replicating the shards onto additional servers produces speed\nand reliability beneﬁts, at the cost of data freshness.\nAnother optimization for data retrieval is a cache, a comparatively small data\nstore on fast and/or expensive media. A cache aggregates recently requested data,\nperforming updates on itself when data that isn’t in the cache is requested. Sub-\nsequent queries will then go directly to the cache and an overall improvement in\nperformance will be realized.\nThreading and queueing give us the tools to deal with ﬂoods of requests,\naggregating them into a structure that allows us to service them individually.\nContent delivery networks provide web-acceleration services, usually by caching\ncontent closer to the user.\nExercises\n1. What is scaling?\n2. What are the options for scaling a service that is CPU bound?\n3. What are the options for scaling a service whose storage requirements are\ngrowing?\n4. The data in Figure 1.10 is outdated because hardware tends to get less expen-\nsive every year. Update the chart for the current year. Which items changed\nthe least? Which changed the most?\n",
      "content_length": 2500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Exercises\n117\n5. Rewrite the data in Figure 1.10 in terms of proportion. If reading from main\nmemory took 1 second, how long would the other operations take? For extra\ncredit, draw your answer to resemble a calendar or the solar system.\n6. Take the data table in Figure 1.10 and add a column that identiﬁes the cost of\neach item. Scale the costs to the same unit—for example, the cost of 1 terabyte\nof RAM, 1 terabyte of disk, and 1 terabyte of L1 cache. Add another column\nthat shows the ratio of performance to cost.\n7. What is the theoretical model that describes the different kinds of scaling\ntechniques?\n8. How do you know when scaling is needed?\n9. What are the most common scaling techniques and how do they work? When\nare they most appropriate to use?\n10. Which scaling techniques also improve resiliency?\n11. Describe how your environment uses a CDN or research how it could be used.\n12. Research Amdahl’s Law and explain how it relates to the AKF Scaling Cube.\n",
      "content_length": 972,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Chapter 6\nDesign Patterns for Resiliency\nSuccess is not ﬁnal, failure is not\nfatal: it is the courage to continue\nthat counts.\n—Winston Churchill\nResiliency is a system’s ability to constructively deal with failures. A resilient\nsystem detects failure and routes around it. Nonresilient systems fall down when\nfaced with a malfunction. This chapter is about software-based resiliency and\ndocuments the most common techniques used.\nResiliency is important because no one goes to a web site that is down. Hard-\nware fails—that is a fact of life. You can buy the most reliable, expensive hardware\nin the world and there will be some amount of failures. In a sufﬁciently large\nsystem, a one in a million failure is a daily occurrence.\nDuring the ﬁrst year of a typical Google datacenter, there will be ﬁve rack-wide\noutages, three router failures large enough to require diverting processing away\nfrom connected machines, and eight network scheduled maintenance windows,\nhalf of which cause 30-minute random connectivity losses. At the same time 1 to\n5 percent of all disks will die and each machine will crash at least twice (2 to 4\npercent failure rate) (Dean 2009).\nGraceful degradation, discussed previously, means software is designed to\nsurvive failures or periods of high load by providing reduced functionality. For\nexample, a movie streaming service might automatically reduce video resolution\nto conserve bandwidth when some of its internet connections are down or oth-\nerwise overloaded. The other strategy is defense in depth, which means that all\nlayers of design detect and respond the failures. This includes failures as small as\na single process and as large as an entire datacenter.\nAn older, more traditional strategy for achieving reliability is to reduce the\nchance of failure at every place it can happen. Use the best servers and the best\nnetwork equipment, and put it in the most reliable datacenter: There will still\n119\n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "120\nChapter 6\nDesign Patterns for Resiliency\n.\nTerms to Know\nOutage: A user-visible lack of service.\nFailure: A system, subsystem, or component that has stopped working.\nMalfunction: Used interchangeably with “failure.”\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of one or more servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls are\nreceived per second.\nbe outages when this strategy is pursued, but they will be rare. This is the most\nexpensive strategy. Another strategy is to perform a dependency analysis and\nverify that each system depends on high-quality parts. Manufacturers calculate\ntheir components’ reliability and publish their mean time between failure (MTBF)\nratings. By analyzing the dependencies within the system, one can predict MTBF\nfor the entire system. The MTBF of the system is only as high as that of its\nlowest-MTBF part.\nSuch a strategy is predictive, meaning that it predicts the likelihood of failure\nor the reliability of the system.\nResilient systems continue where predictive strategies leave off. Assuming\nthat failure will happen, we build systems that react and respond intelligently so\nthat the system as a whole survives and continues to provide service. In other\nwords, resilient systems are responsive to failure. Rather than avoiding failure\nthrough better hardware or responding to it with human effort (and apologies),\nthey take a proactive stance and put in place mechanisms that expect and survive\nfailure.\nResilient systems decouple component failure from user-visible outages. In\ntraditional computing, where there is a failed component, there is a user-visible\noutage. When we build survivable systems, the two concepts are decoupled.\nThis chapter is about the various ways we can design systems that detect\nfailure and work around it. This is how we build survivable systems. The tech-\nniques are grouped into four categories: physical failures, attacks, human errors,\nand unexpected load.\n6.1 Software Resiliency Beats Hardware Reliability\nYou can build a reliable system by selecting better hardware or better software.\nBetter hardware means special-purpose CPUs, components, and storage systems.\n",
      "content_length": 2293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "6.2\nEverything Malfunctions Eventually\n121\nBetter software means adding intelligence to a system so that it detects failures and\nworks around them.\nSoftware solutions are favored for many reasons. First and foremost, they are\nmore economical. Once software is written, it can be applied to many services\nand many machines with no additional cost (assuming it is home-grown, is open\nsource, or does not require a per-machine license.) Software is also more malleable\nthan hardware. It is easier to ﬁx, upgrade, and replace. Unlike hardware upgrades,\nsoftware upgrades can be automated. As a result, software is replaced often. New\nfeatures can be introduced faster and more frequently. It is easy to experiment.\nAs software gets older, it gets stronger: Bugs are ﬁxed; rare edge cases are han-\ndled better. Spolsky’s (2004) essay, “Things You Should Never Do,” gives many\nexamples.\nUsing better hardware, by comparison, is more expensive. The initial pur-\nchase price is higher. More reliable CPUs, components, and storage systems are\nmuch more expensive than commodity parts. This strategy is also more expensive\nbecause you pay the extra expense with each machine as you grow. Upgrading\nhardware has a per-machine cost for the hardware itself, installation labor, capi-\ntal depreciation, and the disposal of old parts. Designing hardware takes longer,\nso upgrades become available less frequently and it is more difﬁcult to experi-\nment and try new things. As hardware gets older, it becomes more brittle and fails\nmore often.\n6.2 Everything Malfunctions Eventually\nMalfunctions are a part of every environment. They can happen at every level. For\nexample, they happen at the component level (chips and other electronic parts),\nthe device level (hard drives, motherboards, network interfaces), and the system\nlevel (computers, network equipment, power systems). Malfunctions also occur\nregionally: racks lose power, entire datacenters go ofﬂine, cities and entire regions\nof the world are struck with disaster. Humans are also responsible for malfunctions\nranging from typos to software bugs, from accidentally kicking a power cable out\nof its socket to intentionally malicious attacks.\n6.2.1 MTBF in Distributed Systems\nLarge systems magnify small problems. In large systems a “one in a million” prob-\nlem happens a lot. A hard drive with an MTBF of 1 million hours has a 1 in 114\nchance of failing this year. If you have 100,000 such hard disks, you can expect two\nto fail every day.\nA bug in a CPU that is triggered with a probability of one in 10 million might\nbe why your parents’ home PC crashed once in 2010. They cursed, rebooted, and\n",
      "content_length": 2644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "122\nChapter 6\nDesign Patterns for Resiliency\ndidn’t think of it again. Such a bug would be hardly within the chip maker’s\nability to detect. That same bug in a distributed computing system, however,\nwould be observed frequently enough to show up as a pattern in a crash detec-\ntion and analysis system. It would be reported to the vendor, which would be\ndismayed that it existed, shocked that anyone found it, and embarrassed that it\nhad been in the core CPU design for multiple chip generations. The vendor would\nalso be unlikely to give permission to have the speciﬁcs documented in a book on\nsystem administration.\nFailures cluster so that it appears as if the machines are ganging up on us.\nRacks of machines trying to boot at the same time after a power outage expose\nmarginal power supplies unable to provide enough juice to spin up dozens of disks\nat once. Old solder joints shrink and crack, leading to mysterious failures. Compo-\nnents from the same manufacturing batch have similar mortality curves, resulting\nin a sudden rush of failures.\nWith our discussion of the many potential malfunctions and failures, we hope\nwe haven’t scared you away from the ﬁeld of system administration!\n6.2.2 The Traditional Approach\nTraditional software assumes a perfect, malfunction-free world. This leaves the\nhardware systems engineer with the impossible task of delivering hardware that\nnever fails. We fake it by using redundant array of inexpensive [independent]\ndisks (RAID) systems that let the software go on pretending that disks never fail.\nSheltered from the reality of a world full of malfunctions, we enable software devel-\nopers to continue writing software that assumes a perfect, malfunction-free world\n(which, of course, does not exist).\nFor example, UNIX applications are written with the assumption that reading\nand writing ﬁles will happen without error. As a result, applications do not check\nfor errors when writing ﬁles. If they did, it would be a waste of time because the\nblocks may not be written to disk until later, possibly after the application has been\nexited. Microsoft Word is written with the assumption that the computer it runs on\nwill continue to run.\n.\nHyperbole Warning\nThe previous paragraph included two slight exaggerations. The application\nlayer of UNIX assumes a perfect ﬁle system but the underlying layers do not\nassume perfect disks. Microsoft Word checkpoints documents so that the user\ndoes not lose data in the event of a crash. However, during that crash the user\nis unable to edit the document.\n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "6.2\nEverything Malfunctions Eventually\n123\nAttempts to achieve this impossible malfunction-free world cause companies\nto spend a lot of money. CPUs, components, and storage systems known for high\nreliability are demonstrably more expensive than commodity parts. Appendix B\ndetails the history of this strategy and explains the economic beneﬁts of distributed\ncomputing techniques discussed in this chapter.\n6.2.3 The Distributed Computing Approach\nDistributed computing, in contrast to the traditional approach, embraces compo-\nnents’ failures and malfunctions. It takes a reality-based approach that accepts\nmalfunctions as a fact of life. Google Docs continues to let a user edit a document\neven if a machine fails at Google: another machine takes over and the user does\nnot even notice the handoff.\nTraditional computing goes to great lengths to achieve reliability through\nhardware and then either accepts a small number of failures as “normal” or adds\nintelligence to detect and route around failures. If your software can route around\nfailure, it is wasteful to also spend money on expensive hardware.\nA popular bumper-sticker says “Eat right. Exercise. Die anyway.” If your\nhardware is going to fail no matter how expensive it is, why buy the best? Why\npay for reliability twice?\n.\nBuying Failed Memory\nEarly in its history, Google tried to see how far it could push the limits of using\nintelligent software to manage unreliable hardware. To do so, the company\npurchased failed RAM chips and found ways to make them useful.\nGoogle was purchasing terabytes of RAM for machines that ran software\nthat was highly resilient to failure. If a chip failed, the OS would mark that\narea of RAM as unusable and kill any process using it. The killed processes\nwould be restarted automatically. The fact that the chip was bad was recorded\nso that it was ignored by the OS even after reboot.\nAs a result, a machine didn’t need to be repaired just because one chip\nhad failed. The machine could run until the machine’s capacity was reduced\nbelow usable limits.\nTo understand what happened next, you must understand that the differ-\nence between high-quality RAM chips and normal-quality chips is how much\ntesting they pass. RAM chips are manufactured and then tested. The ones that\npass the most QA testing are sold as “high quality” at a high price. The ones\nthat pass the standard QA tests are sold as normal for the regular price. All\nothers are thrown away.\n",
      "content_length": 2455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "124\nChapter 6\nDesign Patterns for Resiliency\nGoogle’s purchasing people are formidable negotiators. Google had\nalready been saving money by purchasing the normal-quality chips, relying\non the custom software Google wrote to work around failures. One day the\npurchasing department thought to ask if it was possible to purchase the chips\nthat were being thrown away. The manufacturers had never received such a\nrequest before and were willing to sell the defective chips for pennies on the\ndollar.\nThe “failed” RAM chips worked perfectly for Google’s need. Some didn’t\nwork from the start and others failed soon after. However, services were able\nto keep running.\nGoogle eventually ended this practice but for many years the company\nwas able to build servers with enormous amounts of RAM for less money\nthan any of its competitors. When your business is charging pennies for\nadvertisements, saving dollars is a big advantage!\n6.3 Resiliency through Spare Capacity\nThe general strategy used to gain resiliency is to have redundant units of capacity\nthat can fail independently of each other. Failures are detected and those units are\nremoved from service. The total capacity of the system is reduced but the system\nis still able to run. This means that systems must be built with spare capacity to\nbegin with.\nLet’s use the example of a web server that serves the static images displayed\non a web site. Such a server is easy to replicate because the content does not change\nfrequently. We can, for example, build multiple such servers and load balance\nbetween them. (How load balancers work was discussed in Section 4.2.1.) We call\nthese servers replicas because the same service is replicated by each server. They\nare duplicates in that they all respond to the same queries and give equivalent\nresults. In this case the same images are accessed at the same URLs.\nSuppose each replica can handle 100 QPS and the service receives 300 QPS at\npeak times. Three servers would be required to provide the 300 QPS capacity. An\nadditional replica is needed to provide the spare capacity required to survive one\nfailed replica. Failure of any one replica is detected and that replica is taken out of\nservice automatically. The load is now balanced over the surviving three replicas.\nThe total capacity of the system is reduced to 300 QPS, which is sufﬁcient.\nWe call this N + M redundancy. Such systems require N units to provide\ncapacity and have M units of extra capacity. Units are the smallest discrete system\nthat provides the service. The term N + 1 redundancy is used when we wish to\nindicate that there is enough spare capacity for one failure, such as in our example.\n",
      "content_length": 2665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "6.3\nResiliency through Spare Capacity\n125\nIf we added a ﬁfth server, the system would be able to survive two simultaneous\nfailures and would be described as N + 2 redundancy.\nWhat if we had 3 + 1 redundancy and a series of failures? After the ﬁrst failure,\nthe system is described as 3 + 0. It is still running but there is no redundancy. The\nsecond failure (a double failure) would result in the system being oversubscribed.\nThat is, there is less capacity available than needed.\nContinuing our previous example, when there are two failed replicas, there\nis 200 QPS of capacity. The system is now 3:2 oversubscribed: two replicas exist\nwhere three are needed. If we are lucky, this has happened at a time of day that\ndoes not draw many users and 200 QPS is sufﬁcient. However, if we are unlucky,\nthis has happened at peak usage time and our two remaining servers are faced with\n300 QPS, more than they are designed to handle. Dealing with such an overload is\ncovered later in Section 6.7.1.\n6.3.1 How Much Spare Capacity\nSpare capacity is like an insurance policy: it is an expense you pay now to prepare\nfor future trouble that you hope does not happen. It is better to have insurance and\nnot need it than to need insurance and not have it. That said, paying for too much\ninsurance is wasteful and not good business. Selecting the granularity of our unit\nof capacity enables us to manage the efﬁciency. For example, in a 1 + 1 redundant\nsystem, 50 percent of the capacity is spare. In a 20 + 1 redundant system, less than\n5 percent of the capacity is spare. The latter is more cost-efﬁcient.\nThe other factors in selecting the amount of redundancy are how quickly we\ncan bring up additional capacity and how likely it is that a second failure will\nhappen during that time. The time it takes to repair or replace the down capacity\nis called the mean time to repair (MTTR). The probability an outage will happen\nduring that time is the reciprocal of the mean time between failures. The per-\ncent probability that a second failure will happen during the repair window is\nMTTR/MTBF × 100.\nIf a second failure means data loss, the probability of a second failure becomes\nan important factor in how many spares you should have.\nSuppose it takes a week (168 hours) to repair the capacity and the MTBF is\n100,000 hours. There is a 168/1, 000, 000 × 100 = 1.7 percent, or 1 in 60, chance of\na second failure.\nNow suppose the MTBF is two weeks (336 hours). In this case, there is a\n168/336 × 100 = 50 percent, or 1 in 2, chance of a second failure—the same as a\ncoin ﬂip. Adding an additional replica becomes prudent.\nMTTR is a function of a number of factors. A process that dies and needs to\nbe restarted has a very fast MTTR. A broken hardware component may take only a\nfew minutes to replace, but if that server is in a datacenter 9000 miles away, it may\n",
      "content_length": 2851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "126\nChapter 6\nDesign Patterns for Resiliency\ntake a month before someone is able to reach it. Spare parts need to be ordered,\nshipped, and delivered. Even if a disk can be replaced within minutes of failure,\nif it is in a RAID conﬁguration there may be a long, slow rebuild time where the\nsystem is still N + 0 until the rebuild is complete.\nIf all this math makes your head spin, here is a simple rule of thumb: N + 1 is\na minimum for a service; N + 2 is needed if a second outage is likely while you are\nﬁxing the ﬁrst one.\nDigital computers are either on or off, and we think in terms of a service as\neither running or not: it is either up or down. When we use resiliency through repli-\ncation, the service is more like an analog device: it can be on, off, or anywhere in\nbetween. We are no longer monitoring the service to determine if it is up or down.\nInstead, we are monitoring the amount of capacity in the system and determining\nwhether we should be adding more. This changes the way we think about our sys-\ntems and how we do operations. Rather than being awakened in the middle of the\nnight because a machine is down, we are alerted only if the needle of a gauge gets\nnear the danger zone.\n6.3.2 Load Sharing versus Hot Spares\nIn the previous examples, the replicas are load sharing: all are active, are sharing\nthe workload equally (approximately), and have equal amounts of spare capacity\n(approximately). Another strategy is to have primary and secondary replicas. In\nthis approach, the primary replica receives the entire workload but the secondary\nreplica is ready to take over at any time. This is sometimes called the hot spare or\n“hot standby” strategy since the spare is connected to the system, running (hot),\nand can be switched into operation instantly. It is also known as an active–passive\nor master–slave pair. Often there are multiple secondaries. Because there is only\none master, these conﬁgurations are 1 + M conﬁgurations.\nSometimes the term “active–active” or “master–master” pair will be used to\nrefer to two replicas that are load sharing. “Active–active” is more commonly\nused with network links. “Master–master” is more commonly used in the database\nworld and in situations where the two are tightly coupled.\n6.4 Failure Domains\nA failure domain is the bounded area beyond which failure has no impact. For\nexample, when a car fails on a highway, its failure does not make the entire highway\nunusable. The impact of the failure is bounded to its failure domain.\nThe failure domain of a fuse in a home circuit breaker box is the room or two\nthat is covered by that circuit. If a power line is cut, the failure domain affects a\nnumber of houses or perhaps a city block. The failure domain of a power grid might\n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "6.4\nFailure Domains\n127\nbe the town, region, or county that it feeds (which is why some datacenters are\nlocated strategically so they have access to two power grids).\nA failure domain may be prescriptive—that is, a design goal or requirement.\nYou might plan that two groups of servers are each their own failure domain and\nthen engineer the system to meet that goal, assuring that the failure domains that\nthey themselves rely on are independent. Each group may be in different racks,\ndifferent power circuits, and so on. Whether they should be in different datacenters\ndepends on the scope of the failure domain goal.\nAlternatively, a failure domain may be descriptive. Often we ﬁnd ourselves\nexploring a system trying to determine, or reverse-engineer, what the resulting\nfailure domain has become. Due to a failed machine, a server may have been moved\ntemporarily to a spare machine in another rack. We can determine the new failure\ndomain by exploring the implications of this move.\nDetermining a failure domain is done within a particular scope or assump-\ntions about how large an outage we are willing to consider. For example, we may\nkeep off-site backups 1000 miles away, assuming that an outage that affects two\nbuildings that far apart is an acceptable risk, or that a disaster that large would\nmean we’d have other problems to worry about.\n.\nUnaligned Failure Domains Increase Outage Impact\nA company with many large datacenters used an architecture in which a\npower bus was shared by every group of six racks. A network subsystem\nprovided network connectivity for every eight racks. The network subsystem\nreceived power from the ﬁrst rack of each of its groups.\nIf a power bus needed to be turned off for maintenance, the outage this\nwould create would involve the six racks directly attached to it for power, plus\nother racks would lose network connectivity if they were unlucky enough\nto be on a network subsystem that got power from an affected rack. This\nextended the failure domain to as many as 13 racks. Many users felt it was\nunfair that they were suffering even though the repair didn’t directly affect\nthem.\nThere were additional unaligned failure domains related to cooling and\nwhich machines were managed by which cluster manager. As a result, these\nmisalignments were not just an inconvenience to some but a major factor\ncontributing to system availability.\nEventually a new datacenter design was created that aligned all physical\nfailure domains to a common multiple. In some cases, this meant working with\nvendors to create custom designs. Old datacenters were eventually retroﬁtted\nto the new design at great expense.\n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "128\nChapter 6\nDesign Patterns for Resiliency\nWe commonly hear of datacenters that have perfect alignment of power,\nnetworking, and other factors but in which an unexpected misalignment results\nin a major outage. For example, consider a datacenter with 10 domains, each\nindependently powered, cooled, and networked. Suppose the building has two\nconnections to the outside world and the related equipment is located based on\nwhere the connections come into the building. If cooling fails in the two domains\nthat include those connections, suddenly all 10 domains have no connectivity to\nthe outside world.\n6.5 Software Failures\nAs long as there has been software, there have been software bugs. Long-running\nsoftware can die unexpectedly. Software can hang and not respond. For all these\nreasons, software needs resilience features, too.\n6.5.1 Software Crashes\nA common failure in a system is that software crashes, or prematurely exits. There\nare many reasons software may crash and many ways to respond. Server software\nis generally intended to be long lived. For example, a server that provides a partic-\nular API is expected to run forever unless the conﬁguration changes in a way that\nrequires a restart or the service is decommissioned.\nThere are two categories of crashes:\n• A regular crash occurs when the software does something prohibited by the\noperating system. For example, due to a software bug, the program may try\nto write to memory that is marked read-only by the operating system. The OS\ndetects this and kills the process.\n• A panic occurs when the software itself detects something is wrong and\ndecides the best course is to exit. For example, the software may detect a situ-\nation that shouldn’t exist and cannot be corrected. The software’s author may\nhave decided the safest thing to do in such a scenario is to exit. For example, if\ninternal data structures are corrupted and there is no safe way to rectify them,\nit is best to stop work immediately rather than continue with bad data. A panic\nis, essentially, an intentional crash.\nAutomated Restarts and Escalation\nThe easiest way to deal with a software crash is to restart the software. Sometimes\nthe problem is transient and a restart is all that is needed to ﬁx it. Such restarts\nshould be automated. With thousands of servers, it is inefﬁcient for a human to\nconstantly be checking processes to see if they are down and restarting them as\nneeded. A program that handles this task called a process watcher.\n",
      "content_length": 2485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "6.5\nSoftware Failures\n129\nHowever, restarting a down process is not as easy as it sounds. If it immedi-\nately crashes again and again, we need to do something else; otherwise, we will be\nwasting CPU time without improving the situation. Usually the process watcher\nwill detect that the process has been restarted x times in y minutes and consider\nthat behavior cause to escalate the issue. Escalation involves not restarting the pro-\ncess and instead reporting the problem to a human. An example threshold might\nbe that something has restarted more than ﬁve times in a minute.\nLess frequent restarts are often a sign of other problems. One restart every\nhour is not cause for alarm but it should be investigated. Often these slower restart\nissues are detected by the monitoring system rather than the process watcher.\nAutomated Crash Data Collection and Analysis\nEvery crash should be logged. Crashes usually leave behind a lot of information\nin a crash report. The crash report includes statistics such as amount of RAM\nand CPU usage at the time of the process’s death, as well as detailed informa-\ntion such as a traceback of which function call and line of code was executing\nwhen the problem occurred. A coredump—a ﬁle containing the contents of the\nprocess’s memory—is often written out during a crash. Developers use this ﬁle\nto aid debugging.\nAutomated collection and storage of crash reports is useful because this infor-\nmation may be lost if it is not collected quickly; the information may be deleted or\nthe machine may go away. Collecting the information is inconvenient for humans\nbut easy for automation. This is especially true in a system with hundreds of\nmachines and hundreds of thousands of processes. Storing the reports centrally\npermits data mining and analysis. A simple analytical result, such as which sys-\ntems crash the most, can be a useful engineering metric. More intricate analysis can\nﬁnd bugs in common software libraries, the operating system, hardware, or even\nparticular chips.\n6.5.2 Software Hangs\nSometimes when software has a problem it does not crash, but instead hangs or\ngets caught in an inﬁnite loop.\nA strategy for detecting hangs is to monitor the server and detect if it has\nstopped processing requests. We can passively observe request counts or actively\ntest the system by sending requests and verifying that a reply is generated within a\ncertain amount of time. These active requests, which are called pings, are designed\nto be light-weight, simply verifying basic functionality.\nIf pings are sent at a speciﬁc, periodic rate and are used to detect hangs as well\nas crashes, they are called heartbeat requests. When hangs are detected, an error\ncan be generated, an alert sent, or an attempt to restart the service can be made.\nIf the server is one of many replicas behind a load balancer, rather than simply\n",
      "content_length": 2859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "130\nChapter 6\nDesign Patterns for Resiliency\nrestarting it, you can remove it from the load balancing rotation and investigate\nthe problem. Sometimes adding a new replica is signiﬁcantly more work than\nreturning a replica that has been repaired to service. For example, in the Google\nFile System, a new replica added to the system requires replicating possibly tera-\nbytes of ﬁles. This can ﬂood the network. Fixing a hung replica and returning it to\nservice simply results in the existing data being revalidated, which is a much more\nlight-weight task.\nAnother technique for dealing with software hangs is called a watchdog timer.\nA hardware clock keeps incrementing a counter. If the counter exceeds a certain\nvalue, a hardware subsystem will detect this and reboot the system. Software run-\nning on the system resets the counter to zero after any successful operation. If the\nsoftware hangs, the resets will stop and soon the system will be rebooted. As long as\nthe software keeps running, the counter will be reset frequently enough to prevent\na reboot.\nA watchdog timer is most commonly used with operating system kernels and\nembedded systems. Enabling the Linux kernel watchdog timer on a system with\nappropriate hardware can be used to reduce the need to physically visit a machine\nwhen the kernel hangs or to avoid the need to purchase expensive remote power\ncontrol systems.\nLike crashes, hangs should be logged and analyzed. Frequent hangs are an\nindication of hardware issues, locking problems, and other bugs that should be\nﬁxed before they become big problems.\n6.5.3 Query of Death\nSometimes a particular API call or query exercises an untested code path that\ncauses a crash, a long delay, or an inﬁnite loop. We call such a query a query of\ndeath because it kills the service.\nWhen users discover a query of death for a popular web site, they let all of\ntheir friends know. Soon much of the internet will also be trying it to see what a\ncrashing web site looks like. The better known your company is, the faster word\nwill spread.\nThe best ﬁx is to eliminate the bug that causes the problem. Unfortunately, it\ncan take a long time to ﬁx the code and push a new release. A quick ﬁx is needed\nin the meantime.\nA widely used strategy is to have a banned query list that is easy to update\nand communicate to all the frontends. The frontends automatically reject any query\nthat is found on the banned query list.\nHowever, that solution still requires human intervention. A more automated\nmechanism is required, especially when a query has a large fan-out. For example,\nsuppose the query is received and then sent to 1000 other servers, each one holding\n",
      "content_length": 2661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "6.6\nPhysical Failures\n131\n1/1000th of the database. A query of death would kill 1000 servers along with all\nthe other queries that are in ﬂight.\nDean and Barroso (2013) describe a preventive measure pioneered at Google\ncalled canary requests. In situations where one would normally send the same\nrequest to thousands of leaf servers, systems using this approach send the query\nto one or two leaf servers. These are the canary requests. Queries are sent to the\nremaining servers only if replies to the canary requests are received in a reasonable\nperiod of time. If the leaf servers crash or hang while the canary requests are being\nprocessed, the system ﬂags the request as potentially dangerous and prevents fur-\nther crashes by not sending it to the remaining leaf servers. Using this technique\nGoogle is able to achieve a measure of robustness in the face of difﬁcult-to-predict\nprogramming errors as well as malicious denial-of-service attacks.\n6.6 Physical Failures\nDistributed systems also need to be resilient when faced with physical failures.\nThe physical devices used in a distributed system can fail on many levels. Physi-\ncal failures can range from the smallest electronic component all the way up to a\ncountry’s power grid. Providing resiliency through the use of redundancy at every\nlevel is expensive and difﬁcult to scale. You need a strategy for providing resiliency\nagainst hardware failures without adding excessive cost.\n6.6.1 Parts and Components\nMany components of a computer can fail. The parts whose utilization you mon-\nitor can fail, such as the CPU, the RAM, the disks, and the network interfaces.\nSupporting components can also fail, such as fans, power supplies, batteries, and\nmotherboards.\nHistorically, when the CPU died, the entire machine was unusable. Multi-\nprocessor computers are now quite common, however, so it is more likely that\na machine can survive so long as one processor is still functioning. If the machine\nis already resilient in that way, we must monitor for N + 0 situations.\nRAM\nRAM often fails for strange reasons. Sometimes a slight power surge can affect\nRAM. Other times a single bit ﬂips its value because a cosmic ray from another\nstar system just happened to ﬂy through it. Really!\nMany memory systems store with each byte an additional bit (a parity bit)\nthat enables them to detect errors, or two additional bits (error-correcting code or\nECC memory) that enable them to perform error correction. This adds cost. It also\ndrags down reliability because now there are 25 percent more bits and, therefore,\n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "132\nChapter 6\nDesign Patterns for Resiliency\nthe MTTF becomes 25 percent worse. (Although most of these failures are now cor-\nrected invisibly, the failures are still happening and can be detected via monitoring\nsystems. If the failures persist, the component needs to be replaced.)\nWhen writing to parity bit memory, the system counts how many 1 bits are in\nthe byte and stores a 0 in the parity bit if the total is even, or a 1 if the total is odd.\nAnytime memory is read, the parity is checked and mismatches are reported to the\noperating system. This is sufﬁcient to detect all single-bit errors, or any multiple-\nbit errors that do not preserve parity. ECC memory uses two additional bits and\nHamming code algorithms that can correct single-bit errors and detect multiple-bit\nerrors.\nThe likelihood of two or more bit errors increases the longer that values sit in\nmemory unread and the more RAM there is in a system.\nOne can save money by having no parity or ECC bits—an approach commonly\nused with low-end chipsets—but then all software has to do its own checksum-\nming and error correction. This is slow and costly, and you or your developers\nprobably won’t do it. So spend the money on ECC, instead.\nDisks\nDisks fail often because they have moving parts. Solid-state drives (SSDs), which\nhave no moving parts, wear out since each block is rated to be written only a certain\nnumber of times.\nThe usual solution is to use RAID level 1 or higher to achieve N + 1 redun-\ndancy or better. However, RAID systems are costly and their internal ﬁrmware\nis often a source of frustration, as it is difﬁcult to conﬁgure without interrupting\nservice. (A full explanation of RAID levels is not included here but can be found in\nour other book, The Practice of System and Network Administration.)\nFile systems such as ZFS, Btrfs and Hadoop HDFS store data reliably by pro-\nviding their own RAID or RAID-like functionality. In those cases hardware RAID\ncontrollers are not needed.\nWe recommend the strategic use of RAID controllers, deploying them only\nwhere required. For example, a widely used distributed computing environment\nis the Apache Hadoop system. The ﬁrst three machines in a Hadoop cluster are\nspecial master service machines that store critical conﬁguration information. This\ninformation is not replicated and is difﬁcult to rebuild if lost. The other machines\nin a Hadoop cluster are data nodes that store replicas of data. In this environment\nRAID is normally used on the master machines. Implementing RAID there has a\nﬁxed cost, as no more than three machines with RAID controllers are needed. Data\nnodes are added when more capacity is needed. They are built without RAID since\nHadoop replicates data as needed, detecting failures and creating new replicas as\nneeded. This strategy has a cost beneﬁt in that the expensive hardware is a ﬁxed\nquantity while the nodes used to expand the system are the inexpensive ones.\n",
      "content_length": 2929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "6.6\nPhysical Failures\n133\nPower Supplies\nEach machine has a power supply that converts standard electric voltages into\nlevels needed by the computer. Power supplies frequently die. Servers, network\nequipment, and many other systems can be purchased with redundant power\nsupplies. N + 1 and N + 2 conﬁgurations are commonly available.\nAs with RAID, a strategic use of redundant power supplies is best. They are\nnot needed when the system itself is a replica or some other resilience technique is\nused at a higher level. Do use such power supplies for the remaining systems that\nare not redundant.\nNetwork Interfaces\nNetwork interfaces or network connections themselves often fail. Multiple links\ncan be used in N + 1 conﬁgurations. There are many standards, too many to detail\nhere.\nSome are load sharing, others are active–passive. Some require that all the\nnear-end (machine) connections be plugged into the same network interface con-\ntroller (NIC) daughterboard. If two physical ports share the same daughterboard,\nthe failure of one may cause the other to fail. Some require that all the far-end\n(switch) connections be plugged into the same switch, while others do not have\nsuch a limit. The latter approach provides resiliency against switch failure, not just\nNIC failure.\nMany different algorithms are available for determining which packets go\nover which physical link. With some, it is possible for packets to arrive out of order.\nWhile all protocols should handle this situation, many do not do it well.\nLongitudinal Studies on Hardware Failures\nGoogle has published to two longitudinal studies of hardware failures. Most\nstudies of such failures are done in laboratory environments. Google meticu-\nlously collects component failure information on its entire ﬂeet of machines,\nproviding probably the best insight into actual failure patterns. Both studies\nare worth reading.\n“Failure Trends in a Large Disk Drive Population” (Pinheiro, Weber &\nBarroso 2007) analyzed a large population of hard disks over many years. The\nauthors did not ﬁnd temperature or activity levels to correlate with drive fail-\nures. They found that after a single scan error was detected, drives are 39 times\nmore likely to fail within the next 60 days. They discovered the “bathtub fail-\nure curve” where failures tend to happen either in the ﬁrst month or only many\nyears later.\n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "134\nChapter 6\nDesign Patterns for Resiliency\n“DRAM Errors in the Wild: A Large-Scale Field Study” (Schroeder,\nPinheiro & Weber 2009) analyzed memory errors in a large ﬂeet of machines\nin datacenters over a period of 2.5 years. These authors found that error\nrates were orders of magnitude higher than previously reported and were\ndominated by hard errors—the kind that ECC can detect but not correct.\nTemperature had comparatively small effect compared to other factors.\n6.6.2 Machines\nMachine failures are generally the result of components that have died. If the\nsystem has subsystems that are N + 1, a double failure results in machine death.\nA machine that crashes will often come back to life if it is power cycled off and\nback on, often with a delay to let the components drain. This process can be auto-\nmated, although it is important that the automation be able to distinguish between\nnot being able to reach the machine and the machine being down.\nIf a power cycle does not revive the machine, the machine must be diagnosed,\nrepaired, and brought back into service. Much of this can be automated, especially\nthe reinstallation of the operating system. This topic is covered in more detail in\nSection 10.4.1.\nEarlier we described situations where machines fail to boot up after a power\noutage. These problems can be discovered preemptively by periodically rebooting\nthem. For example, Google drains machines one by one for kernel upgrades. As a\nresult of this practice, each machine is rebooted in a controlled way approximately\nevery three months. This reduces the number of surprises found during power\noutages.\n6.6.3 Load Balancers\nWhether a server fails because of a dead machine, a network issue, or a bug, a\nresilient way to deal with this failure is by use of replicas and some kind of load\nbalancer.\nThe same load balancer described previously to gain scale is also used to\ngain resiliency. However, when using this approach to gain scale, each replica\nadded was intended to add capacity that would be used. Now we are adding spare\ncapacity that is an insurance policy we hope not to use.\nWhen using a load balancer it is important to consider whether it is being used\nfor scaling, resiliency, or both. We have observed situations where it was assumed\nthat the presence of a load balancer means the system scales and is resilient auto-\nmatically. This is not true. The load balancer is not magic. It is a technology that\ncan be used for many different things.\n",
      "content_length": 2477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "6.6\nPhysical Failures\n135\nScale versus Resiliency\nIf we are load balancing over two machines, each at 40 percent utilization, then\neither machine can die and the remaining machines will be 80 percent utilized. In\nsuch a case, the load balancer is used for resiliency.\nIf we are load balancing over two machines, each at 80 percent utilization,\nthen there is no spare capacity available if one goes down. If one machine died,\nthe remaining replica would receive all the trafﬁc, which is 160 percent of what the\nmachine can handle. The machine will be overloaded and may cease to function.\nTwo machines each at 80 percent utilization represents an N + 0 conﬁguration. In\nthis situation, the load balancer is used for scale, not resiliency.\nIn both of the previous examples, the same conﬁguration was used: two\nmachines and a load balancer. Yet in one case resiliency was achieved and in the\nother case scale was achieved. The difference between the two was the utilization,\nor trafﬁc, being processed. In other words, 50 percent is 100 full when you have\nonly two servers.\nIf we take the second example and add a third replica but the amount of trafﬁc\ndoes not change, then 160 percent of the total 300 percent capacity is in use. This is\nan N + 1 conﬁguration since one replica can die and the remaining replicas can still\nhandle the load. In this case, the load balancer is used for both scale and resiliency.\nA load balancer provides scale when we use it to keep up with capacity, and\nresiliency when we use it to exceed capacity. If utilization increases and we have\nnot added additional replicas, we run the risk of no longer being able to claim\nresiliency. If trafﬁc is high during the day and low at night, we can end up with a\nsystem that is resilient during some hours of the day and not others.\nLoad Balancer Resiliency\nLoad balancers themselves can become a single point of failure (SPOF). Redundant\npairs of load balancers are often used to remedy this shortcoming.\nOne strategy is a simple failover. One load balancer (the primary) receives\nall trafﬁc, and the other load balancer (the secondary) monitors the health of the\nprimary by sending heartbeat messages to it. If a loss of heartbeat is detected, the\nsecondary takes over and becomes the active load balancer. Any TCP connections\nthat were “in ﬂight” are disconnected since the primary is unaware of them.\nAnother strategy is stateful failover. It resembles simple failover except that\nthe two load balancers exchange enough information, or state, so that both know\nall existing TCP connections. As a consequence, those connections are not lost in\ncase of failover.\nA single pair of load balancers are often used for many different services—for\nexample, many different web sites. All the web sites are homed at one load bal-\nancer and the other is used for failover. When using non-stateful load balancers, a\ncommon trick is to home half the web sites on one load balancer and half the web\n",
      "content_length": 2959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "136\nChapter 6\nDesign Patterns for Resiliency\nsites on the other load balancer. In this case, in the event of a failover half as many\nconnections are lost.\nHardware or Software Load Balancers\nLoad balancers may be hardware appliances that are purpose-built for the task.\nThey may also be software-based programs that run on standard computers. The\nhardware appliances are usually highly tuned and feature-rich. The software-\nbased ones are more ﬂexible.\nFor smaller services, the load balancer software might run on the machines\nproviding the services. Pushing the load balancing function to the machines them-\nselves reduces the amount of hardware to be managed. However, now the process-\ning of the load balancing software competes for CPU and other resources with the\nservices running on the box. This approach is not recommended for high-volume\nload balancing but is ﬁne for many situations.\nSoftware load balancing can also be pushed even further down the stack to\nthe clients themselves. Client-side load balancing requires that the client software\nknow which servers are available and do its own health checking, server selection,\nand so on. This is frequently done for internal services since they are usually more\ntightly controlled. The client library can load the conﬁguration from a central place,\nload balance requests, and detect and route around failures. The downside is that\nto change algorithms or ﬁx bugs, the client library must be changed, which requires\nupdating the software anywhere it is used.\n6.6.4 Racks\nRacks themselves do not usually fail. They are steel and have no active compo-\nnents. However, many failures are rack-wide. For example, a rack may have a\nsingle power feed or network uplink that is shared by all the equipment in the\nrack. Intrusive maintenance is often done one rack at a time.\nAs a result, a rack is usually a failure domain. In fact, intentionally designing\neach rack to be its own failure domain turns out to be a good, manageable size for\nmost distributed systems.\nRack Diversity\nYou can choose to break a service into many replicas and put one replica in each\nrack. With this arrangement, the service has rack diversity. A simple example\nwould be a DNS service where each DNS server is in a different rack so that a\nrack-wide failure does not cause a service outage.\nIn a Hadoop cluster, data ﬁles are stored on multiple machines for safety. The\nsystem tries to achieve rack diversity by making sure that at least one replica of\nany data block is in a different rack than the other data blocks.\n",
      "content_length": 2548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "6.6\nPhysical Failures\n137\nRack Locality\nMaking a service component self-contained within a single rack also offers cer-\ntain beneﬁts. Bandwidth is plentiful within a rack but sparse between racks. All\nthe machines in the rack connect to the same switch at the top of the rack. This\nswitch has enough internal bandwidth that any machine can talk to any machine\nwithin the rack at full bandwidth, and all machines can do this at the same time—a\nscheme called non-blocking bandwidth. Between racks there is less bandwidth.\nRack uplinks are often 10 times the links between the machines, but they are a\nshared resource used by all the machines in the rack (typically 20 or 40). There\nis contention for bandwidth between racks. The article “A Guided Tour through\nData-center Networking” (Abts & Felderman 2012) drills down into this topic\nusing Google’s networks as examples.\nBecause bandwidth is plentiful inside the rack and the rack is a failure domain,\noften a service component is designed to ﬁt within a rack. Small queries come in,\nthey use a lot of bandwidth to generate the answer, and a small or medium-size\nreply leaves. This model ﬁts well given the bandwidth restrictions.\nThe service component is then replicated on many racks. Each replica has rack\nlocality, in that it is self-contained within the rack. It is designed to take advantage\nof the high bandwidth and the rack-sized failure domain.\nRack-sized replicas are sometimes called pods. A pod is self-contained and\noften forms its own security domain. For example, a billing system may be made\nup of pods, each one self-contained and designed to handle bill processing for a\nspeciﬁc group of customers.\n.\nClos Networking\nIt is reasonable to expect that eventually there will be network products on the\nopen market that provide non-blocking, full-speed connectivity between any\ntwo machines in an entire datacenter. We’ve known how to do this since 1953\n(Clos 1953). When this product introduction happens, it will change how we\ndesign services.\n6.6.5 Datacenters\nDatacenters can also be failure domains. An entire datacenter can go down due to\nnatural disasters, cooling failures, power failures, or an unfortunate backhoe dig\nthat takes out all network connections in one swipe.\nSimilar to rack diversity and rack locality, datacenter diversity and datacenter\nlocality also exist. Bandwidth within a datacenter is generally fast, though not\n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "138\nChapter 6\nDesign Patterns for Resiliency\nas fast as within a rack. Bandwidth between datacenters is generally slower and,\nunlike with data transmitted within a datacenter, is often billed for by the gigabyte.\nEach replica of a service should be self-contained within a datacenter but the\nentire service should have datacenter diversity. Google requires N + 2 diversity\nas a minimum requirement for user-facing services. That way, when one data-\ncenter is intentionally brought down for maintenance, another can go down due\nto unforeseen circumstances without impacting the service.\n6.7 Overload Failures\nDistributed systems need to be resilient when faced with high levels of load that\ncan happen as the result of a temporary surge in trafﬁc, an intentional attack,\nor automated systems querying the system at a high rate, possibly for malicious\nreasons.\n6.7.1 Traffic Surges\nSystems should be resilient against temporary periods of high load. For example, a\nsmall service may become overloaded after being mentioned in a popular web site\nor news broadcast. Even a large service can become overloaded due to load being\nshifted to the remaining replicas when one fails.\nThe primary strategy for dealing with this problem in user-facing services is\ngraceful degradation. This topic was covered in Section 2.1.10.\nDynamic Resource Allocation\nAnother strategy is to add capacity dynamically. With this approach, a sys-\ntem would detect that a service is becoming overloaded and allocate an unused\nmachine from a pool of idle machines that are running but otherwise unconﬁgured.\nAn automated system would conﬁgure the machine and use it to add capacity to\nthe overloaded service, thereby resolving the issue.\nIt can be costly to have idle capacity but this cost can be mitigated by using\na shared pool. That is, one pool of idle machines serves a group of services. The\nﬁrst service to become overloaded allocates the machines. If the pool is large\nenough, more than one service can become overloaded at the same time. There\nshould also be a mechanism for services to give back machines when the need\ndisappears.\nAdditional capacity can be found at other service providers as well. A public\ncloud computing provider can be used as the shared pool. Usually you will not\nhave to pay for unused capacity.\nShared resource pools are not just appropriate for machines, but may also be\nused for storage and other resources.\n",
      "content_length": 2416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "6.7\nOverload Failures\n139\nLoad Shedding\nAnother strategy is load shedding. With this strategy the service turns away some\nusers so that other users can have a good experience.\nTo make an analogy, an overloaded phone system doesn’t suddenly discon-\nnect all existing calls. Instead, it responds to any new attempts to make a call with\na “fast busy” tone so that the person will try to make the call later. An overloaded\nweb site should likewise give some users an immediate response, such as a simple\n“come back later” web page, rather than requiring them to time out after minutes\nof waiting.\nA variation of load shedding is stopping certain tasks that can be put off until\nlater. For example, low-priority database updates could be queued up for process-\ning later; a social network that stores reputation points for users might store the\nfact that points have been awarded rather than processing them; nightly bulk ﬁle\ntransfers might be delayed if the network is overloaded.\nThat said, tasks that can be put off for a couple of hours might cause problems\nif they are put off forever. There is, after all, a reason they exist. For any activity that\nis delayed due to load shedding, there must be a plan on how such a delay is han-\ndled. Establish a service level agreement (SLA) to determine how long something\ncan be delayed and to identify a timeline of actions that should be undertaken to\nmitigate problems or extend the deadlines. Low-priority updates might become a\nhigh priority after a certain amount of time. If many systems are turned off due\nto load shedding, it might be possible to enable them, one at a time, to let each\ncatch up.\nTo be able to manage such situations one must have visibility into the system\nso that prioritization decisions can be made. For example, knowing the age of a\ntask (how long it has been delayed), predicting how long it will take to process,\nand indicating how close it is to a deadline will permit operations personnel to\ngauge when delayed items should be continued.\n.\nDelayed Work Reduces Quality\nAn old version of Google Web Search had two parts: the user-facing web front-\nend and the system that received and processed updates to the search index\n(corpus). These updates arrived in large chunks that had to be distributed to\neach frontend.\nThe quality of the search system was measured in terms of how fresh the\ncorpus was across all the web frontends.\nThe monitoring dashboard displayed the freshness of shards in each\nfrontend. It listed how many shards were in each freshness bucket: up-to-date,\n1 hour old, 2 hours old, 4 hours old, and so on. With this visibility, operations\n",
      "content_length": 2632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "140\nChapter 6\nDesign Patterns for Resiliency\n.\nstaff could see when something was wrong and gain an insight into which\nfrontends were the most out of date.\nIf the system was overloaded, the updater system was paused to free up\nresources for handling the additional load. The dashboard enabled operations\nstaff to understand the effects of the pause. They could unpause high-priority\nupdates to maintain at least minimal freshness.\n6.7.2 DoS and DDoS Attacks\nA denial-of-service (DoS) attack is an attempt to bring down a service by sending\na large volume of queries. A distributed denial-of-service (DDoS) attack occurs\nwhen many computers around the Internet are used in a coordinated fashion to\ncreate an extremely large DoS attack. DDoS attacks are commonly initiated from\nbotnets, which are large collections of computers around the world that have been\nsuccessfully inﬁltrated and are now controlled centrally, without the knowledge of\ntheir owners.\nBlocking the requests is usually not a successful defense against a DDoS attack.\nAttackers can forge packets in a way that obscures where the attack is coming from,\nthereby making it impossible for you to construct ﬁlters that would block the attack\nwithout blocking legitimate trafﬁc. If they do come from a ﬁxed set of sources,\nsimply not responding to the requests still hogs bandwidth used to receive the\nattack—and that alone can overload a network. The attack must be blocked from\noutside your network, usually by the ISP you connect to. Most ISPs do not provide\nthis kind of ﬁltering.\nThe best defense is to simply have more bandwidth than the attacker. This is\nvery difﬁcult considering that most DDoS attacks involve thousands of machines.\nVery large companies are able to use this line of defense. Smaller companies can\nuse DDoS attack mitigation services. Many CDN vendors (see Section 5.8) provide\nthis service since they have bandwidth available.\nSometimes a DDoS attack does not aim to exhaust bandwidth but rather to\nconsume large amounts of processing time or load. For example, one might ﬁnd\na small query that demands a large amount of resources to reply to. In this case\nthe banned query list described previously can be used to block this query until a\nsoftware release ﬁxes the problem.\n6.7.3 Scraping Attacks\nA scraping attack is an automated process that acts like a web browser to query\nfor information and then extracts (scrapes) the useful information from the HTML\npages it receives. For example, if you wanted a list of every book ever published\n",
      "content_length": 2529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "6.8\nHuman Error\n141\nbut didn’t want to pay for such a database from a library supply company, you\ncould write a program that sends millions of search requests to Amazon.com, parse\nthe HTML pages, and extract the book titles to build your database. This use of\nAmazon is considered an attack because it violates the company’s terms of service.\nSuch an attack must be defended against to prevent theft of information,\nto prevent someone from violating the terms of service, and because a very fast\nscraper is equivalent to a DoS attack. Detecting such an attack is usually done by\nhaving all frontends report information about the queries they are receiving to a\ncentral scraping detector service.\nThe scraping detector warns the frontends of any suspected attacks. If there\nis high conﬁdence that a particular source is involved in an attack, the frontends\ncan block or refuse to answer the queries. If conﬁdence in the source of the attack\nis low, the frontends can respond in other ways. For example, they can ask the user\nto prove that he or she is a human by using a Captcha or other system that can\ndistinguish human from machine input.\nSome scraping is permitted, even desired. A scraping detector should have\na whitelist that permits search engine crawlers and other permitted agents to do\ntheir job.\n6.8 Human Error\nAs we design systems to be more resilient to hardware and software failures,\nthe remaining failures are likely to be due to human error. While this sounds\nobvious, this trend was not recognized until the groundbreaking paper “Why Do\nInternet Services Fail, and What Can Be Done about It?” was published in 2003\n(Oppenheimer, Ganapathi & Patterson 2003).\nThe strategies for dealing with human error can be categorized as getting\nbetter humans, removing humans from the loop, and detecting human errors and\nworking around them.\nWe get better humans by having better operational practices, especially\nthose that exercise the skills and behaviors that most need improvement. (See\nChapter 15.)\nWe remove humans from the loop through automation. Humans may get\nsloppy and not do as much checking for errors during a procedure, but automation,\nonce written, will always check its work (See Chapter 12.)\nDetecting human errors and working around them is also a function of auto-\nmation. A pre-check is automation that checks inputs and prevents a process from\nrunning if the tests fail. For example, a pre-check can verify that a recently edited\nconﬁguration ﬁle has no syntax errors and meets certain other quality criteria.\nFailing the pre-check would prevent the conﬁguration ﬁle from being put into use.\n",
      "content_length": 2623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "142\nChapter 6\nDesign Patterns for Resiliency\nWhile pre-checks are intended to prevent problems, the reality is that they\ntend to lag behind experience. That is, after each outage we add new pre-checks to\nprevent that same human error from creating future outages.\nAnother common pre-check is for large changes. If a typical change usually\nconsists of only a few lines, a pre-check might require additional approval if the\nchange is larger than a particular number of lines. The change might be in the size\nof the input, the number of changed lines between the current input and new input,\nor the number of changed lines between the current and new output. For example,\na conﬁguration ﬁle may be used to control a system that generates other ﬁles. The\ngrowth of the output by more than a certain percentage may trigger additional\napproval.\nAnother way to be resilient to human error is to have two humans check\nall changes. Many source code control systems can be conﬁgured to not accept\nchanges from a user until a second user approves them. All system administration\nthat is done via changes to ﬁles in a source code repository are then checked by a\nsecond pair of eyes. This is a very common operational method at Google.\n6.9 Summary\nResiliency is a system’s ability to constructively deal with failures. A resilient\nsystem detects failure and routes around it.\nFailure is a normal part of operations and can occur at any level. Large systems\nmagnify the risk of small failures. A one in a million failure is a daily occurrence if\nyou have enough machines.\nFailures come from many sources. Software can fail unintentionally due to\nbugs or intentionally to prevent a bad situation from getting worse. Hardware can\nalso fail, with the scope of the failure ranging from the smallest component to the\nlargest network. Failure domains can be any size: a device, a computer, a rack, a\ndatacenter, or even an entire company.\nThe amount of capacity in a system is N + M, where N is the amount of capac-\nity used to provide a service and M is the amount of spare capacity available, which\ncan be used in the event of a failure. A system that is N + 1 fault tolerant can survive\none unit of failure and remain operational.\nThe most common way to route around failure is through replication of ser-\nvices. A service may be replicated one or more times per failure domain to provide\nresilience greater than the domain.\nFailures can also come from external sources that overload a system, and\nfrom human mistakes. There are countermeasures to nearly every failure imag-\ninable. We can’t anticipate all failures, but we can plan for them, design solutions,\nprioritize their implementation, and repeat the process.\n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Exercises\n143\nExercises\n1. What are the major sources of failure in distributed computing systems?\n2. What are the most common failures: software, hardware, or human? Justify\nyour answer.\n3. Select one resiliency technique and give an example of a failure and the way\nin which the resiliency technique would prevent a user-visible outage. Do this\nfor one technique in each of these sections: 6.5, 6.6, 6.7, and 6.8.\n4. If a load balancer is being used, the system is automatically scalable and\nresilient. Do you agree or disagree with this statement? Justify your answer.\n5. Which resiliency techniques or technologies are in use in your environment?\n6. Where would you like to add resiliency in your current environment? Describe\nwhat you would change and which techniques you would apply.\n7. In your environment, give an example of graceful degradation under load, or\nexplain how you would implement it if it doesn’t currently exist.\n8. How big can a RAID5 array be? For example, how large can it be before the\nparity checking scheme is likely to miss an error? How long can the rebuild\ntime be before MTBF puts the system at risk of a second failure?\n9. The phrase “Eat right. Exercise. Die anyway.” is mentioned on page 123.\nExplain how this relates to distributed computing.\n",
      "content_length": 1280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Part II\nOperations: Running It\n",
      "content_length": 31,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "Chapter 7\nOperations in a Distributed\nWorld\nThe rate at which organizations\nlearn may soon become the only\nsustainable source of competitive\nadvantage.\n—Peter Senge\nPart I of this book discussed how to build distributed systems. Now we discuss\nhow to run such systems.\nThe work done to keep a system running is called operations. More speciﬁ-\ncally, operations is the work done to keep a system running in a way that meets or\nexceeds operating parameters speciﬁed by a service level agreement (SLA). Oper-\nations includes all aspects of a service’s life cycle: from initial launch to the ﬁnal\ndecommissioning and everything in between.\nOperational work tends to focus on availability, speed and performance, secu-\nrity, capacity planning, and software/hardware upgrades. The failure to do any\nof these well results in a system that is unreliable. If a service is slow, users will\nassume it is broken. If a system is insecure, outsiders can take it down. With-\nout proper capacity planning, it will become overloaded and fail. Upgrades, done\nbadly, result in downtime. If upgrades aren’t done at all, bugs will go unﬁxed.\nBecause all of these activities ultimately affect the reliability of the system, Google\ncalls its operations team Site Reliability Engineering (SRE). Many companies have\nfollowed suit.\nOperations is a team sport. Operations is not done by a single person but\nrather by a team of people working together. For that reason much of what we\ndescribe will be processes and policies that help you work as a team, not as a group\nof individuals. In some companies, processes seem to be bureaucratic mazes that\nslow things down. As we describe here—and more important, in our professional\nexperience—good processes are exactly what makes it possible to run very large\n147\n",
      "content_length": 1783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "148\nChapter 7\nOperations in a Distributed World\n.\nTerms to Know\nInnovate: Doing (good) things we haven’t done before.\nMachine: A virtual or physical machine.\nOncall: Being available as ﬁrst responder to an outage or alert.\nServer: Software that provides a function or API. (Not a piece of hardware.)\nService: A user-visible system or product composed of one or more servers.\nSoft launch: Launching a new service without publicly announcing it. This\nway trafﬁc grows slowly as word of mouth spreads, which gives opera-\ntions some cushion to ﬁx problems or scale the system before too many\npeople have seen it.\nSRE: Site Reliability Engineer, the Google term for systems administrators\nwho maintain live services.\nStakeholders: People and organizations that are seen as having an interest\nin a project’s success.\ncomputing systems. In other words, process is what makes it possible for teams to\ndo the right thing, again and again.\nThis chapter starts with some operations management background, then dis-\ncusses the operations service life cycle, and ends with a discussion of typical\noperations work strategies. All of these topics will be expanded upon in the\nchapters that follow.\n7.1 Distributed Systems Operations\nTo understand distributed systems operations, one must ﬁrst understand how it is\ndifferent from typical enterprise IT. One must also understand the source of tension\nbetween operations and developers, and basic techniques for scaling operations.\n7.1.1 SRE versus Traditional Enterprise IT\nSystem administration is a continuum. On one end is a typical IT department,\nresponsible for traditional desktop and client–server computing infrastructure,\noften called enterprise IT. On the other end is an SRE or similar team responsi-\nble for a distributed computing environment, often associated with web sites and\nother services. While this may be a broad generalization, it serves to illustrate some\nimportant differences.\nSRE is different from an enterprise IT department because SREs tend to be\nfocused on providing a single service or a well-deﬁned set of services. A traditional\nenterprise IT department tends to have broad responsibility for desktop services,\n",
      "content_length": 2178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "7.1\nDistributed Systems Operations\n149\nback-ofﬁce services, and everything in between (“everything with a power plug”).\nSRE’s customers tend to be the product management of the service while IT cus-\ntomers are the end users themselves. This means SRE efforts are focused on a few\nselect business metrics rather than being pulled in many directions by users, each\nof whom has his or her own priorities.\nAnother difference is in the attitude toward uptime. SREs maintain services\nthat have demanding, 24 × 7 uptime requirements. This creates a focus on pre-\nventing problems rather than reacting to outages, and on performing complex\nbut non-intrusive maintenance procedures. IT tends to be granted ﬂexibility with\nrespect to scheduling downtime and has SLAs that focus on how quickly service\ncan be restored in the event of an outage. In the SRE view, downtime is some-\nthing to be avoided and service should not stop while services are undergoing\nmaintenance.\nSREs tend to manage services that are constantly changing due to new soft-\nware releases and additions to capacity. IT tends to run services that are upgraded\nrarely. Often IT services are built by external contractors who go away once the\nsystem is stable.\nSREs maintain systems that are constantly being scaled to handle more trafﬁc\nand larger workloads. Latency, or how fast a particular request takes to process,\nis managed as well as overall throughput. Efﬁciency becomes a concern because\na little waste per machine becomes a big waste when there are hundreds or thou-\nsands of machines. In IT, systems are often built for environments that expect a\nmodest increase in workload per year. In this case a workable strategy is to build\nthe system large enough to handle the projected workload for the next few years,\nwhen the system is expected to be replaced.\nAs a result of these requirements, systems in SRE tend to be bespoke systems,\nbuilt on platforms that are home-grown or integrated from open source or other\nthird-party components. They are not “off the shelf” or turn key systems. They are\nactively managed, while IT systems may be unchanged from their initial delivery\nstate. Because of these differences, distributed computing services are best man-\naged by a separate team, with separate management, with bespoke operational\nand management practices.\nWhile there are many such differences, recently IT departments have begun to\nsee a demand for uptime and scalability similar to that seen in SRE environments.\nTherefore the management techniques from distributed computing are rapidly\nbeing adopted in the enterprise.\n7.1.2 Change versus Stability\nThere is a tension between the desire for stability and the desire for change. Oper-\nations teams tend to favor stability; developers desire change. Consider how each\ngroup is evaluated during end-of-the-year performance reviews. A developer is\npraised for writing code that makes it into production. Changes that result in a\n",
      "content_length": 2951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "150\nChapter 7\nOperations in a Distributed World\ntangible difference to the service are rewarded above any other accomplishment.\nTherefore, developers want new releases pushed into production often. Opera-\ntions, in contrast, is rewarded for achieving compliance with SLAs, most of which\nrelate to uptime. Therefore stability is the priority.\nA system starts at a baseline of stability. A change is then made. All changes\nhave some kind of a destabilizing effect. Eventually the system becomes stable\nagain, usually through some kind of intervention. This is called the change-\ninstability cycle.\nAll software roll-outs affect stability. A change may introduce bugs, which are\nﬁxed through workarounds and new software releases. A release that introduces\nno new bugs still creates a destabilizing effect due to the process of shifting work-\nloads away from machines about to be upgraded. Non-software changes also have\na destabilizing effect. A network change may make the local network less stable\nwhile the change propagates throughout the network.\nBecause of the tension between the operational desire for stability and the\ndeveloper desire for change, there must be mechanisms to reach a balance.\nOne strategy is to prioritize work that improves stability over work that adds\nnew features. For example, bug ﬁxes would have a higher priority than feature\nrequests. With this approach, a major release introduces many new features, the\nnext few releases focus on ﬁxing bugs, and then a new major release starts the cycle\nover again. If engineering management is pressured to focus on new features and\nneglect bug ﬁxes, the result is a system that slowly destabilizes until it spins out of\ncontrol.\nAnother strategy is to align the goals of developers and operational staff. Both\nparties become responsible for SLA compliance as well as the velocity (rate of\nchange) of the system. Both have a component of their annual review that is tied\nto SLA compliance and both have a portion tied to the on-time delivery of new\nfeatures.\nOrganizations that have been the most successful at aligning goals like this\nhave restructured themselves so that developers and operations work as one\nteam. This is the premise of the DevOps movement, which will be described in\nChapter 8.\nAnother strategy is to budget time for stability improvements and time for\nnew features. Software engineering organizations usually have a way to estimate\nthe size of a software request or the amount of time it is expected to take to com-\nplete. Each new release has a certain size or time budget; within that budget a\ncertain amount of stability-improvement work is allocated. The case study at the\nend of Section 2.2.2 is an example of this approach. Similarly, this allocation can be\nachieved by assigning dedicated people to stability-related code changes.\nThe budget can also be based on an SLA. A certain amount of instability is\nexpected each month, which is considered a budget. Each roll-out uses some of\nthe budget, as do instability-related bugs. Developers can maximize the number\n",
      "content_length": 3061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "7.1\nDistributed Systems Operations\n151\nof roll-outs that can be done each month by dedicating effort to improve the code\nthat causes this instability. This creates a positive feedback loop. An example of\nthis is Google’s Error Budgets, which are more fully explained in Section 19.4.\n7.1.3 Defining SRE\nThe core practices of SRE were reﬁned for more than 10 years at Google before\nbeing enumerated in public. In his keynote address at the ﬁrst USENIX SREcon,\nBenjamin Treynor Sloss (2014), Vice President of Site Reliability Engineering at\nGoogle, listed them as follows:\nSite Reliability Practices\n1. Hire only coders.\n2. Have an SLA for your service.\n3. Measure and report performance against the SLA.\n4. Use Error Budgets and gate launches on them.\n5. Have a common stafﬁng pool for SRE and Developers.\n6. Have excess Ops work overﬂow to the Dev team.\n7. Cap SRE operational load at 50 percent.\n8. Share 5 percent of Ops work with the Dev team.\n9. Oncall teams should have at least eight people at one location, or six people\nat each of multiple locations.\n10. Aim for a maximum of two events per oncall shift.\n11. Do a postmortem for every event.\n12. Postmortems are blameless and focus on process and technology, not people.\nThe ﬁrst principle for site reliability engineering is that SREs must be able to code.\nAn SRE might not be a full-time software developer, but he or she should be able\nto solve nontrivial problems by writing code. When asked to do 30 iterations of\na task, an SRE should do the ﬁrst two, get bored, and automate the rest. An SRE\nmust have enough software development experience to be able to communicate\nwith developers on their level and have an appreciation for what developers do,\nand for what computers can and can’t do.\nWhen SREs and developers come from a common stafﬁng pool, that means\nthat projects are allocated a certain number of engineers; these engineers may be\ndevelopers or SREs. The end result is that each SRE needed means one fewer devel-\noper in the team. Contrast this to the case at most companies where system adminis-\ntrators and developers are allocated from teams with separate budgets. Rationally a\nproject wants to maximize the number of developers, since they write new features.\nThe common stafﬁng pool encourages the developers to create systems that can be\noperated efﬁciently so as to minimize the number of SREs needed.\n",
      "content_length": 2383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "152\nChapter 7\nOperations in a Distributed World\nAnother way to encourage developers to write code that minimizes opera-\ntional load is to require that excess operational work overﬂows to the developers.\nThis practice discourages developers from taking shortcuts that create undue oper-\national load. The developers would share any such burden. Likewise, by requiring\ndevelopers to perform 5 percent of operational work, developers stay in tune with\noperational realities.\nWithin the SRE team, capping the operational load at 50 percent limits the\namount of manual labor done. Manual labor has a lower return on investment than,\nfor example, writing code to replace the need for such labor. This is discussed in\nSection 12.4.2, “Reducing Toil.”\nMany SRE practices relate to ﬁnding balance between the desire for change\nand the need for stability. The most important of these is the Google SRE practice\ncalled Error Budgets, explained in detail in Section 19.4.\nCentral to the Error Budget is the SLA. All services must have an SLA, which\nspeciﬁes how reliable the system is going to be. The SLA becomes the standard by\nwhich all work is ultimately measured. SLAs are discussed in Chapter 16.\nAny outage or other major SLA-related event should be followed by the cre-\nation of a written postmortem that includes details of what happened, along with\nanalysis and suggestions for how to prevent such a situation in the future. This\nreport is shared within the company so that the entire organization can learn from\nthe experience. Postmortems focus on the process and the technology, not ﬁnd-\ning who to blame. Postmortems are the topic of Section 14.3.2. The person who is\noncall is responsible for responding to any SLA-related events and producing the\npostmortem report.\nOncall is not just a way to react to problems, but rather a way to reduce future\nproblems. It must be done in a way that is not unsustainably stressful for those\noncall, and it drives behaviors that encourage long-term ﬁxes and problem pre-\nvention. Oncall teams are made up of at least eight members at one location, or\nsix members at two locations. Teams of this size will be oncall often enough that\ntheir skills do not get stale, and their shifts can be short enough that each catches\nno more than two outage events. As a result, each member has enough time to fol-\nlow through on each event, performing the required long-term solution. Managing\noncall this way is the topic of Chapter 14.\nOther companies have adopted the SRE job title for their system administra-\ntors who maintain live production services. Each company applies a different set\nof practices to the role. These are the practices that deﬁne SRE at Google and are\ncore to its success.\n7.1.4 Operations at Scale\nOperations in distributed computing is operations at a large scale. Distributed com-\nputing involves hundreds and often thousands of computers working together. As\na result, operations is different than traditional computing administration.\n",
      "content_length": 2992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "7.1\nDistributed Systems Operations\n153\nManual processes do not scale. When tasks are manual, if there are twice as\nmany tasks, there is twice as much human effort required. A system that is scaling\nto thousands of machines, servers, or processes, therefore, becomes untenable if\na process involves manually manipulating things. In contrast, automation does\nscale. Code written once can be used thousands of times. Processes that involve\nmany machines, processes, servers, or services should be automated. This idea\napplies to allocating machines, conﬁguring operating systems, installing software,\nand watching for trouble. Automation is not a “nice to have” but a “must have.”\n(Automation is the subject of Chapter 12.)\nWhen operations is automated, system administration is more like an assem-\nbly line than a craft. The job of the system administrator changes from being the\nperson who does the work to the person who maintains the robotics of an assembly\nline. Mass production techniques become viable and we can borrow operational\npractices from manufacturing. For example, by collecting measurements from\nevery stage of production, we can apply statistical analysis that helps us improve\nsystem throughput. Manufacturing techniques such as continuous improvement\nare the basis for the Three Ways of DevOps. (See Section 8.2.)\nThree categories of things are not automated: things that should be automated\nbut have not been yet, things that are not worth automating, and human processes\nthat can’t be automated.\nTasks That Are Not Yet Automated\nIt takes time to create, test, and deploy automation, so there will always be things\nthat are waiting to be automated. There is never enough time to automate every-\nthing, so we must prioritize and choose our methods wisely. (See Section 2.2.2 and\nSection 12.1.1.)\nFor processes that are not, or have not yet been, automated, creating proce-\ndural documentation, called a playbook, helps make the process repeatable and\nconsistent. A good playbook makes it easier to automate the process in the future.\nOften the most difﬁcult part of automating something is simply describing the\nprocess accurately. If a playbook does that, the actual coding is relatively easy.\nTasks That Are Not Worth Automating\nSome things are not worth automating because they happen infrequently, they are\ntoo difﬁcult to automate, or the process changes so often that automation is not pos-\nsible. Automation is an investment in time and effort and the return on investment\n(ROI) does not always make automation viable.\nNevertheless, there are some common cases that are worth automating. Often\nwhen those are automated, the more rare cases (edge cases) can be consolidated or\neliminated. In many situations, the newly automated common case provides such\nsuperior service that the edge-case customers will suddenly lose their need to be\nso unique.\n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "154\nChapter 7\nOperations in a Distributed World\n.\nBenefits of Automating the Common Case\nAt one company there were three ways that virtual machines were being pro-\nvisioned. All three were manual processes, and customers often waited days\nuntil a system administrator was available to do the task. A project to automate\nprovisioning was stalled because of the complexity of handling all three vari-\nations. Users of the two less common cases demanded that their provisioning\nprocess be different because they were (in their own eyes) unique and beau-\ntiful snowﬂakes. They had very serious justiﬁcations based on very serious\n(anecdotal) evidence and waved their hands vigorously to prove their point.\nTo get the project moving, it was decided to automate just the most common\ncase and promise the two edge cases would be added later.\nThis was much easier to implement than the original all-singing, all-\ndancing, provisioning system. With the initial automation, provisioning time\nwas reduced to a few minutes and could happen without system administra-\ntor involvement. Provisioning could even happen at night and on weekends.\nAt that point an amazing thing happened. The other two cases suddenly dis-\ncovered that their uniqueness had vanished! They adopted the automated\nmethod. The system administrators never automated the two edge cases and\nthe provisioning system remained uncomplicated and easy to maintain.\nTasks That Cannot Be Automated\nSome tasks cannot be automated because they are human processes: maintaining\nyour relationship with a stakeholder, managing the bidding process to make a large\npurchase, evaluating new technology, or negotiating within a team to assemble an\noncall schedule. While they cannot be eliminated through automation, they can be\nstreamlined:\n• Many interactions with stakeholders can be eliminated through better\ndocumentation. Stakeholders can be more self-sufﬁcient if provided with\nintroductory documentation, user documentation, best practices recommen-\ndations, a style guide, and so on. If your service will be used by many other\nservices or service teams, it becomes more important to have good documen-\ntation. Video instruction is also useful and does not require much effort if you\nsimply make a video recording of presentations you already give.\n• Some interactions with stakeholders can be eliminated by making common\nrequests self-service. Rather than meeting individually with customers to\nunderstand future capacity requirements, their forecasts can be collected via a\nweb user interface or an API. For example, if you provide a service to hundreds\n",
      "content_length": 2605,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "7.2\nService Life Cycle\n155\nof other teams, forecasting can be become a full-time job for a project manager;\nalternatively, it can be very little work with proper automation that integrates\nwith the company’s supply-chain management system.\n• Evaluating new technology can be labor intensive, but if a common case is\nidentiﬁed, the end-to-end process can be turned into an assembly-line process\nand optimized. For example, if hard drives are purchased by the thousand, it is\nwise to add a new model to the mix only periodically and only after a thorough\nevaluation. The evaluation process should be standardized and automated,\nand results stored automatically for analysis.\n• Automation can replace or accelerate team processes. Creating the oncall\nschedule can evolve into a chaotic mess of negotiations between team mem-\nbers battling to take time off during an important holiday. Automation turns\nthis into a self-service system that permits people to list their availability and\nthat churns out an optimal schedule for the next few months. Thus, it solves\nthe problem better and reduces stress.\n• Meta-processes such as communication, status, and process tracking can be\nfacilitated through online systems. As teams grow, just tracking the interac-\ntion and communication among all parties can become a burden. Automating\nthat can eliminate hours of manual work for each person. For example, a web-\nbased system that lets people see the status of their order as it works its way\nthrough approval processes eliminates the need for status reports, leaving\npeople to deal with just exceptions and problems. If a process has many com-\nplex handoffs between teams, a system that provides a status dashboard and\nautomatically notiﬁes teams when hand-offs happen can reduce the need for\nlegions of project managers.\n• The best process optimization is elimination. A task that is eliminated does not\nneed to be performed or maintained, nor will it have bugs or security ﬂaws.\nFor example, if production machines run three different operating systems,\nnarrowing that number down to two eliminates a lot of work. If you provide a\nservice to other service teams and require a lengthy approval process for each\nnew team, it may be better to streamline the approval process by automatically\napproving certain kinds of users.\n7.2 Service Life Cycle\nOperations is responsible for the entire service life cycle: launch, maintenance\n(both regular and emergency), upgrades, and decommissioning. Each phase\nhas unique requirements, so you’ll need a strategy for managing each phase\ndifferently.\n",
      "content_length": 2579,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "156\nChapter 7\nOperations in a Distributed World\nThe stages of the life cycle are:\n• Service Launch: Launching a service the ﬁrst time. The service is brought to\nlife, initial customers use it, and problems that were not discovered prior to\nthe launch are discovered and remedied. (Section 7.2.1)\n• Emergency Tasks: Handling exceptional or unexpected events. This includes\nhandling outages and, more importantly, detecting and ﬁxing conditions that\nprecipitate outages. (Chapter 14)\n• Nonemergency Tasks: Performing all manual work required as part of the\nnormally functioning system. This may include periodic (weekly or monthly)\nmaintenance tasks (for example, preparation for monthly billing events) as\nwell as processing requests from users (for example, requests to enable the\nservice for use by another internal service or team). (Section 7.3)\n• Upgrades: Deploying new software releases and hardware platforms. The bet-\nter we can do this, the more aggressively the company can try new things and\ninnovate. Each new software release is built and tested before deployment.\nTests include system tests, done by developers, as well as user acceptance\ntests (UAT), done by operations. UAT might include tests to verify there are\nno performance regressions (unexpected declines in performance). Vulner-\nability assessments are done to detect security issues. New hardware must\ngo through a hardware qualification to test for compatibility, performance\nregressions, and any changes in operational processes. (Section 10.2)\n• Decommissioning: Turning off a service. It is the opposite of a service launch:\nremoving the remaining users, turning off the service, removing references to\nthe service from any related service conﬁgurations, giving back any resources,\narchiving old data, and erasing or scrubbing data from any hardware before\nit is repurposed, sold, or disposed. (Section 7.2.2)\n• Project Work: Performing tasks large enough to require the allocation of\ndedicated resources and planning. While not directly part of the service life\ncycle, along the way tasks will arise that are larger than others. Examples\ninclude ﬁxing a repeating but intermittent failure, working with stakehold-\ners on roadmaps and plans for the product’s future, moving the service to a\nnew datacenter, and scaling the service in new ways. (Section 7.3)\nMost of the life-cycle stages listed here are covered in detail elsewhere in this book.\nService launches and decommissioning are covered in detail next.\n7.2.1 Service Launches\nNothing is more embarrassing than the failed public launch of a new service. Often\nwe see a new service launch that is so successful that it receives too much trafﬁc,\nbecomes overloaded, and goes down. This is ironic but not funny.\n",
      "content_length": 2745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "7.2\nService Life Cycle\n157\nEach time we launch a new service, we learn something new. If we launch new\nservices rarely, then remembering those lessons until the next launch is difﬁcult.\nTherefore, if launches are rare, we should maintain a checklist of things to do and\nrecord the things you should remember to do next time. As the checklist grows\nwith each launch, we become better at launching services.\nIf we launch new services frequently, then there are probably many peo-\nple doing the launches. Some will be less experienced than others. In this case\nwe should maintain a checklist to share our experience. Every addition increases\nour organizational memory, the collection of knowledge within our organization,\nthereby making the organization smarter.\nA common problem is that other teams may not realize that planning a launch\nrequires effort. They may not allocate time for this effort and surprise operations\nteams at or near the launch date. These teams are unaware of all the potential pitfalls\nand problems that the checklist is intended to prevent. For this reason the launch\nchecklist should be something mentioned frequently in documentation, socialized\namong product managers, and made easy to access. The best-case scenario occurs\nwhen a service team comes to operations wishing to launch something and has been\nusing the checklist as a guide throughout development. Such a team has “done their\nhomework”; they have been working on the items in the checklist in parallel as the\nproduct was being developed. This does not happen by accident; the checklist must\nbe available, be advertised, and become part of the company culture.\nA simple strategy is to create a checklist of actions that need to be completed\nprior to launch. A more sophisticated strategy is for the checklist to be a series\nof questions that are audited by a Launch Readiness Engineer (LRE) or a Launch\nCommittee.\nHere is a sample launch readiness review checklist:\nSample Launch Readiness Review Survey\nThe purpose of this document is to gather information to be evaluated by a Launch Readi-\nness Engineer (LRE) when approving the launch of a new service. Please complete the\nsurvey prior to meeting with your LRE.\n• General Launch Information:\n– What is the service name?\n– When is the launch date/time?\n– Is this a soft or hard launch?\n• Architecture:\n– Describe the system architecture. Link to architecture documents if possible.\n– How does the failover work in the event of single-machine, rack, and\ndatacenter failure?\n– How is the system designed to scale under normal conditions?\n",
      "content_length": 2576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "158\nChapter 7\nOperations in a Distributed World\n• Capacity:\n– What is the expected initial volume of users and QPS?\n– How was this number arrived at? (Link to load tests and reports.)\n– What is expected to happen if the initial volume is 2× expected? 5×? (Link\nto emergency capacity documents.)\n– What is the expected external (internet) bandwidth usage?\n– What are the requirements for network and storage after 1, 3, and 12\nmonths? (Link to conﬁrmation documents from the network and storage\nteams capacity planner.)\n• Dependencies:\n– Which systems does this depend on? (Link to dependency/data ﬂow\ndiagram.)\n– Which RPC limits are in place with these dependencies? (Link to limits and\nconﬁrmation from external groups they can handle the trafﬁc.)\n– What will happen if these RPC limits are exceeded ?\n– For each dependency, list the ticket number where this new service’s use\nof the dependency (and QPS rate) was requested and positively acknowl-\nedged.\n• Monitoring:\n– Are all subsystems monitored? Describe the monitoring strategy and doc-\nument what is monitored.\n– Does a dashboard exist for all major subsystems?\n– Do metrics dashboards exist? Are they in business, not technical, terms?\n– Was the number of “false alarm” alerts in the last month less than x?\n– Is the number of alerts received in a typical week less than x?\n• Documentation:\n– Does a playbook exist and include entries for all operational tasks and\nalerts?\n– Have an LRE review each entry for accuracy and completeness.\n– Is the number of open documentation-related bugs less than x?\n• Oncall:\n– Is the oncall schedule complete for the next n months?\n– Is the oncall schedule arranged such that each shift is likely to get fewer\nthan x alerts?\n• Disaster Preparedness:\n– What is the plan in case ﬁrst-day usage is 10 times greater than expected?\n– Do backups work and have restores been tested?\n• Operational Hygiene:\n– Are “spammy alerts” adjusted or corrected in a timely manner?\n",
      "content_length": 1958,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "7.2\nService Life Cycle\n159\n– Are bugs ﬁled to raise visibility of issues—even minor annoyances or issues\nwith commonly known workarounds?\n– Do stability-related bugs take priority over new features?\n– Is a system in place to assure that the number of open bugs is kept low?\n• Approvals:\n– Has marketing approved all logos, verbiage, and URL formats?\n– Has the security team audited and approved the service?\n– Has a privacy audit been completed and all issues remediated?\nBecause a launch is complex, with many moving parts, we recommend that a single\nperson (the launch lead) take a leadership or coordinator role. If the developer\nand operations teams are very separate, one person from each might be selected to\nrepresent each team.\nThe launch lead then works through the checklist, delegating work, ﬁling bugs\nfor any omissions, and tracking all issues until launch is approved and executed.\nThe launch lead may also be responsible for coordinating post-launch problem\nresolution.\n.\nCase Study: Self-Service Launches at Google\nGoogle launches so many services that it needed a way to make the launch pro-\ncess streamlined and able to be initiated independently by a team. In addition\nto providing APIs and portals for the technical parts, the Launch Readiness\nReview (LRR) made the launch process itself self-service.\nThe LRR included a checklist and instructions on how to achieve each\nitem. An SRE engineer was assigned to shepherd the team through the process\nand hold them to some very high standards.\nSome checklist items were technical—for example, making sure that the\nGoogle load balancing system was used properly. Other items were caution-\nary, to prevent a launch team from repeating other teams’ past mistakes. For\nexample, one team had a failed launch because it received 10 times more\nusers than expected. There was no plan for how to handle this situation. The\nLRR checklist required teams to create a plan to handle this situation and\ndemonstrate that it had been tested ahead of time.\nOther checklist items were business related. Marketing, legal, and other\ndepartments were required to sign off on the launch. Each department had\nits own checklist. The SRE team made the service visible externally only after\nverifying that all of those sign-offs were complete.\n",
      "content_length": 2284,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "160\nChapter 7\nOperations in a Distributed World\n7.2.2 Service Decommissioning\nDecommissioning (or just “decomm”), or turning off a service, involves three major\nphases: removal of users, deallocation of resources, and disposal of resources.\nRemoving users is often a product management task. Usually it involves mak-\ning the users aware that they must move. Sometimes it is a technical issue of\nmoving them to another service. User data may need to be moved or archived.\nResource deallocation can cover many aspects. There may be DNS entries to\nbe removed, machines to power off, database connections to be disabled, and so on.\nUsually there are complex dependencies involved. Often nothing can begin until\nthe last user is off the service; certain resources cannot be deallocated before others,\nand so on. For example, typically a DNS entry is not removed until the machine is\nno longer in use. Network connections must remain in place if deallocating other\nservices depends on network connectivity.\nResource disposal includes securely erasing disks and other media and dis-\nposing of all hardware. The hardware may be repurposed, sold, or scrapped.\nIf decommissioning is done incorrectly or items are missed, resources will\nremain allocated. A checklist, that is added to over time, will help assure decom-\nmissioning is done completely and the tasks are done in the right order.\n7.3 Organizing Strategy for Operational Teams\nAn operational team needs to get work done. Therefore teams need a strategy that\nassures that all incoming work is received, scheduled, and completed. Broadly\nspeaking, there are three sources of operational work and these work items fall\ninto three categories. To understand how to best organize a team, ﬁrst you must\nunderstand these sources and categories.\nThe three sources of work are life-cycle management, interacting with stake-\nholders, and process improvement and automation. Life-cycle management is the\noperational work involved in running the service. Interacting with stakeholders\nrefers to both maintaining the relationship with people who use and depend on\nthe service, and prioritizing and fulﬁlling their requests. Process improvement and\nautomation is work inspired by the business desire for continuous improvement.\nNo matter the source, this work tends to fall into one of these three broad\ncategories:\n• Emergency Issues: Outages, and issues that indicate a pending outage that can\nbe prevented, and emergency requests from other teams. Usually initiated by\nan alert sent by the monitoring system via SMS or pager. (Chapter 14)\n",
      "content_length": 2578,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "7.3\nOrganizing Strategy for Operational Teams\n161\n• Normal Requests: Process work (repeatable processes that have not yet been\nautomated), non-urgent trouble reports, informational questions, and initial\nconsulting that results in larger projects. Usually initiated by a request ticket\nsystem. (Section 14.1.3)\n• Project Work: Small and large projects that evolve the system. Managed with\nwhatever project management style the team selects. (Section 12.4.2)\nTo assure that all sources and categories of work receive attention, we recommend\nthis simple organizing principle: people should always be working on projects,\nwith exceptions made to assure that emergency issues receive immediate attention\nand non-project customer requests are triaged and worked in a timely manner.\nMore speciﬁcally, at any given moment, the highest priority for one person on\nthe team should be responding to emergencies, the highest priority for one other\nperson on the team should be responding to normal requests, and the rest of the\nteam should be focused on project work.\nThis is counter to the way operations teams often work: everyone running\nfrom emergency to emergency with no time for project work. If there is no effort\ndedicated to improving the situation, the team will simply run from emergency to\nemergency until they are burned out.\nMajor improvements come from project work. Project work requires concen-\ntration and focus. If you are constantly being interrupted with emergency issues\nand requests, you will not be able to get projects done. If an entire team is focused\non emergencies and requests, nobody is working on projects.\nIt can be tempting to organize an operations team into three subteams, each\nfocusing on one source of work or one category of work. Either of these approaches\nwill create silos of responsibility. Process improvement is best done by the people\ninvolved in the process, not by observers.\nTo implement our recommended strategy, all members of the team focus on\nproject work as their main priority. However, team members take turns being\nresponsible for emergency issues as they arise. This responsibility is called oncall.\nLikewise, team members take turns being responsible for normal requests from\nother teams. This responsibility is called ticket duty.\nIt is common that oncall duty and ticket duty are scheduled in a rotation.\nFor example, a team of eight people may use an eight-week cycle. Each person is\nassigned a week where he or she is on call: expected to respond to alerts, spending\nany remaining time on projects. Each person is also assigned a different week\nwhere he or she is on ticket duty: expected to focus on triaging and responding\nto request tickets ﬁrst, working on other projects only if there is remaining time.\nThis gives team members six weeks out of the cycle that can be focused on project\nwork.\n",
      "content_length": 2850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "162\nChapter 7\nOperations in a Distributed World\nLimiting each rotation to a speciﬁc person makes for smoother handoffs to the\nnext shift. In such a case, there are two people doing the handoff rather than a large\noperations team meeting. If more than 25 percent of a team needs to be dedicated\nto ticket duty and oncall, there is a serious problem with ﬁreﬁghting and a lack of\nautomation.\nThe team manager should be part of the operational rotation. This practice\nensures the manager is aware of the operational load and ﬁreﬁghting that goes\non. It also ensures that nontechnical managers don’t accidentally get hired into the\noperations organization.\nTeams may combine oncall and ticket duty into one position if the amount of\nwork in those categories is sufﬁciently small. Some teams may need to designate\nmultiple people to ﬁll each role.\nProject work is best done in small teams. Solo projects can damage a team by\nmaking members feel disconnected or by permitting individuals to work without\nconstructive feedback. Designs are better with at least some peer review. Without\nfeedback, members may end up working on projects they feel are important but\nhave marginal beneﬁt. Conversely, large teams often get stalled by lack of consen-\nsus. In their case, focusing on shipping quickly overcomes many of these problems.\nIt helps by making progress visible to the project members, the wider team, and\nmanagement. Course corrections are easier to make when feedback is frequent.\nThe Agile methodology, discussed in Section 8.6, is an effective way to\norganize project work.\n.\nMeta-work\nThere is also meta-work: meetings, status reports, company functions. These\ngenerally eat into project time and should be minimized. For advice, see\nChapter 11, “Eliminating Time Wasters,” in the book Time Management for\nSystem Administrators by Limoncelli (2005).\n7.3.1 Team Member Day Types\nNow that we have established an organizing principle for the team’s work, each\nteam member can organize his or her work based on what kind of day it is: a\nproject-focused day, an oncall day, or a ticket duty day.\nProject-Focused Days\nMost days should be project days for operational staff. Speciﬁcally, most days\nshould be spent developing software that automates or optimizes aspects of the\nteam’s responsibilities. Non-software projects include shepherding a new launch\nor working with stakeholders on requirements for future releases.\n",
      "content_length": 2418,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "7.3\nOrganizing Strategy for Operational Teams\n163\nOrganizing the work of a team through a single bug tracking system has the\nbeneﬁt of reducing time spent checking different systems for status. Bug tracking\nsystems provide an easy way for people to prioritize and track their work. On a\ntypical project day the staff member starts by checking the bug tracking system to\nreview the bugs assigned to him or her, or possibly to review unassigned issues of\nhigher priority the team member might need to take on.\nSoftware development in operations tends to mirror the Agile methodology:\nrather than making large, sudden changes, many small projects evolve the system\nover time. Chapter 12 will discuss automation and software engineering topics in\nmore detail.\nProjects that do not involve software development may involve technical\nwork. Moving a service to a new datacenter is highly technical work that cannot\nbe automated because it happens infrequently.\nOperations staff tend not to physically touch hardware not just because of the\nheavy use of virtual machines, but also because even physical machines are located\nin datacenters that are located far away. Datacenter technicians act as remote\nhands, applying physical changes when needed.\nOncall Days\nOncall days are spent working on projects until an alert is received, usually by\nSMS, text message, or pager.\nOnce an alert is received, the issue is worked until it is resolved. Often there\nare multiple solutions to a problem, usually including one that will ﬁx the problem\nquickly but temporarily and others that are long-term ﬁxes. Generally the quick\nﬁx is employed because returning the service to normal operating parameters is\nparamount.\nOnce the alert is resolved, a number of other tasks should always be done. The\nalert should be categorized and annotated in some form of electronic alert jour-\nnal so that trends may be discovered. If a quick ﬁx was employed, a bug should\nbe ﬁled requesting a longer-term ﬁx. The oncall person may take some time to\nupdate the playbook entry for this alert, thereby building organizational mem-\nory. If there was a user-visible outage or an SLA violation, a postmortem report\nshould be written. An investigation should be conducted to ascertain the root\ncause of the problem. Writing a postmortem report, ﬁling bugs, and root causes\nidentiﬁcation are all ways that we raise the visibility of issues so that they get\nattention. Otherwise, we will continually muddle through ad hoc workarounds\nand nothing will ever get better. Postmortem reports (possibly redacted for tech-\nnical content) can be shared with the user community to build conﬁdence in the\nservice.\nThe beneﬁt of having a speciﬁc person assigned to oncall duty at any given\ntime is that it enables the rest of the team to remain focused on project work. Studies\nhave found that the key to software developer productivity is to have long periods\n",
      "content_length": 2906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "164\nChapter 7\nOperations in a Distributed World\nof uninterrupted time. That said, if a major crisis appears, the oncall person will\npull people away from their projects to assist.\nIf oncall shifts are too long, the oncall person will be overloaded with follow-\nup work. If the shifts are too close together, there will not be time to complete the\nfollow-up work. Many great ideas for new projects and improvements are ﬁrst\nimagined while servicing alerts. Between oncall shifts people should have enough\ntime to pursue such projects.\nChapter 14 will discuss oncall in greater detail.\nTicket Duty Days\nTicket duty days are spent working on requests from customers. Here the cus-\ntomers are the internal users of the service, such as other service teams that use\nyour service’s API. These are not tickets from external users. Those items should\nbe handled by customer support representatives.\nWhile oncall is expected to have very fast reaction time, tickets generally have\nan expected response time measured in days.\nTypical tickets may consist of questions about the service, which can lead to\nsome consulting on how to use the service. They may also be requests for activa-\ntion of a service, reports of problems or difﬁculties people are experiencing, and so\nforth. Sometimes tickets are created by automated systems. For example, a moni-\ntoring system may detect a situation that is not so urgent that it needs immediate\nresponse and may open a ticket instead.\nSome long-running tickets left from the previous shift may need follow-up.\nOften there is a policy that if we are waiting for a reply from the customer, every\nthree days the customer will be politely “poked” to make sure the issue is not for-\ngotten. If the customer is waiting for follow-up from us, there may be a policy that\nurgent tickets will have a status update posted daily, with longer stretches of time\nfor other priorities.\nIf a ticket will not be completed by the end of a shift, its status should be\nincluded in the shift report so that the next person can pick up where the previous\nperson left off.\nBy dedicating a person to ticket duty, that individual can be more focused\nwhile responding to tickets. All tickets can be triaged and prioritized. There is more\ntime to categorize tickets so that trends can be spotted. Efﬁciencies can be realized\nby batching up similar tickets to be done in a row. More importantly, by dedicating\na person to tickets, that individual should have time to go deeper into each ticket:\nto update documentation and playbooks along the way, to deep-dive into bugs\nrather than ﬁnd superﬁcial workarounds, to ﬁx complex broken processes. Ticket\nduty should not be a chore, but rather should be part of the strategy to reduce the\noverall work faced by the team.\nEvery operations team should have a goal of eliminating the need for people\nto open tickets with them, similar to how there should always be a goal to automate\n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "7.3\nOrganizing Strategy for Operational Teams\n165\nmanual processes. A ticket requesting information is an indication that documen-\ntation should be improved. It is best to respond to the question by adding the\nrequested information to the service’s FAQ or other user documentation and then\ndirecting the user to that document. Requests for service activation, allocations, or\nconﬁguration changes indicate an opportunity to create a web-based portal or API\nto make such requests obsolete. Any ticket created by an automated system should\nhave a corresponding playbook entry that explains how to process it, with a link\nto the bug ID requesting that the automation be improved to eliminate the need to\nopen such tickets.\nAt the end of oncall and ticket duty shifts, it is common for the person to email\nout a shift report to the entire team. This report should mention any trends noticed\nand any advice or status information to be passed on to the next person. The oncall\nend-of-shift report should also include a log of which alerts were received and what\nwas done in response.\nWhen you are oncall or doing ticket duty, that is your main project. Other\nproject work that is accomplished, if any, is a bonus. Management should not\nexpect other projects to get done, nor should people be penalized for having the\nproper focus. When people end their oncall or ticket duty time, they should not\ncomplain that they weren’t able to get any project work done; their project, so to\nspeak, was ticket duty.\n7.3.2 Other Strategies\nThere are many other ways to organize the work of a team. The team can rotate\nthough projects focused on a particular goal or subsystem, it can focus on reducing\ntoil, or special days can be set aside for reducing technical debt.\nFocus or Theme\nOne can pick a category of issues to focus on for a month or two, changing themes\nperiodically or when the current theme is complete. For example, at the start of a\ntheme, a number of security-related issues can be selected and everyone commit\nto focusing on them until they are complete. Once these items are complete, the\nnext theme begins. Some common themes include monitoring, a particular service\nor subservice, or automating a particular task.\nIf the team cohesion was low, this can help everyone feel as if they are work-\ning as a team again. It can also enhance productivity: if everyone has familiarized\nthemselves with the same part of the code base, everyone can do a better job of\nhelping each other.\nIntroducing a theme can also provide a certain amount of motivation. If the\nteam is looking forward to the next theme (because it is more interesting, novel, or\nfun), they will be motivated to meet the goals of the current theme so they can start\nthe next one.\n",
      "content_length": 2741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "166\nChapter 7\nOperations in a Distributed World\nToil Reduction\nToil is manual work that is particularly exhausting. If a team calculates the number\nof hours spent on toil versus normal project work, that ratio should be as low as\npossible. Management may set a threshold such that if it goes above 50 percent,\nthe team pauses all new features and works to solve the big problems that are the\nsource of so much toil. (See Section 12.4.2.)\nFix-It Days\nA day (or series of days) can be set aside to reduce technical debt. Technical debt is\nthe accumulation of small unﬁnished amounts of work. By themselves, these bits\nand pieces are not urgent, but the accumulation of them starts to become a problem.\nFor example, a Documentation Fix-It Day would involve everyone stopping all\nother work to focus on bugs related to documentation that needs to be improved.\nAlternatively, a Fix-It Week might be declared to focus on bringing all monitoring\nconﬁgurations up to a particular standard.\nOften teams turn ﬁx-its into a game. For example, at the start a list of tasks (or\nbugs) is published. Prizes are given out to the people who resolve the most bugs.\nIf done company-wide, teams may receive T-shirts for participating and/or prizes\nfor completing the most tasks.\n7.4 Virtual Office\nMany operations teams work from home rather than an ofﬁce. Since work is virtual,\nwith remote hands touching hardware when needed, we can work from anywhere.\nTherefore, it is common to work from anywhere. When necessary, the team meets\nin chat rooms or other virtual meeting spaces rather than physical meeting rooms.\nWhen teams work this way, communication must be more intentional because you\ndon’t just happen to see each other in the ofﬁce.\nIt is good to have a policy that anyone who is not working from the ofﬁce\ntakes responsibility for staying in touch with the team. They should clearly and\nperiodically communicate their status. In turn, the entire team should take respon-\nsibility for making sure remote workers do not feel isolated. Everyone should know\nwhat their team members are working on and take the time to include everyone in\ndiscussions. There are many tools that can help achieve this.\n7.4.1 Communication Mechanisms\nChat rooms are commonly used for staying in touch throughout the day. Chat\nroom transcripts should be stored and accessible so people can read what they\nmay have missed. There are many chat room “bots” (software robots that join the\n",
      "content_length": 2451,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "7.5\nSummary\n167\nchat room and provide services) that can provide transcription services, pass mes-\nsages to ofﬂine members, announce when oncall shifts change, and broadcast any\nalerts generated by the monitoring system. Some bots provide entertainment: At\nGoogle, a bot keeps track of who has received the most virtual high-ﬁves. At Stack\nExchange, a bot notices if anyone types the phrase “not my fault” and responds\nby selecting a random person from the room and announcing this person has been\nrandomly designated to be blamed.\nHigher-bandwidth communication systems include voice and video systems\nas well as screen sharing applications. The higher the bandwidth, the better the\nﬁdelity of communication that can be achieved. Text-chat is not good at conveying\nemotions, while voice and video can. Always switch to higher-ﬁdelity communi-\ncation systems when conveying emotions is more important, especially when an\nintense or heated debate starts.\nThe communication medium with the highest ﬁdelity is the in-person meet-\ning. Virtual teams greatly beneﬁt from periodic in-person meetings. Everyone\ntravels to the same place for a few days of meetings that focus on long-term\nplanning, team building, and other issues that cannot be solved online.\n7.4.2 Communication Policies\nMany teams establish a communication agreement that clariﬁes which methods\nwill be used in which situations. For example, a common agreement is that chat\nrooms will be the primary communication channel but only for ephemeral discus-\nsions. If a decision is made in the chat room or an announcement needs to be made,\nit will be broadcast via email. Email is for information that needs to carry across\noncall shifts or day boundaries. Announcements with lasting effects, such as major\npolicies or design decisions, need to be recorded in the team wiki or other docu-\nment system (and the creation of said document needs to be announced via email).\nEstablishing this chat–email–document paradigm can go a long way in reducing\ncommunication problems.\n7.5 Summary\nOperations is different from typical enterprise IT because it is focused on a par-\nticular service or group of services and because it has more demanding uptime\nrequirements.\nThere is a tension between the operations team’s desire for stability and the\ndevelopers’ desire to get new code into production. There are many ways to reach\na balance. Most ways involve aligning goals by sharing responsibility for both\nuptime and velocity of new features.\n",
      "content_length": 2491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "168\nChapter 7\nOperations in a Distributed World\nOperations in distributed computing is done at a large scale. Processes that\nhave to be done manually do not scale. Constant process improvement and\nautomation are essential.\nOperations is responsible for the life cycle of a service: launch, maintenance,\nupgrades, and decommissioning. Maintenance tasks include emergency and non-\nemergency response. In addition, related projects maintain and evolve the service.\nLaunches, decommissioning of services, and other tasks that are done infre-\nquently require an attention to detail that is best assured by use of checklists.\nChecklists ensure that lessons learned in the past are carried forward.\nThe most productive use of time for operational staff is time spent automating\nand optimizing processes. This should be their primary responsibility. In addition,\ntwo other kinds of work require attention. Emergency tasks need fast response.\nNonemergency requests need to be managed such that they are prioritized and\nworked in a timely manner. To make sure all these things happen, at any given\ntime one person on the operations team should be focused on responding to emer-\ngencies; another should be assigned to prioritizing and working on nonemergency\nrequests. When team members take turns addressing these responsibilities, they\nreceive the dedicated resources required to assure they happen correctly by sharing\nthe responsibility across the team. People also avoid burning out.\nOperations teams generally work far from the actual machines that run their\nservices. Since they operate the service remotely, they can work from anywhere\nthere is a network connection. Therefore teams often work from different places,\ncollaborating and communicating in a chat room or other virtual ofﬁce. Many tools\nare available to enable this type of organizational structure. In such an environ-\nment, it becomes important to change the communication medium based on the\ntype of communication required. Chat rooms are sufﬁcient for general commu-\nnication but voice and video are more appropriate for more intense discussions.\nEmail is more appropriate when a record of the communication is required, or if it\nis important to reach people who are not currently online.\nExercises\n1. What is operations? What are its major areas of responsibilities?\n2. How does operations in distributed computing differ from traditional desktop\nsupport or enterprise client–server support?\n3. Describe the service life cycle as it relates to a service you have experience\nwith.\n4. Section 7.1.2 discusses the change-instability cycle. Draw a series of graphs\nwhere the x-axis is time and the y-axis is the measure of stability. Each graph\nshould represent two months of project time.\n",
      "content_length": 2750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Exercises\n169\nEach Monday, a major software release that introduces instability (9 bugs) is\nrolled out. On Tuesday through Friday, the team has an opportunity to roll out\na “bug-ﬁx” release, each of which ﬁxes three bugs. Graph these scenarios:\n(a) No bug-ﬁx releases\n(b) Two bug-ﬁx releases after every major release\n(c) Three bug-ﬁx releases after every major release\n(d) Four bug-ﬁx releases after every major release\n(e) No bug-ﬁx release after odd releases, ﬁve bug-ﬁx releases after even\nreleases\n5. What do you observe about the graphs from Exercise 4?\n6. For a service you provide or have experience with, who are the stakeholders?\nWhich interactions did you or your team have with them?\n7. What are some of the ways operations work can be organized? How does this\ncompare to how your current team is organized?\n8. For a service you are involved with, give examples of work whose source is\nlife-cycle management, interacting with stakeholders, and process improve-\nment and automation.\n9. For a service you are involved with, give examples of emergency issues,\nnormal requests, and project work.\n",
      "content_length": 1104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "Chapter 8\nDevOps Culture\nThe opposite of DevOps\nis despair.\n—Gene Kim\nThis chapter is about the culture and set of practices known as “DevOps.” In\na DevOps organization, software developers and operational engineers work\ntogether as one team that shares responsibility for a web site or service. This is in\ncontrast to organizations where developers and operational personnel work inde-\npendently and often with conﬂicting goals. DevOps is the modern way to run web\nservices.\nDevOps combines some cultural and attitude shifts with some common-sense\nprocesses. Originally based on applying Agile methodology to operations, the\nresult is a streamlined set of principles and processes that can create reliable\nservices.\nAppendix B will make the case that cloud or distributed computing was the\ninevitable result of the economics of hardware. DevOps is the inevitable result of\nneeding to do efﬁcient operations in such an environment.\nIf hardware and software are sufﬁciently fault tolerant, the remaining prob-\nlems are human. The seminal paper “Why Do Internet Services Fail, and What\nCan Be Done about It?” by Oppenheimer et al. (2003) raised awareness that if web\nservices are to be a success in the future, operational aspects must improve:\nWe ﬁnd that (1) operator error is the largest single cause of failures in two of the three\nservices, (2) operator errors often take a long time to repair, (3) conﬁguration errors\nare the largest category of operator errors, (4) failures in custom-written front-end\nsoftware are signiﬁcant, and (5) more extensive online testing and more thoroughly\nexposing and detecting component failures would reduce failure rates in at least one\nservice.\n171\n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "172\nChapter 8\nDevOps Culture\nIn other words, technology has become so reliable that the remaining problems\nare in the processes used to manage it. We need better practices.\nThe canonical way to improve large-scale operations is through “quality man-\nagement” techniques such as W. Edwards Deming’s “Shewhart cycle” (Deming\n2000) or “Lean Manufacturing” (Spear & Bowen 1999). DevOps’s key principles\nare an application of these principles to web system administration. The book The\nPhoenix Project (Kim, Behr & Spafford 2013) explains these principles in the form\nof a ﬁctional story about a team that learns these principles as they reform a failing\nIT organization.\n8.1 What Is DevOps?\nDevOps is a combination of culture and practices—system administrators, soft-\nware developers, and web operations staff all contribute to the DevOps environ-\nment. With DevOps, sysadmins and developers share responsibility for a service\nand its availability. DevOps aligns the priorities of developers (dev) and sys-\ntem administrators or operations staff (ops) by making them both responsible for\nuptime. DevOps also brings all of the various environments, from development\nthrough test and production, under software version management and control.\nAt its most fundamental level, DevOps is about breaking down silos and remov-\ning bottlenecks and risks that screw up an organization’s Development to Opera-\ntions delivery lifecycle. The goal is to enable change to ﬂow quickly and reliably\nfrom speciﬁcation through to running features in a customer-facing environment.\n(Edwards 2012)\nDevOps is an emerging ﬁeld in operations. The practice of DevOps typically\nappears in web application and cloud environments, but its inﬂuence is spreading\nto all parts of all industries.\nDevOps is about improving operations. Theo Schlossnagle (2011) says\nDevOps is “the operationalism of the world.” Increasingly companies are putting\na greater importance on the operational part of their business. This is because of\nan increasing trend to be concerned with the total cost of ownership (TCO) of a\nproject, not just the initial purchase price, as well as increasing pressure to achieve\nhigher reliability and velocity of change. The ability to make changes is required\nto improve efﬁciency and to introduce new features and innovations. While tra-\nditionally change has been seen as a potential destabilizer, DevOps shows that\ninfrastructure change can be done rapidly and frequently in a way that increases\noverall stability.\nDevOps is not a job title; you cannot hire a “DevOp.” It is not a product;\nyou cannot purchase “DevOps software.” There are teams and organizations that\n",
      "content_length": 2655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "8.1\nWhat Is DevOps?\n173\nexhibit DevOps culture and practices. Many of the practices are aided by one\nsoftware package or another. But there is no box you can purchase, press the\nDevOps button, and magically “have” DevOps. Adam Jacob’s seminal “Choose\nYour Own Adventure” talk at Velocity 2010 (Jacob 2010) makes the case that\nDevOps is not a job description, but rather an inclusive movement that codiﬁes\na culture. In this culture everyone involved knows how the entire system works,\nand everyone is clear about the underlying business value they bring to the table.\nAs a result availability becomes the problem for the entire organization, not just\nfor the system administrators.\nDevOps is not just about developers and system administrators. In his blog\npost “DevOps is not a technology problem. DevOps is a business problem,” Damon\nEdwards (2010) emphasizes that DevOps is about collaboration and optimization\nacross the whole organization. DevOps expands to help the process from idea to\ncustomer. It isn’t just about leveraging cool new tools. In fact, it’s not just about\nsoftware.\nThe organizational changes involved in creating a DevOps environment are\nbest understood in contrast to the traditional software development approach. The\nDevOps approach evolved because of the drawbacks of such methods when devel-\noping custom web applications or cloud service offerings, and the need to meet the\nhigher availability requirements of these environments.\n8.1.1 The Traditional Approach\nFor software packages sold in shrink-wrapped packages at computer stores or\ndownloaded over the Internet, the developer is ﬁnished when the software is\ncomplete and ships. Operational concerns directly affect only the customer; the\ndeveloper is far removed from the operations. At best, operational problems may\nbe fed back to the developer in the form of bug reports or requests for enhancement.\nBut developers are not directly affected by operational issues caused by their code.\nTraditional software development uses the waterfall methodology, where\neach step—gather requirements, design, implement, test, verify, and deploy—is\ndone by a different team, each in isolation from the other steps. Each step (team)\nproduces a deliverable to be handed off to the next step (team).\nThis is called “waterfall development” because the steps look like a cascading\nwaterfall (see Figure 8.1). Information ﬂows down, like the water.\nIt is impossible to understand the operational requirements of a system until\nat least the design is complete, or in many cases until it is deployed and in active\nuse. Therefore the operational requirements cannot be taken into account in the\nrequirements gathering stage, after which the features are “locked.” Thus the result\nof this approach is that operational requirements are not considered until it is too\nlate to do anything about them.\n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "174\nChapter 8\nDevOps Culture\nFigure 8.1: The waterfall methodology: information ﬂows down. Unidirectional\ninformation ﬂows are the antithesis of DevOps.\nIn the waterfall method, system administrators are involved only in deploy-\ning the software, and thereafter are solely responsible for operations and meeting\nuptime requirements. System administrators have very little chance of inﬂuencing\nthe development of the software to better meet their needs. In many cases they\nhave no direct contact with the software developers.\nCompanies that have a business model based on the web, or a signiﬁcant web\npresence, develop their own custom software. Traditionally there was very little\ninteraction between software developers and system administrators even if they\nworked for the same company. They worked in “silos,” with each group unaware\nof the concerns of the other, and neither side seeing the “big picture.” In an orga-\nnizational chart, their hierarchies might meet only at the CEO level. The software\ndevelopers continued to develop software in isolation, without a motivating sense\nof its future use, and the sysadmins continued to struggle to meet high availability\nrequirements with buggy software.\nIn such a situation, operations is generally improved through ad hoc solutions,\nby working around problems and creating optimizations that are limited in scope.\nExcellence in operational efﬁciency, performance, and uptime is hindered by this\n",
      "content_length": 1448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "8.1\nWhat Is DevOps?\n175\napproach. Alas, the waterfall approach is from a time when we didn’t know any\nbetter.1\n8.1.2 The DevOps Approach\nCompanies with a business model based on the web found that traditional soft-\nware development practices did not work well for meeting very high availability\nrequirements. Operational concerns are key for high availability, so these compa-\nnies needed a new approach with tightly coupled development and operations.\nA new set of practices evolved as a result, and the term “DevOps” was coined to\ndescribe them. (See Appendix B for a historical perspective.)\nWeb-based companies typically introduce new features much more frequently\nthan those that sell packaged software. Since it is so easy for end users to switch\nfrom one web search engine to another, for example, these companies need to keep\nimproving their products to maintain their customer base. Also, web companies\nrelease more often because they can—they don’t have to manufacture physical\nmedia and distribute it to a customer. With the traditional packaged software\napproach, each new release is viewed as having a destabilizing inﬂuence—it is a\nsource of new, unknown bugs.\nIn a DevOps environment, developers and sysadmins share the responsibility\nfor meeting uptime requirements, so much so that both share oncall duties. Devel-\nopers have a vested interest in making sure that their software can meet the high\navailability requirements of the site. Developers collaborate in creating operational\nstrategies, and operations staff work closely with developers to provide implemen-\ntation and development input. Development and operations are both handled by\na single team, with developers and sysadmins participating in all stages and being\njointly responsible for the ﬁnal result. The development cycle should be a seam-\nless set of procedures and processes that result in a ﬁnished product—the service.\nThere is no concept of “them,” as in “hand off to them”; there is only “us”—the\nteam working on the product. Team members are largely generalists with deep\nspecialties.\nMost DevOps organizations are focused on clear business objectives such as\nscale, efﬁciency, and high uptime. By emphasizing people and process over ad hoc\ntool use, DevOps allows tight alignment of operations with business needs and\nthus with customer needs.\n1.\nOr did we? Royce’s 1970 paper, which is credited with “inventing” the model, actually identiﬁes it\nso Royce can criticize it and suggest improvements. He wrote it is “risky and invites failure” because\n“design iterations are never conﬁned to the successive step.” What Royce suggests as an alternative\nis similar to what we now call Agile. Sadly, multiple generations of software developers have had to\nsuffer through waterfall projects thanks to people who, we can only assume, didn’t read the entire paper\n(Pfeiffer 2012).\n",
      "content_length": 2864,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "176\nChapter 8\nDevOps Culture\nBy mandating that operations, development, and business departments work\ntogether, the operations process in a DevOps environment becomes a shared\nresponsibility that can respond more quickly and efﬁciently to needs of the service\nbeing operated. The result of the DevOps approach is higher uptime and lower\noperational costs.\n8.2 The Three Ways of DevOps\n“The Three Ways of DevOps” is a strategy for improving operations. It describes\nthe values and philosophies that frame the processes, procedures, and practices of\nDevOps. The Three Ways strategy was popularized by Kim et al.’s (2013) book The\nPhoenix Project. It borrows from “Lean Manufacturing” (Spear & Bowen 1999) and\nthe Toyota Production System’s Kaizen improvement model.\n8.2.1 The First Way: Workflow\nWorkﬂow looks at getting the process correct from beginning to end and improv-\ning the speed at which the process can be done. The process is a value stream—it\nprovides value to the business. The speed is referred to as ﬂow rate or just simply\nﬂow.\nIf the steps in the process are listed on a timeline, one can think of this as\nimproving the process as it moves from left to right. On the left is the business\n(development) and on the right is the customer (operations).\nFor example, a software release process has multiple stages: code is committed\nto a code repository, unit-tested, packaged, integration-tested, and deployed into\nproduction. To put an emphasis on getting the process correct from start to end:\n• Ensure each step is done in a repeatable way. Haphazard and ad hoc steps\nare replaced with repeatable processes.\n• Never pass defects to the next step. Testing is done as early as possible rather\nthan only on the ﬁnal product. Each step has validation or quality assurance\nchecks.\n• Ensure no local optimizations degrade global performance. For example, it\nmight be faster to not package the software but instead have each step pull\nthe software from the source repository. This saves time for the developers\nbecause it eliminates a step for them. At the same time, it introduces uncer-\ntainty that the remaining steps will all be working with the same exact bits,\ncausing confusion and increasing errors. Therefore it is a global regression and\nwe would not do it.\n• Increase the flow of work. Now that the steps are done in a repeatable\nway, the process can be analyzed and improved. For example, steps could\n",
      "content_length": 2421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "8.2\nThe Three Ways of DevOps\n177\nbe automated to improve speed. Alternatively there may be steps where work\nis redone multiple times; the duplicate work can be eliminated.\n8.2.2 The Second Way: Improve Feedback\nA feedback loop is established when information (a complaint or request) is com-\nmunicated upstream or downstream. Amplifying feedback loops means making\nsure that what is learned while going from left (dev) to right (ops) is communicated\nback to the left and through the system again. Feedback (information about prob-\nlems, concerns, or potential improvements) is made visible rather than hidden. As\nwe move from left to right, we learn things; if the lessons learned are thrown away\nat the end, we have missed an opportunity to improve the system. Conversely, if\nwhat we learn is ampliﬁed and made visible, it can be used to improve the system.\nContinuing our software release example, to put an emphasis on amplifying\nfeedback loops:\n• Understand and respond to all customers, internal and external. Each step is\na customer of the previous steps in addition to the obvious “customer” at the\nend of the process. Understanding the customer means understanding what\nthe subsequent steps need. Responding means there is a way for the customer\nto communicate and a way to assure that the request is responded to.\n• Shorten feedback loops. Shortening a feedback loop means making the com-\nmunication as direct as possible. The more stages a message must pass through\nto communicate, the less effective it will be. If the feedback is given to a\nmanager, who types it up and presents it to a vice president, who commu-\nnicates it down to a manager, who tells an engineer, you know you have too\nmany steps. The loop is as short as possible if the person who experienced\nthe problem is able to directly communicate it to the person who can ﬁx the\nproblem.\n• Amplify all feedback. The opposite would be someone noticing a problem\nand muddling through it with their own workaround. The person may think\nhe or she is being a hero for working around the problem, but actually the indi-\nvidual is hiding the problem and preventing it from being ﬁxed. Amplifying\nfeedback makes the issue more visible. It can be as simple as ﬁling a bug report\nor as dramatic as stopping the process until a management decision is made\nwith regard to how to proceed. When all feedback is brought to the surface,\nthen we have the most information available to improve a process.\n• Embed knowledge where it is needed. Specialized knowledge such as con-\nﬁguration information or business requirements is “embedded” in the process\nthrough the use of appropriate documentation and managed via source code\ncontrol. As you move from left to right in the process, the details of what is\n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "178\nChapter 8\nDevOps Culture\nneeded are available at every stage and do not require going outside of the\nloop to acquire them.\n8.2.3 The Third Way: Continual Experimentation and\nLearning\nThe third way involves creating a culture where everyone is encouraged to try new\nthings. This is a requirement for innovation to happen. In the third way, every-\none understands two things: (1) that we learn from the failures that happen when\nwe experiment and take risks and (2) that to master a skill requires repetition and\npractice.\nIn our software release example this means:\n• Rituals are created that reward risk taking. Trying new things, even when the\nnew thing fails, is rewarded at review time if valuable lessons were learned.\n• Management allocates time for projects that improve the system. The back-\nlog of “technical debt” is considered important. Resources are allocated to ﬁx\nthe bugs ﬁled when feedback is ampliﬁed. Mistakes are not repeated.\n• Faults are introduced into the system to increase resilience. Fire drills\n(discussed in Chapter 15) intentionally take down machines or networks to\nmake sure redundant systems kick in.\n• You try “crazy” or audacious things. For example, you might try to get the\nﬂow time from one week down to one day.\nWhen a team can identify its “value streams” (the processes that the business\ndepends on) and apply the Three Ways of DevOps to them, the processes don’t\njust get better—the company also values the IT team more.\n8.2.4 Small Batches Are Better\nAnother principle of DevOps is that small batches are better.\nSmall batches means doing a lot of small releases with a few features rather\nthan a small number of large releases with lots of features. It’s very risky to do large,\ninfrequent releases. The abundance of new features makes it difﬁcult to home in\non bugs in the code. Features may interfere with each other, creating new bugs.\nTo lower overall risk, it’s better to do many small releases containing only a few\nfeatures each.\nThe ﬁrst beneﬁt of this pattern is that each new release is smaller, enabling\nyou to isolate bugs more easily. The second beneﬁt is that code latency is reduced.\nCode latency is how fast code gets from the ﬁrst check-in to production, where it\ncan be making money for you. From a ﬁnancial standpoint, the code going into\n",
      "content_length": 2305,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "8.2\nThe Three Ways of DevOps\n179\nproduction sooner means the code can be generating return on investment (ROI)\nsooner. Lastly, small batches mean the process is done over many iterations. This\nmeans getting more practice at it, so you have more opportunities to get better at\nthe process. This reduces risk.\nVelocity is how many times you ship in a month. High velocity and low latency\nare realized through releasing small batches.\nThe small batches principle is counter-intuitive because there is a human ten-\ndency to avoid risky behavior. Deploying software in production involves risk;\ntherefore businesses traditionally minimize the frequency of deployments. While\nthis makes them feel better, they actually are shooting themselves in the foot\nbecause the deployments that are done are bigger and riskier, and the team doing\nthem is out of practice by the time the next one rolls around.\nThis principle applies to deployments and any other process that involves\nmaking frequent changes.\n8.2.5 Adopting the Strategies\nThe ﬁrst step in adopting the Three Ways is to identify the team’s value\nstreams—processes done for the business, or requested by the business.\nGo through each process several times until it can be done from beginning to\nend without failure. It doesn’t have to be optimal, but each step needs to be clearly\ndeﬁned so that it can be done in a repeatable fashion. That is, a reasonably well-\ntrained person should be able to do the step and the result will be the same as the\nresult from another reasonably trained person. Now the process is deﬁned.\nOnce the process is deﬁned, amplify the feedback loops. That is, make sure\neach step has a way to raise the visibility of problems so that they are worked on,\nnot ignored. Collect measurements on the length, frequency, and failure rate of the\nsteps. Make this data available to all involved.\nThis feedback is used to optimize the process. Find the steps that are the most\nerror prone, unreliable, or slow. Replace them, improve them, or eliminate them.\nThe two biggest inefﬁciencies are rework (ﬁxing mistakes) and redundant work\n(duplicate effort that can be consolidated).\nEvery process has a bottleneck—a place where work is delayed while it waits\non other dependencies. The most beneﬁcial place to put energy into improvement\nis at the bottleneck. In fact, optimizations anywhere else are wasted energy. Above\nthe bottleneck, incomplete work accumulates. Below the bottleneck, workers are\nstarved for things to do. Optimizing steps above the bottleneck simply makes more\nwork accumulate. Optimizing steps below the bottleneck simply improves steps\nthat are underutilized. Therefore ﬁxing the bottleneck is the only logical thing to do.\nMaking all of this happen requires a culture of innovation and a willingness\nto take risks. Risk must be rewarded and failure embraced. In fact, once the Three\nWays of DevOps have been used to make the process smooth and optimized,\n",
      "content_length": 2942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "180\nChapter 8\nDevOps Culture\nyou should introduce defects into the system to verify that they are detected and\nhandled. By embracing failure this way, we go from optimized to resilient.\n8.3 History of DevOps\nThe term “DevOps” was coined by Patrick Debois in 2008. Debois noticed that\nsome sites had evolved the practice of system administration into something fun-\ndamentally different. That is, they had independently reached the conclusion that\nweb sites could be better run when development and operations were done in col-\nlaboration. Debois thought there would be value in getting these people together\nto share what they had learned. He started a series of mini-conferences called\n“DevOps Days” starting in 2009 in Belgium. The name came from the concept of\nbringing developers (dev) and operations people (ops) together.\nDevOps Days was a big success and helped popularize the term “DevOps.”\nThe conversations continued on mailing lists and blogs. In May 2010, John Willis\nand Damon Edwards started the DevOps Cafe Podcast, which soon became a clear-\ninghouse for DevOps ideas and discussion. The hashtag “#devops” arose as a way\nfor DevOps followers to identify themselves on Twitter, which was a relatively\nnew service at the time. The 2011 USENIX LISA Conference (Large Installation\nSystem Administration) selected DevOps as its theme and since then has evolved\nto incorporate a DevOps focus.\n8.3.1 Evolution\nSome practitioners say that DevOps is a logical evolution of having sysadmins and\ndevelopers participating in an Agile development cycle together and using Agile\ntechniques for system work. While the use of Agile tools is common in DevOps,\nAgile is merely one of many ways to apply DevOps principles. Techniques such\nas pair programming or scrum teams are not required to create a DevOps envi-\nronment. However, adherence to some of the basic Agile principles is deﬁnitely\nrequired.\nOther practitioners say that DevOps is the logical evolution of developers\ndoing system administration themselves due to the popularity of Amazon AWS\nand (later) similar services. In other words, it is developers reinventing system\nadministration. In the past, setting up a new machine required the skill and train-\ning of a sysadmin. Now developers were allocating virtual machines using API\ncalls. Without the need for full-time sysadmins, developers were learning more\nand more system skills and bringing with them their penchant to automate tasks.\nBy making deployment and test coding part of the development cycle, they created\na new emphasis on repeatability that led to many of the techniques discussed in\nthis chapter.\n",
      "content_length": 2629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "8.4\nDevOps Values and Principles\n181\nOther practitioners counter by saying that some sysadmins have always had\nan emphasis on automation, though outside of web environments management\ndid not value such skills. From their perspective, DevOps was driven by sysad-\nmins who concentrated on their coding skills and began collaborating with the\ndevelopers on deployment and code testing. Taking these steps into the develop-\nment cycle led to closer ties with developers and working as a tight-knit group to\naccomplish the shared goal of increased uptime and bug-free deploys. More cyn-\nically, DevOps can be viewed as system administration reinvented by sysadmins\nwho ﬁnally had management support to do it the right way.\n8.3.2 Site Reliability Engineering\nAround the same time, companies such as Google started being more open about\ntheir internet sysadmin practices. Google had evolved system administration into\nthe concept of a Site Reliability Engineer (SRE) by recognizing that all functions\nof system administration, from capacity planning to security, were crucial to the\nreliability of a site. Since 2005, Google’s SRE model has organized the company’s\ndevelopers and operational engineers to share responsibility for reliability and per-\nformance. The SRE model can be thought of as DevOps at large scale: how do you\nempower 10,000 developers and 1000 SREs to work together? First, each product or\ninfrastructure component has a small team responsible for it. For critical and highly\nvisible systems, developers and SREs work together in an arrangement that mirrors\nthe DevOps model. Unfortunately, there are not enough SREs to go around. There-\nfore the vast majority of these teams consist of developers using tools developed by\nthe SREs. The tools are engineered to make operations self-service for developers.\nThis empowers developers to do their own operations. The SREs build tools that\nare speciﬁcally engineered to make it easy to achieve high-quality results without\nadvanced knowledge.\nDevOps is rapidly expanding from a niche technique for running web sites\nto something that can also be applied to enterprise and industrial system admin-\nistration. There is nothing uniquely web-centric about DevOps. Marc Andreessen\nfamously said, “Software eats the world” (Anderson 2012). As this happens, all\nfacets of society will require well-run operations. Thus, DevOps will be applied to\nall facets of computing.\n8.4 DevOps Values and Principles\nDevOps can be divided into roughly four main areas of practice (Kartar 2010):\n• Relationships\n• Integration\n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "182\nChapter 8\nDevOps Culture\n• Automation\n• Continuous improvement\n8.4.1 Relationships\nIn a traditional environment, the tools and scripts are seen as the primary focus of\noperational maintenance. DevOps gives more weight to the relationships among\nthe teams and the various roles in the organization. Developers, release managers,\nsysadmins, and managers—all need to be in close coordination to achieve the\nshared goal of highly reliable and continuously improving services.\nRelationships are so important in a DevOps environment that a common\nmotto is “People over process over tools.” Once the right people are performing\nthe right process consistently, only then does one create a tool to automate the\nfunction. One of the key deﬁning principles of DevOps is the focus on people and\nprocess over writing a script and then ﬁguring out who should run it and when.\n8.4.2 Integration\nPart of breaking down silos is ensuring that processes are integrated across teams.\nRather than seeing operational duties as merely following a script, in a DevOps\nenvironment one views them as end-to-end processes that combine tools and\ndata with people processes such as peer reviews or coordination meetings. Pro-\ncesses must be linked across domains of responsibility to deliver end-to-end\nfunctionality.\nIntegration of the communities responsible for different parts of the service\noperation is also a given for DevOps. A quick way to assess the DevOps culture in\nyour environment might be to ask sysadmins who they have lunch with regularly.\nIf the answer is “Mostly sysadmins,” and hardly ever folks from software devel-\nopment, web operations, networking, or security, this is a sign that integration of\nteams has not been achieved.\n8.4.3 Automation\nUnder the auspices of automation, DevOps strives for simplicity and repeatability.\nConﬁgurations and scripts are handled as source code and kept under version con-\ntrol. Building and management of the source code are scripted to the fullest extent\npossible once the entire process is understood.\nSimplicity increases the efﬁciency and speed of communication and avoids\nconfusion. It also saves time in training, documentation, and support. The goal is\nto design simple, repeatable, reusable solutions.\n",
      "content_length": 2244,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "8.4\nDevOps Values and Principles\n183\n8.4.4 Continuous Improvement\nEach time a process is carried out, the goal is to make it dependably repeatable\nand more functional. For example, every time there is a failure, tests are added to\nthe release process to detect that failure mode and prevent another release with\nthe same failure from being passed to the next step. Another example might be\nimproving a process that needs occasional manual intervention by handling more\nedge cases in the tool, until eventually manual intervention is no longer required.\nBy taking an end-to-end view, we often ﬁnd opportunities to eliminate pro-\ncesses and tools, thus simplifying the system. Problems are ﬁxed by looking for root\ncauses rather than making local optimizations that degrade global performance.\nThe mindset required is eloquently summed up by Kartar (2010):\nTreat your processes like applications and build error handling into them. You can’t\npredict every ... pitfall ... but you can ensure that if you hit one your process isn’t\nderailed.\n8.4.5 Common Nontechnical DevOps Practices\nDevOps includes many nontechnical practices that fall under the DevOps\numbrella. Not all DevOps organizations use all of these techniques. In fact, it is\nimportant to pick and choose among them, using the techniques that are needed\nrather than blindly following all the practices for completeness. These are the\n“people processes”; the more technical practices will be covered in the next section.\n• Early Collaboration and Involvement: Ops staff are included in development\nplanning meetings, and developers have full access to ops monitoring. Key\nissues such as architecting for scalability are jointly developed in the planning\nstage. (See Chapter 5.)\n• New Features Review: Ops staff participate in and guide development toward\nbest practices for operability during design time, not as an after-thought. Key\nmonitoring indicators for services are deﬁned through collaboration. Deploy-\nment details are sketched out so development of deployment code and tests\nare part of the main development effort. (See Chapter 2.)\n• Shared Oncall Responsibilities: These responsibilities include not only pager\nduties shared between developers and Ops staff, but also shared review of\noncall trends—for example, a weekly meeting to review SLA compliance and\nany outages. Developers have full access to all monitoring output, and Ops\nstaff have full access to all build/deploy output. That way everyone is fully\nempowered to research any issues that come up while oncall or during a\nfailure analysis.\n",
      "content_length": 2571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "184\nChapter 8\nDevOps Culture\n• Postmortem Process: In addition to a regular stand-up meeting to review out-\nages and trends, there should be a thorough postmortem or failure analysis\ndone for every outage. Recurring patterns of minor failures can point to a\nlarger gap in process. Findings of a postmortem—speciﬁcally, tasks needed\nto correct issues—should be added to the current development backlog and\nprioritized accordingly.\n• Game Day Exercises: Sometimes known as “ﬁre drills,” these are deliberate\nattempts to test failover and redundancy by triggering service disruption in a\nplanned fashion. Teams of people are standing by to ensure that the “right\nthing” happens, and to ﬁx things manually if it does not. Only by induc-\ning failure can you actually test what will happen when service components\nfail. A simple example of a game-day exercise is rebooting randomly selected\nmachines periodically to make sure all failover systems function properly.\n• Error Budgets: Striving for perfection discourages innovation, but too much\ninnovation means taking on too much risk. A system like Google’s Error\nBudgets brings the two into equilibrium. A certain amount of downtime is\npermitted each month (the budget). Until the budget is exhausted, develop-\ners may do as many releases as they wish. Once the budget is exhausted, they\nmay do only emergency security ﬁxes for the rest of the month. To conserve\nthe Error Budgets, they can dedicate more time for testing and building frame-\nworks that assure successful releases. This aligns the priorities of operations\nand developers and helps them work together better. See Section 19.4 for a full\ndescription.\n8.4.6 Common Technical DevOps Practices\nDevOps is, fundamentally, a structural and organizational paradigm. However, to\nmeet the goals of DevOps, a number of technical practices have been adopted or\ndeveloped. Again, not all of them are used by every DevOps organization. These\npractices are tools in your toolbox, and you should choose those that will best serve\nyour situation.\n• Same Development and Operations Toolchain: Development and operations\ncan best speak the same language by using the same tools wherever possible.\nThis can be as simple as using the same bug-tracking system for both develop-\nment and operations/deployment issues. Another example is having a uniﬁed\nsource code management system that stores not just the product’s source code,\nbut also the source code of operational tools and system conﬁgurations.\n• Consistent Software Development Life Cycle (SDLC): Bringing both the\napplication itself and the deployment/operations code together into the same\n",
      "content_length": 2639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "8.4\nDevOps Values and Principles\n185\nSDLC is key to keeping the two in sync. The “throw it over the wall to deploy-\nment” model is anathema in DevOps, where development and operations are\ntightly coupled. The deployment tools are developed and tested in lockstep\nwith the applications themselves, following a shared release cycle.\n• Managed Configuration and Automation: The conﬁguration ﬁles of all appli-\ncations that are required for the service are kept in source code control and are\nsubject to the same change management as the rest of the code. The same is\ntrue for all automation scripts.\n• Infrastructure as Code: With a software-deﬁned datacenter (i.e., virtual\nmachines), you can keep a description of the entire infrastructure as code that\ncan be maintained under revision control. Infrastructure as code is further\ndescribed in Section 10.6.\n• Automated Provisioning and Deployment: Every step of the deployment\nprocess is automated and/or scripted so that one can trigger a build that will\ngo all the way through self-test to a deployment, or can trigger a deployment\nvia a separate build command.\n• Artifact-Scripted Database Changes: Rather than manual manipulation of\ndatabase schema, changes to databases are also treated as code. They are\nscripted, tested, versioned, and released into staging environments.\n• Automated Build and Release: The output of a build cycle is a valid set of\napplication and deployment objects that can be deployed to a staged environ-\nment. Builds have makeﬁles or other conﬁguration ﬁles that treat the build as\na series of dependencies and contracts to fulﬁll, and can be triggered by check-\nins or by speciﬁc command. Assembling stages of a build by hand is counter\nto repeatability and ease of operation.\n• Release Vehicle Packaging: As noted earlier, the build cycle creates packaging\nfor the application to facilitate its deployment. The end product of a build does\nnot require by-hand packaging to prepare it for deployment, nor is software\ndeployed to live systems via checkout from a repository or compiled on each\nhost before use.\n• Abstracted Administration: Abstracted administration describes system\nadministration tasks at a high level and lets automation decide the right\nsteps to perform for a given operating system. Thus we might have a con-\nﬁguration ﬁle to provision a new user that says “create user” rather than\nthe steps required for Linux (“append this line to /etc/passwd, this line to\n/etc/shadow, and this line to /etc/group”) or Windows (“create the user in\nActiveDirectory”). By doing the initial setup work with the tools, we simplify\nthe interface between goals and conﬁgurations. Some commonly used tools in\nthis area include CFEngine, Puppet, and Chef.\n",
      "content_length": 2732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "186\nChapter 8\nDevOps Culture\n8.4.7 Release Engineering DevOps Practices\nCertain release engineering practices have become closely related to DevOps prac-\ntices. Release engineering is the process of taking software in source form, building\nit, packaging it, testing it, and deploying it into the ﬁeld.\nWhile not DevOps practices in themselves, these development practices have\na great deal to offer in achieving operational efﬁciency (DevOps-Toolchain 2010).\nEach of these practices is discussed in more detail in Chapters 9, 10 and 11.\n• Continuous Build: With each change, attempt to compile the code base and\ngenerate the packaged software. This detects build-related problems as soon\nas possible.\n• Continuous Test: The software is tested in an automated fashion with each\nchange to the code base. This prevents problems from becoming embedded in\nthe system.\n• Automated Deployment: The process of deploying the software for use in test\nand live environments is automated.\n• Continuous Deployment: With fully automated build, test, and deployment,\nthe decision whether to deploy a particular release is also automated. Multiple\nreleases are deployed in the ﬁeld each day.\n• Automated Provisioning: Additional resources, such as CPU, storage, mem-\nory, and bandwidth, are allocated based on a predictive model. As the system\ndetects that more resources are needed, they are allocated for use by the\nsystem.\n8.5 Converting to DevOps\nBefore implementing any of these recommendations, developers and operations\nneed to open a dialogue. Building bridges between the groups needs to start by\nforming collegial connections. This is often best done away from the ofﬁce, prefer-\nably over a beer or other beverage. That is when operations and developers really\nstart talking to each other, sharing their perspectives, and ﬁnding common ground\nthat make adopting DevOps practices happen. In an interview on the DevOps Cafe\npodcast, Jesse Robbins noted that spending $50 on fries and drinks may be the best\ninvestment some companies ever make (Willis & Edwards 2011).\nWhen adopting DevOps principles in a traditional, non-DevOps organization,\nit is important to start slowly. Adopt a few new practices at ﬁrst and add more\npractices as they get buy-in from the team. There are three basic phases involved\nin this type of conversion.\nFirst, make operations feedback available to the overall project team. This\ncan take several forms. Most commonly, organizations make monitoring available\n",
      "content_length": 2481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "8.5\nConverting to DevOps\n187\nto developers and ask them to do joint root causes analysis of issues or failures,\nidentify recurring problems, and collaborate on solutions.\nSecond, begin to embed product knowledge into operations. Invite developers\nto key operations meetings on deployment and maintenance, and set escalation\npaths that involve developers being reachable after hours.\nThird, enable operations knowledge to be available during all project phases.\nThis involves operations being included in daily or weekly status meetings, being\ninvolved in prioritization of the product backlog tasks, and being a full partner in\nplanning meetings.\n8.5.1 Getting Started\nTo start a DevOps relationship, you must ﬁrst get off the computer and begin\nface-to-face discussions. Where do you start? Begin with development, product\nmanagers, or other members of the product team for the product(s) you support.\nIt is best to choose someone approachable, with whom you may already have\nsome rapport. You can arrange a meeting or simply go get coffee some afternoon.\nYour initial conversation should be about mutual problems that can be solved.\nAs part of the conversation, explain the improvements that could come from closer\ncollaboration, such as improved release efﬁciency or better operations response to\ndevelopers’ needs.\nAs you discuss problems to solve, you will ﬁnd one that will be a good starting\npoint for a joint DevOps project. Choose something that has obvious beneﬁts to the\ndevelopment team rather than something that is primarily operations focused. It\nis best to describe the project in terms of mutual beneﬁts.\nAs part of the collaboration on this project, get in the habit of holding reg-\nular meetings with the development and product teams. Attend their planning\nmeetings, and invite them to yours.\nAs you successfully complete your starter project, identify another project for\ncollaboration. Good candidates are processes that affect both development and\noperations, such as roll-outs or build toolchains. Involving multiple stakehold-\ners from development, operations, and the product team is a good way to build\nrelationships.\nOnce again, one easy way to measure your DevOps success is to ask, “Who do\nyou go to lunch with?” If you’re routinely going to lunch with folks from develop-\nment, networking, release, or similar groups, chances are very good that you are\ndoing DevOps.\n8.5.2 DevOps at the Business Level\nThe next stage of conversion to a DevOps environment is getting your manage-\nment to buy into the DevOps philosophy. A true DevOps environment often\n",
      "content_length": 2582,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "188\nChapter 8\nDevOps Culture\ninvolves changes to the organizational chart that break down the barriers between\ndevelopment and operations. The organizational structure needs to foster a close\nrelationship between development and operations. Ideally, get development and\noperations under one management team, the same vice president, or something\nsimilar. Also try to colocate the groups so that they are near each other, or at least\nin the same building or time zone. This kind of change can open up a whole new\nlevel of functionality. Getting buy-in is fairly difﬁcult. Management needs to have\nthe value explained in business terms.\nDevOps doesn’t stop with collaboration between developers and operations\nstaff. It can be useful up and down the entire organizational chain. Recently one\nof the authors witnessed an internal project where developers, operations staff,\nproduct management, and the legal department worked side by side to create a\nsolution that would properly handle a tight set of constraints. The staff from the\nlegal department were amazed at the level of collaboration and commitment that\ncould be focused on the problem. Buying an expensive third-party system that\nwould have needed conﬁguration and outside management was avoided.\n.\nDevOps: Not Just for the Web\nA Practical Approach to Large-Scale Agile Development: How HP Transformed\nHP LaserJet FutureSmart Firmware by Gruver, Young, and Fulghum (2012)\ndescribes applying DevOps to the creation of HP LaserJet software. The result\nwas that developers spent less time doing manual testing, which gave them\nmore time to develop new features. Their success story is summarized in\nEpisode 33 of the DevOps Cafe Podcast (Willis, Edwards & Humble 2012).\n8.6 Agile and Continuous Delivery\nDevOps is a natural outgrowth of the software development methodology known\nas “Agile” and the practices called “continuous delivery.” While this book is not\nabout either of those topics, a brief look at both Agile and continuous delivery is\nhelpful to show the origins of many DevOps practices. The principles involved are\ndirectly transferable to DevOps practices and serve as a strong foundation for a\nDevOps mindset.\n8.6.1 What Is Agile?\nAgile is a collection of software development principles that originated in an\nunusual summit of representatives from various nontraditional software practices\n",
      "content_length": 2360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "8.6\nAgile and Continuous Delivery\n189\nsuch as Extreme Programming, Scrum, and Pragmatic Programming, to name a\nfew. They called themselves “The Agile Alliance” and created the highly inﬂuential\n“Agile Manifesto” (Beck et al. 2001).\nThe Agile Manifesto\n• Individuals and interactions over processes and tools\n• Working code over comprehensive documentation\n• Customer collaboration over contract negotiation\n• Responding to change over following a plan\nAs a coda to the Agile Manifesto, the authors added, “While there is value in the\nitems on the right, we value the items on the left more.” Agile practices stress direct\nconnections between the business objectives and the development team. Develop-\ners work closely with product owners to build software that meets speciﬁc business\nobjectives. The waterfall method of development is bypassed in favor of collabo-\nrative methods such as pair programming, where two developers work on code\ntogether, or scrum, where a whole team commits to “sprints” of one to four weeks\nworking on a prioritized backlog of features. Plain statements called “user stories”\nprovide requirements for software development—for example, “As a bank cus-\ntomer, I want to receive an electronic bill for my credit card statement” or “As a\nphoto site user, I want to crop and edit pictures in my web browser.”\nIn Agile development, waterfall methods are also bypassed with respect to\ntesting and integration. Unit and integration tests are created with new feature\ncode, and testing is applied automatically during the build process. Often the only\ndocumentation for code releases is the user stories that provided the initial require-\nments, in strong contrast to waterfall’s functional speciﬁcations and requirements\ndocuments. The user stories come out of a prioritized backlog of feature stories\nmaintained by the product owner. By keeping a prioritized list that can change with\nbusiness needs, the agility of the development process is maintained and devel-\nopment can respond to changing business needs easily without wasted effort and\nrework.\nDevOps is the application of Agile methodology to system administration.\n8.6.2 What Is Continuous Delivery?\nContinuous delivery (CD) is a set of principles and practices for delivery of soft-\nware and updates on an ongoing basis. Software delivery means the process\nby which software goes from source code to ready-to-use installation packages.\nIncluded in this process are the building (compiling) of the software packages as\nwell as any quality testing. The process stops if the build or testing fails. CD began\n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "190\nChapter 8\nDevOps Culture\nas a design pattern in Extreme Programming but has since become a discipline of\nits own.\nFor example, an organization that practices CD builds the software frequently,\noften immediately after any new source code changes are detected. The change\ntriggers the build process, which, on successful completion, triggers an automated\ntesting process. On successful completion of the tests, the software packages are\nmade available for use.\nCD is different from traditional software methodologies where new software\nreleases are infrequent, perhaps yearly. In the latter approach, when the software\nrelease date is near, the packages are built, possibly involving a very manual pro-\ncess involving human intervention and perhaps ad hoc processes. The testing is a\nmixture of automation and manual testing that may last for days. If any problems\nare found, the entire process starts all over.\nContinuous delivery stresses automation, packaging, and repeatability, with a\nculture of shared responsibility for good outcomes. In continuous delivery we rec-\nognize that every step of the process is part of the ultimate delivery of the software\nand updates, and that the process is always occurring at every stage.\nSystem operations and maintenance can be thought of as a form of contin-\nuous delivery. Operations tasks are part of how software gets from a packaged\nbuild on the repository onto the system. While these principles and practices came\nfrom software development, it is easy to map them onto operations tasks and gain\nefﬁciency and reliability.\nThe eight principles of continuous delivery are codiﬁed by Humble and Farley\n(2010).\nThe Eight Principles of Continuous Delivery\n1. The process for releasing/deploying must be repeatable and reliable.\n2. Automate everything.\n3. If something is difﬁcult or painful, do it more often to improve and automate\nit.\n4. Keep everything in source control.\n5. “Done” means “released, working properly, in the hands of the end user.”\n6. Build quality in.\n7. Everybody has responsibility for the release process.\n8. Improve continuously.\nWhile most of these principles are self-explanatory, it is worthwhile expanding on\nthe third principle. Often our response to error-prone, painful, and difﬁcult tasks is\nto ﬁnd ways to do them less often. Continuous delivery says to do them more often\n",
      "content_length": 2353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "8.6\nAgile and Continuous Delivery\n191\nto improve our skill (“Practice makes perfect”), ﬁx the problems in the processes,\nand automate them.\nThere are also four practices of continuous delivery.\nThe Four Practices of Continuous Delivery\n1. Build binaries only once.\n2. Use the same repeatable process for deployment to every environment.\n3. Do basic functionality tests (“smoke tests”) on your deployment (e.g., include\ndiagnostics).\n4. If anything fails, stop the line and start again.\nFor any given release cycle, binary packages are built only once. Contrast this with\nenvironments where QA builds a package and tests it, and then deployment checks\nout the same code and builds the package to be used in deployment. Not only is\nthis duplication of effort, but errors can also creep in. Multiple build requests may\nbe confused or miscommunicated, resulting in packages being built from slightly\ndifferent source code revisions each time. One team may be tempted to “slip in a\nfew small ﬁxes,” which might seem helpful but is dangerous because it means some\nnew code did not receive the full, end-to-end testing. It is difﬁcult to verify that the\nsecond time the package is built, the same exact OS release, compiler release, and\nbuild environment are used.\n.\nVersion-Controlled Builds\nAt one of Tom’s former employers, the build system checked the MD5 hash\nof the OS kernel, compiler, and a number of other ﬁles. The build tools team\nwould determine that version x.y was to be built with a speciﬁc tool-chain and\nthe build system wouldn’t let you build that version with any other toolchain.\nIf a customer ﬁve years later required a patch to version X, the company knew\nit could build the software exactly as needed with only the patch being the dif-\nferent item, not the compiler, OS, or other tool. This was particularly important\nbecause the company made software for designing chips, and chip designers\ndidn’t change toolchains once a design was started. If you started a design\nwith version X, the vendor promised it could ﬁx bugs in that speciﬁc version\nuntil your chip design was ﬁnished.\nBy automating as much as possible and creating documented processes for\nsteps that cannot be automated, we create a repeatable process for deployment.\n",
      "content_length": 2248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "192\nChapter 8\nDevOps Culture\nIn a traditional software development methodology, release engineers hand-build\nbinaries for the QA environment and deploy them in an ad hoc fashion. In contin-\nuous deployment, the build is automated and each stage uses the same process for\ndeployment.\nA “smoke test” is a basic functionality test, like plugging in a device and seeing\nif it starts to smoke (it shouldn’t!). Including built-in diagnostic tests as part of your\nbuild and deployment automation gives you high conﬁdence that what you just\ndeployed meets the basic functionality requirements. The build or deploy will error\nout if the built-in tests fail, letting you know something is wrong.\nInstead of patching a failure and proceeding with patches, continuous delivery\nwants us to ﬁnd a root cause and ﬁx the problem. We then work the process again\nto verify that the problem has disappeared.\nCD produces an installation package that is tested sufﬁciently so that it can\nbe deployed into production. However, actually pushing that release into pro-\nduction is a business decision. Deployments are usually less frequent, perhaps\ndaily or weekly. There may be additional tests that the deployment team performs\nand the actual deployment itself may be complex and manual. Automating those\npractices and doing them frequently (perhaps as frequently as new packages are\navailable) constitutes continuous deployment. This approach is discussed further\nin Section 11.10.\n8.7 Summary\nIn this chapter, we examined the culture and principles of DevOps and looked at\nits historical antecedents—namely, Agile and continuous delivery. The principles\nand practices in this chapter serve as a strong foundation for a shift in culture and\nattitude in the workplace about operations and maintenance tasks. This shift brings\nmeasurable efﬁciency and increased uptime when it is strongly applied, and you\nshould explore it for yourself.\nTheo Schlossnagle (2011) describes DevOps as the natural response to “the\noperationalism of the world.” As velocity increases in companies of every type,\nthe most successful competitors will be those organizations that are “operationally\nfocused” in every business unit.\nDevOps is not about a technology. DevOps is about solving business problems\n(Edwards 2010). It starts with understanding the business needs and optimizing\nprocesses to best solve them.\nDevOps makes businesses run more smoothly and enables the people\ninvolved to work together more effectively. It gives us hope that operations can\nbe done well at any scale.\n",
      "content_length": 2543,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Exercises\n193\nExercises\n1. What are the fundamental principles behind DevOps?\n2. How does a waterfall environment differ from a DevOps environment?\n3. Why is DevOps not intended to be a job title?\n4. Describe the relationship between Agile and DevOps practices.\n5. Whose participation is vital to DevOps collaboration, at a minimum?\n6. Name three release engineering best practices.\n7. What are the basic steps involved in converting a process to DevOps?\n8. Describe your release process (code submission to running in production).\nIdentify the manual steps, team-to-team handoffs, and steps that are ill deﬁned\nor broken. Based on the Three Ways of DevOps, which improvements could\nbe made?\n",
      "content_length": 692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "Chapter 9\nService Delivery: The Build\nPhase\nAnd they’re certainly not showing\nany signs that they are slowing.\n—Willy Wonka\nService delivery is the technical process of how a service is created. It starts with\nsource code created by developers and ends with a service running in production.\nThe system that does all of this is called the service delivery platform. It includes\ntwo major phases: build and deploy. The build phase starts with the software\nsource code and results in installable packages. That phase is covered in this chap-\nter. The deployment phase takes those packages, readies the service infrastructure\n(machines, networks, storage, and so on), and produces the running system. The\ndeployment phase is covered in Chapter 10.\nThe service delivery ﬂow is like an assembly line. Certain work is done at\neach step. Tests are performed along the way to verify that the product is work-\ning correctly before being passed along to the next step. Defects are detected and\nmonitored.\nThe faster the entire process can run, the sooner the test results are known.\nHowever, comprehensive tests take a long time. Therefore the earlier tests should\nbe the fastest, broadest tests. As conﬁdence builds, the slower, more detailed tests\nare done. This way the most likely failures happen early on, removing the need for\nthe other tests. For example, the early stages compile the software, which is sensi-\ntive to easy-to-detect syntax errors but provides little guarantee that the software\nworks. Testing performance, security, and usability can be the most time consum-\ning. Manual tests are saved for the end, after all other failures have been detected,\nconserving manual labor.\n195\n",
      "content_length": 1688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "196\nChapter 9\nService Delivery: The Build Phase\n.\nTerms to Know\nInnovate: Doing (good) things we haven’t done before.\nStakeholders: People and organizations that are seen as having an interest\nin a project’s success.\nArtifacts: Any kind of tangible by-product produced during the develop-\nment of software—source ﬁles, executables, documentation, use cases,\ndiagrams, packages, and so on.\nService Delivery Flow: The trip through the system from beginning to end;\noften shortened to flow.\nCycle Time: How frequently a ﬂow completes.\nDeployment: The process of pushing a release into production; often short-\nened to deploy.\nGate: An intentional limit that prevents ﬂow. For example, the deploy step\nis gated by whether quality assurance tests succeeded.\nRelease Candidate: The end result of the build phase. Not all release candi-\ndates are deployed.\nRelease: The successful completion of an entire ﬂow, including deploy. May\nresult in users seeing a visible change.\nA good service delivery platform takes into account the fact that a service is not\njust software, but also the infrastructure on which the service runs. This includes\nmachines with an operating system loaded and properly conﬁgured, the software\npackages installed and conﬁgured, plus network, storage, and other resources\navailable. While infrastructure can be set up manually, it is best to automate that\nprocess. Virtualization enables this kind of automation because virtual machines\ncan be manipulated via software, as can software-deﬁned networks. The more we\ntreat infrastructure as code, the more we can beneﬁt from software development\ntechniques such as revision control and testing. This automation should be treated\nlike any other software product and put through the same service delivery ﬂow as\nany application.\nWhen service delivery is done right, it provides conﬁdence, speed, and\ncontinuous improvement. Building, testing, and deployment of both an applica-\ntion and virtual infrastructure can be done in a completely automated fashion,\nwhich is streamlined, consistent, and efﬁcient. Alternatively, it can be done via\nmanual, ad hoc processes, inconsistently and inefﬁciently. It is your mission to\nachieve the former.\n",
      "content_length": 2202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "9.1\nService Delivery Strategies\n197\n9.1 Service Delivery Strategies\nThere are many possible service delivery strategies. Most methodologies fall along\nthe continuum between the older waterfall methodology and the more modern\nmethodology associated with the DevOps world. We recommend the latter because\nit gets better results and encourages faster rates of innovation. It focuses on\nautomation, instrumentation, and improvement based on data.\n9.1.1 Pattern: Modern DevOps Methodology\nThe DevOps methodology divides the platform into two phases: the build phase\nand the deployment phase. The build phase is concerned with taking the source\ncode and producing installation packages. The deployment phase takes those\npackages and installs them in the environment where they are to be run. At each\nstep along the way tests are performed. If any test fails, the process is stopped.\nSource code is revised and the process starts again from the beginning.\nFigure 9.1 represents this service delivery platform. There are two primary\nﬂows. The upper ﬂow delivers the application. The lower ﬂow delivers the infra-\nstructure. The four quadrants represent the build and deployment phases of each\nﬂow.\nEach phase has a repository: The build phase uses the source repository. The\ndeployment phase uses the package repository. Each phase has a console that pro-\nvides visibility into what is going on. The application ﬂow and the infrastructure\nﬂow have the same steps and should use the same tools when possible.\nDeployments are done in one of at least two environments: the test environ-\nment and the live production environment. The service is created initially in the\ntest environment, which is not exposed to customers. In this environment, a series\nof tests are run against the release. When a release passes these tests, it becomes a\nrelease candidate. A release candidate is a version with the potential to be a ﬁnal\nproduct—that is, one that is deployed in production for customer use.\nThe software may be deployed in other environments, such as a private sand-\nbox environment where the engineering team conducts experiments too disruptive\nto attempt in the test environment. Individual developers should each have at least\none private environment for their own development needs. These environments\nare generally scaled-down versions of the larger system and may even run on the\ndevelopers’ own laptops. There may be other separate environments for other rea-\nsons. For example, there may be a demo environment used to give previews of new\nreleases to stakeholders. Alternatively, new releases may be deployed in an inter-\nmediate production environment used by “early access customers” before they are\ndeployed into the main production environment.\n",
      "content_length": 2747,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "198\nChapter 9\nService Delivery: The Build Phase\nFigure 9.1: The parts of the modern service delivery platform pattern. (Reprinted\nwith permission from Damon Edwards of DTO Solutions.)\nAt a minimum, every site must have separate testing and production environ-\nments, where the testing environment is built exactly like production. Developers’\nown environments are typically less well controlled, and testing in such an envi-\nronment may miss some issues. It is negligent to move a release straight from\ndeveloper testing into production, or to not have an environment for testing the\nconﬁguration management systems. There is no good reason to not invest in the\nadditional infrastructure for a proper test environment—it always pays for itself\nin increased service availability.\nWhile one may think of the build phase as the domain of developers and the\ndeployment phase as the domain of operations, this is not true in a DevOps envi-\nronment. Both groups share responsibility for the construction and use of the entire\n",
      "content_length": 1020,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "9.1\nService Delivery Strategies\n199\nsystem. The handoff between steps marks the ﬂow of work, not organizational\nboundaries.\n9.1.2 Anti-pattern: Waterfall Methodology\nThe waterfall methodology works differently from the modern DevOps methodol-\nogy. It is predicated on multiple phases, each controlled by a different organization.\nHandoffs not only mark the ﬂow of work, but also indicate the end of each orga-\nnization’s responsibility. The waterfall methodology was previously discussed in\nSection 8.1.1.\nThe waterfall methodology has many phases. The ﬁrst phase is controlled by\nthe development organization, which has two teams: software engineers (SWEs)\nand quality assurance (QA) engineers. The SWEs create the source code, which\nthey compile and package. The QA team tests the packages in its own environment.\nWhen the team approves the software, it is designated as a release candidate and\npassed to the next phase. The next phase is controlled by the system administration\nteam. This team uses the release candidate to build a beta environment; the beta\nenvironment is used to verify that the software works. Product management then\ngets involved and veriﬁes that the software is functioning as expected. Once the\nrelease is approved, the system administrators use it to upgrade the live production\nenvironment.\nThere are many problems with the waterfall approach. First, both QA and\noperations build their own environments, each using different methods. This\nmeans there is a duplication of effort, with each team developing overlapping tools.\nSecond, because the QA environment is built using different methods, it is not a\nvalid test of how the system will work in production. The production environment\nmight not have the same OS release, host conﬁguration, or supporting packages.\nAs a consequence, the testing is incomplete. It also makes it difﬁcult for developers\nto reproduce bugs found in QA.\nAnother problem with the waterfall methodology is that because the handoff\nbetween phases is also a handoff between organizations, the discovery of bugs or\nother problems can become a game of ﬁnger pointing. Is the softwarenot working in\nproduction because developers didn’t do their job, or did the problem arise because\noperations didn’t build the test environment correctly? Why ﬁnd out the truth when\nit is easier to create a political battle and see who can shift blame the fastest?\nThe DevOps methodology has many beneﬁts. Rather than the two phases cre-\nating a dividing wall between two organizations, the developers and operations\nstaff work collaboratively on each phase. The testing and production environments\nare built using the same tools, so testing is more accurate. The reuse of tools is more\nefﬁcient and means that the improvements made to the tools beneﬁt all. Because\nboth teams have a shared responsibility for the entire process, cooperation trumps\nﬁnger pointing.\n",
      "content_length": 2899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "200\nChapter 9\nService Delivery: The Build Phase\nThe DevOps methodology is also more simple. There are just two distinct\nphases, each with well-deﬁned concerns, inputs, and outputs.\n9.2 The Virtuous Cycle of Quality\nGood ﬂow creates a virtuous cycle of quality. Rigorous testing creates a solid\nfoundation that results in better releases. This improves conﬁdence, which in turn\nencourages faster and better releases. Because they are smaller releases, testing is\nimproved. The cycle then repeats.\nWhen discussing service delivery, people often focus on how fast their release\ncycle has become. When someone brags about improving cycle time from six weeks\nto an hour, they are missing the point.\nWhat’s really important is conﬁdence in the quality of the ﬁnal product.\nImproved code management, speed, packaging, and cycle time are all means, not\nends. Conﬁdence is a result of the platform’s ability to provide better testing and\nother processes that are automated to assure consistency. An excellent discussion\nof this can be heard in Episode 33 of the DevOps Cafe Podcast (Willis, Edwards &\nHumble 2012).\nMore speciﬁcally, a good service delivery platform should result in the follow-\ning outcomes:\n• Confidence: We want a process that assures a high likelihood that each deploy-\nment into production will be successful. Success means application bugs are\nfound and resolved early, ensuring a trouble-free deployment without out-\nages. The more conﬁdent we are in our service delivery process, the more\naggressively we can try new things. Innovation requires the ability to aggres-\nsively and fearlessly experiment. Consider the opposite case: a company full\nof people who resist change does not innovate. As fear of change increases,\ninnovation declines. If we can conﬁdently try new things, then we can experi-\nment and innovate, knowing that we can count on our service delivery system\nto support our innovations.\n• Reduced Risk: Faster iterations are less risky. As discussed in Section 8.2.4,\nmore frequent releases mean that each release will contain a smaller number\nof changes and, therefore, is less risky.\n• Shorter Interval from Keyboard to Production: We want the end-to-end pro-\ncess to happen quickly. We want to have our capital—the code developers\ncreate—in the hands of the customers as quickly as possible. Compare yearly\nreleases to weekly releases. With the former, new features sit idle for months\nbefore they see the light of day. That would be like an automotive factory mak-\ning cars all year but selling them only in December. Faster iterations mean\nfeatures get into production faster. This is important because the investment\nrequired to create a new feature is huge.\n",
      "content_length": 2696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "9.2\nThe Virtuous Cycle of Quality\n201\n• Less Wait Time: Faster iterations also mean code gets to the testing process\nsooner. This improves productivity because it is easier to debug code that\nwas written recently. Developers lose context over time; regaining lost con-\ntext takes time and is error prone. Less work has to be redone because testing\nhelps get things right the ﬁrst time.\n• Less Rework: We want to reduce the amount of effort spent redoing\nwork that was done previously. It is more efﬁcient to get things right the\nﬁrst time.\n• Improved Execution: Doing faster iterations improves our ability to execute\nall the phases. When there is a lot of time between each iteration, any man-\nual steps become less practiced and we don’t do them as well. If releases are\nextremely infrequent, the processes will have changed enough that it gives us\nan excuse not to automate. We throw up our hands and revert to old, manual\nmethods. Frequent releases keep automation fresh and encourage us to update\nit to reﬂect small changes before they turn into major ones.\n• A Culture of Continuous Improvement: The ideal process is always evolving\nand improving. Initially it might have manual steps. That’s to be expected, as\nprocesses are malleable when initially being invented. Once the end-to-end\nprocess is automated, it can be instrumented and metrics can be collected auto-\nmatically. With metrics we can make data-driven improvements. For a process\nto be continuously improved, we not only need the right technology but also\nneed a culture that embraces change.\n• Improved Job Satisfaction: It is exciting and highly motivating to see our\nchanges rapidly put into production. When the interval between doing work\nand receiving the reward is small enough, we associate the two. Our job\nsatisfaction improves because we get instant gratiﬁcation from the work\nwe do.\nRather than focusing purely on cycle time, a team should have metrics that balance\nthe velocity of individual aspects of the software delivery platform. We recommend\nthat every DevOps team collect the following metrics:\n1. Bug lead time: Time from initial bug report to production deployment of ﬁxed\ncode.\n2. Code lead time: Time from code commit to production deployment.\n3. Patch lead time: Time from vendor patch release to production deployment.\n4. Frequency of deployment: How many deployments to production are done\neach month.\n5. Mean time to restore service: Duration of outages; from initial discovery to\nreturn to service.\n6. Change success rate: Ratio of successful production deployments to total\nproduction deployments.\n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "202\nChapter 9\nService Delivery: The Build Phase\n9.3 Build-Phase Steps\nThe goal of the build phase is to create installation packages for use by the\ndeployment phase. It has ﬁve steps:\n1. Code is developed.\n2. Code is committed to the source repository.\n3. Code is built.\n4. Build results are packaged.\n5. Packages are registered.\nEach step includes some kind of testing. For example, building the software veriﬁes\nthat it compiles and packaging it veriﬁes that all the required ﬁles are available.\nAs Figure 9.1 shows, the source repository is used as the primary storage facil-\nity for this phase. The ﬁnal output is handed off to the next phase by stashing it in\nthe package repository.\n9.3.1 Develop\nDuring the develop step, engineers write code or produce other ﬁles. For example,\nthey may write C++, Python, or JavaScript code. Graphic designers create images\nand other artifacts.\nEngineers check out the existing ﬁles from the source repository, download-\ning the source onto the engineer’s machine or workspace. From there, the ﬁles are\nedited, revised, and altered. New ﬁles are created. For example, a software engineer\nworking on a new feature may make many revisions to all related ﬁles, compiling\nand running the code, and repeating this process until the source code compiles\nand functions as desired.\n9.3.2 Commit\nDuring the commit step, the ﬁles being developed are uploaded to the source\nrepository. Generally this is done infrequently, as it indicates the ﬁles have reached\na certain level of completeness. All committed code should be working code. If not,\nother developers’ work will come to a halt. They will pull recent changes into their\nworkspaces and the result will be code that doesn’t work. They will not be able to\ntell if the problem is due to their own error or if the source is just in bad shape.\nChecking in code that does not work is known as “breaking the build.” It may be\nbroken in that the code no longer compiles, or because the code compiles but auto-\nmated tests (discussed later) fail. If the build is broken, returning it to working state\nshould be considered a high priority.\nThe gate for this step is the pre-submit check. To prevent obviously bad or\nbroken code from entering, source repository systems can be conﬁgured to call\n",
      "content_length": 2273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "9.3\nBuild-Phase Steps\n203\nprograms that will check the new ﬁles for basic validity and reject attempts to\ncommit code that fails. The checks are not able to determine that the code is perfect\nand bug free, but obvious mistakes can be detected such as syntax errors. Often unit\ntests, described later, are run to verify the change does not break basic functionality.\nPre-submit checks often check for style guide conformance (discussed later in\nSection 12.7.4).\nBecause pre-submit checks can call any program, people have found many cre-\native uses for them beyond simple sanity checks. Pre-submit checks can be used to\nenforce policies, update status displays, and check for common bugs. For example,\nwe once experienced an outage caused by a ﬁle with incorrect permissions. Now\na pre-submit check prevents that same problem from reoccurring.\n9.3.3 Build\nDuring the build step, source ﬁles are processed to generate new artifacts. This\nusually means source code is compiled to produce executable ﬁles. Other tasks\nsuch as converting images from one format to another, extracting documentation\nfrom source code, running unit tests, and so on might also be performed during\nthis step.\nThis step is gated based on whether all the build processes complete success-\nfully. The most important of these checks are the unit tests. Unit tests are quality\nassurance tests that can run on compilation units such as function libraries and\nobject class deﬁnitions. In contrast, system tests, discussed in the next chapter,\ninvolve running the service and testing its complete functionality.\nUnit tests often take the form of an additional executable that does nothing\nexcept call functions in the code in many different ways, checking whether the\nresults are as expected. For example, suppose the source code includes a library of\nfunctions for manipulating usernames. Suppose one function in the library tests\nwhether a string can be used as a valid username. The unit test may call that func-\ntion many times, each time testing a string that is known to be invalid a different\nway (too short, too long, contains spaces, contains invalid characters) to verify that\nall of those cases are rejected. Another unit test may do the inverse for strings that\nare known to be valid.\nUnit tests can be rather sophisticated. For example, to test functions that need\naccess to a database, the unit test code may set up a mini-database with sample\ndata. Setting up a database is complex, so testing frameworks have been developed\nthat permit one to replace functions for testing purposes. For example, suppose\nyou are testing a function that opens a connection to a database, sends a query,\nand manipulates the results. You might replace the “connect to database” function\nwith one that does nothing and replace the “query database” function with one that\nalways returns a particular set of results. Now you can test the function without\nneeding to have an actual database.\n",
      "content_length": 2950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "204\nChapter 9\nService Delivery: The Build Phase\nThis step provides us with an opportunity to perform aggressive testing early\nin the process to avoid wasted effort later.\n9.3.4 Package\nDuring the package step, the ﬁles left behind from the previous step are used\nto create the installation packages. A package is a single ﬁle that encodes all the\nﬁles to be installed plus the machine-readable instructions for how to perform the\ninstallation. Because it is a single ﬁle, it is more convenient to transport.\nThis step is gated based on whether package creation happened successfully.\nA simple package format would be a Zip or tar ﬁle that contains all the ﬁles\nthat will be installed plus an installation script. When the installer runs, it reads\nthe package, extracts all the ﬁles, and then runs the installation script. A more\ndetailed description appears in the “Software Repositories” chapter of the third\nedition of The Practice of System and Network Administration (Limoncelli, Hogan &\nChalup 2015).\nSoftware packages should be designed to run in any environment. Do not\ncreate separate packages for the testing environment and the production environ-\nment. Or worse, do not build the package for testing, then after testing rebuild it\nfor the production environment. Production should run packages that were tested,\nnot packages that are similar to ones that were tested.\n.\nWhat Is a Software Package?\nA software package is a container. This single ﬁle contains everything needed\nto install an application, patch, or library. Packages typically include the\nbinary executables to be installed, any related data ﬁles, conﬁguration data,\nand machine-interpretable instructions describing how to install and remove\nthe software. You may be familiar with ﬁle formats such as Zip, UNIX tar, and\ncpio. Such ﬁles contain the contents of many smaller ﬁles plus metadata. The\nmetadata as a whole is like a table of contents or index. It encodes information\nneeded to unpack the individual ﬁles plus ﬁle ownership, permissions, and\ntimestamps.\n9.3.5 Register\nDuring the register step, the package is uploaded to the package repository. At this\npoint the package is ready to be handed off to the deploy phase. This step is gated\nbased on whether upload is a success.\n",
      "content_length": 2262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "9.4\nBuild Console\n205\n9.4 Build Console\nThe build console is software that manages all of the build steps, making it easy\nto view results and past history, and to keep statistics on success rates, the amount\nof time the process takes, and more. Build consoles are invariably web-based tools\nthat provide a dashboard to view status as well as control panels to manage the\nprocesses. There are many such tools, including Hudson, Jenkins CI, TeamCity, Go\nContinuous Delivery, and Atlassian Bamboo.\nOnce you ﬁnd a tool that you like, you will ﬁnd yourself wanting to use it\nwith everything. Therefore, when selecting such a tool, make sure it operates with\nthe tools you currently use—source code repository software, compilers and other\nbuild tools—and offers support for all your operating systems and platforms. Such\ntools generally can be extended through a plug-in mechanism. This way you are\nnot at the whim of the vendor to extend it. If there is a community of open source\ndevelopers who maintain freely available plug-ins, that is a good sign. It usually\nmeans that the tool is extensible and well maintained.\nA build console also should have an API for controlling it. You will want to be\nable to write tools that interact with it, kick off jobs, query it, and so on. One of the\nmost simple and useful APIs is an RSS feed of recently completed builds. Many\nother systems can read RSS feeds.\n.\nCase Study: RSS Feeds of Build Status\nStackExchange has an internal chat room system. It has the ability to monitor\nan RSS feed and announce any new entries in a given room. The SRE chat room\nmonitors an RSS feed of build completions. Every time a build completes,\nthere is an announcement of what was built and whether it was successful,\nplus a link to the status page. This way the entire team has visibility to their\nbuilds.\n9.5 Continuous Integration\nContinuous integration (CI) is the practice of doing the build phase many times\na day in an automated fashion. Each run of the build phase is triggered by\nsome event, usually a code commit. All the build-phase steps then run in a fully\nautomated fashion.\nAll builds are done from the main trunk of the source code repository. All\ndevelopers contribute code directly to the trunk. There are no long-lived branches\nor independent work areas, created for feature development.\n",
      "content_length": 2327,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "206\nChapter 9\nService Delivery: The Build Phase\nThe term “continuous” refers to the fact that every change is tested. Imagine\na graph depicting which changes were tested. In CI, the line is unbroken—that is,\ncontinuous. In other methodologies, not every release is tested and the line would\nbe broken or discontinuous.\nThe beneﬁt of triggering the build process automatically instead of via a man-\nual trigger is not just that no one has to stand around starting the process. Because\nthe process is triggered for each code commit, errors and bad code are discov-\nered shortly thereafter. In turn, problems are easier to ﬁnd because of the “small\nbatches” principle discussed in Section 8.2.4, and they are easier to ﬁx because the\ncontext of the change is still in the developer’s short-term memory. There is also\nless of a chance that new changes will be made on top of buggy code; such changes\nmay need to be reversed and reengineered, wasting everyone’s time.\nDoing the process so frequently also ensures that it stays automated. Small\nchanges in the process that break the system can be ﬁxed while they are small.\nContrast this to monthly builds: by the time the next build runs, much of the pro-\ncess may have changed. The breakage may be large and the temptation to returning\nto doing things manually becomes a real issue.\n.\nDon’t Register Packages Yourself\nOnly packages built via the console are allowed to be registered in the package\nrepository. In other words, people can’t register packages. Usually developers\nwill have a way to run the entire build phase from their own workstation so\nthat they can maintain and debug it. However, just because they can create\na package doesn’t mean they should be allowed to upload that package into\nthe repository. The packages they build may be somehow dependent on their\nworkstation or environment, either accidentally or intentionally. Having all\npackages be built through the console ensures that anyone can build the pack-\nage. Thus a good rule is this: ofﬁcial packages are always built by the build\nconsole automation.\nGoing from manual builds to fully automated builds can be very difﬁcult.\nThere are three key ingredients. First, make sure that you can do the process manu-\nally from beginning to end. This may be a challenge if builds are done infrequently,\ndifferent people do the process differently, or different people do separate steps.\nSecond, make sure the source ﬁles are all stored in a source control system. If one\nstep in the build process is to walk over to the graphic artist to ask if there are\nany updated images, you have a broken process. Graphic artists, when they have\nan updated image, should be able to check the ﬁle into the repository. The build\n",
      "content_length": 2729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "9.6\nPackages as Handoff Interface\n207\nsystem should simply take the most recently committed ﬁle. The third ingredient\nis to automate each step until it requires no interactive (keyboard/mouse) input.\nAt that point all the steps can be loaded into the console and debugged, to ﬁnally\nbecome the new ofﬁcial process.\nCI may seem like the obvious thing to do but you would be surprised at how\nmany major companies do builds very infrequently.\n.\nThe Risky 14-Day Build\nA San Francisco software company with which most system administrators\nare familiar had a build process that was manual and took two weeks to com-\nplete. Its software releases happened on the ﬁrst day of each quarter. Two\nweeks beforehand, an engineer would start doing the build. If the develop-\ners were late, the build would have even less time. The software was built\nfor three Windows releases and a dozen Linux/UNIX variations. Some of the\nbuild steps were so unreliable and ad hoc that it put the entire process at risk.\nBugs found along the way would be hot patched for that particular OS so as\nto not put the schedule at risk by starting over. Each quarter the company was\nat risk of shipping late, which would be highly visible and embarrassing.\nA newly hired build engineer was shocked at the process and informed\nthe chief technology ofﬁcer (CTO) that automating it would be a top priority.\nThe CTO disagreed and explained that it didn’t need to be automated: “I hired\nyou to do it!”\nThe engineer automated it anyway. Within a few months the entire pro-\ncess was automated for all operating systems. Later it was put into a build\nconsole and CI was achieved.\nThis transformed the development organization. Developers were now\nmore productive and produced better software. The release engineer could\nthen focus on more important and more interesting work.\nIn this situation, defying the CTO was the right thing to do. This person\nis a hero.\n9.6 Packages as Handoff Interface\nBefore we move on to the deployment phase, we need to pause and discuss the\nhandoff that happens between phases.\nThe handoff step between the build phase and the deployment phase involves\nthe delivery of an installation package. Using packages makes it easier to ensure\n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "208\nChapter 9\nService Delivery: The Build Phase\nthat the same bits are used in testing as well as deployment. Deployment may\nhappen hours or weeks after testing and it is important that the same ﬁles be used\nfor other deployments. Otherwise, untested changes can slip in.\nIt is easier to manage a package than the individual ﬁles. Packages are usually\ncryptographically hashed or digitally signed so that the receiving end can verify\nthat the ﬁle was not altered along the way. It is safer to upload a single ﬁle than a\nhierarchy of ﬁles. Because the metadata is encoded inside the package, there is no\nworry that it will be accidentally changed along the way.\nOther mechanisms for passing ﬁles between phases should be avoided. We’ve\nseen individual ﬁles emailed between teams, losing any permissions and owner-\nship metadata. We’ve seen organizations that do their handoff by placing all the\nindividual ﬁles in a particular subdirectory on a ﬁle server. There was no easy way\nto tell if the ﬁles had changed between test and deploy. The person maintaining the\ndirectory could not prepare the next release until we were done with the current\nﬁles. In this system there was no way to access older releases. Even worse, we’ve\nseen this process involve a subdirectory in a particular person’s home directory,\nwhich meant if that individual left the company, the entire process would break.\nAlso avoid using the source code repository as the handoff mechanism. Source\nrepositories usually have a feature that lets you label, or tag, all ﬁles at a particu-\nlar moment in time. The tag name is then given to the deployment phase as the\nhandoff. This approach may create many problems. For example, it is difﬁcult to\nverify the integrity of all the ﬁles and the metadata. Permissions may be acciden-\ntally changed. File ownership, by design, is changed. If this technique is used and\nbinaries are checked into the repository, the repository will grow quite large and\nunwieldy. If binaries are not checked in, it means deployment will have to build\nthem. This is a duplication of effort and risks introducing subtle changes that will\nhave bypassed testing.\n9.7 Summary\nThis chapter was an overview of service delivery and a detailed examination of the\nbuild phase. The next chapter will examine the deployment phase in detail.\nService delivery comprises the technical processes involved in turning source\ncode into a running service. The service delivery platform is the software and\nautomation that drives the process.\nService delivery has two phases: build and deploy. Build turns source code\ninto packages. Deploy takes the packages and deploys them in an environment.\nThere are different environments for different purposes. The test environment is\nused to test the service. The live environment is where the service runs to pro-\nvide service for customers. The two environments should be engineered and built\nthe same way to make testing as meaningful as possible. The test environment\n",
      "content_length": 2984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Exercises\n209\ntypically differs only in its size and the fact that it stores ﬁctional data instead\nof real user data. There are many other environments, including some used for\nexploratory testing, performance testing, beta testing, and so on.\nA ﬂow is one trip through the service delivery platform. Each step includes\ntesting with the aim of ﬁnding bugs as early as possible. Bugs found in the live envi-\nronment are the result of insufﬁcient testing in earlier phases and environments.\nWhen a service delivery platform is functioning at its best, the result is high\nconﬁdence in the service being delivered. As a result, the organization can be\nmore aggressive with making changes, and features can be released faster. In other\nwords, innovation is accelerated. Compare this to an organization that is unsure of\nits delivery system. In such a case, releases take months to produce. Bugs are found\nlong after code is written, making it more difﬁcult to correct problems. Innovation\nis hindered because change becomes stiﬂed as fear and despair rule.\nThe handoff from build to deploy is a package, a ﬁle that contains many ﬁles. A\npackage is easier to transport, more secure, and less error prone than transporting\nmany individual ﬁles.\nExercises\n1. Describe the steps of the service delivery platform’s build phase.\n2. Why are the build and deployment phases separate?\n3. Why are the application and infrastructure ﬂows separate?\n4. What are the pros and cons of the waterfall methodology compared with the\nmodern DevOps methodology?\n5. How does “good ﬂow” enable a business to reach its goals?\n6. Describe continuous integration and its beneﬁts.\n7. Why do testing and production need separate environments?\n8. Describe the service delivery platform used in your organization. Based on the\nbeneﬁts described in this chapter, which changes would you recommend?\n9. What are the beneﬁts of using packages as the handoff mechanism?\n10. Does your your organization use CI? Which parts of your current platform\nwould need to change to achieve CI?\n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "Chapter 10\nService Delivery: The\nDeployment Phase\nLet ’er roll!\n—Elvia Allman\nto Lucy and Ethel\nIn the previous chapter we examined the build phase, which ends with the creation\nof a software package. In this chapter we’ll examine the deployment phase, which\nuses the package to create a running service.\nThe deployment phase creates the service in one or more testing and produc-\ntion environments. Deciding if a release used in the testing environment is ready\nto be used in the production environment requires approval.\nThe goal of the deployment phase is to create a running environment. This\nenvironment is then used for testing or for live production services.\nAs Figure 9.1 (page 198) showed, packages are retrieved from the package\nrepository and then installed and conﬁgured to create an environment. The envi-\nronment created may be the testing environment, which is set up to verify that all\nthe pieces of the system work together. It may also be the live environment, which\nprovides service to users. Alternatively, it may be one of the other environments\ndescribed previously in Section 9.1.1.\n10.1 Deployment-Phase Steps\nThere are three steps in the deployment phase: packages are promoted, installed,\nand conﬁgured.\n211\n",
      "content_length": 1235,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "212\nChapter 10\nService Delivery: The Deployment Phase\n10.1.1 Promotion\nThe promotion step is where a release is selected and promoted for use in the\ndesired environment. The desired version is selected and marked as the right\nversion for the environment being built.\nFor example, suppose building an environment requires three packages called\nA, B, and C. Each trip through the build phase results in a new package. Package\nA has versions 1.1, 1.2, and 1.3. B has versions 1.1, 1.2, 1.3, and 1.4. There are more\nversions because there have been more check-ins. C has versions 1.1 and 1.3 (1.2 is\nmissing because there was a build failure).\nLet’s say that the combination of A-1.2, B-1.4, and C-1.3 has been tested\ntogether and approved for production. The promotion step would tell the package\nrepository to mark them as the designated production versions.\nSelecting speciﬁc versions like this is generally done for production, beta, and\nearly access environments, as mentioned in Section 9.1.1. However, development\nand testing environments may simply use the latest release: A-1.3, B-1.4, and C-1.3.\nHow packages are marked for particular environments depends on the pack-\nage repository system. Some have a tagging mechanism, such that only one version\nof a particular package can have the “production” tag at a time. There may be many\npackages, each with one version designated as the production version. There is\nusually a virtual tag called “latest” that automatically refers to the newest version.\nSome repository systems use a technique called pinning. A package is pinned\nat a particular version, and that version is always used even if newer versions are\navailable. While tags are usually global, pinning is done at the environment level.\nFor example, testing and live environments would each have packages pinned at\ndifferent versions.\nOther package repository systems work very differently. They can store only\none version of a package at a given time. In this case, there will be multiple repos-\nitories and packages will be copied between them. For example, all new packages\nwill be put in a repository called “development.” When the package is ready to\nbe used in the testing environment, it is copied to a repository called “testing.”\nAll machines in the testing environment point at this repository. If the package\nis approved for use in production, it is copied to a third repository called “pro-\nduction”; all the machines in the production environment read packages from this\nrepository. The beneﬁt of this kind of system is that the production environment\ncannot accidentally install unapproved packages because it does not know they\nexist. Nevertheless, keeping a history of past package versions is more difﬁcult,\nsometimes done by some kind of separate archive subsystem.\n10.1.2 Installation\nIn the installation step, the packages are copied to machines and installed. This\nis done by an installer that understands the package format. Most operating\n",
      "content_length": 2974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "10.1\nDeployment-Phase Steps\n213\nsystems have their own installer software, each generally tied to its native package\nrepository system.\nA package can include scripts to run before and after installation. The actual\ninstallation process involves running the pre-install script, copying ﬁles from the\npackage to their ﬁnal destination, and then running the post-install script. Pre-\ninstall scripts do tasks like creating directories, setting permissions, verifying\npreconditions are met, and creating accounts and groups that will own the ﬁles\nabout to be installed. Post-install scripts do tasks like copying a default conﬁgura-\ntion if one doesn’t already exist, enabling services, and registering the installation\nwith an asset manager. Post-install scripts can also perform smoke tests, such as\nverifying the ability to access remote services, middleware, or other infrastructure\nservices such as databases.\n10.1.3 Configuration\nIn the configuration step, local settings and data are put in place to turn the\ninstalled package into the running service.\nWhile packages often include installation scripts that do some generic conﬁg-\nuration, this step does machine-speciﬁc work required to create a working service.\nFor example, installing a web server package creates a generic web server conﬁg-\nured to host static ﬁles from a particular directory. However, determining which\ndomains are served from this machine, making the load balancer aware of its\npresence, and other tasks are speciﬁc to the machine and would be done here.\nThis step is gated by health checks—that is, a few simple tests that verify the\nsystem is running. For example, one common health check is done by requesting\na particular URL that responds only after carrying out a few quick internal tests.\nThere are many software frameworks for conﬁguration management. Some\npopular ones include CFEngine, Puppet, and Chef. They all permit the creation\nof modules for conﬁguring speciﬁc services and applying those modules to dif-\nferent machines as needed. Conﬁguration management is discussed further in\nSection 12.6.4.\nThe two major strategies for conﬁguration are called convergent orchestra-\ntion and direct orchestration. Convergent orchestration takes a description of how\nthe environment should be conﬁgured, and the conﬁguration management system\nthen makes individual changes that lead to the entire system converging on that\ndesired state. If for some reason an undesired change is made (accidentally by a\nuser, on purpose by a user, or by an external event such as a machine failure), the\norchestration system will detect this and make changes until the desired conﬁg-\nuration has converged again. When the next conﬁguration is deﬁned, the system\nstarts to converge toward this new deﬁnition, making the fewest changes required\nto get there. Convergent orchestration can be described as getting the environment\nto a particular state and keeping it there.\n",
      "content_length": 2934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "214\nChapter 10\nService Delivery: The Deployment Phase\nDirect orchestration can be described as a method to execute a multistep pro-\ncess during which certain invariants hold true. For example, moving a database\nfrom one machine to another requires many steps that must happen in a certain\norder, all while the the invariant of “clients always have access to the database”\nremains true.\nThe steps might be as follows:\n1. Machine B is conﬁgured to be a database replica.\n2. Wait for the replica to become synchronized with the primary database.\n3. The database clients are put in temporary read-only mode.\n4. The roles of machines A and B are swapped, making A the read-only replica.\n5. The database clients are taken out of read-only mode and conﬁgured to send\nwrites to machine B.\nAchieving this goal with convergent orchestration would require an unwieldy\nprocess. It would require creating a desired state for each step, and waiting for\none state to be achieved before switching to the next.\nA challenge in direct orchestration is how to handle multiple processes hap-\npening at the same time. For example, imagine this process happening at the same\ntime as a load balancer is being added to the system and the web servers are being\nreconﬁgured to add a new service. These processes all involve overlapping sets of\nmachines and resources. The steps have to be ordered and coordinated in ways\nthat prevent conﬂicts and assure that the system does not paint itself into a corner.\nCurrently this is done manually, which is an error-prone process. Automating such\nthings at a large scale is the kind of thing that researchers are only just beginning\nto consider.\nOne of the barriers to moving to convergent orchestration is that systems have\nto be architected to support it. This is a major problem for enterprises automating\ntheir deployment and infrastructure management processes, especially with com-\nmercial products that cannot be modiﬁed. Home-grown systems can be designed\nwith support from the start or, alternatively, modiﬁed after the fact.\n10.2 Testing and Approval\nBefore a release is used in production, it must be tested and approved. First, auto-\nmated testing is done. Next, manual testing, if there is any, is performed. Lastly,\nmanagement approves or signs off on the release. The list of people or departments\nthat must sign off on a release is called the approval chain. After all this activity is\ncomplete, the release can be promoted and pushed into production.\n",
      "content_length": 2483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "10.2\nTesting and Approval\n215\n10.2.1 Testing\nTesting involves many different categories of tests. In the build phase, unit testing\nis performed on each component. There are four kinds of testing in the deployment\nphase:\n• System Testing: This testing brings together all the various pieces of the ser-\nvice and tests the ﬁnal product or system. It is performed on the service\nrunning in the testing environment. Passing these tests is a precondition for\nthe release being used in production and any other environment that includes\nexternal customers. Every individual feature should be tested. Multistep\nworkﬂows such as making a purchase should also be tested. There are test-\ning frameworks that can perform tests as if they are being done by a user.\nFor example, Selenium WebDriver is an open source project that automates\nweb site testing by sending HTTP requests as if they came from various web\nbrowsers. No matter how a user interacts with software, there is a testing tool\nthat can automate the tests. This includes PC-based GUIs, APIs, consoles/\nkeyboards, mobile phones, and, as documented in Gruver, Young & Fulghum\n(2012), even the front panels of laser printers.\n• Performance Testing: These tests determine the speed of the service under\nvarious conditions. This testing is performed on the service while in the\ntesting environment or in a specially built performance testing environ-\nment. It should determine if the performance meets written speciﬁcations or\nrequirements. All too often, however, such speciﬁcations are nonexistent or\nvague. Therefore often the results of this testing are just compared to pre-\nvious results. If the entire system, or a speciﬁc feature, works signiﬁcantly\nmore slowly than the previous release—a performance regression—the test\nfails.\n• Load Testing: This special kind of performance testing determines how much\nload the system can sustain. It is usually done in the testing environment or\nin a special performance testing environment. Such testing involves subject-\ning the service to increasingly larger amounts of trafﬁc, or load, to determine\nthe maximum the system is able to process. As an example, Google does not\nuse a new Linux kernel without ﬁrst doing load testing to verify the kernel\nchanges have not negatively affected how much load a search cluster can sus-\ntain. An entire cluster is built with machines running this kernel release. Search\nqueries are artiﬁcially generated at larger and larger QPS. Eventually the clus-\nter maxes out, unable to do more QPS, or the system gets so slow that it cannot\nanswer queries in the required number of milliseconds. If this maximum QPS\nis signiﬁcantly less than the previous release, Google does not upgrade to that\nkernel.\n",
      "content_length": 2731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "216\nChapter 10\nService Delivery: The Deployment Phase\n• User Acceptance Testing (UAT): This testing is done by customers to verify\nthat the system meets their needs and to verify claims by the producer. Cus-\ntomers run their own tests to verify the new release meets their requirements.\nFor example, they might run through each business process that involves\nthe service. System testing involves developers making sure that they don’t\nship products with defects. UAT involves customers making sure they don’t\nreceive products with defects. Ideally, any test developed for UAT will be\nmade known to the developers so that it can be added to their own battery\nof tests. This would verify such concerns earlier in the process. Sadly this\nis not always possible. UAT may include tests that use live data that cannot\nbe shared, such as personally identiﬁable information (PII). UAT also may be\nused to determine if an internal process needs to be revised.\n.\nEvery Step Has a Gate\nThe build process at StackExchange has a few different handoffs. Each is\ngated by a test so that defects aren’t passed forward, to use the assembly-line\nanalogy. There are many kinds of tests, each a separate module. The service\ndelivery ﬂow has the following handoffs and gates:\n1. Code is built and then packaging is gated by unit tests.\n2. Digital signature veriﬁcation gates whether the packager can pass code\nto the test environment.\n3. Promoting a release from the test environment to production is gated by\nsystem tests.\n4. Production environment upgrade success is gated by health checks.\n10.2.2 Approval\nIf all the tests pass, the release is called a production candidate. Candidates are put\nthrough an approval process. If they are approved they are installed in production.\nAt this point, the members of the approval chain are asked to sign off on\nthe production candidate. The approval chain is a list of speciﬁc people, or their\ndelegates, who must sign off on production releases. For example, the list might\ninclude the product manager, the director of marketing, and the director of engi-\nneering. Often departmental approval is required from the security, legal, and\nprivacy compliance departments.\nEach environment may have a different set of tests and approval chain to gate\nwhich releases may enter it. Deploying releases in the development and testing\n",
      "content_length": 2348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "10.4\nInfrastructure Automation Strategies\n217\nenvironments is automatically approved. UAT and performance testing generally\nselect the latest release that has passed system testing. Environments that will have\nlive users, such as beta and demo environments, may be automatically upgraded\nperiodically. For example, it is common for demo environments to be wiped and\nreloaded on a speciﬁc day of the week or a certain duration before the ofﬁcial\nrelease date.\nEnvironments that will be exposed to live, or revenue-generating, customers\nshould be gated with the most scrutiny. Thus the production environment gen-\nerally requires all of the preceding tests plus positive conﬁrmation by the entire\napproval chain.\n10.3 Operations Console\nThe operations console is software that manages the operational processes, espe-\ncially the deployment steps. Like the build console, it is a web-based system that\nmakes it easy to view results and past history, and keeps statistics about success\nrates, process duration, and more.\nNearly everything said about the build console can be repeated in regard to\nthe operations console, so refer to Section 9.4. Security and authorization might\nbe more important here because processes can affect live services. For example,\nthere may tighter controls over who may initiate a launch for a new release into\nproduction.\n10.4 Infrastructure Automation Strategies\nA few strategic tips will help you fully automate the deploy phase so that it can\nrun unattended in the console. As discussed earlier, there is a ﬂow for deploy-\ning infrastructure and another ﬂow for deploying the service itself. Deploying the\nentire stack can be broken down even further: preparing and testing the physical\nor virtual machine, installing the operating system, installing and conﬁguring the\nservice. Each of these is a discrete step that can be automated separately. In fact, in\nlarge environments you’ll ﬁnd different teams responsible for each step.\n10.4.1 Preparing Physical Machines\nPreparing a physical machine involves unboxing it, mounting it in a rack, cabling\nit, conﬁguring BIOS settings, and testing. The earlier steps require physical work\nand are very difﬁcult to automate. Very few companies can afford robotics to do\nthese steps. However, we can reduce the labor by installing them one rack at a time\ninstead of one machine at a time, thereby taking advantage of economies of mass\nproduction. Another way to improve the process is to use blade servers. Blade\nservers are a technology made up of one chassis for many individual computers,\n",
      "content_length": 2561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "218\nChapter 10\nService Delivery: The Deployment Phase\neach on a “blade,” which makes installation and maintenance easier. At very large\nscale, machines are designed with speciﬁc features to enable fast and efﬁcient mass\ninstallation. Google and other companies design their own hardware to ensure\ndesign features meet their needs.\nWe have seen some impressive systems that automate the new hardware\nintake process. The Tumblr Invisible Touch system automates upgrading ﬁrmware,\nsetting up the Baseboard Management Controller (BMC), adding the machine to\nTumblr’s inventory system, performing a multi-hour stress test, and conﬁguring\nthe network.\nA quick-and-dirty solution is to manually install hardware and conﬁgure BIOS\nsettings but automate the process of verifying that the settings are correct. Since\nBIOS settings change rarely, if at all, this may be good enough for some sites, or at\nleast a good half-measure on the way to full automation.\nAnother strategy is to reduce complexity through standardization. Standard-\nizing on a few hardware conﬁgurations makes machines interchangeable. Usually\nyou create one model for heavy computation with lots of RAM and CPU, and one\nmodel for mass storage with lots of disk. Now all machines in a category can be\ntreated as a pool of machines, allocating one when needed and returning it to the\npool when it is not. You can create an API that permits such allocations to happen\nalmost as easily as creating virtual machines.\n10.4.2 Preparing Virtual Machines\nPreparing virtual machines should be a matter of making an API call. That said,\nhaving a few standard sizes can make management easier.\nFor example, one might allocate VMs in sizes such that either four small, two\nmedium, or one large VM perfectly ﬁlls the physical machine. Then there is less\nchance that a physical machine may have some space unused, but not enough to\ncreate a new VM.\nOne can also use multiples of Fibonacci numbers. If a 5-unit machine is deallo-\ncated, for example, that leaves room for ﬁve 1-unit machines, a 2-unit plus a 3-unit\nmachine, and so on.\nThis not only helps fully utilize the physical machines but also makes reor-\nganizing them easier. Imagine a situation where a medium VM is needed but the\nonly free space is the size of a small VM on one physical machine and a small VM\non another physical machine. If the VM sizes are standardized, it is easy to deter-\nmine how VMs could be moved to create a medium-sized space on one physical\nmachine. If each VM is a custom size, moving VMs around might still be possible\nbut the movements could be like the Towers of Hanoi problem, requiring many\nintermediate steps and inefﬁciencies.\n",
      "content_length": 2669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "10.4\nInfrastructure Automation Strategies\n219\n10.4.3 Installing OS and Services\nInstalling the operating system and service conﬁguration can be done in many\nways. The two main approaches are an image method and a conﬁguration man-\nagement method.\nThe image method involves creating a disk image for each kind of service.\nThis image is then copied onto the disk and, after a reboot, the machine comes up\npreconﬁgured. Images can be deployed to physical machines, virtual machines, or\ncontainers, as described in Section 3.2. The image contains the operating system, all\nrequired packages, and the service in a ready-to-run form. It is known as a baked\nimage because the service is “baked in.”\nThe configuration management strategy involves using an installer or other\nmechanism to get a minimal operating system running. Conﬁguration manage-\nment tools can then install any additional packages and bring up the service. This\ntechnique is known as frying the image, because the image is cooked up while\nyou wait.\nAutomated Baking\nBaked images can be installed more rapidly because all the conﬁguration is done\nahead of time. When turning up hundreds of machines, this can be a signiﬁcant\nwin. Unfortunately, maintaining many images can be a burden. Installing a patch\non many images, for example, is labor intensive. When installation is done this\nway, there is no version control, which is bad.\nThe solution is to automate the creation of baked images. Software frame-\nworks for creating baked images include Vagrant, Docker, and Netﬂix Aminator.\nAll of these options provide languages for describing how to build the image from\nscratch, by specifying the base OS release, packages, ﬁle settings, and so on. The\nimage is then created from this description. The description can be kept under ver-\nsion control, such that the service delivery platform can be used to build, test, and\ndeploy images.\nBaked images can be used for building a new service as well as upgrading\nan existing one. For example, if there are 10 web servers behind a load balancer,\nupgrading them involves taking each one out of the load balancer’s rotation,\ndeleting it, and re-creating it from the image. All 10 web servers running the new\nrelease will then have a well-understood conﬁguration.\nPersistent Data\nNot all machines can simply be wiped and reinstalled in this way. For example,\na database server or ﬁle server has irreplaceable data that is not part of the\nbuild or conﬁguration process. The solution in such a case is to put such data\non virtual disks, mounted from a storage area network (SAN) or other remote\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "220\nChapter 10\nService Delivery: The Deployment Phase\nstorage system. The boot disk is replaced, but on boot-up it mounts its virtual\ndisk, thereby reattaching the system to the data it needs. By decoupling where\nstorage is provided from where it is used, machines become more disposable.\nBaked versus Fried\nConﬁguration management is often faster than upgrading a machine by installing\nan image. Conﬁguration management is also less disruptive, because it makes just\nthe minimum number of changes needed to achieve the desired conﬁguration.\nThose same 10 web servers needing to be upgraded can simply be individually\nrotated out of the load balancer and only the desired package upgraded. With this\ntechnique, the machines are not wiped, so no data is lost.\nOn the downside, the machines are now in a less well-understood conﬁgura-\ntion. Imagine that an 11th web server is added. It starts out with the new software\nrelease. Does this 11th machine have the same conﬁguration as the 10 that got to\ntheir current state through upgrades? In theory, yes, but small differences may\ncreep in. Future upgrades require testing that covers both situations, which adds\ncomplexity. Proponents of baked images would argue that it is better to refresh a\nmachine from scratch than to let entropy accumulate.\n.\nDifferent Files for Different Environments\nOften there is a group of ﬁles that must be different in the testing and produc-\ntion environments. For example, logos and other images might have special\nversions used solely in the actual live service. There may also be special\ncredentials, certiﬁcates, or other data.\nOne way to handle this situation is to include both sets of ﬁles in the pack-\nage and use conﬁguration management to point the server at the proper set.\nHowever, this approach does not work for certiﬁcates and other ﬁles that one\nmight not want to expose to all environments.\nAnother solution is to move the environment-speciﬁc ﬁles into separate\npackages. Each environment then has its own configuration package. The\ninstallation process would install the main application package plus the one\nconﬁguration package appropriate for that environment.\nThe problem with using environment-speciﬁc packages is that these ﬁles\nhave, essentially, bypassed the testing process. For that reason it is best to keep\nthe use of this mechanism to an absolute minimum, preferably restricting it\nto ﬁles that are low risk or ﬁles that can be tested other ways, such as via\npre-submit checks.\n",
      "content_length": 2488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "10.6\nInfrastructure as Code\n221\n10.5 Continuous Delivery\nContinuous delivery (CD) is the technique in which testing is fully automated and\ntriggered to run for each build. With each build, the testing environment is created,\nthe automated testing runs, and the release is “delivered,” ready to be considered\nfor use in other environments. This doesn’t mean every change is deployed to\nproduction, but rather that every change is proven to be deployable at any time.\nCD has similar beneﬁts as continuous integration. In fact, it can be consid-\nered an extension to CI. CD makes it economical and low risk to work in small\nbatches, so that problems are found sooner and, therefore, are easier to ﬁx. (See\nSection 8.2.4.)\nCD incorporates all of continuous integration, plus system tests, performance\ntests, user acceptance tests, and all other automated tests. There’s really no excuse\nnot to adopt CD once testing is automated. If some tests are not automated, CD\ncan deliver the release to a beta environment used for manual testing.\n10.6 Infrastructure as Code\nRecall in Figure 9.1 that the service delivery platform (SDP) pattern ﬂow has quad-\nrants that represent infrastructure as well as applications. The infrastructure for all\nenvironments should be built through automation that is treated just like any other\nservice delivered by the SDP.\nThis approach is called infrastructure as code. Like application code, the code\nthat describes the infrastructure is stored in the source repository, revision con-\ntrolled, and tested in a test environment before being approved for deployment in\nthe live production environment.\nConﬁguration code includes the automation to do conﬁguration management\nas well as any conﬁguration ﬁles and data. Conﬁguration management code and\ndata are built, packaged, and so on just like application software. Even images for\nVMs and containers can be built and packaged.\nWhen the infrastructure as code technique is done correctly, the same code\ncan build development, testing, and production environments. Each environment\ndiffers only in terms of which machines to use, the number of replicas, and\nother settings. This minimizes the difference between the testing and production\nenvironments and makes testing more accurate.\nAnyone should be able to build an environment for development or testing.\nThis lets teams and individuals operate and experiment without needing to bother\noperations. Developers and QA personnel can build multiple test environments.\nSWEs can build their own sandbox environments, using virtual machines on their\nlaptops.\n",
      "content_length": 2581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "222\nChapter 10\nService Delivery: The Deployment Phase\nInfrastructure as code becomes easier to implement as hardware becomes\nmore and more virtual. Over time, storage, machines, and networks have all gone\nvirtual. Storage volumes, virtual machines, and network topologies can be created,\nmanipulated, and deallocated through software and controlled by APIs.\n10.7 Other Platform Services\nA few other services are involved in an SDP and are worth a brief mention in this\nchapter:\n• Authentication: There needs to be some form of authentication so that the\nsystem can restrict who can access what. In turn, security facilities are needed\nthat provide authentication, authorization, and accounting (AAA). Often this\nservice consists of LDAP plus Kerberos, Open Directory, or Active Directory.\n• DNS: DNS translates names of machines to IP addresses. In an SDP, machines\nare brought up and turned down rapidly. It is important to have the ability to\nupdate DNS zones via an API or dynamic DNS (DynDNS/DDNS).\n• Configuration Management Database (CMDB): The deployment phase\nshould be database driven. Conﬁgurations and machine relationships are\nstored in a database, and the tools that build the environment use this informa-\ntion to guide their work. This means modifying the environment is as simple\nas updating a database. For example, the database might store the list of web\nserver frontends associated with a particular service. By adding to this list, the\nconﬁguration management tools will bring up web servers on those machines.\n10.8 Summary\nThe deployment phase of the software delivery platform is where software pack-\nages are turned into a running service. First the service is run in a testing environ-\nment. After an approval process is passed, the packages are used to build the live\nproduction environment.\nThe deployment phase involves selecting a group of packages that represent\nall the parts of the service. Together they are considered a release candidate. They\nare installed on the machines that make up the environment. The service is then\nconﬁgured and becomes ready for testing or use.\nAn environment requires infrastructure: hardware, network, and other com-\nponents. Virtual infrastructure can be conﬁgured through automation. Physical\nmachines can also be manipulated through software, although doing so requires\nmore planning and is slower and less ﬂexible. When infrastructure conﬁguration\nis encoded in software, the entire infrastructure can be treated as software, with all\nthe beneﬁts of source code: revision control, testing, and so on.\n",
      "content_length": 2568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "Exercises\n223\nBefore a release is used in the live environment, it must pass many tests.\nSystem tests check the system as a whole. Performance and load testing verify the\nsoftware’s performance. User acceptance testing is an opportunity for stakeholders\nto approve a release. There may be other approvals such as those issued by legal,\nmarketing, and product management.\nContinuous delivery is achieved when the deployment phase and all testing\nis automated. New release candidates are produced automatically and conﬁdence\nis improved as the testing becomes more and more extensive.\nService delivery engineering is a large and changing ﬁeld. We have only\nscratched the surface in this chapter. We recommend the book Continuous Delivery\n(Humble & Farley 2010) for a deeper look at the subject.\nExercises\n1. Describe the steps of the service delivery platform’s deployment phase.\n2. Describe continuous delivery and its beneﬁts.\n3. Which kinds of testing are common in service delivery?\n4. In your organization’s service delivery platform, which of the kinds of testing\nlisted in Section 10.2 are and aren’t done? Which beneﬁts would you hope to\ngain by adding the missing tests?\n5. What is the approval chain for software in your organization’s service delivery\nprocess? How are the approvals requested and responses communicated?\n6. How could the approval chain in your organization be automated?\n7. Apply the deployment-phase techniques from this chapter to an organization\nthat does not do software development, but instead chooses to use off-the-\nshelf commercial software.\n8. In your environment, what are the steps to prepare a new machine? Which are\nautomated? How could the non-automated steps be automated?\n9. Does your your organization use continuous delivery? Which parts of your\ncurrent platform would need to change to achieve CD?\n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "Chapter 11\nUpgrading Live Services\nThe things that make you strong,\nand make you feel as though you’ve\naccomplished something, are not\nthe easy ones.\n—Dr. Jerri Nielsen\nThis chapter is about deploying new releases to the production environment. It is\ndifferent from deploying into any other environment.\nThe process of upgrading an environment with a new software release is called\na code push. Pushing code into production can be tricky because we are modify-\ning a system while it is running. This is like changing the tires of a car while it is\nspeeding down the highway at 90 km/h: you can do it, but it requires a lot of care\nand planning.\nLuckily there are many techniques available, each appropriate for different\nsituations. This chapter catalogs the most common techniques. We then discuss\nbest practices such as continuous deployment and other practical matters.\n11.1 Taking the Service Down for Upgrading\nOne way to upgrade a service is to take it down, push the new code out to all\nsystems, and bring the service back up. This has the beneﬁt of being very simple to\nimplement, and it permits testing of the service before real users are given access\nto the newly upgraded service.\nSadly, this technique requires downtime, which makes it unacceptable for\nmost services. However, it may be appropriate for development and demo envi-\nronments where prescheduled downtime may be permitted.\nThis technique also works when the service is replicated in its entirety. Each\nservice replica can be taken down, upgraded, and brought back up if there is\nenough spare capacity. In this case, usually a global load balancer (GLB) divides\n225\n",
      "content_length": 1640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "226\nChapter 11\nUpgrading Live Services\ntrafﬁc among the working replicas. One replica at a time is drained by removing\nit from the GLB and waiting until all in-ﬂight requests are completed. The replica\ncan then be taken down without affecting the service.\n.\nUpgrading Blog Search\nWhen Tom was an SRE for Google’s Blog Search service, the customer-facing\nstack was replicated in four datacenters. Each replica was independent of the\nothers. There was enough capacity that any one stack could be down and the\nothers could handle the entire trafﬁc load. One at a time, each stack would\nbe drained by removing it from the GLB, upgrading it, checking it, and then\nadding it back to the GLB.\nMeanwhile, another part of the system was the “pipeline”: a service that\nscanned for new blog posts, ingested them, produced the new corpus, and dis-\ntributed it to the four customer-facing stacks. The pipeline was very important\nto the entire service, but if it was down customers would not notice. However,\nthe freshness of the search results would deteriorate the longer the pipeline\nwas down. Therefore uptime was important but not essential and upgrades\nwere done by bringing down the entire pipeline.\nMany services at Google were architected in a similar way and upgrades\nwere done in a similar pattern.\n11.2 Rolling Upgrades\nIn a rolling upgrade, individual machines or servers are removed from ser-\nvice, upgraded, and put back in service. This is repeated for each element being\nupgraded; the process rolls through all of them until it is complete.\nThe customer sees continuous service because the individual outages are hid-\nden by a local load balancer. During the upgrade, some customers will see the\nnew software and some will see the old software. There is a chance that a partic-\nular customer will see new features appear and disappear as sequential requests\ngo to new and old machines. This is rare due to load balancer stickiness, discussed\nin Section 4.2.3, and other factors, such as deploying new features toggled off, as\ndescribed in Section 2.1.9.\nDuring the upgrade, there is a temporary reduction in capacity. If there are\n10 servers, as each is upgraded the service is at 90 percent capacity. Therefore this\ntechnique requires planning to assure there is sufﬁcient capacity.\nThe process works as follows. First the server or machine is drained. This can\nbe done by reconﬁguring the load balancer to stop sending requests to it or by\n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "11.3\nCanary\n227\nhaving the replica enter “lame duck mode,” as described in Section 2.1.3, where\nit “lies,” telling the load balancer it is unhealthy so that the load balancer stops\nsending requests to it. Eventually no new trafﬁc will have been received for a while\nand all in-ﬂight requests will be ﬁnished. Next the server is upgraded, the upgrade\nis veriﬁed, and the draining process is undone. Then the upgrade process begins\nagain with the next server.\n.\nAvoiding Code Pushes When Sleepy\nThe best time to do a code push is during the day. You are wide awake and\nmore co-workers are available if something goes wrong.\nMany organizations do code pushes very late at night. The typical excuse\nfor a 3 upgrade is that the upgrade is risky and doing it late at night\ndecreases exposure.\nDoing critical upgrades while half-asleep is a much bigger risk. Ideally,\nby now we’ve convinced you that a much better strategy for reducing risk is\nautomated testing and small batches.\nAlternatively, you can have a team eight time zones east of your primary\nlocation that does code pushes. Those deployments will occur in the middle\nof the night for your customers but not for your team.\n11.3 Canary\nThe canary process is a special form of the rolling upgrade that is more appropri-\nate when large numbers of elements need to be upgraded. If there are hundreds\nor thousands of servers or machines, the rolling upgrade process can take a long\ntime. If each server takes 10 minutes, upgrading 1000 servers will take about\na week. That would be unacceptable—yet upgrading all the servers at once is\ntoo risky.\nThe canary process involves upgrading a very small number of replicas, wait-\ning to see if obvious problems develop, and then moving on to progressively larger\ngroups of machines. In the old days of coal mining, miners would bring caged\ncanaries into the mines. These birds are far more sensitive than humans to harmful\ngases. If your canary started acting sick or fell from its perch, it was time to get out\nof the mine before you became incapacitated by the gases.\nLikewise, the canary technique upgrades a single machine and then tests it for\na while. Problems tend to appear in the ﬁrst 5 or 10 minutes. If the canary lives, a\ngroup of machines are upgraded. There is another wait and more testing, and then\na larger group is upgraded.\n",
      "content_length": 2338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "228\nChapter 11\nUpgrading Live Services\nA common canary process is to upgrade one server, then one server per minute\nuntil 1 percent of all servers are upgraded, and then one server per second until all\nare upgraded. Between each group there may be an extended pause. While this is\nhappening, veriﬁcation tests are run against all the upgraded servers. These tests\nare usually very simplistic, generally just verifying that the code is not crashing\nand live queries are succeeding.\nIf trouble is found (i.e., if the canary dies), the process is stopped. At this point\nthe servers that were upgraded can be rolled back. Alternatively, if there is enough\ncapacity, they can be shut down until a new release becomes available.\nCanarying is not a testing process. The canary process is a method for deploy-\ning a release into production that detects bad pushes and prevents them from\nbeing visible to users. The main difference between a testing process and a canary\nprocess is that it is acceptable for the test process to fail. If a testing process fails,\nyou’ve prevented a bad release from reaching live users. More pedantically, the\ntesting process has succeeded in detecting a defective release. That is a good thing.\nConversely, you don’t want a canary process to fail. A failed canary means\nsomething was missed by the testing process. A failed canary should be so rare\nthat it is cause to stop development and dedicate resources to determining what\nwent wrong and which additional testing needs to be added to prevent this failure\nin the future. Only then can new roll-outs begin. Canarying is an insurance policy\nagainst accidental bad releases, not a way to detect bad releases.\nTesting and canarying are often conﬂated, but should not be. What some peo-\nple call canarying is really testing new releases on live users. Testing should be\ndone in a testing environment, not on live users.\n.\nCanarying Is Not a Substitute for System Testing\nWe’ve observed situations where canarying was used to test new releases on\nlive users. In one case it was done unintentionally—a fact that was not realized\nuntil a major outage occurred. The SREs received a thoroughly tested package\nand would canary it into their production environment. This worked ﬁne for\nmany years because the test and live environments were very similar.\nOver time, however, many tools were developed by the SREs for use in\nthe production environment. These tools were not tested by the developers’\ntesting system. The developers were not responsible for the tools, plus many\nof the tools were considered ad hoc or temporary.\nThere’s an old adage in engineering, “Nothing is more permanent than a\ntemporary solution.” Soon these tools grew and begat complex automated sys-\ntems. Yet, they were not tested by the developers’ testing system. Each major\n",
      "content_length": 2818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "11.4\nPhased Roll-outs\n229\n.\nrelease broke the tools and the operations staff had to scurry to update them.\nThese problems were trivial, however, compared to what happened next.\nOne day a release was pushed into production and problems were not\ndiscovered until the push was complete. Service for particular users came to\na halt.\nBy now the hardware used in the two environments had diverged enough\nthat kernel drivers and virtualization technology versions had diverged. The\nresult was that virtual machines running certain operating systems stopped\nworking.\nAt this point the SREs realized the environments had diverged too much.\nThey needed to completely revamp their system testing environment to make\nsure it tested the speciﬁc combination of main service release, kernel version,\nvirtualization framework version, and hardware that was used in production.\nIn addition, they needed to incorporate their tools into the repository and\nthe development and testing process so that each time they wouldn’t have\nto scramble to ﬁx incompatibilities with the tools they had developed.\nCreating a proper system testing environment, and a mechanism to keep\ntest and production in sync, required many months of effort.\n11.4 Phased Roll-outs\nAnother strategy is to partition users into groups that are upgraded one at a time.\nEach group, or phase, is identiﬁed by its tolerance for risk.\nFor example, Facebook has clusters dedicated to providing service to its own\nemployees. These clusters receive upgrades ﬁrst because their employees are will-\ning testers of new releases—it’s part of their job. Next, a small set of outside-user\nclusters are upgraded. Lastly, the remaining clusters are upgraded.\nStack Exchange’s upgrade process involves many phases. Stack Exchange has\nmore than 110 web communities, plus each community has a meta-community\nassociated with it for discussing the community itself. The same software is used\nfor all of these communities, though the colors and designs are different. The\ndeployment phases are the test environment, then the meta-communities, then\nthe less populated communities, and lastly the largest and most active commu-\nnity. Each phase starts automatically if the previous phase saw no problems for\na certain amount of time. By the time the upgrade reaches the last phase, Stack\nExchange has high conﬁdence in the release. The earliest phases can tolerate more\noutages for many reasons, including the fact that they are not revenue-generating\nunits.\n",
      "content_length": 2485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "230\nChapter 11\nUpgrading Live Services\n11.5 Proportional Shedding\nProportional shedding is a deployment technique whereby the new service is built\non new machines in parallel to the old service. Then the load balancer sends, or\nsheds, a small percentage of trafﬁc to the new service. If this succeeds, a larger\npercentage is sent. This process continues until all trafﬁc is going to the new service.\nProportional shedding can be used to move trafﬁc between two systems. The\nold cluster is not turned down until the entire process is complete. If problems are\ndiscovered, the load can be transferred back to the old cluster.\nThe problem with this technique is that twice as much capacity is required\nduring the transition. If the service ﬁts on a single machine, having two machines\nrunning for the duration of the upgrade is reasonable.\nIf there are 1000 machines, proportional shedding can be very expensive.\nKeeping 1000 spare machines around may be beyond your budget. In this case,\nonce a certain percentage of trafﬁc is diverted to the new cluster, some older\nmachines can be recycled and redeployed as part of the new cluster.\n11.6 Blue-Green Deployment\nBlue-green deployment is similar to proportional shedding but does not require\ntwice as many resources. There are two environments on the same machine, one\ncalled “blue” and the other called “green.” Green is the live environment and blue\nis the environment that is dormant. Both exist on the same machine by a mecha-\nnism as simple as two different subdirectories, each of which is used as a different\nvirtual host of the same web server. The blue environment consumes very little\nresources.\nWhen the new release is to go live, trafﬁc is directed to the blue environment.\nWhen the process is ﬁnished, the names of the environments are swapped. This\nsystem permits rolling back to the previous environment to take place easily.\nThis is a very simple way of providing zero-downtime deployments on appli-\ncations that weren’t designed for it, as long as the applications support being\ninstalled in two different places on the same machine.\n11.7 Toggling Features\nAs discussed in Section 2.1.9, it is a common practice to tie each new feature to\na software ﬂag or conﬁguration setting so that such features can be individually\nturned on or off. Toggling the switch is known as flag flipping. New features that\nare incomplete and not ready for use by live users have their ﬂags off. When they\nare ready for use, the ﬂag is turned on. The feature can be disabled if problems are\nfound. Having a ﬂag off is also called hiding a feature behind a ﬂag.\n",
      "content_length": 2604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "11.7\nToggling Features\n231\nFeature toggling is one way to implement the general principle of decoupling\ndeployment and release. Deployment is putting a package into an environment.\nRelease is making a feature available to users. We often achieve release through\ndeployment and, therefore, assume one implies the other. In fact, feature toggles\ndecouples these two concepts so that deployment can happen all the time while\nrelease happens on demand. Deployment becomes a purely technical activity that\nusers are unaware of.\nThere are many ways to implement the ﬂag mechanism. The ﬂags can be\ncommand-line ﬂags, used when starting the service. For example, starting a spell\nchecker service with version 2 of the spell check algorithm enabled, and the mor-\nphological algorithm disabled, might involve running a command such as the\nfollowing:\n$ spellcheck-server --sp-algorithm-v2 --morphological=off\nThe ﬂag might be set via shell environment variable. The variables are set prior\nto running the command:\n$ export SP_ALGORITHM_V2=yes\n$ export SP_MORPHOLOGICAL=off\n$ spellcheck-server\nThis becomes cumbersome as the number of ﬂags increases. A service may\nhave dozens or hundreds of ﬂags at any given time. Therefore ﬂag systems can\nread ﬂags from ﬁles instead:\n$ cat spell.txt\nsp-algorithm-v2=on\nmorphological=off\n$ spellcheck-server --flagfile=spell.txt\nAll of these approaches suffer from the fact that to change any ﬂag, the pro-\ncess must be restarted. This interrupts service. Other techniques permit ﬂags to\nbe updated at will. Systems like Google’s Chubby and the Apache Zookeeper\nproject can be used to store ﬂags and efﬁciently notify thousands of servers when\nthey change. With this approach, only one ﬂag is needed—the one specifying the\nzookeeper namespace—to ﬁnd the conﬁguration:\n$ spellcheck-server --zkflags=/zookeeper/config/spell\nFlag ﬂips are used for many reasons:\n• Rapid Development: To enable rapid development, features are built up by\na series of small changes to a main source branch. Large features take a long\n",
      "content_length": 2036,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "232\nChapter 11\nUpgrading Live Services\ntime to develop. The longer one waits to merge code changes into the main line\nsource, the more difﬁcult and risky the merge becomes. Sometimes the source\ncode may have been changed by other developers, which creates a merge con-\nﬂict. Merging small amounts of code is less error prone. (See Section 8.2.4 for\nmore details.) Given this fact, incomplete features are hidden by ﬂags that are\ndisabled until the feature is ready. The ﬂags may be enabled earlier in some\nenvironments than in others. For example, previewing a feature to product\nmanagement might be done by enabling that ﬂag in the demo environment.\n• Gradual Introduction of New Features: Often some features are introduced\nto some users but not others. Beta users, for example, may receive earlier\naccess to new features. When users are logged in, whether they have access to\nthe beta features is dynamically controlled by adjusting the ﬂags. Prior to the\ninvention of this technique, beta users would go to an entirely different set\nof servers. It is very costly to set up a complete replica of the server environ-\nment just to support beta users. Moreover, granularity was an all-or-nothing\nproposition. With ﬂags, each individual feature can be taken in and out of\nbeta testing.\n• Finely Timed Release Dates: Timing a ﬂag ﬂip is easier than timing a soft-\nware deployment. If a new feature is to be announced at noon on Tuesday, it\nis difﬁcult to roll out new software exactly at that time, especially if there are\nmany servers. Instead, ﬂags can be controlled dynamically to ﬂip at a speciﬁc\ntime.\n• Dynamic Roll Backs: It is easier to disable an ill-behaved new feature by dis-\nabling a ﬂag than by rolling back to an older release of the software. If a new\nrelease has many new features, it would be a shame to have to roll back the\nentire release because of one bad feature. With ﬂag ﬂips, just the one feature\ncan be disabled.\n• Bug Isolation: Having each change associated with a ﬂag helps isolate a bug.\nImagine a memory leak that may be in one of 30 recently added features. If\nthey are all attached to toggles, a binary search can identify which feature is\ncreating the problem. If the binary search fails to isolate the problem in a test\nenvironment, doing this bug detection in production via ﬂags is considerably\nmore sane than via many individual releases.\n• A-B Testing: Often the best way to tell if users prefer one design or another is to\nimplement both, show certain users the new feature, and observe their behav-\nior. For example, suppose sign-ups for a product are very low. Would sign-ups\nbe improved if a check box defaulted to checked, or if instead of a check box\nan entirely different mechanism was used? A group of users could be selected,\nhalf with their check box default checked (group A) and the other half with\nthe new mechanism (group B). Whichever design has the better results would\n",
      "content_length": 2924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "11.7\nToggling Features\n233\nbe used for all users after the test. Flag ﬂips can be used both to control the test\nand to enable the winning design when the test is ﬁnished.\n• One Percent Testing: This testing exposes a feature to a statistical sample of\nusers. Sometimes, similar to the canary process, it is done to test a new fea-\nture before deploying it globally. Sometimes, similar to A-B testing, it is done\nto identify the reaction to an experimental feature before deciding whether\nit should be deployed at all. Sometimes it is done to collect information via\nstatistical sampling. For example, a site might like to gather performance\nstatistics about page load times. JavaScript code can be inserted into HTML\npages that transmits back to the service data about how long the page took\nto load. Collecting this information would, essentially, double the number of\nqueries that reach the service and would overload the system. Therefore, the\nJavaScript is inserted only 1 out of every 100 times. The ﬂag would be set to 0\nto disable collection, or a value specifying the sampling percentage.\n• Differentiated Services: Sometimes there is a need to enable different ser-\nvices for different users. A good ﬂag system can enable paid customers to\nsee different features than unpaid users see. Many membership levels can be\nimplemented by associating a set of ﬂags with each one.\nSometimes a ﬂag is used to disable everything to do with a new feature; at other\ntimes it just obscures its visibility. Suppose a web site is going to add auto-\ncompletion to an input ﬁeld. This feature will require changes in the HTML that is\ngenerated for the input prompt, the addition of a new API call to the database for\npartial queries, and possibly other code. The ﬂag could disable everything to do\nwith the feature or it may simply control whether the feature appears in the UI.\n.\nCase Study: Facebook Chat’s Dark Launch\nIn 2011 Facebook’s Chuck Rossi gave a presentation titled “Pushing Millions\nof Lines of Code Five Days a Week” (Rossi 2011) that described an in-house\ntool called “Gatekeeper” that manages dark launches: code launched into\nproduction with no user-visible component. Rossi’s talk revealed that at any\ngiven moment on Facebook.com, there is already the code for every major\nthing Facebook is going to launch in the next six months and beyond. Gate-\nkeeper permits ﬁne-grained control over which features are revealed to which\nusers. Some ﬁlters are obvious, such as country, age, and datacenter. Others\nare unexpected, including one to exclude known employees of certain media\noutlets such as TechCrunch (Siegler 2011). (Also see the related case study in\nSection 18.4.)\n",
      "content_length": 2681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "234\nChapter 11\nUpgrading Live Services\n11.8 Live Schema Changes\nSometimes a new software release expects a different database schema from the\nprevious release. If the service could withstand downtime, one would bring down\nthe service, upgrade the software and change the database schema, and then\nrestart the service. However, downtime is almost always unacceptable in a web\nenvironment.\nIn a typical web environment there are many web server frontends, or repli-\ncas, that all talk to the same database server. The older software release is unable\nto understand the new database schema and will malfunction or crash. The newer\nsoftware release is unable to understand the old database schema and will also\nmalfunction. Therefore you cannot change the database schema and then do the\nsoftware upgrade: the older replicas will fail as soon as the database is modiﬁed.\nThere will be no service until replicas are upgraded. You cannot upgrade the repli-\ncas and then change the database schema because any upgraded replica will fail.\nThe schema cannot be upgraded during the rolling upgrade: new replicas will fail,\nand then at the moment the database schema changes, all the failing replicas will\nstart to work and the working replicas will start to fail. What chaos! Plus, none of\nthese methods has a decent way to roll back changes if there is a problem.\nOne way to deal with this scenario is to use database views. Each view pro-\nvides a different abstraction to the same database. A new view is coded for each\nnew software version. This decouples software upgrades from schema changes.\nNow when the schema changes, each view’s code must change to provide the same\nabstraction to the new schema. The change in schema and the upgrade of view code\nhappen atomically, enabling smooth upgrades. For example, if a ﬁeld is stored in a\nnew format, one view would store it in the new format and the other view would\nconvert between the old and new formats. When the schema changes, the views\nwould reverse roles.\nSadly, this technique is not used very frequently. Views are a rare feature on\nrelational databases. As of this writing, no key–value store (NoSQL) systems sup-\nport views. Even when the feature is available, it isn’t always used. In these cases\nanother strategy is required.\nAn alternative is to change the database schema by making the change over\na period of two software releases: one after adding any new ﬁelds to the database\nand another before removing any obsolete ﬁelds. We learned this technique from\nStephen McHenry and refer to it as the McHenry Technique. Similar techniques\nhave been called “expand/contract.”\nHere are the phases of this technique:\n1. The running code reads and writes the old schema, selecting just the ﬁelds that\nit needs from the table or view. This is the original state.\n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "11.8\nLive Schema Changes\n235\n2. Expand: The schema is modiﬁed by adding any new ﬁelds, but not removing\nany old ones. No code changes are made. If a roll back is needed, it’s painless\nbecause the new ﬁelds are not being used.\n3. Code is modiﬁed to use the new schema ﬁelds and pushed into production. If a\nroll back is needed, it just reverts to to Phase 2. At this time any data conversion\ncan be done while the system is live.\n4. Contract: Code that references the old, now unused ﬁelds is removed and\npushed into production. If a roll back is needed, it just reverts to Phase 3.\n5. Old, now unused, ﬁelds are removed from the schema. In the unlikely event\nthat a roll back is needed at this point, the database would simply revert to\nPhase 4.\nA simple example of this technique involves a database that stores user proﬁles.\nSuppose the ability to store the user’s photograph is being added. The new ﬁeld\nthat stores photographic information is added to the schema (Phase 2). A software\nrelease is pushed that handles the new ﬁeld, including the situation where the ﬁeld\nis empty. The next software release removes the legacy code and the need for the\nﬂag. There is no need for Phase 5 because the schema change only adds ﬁelds.\nAs the next example, suppose a ﬁeld is replaced by new ﬁelds. The user’s\nentire name was stored in one ﬁeld. The new schema uses three ﬁelds to store the\nﬁrst name, middle name, and last name separately. In Phase 2, the three new ﬁelds\nare added to the schema. In Phase 3, we introduce a software release that reads the\nold and new ﬁelds. The code does the right thing depending on whether the old\nor new ﬁelds are populated. Updates from the users write to the new individual\nﬁelds and mark the old ﬁeld as unused. At this time a batch job may be run that\nﬁnds any proﬁles still using the old ﬁeld and converts them to the new ﬁelds.\nOnce all legacy data are converted, any code that uses the old ﬁeld is removed.\nThis new release is pushed into production (Phase 4). Once this release is deployed,\nPhase 5 removes the old ﬁeld from the database schema.\nOne way to streamline the process is to combine or overlap Phases 4 and 5.\nAnother optimization is to lazily remove old ﬁelds, perhaps the next time the\nschema is changed. In other words, Phase 5 from schema version n is combined\nwith Phase 2 from schema version n + 1.\nDuring Phase 2, software will see the new ﬁelds. This may cause problems.\nSoftware must be written to ignore them and otherwise not break when unex-\npected ﬁelds exist. Generally this is not a problem, as most SQL queries request\nthe exact ﬁelds they need, and software accessing NoSQL databases tends to do\nthis by convention. In SQL terms, this means that any use of SELECT * should\nnot make assumptions about the number or position of ﬁelds in the data that is\nreceived. This is generally a good practice anyway, because it makes your code\nmore robust.\n",
      "content_length": 2912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "236\nChapter 11\nUpgrading Live Services\n11.9 Live Code Changes\nSome systems permit live code changes. This makes performing upgrades much\neasier. Generally we frown on the technique of modifying a live system but some\nlanguages are designed speciﬁcally to support it.\nErlang is one such language. A service written in Erlang can be upgraded\nwhile it is running. Properly structured Erlang programs are designed as event-\ndriven ﬁnite-state machines (FSM). For each event received, a speciﬁc function is\ncalled. One event that the service can receive is a notiﬁcation that code has been\nupgraded. The function that is called is responsible for upgrading or converting\nany data structures. All subsequent events will trigger the new versions of the\nfunctions. No processes survive a restart of the Erlang engine itself, but carefully\nplanned upgrades can be arranged with zero downtime.\nA very readable introduction to Erlang’s basic features and structure is avail-\nable online at http://learnyousomeerlang.com/what-is-otp.\nWe expect to see more systems like this in the future. Geordi edited live code\non Star Trek: The Next Generation. We are optimistic that someday we will be able to\ndo so, too.\n11.10 Continuous Deployment\nContinuous deployment means every release that passes tests is deployed to\nproduction automatically. Continuous deployment should be the goal of most\ncompanies unless constrained by external regulatory or other factors.\nAs depicted in Figure 11.1, this requires continuous integration and con-\ntinuous delivery, plus automating any other testing, approval, and code push\nprocesses.\nRecall that continuous delivery results in packages that are production ready,\nbut whether to actually deploy each package into production is a business decision.\nContinuous deployment is a business decision to automate this approval and\nalways deploy approved releases.\nFigure 11.1: Continuous integration, delivery, and deployment build on each other.\n",
      "content_length": 1962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "11.10\nContinuous Deployment\n237\nIt may sound risky to always be doing pushes, and it is. Therefore many pre-\ncautions are taken. One precaution is to use continuous deployment only to push\nto noncritical environments such as beta and UAT environments.\nAnother is to use this approach in production, but only for speciﬁc subsystems\nthat have achieved high conﬁdence in their releases. For example, it is common to\nsee continuous deployment for subsystems that provide APIs and backend services\nbut not web UI subsystems, which may still be tested manually. This is another\nbeneﬁt of service-oriented architectures.\nEven then, some people may be uncomfortable with continuous deploy-\nment. As an experienced system administrator, you may pay attention to various\n“hunches” that help you decide whether today is a good day for a code push. For\nexample, you might have a policy of not doing pushes on critical days such as when\nthe ﬁnance department is closing the books and outages would be especially bad.\nYou might notice an unusually large number of broken builds lately, which leads\nyou to realize the developers may not be having a good week. This might make\nyou wary of accepting a release without ﬁrst having a chat with the lead develop-\ners to see if they’ve been under a lot of stress. These issues seem like reasonable\nthings to consider.\nFor some people, these hunches may be an excuse to avoid continuous deploy-\nment. In reality, hunches provide a roadmap for how to embrace it. There is a bit\nof truth to all such hunches, so why not turn them into measurable things that can\nautomatically disable automatic deployments?\nHere is a list of factors that should be taken into consideration when decid-\ning whether to pause continuous delivery. Some of them may initially sound like\nthings only humans can determine. However, with a little creativity, they can all\nbe automated and used to gate automated code pushes:\n• Build Health: A recent bout of build failures often indicates other stability\nissues. Automatic pushes can be blocked if n of the last m builds failed. This\ncan serve as a proxy for measuring developer stress or rushed work.\n• Test Comprehensiveness: Only highly tested code should be automatically\npushed. There are metrics such as “percentage of code coverage” that can be\nused as indicators that testing has become lax.\n• Test Reproducibility: Flakey tests are tests that sometimes pass and some-\ntimes fail. These can be detected by running the tests multiple times and in\nrandom order. Delivery can be paused automatically if test reproducibility\nis low.\n• Production Health: There is no single test that indicates a system is healthy,\nbut a good proxy is that the monitoring system has no outstanding alerts.\n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "238\nChapter 11\nUpgrading Live Services\n• Schedule Permission: Automated pushes are forbidden if there is a scheduled\n“change freeze” for holidays, quarterly ﬁnancial reporting, or days when too\nmany key people are on vacation. A simple list of dates to pause deployments\ncovers these situations.\n• Oncall Schedule: To avoid dragging the current oncall engineer out of bed,\npushes may be paused during typical sleeping hours. If there are multiple\noncall teams, each covering different times of the day, each may have a differ-\nent idea of waking hours. Even a three-shift, follow-the-sun arrangement may\nhave hours when no one is entirely awake.\n• Manual Stop: There should be a list of people who can, with the click of a\nbutton, halt automated pushes. This is akin to assembly lines where anyone\ncan halt production if a defect is found. The reason for a manual pause does\nnot have to be an emergency.\n• Push Conflicts: A service may be made up of many subservices, each on its\nown release schedule. It can be prudent to permit only one subservice deploy-\nment at a time. Similarly, the next push should not start if the current one\nhasn’t ﬁnished.\n• Intentional Delays: It can be useful to have a pause between pushes to let the\ncurrent one “soak”—that is, to run long enough to verify that it is stable. Doing\npushes too rapidly may make it difﬁcult to isolate when a problem began.\n• Resource Contention: Pushes should be paused if resources are low—for\nexample, if disk space is low or there is unusually high CPU utilization. Load\nmust be below a particular threshold: don’t push when when the system is\nﬂooded. Sufﬁcient redundancy must exist: don’t push if replicas are not N + 2.\nIt might seem risky to turn these hunches into automated processes. The truth\nis that it is safer to have them automated and always done than letting a per-\nson decide to veto a push because he or she has a gut feeling. Operations should\nbe based on data and science. Automating these checks means they are executed\nevery time, consistently, no matter who is on vacation. They can be ﬁne-tuned\nand improved. Many times we’ve heard people comment that an outage happened\nbecause they lazily decided not to do a certain check. The truth is that automated\nchecks can improve safety.\nIn practice, humans aren’t any better at catching regressions than automated\ntests. In fact, to think otherwise is absurd. For example, one regression that people\nusually watch for is a service that requires signiﬁcantly more RAM or CPU than\nthe previous release. This is often an indication of a memory leak or other coding\nerror. People often miss such issues even when automated systems detect them and\nprovide warnings. Continuous deployment, when properly implemented, will not\nignore such warnings.\n",
      "content_length": 2775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "11.11\nDealing with Failed Code Pushes\n239\nImplementing continuous deployment is nontrivial, and easier if done at the\nstart of new projects when the project is small. Alternatively, one can adopt a\npolicy of using continuous deployment for any new subsystems. Older systems\neventually are retired or can have continuous deployment added if they stick\naround.\n11.11 Dealing with Failed Code Pushes\nDespite all our testing and rigor, sometimes code pushed into production fails.\nSometimes it is a hard failure where the software refuses to start or fails soon after\nstarting. In theory, our canarying should detect hard failures and simply take the\nlone replica out of production. Unfortunately, not all services are replicated or are\nnot replicated in a way that canarying is possible. At other times the failure is more\nsubtle. Features may, for example, fail catastrophically in a way that is not noticed\nright away and cannot be mitigated through ﬂags or other techniques. As a result,\nwe must change the software itself.\nOne method is to roll back to the last known good release. When problems are\nfound, the software is uninstalled and the most recent good release is reinstalled.\nAnother method is to roll forward to the next release, which presumably ﬁxes\nthe problem discovered in the failed release. The problem with this technique is\nthat the next release might be hours or days away. The failed release must be\nresilient enough to be usable for the duration, or workarounds must be avail-\nable. The resilience techniques discussed in Chapter 6 can reduce the time pressure\ninvolved. Teams wishing to adopt this technique need to focus on reducing SDP\ncode lead time until it is short enough to make roll forward viable.\nRoll forward works best when servers are highly replicated and canarying\nis used for deployments. A catastrophic failure, such as the server not starting,\nshould be found in the test environment. If for some reason it was not, the ﬁrst\ncanary would fail, thus preventing the other replicas from being upgraded. There\nwould be a slight reduction in capacity until a working release is deployed.\nCritics of roll back point out that true roll back is impossible. Uninstalling soft-\nware and reinstalling a known good release is still a change. In fact, it is a change\nthat has likely not been tested in production. Doing untested processes in produc-\ntion should be avoided at all costs. Doing it only as an emergency measure means\ndoing a risky thing when risk is least wanted.\nRoll forward has overwhelming beneﬁts and a continuous deployment envi-\nronment creates the conﬁdence that makes roll forward possible and less risky.\nPragmatically speaking, sometimes roll forward is not possible. Therefore most\nsites use a hybrid solution: roll forward when you can, roll back when you have\nto. Also, in this situation, sometimes it is more expedient to push though a small\n",
      "content_length": 2899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "240\nChapter 11\nUpgrading Live Services\nchange, one that ﬁxes a speciﬁc, otherwise unsurmountable problem. This emer-\ngency hotfix is risky, as it usually has not received full testing. The emergency\nhotﬁxes and roll backs should be tracked carefully and projects should be spawned\nto eliminate them.\n11.12 Release Atomicity\nDoes a release involve a speciﬁc component or the entire system?\nLoosely coupled systems include many subsystems, each with its own\nsequence of versions. This increases the complexity of testing. It is difﬁcult, and\noften impossible, to test all combinations of all versions. It is also often a waste of\nresources, as not all combinations will be used.\nOne technique is to test a particular combination of component versions: for\nexample, Service A running release 101, Service B running release 456, and Service\nD running release 246. The “(101 + 456 + 246) tuple” is tested as a set, approved as a\nset, and deployed as a set. If a test fails, one or more components is improved and a\nnew tuple is tested from scratch. Other combinations might work, but we develop\na list of combinations that have been approved and use only those combinations.\nIf roll back is required, we roll back to the previous approved tuple.\nThis technique is generally required when components are tightly coupled.\nFor example, load balancer software, its plug-ins, and the OS kernel being used are\noften tested as a tuple in highly demanding environments where a performance\nregression would be devastating. Another example might be the tightly coupled\ncomponents of a virtualization management system, such as Ganeti or OpenStack;\nvirtualization technology, such as KVM or Xen; storage system (DRBD); and OS\nkernel.\nThe problem with this technique is that it reduces the potential rate of change.\nWith all components moving in lock-step, one delayed release will mean the\ncomponents that are ready to move forward have to wait.\nIf the components are loosely coupled, then each component can be tested\nindependently and pushed at its own velocity. Problems are more isolated. The\nchanged component may fail, in which case we know it is a problem with that com-\nponent. The other components may fail, in which case we know that the changed\ncomponent has introduced an incompatibility.\nSuch a system works only when components are loosely coupled. For exam-\nple, at Google the entire infrastructure is an ecosystem of small, loosely coupled\nservices. Services are constantly being upgraded. It is not possible to ask the entire\ncompany to stop so that your system can be tested. Google’s infrastructure is more\nlike a biological system than a mechanical clock. Because of this, each service has\nto take upward compatibility seriously. Incompatible changes are announced long\nin advance. Tools are created speciﬁcally to help manage and track such changes.\n",
      "content_length": 2855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Exercises\n241\nFor example, when an API will change in an incompatible way, there is a way\nto detect uses that will soon be deprecated and warn the service owners who are\naffected.\n11.13 Summary\nUpgrading the software running in an environment is called a code push. Push-\ning code requires planning and techniques engineered speciﬁcally for upgrading\nservices while they are running.\nSometimes a service can be taken down for upgrade because it runs “behind\nthe scenes.” More frequently a service is upgraded by taking down parts, upgrad-\ning them, and bringing them back up (i.e., a rolling upgrade). If there are many\nreplicas, one can be designated as the canary, which is upgraded before the oth-\ners as a test. Proportional shedding involves slowly migrating trafﬁc from an old\nsystem to a new system, until the old system is receiving no new trafﬁc.\nFlag ﬂips are a technique whereby new features are included in a release\nbut are disabled. A ﬂag or software toggle enables the feature at a designated\ntime. This makes it easy to revert a change if problems are found: simply turn the\ntoggle off.\nDatabase schema changes on live systems can be difﬁcult to coordinate. It\nis not possible to change the software of all clients at the exact same time as the\ndatabase schema. Instead, the McHenry Technique is used to decouple the changes.\nNew ﬁelds are added, software is upgraded to use the old and new ﬁelds, software\nis upgraded to use only the new ﬁelds, and any old ﬁelds are then removed from\nthe schema.\nWhen a push fails, it is sometimes possible to roll forward to the next release\nrather than reverting code back to the last known good release. Reverting code is\nrisky, and a failed roll back can be a bigger problem than the original failed push.\nContinuous deployment is the process of automating code pushes, automati-\ncally pausing or delaying them when certain criteria indicate the risk would be too\nhigh or if business conditions require a pause. As with continuous integration and\ncontinuous delivery, by doing something in an automated manner, frequently, we\nreduce risk and improve our processes.\nExercises\n1. Summarize the different techniques for upgrading software in live services.\n2. Why is a failed canary process so tragic that development should stop until\nthe cause is found?\n3. Which push techniques in this chapter are in use in your current environment?\n",
      "content_length": 2389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "242\nChapter 11\nUpgrading Live Services\n4. Which push techniques should your current environment adopt? What would\nbe their beneﬁts?\n5. How would, or could, a dark launch be used in your environment?\n6. What is continuous deployment and what are the beneﬁts?\n7. Why are roll backs discouraged? How could they be made safer?\n8. How would you begin to implement continuous deployment in your environ-\nment? If you wouldn’t do so, why not?\n9. Who is Dr. Jerri Nielsen?\n",
      "content_length": 465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "Chapter 12\nAutomation\nProgressive improvement beats\ndelayed perfection.\n—Mark Twain\nAutomation is when computers do work for us. The desire for automation histor-\nically has been motivated by three main goals: more precision, more stability, and\nmore speed. Other factors, such as increased safety, increased capacity, and lower\ncosts are desired side effects of these three basic goals. As a system administra-\ntor, automating the work that needs to be done should account for the majority of\nyour job. We should not make changes to the system; rather, we should instruct\nthe automation to make those changes.\nManual work has a linear payoff. That is, it is performed once and has beneﬁts\nonce. By comparison, time spent automating has a beneﬁt every time the code is\nused. The payoff multiplies the more you use the code.\nMaking changes to the running system should involve making changes to\ncode and data ﬁles stored in a revision-controlled central repository. Those changes\nare then picked up by the software delivery platform, tested, and deployed to pro-\nduction. To work in this way modern system administrators must be software\ndevelopers. Ever since installing a new computer became an API call, we have\nall become programmers. This sort of scripting isn’t heavy computer science and\ndoes not require formal training. It leads to learning more and more software\ndevelopment skills over time (Rockwood 2013). Automation does not put system\nadministrators out of work. Indeed, there is always more work to be done. Sys-\ntem administrators who can write code are more valuable to an employer. In fact,\nbecause system administration can scale only through automation, not knowing\nhow to code puts your career at risk.\nHowever, automation should not be approached as simply developing faster,\nmore predictable functional replacements for the tasks that people perform. The\nhuman–computer system needs to be viewed as a whole. It is a rare system that can\n243\n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "244\nChapter 12\nAutomation\n.\nTerms to Know\nDomain-Specific Language (DSL): A language that was purpose-built for\na particular use, such as system administration, mathematics, or text\nmanipulation.\nToil: Exhausting physical labor.\nbe fully automated. Most systems experience exceptions that cannot be handled by\nthe automation, but rather need to be handled by people. However, if the people\nrunning the automation have no insight into how it works, it is much harder for\nthem to ﬁgure out how and why it failed, and therefore handle the exception appro-\npriately, than it would be if they were more involved in the process. This situation\nis common where the person handling the exception is not the person who wrote\nthe automation. And at some point, that is the situation for all automation.\n12.1 Approaches to Automation\nThere are three primary approaches to automation design. The ﬁrst, and most\ncommon, approach is to automate using the “left-over” principle, where the\nautomation handles as much as possible, with people being expected to handle\nwhatever is left over. The second approach is the “compensatory” principle, where\nthe work is divided between people and the automation based on which one\nis better at which task. The third approach is based on the complementarity\nprinciple, which aims to improve the long-term health of the combined system\nof people and computers.\nAutomation impacts things beyond the immediate intended goal. Under-\nstanding what the unintended consequences of automation are likely to be should\naid you in building a healthier overall system. For example, antilock brakes (ABS)\nmake it easier for a driver to stop a car more quickly in an emergency. In isolation,\nthis automation should reduce accidents. However, the presence of this automa-\ntion affects how people drive. They drive more quickly in slippery conditions than\nthey would without ABS, and they leave shorter gaps to the car in front because\nof their conﬁdence in the ABS automation. These are unintended consequences of\nthe automation. When human factors are not taken into account, automation can\nhave unexpected, and perhaps negative, consequences.\nBear in mind that not all tasks should be automated. Some difﬁcult tasks\nare so rare that they happen exactly once and cannot happen again—for exam-\nple, installing a large, complex, software system. This is where you outsource.\nHire a consultant who has done it before, since the situation is so rare that the\n",
      "content_length": 2465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "12.1\nApproaches to Automation\n245\nlearning curve has no payoff. The result of the consultant’s work should include a\nset of common tasks that are performed more frequently and are handed off to the\nregular operations staff or to the automation.\nAlso bear in mind that eliminating a task, whether it is easy, difﬁcult, frequent,\nor rare, is beneﬁcial because it becomes one less thing to know, do, or maintain.\nIf you can eliminate the task, rather than automating it, that will be the most\nefﬁcient approach. The co-creator of UNIX and the C programming language,\nKen Thompson, famously wrote, “One of my most productive days was throwing\naway 1,000 lines of code.”\n12.1.1 The Left-Over Principle\nUsing the left-over principle, we automate everything that can be automated\nwithin reason. What’s left is handled by people: situations that are too rare or too\ncomplicated to automate. This view makes the unrealistic assumption that people\nare are inﬁnitely versatile and adaptable, and have no capability limitations.\nThe left-over principle starts by looking at the tasks that people do, and\nuses a number of factors to determine what to automate. In the area of system\nadministration, there are many things that operations staff do. All of them can be\nclassiﬁed along the axes shown in Figure 12.1.\nFigure 12.1: Tasks can be classiﬁed by effort and frequency, which determines the\nnext step in how to optimize them.\n",
      "content_length": 1417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "246\nChapter 12\nAutomation\nThe x-axis is labeled from “rare” to “frequent,” representing how often a task\nis done. The y-axis is labeled from “easy” to “difﬁcult,” representing how much\neffort the task requires each time it is performed.\n• Tasks classiﬁed as rare/easy can remain manual. If they are easy, anyone\nshould be able to do them successfully. A team’s culture will inﬂuence if the\nperson does the right thing.\n• Tasks classiﬁed as rare/difﬁcult should be documented and tools should\nbe created to assist the process. Documentation and better tools will make\nit easier to do the tasks correctly and consistently. This quadrant includes\ntroubleshooting and recovery tasks that cannot be automated. However,\ngood documentation can assist the process and good tools can remove the\nburden of repetition or human error.\n• Tasks classiﬁed as frequent/easy should be automated. The return on invest-\nment is obvious. Interestingly enough, once something is documented, it\nbecomes easier to do, thus sliding it toward this quadrant.\n• Tasks classiﬁed as frequent/difﬁcult should be automated, but it may be best\nto acquire that automation rather than write it yourself. Purchasing commer-\ncial software or using free or open source projects leverages the skills and\nknowledge of hundreds or thousands of other people.\nWith this principle, the aim is to achieve efﬁciency by automating everything\nthat it is feasible to automate. The human component is not explicitly considered.\nHowever, it is easy to describe and reasonably easy to decide what to auto-\nmate. Some of the lessons learned using this approach can be applied even\nif this is not the only principle considered. In particular, considering tasks on\nthe easy–difﬁcult and rare–frequent axes is a useful tool for those looking into\nautomation.\n12.1.2 The Compensatory Principle\nThe compensatory principle is based on Fitts’s list, named after Fitts (1951), who\nproposed a set of attributes to use when deciding what to automate. The attributes\nare shown in Table 12.1. Despite the more than 60 years that have passed since Fitts\nperformed his work, these attributes still apply reasonably well.\nThis principle is based on the assumption that the capabilities of people\nand machines are reasonably static, with the work being divided accordingly.\nRather than implicitly considering humans to be inﬁnitely versatile machines, this\napproach aims to avoid putting excessive demands on people.\nUsing the compensatory principle, we would determine that a machine is\nbetter suited than a person to collecting monitoring data at 5-minute intervals\nfrom thousands of machines. Therefore we would automate monitoring. We\n",
      "content_length": 2671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "12.1\nApproaches to Automation\n247\nTable 12.1: The Principles of the Fitts List\nAttribute\nMachine\nOperator/Human\nSpeed\nMuch superior.\nComparatively slow,\nmeasured in seconds.\nPower output Much superior in level\nComparatively weak, about\nand consistency.\n1500 W peak, less than 150 W\nduring a working day.\nConsistency\nIdeal for consistent,\nUnreliable, subject to learning\nrepetitive actions.\n(habituation) and fatigue.\nInformation\nMultichannel. Information\nMainly single channel,\ncapacity\ntransmission in megabits/second. low rate < 10 bits/second\nMemory\nIdeal for literal reproduction,\nBetter for principles and\naccess restricted and formal.\nstrategies, access versatile\nand innovative.\nReasoning,\nDeductive, tedious to program.\nInductive. Easy to program.\ncomputation\nFast, accurate. Poor error\nSlow inaccurate. Good error\ncorrection.\ncorrection.\nSensing\nSpecialized, narrow range.\nWide energy ranges, some\nGood at quantitative assessment.\nmultifunction capability.\nPoor at pattern recognition.\nGood at pattern recognition.\nPerceiving\nCopes poorly with variations in\nCopes well with variation in\nwritten/spoken material.\nwritten/spoken material.\nSusceptible to noise.\nSusceptible to noise.\nalso might determine that a person cannot survive walking into a highly toxic\nnuclear accident site, but that properly designed robots can.\nAccording to this principle, people are considered information processing sys-\ntems, and the work is described in terms of the interaction between people and\ncomputers, with each having separate, identiﬁable tasks. It looks at optimizing the\nhuman–computer interactions to make the processes more efﬁcient.\n12.1.3 The Complementarity Principle\nThe complementarity principle looks at automation from the human perspective. It\naims to help people to perform efﬁciently in the long term, rather than just looking\nat short-term effects. It looks at how people’s behavior will change as a result of\nthe automation, as well as without the automation.\nIn this approach, one would consider what people learn over time by doing\nthe task manually, and how that would change with the automation. For example,\nsomeone who starts doing a new task starts by understanding the primary goal of\n",
      "content_length": 2208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "248\nChapter 12\nAutomation\nthe task and the basic functions required to meet that goal. Over time the person\nunderstands more about the ecosystem surrounding that task, exceptional cases,\nand some bigger picture goals, and adapts his or her practices based on this knowl-\nedge. With automation, we build in the knowledge that we have accumulated so\nfar. Does that inhibit further learning? How does that change the learning ability\nof a person who is new to the environment? How do a person’s habits change over\nthe longer term due to the automation?\nThe complementarity principle takes more of a cognitive systems engineering\n(CSE) approach of looking at the automation and the people together as a joint\ncognitive system (JCS). It takes into account the fact that people are driven by goals\n(proactive) as well as by events (reactive). A joint cognitive system is characterized\nby its ability to stay in control of a situation, despite disrupting inﬂuences from the\nprocess itself or from the environment. It takes into account the dynamics of the\nsituation, including the fact that capabilities and needs may vary over time and\nwith different situations.\nOne way of coping with changing capabilities and needs is to allow for\nfunctional overlap between the people and the automation, rather than a rigid seg-\nregation of tasks. This allows functions to be redistributed as needed. People are\nviewed as taking an active part in the system and are adaptive, resourceful learning\npartners who are essential to the functioning of the system as a whole.\nUnfortunately, there is no silver bullet or easy-to-follow formula for automa-\ntion using the complementarity principle. But if you remember to consider the\nhuman factor and the long-term effects of the automation on the people running\nthe system, you have a better chance of designing good automation.\n12.1.4 Automation for System Administration\nIn “A Model for Types and Levels of Human Interaction with Automation,”\nParasuraman, Sheridan, and Wickens (2000) propose an approach to automation.\nMuch like the discussion in Section 14.2.4, they observe that there are four stages\nof information processing: information is gathered, that information is analyzed,\na decision is made, and action is taken. These authors also observe that there\nare gradations between fully manual and fully automated. The various degrees\ninclude automation levels where the system makes a suggestion for a person to\napprove, the system makes a suggestion and executes it if a person does not veto\nit within a certain amount of time, the system executes its decision and informs\npeople after the fact, and the computer decides everything and acts autonomously,\nnot requiring or asking for any human input.\nParasuraman et al.’s conclusion is that the information gathering and analysis\nstages are appropriate for a high level of automation. The decision stage should be\nautomated at a level appropriate for the function’s risk. Low-risk functions should\nbe highly automated and high-risk functions should be less automated. When it\n",
      "content_length": 3061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "12.1\nApproaches to Automation\n249\ncomes to taking action, however, the appropriate level of automation is somewhere\nin the middle for high-risk functions.\nAllspaw’s (2012c) two-part blog post “A Mature Role for Automation” gives\na detailed explanation of Parasuraman et al.’s work and relates it to system\nadministration. He concludes that an even better model for automation is that\nof a partnership—applying the complementarity principle. When automation and\noperations staff work together, they enhance each other’s value the same way that\nmembers of a team do.\nA key element of teamwork is trust and safety. Allspaw cites Lee and See’s\n(2004) methods for making automation trustworthy:\n• Design for appropriate trust, not greater trust.\n• Show the past performance of the automation.\n• Show the process and algorithms of the automation by revealing intermediate\nresults in a way that is comprehensible to the operators.\n• Simplify the algorithms and operation of the automation to make it more\nunderstandable.\n• Show the purpose of the automation, design basis, and range of applications\nin a way that relates to the users’ goals.\n• Train operators regarding its expected reliability, the mechanisms governing\nits behavior, and its intended use.\n• Carefully evaluate any anthropomorphizing of the automation, such as using\nspeech to create a synthetic conversational partner, to ensure appropriate trust.\n12.1.5 Lessons Learned\nEfforts to automate can backﬁre. Automation is generally viewed as a pure engi-\nneering task, so the human component may all too often be neglected. Systems\nthat aim to eliminate boring and tedious tasks so that people can tackle more dif-\nﬁcult tasks leave the hardest parts to humans because they are too complex to be\nautomated. Thus mental fatigue due to many tedious tasks is eliminated, but it is\nreplaced by an even more burdensome mental fatigue due to the need to tackle\ndifﬁcult problems on a continual basis.\nAutomation can bring stability to a system, yet this stability results in opera-\ntors becoming less skilled in maintaining the system. Emergency response becomes\nparticularly brittle, a subject that we will address in Chapter 15.\nWhen designing automation, ask yourself which view of the human compo-\nnent is being assumed by this automation. Are people a bottleneck, a source of\nunwanted variability, or a resource? If people are a bottleneck, can you remove\nthe bottleneck without removing their visibility into what the system is doing, and\nwithout making it impossible for them to adjust how the system works, when nec-\nessary? If people are a source of unwanted variability, then you are constraining\n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "250\nChapter 12\nAutomation\nthe environment and inputs of the automation to make it more reliable. What effect\ndoes that have on the people running the automation? How does it constrain their\nwork? How can exceptions be handled? If people are to be a resource, then you\nneed closer coupling between the people and the automation. But in that case,\nthere are two thorny issues: who does which tasks (allocation) and who is in charge\n(responsibility)?\nThe long-term operation of a system can be broken down into four stages:\ntracking, regulating, monitoring, and targeting. Tracking covers event detection\nand short-term control in response to inputs or detected events. Automation typ-\nically starts at this level. Regulation covers long-term control, such as managing\ntransition between states. The case study in Section 12.3 is an example of automa-\ntion at the regulation level. Monitoring covers longer-term controls, interpreting\nsystem state and selecting plans to remain within the desired parameters. For\nexample, capacity planning, hardware evaluation and selection, and similar tasks\nwould fall into this category. Targeting covers setting the overall goals for the\nsystem based on the overall corporate goals—for example, using key performance\nindicators (KPIs; see Chapter 19) to drive the desired behavior. As you move up the\nchain, the higher-level tasks are generally more suited to people than to machines.\nAs you automate further up the chain, people become more disconnected from\nwhat the system is doing and why, and the long-term health of the system as a\nwhole may be jeopardized.\n.\nHidden Cost of Automation\nSuper automated systems often require super training, which can be super\nexpensive. Hiring becomes super difﬁcult, which begins to limit the com-\npany’s ability to grow at its desired rate. The missed opportunities that result\nbecome a burdensome cost. This opportunity cost may be more expensive\nthan what the system saves. Such dilemmas are why companies like Google\nimplement super aggressive recruiting campaigns to hire SREs.\n12.2 Tool Building versus Automation\nThere is a distinction between tool building and automation. Tool building\nimproves a manual task so that it can be done better. Automation seeks to elimi-\nnate the need for the person to do the task. A process is automated when a person\ndoes not have to do it anymore, yet this does not eliminate the need for people.\nOnce a process is automated, a system administrator’s role changes from doing\nthe task to maintaining the automation.\n",
      "content_length": 2532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "12.2\nTool Building versus Automation\n251\n12.2.1 Example: Auto Manufacturing\nThis is analogous to what happened in auto manufacturing. Originally the process\nof painting the metal panels that make up the outer body of the car was a task\ndone by people. It was a slow, delicate, and difﬁcult process requiring great skill.\nThen a high-power paint sprayer was invented to improve this process. The same\nperson could do a better job, with less wasted paint, in less time. This technology\nalso reduced the amount of skill required, thereby lowering the barrier to entry for\nthis job. However, there was still a car panel painter job. The process had not been\nautomated, but there was a better tool for the job.\nIn the 1970s, auto manufacturing plants automated the car painting process.\nThey deployed robotic painting systems and the job of car panel painter was elimi-\nnated. Employees now maintain the robotic painting system, or automation, which\nis a very different job from painting metal panels.\n12.2.2 Example: Machine Configuration\nIn IT we are making a similar transformation. In a typical cloud computing envi-\nronment, every new machine must be conﬁgured for its role in the service. The\nmanual process might involve loading the operating system, installing certain\npackages, editing conﬁguration ﬁles, running commands, and starting services.\nA system administrator (SA) could write a script that does these things. For each\nnew machine, the SA runs the script and the machine is conﬁgured. This is an\nimprovement over the manual process. It is faster and less error prone, and the\nresulting machines will be more consistently conﬁgured. However, an SA still\nneeds to run the script, so the process of setting up a new machine is not fully\nautomated.\nAutomated processes do not require system administrator action, or else they\nreduce it to handling special cases. To continue our example, an automated solution\nmeans that when a machine boots, it discovers its identity, conﬁgures itself, and\nbecomes available to provide service. The role for an SA who conﬁgures machines\nis eliminated in most cases. The SA’s role is transformed into maintaining the\nautomation that conﬁgures machines and handling unusual hardware or operating\nsystems.\n12.2.3 Example: Account Creation\nCloud administrators often maintain the systems that make up a service delivery\nplatform. To give each new developer access, an SA might have to create accounts\non several systems. The SA can create the accounts manually, but it would save\na lot of time if the SA wrote a script that creates the accounts on all the relevant\nmachines, and there would be less chance of skipping a step. Each time a new\ndeveloper joins the company, the SA would use this account creation tool.\n",
      "content_length": 2756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "252\nChapter 12\nAutomation\nBetter yet would be if the SA wrote a job that runs periodically to check if\nnew developers are listed in the human resources database and then automatically\ncreate the new accounts. In this case, the SA no longer creates accounts; the human\nresources department does. The SA’s job is maintaining and enhancing the account\ncreation automation.\n12.2.4 Tools Are Good, But Automation Is Better\nMuch of operational work consists of repeated tasks, such as conﬁguring machines,\ncreating accounts, building software packages, testing new releases, deploying\nnew releases, increasing capacity, failing over services, moving services, and mov-\ning or reducing capacity. All of these tasks can be improved with better tools, and\nthese tools are often stepping stones to automation.\nAnother advantage of automation is that it enables the collection of statis-\ntics about defects or, in IT terms, failures. If certain situations tend to make the\nautomation fail, those situations can be tracked and investigated. Often automa-\ntion is incomplete and certain edge cases require manual intervention. Those cases\ncan also be tracked and categorized, and the more pervasive ones can be prioritized\nfor automation.\nTool building is good, but automation is required for scalable cloud\ncomputing.\n12.3 Goals of Automation\nIn cloud services, automation is a must, not a “nice to have”—it is required for\ngrowth. Cloud-based systems grow in many ways: more machines, more sub-\nsystems and services, and new operational responsibilities. If a new person had\nto be hired for each new service or every n new machines, a cloud-based service\nwould not be able to function. No company could ﬁnd enough new qualiﬁed SAs,\nnor could it afford to pay so many people. Considering that larger organizations\nare more difﬁcult to manage, the people management challenge alone would be\ninsurmountable.\nThere is a popular misconception that the goal of automation is to do tasks\nfaster than they could be done manually. That is just one of the goals. Other goals\ninclude the following:\n• Help scaling. Automation is a workforce multiplier. It permits one person to\ndo the work of many.\n• Improve accuracy. Automation is less error prone than people are. Automa-\ntion does not get distracted or lose interest, nor does it get sloppy over\ntime. Over time software is improved to handle more edge cases and error\nsituations. Unlike hardware, software gets stronger over time (Spolsky 2004,\np. 183).\n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "12.3\nGoals of Automation\n253\n• Increase repeatability. Software is more consistent than humans when doing\ntasks. Consistency is part of a well-controlled environment.\n• Improve reliability. Once a process is automated, it is easier to collect statis-\ntics and metrics about the process. These data can then be used to identify\nproblems and improve reliability.\n• Save time. There is never enough time to do all the work that needs to be done.\nAn automated task should require less SA time than a manual one.\n• Make processes faster. Manual processes are slower because they involve\nthinking and typing. Both are error prone and correcting mistakes often has a\nlarge time penalty itself.\n• Enable more safeguards. Adding additional pre- and post-checks to an auto-\nmated process is easy. Doing so incurs a one-time cost, but improves the\nautomation for all future iterations of the process. Adding more checks to a\nmanual process adds a burden and creates temptation for the person to skip\nthem.\n• Empower users. Automation often makes it possible for a non-SA to do a\ntask. Automation turns a task that only an expert can do into one that an end\nuser can do using a self-service tool. In the example in Section 12.2.3, someone\nfrom the human resources department is now able to provision new accounts\nwithout the involvement of system administrators. Delegation saves time and\nresources.\n• Reduce user wait time. Manual processes can be done only when an SA is\navailable. Automation can be running all day and all night, and will usually\nhave completed a task before an SA would have been available to start it. Even\nif automation were slower than a person doing the same task, the net user wait\ntime would likely be shorter.\n• Reduce system administrator wait time. Many manual processes involve\ndoing one step, then waiting some time before the next step can proceed—for\nexample, waiting for new data to propagate through a system or waiting for\na machine to reboot. A process like this is said be full of “hurry up and wait”\nsteps. If the wait is long, an SA may be able to ﬁll the time with other work.\nHowever, this is often inefﬁcient. All too often, we start working on some-\nthing else, get distracted, and forget to return to the ﬁrst task or lose context.\nComputers are better at waiting than people are.\nFor example, building software packages can be very complex. A good tool will,\nwhen the user enters one command, compile the software, run unit tests, build the\npackage, and possibly more. Automation, however, should eliminate the need for\nsomeone to run the tool. Buildbot is a system that continuously monitors a source\ncode repository for changes. After any change, the software is checked out, built,\npackaged, and tested. If all tests pass, the new package is placed in a repository so it\nis available to be deployed. The latest working package is always available. Many\n",
      "content_length": 2891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "254\nChapter 12\nAutomation\npackages may be created and never used, but since no SA effort is involved, there\nis no additional cost, assuming that the CPU cycles are available. In addition, the\nautomated build process should give the developers immediate feedback about\nany automated tests that fail, resulting in faster bug ﬁxes and better software (see\nChapter 9).\nAnother example involves conﬁguring new machines. A good tool for\nmachine conﬁguration is run by a system administrator, perhaps with a few\nparameters like hostname and IP address, and the result is a fully conﬁgured\nmachine. Automation, however, would be applied where each freshly installed\nmachine looks up its hostname in a directory or external database to ﬁnd its func-\ntion. It then conﬁgures the machine’s OS, installs various packages, conﬁgures\nthem, and starts the services the machine is intended to run. The manual steps\nare eliminated, such that machines come to life on their own.\nSometimes automation eliminates the need for a process entirely. Typically\nload balancers require manual conﬁguration to add replicas. Many tools are avail-\nable that make such conﬁguration easier or less error prone. However, a fully\nautomated solution eliminates the need for constant reconﬁguration. We’ve seen\nsystems where the load balancer is conﬁgured once, and then replicas commu-\nnicate their availability to the load balancer, which includes them in its rotation.\nRather than automating an existing process, this project eliminates the constant\nreconﬁguration by inventing a new way to operate.\n.\nCase Study: Automated Repair Life Cycle\nGoogle uses the Ganeti open source virtual cluster management system to\nrun many large clusters of physical machines, which in turn provide virtual\nmachines to thousands of users. A physical machine rarely fails, but because\nof the sheer number of machines, hardware failures became quite frequent.\nAs a result, SAs spent a lot of time dealing with hardware issues. Tom became\ninvolved in a project to automate the entire repair life cycle.\nFirst, tools were developed to assist in common operations, all of which\nwere complex, error prone, and required a high level of expertise:\n• Drain Tool: When monitoring detected signs of pending hardware prob-\nlems (such as correctable disk or RAM errors), all virtual machines would\nbe migrated to other physical machines.\n• Recovery Tool: When a physical machine unexpectedly died, this tool\nmade several attempts to power it off and on. If these efforts failed to\nrecover the machine, the virtual machines would be restarted from their\nlast snapshot on another physical machine.\n",
      "content_length": 2633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "12.4\nCreating Automation\n255\n.\n• Send to Repairs Tool: When a machine needed physical repairs, there\nwas a procedure for notifying the datacenter technicians about which\nmachine had a problem and what needed to be done. This tool gathered\nproblem reports and used the machine repair API to request the work. It\nincluded the serial number of any failing disks, the memory slot of any\nfailing RAM, and so on. In most cases the repair technician was directed\nto the exact problem, reducing repair time.\n• Re-assimilate Tool: When a machine came back from repairs, it needed\nto be evaluated, conﬁgured, and readded to the cluster.\nEach of these tools was improved over time. Soon the tools did their tasks\nbetter than people could, with more error checking than a person would be\nlikely to do. Oncall duties involved simply running combinations of these\ntools.\nNow the entire system could be fully automated by combining these tools.\nA system was built that tracked the state of a machine (alive, having problems,\nin repairs, being re-assimilated). It used the APIs of the monitoring system and\nthe repair status console to create triggers that activated the right tool at the\nright time. As a result the oncall responsibilities were reduced from multiple\nalerts each day to one or two alerts each week.\nThe logs from the automation were used to drive business decisions.\nDowntime was improved dramatically by accelerating the move away from a\nmodel of hardware that proved to be the least reliable.\n12.4 Creating Automation\nAutomation has many beneﬁts, but it requires dedicated time and effort to cre-\nate. Automation, like any programming, is best created during a block of time\nwhere there are no outside interruptions. Sometimes there is so much other work\nto be done that it is difﬁcult to ﬁnd a sufﬁcient block of time to focus on creating\nthe automation. You need to deliberately make the time to create the automa-\ntion, not hope that eventually things will quiet down sufﬁciently so that you have\nthe time.\nIn a well-run team, the majority of the team’s time should be spent creating\nand maintaining automation, rather than on manual tasks. When manual tasks\nstart to become the greater part of the workload, it is time to take a step back\nand see how new automation might restore the balance. It is important to be\nable to identify which automation will have the biggest impact, and to tackle\nthat ﬁrst. It is also important to understand the places where automation is not\nappropriate.\n",
      "content_length": 2494,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "256\nChapter 12\nAutomation\n12.4.1 Making Time to Automate\nSometimes there isn’t enough time to automate because we’re so busy with urgent\nwork that blocks long-term work such as creating automation. To use an analogy,\nwe don’t have time to shut off the leaking faucet because we’re spending all of our\ntime mopping the ﬂoor. When this happens, it can be difﬁcult to get ourselves out\nof the rut and ﬁx the root causes that prevent us from having a healthy amount of\ntime to create automation.\nHere are some suggestions for making the time to work on automation:\n• Get management involved. Managers should reprioritize the work to empha-\nsize automation.\n• Find the top thing that is wrecking your ability to get the big things done\nand ﬁx, mitigate, or eliminate it. This may mean ignoring other work for a\ntime—even letting some things fail for a bit—while you ﬁx the leaking faucet.\n• Eliminate rather than automate. Find work that can be eliminated. For exam-\nple, spot tasks that you do on behalf of other people and push the responsi-\nbilities back onto those people. Eliminate duplicate effort. For example, if you\nare maintaining 10 different Linux versions, maybe you would be best served\nby narrowing that number down to only a few, or one.\n• Hire a temporary consultant to put into place the high-level automation\nframework and train people on how to use it.\n• Hire a junior person (maybe even temporarily) to do the “grunge work” and\nfree up senior people for bigger projects that would ﬁx the root problem.\n• Start small. Automating one small task can be contagious. For example, use\nconﬁguration management tools such as CFEngine or Puppet to automate one\naspect of conﬁguring new machines. Once one thing is automated, doing more\nis much easier. Don’t try to ﬁx every problem with the ﬁrst bit of automation\nyou create.\n• Don’t work harder; manage your time better. Books like Time Management for\nSystem Administrators (Limoncelli 2005) have a lot of useful advice.\n.\nLess Is More\nEtsy wrote a blog post explaining why the company decided not to adopt a\nnew database technology. The issue wasn’t that the technology wasn’t good; it\nwas that then Etsy would be maintaining two different database software sys-\ntems, two ways to do backups, two testing processes, two upgrade processes,\nand so on. It would be too much work (McKinley 2012).\n",
      "content_length": 2352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "12.4\nCreating Automation\n257\n12.4.2 Reducing Toil\nToil is exhausting physical labor. It is the extreme opposite of the goal of automa-\ntion. However, toil tends to build up in an operations team. It starts small and\ngrows. For example, there may be a small project that can’t be automated because\nit is highly speciﬁc and unlikely to repeat. Over time, it does repeat, and soon that\none action becomes a huge burden. Soon the team is overloaded by toil.\nA little toil isn’t so bad. In fact, it is normal. Not everything is worth automat-\ning. If everything is automated perfectly and there is no work to be done, this\ngenerally means that the rate of change and innovation has been stiﬂed.\nIf more than half of a team’s collective time is spent on operational “toil,” that\nfact should raise a proverbial red ﬂag. The team should review how the members\nare spending their time. Usually there are a few deeper “root cause” problems that\ncould be ﬁxed to eliminate large chunks of manual work. Establish projects that\nwill strike at the root of those problems. Put all other projects on hold until the\n50/50 balance is achieved again.\nIn companies like Google, there is an established policy to deal with this\ntype of situation. Speciﬁcally, there is an ofﬁcial process for a team to declare a\ntoil emergency. The team pauses to consider its options and make a plan to ﬁx\nthe biggest sources of toil. Management reviews the reprioritization plans and\napproves putting other projects on hold until balance is achieved.\nSystem administrator time is too valuable to be spent on something with a\nlinear pay-off like manual work. Toil leads to burn-out and lack of morale. By\nreducing toil, we not only help the company but we also help ourselves.\n12.4.3 Determining What to Automate First\nApply any and all effort to ﬁx the biggest bottleneck ﬁrst. There may be multiple\nareas where automation is needed. Choose the one with the biggest impact ﬁrst.\nAnalyze the work and processes that the team is involved with to ﬁnd the\nbiggest bottleneck. A bottleneck is where a backlog of work accumulates. By\neliminating the bottleneck, you improve throughput.\nThink about the work being done as an assembly line. A project has many\nsteps and moves down the assembly line one step at a time. One of those steps is\ngoing to be the bottleneck.\nAny improvements made upstream of the bottleneck just increase the backlog.\nIt is tempting to make improvements above the bottleneck. Often they are easy or\nthe ﬁrst thing you noticed. Sometimes ﬁxing these items improves the efﬁciency of\nyour team, but the bottleneck is buried deep in some other team’s process. There-\nfore you have made an improvement but the total throughput of the system has\nnot changed. It may even make the problem worse by creating a bigger trafﬁc jam\nof work waiting at the bottleneck.\n",
      "content_length": 2838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "258\nChapter 12\nAutomation\nImprovements made downstream of the bottleneck don’t help overall\nthroughput. Again, they may be the easiest improvements that can be made, but if\nwork is not getting to this part of the system then improving this part will not help\nthe entire system.\nThe bottleneck might not involve a technical problem. It may be a particular\nperson who is overloaded or an entire team that is unable to get important tasks\ndone because they are drowning in small, urgent tasks.\n12.5 How to Automate\nYou can’t automate what you can’t do manually. The ﬁrst step in creating automa-\ntion is knowing what work needs to be done. Begin by doing the process manually\nand documenting what is done in a step-by-step manner. Repeat the procedure,\nfollowing and correcting the documentation as you go. Have someone else follow\nthe same documentation to make sure it is clear. This is the best way to discover\nwhich steps are ill deﬁned or missing.\nThe next step toward automation is prototyping. Create tools to do the indi-\nvidual steps in isolation, making certain that each one is self-contained and works\ncorrectly.\nNow combine the individual tools into a single program. Mature automation\nneeds logging and sanity checking of input parameters and the return values of\nall calls. These features should be incorporated into each stage as well as the ﬁnal\nversion. The automation should check the return value of the previous step and, if\nit failed, should be able to undo the prior steps.\nThe next stage is to make a fully automated system. Identify what would cause\nan SA to run the tool, and have those conditions trigger automation instead. Com-\nplete logging and robust error handling must be in place before this ﬁnal step is\nimplemented.\nIn some cases it is appropriate to create a self-service tool so that non-SAs can\ndo this task. In this situation, security becomes a larger issue. There needs to be\na permission model to control who may use the tool. There need to be checks to\nverify that people can’t accidentally do bad things or circumvent the system to gain\nprivileged access.\n12.6 Language Tools\nMany products, languages, and systems are available for creating automation, and\neach has its own pros and cons. The capabilities of these tools range from basic to\nquite high level.\n",
      "content_length": 2301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "12.6\nLanguage Tools\n259\n12.6.1 Shell Scripting Languages\nShell languages provide the commands one can type at the operating system\ncommand-line prompt. It is easy to turn a sequence of commands typed inter-\nactively at the Bash or PowerShell prompt into a script that can be run as a single\ncommand—either manually or by another process, such as cron.\nShell scripts have many advantages. For example, they allow for very fast\nprototyping. They let the programmer work at a very high level. You can com-\nbine commands or programs to make larger, more powerful programs. There are\nalso disadvantages to using shell scripts for your automation. Most importantly,\nshell-scripted solutions do not scale as well as other languages, are difﬁcult to test,\nand cannot easily do low-level tasks.\nThe most common UNIX shell is the Bourne Again Shell (bash), a superset\nof the Bourne Shell (the UNIX /bin/sh command). In the Microsoft Windows\nworld, PowerShell is a very powerful system that makes it easy to automate and\ncoordinate all the Windows systems and products.\n12.6.2 Scripting Languages\nScripting languages are interpreted languages designed for rapid development,\noften focusing on systems programming.\nSome common examples include Perl, Python, and Ruby. Perl is older and\nvery popular with system administrators because it is similar to C and awk,\nlanguages that UNIX system administrators traditionally know. Python has a\ncleaner design and the code is much more readable than Perl code. Ruby has a\nstrong following in the system administrator community. It is similar to Perl and\nPython in syntax, but adds features that make it easy to create mini-languages,\npurpose-built for a speciﬁc task. More programs can then be written in that\nmini-language.\nScripting languages are more ﬂexible and versatile than shell scripts. They are\nmore expressive, permit better code organization, scale better, and encourage more\nmodern coding practices such as object-oriented coding and functional program-\nming. Better testing tools are available, and there are more prewritten libraries.\nScripting languages have the ability to access networks, storage, and databases\nmore easily than shell scripts. Better error checking is also available.\nPerl, Python, and Ruby all have large libraries of modules that perform\ncommon system administration tasks such as ﬁle manipulation, date and time\nhandling, transactions using protocols such as HTTP, and database access. You\nwill often ﬁnd the program you write is leveraging many modules, gluing them\ntogether to create the functionality you need.\n",
      "content_length": 2582,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "260\nChapter 12\nAutomation\nA disadvantage of scripting languages is that they execute more slowly than\ncompiled languages. This drawback has been mitigated in recent years by new,\nfaster interpreter technology. Nevertheless, for the reasons described earlier, speed\nis not always essential in automation. Thus language speed is often not a factor in\nsystem administration tools whose speed is bounded by other factors. For example,\nif the program is always waiting on disk I/O, it might not matter if the program\nitself is written in a fast or slow language.\nThe primary disadvantage of scripting languages is that they are inappropri-\nate for very large software projects. While nothing prevents you from writing a\nvery large software project in Perl, Python, or Ruby (and many projects have been\ndone this way), it becomes logistically difﬁcult. For example, these languages are\nnot strongly typed. That is, the type of a variable (integer, string, or object) is not\nchecked until the variable is used. At that point the language will try to do the\nright thing. For example, suppose you are concatenating a string and a number: the\nlanguage will automatically convert the number to a string so that it can be con-\ncatenated. Nevertheless, there are some situations the language can’t ﬁx, in which\ncase the program will crash. These problems are discovered only at run-time, and\noften only in code that is rarely used or tested. Contrast this to a compiled language\nthat checks types at compilation time, long before the code gets into the ﬁeld.\nFor this reason scripting languages are not recommended for projects that will\ninclude tens of thousands of lines of code.\n12.6.3 Compiled Languages\nCompiled languages can be a good choice for large-scale automation. Automation\nwritten in a compiled language typically scales better than the same automation\nwritten in a scripting language.\nCompiled languages often used by system administrators include C, C++, and\nGo. As described earlier, compiled languages are usually statically typed and catch\nmore errors at compile time.\n12.6.4 Configuration Management Languages\nConﬁguration management (CM) languages are domain-speciﬁc languages (DSLs)\ncreated speciﬁcally for system administration tasks. CM systems are created for\nmaintaining the conﬁguration of machines, from low-level settings such as the net-\nwork conﬁguration to high-level settings such as which services should run and\ntheir conﬁguration ﬁles.\nConﬁguration languages are declarative. That is, the programmer writes code\nthat describes how the world should be and the language ﬁgures out which\nchanges are needed to achieve that goal. As discussed in Section 10.1.3 (page 213),\n",
      "content_length": 2692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "12.6\nLanguage Tools\n261\nsome CM systems are designed around convergent orchestration (bringing the\nsystem to a desired state) while others favor directed orchestration (the ability to\nfollow a multistep plan of action).\nTwo popular CM systems are CFEngine and Puppet. Listing 12.1 and List-\ning 12.2 illustrate how to specify a symbolic link in CFEngine and Puppet, respec-\ntively. Listing 12.3 and Listing 12.4 show the equivalent tasks in Python and Perl,\nlanguages that are imperative (not declarative). Notice that ﬁrst two simply specify\nthe desired state: the link name and its destination. The Python and Perl versions,\nin contrast, need to check if the link already exists, correct it if it is wrong, cre-\nate it if it doesn’t exist, and handle error conditions. A high level of expertise is\nneeded to know that such edge cases exist, to handle them properly, and to test\nthe code under various conditions. This is very tricky and difﬁcult to get exactly\nright. The CM languages simply specify the desired state and leverage the fact\nthat the creator of the CM system has the knowledge and experience to do things\ncorrectly.\nListing 12.1: Specifying a symbolic link in CFEngine\nfiles:\n\"/tmp/link-to-motd\"\nlink_from => ln_s(\"/etc/motd\");\nListing 12.2: Specifying a symbolic link in Puppet\nfile { '/tmp/link-to-motd ':\nensure => 'link',\ntarget => '/etc/motd',\n}\nListing 12.3: Creating a symbolic link in Python\nimport os\ndef make_symlink(filename , target):\nif os.path.lexists(filename):\nif os.path.islink(filename):\nif os.readlink(filename) == target:\nreturn\nos.unlink(filename)\nos.symlink(target , filename)\nmake_symlink('/tmp/link-to-motd', '/etc/motd ')\n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "262\nChapter 12\nAutomation\nListing 12.4: Creating a symbolic link in Perl\nsub make_symlink {\nmy ($filename , $target) = ($_[0], $_[1]);\nif (-l $filename) {\nreturn if readlink($filename) eq $target;\nunlink $filename;\n} elsif (-e $filename) {\nunlink $filename;\n}\nsymlink($target , $filename);\n}\nmake_symlink('/tmp/link-to-motd', '/etc/motd ');\nConﬁguration management systems usually have features that let you build\nup deﬁnitions from other deﬁnitions. For example, there may be a deﬁnition of a\n“generic server,” which includes the settings that all servers must have. Another\ndeﬁnition might be for a “web server,” which inherits all the attributes of a “generic\nserver” but adds web serving software and other attributes. A “blog server” may\ninherit the “web server” deﬁnition and add blogging software. In contrast, an\n“image server” may inherit the “web server” attributes but create a machine that\naccesses an image database and serves the images, perhaps tuning the web server\nwith settings that are more appropriate for serving static images.\nBy building up these deﬁnitions or “classes,” you can build up a library that\nis very ﬂexible and efﬁcient. For example, a change to the “server” deﬁnition\nautomatically affects all the deﬁnitions that inherit it.\nThe key advantages of a CM system are that SAs can deﬁne things concisely at\na high level, and that it is easy to enshrine best practices in shared deﬁnitions and\nprocesses. The primary disadvantage is the steep learning curve for the domain-\nspeciﬁc language and the need to initially create all the necessary deﬁnitions.\n12.7 Software Engineering Tools and Techniques\nAutomation is just like any other software development project, so it needs the\nfacilities that beneﬁt all modern software development projects. You are probably\nfamiliar with the tools that are used for automation. Even so, we constantly ﬁnd\noperations teams not using them or not using them to their fullest potential.\nThe automation tools and their support tools such as bug trackers and source\ncode repositories should be a centralized, shared service used by all involved\nin software development. Such an approach makes it easier to collaborate. For\n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "12.7\nSoftware Engineering Tools and Techniques\n263\nexample, moving bugs and other issues between projects is easier if all teams use\nthe same bug tracking system.\nThe service delivery platform and related issues such as the need for contin-\nuous test, build, and deploy capabilities were discussed in Chapters 9 and 10. In\nthis section, we discuss issue tracking systems, version control systems, packaging,\nand techniques such as test-driven development.\n12.7.1 Issue Tracking Systems\nIssue tracking systems are for recording and managing bug reports and feature\nrequests. Every bug report and feature request should go through the system.\nThis raises the issue’s visibility, ensures the related work is recognized in statis-\ntics generated by management, and potentially allows someone else to get around\nto working on the issue before you do. Managing all work through one system\nalso makes it easier to prevent overlapping work. It allows members of the team\nto understand what the others are working on, and it makes it easier to hand off\ntasks to others (this last point is especially valuable when you won’t be available\nto work on these tasks).\nOften it is tempting to not report an issue because it takes time and we are\nbusy people, or we ﬁgure “everyone knows this issue exists” and therefore it\ndoesn’t need to be reported. These are exactly the kind of issues that need to be\nreported. Everyone doesn’t know the issue exists, especially management. Raising\nthe visibility of the issue is the ﬁrst step toward getting it ﬁxed.\nBugs versus Feature Requests\nWe colloquially refer to issue tracking systems as “bug trackers,” but this is actu-\nally a misnomer. Feature requests are not bugs and need to be tracked differently.\nResources are often allocated separately for bug ﬁxing and new feature develop-\nment. These workﬂows are often different as well. If you choose not to implement\na feature request, the matter is settled and therefore it is appropriate to mark the\nissue as being resolved. If you choose not to ﬁx a bug, the bug still exists. The issue\nshould remain open, just marked at a low priority designated for issues that will\nnot be ﬁxed.\nIssue tracking systems usually can be conﬁgured to serve different teams or\nprojects, with each one having its bugs stored in its own queue. You may choose\nto establish a different queue for bugs and one for features (“Operations—Feature\nRequests” and “Operations—Bugs”) or the system may have a way to tag issues\nas being one or the other.\nLink Tickets to Subsystems\nEstablish a way to relate tickets to a speciﬁc subsystem. For example, you could\nestablish a separate queue for each subsystem or use other kinds of categorization\n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "264\nChapter 12\nAutomation\nor tagging mechanisms. A small amount of planning now can prevent headaches\nin the future. For example, imagine there are three SRE teams, responsible for ﬁve\nsubsystems each. Suppose a reorganization is going to change which teams are\nresponsible for which subsystems. If each subsystem has its own queue or category,\nmoving tickets to the new team is as simple as changing who owns a particular\nqueue or mass-moving all tickets with a particular category marker. If there is no\nclassiﬁcation, the reorganization will be painstakingly complex. Each issue will\nneed to be individually evaluated and moved to the proper team. This will have\nto be done for all current issues and, if you want to maintain history, for all past\nissues.\nEstablish Issue Naming Standards\nIssues should be named in a uniform way. It simply makes it easier to read and\nprocess many issues if they are all phrased clearly. Bugs should be phrased in terms\nof what is wrong. If some are phrased in terms of what is wrong and others are\nphrased in terms of how the feature should work, it can be quite confusing when a\ntitle is ambiguous. If someone reports that “The help button links to a page about\nthe project,” for example, it is unclear if this statement describes a bug that needs\nto be ﬁxed (it should link to the help page) or explains how the system should\nwork (in which case the bug could be closed and marked “seems to already work\nas requested”).\nFeature requests should be described from the perspective of the person who\ndesires the new capability. In Agile methodology, the recommended template is\n“As a [type of user], I want [some goal] so that [some reason].” For example,\na request might be stated as follows: “As an SRE, I want a new machine type\n‘ARM64’ to be supported by the conﬁguration management system so that we can\nmanage our new tablet-based Hadoop cluster” or “As a user, I want to be able to\nclone a virtual machine via an API call so that I can create clones programatically.”\nChoose Appropriate Issue Tracking Software\nSoftware issue tracking systems are similar to IT helpdesk ticket systems. At the\nsame time, they are different enough that you will need different software for the\ntwo functions. Issue tracking systems focus on the bug or feature request, whereas\nIT helpdesk ticket systems focus on the user. For example, in an issue tracking\nsystem, if two different people report the same bug, they are merged or the second\none is closed as a duplicate. Every issue exists only once. In contrast, an IT helpdesk\nticket system is a mechanism for communicating with users and helping them with\nproblems or fulﬁlling requests. If two people submit similar requests, they would\nnot be merged as each is as unique as the person who made the request.\nSoftware issue tracking systems and IT helpdesk ticket systems also have dif-\nferent workﬂows. An issue tracking system should have a workﬂow that reﬂects\n",
      "content_length": 2940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "12.7\nSoftware Engineering Tools and Techniques\n265\nthe software development process: bugs are received, veriﬁed, and ﬁxed; a differ-\nent person veriﬁes they are ﬁxed; and then the issue is closed. This involves many\nhandoffs. The statistics one needs to be able to generate include how much time is\nspent in each step of the process. The workﬂow for an IT helpdesk is more about\nthe back-and-forth communication with a person.\n12.7.2 Version Control Systems\nA version control system (VCS) is a central repository for storing, accessing, and\nupdating source code. Having all source code in one place makes it easier to col-\nlaborate and easier to centralize functions such as backups, build processes, and\nso on. A VCS stores the history of each ﬁle, including all the changes ever made.\nAs a consequence, it is possible to see what the software looked like at a particular\ndate, revert changes, and so on. Although version control systems were originally\nused for source code control, a VCS can store any ﬁle, not just source code.\nVCS frameworks all have a similar workﬂow. Suppose you want to make a\nchange to a system. You “check out” the source code, thus copying the entire source\nto your local directory. You can then edit it as needed. When your work is complete,\nyou “commit” or “check out” your changes.\nOlder VCS frameworks permit only one person to check out a particular\nproject at a given time so that people can’t create conﬂicting changes. Newer sys-\ntems permit multiple people to check out the same project. The ﬁrst person to check\nin his or her changes has it easy; all of the others go through a process of merging\ntheir changes with past changes. VCS software helps with merges, doing non-\noverlapping merges automatically and bringing up an editor for you to manually\nmerge the rest.\nA distributed version control system (DVCS) is a new breed of VCS. In a DVCS,\neveryone has his or her own complete repository. Sets of changes are transmitted\nbetween repositories. Check-outs don’t just give you a copy of the source code at\na particular version, but create a local copy of the entire repository including the\nentire revision history. You can check in changes to the local repository indepen-\ndently of what is going on in the central system. You then merge a group of such\nchanges to the master repository when you are done. This democratizes source\ncode control. Before DVCS, you could make only one change at a time and that\nchange had to be approved by whoever controlled the repository. It was difﬁcult to\nproceed with the next change until the ﬁrst change was accepted by the repository\nowner. In a DVCS, you are the master of your own repository. You do development\nin parallel with the main repository. Changes made to the main repository can be\npulled into your repository in a controlled manner and only when you want to\nimport the changes. Changes you make in your repository can be merged upstream\nto the main repository when desired, or not at all.\n",
      "content_length": 2984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "266\nChapter 12\nAutomation\nA VCS should not be used only for source code; that is, conﬁguration ﬁles\nmust also be revision controlled. When automation or use of tools involves conﬁg-\nuration ﬁle changes, you should automate the steps of checking the conﬁg ﬁle out\nof version control, modifying it, and then checking it back in. Tools should not be\nallowed to modify conﬁg ﬁles outside of the VCS.\n12.7.3 Software Packaging\nSoftware, once developed, should be distributed in packages. Although this topic\nwas discussed in Section 9.6, we raise the issue again here because the same oper-\nations people who maintain a service delivery platform often distribute their own\nsoftware tools by ad hoc means.\nDistributing your own software via packages enables you to take advantage\nof all the systems used to keep software up-to-date. Without this capability, oper-\national tools end up being copied manually. The result is many systems, all out of\ndate, and all the problems that can bring.\nIdeally the packages should go through the same development, beta, and\nproduction phases as other software. (See Chapter 9.)\n12.7.4 Style Guides\nA style guide is a standard indicating how source code should be formatted and\nwhich language features are encouraged, discouraged, or banned. Having all code\nconform to the same style makes it easier to maintain the code and raises code\nquality. It sets a high bar for quality and consistency, raising standards for the entire\nteam. A style guide also has a mentoring effect on less experienced people.\nTypically programmers spend the vast majority of their time modifying code\nand adding features to existing code. Rarely do we have the opportunity to write\na new program from scratch and be the sole maintainer for its entire life. When\nwe work together as a team, it becomes critical to be able to look at a ﬁle for the\nﬁrst time and assimilate its contents quickly. This enhances productivity. When we\nwrite code that conforms to the style guide, we pay this productivity forward.\nStyle Guide Basics\nStyle guides make both formatting and feature recommendations. Formatting\nrecommendations include specifying how indenting and whitespace are used. For\nexample, most companies standardize on indenting with four (or less commonly\ntwo) spaces instead of tabs, eliminating extra blank lines at the end of ﬁles or\nwhitespace at the end of lines, and placing a single blank line between functions,\nand possibly two blank lines before major sections or classes.\nStyle guides prescribe more than just aesthetics. They also recommend par-\nticular language features over others, and often outright ban certain language\n",
      "content_length": 2644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "12.7\nSoftware Engineering Tools and Techniques\n267\nfeatures that have proved to be difﬁcult to support. Languages often have two ways\nof doing something, and the style guide will select which is to be used. Languages\nmay also have features that are error prone or troublesome for other reasons; the\nstyle guide may ban or discourage those features.\nMany languages have style guides of their own (Python and Puppet). Every\nmajor open source project has a style guide for its community. Most companies\nhave a style guide for internal use. Google runs many open source projects and has\npublished its internal style guides (minus redactions) for more than a dozen lan-\nguages (https://code.google.com/p/google-styleguide/). This enables com-\nmunity members to participate and adhere to the current style. The Google style\nguides are very mature and are a good basis for creating your own.\nAdditional Recommendations\nOften special notations are recommended in style guides. For example, the Google\nstyle guide recommends special notation for comments. Use TODO comments for\ncode that is temporary, a short-term solution, or good enough but not perfect. Use\nNB comments to explain a non-obvious decision. Use FIXME comments to point\nout something that needs to be ﬁxed and list the bug ID of the issue. The annotation\nis followed by the username of the person who wrote the comment. Figure 12.2\nshows examples.\nAs mentioned in Section 9.3.2, source repositories can call programs to val-\nidate ﬁles before they are committed. Leverage these “pre-submit tests” to call\nstyle-checking programs and stop ﬁles with style violations from being commit-\nted. For example, run PyLint on any Python ﬁle, puppet-lint on any Puppet ﬁles,\nand even home-grown systems that pedantically reject CHANGELOG entries if their\nentries are not perfectly formatted. The result is consistency enforced consistently\nno matter how large the team grows.\n12.7.5 Test-Driven Development\nTest-driven development (TDD) is a software engineering practice that leads to\ncode with minimal bugs and maximizes conﬁdence.\n.\nTODO(george): The following works but should be refactored to use\ntemplates.\nNB(matt): This algorithm is not popular but gets the right result.\nFIXME(kyle): This will break in v3, fix by 2013-03-22. See bug\n12345.\nFigure 12.2: Special-purpose comments from the Google style guide\n",
      "content_length": 2362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "268\nChapter 12\nAutomation\nTraditional software testing methodology involves ﬁrst writing code and then\nwriting code that tests the code. TDD reverses these steps. First one writes the code\nto test the code. Running these tests fails because the code hasn’t been written yet.\nThe developer then makes fast iterations of writing code and rerunning the tests;\nthis continues until all the tests pass. At that point, the developer is done.\nFor example, imagine writing a function that takes a URL and breaks it down\ninto its component parts. The function needs to handle many different kinds of\nURLs, with and without embedded usernames and passwords, different protocols,\nand so on. You would come up with a list of URLs that are examples of all the\ndifferent kinds that the code should be able to handle. Next, you would write code\nthat calls the function with each example URL and compares the result against\nwhat you think the result should be. As we saw in Section 9.3.3 and Section 10.2,\nthese are called unit tests.\nNow that the tests are written, the code itself is written. Step by step, the code\nis improved. The tests are run periodically. At ﬁrst only the tests involving the most\nsimple URLs work. Then more and more fancy URL formats work. When all the\ntests work, the function is complete.\nThe tests are retained, not deleted. Later you can make big changes to code\nwith conﬁdence that if the unit tests do not break, the changes should not have\nunintended side effects. Conversely, if you need to modify the code’s behavior,\nyou can start by updating the tests to expect different results and then modify the\ncode until these tests pass again.\nAll modern languages have systems that make it easy to list tests and run them\nin sequence. They enable you to focus on writing tests and code, not managing the\ntests themselves.\n12.7.6 Code Reviews\nCode reviews are a process by which at least one other person reviews any changes\nto ﬁles before they are committed to the VCS. The code review system (CRS) pro-\nvides a user interface that lets the reviewer(s) add commentary and feedback to\nparticular lines of the ﬁle. The original author uses the feedback to update the ﬁles.\nThe process repeats until all the reviewers approve the change and it is integrated\ninto the VCS.\nSoftware engineers conduct code reviews on source code to have a “second\npair of eyes” ﬁnd errors and make suggestions before code is incorporated into\nthe main source. It is also useful to do this on conﬁguration ﬁles. In fact, in an\nenvironment where conﬁguration ﬁles are kept in a source repository, code reviews\nare a great way to ask others what they think about a change you’re making.\nCRS can also be used to delegate tasks to others while still being able to pro-\nvide quality control. This enables scaling of authority via delegation. For example,\n",
      "content_length": 2840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "12.7\nSoftware Engineering Tools and Techniques\n269\nmost companies would not let just anyone edit their load balancer conﬁgurations.\nHowever, if they are kept in VCS and require approval by someone in the load\nbalancer team, the process of load balancer updates becomes self-service. Given a\ngood “how to” document, anyone can edit the conﬁg ﬁle and submit the proposed\nchange to the load balancer team, who check it over and then submit the change\nto activate it.\nThere are other reasons to use a code review system:\n• Better written code. Code review is not just a style check; it is a time to deeply\nconsider the code and suggest better algorithms and techniques.\n• Bidirectional learning. Whether the reviewer or reviewee is more experi-\nenced, both improve their skill and learn from each other.\n• Prevent bugs. A second set of eyes catches more bugs.\n• Prevent outages. Using a CRS for conﬁguration ﬁles catches problems before\nthey hit production.\n• Enforce a style guide. Code reviewers can give feedback on style violations\nso they can be ﬁxed.\nThe time spent on a code review should be proportional to the importance of a\nﬁle. If a change is temporary and has a small inﬂuence on the overall system, the\nreview should be thorough but not overly so. If the ﬁle is in the core of an important\nsystem with many dependencies, more thought should go into the review.\nMembers of a healthy team accept criticism well. However, there is a trick\nto doing code reviews without being mean, or being perceived as mean (which\nis just as important): criticize the code, not the person. For example, “This algo-\nrithm is slow and should be replaced with a faster one” is neutral. “You shouldn’t\nuse this algorithm” is blaming the person. This subtle kind of blame undermines\nteam cohesion. The ability for team members to give criticism without being crit-\nical should be role-modeled by managers and subtly policed by all. Not doing\nso risks poisoning the team. Managers who do not follow this advice shouldn’t\nact surprised when members of their team unfairly criticize each other in other\nforums.\n12.7.7 Writing Just Enough Code\nWrite just enough code to satisfy the requirements of the feature or to ﬁx the bug.\nNo more. No less.\nWriting too little code means the feature request is not satisﬁed. Alternatively,\nwe may have written code that is so terse that it is difﬁcult to understand. Writing\ntoo much code creates a maintenance burden, creates more bugs, and wastes time.\n",
      "content_length": 2475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "270\nChapter 12\nAutomation\nWriting too much code is an easy trap to fall into. We may write code that\nis more ﬂexible, or more conﬁgurable, than we currently require, or we may add\nfeatures that we think might be needed someday. We call this practice “future-\nprooﬁng” and justify it by suggesting that it will save us time in the future when\nthe feature is needed. It is fun and exciting to go beyond the requirements of what\nwe are creating.\nIn reality, history teaches us that we are bad at predicting the future. What\nwe think will be needed in the future is wrong 80 percent of the time. Extra code\ntakes time to develop. In an industry where everyone complains there isn’t enough\ntime to do their work, we shouldn’t be creating extra work. Extra code is a burden\nfor people in the future who will have to maintain the code. A program that is\n20 percent larger because of unused code is more than 20 percent more difﬁcult\nto maintain. Writing less code now saves your entire team time in the future. The\nvast majority of coding involves maintaining and adding features to existing code;\nrarely do we start a new project from scratch. Given this fact, making maintenance\neasier is critical.\nUnused code tends to include more bugs because it doesn’t get exercised or\ntested. If it is tested, that means we spent time adding the tests, the automated test-\ning system now has more work to do, and future maintainers will be unsure if they\ncan delete a feature because all the automated testing “must be there for a reason.”\nWhen the feature ﬁnally is needed, possibly months or years in the future, new\nbugs will be discovered as the requirements and environment will have changed.\nThat said, there is some reasonable future-prooﬁng to do. Any constant such as\na hostname or ﬁlename should be settable via a command-line ﬂag or conﬁguration\nsetting. Split out major functions or classes into separate libraries so that they may\nbe reused by future programs.\nThere are three tips we’ve found that help us resist the temptation to future-\nproof. First, use test-driven development and force yourself to stop coding when\nall tests pass. Second, adding TODO() comments listing features you’d like to add\noften reduces the emotional need to actually write the code. Third, the style guide\nshould explicitly discourage excessive future-prooﬁng and encourage aggressively\ndeleting unused code or features that have become obsolete. This establishes a high\nstandard that can be applied at code review time.\n12.8 Multitenant Systems\nMultitenancy is the situation in which one system provides service for multiple\ngroups, each compartmentalized so that it is protected from the actions of the\nothers.\nBetter than creating automation for just your team is ﬁnding a way to automate\nsomething in such a way that all other teams can beneﬁt from it. You could provide\n",
      "content_length": 2853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "12.9\nSummary\n271\nthe software releases internally so that others can run similar systems. Even better,\nyou could create your system such that it can serve other teams at the same time.\nWhen laying out the conﬁguration management aspects of your system, you\ncan achieve greater ﬂexibility and delegate management through multitenancy.\nThis simply means setting up the system to allow individual groups, or “tenants,”\nto control their own code base.\nImportant qualities of a multitenant framework are that it can be used by mul-\ntiple groups on a self-service basis, where each group’s usage is isolated from that\nof the other groups. Doing so requires a permission model so that each team is\nprotected from changes by the other teams. Each team is its own security domain,\neven though just one service is providing services to all teams.\n.\nCase Study: Multitenant Puppet\nGoogle has one centralized Puppet server system that provides multitenant\naccess to the individual teams that use it. The Mac team, Ubuntu team, Ganeti\nteam, and others are all able to manage their own conﬁgurations without\ninterfering with one another.\nThe system is very sophisticated. Each tenant is provided with a fully\nsource code–controlled area for its ﬁles plus separate staging environments\nfor development, testing, and production. Any feature added to the cen-\ntral system beneﬁts everyone. For example, any work the Puppet team does\nto make the servers handle a larger number of clients beneﬁts all tenants.\nWhen the Puppet team made it possible for the Puppet servers to be securely\naccessed from outside the corporate ﬁrewall, all teams gained the ability for\nall machines to stay updated even when mobile.\nWhile Google’s system enables each tenant to work in a self-service man-\nner, protections exist so that no team can modify any other’s ﬁles. Puppet\nmanifests (programs) run as root and can change any ﬁle on the machine\nbeing run on. Therefore it is important that (for example) the Ubuntu team\ncannot make changes to the Mac team’s ﬁles, and vice versa. Doing so would,\nessentially, give the Ubuntu team access to all the Macs. This is implemented\nthrough a simple but powerful permission system.\n12.9 Summary\nThe majority of a system administrator’s job should focus on automating SA tasks.\nA cloud computing system administrator’s goal should be to spend less than half\nthe time doing manual operational work.\n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "272\nChapter 12\nAutomation\nTool building optimizes the work done by a system administrator and is an\nimportant step on the way to automation. Automation means replacing a human\ntask with one done by software, often working in partnership with a person.\nAutomation starts by having a documented, well-deﬁned, repeatable process. That\ndocument can be used to create scripts that can be used as tools to speed up the\nprocess and make it more reliable. Finally, the task can be fully automated by cre-\nating a self-service tool or a process that is automatically triggered by changes in a\ndatabase or conﬁguration ﬁle. A self-service tool is particularly useful, as it renders\nothers self-sufﬁcient. When something is automated, the SA’s job changes from\ndoing the task to maintaining the automation that does the task. A more enlight-\nened way of thinking of automation is as a partner for operations rather than a\nreplacement.\nMany levels of automation exist, from fully manual to systems that work\nautonomously without human veto or approval. Different levels are appropriate\ndepending on the task type and risk.\nSAs can choose between scripting languages and compiled languages for cre-\nating automation. Understanding the beneﬁts and disadvantages of each approach\nenables the SA to choose the right tool for the job. Conﬁguration management sys-\ntems are also useful for some forms of automation. Conﬁguration management\ntools use a declarative syntax so that you can specify what the end result should\nbe; the CM tool then ﬁgures out how to bring the system to that state.\nBest practices dictate that all code for automation and conﬁguration ﬁles be\nkept under revision control or in a database that tracks changes. As is true for the\nsoftware development environment, automation tools should not be developed or\ntested in the production environment. Instead, they should be developed in a dedi-\ncated development environment, then staged to a testing area for quality assurance\ntests, before being pushed to the production environment.\nExercises\n1. List ﬁve beneﬁts of automation. Which ones apply to your own environment\nand why?\n2. Document a process that needs automation in your current environment.\nRemember that the document should include every step needed to do the\nprocess!\n3. Describe how to prioritize automation using a decision matrix.\n4. When would you use a scripting language instead of a compiled language,\nand why? In which circumstances would you use a compiled language for\nautomation?\n",
      "content_length": 2509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Exercises\n273\n5. Imagine that you are going to implement a conﬁguration management system\nin your current environment. Which one would you choose and why?\n6. What percentage of your time is spent on one-off tasks and operational work\nthat is not automated? What could you do to change this for the better?\n7. List three challenges that automation can introduce. For each of them, describe\nwhich steps you could take to address that challenge.\n",
      "content_length": 443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Chapter 13\nDesign Documents\nBe sincere; be brief; be seated.\n—Franklin D. Roosevelt\nIn this chapter we introduce design documents and discuss their uses and the best\npractices surrounding them. Design documents are written descriptions of pro-\nposed and completed projects, big or small. Design documents serve as a roadmap\nfor your projects and documentation of your accomplishments.\n13.1 Design Documents Overview\nDesign documents are descriptions of proposed or completed projects. They record\nthe goals, the design itself, alternatives considered, plus other details such as cost,\ntimeline, and compliance with corporate policies.\nWriting out what you are going to do forces you to think through the details.\nIt forces you to plan. Having a written document can make design collabora-\ntive when the document becomes a communication vehicle for ideas. Written\ndocuments mean fewer surprises for your teammates, and they help you to get\nconsensus among the team before moving forward. After the project is completed,\nthe design document serves as an artifact that documents the work, a reference for\nthe team.\nA good design document goes into speciﬁc detail about the proposed project\nor change, including information on why choices were made. For instance, the\ndocument might contain detailed descriptions of algorithms, speciﬁc packaging\nparameters, and locations where binary ﬁles are to be installed, and explain why\nthe conﬁg ﬁles are going into /etc instead of elsewhere. A design document about\nnamespace selection, such as the design of a namespace for server/rack names,\nwill not only describe the naming structure, but also give the background on why\nthis namespace was chosen and how it does or does not integrate with existing\nnamespaces.\n275\n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "276\nChapter 13\nDesign Documents\nThe document itself is useful for achieving consensus on a project. Some-\ntimes an “obvious” change isn’t really so obvious, and people ﬁnd errors or have\nquestions. As Linus Torvolds said, “Many eyes make all bugs shallow.”\n.\nCase Study: Working Backward at Amazon\nAmazon encourages engineers to start by describing what the customer will\nsee, then work backward to build the design. Engineers start by writing the\npress release that they would like to see announce the product. They design the\nmarketing materials describing how the customer will beneﬁt from the prod-\nuct, identifying the features, and answering FAQs. This develops the vision of\nwhat will be created. Only after this work is ﬁnished is the design created that\nwill achieve the vision. The process is fully described in Black (2009).\n13.1.1 Documenting Changes and Rationale\nDesign documents can also be used to describe changes instead of projects. A short\ndesign document is a good format to get consensus on a small change such as a\nnew new router conﬁguration, a plan to adopt a new feature of a conﬁguration\nmanagement system, or a new naming scheme for a ﬁle hierarchy.\nA design document is created to capture the speciﬁcs of the proposed change,\nand used as a sounding board for team input. If a formal change control process is\nin place, the same document can be included in the change request.\n13.1.2 Documentation as a Repository of Past Decisions\nAn archive of design documents becomes a reference for anyone needing to under-\nstand how things work or how things got the way they are. New team members can\ncome up to speed more quickly if there is documentation they can study. Existing\nteam members ﬁnd it useful to have a reference when they need a quick refresher\non a particular subject. When collaborating with other teams, it is useful, and looks\nmuch more professional, to point people to documents rather than to explain things\nextemporaneously.\nWhen knowledge can be conveyed without requiring personal interaction\nwith an expert, it makes the team more efﬁcient and effective. Teams can grow\nfaster, other people can adopt our service more rapidly, and we have the freedom\nto transfer between teams because knowledge is not locked to one person.\nDesign documents convey not just the design, but also goals and inspirations.\nSince people will refer to design documents for a long time, they are a good place\nto store background and context.\n",
      "content_length": 2463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "13.2\nDesign Document Anatomy\n277\nOften teams retain the history of decisions in their email archive. The problem\nis that email is, by nature, sequestered in a person’s mailbox. New team members\ncannot ﬁnd those emails, nor will they appear in a search of a documents archive.\nMultiply that one simple email by a year’s worth of changes and you have an entire\nset of information that is basically inaccessible to the team.\nDon’t say it; write it down. Procedures, designs, and wisdom count only when\nthey are written down. People can’t refer to your words unless they are in writing.\nIn that regard, email is speech, not writing (Hickstein 2007).\n13.2 Design Document Anatomy\nA design document has many parts. They start with the highest-level information,\ngetting more detailed as the document proceeds. We aid reader comprehension by\nstandardizing the format, headings, and order of these parts.\nAppendix D includes an example of a complete design document in Sec-\ntion D.2. A sample template appears in Section D.1.\nThe sections are:\nTitle: The title of the document.\nDate: The date of the last revision.\nAuthor(s)/Reviewer(s)/Approver(s): Reviewers are people whose feedback is\nrequested. Approvers are people who must approve the document.\nRevision Number: Documents should have revisions numbered like software\nreleases.\nStatus: Status can be draft, in review, approved, or in progress. Some organi-\nzations have fewer or more categories.\nExecutive Summary: A two- to four-sentence summary of the project that\ncontains the major goal of the project and how it is to be achieved.\nGoals (Alternatively, “In Scope”): What is to be achieved by the project,\ntypically presented as a bullet list. Include non-tangible, process goals such as\nstandardization or metrics achievements. It is important to look at your goals\nand cross-check each one to see that your design document addresses each goal.\nIn some organizations, each goal is given a number and the goals are referred\nto by number throughout the document, with each design description citing the\nappropriate goals.\nNon-goals (Alternatively, “Out of Scope”): A bullet list is typically used to\npresent this information. A list of non-goals should explicitly identify what is not\nincluded in the scope for this project. This heads off review comments like “This\nproject doesn’t solve XYZ” because we are communicating here that this project is\nnot intended to solve XYZ.\nBackground: What a typical reader needs to know to be able to understand the\ndesign. This section might provide a brief history of how we got here. Identify any\n",
      "content_length": 2588,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "278\nChapter 13\nDesign Documents\nacronyms or unusual terminology used. Document any previous decisions made\nthat have placed limitations or constraints on this project, such as “Our company\npolicy of using only open source software when possible precludes commercial\nsolution XYZ in this project.”\nHigh-Level Design: A brief overview of the design including how it works\nat a high level. The design principles should be described but not necessarily the\nimplementation details.\nDetailed Design: The full design, including diagrams, sample conﬁguration\nﬁles, algorithms, and so on. This will be your full and detailed description of what\nyou plan to accomplish on this project.\nAlternatives Considered: A list of alternatives that were rejected, along with\nwhy they were rejected. Some of this information could have been included\nin the “Background” section, but including it in a speciﬁc location quiets the\ncritics.\nSpecial Constraints: A list of special constraints regarding things like security,\nauditing controls, privacy, and so on. These are sometimes optional. Document\nthings that are part of the process at your company, such as architectural review,\ncompliance checks, and similar activities. You can lump all of the constraints\ntogether in this section or include a section for each of them. Any mandatory pro-\ncess for project review at your company should probably have its own section on\nyour template.\nThe document may have many more sections, some of which may be optional.\nHere are some sections that might be useful in a template:\n• Cost Projections: The cost of the project—both initial capital and operational\ncosts, plus a forecast of the costs to keep the systems running.\n• Support Requirements: Operational maintenance requirements. This ties\ninto Cost Projections, as support staff have salary and beneﬁt costs, hardware\nand licensing have costs, and so on.\n• Schedule: Timeline of which project events happen when, in relation to each\nother.\n• Security Concerns: Special mention of issues regarding security related to\nthe project, such as protection of data.\n• Privacy and PII Concerns: Special mention of issues regarding user privacy\nor anonymity, including plans for handling personally identiﬁable informa-\ntion (PII) in accordance with applicable rules and regulations.\n• Compliance Details: Compliance and audit plans for meeting regulatory\nobligations under SOX, HIPPA, PCI, FISMA, or similar laws.\n• Launch Details: Roll-out or launch operational details and requirements.\n",
      "content_length": 2509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "13.4\nDocument Archive\n279\n13.3 Template\nGiving someone a template to ﬁll out guides that person through the process of\ncreating his or her own design documents better than a step-by-step guide or a\nsample document could ever do. The template should include all headings that\nthe ﬁnal document should have, even optional ones. The template can be annotated\nwith a description of what is expected in each section as well as helpful hints and\ntips. These annotations should be in a different font or color and are usually deleted\nas the user ﬁlls out the template.\nAn example template can be found in Section D.1 of Appendix D. Use it as the\nbasis for your organization’s template.\nThe template should be easy to ﬁnd. When the template is introduced, every-\none should be notiﬁed that it exists and provided with a link to where the template\ncan be found. The link should appear in other places, such at the table of contents\nof the design document archive or another place that people frequently see. Make\nsure the template shows up in intranet searches.\nThe template should be available in all the formats that users will be using,\nor any format with which they feel comfortable. If people use MS-Word, HTML,\nMarkDown (a wiki format), or OpenOfﬁce/LibreOfﬁce, provide the template in all\nof those formats. Providing it in your favorite format only and assuming everyone\nelse can improvise is not making it easy for others to adopt the system.\nIt can also be useful to provide a short template and a long template. The short\ntemplate might be used for initial proposals and small projects. The long template\nmight be used for complex projects or formal proposals.\nThe template should also be easy to use. Anytime you see someone not using\nthe template is an opportunity to debug the problem. Ask them, in a noncon-\nfrontational way, why the template wasn’t used. For example, tell them that you’re\nlooking for feedback about how to get more people to adopt the template and ask\nwhat you could do to have made it easier for them to use it. If they say they couldn’t\nﬁnd the template, ask where they looked for it and make sure all those places are\nupdated to include links.\n13.4 Document Archive\nThere should be a single repository for all design documents. Generally this can\nbe as simple as a list of documents, each linked to the document itself. For the\ndocument archive to be useful to readers, it should be easy for people to ﬁnd the\ndocuments that they are looking for. A simple search mechanism can be useful,\nalthough people will generally be happy if there is one page that lists the titles of\nall documents and that page itself can be searched.\n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "280\nChapter 13\nDesign Documents\nIt should be easy to add a design document to the document archive. The\nprocess of adding to or updating the archive should be self-service. If the process\ninvolves emailing a person, then that person will inevitably become a bottleneck.\nSuch an obstacle will discourage people from adding new documents.\nThe easiest way to create an archive is to use a wiki. With this approach,\npeople add links to their documents on a main page. The actual documents are\nthen stored either in the wiki or elsewhere. People will generally add to whatever\nstructure already exists. Consequently, it is important to put time and thought into\nmaking a main page that is simple and easy for others to maintain.\nAnother way to maintain a repository is to create a location for people to add\ndocuments—for example, a source repository or subdirectory on a ﬁle server—and\nwrite scripts that automatically generate the index. Create a system for ﬁle nam-\ning so that the document organization happens automatically. The easiest method\nis for each team to have a unique preﬁx and to let the teams self-select their pre-\nﬁxes. Generating the index requires either a document format that can be parsed\nto extract data or a standardized place for people to list the data that is needed for\nthe index—for example, a YAML or JSON ﬁle that people include along with the\ndocument itself. The scripts then automatically generate an index of all documents\nthat lists the ﬁlename, title, and most recent revision number.\n13.5 Review Workflows\nThere should be a deﬁned way for design documents to be approved. An approval\nprocess does not need to be a high-overhead activity, but rather can be as simple\nas replying to an email containing the design document. People need to know that\ngetting approval does not entail a huge workload or they won’t seek it.\nThere are as many approval workﬂows as there are organizations using design\ndocuments. Most styles fall into one of three main groups:\n• Simple: You create a draft, email it around for comments, and revise it if\nnecessary. No formal approval.\n• Informal: Similar to the simple workﬂow, but with review from project\napprovers or an external review board. This review board exists to give feed-\nback and warn against repeating past mistakes rather than providing a stamp\nof approval.\n• Formal Workflow: Multiple approval stages, including project approvers and\nanexternalreviewboard.Typicalexamplesincludeaﬁrewallorsecurityreview,\na project budget review, and a release schedule review. Each review stage is\nnoted with any comments, and revision numbers or dates track changes.\nDifferent kinds of documents may need different approval workﬂows. Informa-\ntional documents may have a very lightweight or no approval process. More\n",
      "content_length": 2780,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "13.5\nReview Workﬂows\n281\nextensive approval may be required depending on the scope or impact of the doc-\nument. Your approval workﬂow is likely to be determined by your change control\nor compliance requirements.\n13.5.1 Reviewers and Approvers\nA design document usually lists authors, reviewers, and approvers. The authors\nare the people who contributed to the document. Reviewers are the people whose\nfeedback is requested. Approvers are the people whose approval is required to\nmove forward.\nReviewers typically include team members who might end up doing the\nactual work of the project. They can also include subject-matter experts and any-\none whose opinion the author wants to seek. Adding someone as a reviewer is\na good way to “FYI” someone in another group that might be impacted by the\nproject work. You should expect meaningful comments from a reviewer, including\npossibly comments that require you to make changes to the document.\nApprovers typically include internal customers who are dependent on the\nresults of the project as well as managers who need to sign off on the resources\nused for the project. Sometimes approvers include other audit and control pro-\ncesses mandated by the company—for example, a privacy assessment or review\nof PII used in the project. People who conduct audits of compliance with regu-\nlations such as SOX, HIPAA, or PCI are also approvers in the design document\nprocess. Approvers are generally a yes-or-no voice, with commentary usually\naccompanying only a “no” decision. It will be up to your individual process\nto determine whether approvals must be unanimous, or whether one or more\nnegative approvals can be overridden by a majority of positive approvals.\n13.5.2 Achieving Sign-off\nThe reason to make a distinction between reviewers and approvers is that\napprovers can block a project. Trying to get consensus from everyone will pre-\nvent any progress, especially if one asks for approval from people who are not\ndirectly affected by the success of a project. People on the perimeter of a project\nshould not be able to block it. Everyone has a voice, but not everyone has a vote.\nThe reviewer/approver distinction clariﬁes everyone’s role.\n.\nMultiple, Highly Specialized Approvals\nThe design documents approval process used at Google includes sign-off from\nthe group managing various centralized services. For example, if the project\nwill be sending data to the centralized log archive, the design document must\n",
      "content_length": 2459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "282\nChapter 13\nDesign Documents\ninclude a section called “Logging” that details how many bytes of log data\neach transaction will generate and justiﬁes the storage and retention policy.\nSign-off by the central log team is required. There are similar sections for\nsecurity, privacy, and other approvers.\nWorking with a review board can be made easier if the board publishes a\nchecklist of items it is looking for in a document. This permits the author to be\nbetter prepared at the review. Publishing the review board checklist is respectful\nof your colleagues’ time as well as your own, as you will see fewer deﬁcient doc-\numents once your checklists are made public. A review board may be a group of\napprovers or of reviewers, or a mix of both—it is in your best interest to determine\nthis ahead of time as you prepare your document.\n13.6 Adopting Design Documents\nImplementing a design document standard within an organization can be a chal-\nlenge because it requires a cultural change. People change slowly. Make it easy\nfor people to adopt the new behavior, be a role model, and enlist management\nsupport.\nProviding a template is the most important thing you can do to drive adoption\nof such a policy. Make sure the template is easy to access and available in the format\npeople most want to use.\nModel the behavior you wish to see in others. Use the design document\ntemplate everywhere that it is appropriate and strictly adhere to the format.\nPeople emulate the behaviors they see successful people demonstrate. Talk\ndirectly with highly respected members of the organization about being early\nadopters. Their feedback on why they are or are not willing to adopt the design\ndocument standard will be enlightening.\nEarly adopters should be rewarded and their success highlighted. This doesn’t\nneed to be a contest with prizes. It can be as simple as saying something positive\nat a weekly staff meeting. Compliment them even if they didn’t use the template\nperfectly; you are rewarding them for taking the ﬁrst steps. Save the suggestions\non how to use the template more properly for one-on-one feedback.\nEnlisting management support is another avenue to adoption. If you are a\nmanager, you have the luxury of making design documents a top-down require-\nment and insisting that your team use them. Another way to start using design\ndocuments is to make them a requirement for certain approval processes, such\nas budget review, security or ﬁrewall review boards, or change control. Avoid\n",
      "content_length": 2488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "13.7\nSummary\n283\nrequiring blind adherence to the template format as a requirement for approvals.\nContent is more important than presentation.\nWhen they see that the template is easy to use, team members will start\nadopting it for other projects.\nUse the template yourself. If the template is not beneﬁting you personally, it\nis probably not beneﬁcial to others. Using it also helps spot problems that often\ncan be ﬁxed as easily as updating the template. When Tom joined Stack Exchange,\none of the ﬁrst things he did was create a design document template and put\nit on the department wiki. He used it for two proposals before mentioning it,\nrevising and perfecting the template each time he used it. Only after it had been\nused a few times did he mention the template to his teammates. Soon others were\nusing it, too.\n13.7 Summary\nA design document serves as a project roadmap and documentation. Its primary\nfunction is to describe a project, but it also communicates key aspects of the project\n(who, what, why, when, and how) to the team. Design documents are useful for\nchange control as well as for project speciﬁcations. Good design documents will\ncapture not only the decisions made but also the reasoning behind the decisions.\nAdopting design documents can pose cultural challenges for a team. Start with\na good template in an easy-to-ﬁnd location, made available in the formats your\nteam uses for documents. Create a central repository for design documents, orga-\nnized in order of most recent updates. Make it easy to add a design document to\nthe repository’s main directory page.\nApprovals are part of good design document culture. Create a draft document\nand solicit comments from some portion of the team. For simple workﬂows, email\nis sufﬁcient to pass the document for review. For more formal workﬂows, there\nmay be mandatory review boards or multiple approval steps. In all workﬂows, the\nversion number of the design document is updated each time review comments\nare added to the document. Ideally, the document repository’s main page will also\nbe updated as revisions occur.\nTemplates should contain the major points of who, what, why, when, and how.\nShorter templates may be used for small or informal projects, with larger, more\ncomplete templates being preferred for complex or formal projects. At a minimum,\nthe template should have title, date, author(s), revision number, status, executive\nsummary, goals, high-level design, and detailed design sections. Alternative or\noptional sections can be added to customize the template based on the needs of\nyour organization.\n",
      "content_length": 2590,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "284\nChapter 13\nDesign Documents\nExercises\n1. What are the primary functions of a design document?\n2. List qualities of a good document management system for design documents.\n3. Does your organization currently have a standardized design document for-\nmat? If not, what would be required to implement it in your organization?\n4. Which mandatory processes are part of a design review in your organization?\nWhich stakeholders are mandatory reviewers or approvers?\n5. Describe your organization’s equivalent of design documents, and their\nstrengths and weaknesses when contrasted with the model presented here.\n6. Create a design document template for your organization. Show it to co-\nworkers and get feedback about the format and ways that you can make it\neasier to use.\n7. Write the design for a proposed or existing project using the design document\nformat.\n8. Take an existing document that your organization uses and rewrite it using\nthe design document format.\n9. Use the design document format for something silly or fun—for example, a\ndesign for a company picnic or plans to get rich quick.\n",
      "content_length": 1097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "Chapter 14\nOncall\nBe alert... the world\nneeds more lerts.\n—Woody Allen\nOncall is the way we handle exceptional situations. Even though we try to auto-\nmate all operational tasks, there will always be responsibilities and edge cases that\ncannot be automated away. These exceptional situations can happen at any time\nof the day; they do not schedule themselves nicely between the hours of 9 and\n5 .\nExceptional situations are, in brief, outages and anything that, if left unat-\ntended, would lead to an outage. More speciﬁcally, they are situations where the\nservice is, or will become, in violation of the SLA.\nAn operations team needs a strategy to assure that exceptional situations\nare attended to promptly and receive appropriate action. The strategy should be\ndesigned to reduce future reoccurrence of such exceptions.\nThe best strategy is to establish a schedule whereby at any given time at least\none person is responsible for attending to such issues as his or her top priority.\nFor the duration of the oncall shift, that person should remain contactable and\nwithin reach of computers and other facilities required to do his or her job. Between\nexceptions, the oncall person should be focused on follow-up work related to the\nexceptions faced during his or her shift.\nIn this chapter we will discuss this basic strategy plus many variations.\n14.1 Designing Oncall\nOncall is the practice of having a group of people take turns being responsible for\nexceptional situations, more commonly known as emergencies or, less dauntingly,\nalerts. Oncall schedules typically provide 24 × 7 coverage. By taking turns, people\nget a break from such heightened responsibilities, can lead normal lives, and take\nvacations.\n285\n",
      "content_length": 1721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "286\nChapter 14\nOncall\nWhen an alert is received, the person on call responds and resolves the issue,\nusing whatever means necessary to prevent SLA violations, including shortcut\nsolutions that will not solve the problem in the long term. If he or she cannot resolve\nthe issue, there is an escalation system whereby other people become involved.\nAfter the issue is managed, any follow-up work should be done during normal\nbusiness hours—in particular, root causes analysis, postmortems, and working on\nlong-term solutions.\nNormally one person is designated the “oncall person” at any given time. If\nthere is an alert from the monitoring system, that individual receives the alert and\nmanages the issue until it is resolved. During business hours this person works\nas normal, except that he or she always works on projects that can be interrupted\neasily. After normal business hours, the oncall person should be near enough to a\ncomputer so he or she can respond quickly.\nThere also needs to be a strategy to handle the situation when the oncall person\ncannot be reached. This can happen due to commuting, network outages, health\nemergencies, or other issues. Generally a secondary oncall person is designated to\nrespond if the primary person does not respond after a certain amount of time.\n14.1.1 Start with the SLA\nWhen designing an oncall scheme for an organization, begin with the SLA for the\nservice. Work backward to create an SLA for oncall that will result in meeting the\nSLA for the service. Then design the oncall scheme that will meet the oncall SLA.\nFor example, suppose a service has an SLA that permits 2 hours of downtime\nbefore penalties accrue. Suppose also that typical problems can be solved in 30\nminutes, and extreme problems take 30 minutes to cause system failover but usu-\nally only after 30 minutes of trying other solutions. This would mean that the time\nbetween when an outage starts and when the issue is being actively worked on\nmust be less than an hour.\nIn that hour, the following things must happen. First, the monitoring sys-\ntem must detect the outage. If it polls every 5 minutes and alerts only after three\nattempts, a maximum of 15 minutes may pass before someone is alerted. This\nassumes the worst case of the last good poll happening right before the outage.\nLet’s assume that alerts are sent every 5 minutes until someone responds; every\nthird alert results in escalation from primary to secondary or from secondary to\nthe entire team. The worst case (assuming the team isn’t alerted) is six alerts, or\n30 minutes. From receiving the alert, the oncall person may need 5–10 minutes to\nlog into the system and begin working. So far we have accumulated about 50–55\nminutes of outage before “hands on keyboard” has been achieved. Considering we\nestimated a maximum of 60 minutes to ﬁx a problem, this leaves us with 5 minutes\nto spare.\nEvery service is different, so you must do these calculations for each one. If\nyou are managing many services, it can be worthwhile to simplify the process by\n",
      "content_length": 3031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "14.1\nDesigning Oncall\n287\ncreating a few classes of service based on the required response time: 5 minutes,\n15 minutes, 30 minutes, and longer. Monitoring, alerting, and compensation\nschemes for each class can be deﬁned and reused for all new services rather than\nreinventing the wheel each time.\n14.1.2 Oncall Roster\nThe roster is the list of people who take turns being oncall. The list is made up of\nqualiﬁed operations staff, developers, and managers. All operations staff should\nbe on the roster. This is generally considered part of any operations staff member’s\nresponsibility.\nWhen operations staff are new to the team, their training plan should be\nfocused on getting them up to speed on the skills required to be oncall. They should\nfollow or shadow someone who is oncall as part of their training. In most compa-\nnies a new hire should be able to handle oncall duties within three to six months,\nthough this time varies.\nSome organizations do not require senior operations staff to share oncall\nresponsibilities. This is unwise. The senior staff risk becoming disconnected from\nthe operational realities for which they are responsible.\nDevelopers should be on the roster. This gives them visibility into the opera-\ntional issues inherent to the system that they have made and helps guide them in\nfuture design decisions. In other words, their resistance to add the features listed\nin Chapter 2 goes away when the lack of such features affects them, too. It creates\nan incentive to ﬁx operational issues rather than provide workarounds.\nFor example, if a problem wakes up the oncall person at 3 , he or she\nmight use a workaround and ﬁle a bug report to request a more permanent ﬁx,\none that would prevent the oncall person from being woken up at odd hours in\nthe future. Such a bug is easy to ignore by developers if they do not have oncall\nshifts themselves. If they are on the oncall roster, they have an incentive to ﬁx\nsuch bugs.\nIt is important that the priorities of operations and developers are aligned,\nbecause otherwise operational issues will not get the attention they deserve. In\nthe old days of shrink-wrapped software, people accepted that developers were\ndisconnected from operational needs, but those days are long gone. (If you do not\nknow what shrink-wrapped software is, ask your grandparents.)\nTechnical managers, team leads, and technical project managers should also\nshare oncall responsibilities. This keeps them in touch with the realities of opera-\ntions and helps them to be better managers. We also ﬁnd that oncall playbooks and\ntools tend to be held to a higher standard for accuracy and efﬁciency when a wider\ndiversity of talent will be using them. In other words, if your playbook is written\nwell enough that a technical manager can follow it, it’s going to be pretty good. (To\nthe managers reading this: we mean that people keep to higher standards when\nthey know you’ll see the results of their work.)\n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "288\nChapter 14\nOncall\n14.1.3 Onduty\nOnduty is like oncall but focuses on dealing with non-emergency requests from\nusers. Onduty is different from oncall.\nMany operations teams also have a queue of requests from users managed\nsimilarly to the way IT helpdesks receive and track requests via a helpdesk automa-\ntion tool. Onduty is a function that assures there is always one person working on\nthese tickets. Otherwise, tickets may be ignored for extended periods of time. Col-\nloquially this function goes by many names: ticket time, ticket week, ticket duty,\nor simply onduty. Generally the onduty roster is made up of the same people who\nare on the oncall roster.\nThe primary responsibility of the onduty person is to respond to tickets within\nthe SLA—for example, 1 business day for initial response; no service at night, week-\nends, and holidays. Usually the onduty person is responsible for triaging new\ntickets, prioritizing them, working most of them, and delegating special cases to\nappropriate people.\nMost organizations have different oncall and onduty schedules. In some orga-\nnizations, whoever is oncall is automatically onduty. This simpliﬁes scheduling\nbut has the disadvantage that if an emergency occurs, tickets will be ignored. That\nsaid, for large emergencies tickets will be ignored anyway. Moreover, if there is\nan emergency that keeps the oncall person awake late at night, the next day that\nperson may not be at his or her best; expecting this individual to respond to tickets\nunder such conditions isn’t a good idea. For that reason, you should try to keep a\nseparate oncall schedule unless ticket load is extremely light.\nSome organizations do not have onduty at all. They may not be in a situa-\ntion where there are users who would ﬁle tickets with them. Some teams feel that\naccepting tickets goes against the DevOps philosophy. They are collaborators, they\nsay, not a service desk that people come to with requests. If there is a request to\ncollaborate, it should be handled through the normal business channels. During\ncollaboration, any tasks assigned should be in the work tracking system just like\nall other project-related tasks.\n14.1.4 Oncall Schedule Design\nThere are many variations on how to structure the oncall schedule. The duration\nof a person’s oncall shift should be structured in a way that makes the most sense\nfor your team. Here are some variations that are commonly used:\n• Weekly: A person is oncall for one week at a time. The next shift starts the\nsame time each week, such as every Wednesday at noon. Having the change\noccur mid-week is better than during the weekend. If the change happens on a\nWednesday, each onduty person has one complete weekend where travel and\n",
      "content_length": 2720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "14.1\nDesigning Oncall\n289\nother fun are limited. If the transition happens on Saturday, then it ruins the\nweekend of both oncall people for two weekends in a row. On Mondays, there\nmay be a backlog of follow-up work from the weekend. It is best to let the per-\nson who handled the weekend alerts complete the follow-up work, or at least\ngenerate the appropriate tickets and documentation, while still oncall. This\nenables a clean handover and should allow each person to return to project\nwork as quickly as possible after oncall duty ends.\n• Daily: A person is oncall for one day at a time. This may seem better than\na weekly schedule because the shift is not as long, but it means being oncall\nmuch more often. A weekly schedule might mean being oncall one week out of\nevery six. With a small team, a daily schedule might mean being oncall every\nsix days, never having a complete week to take a vacation.\n• Split Days: On a given day multiple people are oncall, each one responsible\nfor a different part of the day or shift. For example, a two-shift schedule might\ninvolve two 12-hour shifts per day. One person works 9 to 9 and another\nis oncall for the overnight. This way, if an alert happens in each shift, someone\nis always able to sleep. A three-shift schedule might be 8 hours each: 9 to\n5 , 5 to 1 , and 1 to 9 .\n• Follow the Sun: Members of the operations team live in different time zones,\nand each is oncall for the hours that he or she would normally be awake (sun-\nlight hours). If the team resides in California and Dublin, a shift change at\n10 and 10 California time means all members have some responsibil-\nities during ofﬁce hours and sleeping hours, plus there are enough overlap\nhours for inter-team communication. A team split between many time zones\nmay have three or four shifts per day.\nMany variations are also possible. Some teams prefer half-weeks instead of full\nweeks. Follow the sun can be done with with two, three, or four shifts per day\ndepending on where people are located.\nAlert Frequency\nWith so many variations, it can be difﬁcult to decide which to use. Develop a\nstrategy where the frequency and duration of oncall shifts are determined by how\nfrequent alerts are. People do their best oncall work when they are not overloaded\nand have not been kept awake for days on end. An oncall system improves oper-\nations when each alert receives follow-up work such as causal analysis. Before\nsomeone goes oncall again, that person should have had enough time to complete\nall follow-up work.\nFor example, if alerts are extremely rare, occurring less than once a week, a\nsingle person being oncall for 7 days at a time is reasonable. If there are three\nalerts each day, two or three shifts gives people time to sleep between alerts and do\n",
      "content_length": 2790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "290\nChapter 14\nOncall\nfollow-up tasks. If follow-up work is extensive, half-week rotations of two shifts\nmay be required.\nIf there is a signiﬁcantly high alert ratio, more complex schemes are needed.\nFor example, in some schedules, two people are oncall: one receives the alert if the\nother is busy, plus two secondaries shadow the primaries. Some systems divide the\nwork geographically, with a different rotation scheme and schedule for each major\nregion of the world.\nSchedule Coordination\nOncall schedules may be coordinated with other schedules. For example, it may be\na guideline that the week before your oncall shift, you are onduty. There may be a\ndedicated schedule of escalation points. For example, people knowledgeable in a\nparticular service may coordinate to make sure that they aren’t all on vacation at the\nsame time. That ensures that if oncall needs to escalate to them, someone is avail-\nable. Sometimes an escalation schedule is an informal agreement, and sometimes\nit is a formal schedule with an SLA.\nCompensation\nCompensation drives some design elements of the schedule. In some countries,\nbeing oncall requires compensation if response time is less than a certain interval.\nThe compensation is usually a third of the normal hourly salary for any hour oncall\noutside of normal business hours. It may be paid in cash or by giving the oncall\nperson time off. Compensation rates may be different if the person is called to\naction. Your human resources department should be able to provide all the details\nrequired. In some countries, there is no legal obligation for oncall compensation\nbut good companies do it anyway because it is unethical otherwise. One beneﬁt of\nfollow-the-sun coverage is that it can be constructed in a way that maximizes time\noncall during normal business hours for a location, while minimizing the amount\nof additional compensation that needs to be budgeted.\n14.1.5 The Oncall Calendar\nThe oncall calendar documents who is oncall when. It turns the theory of the\nschedule and roster into speciﬁcs. The monitoring system uses this information\nto decide who to send alerts to.\nSet the calendar far enough ahead to permit all concerned to plan vacations,\ntravel, and other responsibilities in advance. Three to six months is usually sufﬁ-\ncient. The details of building the calendar are as varied as there are teams. A team of\nsix that changes the oncall person every Wednesday may simply use the “repeating\nevent” functionality of an online calendar to schedule who is oncall when. Conﬂicts\nand other issues can be worked out between members.\n",
      "content_length": 2587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "14.1\nDesigning Oncall\n291\nA more complex schedule and a larger team require proportionately more\ncomplex calendar building strategies. One such system used a shared, online\nspreadsheet such as Google Drive. The spreadsheet cells represented each time slot\nfor the next three months. Due to the size of the team, everyone was expected to\ntake three time slots. The system was “ﬁrst come, ﬁrst served,” and there was a lot\nof back-channel discussion that enabled people to trade time slots. The negotiating\ncontinued until a certain cut-off point, at which time the schedule was locked. This\nsystem was unfair to people who happened to be out the day the schedule was\nmade.\nSome companies take a more algorithmic approach. Google had hundreds of\nindividual calendars to create for any given month due to the existence of many\ninternal and external services. Each team spent a lot of time negotiating and assem-\nbling calendars until someone wrote a program that did the task for them. To use\nthe system, a team would create a Google Calendar and everyone inserted events to\nmark which days they were entirely unavailable, available but not preferred, avail-\nable, or preferred. The system took a conﬁguration ﬁle that described parameters\nsuch as how long each shift was, whether there was a required gap of time before\nsomeone could have another rotation, and so on. The system then read people’s\npreferences from the Google Calendar and churned on the data until a reasonable\noncall calendar was created.\n14.1.6 Oncall Frequency\nThe frequency of how often a person goes oncall needs careful consideration. Each\nalert has a certain amount of follow-up work that should be completed before the\nnext turn at oncall begins. Each person should also have sufﬁcient time between\noncall shifts to work on projects, not just follow-up work.\nThe follow-up work from an alert can be extensive. Writing a postmortem can\nbe an arduous task. Root cause analysis can involve extensive research that lasts\ndays or weeks.\nThe longer the oncall shift, the more alerts will be received and the more\nfollow-up projects the person will be trying to do at the same time. This can\noverload a person.\nThe more closely the shifts are spaced, the more likely the work will not be\ncompleted by the time the next shift starts.\nDoing one or two postmortems simultaneously is reasonable, but much more\nis impossible. Therefore shifts should be long enough that only one or two signif-\nicant alerts have accumulated. Depending on the service, this may be one day, a\nweek of 8-hour periods, or a week of 24 × 7 service. The next such segment should\nbe spaced at least three weeks apart if the person is expected to complete both\npostmortems, do project work, and be able to go on an occasional vacation. If a\n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "292\nChapter 14\nOncall\nservice receives so many alerts that this is not possible, then the service has deeper\nissues.\nOncall shifts can be stressful. If the source of stress is that the shift is too busy,\nconsider using shorter shifts or having a second person oncall to handle overﬂow.\nIf the source of stress is that people do not feel conﬁdent in their ability to handle\nthe alerts, additional training is recommended. Ways to train a team to be more\ncomfortable dealing with outage situations are discussed in Chapter 15.\n14.1.7 Types of Notifications\nThere are many levels of urgency at which monitoring and other services need to\nraise the attention of human operators. Only the most urgent is an alert.\nEach level of urgency should have its own communication method. If urgent\nalerts are simply sent to someone’s email inbox, they may not be noticed in time. If\nnon-urgent messages are communicated by sending an SMS to the person oncall,\nthe “Boy Who Cried Wolf” syndrome will develop.\nThe best option is to build a very high-level classiﬁcation system:\n• Alert Oncall: The SLA is in violation, or if a condition is detected that, if left\nunattended, will result in an SLA violation.\n• Create a Ticket: The issue needs attention within one business day.\n• Log to a File: The condition does not require human attention. We do not want\nto lose the information, but we do not need to be notiﬁed.\n• Do Nothing: There is no useful information; nothing should be sent.\nIn some organizations, all of these situations are communicated by email to the\nentire system administration team. Under these conditions, all team members\nmight be compelled to ﬁlter all messages to a folder that is ignored. This defeats\nthe purpose of sending the messages in the ﬁrst place.\nEmail is, quite possibly, the worst alerting mechanism. Expecting someone to\nsit and watch an email inbox is silly, and a waste of everyone’s time. With this\nstrategy, staff will be unaware of new alerts if they step away or get involved in\nother projects.\nDaily emails that report on the result of a status check are also a bad idea.\nIf the status is ﬁne, log this fact. If a problem was detected, automatically open a\nticket. This prevents multiple people from accidentally working on the same issue\nat the same time. If the problem is urgent enough that someone should be alerted\nimmediately, then why is the check being done only once per day? Instead, report\nthe status to the monitoring system frequently and send alerts normally.\nHowever, it is a good idea to use email as a secondary mechanism. That is,\nwhen sending a message to a pager or creating a ticket, also receiving a copy via\n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "14.1\nDesigning Oncall\n293\nemail is useful. Many systems have mechanisms to subscribe to such messages in a\nway that permits precise ﬁltering. For example, it is usually possible to conﬁgure a\nticket system to email notiﬁcations of new or updated tickets in a particular queue.\nOnce an alert is triggered, there are many ways to notify the person who is\noncall. The most common alert methods are identiﬁed here:\n• One-Way and Two-Way Pagers: Hand-held devices that receive text mes-\nsages via terrestrial broadcasts. Two-way pagers permit sending a reply to\nacknowledge that the message was received.\n• SMS or Text Message to a Mobile Phone: Sending a text or SMS message to\na person’s mobile phone is convenient because most people already carry a\nmobile phone. In some countries, pagers are signiﬁcantly more reliable than\nSMS; in others, the reverse is true. If you are creating an alerting system for\nco-workers in another country, do not assume that what works well for you\nwill be viable elsewhere. Local people should test both.\n• Smart Phone App: Smart phone apps are able to display additional informa-\ntion beyond a short text message. However, they often depend on Internet\nconnectivity, which may not always be available.\n• Voice Call: A voice synthesizer and other software is used to call a per-\nson’s phone and talk to him or her, asking the person to press a button to\nacknowledge the message (otherwise, the escalation list will be activated).\n• Chat Room Bot: A chat room bot is a software robot that sits in the team’s chat\nroom and announces any alerts. This is a useful way to keep the entire team\nengaged and ready to help the oncall person if needed.\n• Alerting Dashboard: The alerting dashboard is a web page that shows the\nhistory of alerts sent. It provides useful context information.\n• Email: Email should never be the only way the oncall person is alerted. Sitting\nat your computer watching your inbox is a terrible use of your time. Never-\ntheless, it is useful to have every alert emailed to the oncall person as a backup\nmethod. This way the full message is received; SMS truncates messages to 160\ncharacters.\nAt least two methods should be used to make sure the message gets through.\nA pager service might have an outage. A sleeping oncall person might require an\nSMS tone and a voice call to wake him or her. One method should communicate\nthe complete, non-truncated, message to a stable storage medium. Email works\nwell for this. Lastly, chat room bots and other methods should also be deployed,\nespecially given that this strategy enables the entire team to stay aware of issues\nas they happen. They may seem like novelties at ﬁrst but they quickly become\nindispensable.\n",
      "content_length": 2706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "294\nChapter 14\nOncall\n14.1.8 After-Hours Maintenance Coordination\nSometimes maintenance must be done after hours. For example, another team may\ndo maintenance that requires your participation, such as failing over services so\nthey are not affected by scheduled outages, or being around to verify status, and\nso on.\nIt is very common to assign such tasks to whoever will be oncall at the time.\nFor most situations, this is ﬁne. However, it does create the possibility that an oncall\nperson might be dealing with an alert and coordinating an unrelated issue at the\nsame time.\nIf the oncall person is used for such tasks, the secondary oncall person should\ntake any alerts that happen during the maintenance window. Alternatively, assign\nthe maintenance window coordination to the secondary person.\n14.2 Being Oncall\nNow that we know how to develop a roster, schedule, and calendar, we can\nconsider the responsibilities associated with being oncall. An oncall person has\nresponsibilities before, during, and after each shift.\n14.2.1 Pre-shift Responsibilities\nBefore a shift begins, you should make sure you are ready. Most teams have an\noncall shift preparedness checklist. Items on the checklist verify reachability and\naccess. Reachability means that the alert notiﬁcation system is working. You might\nsend a test alert to yourself to verify that everything is working. Access means\nthat you have access to the resources needed when responding to an alert: your\nVPN software works, your laptop’s batteries are charged, you are sober enough to\nperform operations, and so on.\nCorrecting any problems found may take time. Therefore the checklist should\nbe activated appropriately early, or early enough to negotiate extending the current\noncall person’s shift or ﬁnding a replacement. For example, discovering that your\ntwo-factor authenticator is not working may require time to set up a new one.\n14.2.2 Regular Oncall Responsibilities\nOnce the shift begins you should do…nothing special. During working hours you\nshould work as normal but take on only tasks that can be interrupted if needed. If\nyou attend a meeting, it is a good idea to warn people at the start that you are oncall\nand may have to leave at any time. If you are oncall outside of normal working\nhours, you should sleep as needed and basically live a normal life, other than being\naccessible. If travel is required, such as to go home from work, establish temporary\ncoverage from your secondary oncall person for the duration of the drive.\n",
      "content_length": 2506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "14.2\nBeing Oncall\n295\nWhat you should not do during oncall is sit at your computer watching your\nservice dashboards and monitoring systems to see if you can spot problems. This\nis a waste of time and is exactly what monitoring systems are made for. If this kind\nof activity is a requirement of the oncall shift, then your oncall system has been\npoorly designed.\nSome teams have a list of tasks that are done during each shift. Some example\ntasks include verifying the monitoring system is working, checking that backups\nran, and checking for security alerts related to software used in-house. These tasks\nshould be eliminated through automation. However, until they are automated,\nassigning responsibility to the current oncall person is a convenient way to spread\nthe work around the team. These tasks are generally ones that can be done between\nalerts and are assumed to be done sometime during the shift, though it is wise to\ndo them early in the shift so as not to forget them. However, if a shift starts when\nsomeone is normally asleep, expecting these tasks to be done at the very start of\nthe shift is unreasonable. Waking people up for non-emergencies is not healthy.\nTasks may be performed daily, weekly, or monthly. In all cases there should\nbe a way to register that the task was completed. Either maintain a shared spread-\nsheet where people mark things as complete, or automatically open a ticket to be\nclosed when the task is done. All tasks should have an accompanying bug ID that\nrequests the task be eliminated though automation or other means. For example,\nverifying that the monitoring system is running can be automated by having a sys-\ntem that monitors the monitoring system. (See Section 16.5, “Meta-monitoring.”)\nA task such as emptying the water bucket that collects condensation from a tem-\nporary cooling device should be eliminated when the temporary cooling system is\nﬁnally replaced.\nOncall should be relatively stress-free when there is no active alert.\n14.2.3 Alert Responsibilities\nOnce alerted, your responsibilities change. You are now responsible for verifying\nthe problem, ﬁxing it, and ensuring that follow-up work gets completed. You may\nnot be the person who does all of this work, but you are responsible for making\nsure it all happens through delegation and handoffs.\nYou should acknowledge the alert within the SLA described previously.\nAcknowledging the alert tells the alerting system that it should not try to alert\nthe next contact on the escalation list.\nQuick Fixes versus Long-Term Fixes\nNow the issue is worked on. Your priority is to come up with the best solution that\nwill resolve the issue within the SLA. Sometimes we have a choice between a long-\nterm ﬁx and a quick ﬁx. The long-term ﬁx will resolve the fundamental problem\nand prevent the issue in the future. It may involve writing code or releasing new\nsoftware. Rarely can that be done within the SLA. A quick ﬁx ﬁts within the SLA\n",
      "content_length": 2944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "296\nChapter 14\nOncall\nbut may simply push the issue farther down the road. For example, rebooting a\nmachine may ﬁx the problem for now but will require rebooting it again in a few\ndays because the technical problem was not ﬁxed. However, the reboot can be done\nnow and will prevent an SLA violation.\nIn general, encourage a bias toward long-term ﬁxes over quick ﬁxes: “a stitch\nin time saves nine.” However, oncall is different from normal engineering. Oncall\nplaces a higher priority on speed than on long-term perfection. Since solutions that\ndo not ﬁt within the SLA must be eliminated, a quick ﬁx may be the only option.\nAsking for Help\nIt is also the responsibility of the oncall person to ask for help when needed. Esca-\nlate to more experienced or knowledgable people, or if the issue was raised long\nenough ago, ﬁnd someone who is better rested than you are. You don’t have to save\nthe world single-handedly. You are allowed to call other folks for help. Reach out to\nothers especially if the outage is large or if there are multiple alerts at the same time.\nYou don’t have to ﬁx the problem yourself necessarily. Rather, it is your respon-\nsibility to make sure it gets ﬁxed, which sometimes is best done by looping in the\nright people and coordinating rather than trying to handle everything yourself.\nFollow-up Work\nOnce the problem has been resolved, the priority shifts to raising the visibility of\nthe issue so that long-term ﬁxes and optimizations will be done. For simple issues,\nit may be sufﬁcient to ﬁle a bug report or add annotations to an existing one. More\ncomplex issues require writing a postmortem report that captures what happened\nand makes recommendations about how it can be prevented in the future. By doing\nthis we build a feedback loop that assures operations get better over time, not\nworse. If the issue is not given visibility, the core problem will not be ﬁxed. Do\nnot assume that “everyone knows it is broken” means that it will get ﬁxed. Not\neveryone does know it is broken. You can’t expect that managers who prioritize\nwhich projects are given resources will know everything or be able to read your\nmind. Filing bug reports is like picking up litter: you can assume someone else\nwill do it, but if everyone did that nothing would ever be clean.\nOnce the cause is known, the alert should be categorized so that metrics can be\ngenerated. This helps spot trends and the resulting information should be used to\ndetermine future project priorities. It is also useful to record which machines were\ninvolved in a searchable way. Future alerts can then be related to past ones, and\nsimple trends such as the same machine failing repeatedly can be spotted.\nOther follow-up tasks are discussed in Section 14.3.\n14.2.4 Observe, Orient, Decide, Act (OODA)\nThe OODA loop was developed for combat operations by John Boyd. Designed\nfor situations like ﬁghter jet combat, it ﬁts high-stress situations that require quick\n",
      "content_length": 2946,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "14.2\nBeing Oncall\n297\nresponses. Kyle Brandt (2014) popularized the idea of applying OODA to system\nadministration.\nSuppose the alert relates to indicators that your web site is slow and\noften timing out. First we Observe: checking logs, reading I/O measurements,\nand so on.\nNext we Orient ourselves to the situation. Orienting is the act of analyzing\nand interpreting the data. For example, logs contain many ﬁelds, but to turn that\ndata into information the logs need to be queried to ﬁnd anomalies or patterns. In\nthis process we come up with a hypothesis based on the data and our experience\nto ﬁnd the real cause.\nNow we Decide to do something. Sometimes we decide that more informa-\ntion is needed and begin to collect it. For example, if there are indications that the\ndatabase is slow, then we collect more speciﬁc diagnostics from the database and\nrestart the loop.\nThe last stage is to Act and make changes that will either ﬁx the problem, test\na hypothesis, or give us more data to analyze. If you decide that certain queries\nare making the database server slow, eventually someone has to take action to ﬁx\nthem.\nThe OODA loop will almost always have many iterations. More experienced\nsystem administrators can iterate through the loop logically, rapidly, and smoothly.\nAlso, over time a good team develops tools to make the loop go faster and gets\nbetter at working together to tighten the loop.\n14.2.5 Oncall Playbook\nIdeally, every alert that the system can generate will be matched by documentation\nthat describes what to do in response. An oncall playbook is this documentation.\nThe general format is a checklist of things to check or do. If the end of the list\nis reached, the issue is escalated to the oncall escalation point (which itself may be\na rotation of people). This creates a self-correcting feedback loop. If people feel that\nthere are too many escalations waking up them late at night, they can correct the\nproblem by improving the documentation to make oncall more self-sufﬁcient.\nIf they feel that writing documentation is unimportant or “someone else’s job,”\nthey can, by virtue of not creating proper checklists, give oncall permission to wake\nthem up at all hours of the night. It is impressive how someone who feels that\nwriting documentation is below them suddenly learns the joy of writing after being\nwoken up in the middle of the night. The result of this feedback loop is that each\nchecklist becomes as detailed as needed to achieve the right balance.\nWhen writing an oncall playbook, it can be a challenge to determine how\ndetailed each checklist should be. A statement like “Check the status of the\ndatabase” might be sufﬁcient for an experienced person. The actual steps required\nto do that, and instructions on what is considered normal, should be included. It\nis usually too much detail to explain very basic information like how to log in.\n",
      "content_length": 2886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "298\nChapter 14\nOncall\nIntentionally select a baseline of knowledge that is assumed. There should be\ndocumentation that will take a new employee and bring him or her up to that level.\nThe oncall documents, then, can assume that level of knowledge. This also helps\nprevent the situation where documentation becomes too verbose and repetitive.\nRequiring authors to include too much detail can become an impediment to writing\ndocumentation at all.\n14.2.6 Third-Party Escalation\nSometimes escalations must include someone outside the operations team, such as\nthe oncall team of a different service or a vendor. Escalating to a third party has\nspecial considerations.\nDuring an outage, one should not have to waste time researching how to con-\ntact the third party. All third-party dependencies should be documented. There\nshould be a single globally accessible list that has the oncall information for each\ninternal team. If the oncall numbers change with each shift, this list should be\ndynamically generated with the current information.\nLists of contact information for vendors usually contain information that\nshould not be shared outside of the team and, therefore, is stored differently. For\neach dependency, this private list should include the vendor name, contact infor-\nmation, and anything needed to open a support issue. For example, there may be\nlicense information or support contract numbers.\n.\nTips for Managing Vendor Escalations\nSometimes support cases opened with a vendor remain open for many days.\nWhen this happens:\n• One person should interface with the vendor for the duration of the issue.\nOtherwise, communication and context can become disorganized.\n• Trust but verify. Do not let a vendor close the case until you have veriﬁed\nthe issue is resolved.\n• Report the issue to your sales contact only if you are not getting results.\nOtherwise, the sales representative may meddle, which is annoying to\ntechnical support people.\n• Take ownership of the issue. Do not assume the vendor will. Be the person\nwho follows up and makes sure the process keeps moving.\n• When possible, make follow-up calls early in the day. This sets up the\nvendor to spend the day working on your issue.\n",
      "content_length": 2198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "14.3\nBetween Oncall Shifts\n299\n14.2.7 End-of-Shift Responsibilities\nEventually your shift will end and it will be time to hand off oncall to the next\nperson. Making sure the transition goes well ensures that context is transferred to\nthe next person and open issues are not forgotten.\nOne strategy is to write an end-of-shift report that is emailed to the entire\noncall roster. Sending it to just the next person oncall is vulnerable to error. You\nmay pick the wrong person by mistake, or may not know about a substitution that\nwas negotiated. Sending the report to everyone keeps the entire team up-to-date\nand gives everyone an opportunity to get involved if needed.\nThe end-of-shift report should include any notable events that happened and\nanything that the next shift needs to know. For example, it should identify an\nongoing outage or behaviors that need manual monitoring.\nAnother strategy is to have an explicit handoff to the next shift. This means the\noutgoing person must explicitly state that he or she is handing off responsibility to\nthe next person, and the next person must positively acknowledge the handoff.\nIf the next person does not or cannot acknowledge the handoff, then responsi-\nbility stays with the original person. This technique is used in situations where\navailability requirements are very strict.\nIn this case the end-of-shift report may be verbal or via email. A written report\nis better because it communicates to the entire team.\nIf shift change happens when one or both parties may be asleep, a simple email\nmay be sufﬁcient if there are no ongoing issues. If there are ongoing issues, the next\noncall person should be alerted. If the outgoing person will be asleep when the\nshift changes, it is common practice to send out a provisional end-of-shift report,\nnoting that unless there are alerts between now and the shift change, this report\ncan be considered ﬁnal.\n14.3 Between Oncall Shifts\nThe normal working hours between shifts should be spent on project work. That\nincludes follow-up tasks related to oncall alerts. While such work can be done\nduring oncall time, sleep is usually more important.\nProjects related to oncall include working on the long-term solutions that\nweren’t possible to implement during oncall and postmortem reports.\n14.3.1 Long-Term Fixes\nEach alert may generate follow-up work that cannot be done during the oncall\nperiod. Large problems may require a causal analysis to determine the root cause\nof the outage.\n",
      "content_length": 2478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "300\nChapter 14\nOncall\nIn our previous example, a problem was solved by rebooting a machine.\nCausal analysis might indicate that the software has a memory leak. Working with\nthe developers to ﬁnd and ﬁx the memory leak is a long-term solution.\nEven if you are not the developer who will ultimately ﬁx the code, there is\nplenty of work that can be done besides hounding the developers who will pro-\nvide the ﬁnal solution. You can set up monitoring to collect information about the\nproblem, so that before and after comparisons can be made. You can work with\nthe developers to understand how the issue is affecting business objectives such as\navailability.\n14.3.2 Postmortems\nA postmortem is a process that analyzes an outage and documents what happened\nand why, and makes recommendations about how to prevent that outage in the\nfuture.\nA good postmortem process communicates up and down the management\nchain. It communicates to users that action is being taken. It communicates to peer\nteams so that interactions (good and bad) are learned. It can also communicate to\nunrelated teams so they can learn from your problems.\nThe postmortem process should not start until after the outage is complete. It\nshould not be a distraction from ﬁxing the outage.\nA postmortem is part of the strategy of continuous improvement. Each user-\nvisible outage or SLA violation should be followed by a postmortem and conclude\nwith implementation of the recommendations in the postmortem report. By doing\nso we turn outages into learning, and learning into action.\nPostmortem Purpose\nA postmortem is not a ﬁnger-pointing exercise. The goal is to identify what went\nwrong so the process can be improved in the future, not to determine who is to\nblame. Nobody should be in fear of getting ﬁred for having their name associated\nwith a technical error. Blame discourages the kind of openness required to have\nthe transparency that enables problems to be identiﬁed so that improvements can\nbe made. If a postmortem exercise is a “name and shame” process, then engineers\nbecome silent on details about actions and observations in the future. “Cover your\nass” (CYA) behavior becomes the norm. Less information ﬂows, so management\nbecomes less informed about how work is performed and other engineers become\nless knowledgeable about pitfalls within the system. As a result, more outages\nhappen, and the cycle begins again.\nThe postmortem process records, for any engineers whose actions have con-\ntributed to the outage, a detailed account of actions they took at the time, effects\nthey observed, expectations they had, assumptions they had made, and their\nunderstanding of the timeline of events as they occurred. They should be able to\ndo this without fear of punishment or retribution.\n",
      "content_length": 2760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "14.3\nBetween Oncall Shifts\n301\nThis is not to say that staff members get off the hook for making mistakes.\nThey are on the hook for many things. They are now the experts responsible for\neducating the organization on how not to make that mistake in the future. They\nshould drive engineering efforts related to improving the situation.\nA culture of accountability, rather than blame, fosters an organization that\nvalues innovation. If blame is used to avoid responsibility, the whole team suf-\nfers. For more information about this topic, we recommend Allspaw’s (2009) article\n“Blameless Postmortems and a Just Culture.”\n.\nA Postmortem Report for Every High-Priority Alert\nAt Google many teams had a policy of writing a postmortem report every time\ntheir monitoring system paged the oncall person. This was done to make sure\nthat no issues were ignored or “swept under the rug.” As a result there was\nno back-sliding in Google’s high standards for high uptime. It also resulted\nin the alerting system being highly tuned so that very few false alarms were\ngenerated.\nPostmortem Report\nPostmortem reports include four main components: a description of the outage,\na timeline of events, a contributing conditions analysis (CCA), and recommenda-\ntions to prevent the outage in the future. The outage description should say who\nwas affected (for example, internal customers or external customers) as well as\nwhich services were disrupted. The timeline of events may be reconstructed after\nthe fact, but should identify the sequence of what actually happened and when\nso that it is clear. The CCA should go into detail as to why the outage occurred\nand include any signiﬁcant context that may have contributed to the outage (e.g.,\npeak service hours, signiﬁcant loads). Finally, the recommendations for prevention\nin the future should include a ﬁled ticket or bug ID for each recommendation in\nthe list.\nYou will ﬁnd a sample postmortem template in Section D.3 of Appendix D. If\nyour organization does not have a postmortem template, you can use this as the\nbasis for yours.\nThe executive summary should include the most basic information of when\nthe incident happened and what the root causes were. It should reiterate any rec-\nommendations that will need budget approval so that executives can connect the\nbudget request to the incident in their mind.\nCausal analysis or contributing conditions analysis ﬁnds the conditions that\nbrought about the outage. It is sometimes called root cause analysis but that\nimplies that outages have only one cause. If tradition or ofﬁce politics requires\n",
      "content_length": 2583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "302\nChapter 14\nOncall\nusing the word “root,” at least call it a root causes analysis to emphasize that there\nare many possible causes.\nWhile emotionally satisfying to be able to point to a single cause, the reality\nis that there are many factors leading up to an outage. The belief that an outage\ncould have a single cause implies that operations is a series of dominos that topple\none by one, leading up to an outage. Reality is much more complex. As Allspaw’s\n(2012a) article “Each Necessary, But Only Jointly Sufﬁcient” points out, ﬁnding the\nroot cause of a failure is like ﬁnding the root cause of a success.\nPostmortem Communication\nOnce the postmortem report is complete, copies should be sent to the appropriate\nteams, including the teams involved in ﬁxing the outage and the people affected\nby the outage. If the users were external to the company, a version with proprietary\ninformation removed should be produced. Be careful to abide by your company’s\npolicy about external communications. The external version may be streamlined\nconsiderably. Publishing postmortems externally builds customer conﬁdence, and\nit is a best practice.\nWhen communicating externally, the postmortem report should be accompa-\nnied by an introduction that is less technical and highlights the important details.\nMost external customers will not be able to understand a technical postmortem.\nInclude speciﬁc details such as start and end times, who or what was impacted,\nwhat went wrong, and what were the lessons learned. Demonstrate that you are\nusing the experience to improve in the future. If possible, include human elements\nsuch as heroic efforts, unfortunate coincidences, and effective teamwork. You may\nalso include what others can learn from this experience.\nIt is important that such communication be authentic, admit failure, and sound\nlike a human, not a press agent. Figure 14.1 is an example of good external commu-\nnication. Notice that it is written in the ﬁrst person, and contains real remorse—no\nhiding here. Avoid the temptation to hide by using the third person or to mini-\nmize the full impact of the outage by saying something like “We regret the impact\nit may have had on our users and customers.” Don’t regret that there may have\nbeen impact. There was impact—otherwise you wouldn’t be sending this mes-\nsage. More advice can be found in the blog post “A Guideline for Postmortem\nCommunication” on the Transparent Uptime blog (Rachitsky 2010).\n14.4 Periodic Review of Alerts\nThe alert log should be reviewed periodically to spot trends and allocate resources\nto create long-term ﬁxes that ultimately reduce the total number of alerts received.\nWhen this strategy is implemented, alerts become more than just a way to be made\naware of problems—they become one of your primary vehicles for improving\nsystem stability.\n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "14.4\nPeriodic Review of Alerts\n303\n.\nSubject: CloudCo Incident Report for February 4, 2014; brief interruption in DNS service\nLast week on Tuesday, February 4, CloudCo experienced an outage of our DNS service lasting\napproximately 4 minutes. As a result of this outage, our customers experienced 4 minutes of\ndowntime as we worked to restore full service. I would like to apologize to our customers for the\nimpact to your daily operations as a result of this outage. Unplanned downtime of any length is\nunacceptable to us. In this case we fell short of both our customers’ expectations and our own.\nFor that, I am truly sorry.\nI would like to take a moment and explain what caused the outage, what happened during the\noutage, and what we are doing to help prevent events like this in the future.\nTechnical Summary\nCloudCo experienced a DNS outage lasting 4 minutes. The issue was the result of accidental\ninput into a DNS rate-limiting tool, which resulted in a rate limit of zero queries per second being\nset. The problem was recognized by the Network Operations team immediately, the rate limit\nwas identiﬁed within 2 minutes, and the conﬁguration was reverted the following minute. DNS\nservice was returned to normal status on a network-wide basis within 4 minutes of the beginning\nof the incident.\nIncident Description\nAt 17:10 Paciﬁc Standard Time (01:10 UTC), the CloudCo DNS service was not responding to\nDNS queries. All DNS queries to CloudCo DNS were being dropped during the 4 minutes the\nrate limit was applied. This affected all DNS queries sent to BIND.\nTimeline of Events – February 4, 2014\n17:10 PST (01:10 UTC) - An erroneous rate limit was applied to CloudCo DNS service\n17:11 PST - Internal automated alerts\n17:13 PST - Issue discovered to be with DNS rate limit\n17:14 PST - Erroneous DNS rate limit removed\n17:14 PST - CloudCo DNS conﬁrmed returned to normal status\n17:17 PST - CloudCoStatus tweets status update\nResolution\nThe erroneous change was made, recognized, and corrected within 4 minutes.\nRecommendations\n• Internal process revision for applying global DNS rate limits, with peer review.\n• Implement additional software controls to validate user input for tools.\nFigure 14.1: Example postmortem email sent to customers after an outage\nThere should be a systematic approach to reduce the number of alerts or\nentropy is likely to make alerts more frequent over time until the volume of alerts\nspirals out of control. Alerts should also be analyzed for trends because each alert\nis a potential indicator of a larger issue. If we aim to improve our system continu-\nously, we must take every opportunity to seek out clues to where improvements\nare needed.\nIt’s useful to have a weekly meeting to review alerts and issues and look\nfor trends. At this meeting, you can identify projects that would ﬁx more com-\nmon issues as well as take an overall snapshot of the health of your production\n",
      "content_length": 2914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "304\nChapter 14\nOncall\nenvironment. Quarterly reviews can be useful to spot even larger trends and can\nbe folded into quarterly project planning cycles.\nThe alert log should be annotated by the person who received the alert. Most\nsystems permit alerts to be tagged with keywords. The keywords can then be\nanalyzed for trends. Some sample keywords are listed here:\ncause:network\ncause:human\ncause:bug\ncause:hardware\ncause:knownissue\nseverity:small\nseverity:medium\nseverity:large\nbug:BUGID\ntick:TICKETNUMBER\nmachine:HOSTNAME\nThese keywords enable you to annotate the general cause of the alert, its sever-\nity, and related bug and ticket IDs. Multiple tags can be used. For example, a\nsmall outage caused by a hardware failure might be tagged cause:hardware and\nseverity:small. If the problem was caused by a known bug, the alert might be\ntagged cause:knownissue and bug:12345, assuming the known issue has a bug ID\n12345. Items marked as such would be reserved for situations where there has been\na management decision to not ﬁx a bug or the ﬁx is in progress and a workaround\nis in place until the ﬁx is delivered.\nAs part of your analysis, produce a report showing the most common causes\nof alerts. Look for multiple alerts with the same bug ID or ticket numbers. Also\nlook for the most severe outages and give them special scrutiny, examining the\npostmortem reports and recommendations to see if causes or ﬁxes can be clustered\nor applied to more than one outage.\n14.5 Being Paged Too Much\nA small number of alerts is reasonable but if the number grows too much, inter-\nvention may be required. What constitutes too many alerts is different for different\nteams. There should be an agreed-upon threshold that is tolerated.\nIf the threshold is constantly being violated and things are getting worse, here\nare some interventions one may consider:\n• If a known bug results in frequent pages after a certain amount of time (say,\ntwo release cycles), in the future this alert should automatically be directed to\n",
      "content_length": 2007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "14.6\nSummary\n305\nthe developers’ oncall rotation. If there is no developers’ oncall rotation, push\nto start one. This aligns motivations to have problems ﬁxed.\n• Any alerts received by pager that are not directly related to maintaining the\nSLA should be changed from an alert that generates a page to an alert that\ngenerates a ticket in your trouble-ticketing system.\n• Meet with the developers about this speciﬁc problem. Ensure they understand\nthe seriousness of the issue. Create shared goals to ﬁx the most frequent or\nrecurring issues. If it is part of your culture, set up a list of bugs and have bug\nbash parties or a Fix-It Week.\n• Negotiate to temporarily reduce the SLA. Adjust alerts accordingly. If alerting\nthresholds are already not aligned with SLA (i.e., you receive alerts for low-\npriority issues), then work to get them into alignment. Get agreement as to\nthe conditions by which the temporary reduction will end. It might be a ﬁxed\namount of time, such as a month, or a measurable condition, such as when\nthree successive releases have been pushed without failure or rollback.\n• If all else fails, institute a code yellow: allow the team to defer all other work\nuntil the situation has improved. Set up a metric to measure success and work\ntoward that goal.\n14.6 Summary\nAs part of our mission to maintain a service, we must have a way to handle excep-\ntional situations. To assure that they are handled properly, an oncall rotation is\ncreated.\nThe people who are in the oncall rotation should include operations peo-\nple and developers, so as to align operational priorities. There are many ways\nto design an oncall schedule: weekly, daily, or multiple shifts per day. The goal\nshould be to have no more than a few alerts per shift so that follow-up work can be\ncompleted.\nAn oncall person can be notiﬁed many ways. Generally alerts are shared\nby sending a message to a hand-held device such as a phone plus one other\nmechanism for redundancy.\nBefore a shift, an oncall preparedness checklist should be completed. People\nshould be reachable while oncall in case there is an alert; otherwise, they should\nwork and sleep as normal.\nOnce alerted, the oncall person’s top priority is to resolve the situation, even\nif it means implementing a quick ﬁx and reserving the long-term ﬁx for later.\nAn oncall playbook documents actions to be taken in response to various\nalerts. If the documentation is insufﬁcient, the issue should be escalated to the\nservice owner or other escalation rotation.\n",
      "content_length": 2508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "306\nChapter 14\nOncall\nAlerts should be logged. For major alerts, a postmortem report should be writ-\nten to record what happened, what was done to ﬁx the problem, and what can be\ndone in the future to prevent the problem.\nAlert logs and postmortems should be reviewed periodically to determine\ntrends and select projects that will solve systemic problems and reduce the number\nof alerts.\nExercises\n1. What are the primary design elements of an oncall system?\n2. Describe your current oncall policy. Are you part of an oncall rotation?\n3. How do priorities change for an oncall staffer when alerted?\n4. What are the prerequisites for oncall duty at your organization?\n5. Name 10 things that you monitor. For each of them, which type of notiﬁcation\nis appropriate and why?\n6. Which four elements go into a postmortem, and which details are required\nwith each element?\n7. Write a postmortem report for an incident in which you were involved.\n8. How should an oncall system improve operations over time?\n",
      "content_length": 1000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Chapter 15\nDisaster Preparedness\nFailure is not falling down\nbut refusing to get back up.\n—Theodore Roosevelt\nDisasters and major outages happen. Everyone in the company from the top down\nneeds to recognize that fact and adopt a mindset that accepts outages and learns\nfrom them. An operations organization needs to be able to handle outages well and\navoid repeating past mistakes.\nPreviously we’ve examined technology related to being resilient to failures\nand outages as well as organizational strategies like oncall. In this chapter we dis-\ncuss disaster preparedness at the individual, team, procedural, and organizational\nlevels. People must be trained so that they know the procedure well enough that\nthey can execute it with conﬁdence. Teams need to practice together to build team\ncohesion and conﬁdence, and to ﬁnd and ﬁx procedural problems. Organizations\nneed to practice to ﬁnd inter-team gaps and to ensure the organization as a whole\nis ready to handle the unexpected.\nEvery organization should have a strategy to ensure disaster preparedness at\nall these levels. At the personnel level, training should be both formal (books, doc-\numentation, mentoring) and through game play. Teams and organizations should\nuse ﬁre drills and game day exercises to improve processes and ﬁnd gaps in cov-\nerage. Something like the Incident Command System (ICS) model, described later,\nshould be used to coordinate recovery from outages.\nSuccessful companies operating large distributed systems like Google, Face-\nbook, Etsy, Netﬂix, and others realize that the right way to handle outages is to be\nprepared for them, adopt practices that reduce outages in the future, and reduce\nrisk by practicing effective response procedures. In “Built to Win: Deep Inside\nObama’s Campaign Tech” (Gallagher 2012), we learned that even presidential\ncampaigns have found game day exercises critical to success.\n307\n",
      "content_length": 1897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "308\nChapter 15\nDisaster Preparedness\n15.1 Mindset\nThe ﬁrst step on the road to disaster preparedness is to acknowledge that disas-\nters and major outages happen. They are a normal and expected part of business.\nTherefore we prepare for them and respond appropriately when they occur.\nWe want to reduce the number of outages, but eliminating them totally is\nunrealistic. No technology is perfect. No computer system runs without fail. Elim-\ninating the last 0.00001 percent of downtime is more expensive than mitigating\nthe ﬁrst 99.9999 percent. Therefore as a tradeoff we tolerate a certain amount of\ndowntime and balance it with preparedness so that the situation is handled well.\nEqually, no individual is perfect. Everyone makes mistakes. We strive to make\nas few as possible, and never the same one twice. We try to hire people who are\nmeticulous, but also innovative. And we develop processes and procedures to try\nto catch the mistakes before they cause outages, and to handle any outages that\ndo occur as well as possible. As discussed in Section 14.3.2, each outage should\nbe treated as an opportunity to learn from our own and others’ mistakes and to\nimprove the system. An outage exposes a weakness and enables us to identify\nplaces to make the system more resilient, to add preventive checks, and to edu-\ncate the entire team so that they do not make the same mistake. In this way we\nbuild an organization and a service that is antifragile.\nWhile it is common practice at some companies, it is counterproductive to\nlook for someone to blame and ﬁre when a major incident occurs. When people\nfear being ﬁred, they will adopt behaviors that are antithetical to good operations.\nThey will hide their mistakes, reducing transparency. When the real root causes are\nobscured, no one learns from the mistake and additional checks are not put in place,\nmeaning that it is more likely to recur. This is why a part of the DevOps culture is\nto accept and learn from failure, which exposes problems and thus enables them\nto be ﬁxed.\nUnfortunately, we often see that when a large company or government web\nsite has a highly visible outage, its management scrambles to ﬁre someone to\ndemonstrate to all that the matter was taken seriously. Sometimes the media will\ninﬂame the situation, demanding that someone be blamed and ﬁred and question-\ning why it hasn’t happened yet. The media may eventually lay the blame on the\nCEO or president for not ﬁring someone. The best approach is to release a public\nversion of the postmortem report, as discussed in Section 14.3.2, not naming indi-\nviduals, but rather focusing on the lessons learned and the additional checks that\nhave been put in place to prevent it from happening again.\n15.1.1 Antifragile Systems\nWe want our distributed computing systems to be antifragile. Antifragile sys-\ntems become stronger the more they are stressed or exposed to random behavior.\n",
      "content_length": 2908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "15.1\nMindset\n309\nResilient systems survive stress and failure, but only antifragile systems actually\nbecome stronger in response to adversity.\nAntifragile is not the opposite of fragile. Fragile objects break, or change, when\nexposed to stress. Therefore the opposite of fragile is the ability to stay unchanged\nin the face of stress. A tea cup is fragile and breaks if not treated gently. The\nopposite would be a tea cup that stays the same (does not break) when dropped.\nAntifragile objects, by comparison, react to stress by getting stronger. For example,\nthe process of making steel involves using heat that would destroy most objects,\nbut the process instead makes steel stronger. The mythical Hydra is antifragile\nbecause it grows two heads for each one it loses. The way we learn is antifragile:\nstudying for an exam involves working our brain hard, which strengthens it.\nFragile systems break when the unexpected happens. Therefore, if we\nwant our distributed computing systems to be antifragile, we should introduce\nrandomness, frequently and actively triggering the resiliency features. Every time\na malfunction causes a team to failover a database to a replica, the team gets more\nskilled at executing this procedure. They may also get ideas for improvements.\nBetter execution and procedural improvements make the system stronger.\nTo accelerate this kind of improvement, we introduce malfunctions artiﬁcially.\nRather than trying to avoid malfunctions, we instigate them. If a failover process\nis broken, we want to learn this fact in a controlled way, preferably during nor-\nmal business hours when the largest number of people are awake and available\nto respond. If we do not trigger failures in a controlled way, we will learn about\nsuch problems only during a real emergency: usually after hours, usually when\neveryone is asleep.\n15.1.2 Reducing Risk\nPracticing risky procedures in a controlled fashion enables us to reduce risk. Do not\nconfuse risky behavior with risky procedures. Risky behavior is by deﬁnition risky\nand should be avoided. Procedures that are risky can be improved. For example,\nbeing careless is risky behavior that should be avoided. There’s no way to make\ncarelessness less risky other than to stop being careless. The risk is built in. How-\never, a procedure such as a web service failover may or may not be risky. If it is a\nrisky procedure, it can be improved and reengineered to make it less risky.\nWhen we confuse risky behavior with risky procedures, we end up applying\na good practice to the wrong thing. We make the mistake of avoiding procedures\nthat should, instead, be repeated often until the risk is removed. The technical term\nfor improving something through repetition is called “practice.” Practice makes\nperfect.\nThe traditional way of thinking is that computer systems are fragile and must\nbe protected. As a consequence, they were surrounded by protection systems,\nwhether they be management policies or technical features. Instead of focusing on\n",
      "content_length": 3003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "310\nChapter 15\nDisaster Preparedness\nbuilding a moat around our operations, we should commit to antifragile practices\nto improve the strength and conﬁdence of our systems and organizations.\nThis new attitude toward failure aligns management practices with the reality\nof complex systems:\n• New software releases always have new unexpected bugs.\n• Identifying the root cause is not intended to apportion blame, but rather to\nlearn how to improve the system and operational practices.\n• Building reliable software on top of unreliable components means that\nresiliency features are expected, not an undue burden, a “nice to have” feature,\nor an extravagance.\n• At cloud scale, complex failures are inevitable and unpredictable.\n• In production, complex systems often interact in ways that aren’t explicitly\nknown at ﬁrst (timeouts, resource contention, handoffs).\nIdeally we’d like perfect systems that have perfect uptime. Sadly, such systems\ndon’t exist outside of sales presentations. Until such systems do exist, we’d rather\nhave enough failures to ensure conﬁdence in the precautionary measures we put\nin place. Failover mechanisms need to be exercised whether they are automatic or\nmanual. If they are automatic, the more time that passes without the mechanism\nbeing activated, the less conﬁdent we can be that it will work properly. The sys-\ntem may have changed in ways that are unexpectedly incompatible and break the\nfailover mechanism. If the failover mechanism is a manual procedure, we not only\nlose conﬁdence in the procedure, but we also lose conﬁdence in the team’s ability\nto do the procedure. In other words, the team gets out of practice or the knowledge\nbecomes concentrated among a certain few. Ideally, we want services that fail often\nenough to maintain conﬁdence in the failover procedure but not often enough to\nbe detrimental to the service itself. Therefore, if a component is too perfect, it is\nbetter to artiﬁcially cause a failure to reestablish conﬁdence.\n.\nCase Study: Repeating Risky Behavior to Reduce Risk\nAt one company everyone knew that the last time a database needed to be\nfailed over, it didn’t go well. Therefore the team feared doing the procedure\nand avoided it, thinking this was good risk management. Instead, it actually\nincreased risk. If the process was needed in an emergency, it was unlikely\nto work. Realizing this fact, the manager made the team fail the service over\nevery week.\nThe ﬁrst few times were rough, but the entire team watched and com-\nmented as the person performed the database failover. The procedure book\n",
      "content_length": 2571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "15.2\nIndividual Training: Wheel of Misfortune\n311\n.\nwas updated as comments were made. Pre-checks were documented. A team\nmember pointed out that before she did the failover, she veriﬁed that the disks\nhad plenty of free disk space because previously that had been a problem. The\nrest of the team didn’t know about this pre-check, but now it could be added\nto the procedure book and everyone would know to do it.\nMore importantly, this realization raised an important issue: why wasn’t\nthe amount of free disk space always being monitored? What would hap-\npen if an emergency failover was needed and disk space was too low? A\nside project was spawned to monitor the system’s available disk space. Many\nsimilar issues were discovered and ﬁxed.\nEventually the process got more reliable and soon conﬁdence increased.\nThe team had one less source of stress.\n15.2 Individual Training: Wheel of Misfortune\nThere are many different ways a system can break. People oncall should be able to\nhandle the most common ones, but they need conﬁdence in their ability to do so.\nWe build conﬁdence by providing training in many different ways: documentation,\nmentoring, shadowing more experienced people, and practice.\nWheel of Misfortune is a game that operational teams play to prepare peo-\nple for oncall. It is a way to improve an individual’s knowledge of how to handle\noncall tasks and to share best practices. It can also be used to introduce new pro-\ncedures to the entire team. This game enables team members to maintain skills,\nlearn new skills as needed, and learn from each other. The game is played as\nfollows.\nThe entire team meets in a conference room. Each round involves one person\nvolunteering to be the contestant and another volunteering to be the Master of Dis-\naster (MoD). The MoD explains an oncall situation and the contestant explains how\nthey would work the issue. The MoD acts as the system, responding to any actions\ntaken by the contestant. The MoD can give hints about how to ﬁx the problem\nonly after sufﬁcient guessing or if the contestant has reached a dead end. Eventu-\nally either the contestant takes the issue to resolution or the MoD explains how it\nshould be solved.\nThe MoD usually begins by saying an alert has been received from the moni-\ntoring system and identifying what the message is. The contestant might respond\nby saying which dashboard or log should be checked. The MoD explains what is\nseen on the dashboard or in the logs. This back and forth continues as the contes-\ntant ﬁrst tries to determine what the problem is, then talks through the steps to\nﬁx the problem. It can help a lot if the MoD has prepared screenshots (real or fake)\nthat he or she can pull out and show the contestant, if/when the contestant asks the\n",
      "content_length": 2757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "312\nChapter 15\nDisaster Preparedness\nright questions. These might be screenshots from an actual incident or something\nprepared from scratch. Either way, they add a lot of realism to the exercise.\nSome teams have a “no laptops” rule when playing Wheel of Misfortune. Con-\ntestants aren’t allowed to use a laptop directly to check dashboards and such, but\nrather must describe what they want to look at or do to one of their teammates,\nwho drives a laptop and projects the result onto the shared screen for everybody\nelse to see. This forces contestants to be explicit about what they’re doing and why.\nIt serves to expose the contestants’ thought processes to the rest of the audience.\nIf the contestant is new, basic scenarios should be used: the most common\nalerts and the most typical solutions. More advanced contestants should get sce-\nnarios that involve more nuanced problems so that the audience can learn. The goal\nshould never be to trick the contestant or to provide a no-win, or Kobayashi Maru,\nsituation. The goal of the MoD is to educate workers, not to “win” by defeating the\nopponent.\nTo keep it fun, teams can add game show theatrics, starting the session by\nplaying a theme song or spinning a wheel to select the next situation or just for\neffect.\nNo score or points are tallied, because Wheel of Misfortune is purely a training\nexercise. The team leader or manager should observe the game as a way to identify\nwhich members should be given additional coaching. There is no need to embar-\nrass individuals; the suggestion that they get more coaching should be made in\nprivate. On the positive side, the game is also a way to identify which team mem-\nbers have strong troubleshooting skills that can be nurtured and perhaps used to\nhelp others develop their own.\nThe game should be played more frequently when the group is adding new\npeople or technology, and less frequently when things are static. By turning the\neducational process into a game, people have an incentive to want to play. The\ngame keeps the process fun and involves the entire team.\n15.3 Team Training: Fire Drills\nFire drills exercise a particular disaster preparedness process. In these situations\nactual failures are triggered to actively test both the technology and the people\ninvolved.\nThe key to building resilient systems is accepting that failure happens and\nmaking a commitment to being prepared to respond quickly and effectively to\nthose failures. An untested disaster recovery plan isn’t really a plan at all. Fire drills\nare processes where we preemptively trigger the failure, observe it, ﬁx it, and then\nrepeat until the process is perfected and the people involved are conﬁdent in their\nskills.\n",
      "content_length": 2696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "15.3\nTeam Training: Fire Drills\n313\nDrills work because they give us practice and ﬁnd bugs in procedures. It’s\nbetter to prepare for failures by causing them in production while you are watching\nthan to rely on a strategy of hoping the system will behave correctly when you\naren’t watching. Doing these drills in production does carry the risk that something\ncatastrophic will happen. However, what better time for a catastrophe than when\nthe entire engineering team is ready to respond?\nDrills build conﬁdence in the disaster recovery technology because bugs are\nfound and ﬁxed. The less often a failover mechanism is used, the less conﬁdence\nwe have in it. Imagine if a failover mechanism has not been triggered in more than a\nyear. We don’t know if seemingly unrelated changes in the environment have made\nthe process obsolete. It is unreasonable to expect it to work seamlessly. Ignorance\nis not bliss. Being unsure if a failover mechanism will work is a cause of stress.\nDrills build conﬁdence within the operations team. If a team is not accustomed\nto dealing with disaster, they are more likely to react too quickly, react poorly,\nor feel undue levels of pressure and anxiety. This reduces their ability to handle\nthe situation well. Drills give the team a chance to practice reacting calmly and\nconﬁdently.\nDrills build executive-level conﬁdence in their operations teams. While some\nexecutives would rather remain ignorant, smarter executives know that failures\nhappen and that a company that is prepared and rehearsed is the best defense.\nDrills can be done to gain conﬁdence in an individual process, in larger tests\ninvolving major systems, or even in larger tests that involve the entire company for\nmultiple days. Start small and work your way up to the biggest drills over time.\nTrying a large-scale exercise is futile and counterproductive, if you haven’t ﬁrst\nbuilt up capability and conﬁdence through a series of smaller but ever-growing\ndrills.\n15.3.1 Service Testing\nA very practical place to start is by picking individual failover mechanisms to\ntest. Identify failover mechanisms that are not exercised often and pick one. These\ntests should be contained such that if an issue is uncovered, only the immediate\nteam members are needed to respond. Typically these tests ﬁnd broken processes,\ndocumentation gaps, knowledge gaps, and conﬁguration problems.\nThe drill involves causing a failure that will trigger the failover mechanism.\nIt may involve powering off a machine (pulling the power cord), shutting down a\nvirtual machine, killing running software, disconnecting a network connection, or\nany other way one can disrupt service.\nOne strategy is to use drills to improve the process. If the process is unreliable\nor untested, it is worthwhile having the same people run the drill over and over,\nimproving the process documentation and software for each iteration. By having\n",
      "content_length": 2899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "314\nChapter 15\nDisaster Preparedness\nthe same people do each iteration, they will learn the system well enough to make\nbig improvements. They may do many iterations in one day, seeking to ﬁne-tune it.\nAlternatively, doing iterations further apart permits deeper problems to be ﬁxed.\nAnother strategy is to use drills to improve conﬁdence of the individual team\nmembers. Each iteration could be done by a different person until everyone has\nsuccessfully done the process, possibly more than once.\nA hybrid approach uses drills with the same people until the process is solid,\nthen uses an individual team member strategy to verify that everyone on the team\ncan do the process. Once this level of improvement is achieved, the drill is needed\nless frequently. One may choose to do the drill anytime the process has not been\ntriggered by a real incident for more than a certain amount of time, if the process\nchanges, or if new people join the team.\nPicking which day to do such a drill requires serious deliberation. It’s best to\ndo it on a day when no major sales demonstrations or new product releases are\nplanned. It should be a day when everyone is available, and probably not the day\npeople return from vacation or the day before people leave for vacation.\n.\nGoogle Chubby Outage Drills\nInside Google is a global lock service called “Chubby.” It has such an excellent\nreputation for reliability that other teams made the mistake of assuming it was\nperfect. A small outage created big problems for teams that had written code\nthat assumed Chubby could not fail.\nNot wanting to encourage bad coding practices, the Chubby team\ndecided that it would be best for the company to create intentional outages.\nIf a month passed without at least a few minutes of downtime, they would\nintentionally take Chubby down for ﬁve minutes. The outage schedule was\nannounced well in advance.\nThe ﬁrst planned outage was cancelled shortly before it was intended\nto begin. Many critical projects had reported that they would not be able to\nsurvive the test.\nTeams were given 30 days to ﬁx their code, but warned that there would\nbe no further delays. Now the Chubby team was taken seriously. The planned\noutages have happened ever since.\n15.3.2 Random Testing\nAnother strategy is to test a wide variety of potential failures. Rather than pick-\ning a speciﬁc failover process to improve, select random machines or other failure\n",
      "content_length": 2408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "15.4\nTraining for Organizations: Game Day/DiRT\n315\ndomains, cause them to fail, and verify the system is still running. This can be done\non a scheduled day or week, or in a continuous fashion.\nFor example, Netﬂix created many autonomous agents, each programmed to\ncreate different kinds of outages. Each agent is called a monkey. Together they form\nthe Netﬂix Simian Army.\nChaos Monkey terminates random virtual machines. It is programmed to\nselect different machines with different probabilities, and some machines opt\nout entirely. Each hour Chaos Monkey wakes up, picks a random machine, and\nterminates it.\nChaos Gorilla picks an entire datacenter and simulates either a network par-\ntition or a total failure. This causes massive damage, such that recovery requires\nsophisticated control systems to rebalance load. Therefore it is run manually as part\nof scheduled tests.\nThe Simian Army is always growing. Newer members include Latency Mon-\nkey, which induces artiﬁcial delays in API calls to simulate service degradation.\nAn extensive description of the Simian Army can be found in the article “The\nAntifragile Organization” (Tseitlin 2013).\nDrills like this carry a larger risk at the start because they span so many fail-\nure domains; consequently, they should be approved by management. Convincing\nmanagement of the value of this kind of test may be difﬁcult. Most managers want\nto avoid problems, not induce them. It is important to approach the topic from the\npoint of view of improving the ability to respond to problems that will inevitably\nhappen, and by highlighting that the best time to ﬁnd problems is in a controlled\nenvironment and not late at night when employees are asleep. Tie the process to\nbusiness goals involving overall service uptime. Doing so also improves the morale\nand conﬁdence of team members.\n15.4 Training for Organizations: Game Day/DiRT\nGame Day exercises are multi-day, organization-wide disaster preparedness tests.\nThey involve many teams, often including non-technical teams such as commu-\nnications, logistics, and ﬁnance. Game Day exercises focus on testing complex\nscenarios, trying out rarely tested interfaces between systems and teams, and\nidentifying unknown organizational dependencies.\nGame Day exercises may involve a multi-day outage of a datacenter, a com-\nplex week of network and system outages, or veriﬁcation that secondary coverage\npersonnel can successfully run a service if the primary team disappeared for an\nextended amount of time. Such exercises can also be used to rehearse for an\nupcoming event where additional load is expected and the team’s ability to handle\nlarge outages would be critical. For example, many weeks before the 2012 election\nday, the Obama campaign performed three all-day sessions where its election-day\n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "316\nChapter 15\nDisaster Preparedness\n“Get Out the Vote” operation was put to the test. This is described in detail in the\narticle “When the Nerds Go Marching in” (Madrigal 2012).\nBecause of the larger scale and scope, this kind of testing can have more impact\nand prevent larger outages. Of course, because of the larger scale and scope, this\nkind of testing also requires more planning, more infrastructure, and a higher level\nof management approval, buy-in, and support.\nThe organization needs to believe that the value realized through learning jus-\ntiﬁes the cost. Game Day exercises might be a sizable engineering effort involving\nhundreds of staff-days of effort. There is a potential for real accidental outages that\nresult in revenue loss.\nExecutives need to recognize that all systems will inevitably fail and that con-\nﬁdence is best gained though practice, not avoidance. They should understand\nthat it is best to have these failures happen in a controlled environment when key\npeople are awake. Learning that a failover system does not work at 4 when\nkey people are asleep or on vacation is the risk to avoid.\nGoogle’s Disaster Recovery Testing (DiRT) is this company’s form of Game\nDay exercises. DiRT is done at a very large scale and focuses on testing interactions\nbetween teams. Such interactions are generally less frequently exercised, but when\nthey are required it is due to larger disasters. These exercises are more likely to lead\nto customer-visible outages and loss of revenue than team-based service testing or\nrandom testing. For this reason teams should be doing well at their own particular\nﬁre drills before getting involved in DiRT.\n15.4.1 Getting Started\nWhile DiRT exercises can be very large, it is best to introduce a company to the\nconcept by starting small. A small project is easier to justify to management. A\nworking small example is easier to approve than a hypothetical big project, which,\nin all honesty, sounds pretty terrifying to someone who is unfamiliar with the con-\ncept. An executive should be very concerned if the people who are supposed to\nkeep their systems up start telling them about how much they want to take those\nsystems down.\nStarting small also means simpler tests. Google’s DiRT started with only a\nfew teams. Tests were safe and were engineered so they could not create any\nuser-visible disruptions. This was done even if it meant tests weren’t very useful.\nIt got teams used to the concept, reassured them that lessons learned would be\nused for constructive improvements to the system, and let them know that failures\nwould not become blame-fests or hunts for the guilty. It also permitted the DiRT\ntest coordinators to keep the processes simple and try out their methodology and\ntracking systems.\nGoogle’s original Site Reliability Engineering (SRE) teams were located in the\ncompany’s headquarters in Mountain View, California. Eventually Google added\n",
      "content_length": 2921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "15.4\nTraining for Organizations: Game Day/DiRT\n317\nSRE teams in Dublin, Ireland, to handle oncall at night-time. The ﬁrst DiRT exercise\nsimply tested whether the Dublin SREs could run the service for an entire week\nwithout help. Mountain View pretended to be unavailable. This exposed instances\nwhere Dublin SREs were depending on the fact that if they had questions or issues,\neventually a Mountain View SRE would wake up and be able to assist.\nAnother of Google’s early, simple tests was to try to work for one day without\naccess to the source code repository. This exercise found areas where production\nprocesses were dependent on a system not intended to be always available. In fact,\nmany tests involved verifying that production systems did not rely on systems\nthat were not built to be in the critical path of production systems. The larger an\norganization grows, the more likely that these dependencies will be found only\nthrough active testing.\n15.4.2 Increasing Scope\nOver time the tests can grow to include more teams. One can raise the bar for testing\nobjectives, including riskier tests, live tests, and the removal of low-value tests.\nToday, Google’s DiRT process is possibly the largest such exercise in the world.\nBy 2012 the number of teams involved had multiplied by 20, covering all SRE teams\nand nearly all services.\nGrowing the process to this size depended on creating a culture where identi-\nfying problems is considered a positive way to understand how the system can\nbe improved rather than a cause for alarm, blame, and ﬁnger-pointing. Some\noperations teams could not see the beneﬁt of testing beyond what their service\ndelivery platform’s continuous delivery system already provided. The best predic-\ntor of a team’s willingness to start participating was whether previous failures had\nresulted in a search for a root cause to be ﬁxed or a person to be blamed. Being able\nto point to earlier, smaller successes gave new teams and executive management\nconﬁdence in expanding the program.\nAn example complex test might involve simulating an earthquake or other\ndisaster that makes the company headquarters unavailable. Forbid anyone at head-\nquarters from talking to the rest of the company. Google DiRT did this and learned\nthat its remote sites could continue, but the approval chain for emergency pur-\nchases (such as fuel for backup generators) required the consent of people at\nthe company headquarters. Such key ﬁndings are non-technical. Another non-\ntechnical ﬁnding was that if all the tests leave people at headquarters with nothing\nto do, they will ﬂood the cafeteria, creating a DoS ﬂood of the food kind.\nCorporate emergency communications plans should also be tested. During\nmost outages people can communicate using the usual chat rooms and such. How-\never, an emergency communication mechanism is needed in the event of a total\nnetwork failure. The ﬁrst Google DiRT exercise found that exactly one person was\nable to ﬁnd the emergency communication plan and show up on the correct phone\n",
      "content_length": 3028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "318\nChapter 15\nDisaster Preparedness\nbridge. Now periodic ﬁre drills spot-check whether everyone has the correct infor-\nmation with them. In a follow-up drill, more than 100 people were able to ﬁnd and\nexecute the emergency communication plan. At that point, Google learned that the\nbridge supported only 40 callers. During another drill, one caller put the bridge\non hold, making the bridge unusable due to “hold music” ﬂooding the bridge. A\nrequirement to have the ability to kick someone off the bridge was identiﬁed. All of\nthese issues were discovered during simulated disasters. Had they been discovered\nduring a real emergency, it would have been a true disaster.\n15.4.3 Implementation and Logistics\nThere are two kinds of tests. Global tests involve major events such as taking down\na datacenter and are initiated by the event planners. Team tests are initiated by\nindividual teams.\nAn event may last multiple days or a single terrible day. Google schedules an\nentire week but ends DiRT sessions as soon as the tests’ goals have been satisﬁed.\nTo keep everyone on their toes, DiRT doesn’t start exactly at the announced start\ntime but rather after a small delay. The length of delay is kept secret.\nA large event has many moving parts to coordinate. The coordinator should\nbe someone with both technical and project management experience, who can ded-\nicate a sufﬁcient amount of time to the project. At a very large scale, coordination\nand planning may require a dedicated, full-time position even though the event\nhappens every 12 months. Much of the year will be spent planning and coordi-\nnating the test. The remaining months are spent reviewing outcomes and tracking\norganization-wide improvement.\nPlanning\nThe planning for the event begins many months ahead of time. Teams need time to\ndecide what should be tested, select proctors, and construct test scenarios. Proctors\nare responsible for designing and executing tests. Long before the big day, they\ndesign tests by documenting the goal of the test, a scenario, and a script that will\nbe followed. For example, the script might involve calling the oncall person for\na service and having that individual simulate a situation much like a Wheel of\nMisfortune exercise. Alternatively, the company may plan to actively take down\na system or service and observe the team’s reaction. During the actual test, the\nproctor is responsible for the tests execution.\nKnowing the event date as early as possible enables teams to schedule project\nwork and vacations. Teams may also use this time to do individual drills so that the\nevent can focus on tests that ﬁnd the gaps between teams. If the team’s individual\nprocesses are not well practiced, then DiRT itself will not go well.\nPrior to the ﬁrst Game Day at Amazon, John Allspaw conducted a series of\ncompany-wide brieﬁngs advising everyone of the upcoming test. He indicated it\n",
      "content_length": 2887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "15.4\nTraining for Organizations: Game Day/DiRT\n319\nwould be on the scale of destroying a complete datacenter. People did not know\nwhich datacenter, which inspired more comprehensive preparation (Robbins,\nKrishnan, Allspaw & Limoncelli 2012).\nRisk is mitigated by having all test plans be submitted in advance for review\nand approval by a cross-functional team of experts. This team checks for unrea-\nsonable risks. Tests never done before are riskier and should be done in a sand-\nbox environment or through simulation. Often it is known ahead of time that\ncertain systems are ill prepared and will not survive the outage. Pre-fail these\nsystems, mark them as failing the test, and do not involve them in the event.\nThere is nothing to be learned by involving them. These machines should be\nwhitelisted so they still receive service during the event. For example, if the out-\nage is simulated using network ﬁltering, these machines can be excluded from the\nﬁlter.\nOrganization\nTwo subteams are needed to make the event a success. The tech team designs the\nglobal tests and evaluates team test plans. The tests are graded on quality, impact,\nand risk. During the event this team is responsible for causing the global outages,\nmonitoring them, and making sure things don’t go awry. This team also handles\nunforeseen issues caused by the tests.\nThe coordination team is responsible for planning, scheduling, budgets, and\nexecution. It works with the tech team to prevent conﬂicting tests. Coordination\nteam members verify that all prep work is complete. They also are in charge of\ncommunication both with management and corporate-wide.\nDuring the event, both subteams reside in the command center, which oper-\nates as a kind of “Mission Control.” Due to the nature of the tests, the command\ncenter must be a physical location, as relying on virtual meeting spaces is too risky.\nFor large organizations, this involves a considerable amount of travel.\nIn the command center, individuals call proctors to tell them to start their tests.\nTest controllers deal with unexpected issues as they come up and communicate\nglobally about the status of the event. They monitor the progress of the tests as\nwell as the effects of the tests on the technical systems that should not be affected.\nTest controllers can halt all tests if something goes massively wrong.\nOne of the most valuable roles in the command center is the story teller. One\nperson concocts and narrates the disaster. He or she should create a story that is\njust unrealistic enough that people know that it is a test. For example, the story\nmight involve a zombie attack, an evil genius who has put everyone at head-\nquarters under hypnosis, or an errant fortune-teller with mystical powers. Daily\nupdates that move the story forward should be distributed by email or old-time\nradio “broadcasts.” The last bulletin should resolve the story and let everyone\nknow that the test is over, the zombies have been defeated, or whatever.\n",
      "content_length": 2981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "320\nChapter 15\nDisaster Preparedness\nThe story teller helps counteract the drudgery of the test. If this person does\na good job, people might actually look forward to next year’s event.\n.\nSuggested Reading\nFor many years DiRT and related practices were company secrets. However, in\n2012 a number of companies and practitioners revealed their disaster testing\nprocedures in a series of articles that were curated by Tom and published in\nACM Queue magazine:\n• “Resilience Engineering: Learning to Embrace Failure” is a group inter-\nview Tom conducted with Kripa Krishnan, the coordinator of Google’s\nDiRT program; Jesse Robbins, the architect of Game Day at Amazon; and\nJohn Allspaw, senior vice president of technological operations at Etsy\n(Robbins et al. 2012).\n• Krishnan details Google’s DiRT program in “Weathering the Unex-\npected” (Krishnan 2012).\n• Allspaw explained the theory and practice of Etsy’s program in “Fault\nInjection in Production” (Allspaw 2012b).\n• Tseitlin details the Netﬂix Simian Army and explains how it has improved\nresilience and maximized availability in “The Antifragile Organization”\n(Tseitlin 2013).\nLater Steven Levy was allowed to observe Google’s annual DiRT process ﬁrst-\nhand for an article he wrote for Wired magazine titled “Google Throws Open\nDoors to Its Top-Secret Data Center” (Levy 2012).\nAfter the 2012 U.S. presidential election, an article in The Atlantic maga-\nzine, “When the Nerds Go Marching in,” described the Game Day exercises\nconducted by the Obama for America campaign in preparation for election\nday 2012 (Madrigal 2012). Dylan Richard’s talk “Gamedays on the Obama\nCampaign” provided a ﬁrst-hand account and the disclaimer that technology\ndidn’t win the election, but it certainly could have lost it (Richard 2013).\n15.4.4 Experiencing a DiRT Test\nWhat follows is a ﬁctionalized account of a Google DiRT exercise as seen from the\nperspective of the engineers involved in the test. The names, locations, and situa-\ntions have been changed. This description is adapted from an article Tom wrote for\nACM Queue magazine titled “Google DiRT: The View from Someone Being Tested”\n(Limoncelli 2012).\n",
      "content_length": 2150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "15.4\nTraining for Organizations: Game Day/DiRT\n321\n[Phone rings]\nTom: Hello?\nRosanne: Hi, Tom. I’m proctoring a DiRT exercise. You are on call for [name of\nservice], right?\nTom: I am.\nRosanne: In this exercise we pretend the [name of service] database needs to be\nrestored from backups.\nTom: OK. Is this a live exercise?\nRosanne: No, just talk me through it.\nTom: Well, I’d follow the directions in our operational docs.\nRosanne: Can you ﬁnd the doc?\n[A couple of key clicks later]\nTom: Yes, I have it here.\nRosanne: OK, bring up a clone of the service and restore the database to it.\nOver the next few minutes, I make two discoveries. First, one of the commands\nin the document now requires additional parameters. Second, the temporary area\nused to do the restore does not have enough space. It had enough space when the\nprocedure was written, but the database has grown since then.\nRosanne ﬁles a bug report to request that the document be updated. She also\nﬁles a bug report to set up a process to prevent the disk-space situation from\nhappening.\nI check my email and see the notiﬁcations from our bug database. The noti-\nﬁcations are copied to me and the bugs are tagged as being part of DiRT2011.\nEverything with that tag will be watched by various parties to make sure they get\nattention over the next few months. I ﬁx the ﬁrst bug while waiting for the restore\nto complete.\nThe second bug will take more time. We’ll need to add the restore area to our\nquarterly resource estimation and allocation process. Plus, we’ll add some rules to\nour monitoring system to detect whether the database size is nearing the size of\nthe restore area.\nTom: OK, the service’s backup has been read. I’m running a clone of the service on\nit, and I’m sending you an instant message with a URL you can use to access it.\n[A couple of key clicks later]\nRosanne: OK, I can access the data. It looks good. Congrats!\nTom: Thanks!\nRosanne: Well, I’ll leave you to your work. Oh, and maybe I shouldn’t tell you this,\nbut the test controllers say at 2 there will be some fun.\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "322\nChapter 15\nDisaster Preparedness\nTom: You know my oncall shift ends at 3 , right? If you happen to be delayed an\nhour...\nRosanne: No such luck. I’m in California and 3 your time is when I’ll be leaving\nfor lunch.\nA minute after the exercise is over, I receive an email message with a link to a\npost-exercise document. I update it with what happened and provide links to the\nbugs that were ﬁled. I also think of a few other ways of improving the process and\ndocument them, ﬁling feature requests in our bug database for each of them.\nAt 2 my pager doesn’t go off, but I see on my dashboard that there is an\noutage in Georgia. Everyone in our internal chat room is talking about it. I’m not\ntoo concerned. Our service runs out of four datacenters around the world, and the\nsystem has automatically redirected web requests to the other three locations.\nThe transition is ﬂawless, losing only the queries that were “in ﬂight,” which\nis well within our SLA.\nA new email appears in my inbox explaining that zombies have invaded Geor-\ngia and are trying to eat the brains of the datacenter technicians there. The zombies\nhave severed the network connections to the datacenter. No network trafﬁc is going\nin or out. Lastly, the email points out that this is part of a DiRT exercise and no\nactual technicians have had their brains eaten, but the network connections really\nhave been disabled.\n[Again, phone rings]\nRosanne: Hi! Having fun yet?\nTom: I’m always having fun. But I guess you mean the Georgia outage?\nRosanne: Yup. Shame about those technicians.\nTom: Well, I know a lot of them and they have big brains. Those zombies will feed\nfor hours.\nRosanne: Is your service still within SLA?\nI look at my dashboard and see that with three datacenters doing the work\nnormally distributed to four locations the latency has increased slightly, but it is\nwithin SLA. The truth is that I don’t need to look at my dashboard because I would\nhave gotten paged if the latency was unacceptable (or growing at a rate that would\nreach an unacceptable level if left unchecked).\nTom: Everything is ﬁne.\nRosanne: Great, because I’m here to proctor another test.\nTom: Isn’t a horde of zombies enough?\nRosanne: Not in my book. You see, your SLA says that your service is supposed to\nbe able to survive two datacenter outages at the same time.\n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "15.5\nIncident Command System\n323\nShe is correct. Our company standard is to be able to survive two outages\nat the same time. The reason is simple. Datacenters and services need to be able\nto be taken down occasionally for planned maintenance. During this window of\ntime, another datacenter might go down for unplanned reasons such as a network\nor power outage. The ability to survive two simultaneous outages is called N + 2\nredundancy.\nTom: So what do you want me to do?\nRosanne: Pretend the datacenter in Europe is going down for scheduled preventive\nmaintenance.\nI follow our procedure and temporarily shut down the service in Europe.\nWeb trafﬁc from our European customers distributes itself over the remaining two\ndatacenters. Since this is an orderly shutdown, no queries are lost.\nTom: Done!\nRosanne: Are you within the SLA?\nI look at the dashboard and see that the latency has increased further. The\nentire service is running on the two smaller datacenters. Each of the two down\ndatacenters is bigger than the combined, smaller, working datacenters, yet there is\nenough capacity to handle this situation.\nTom: We’re just barely within the SLA.\nRosanne: Congrats. You pass. You may bring the service up in the European\ndatacenter.\nI decide to ﬁle a bug anyway. We stayed within the SLA, but it was too close\nfor comfort. Certainly we can do better.\nI look at my clock and see that it is almost 3 . I ﬁnish ﬁlling out the post-\nexercise document just as the next oncall person comes online. I send her an instant\nmessage to explain what she missed.\nI also remind her to keep her ofﬁce door locked. There’s no telling where the\nzombies might strike next.\n15.5 Incident Command System\nThe public safety arena uses the Incident Command System to manage outages. IT\noperations can adapt that process for handling operational outages. This idea was\nﬁrst popularized by Brent Chapman in his talk “Incident Command for IT: What\nWe Can Learn from the Fire Department” (Chapman 2005). Brent has extensive\nexperience in both IT operations and public safety.\n",
      "content_length": 2056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "324\nChapter 15\nDisaster Preparedness\nOutside the system administration arena, teams from different public safety\norganizations such as ﬁre, police, and paramedics come together to respond to\nemergencies and disasters. A system called the Incident Command System (ICS)\nhas been developed to allow this to happen productively in a way that can scale\nup or down as the situation changes.\nICS is designed to create a ﬂexible framework within which people can work\ntogether effectively. The key principles of ICS are as follows:\n• Standardized Organizational Structure: An ICS team is made up of the\nIncident Commander and subcommand systems: Operations, Logistics,\nPlanning, Admin/Finance, and optionally a Uniﬁed Command, a Public\nInformation Ofﬁcer, and a Liaison Ofﬁcer.\n• Unambiguous Definition of Who Is in Charge: There is one and only one\nIncident Commander. Each person on an ICS team reports to only one super-\nvisor for that ICS incident.\n• Explicit Delegations of Authority: The Incident Commander sets up certain\nkey branches, such as Logistics and Operations and Planning, and delegates\nthose functions to their commanders.\n• Management by Objective: Clear objectives and priorities are established for\nthe incident. Tell people what you want to get done, not how to do it; let them\nﬁgure out the best way to get it done in the current circumstances.\n• Limited Span of Control That Can Scale: Under ICS, a supervisor should\nnot have more than seven direct reports. Ideally, three to ﬁve individu-\nals should report to one supervisor. As the number of people involved\ngrows, the organization expands and new supervisors are created. Through\nthis approach, responsibilities stay limited. Remember that this is a tem-\nporary organizational structure created for responding to this speciﬁc\nevent. The same group of people might organize differently for a different\nevent.\n• Common Terminology and Organizational Framework: By using the ICS\nroles and responsibilities as a common framework, different teams from differ-\nent organizations can understand clearly who is doing what and where their\nresponsibilities are.\nA full description of ICS can be found on the U.S. Federal Emergency\nManagement Administration (FEMA) web site (http://www.fema.gov/incident-\ncommand-system). The FEMA Emergency Management Institute publishes free\nself-study and other training materials (http://training.fema.gov/EMI/).\nA more approachable introduction is the Wikipedia article on ICS (http://en.\nwikipedia.org/wiki/Incident_command_system).\n",
      "content_length": 2530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "15.5\nIncident Command System\n325\n15.5.1 How It Works: Public Safety Arena\nWhen a public safety incident begins, the ﬁrst order of business is to get orga-\nnized, starting by ﬁguring out who is going to be in charge. In an incident, the ﬁrst\nqualiﬁed responder to arrive automatically becomes the Incident Commander (IC).\nAn ICS team is made up of the Incident Commander and the subcom-\nmand systems of Operations, Logistics, Planning/Status, and Administration/\nFinance, as shown in Figure 15.1. A Public Information Ofﬁcer is also appointed\nto deal with communication beyond the response team (internal and exter-\nnal) about the incident. A Liaison Ofﬁcer is the primary contact for out-\nside agencies, such as third-party vendors. A Safety Ofﬁcer monitors safety\nconditions.\nAll incidents have a Command section (the management) and most have an\nOperations section, which directly provides emergency services such as putting\nout the ﬁre or providing medical attention. The management structure is a strict\nhierarchy. In an emergency there is no time for the vagaries and inefﬁciency of\nmatrix management.\nWhen the IC arrives, he or she is often alone and assumes all roles. As more\npeople arrive, the IC delegates roles as needed. As new people arrive, they check\nin with Admin/Finance (if it exists) for bookkeeping purposes, but then the IC\nassigns them to a function. The IC has the global view needed to best determine\nhow to allocate new resources.\nThe IC role does not transfer to anyone just because they have a higher rank or\nbecause a different department arrives. For example, a police chief does not auto-\nmatically trump a police ofﬁcer, and a ﬁre ﬁghter does not automatically trump a\nparamedic. Transfer of control is explicit, implemented by the current IC hand-\ning control to someone else who explicitly accepts the role. Role handoffs are\ndisruptive and are done only when needed.\nTeams stay small. A team size of three to ﬁve people is considered optimal. If\na subcommand gains seven direct reports, the team will be split in two when the\nnext person is added.\nFigure 15.1: The basic ICS organizational structure\n",
      "content_length": 2134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "326\nChapter 15\nDisaster Preparedness\n15.5.2 How It Works: IT Operations Arena\nWhen adapting this system to an IT operations organization, the Operations\nteam handles the operational aspects of the incident—the actual ﬁreﬁghting, as\nit were. The Logistics team handles resources, such as people and materials, and\nmakes sure that Operations has what it needs to do its job. The Planning team is\nresponsible for forecasting situation and resource needs, and for collecting and dis-\nplaying information about the incident. The Admin/Finance team handles general\nadministrative support and budgets.\nInitially the IC is whoever is ﬁrst on site. In IT, that is usually the oncall person\nwho responded to the alert. However, since this person has already been trouble-\nshooting the alert, it usually makes sense for him or her to sound the alarm and\nhandoff the IC role to the next qualiﬁed responder, continuing as the Operations\n(Ops) lead. The IC role is transferred through explicit handoff at shift change or\nif the IC becomes tired and needs relief. The key point here is that the IC role is\nalways handed off as part of a thoughtful process, not automatically as new people\nshow up. If a better-qualiﬁed person arrives on scene, the current IC may decide\nit’s worth the disruption of a handoff to switch ICs.\nFor small IT incidents, the IC handles all leadership roles on his or her own.\nHowever, as an incident grows larger, more people get involved, either because\nmore co-workers notice the outage or because the IC reaches out to them. As people\nshow up, the IC assigns them roles, creating subteams as needed. It’s worth noting\nthat the IC and the Ops lead should be made separate individual roles as quickly as\npossible. The IC has the big-picture view and the Ops lead is down in the trenches\ndealing with the incident directly. Trying to handle both roles simultaneously often\nresults in doing neither role well.\nOutages of long duration require frequent status updates to management and\nother stakeholders. By designating a single person to be the Public Information\nOfﬁcer, the IC can keep from being distracted by executives demanding updates\nor users asking, “Is it up yet?” Every status update should end by noting when the\nnext status can be expected. Many organizations standardize on update frequency.\nHourly updates balance executives’ need to know with technical workers’ need\nto focus on the technical issues at hand. Multi-day outages require less frequent\nupdates to avoid repetition.\n15.5.3 Incident Action Plan\nIn ICS, an Incident Action Plan (IAP) is created and continually reﬁned during the\nincident. This one- to two-page document answers four questions:\n• What do we want to do?\n• Who is responsible for doing it?\n",
      "content_length": 2742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "15.5\nIncident Command System\n327\n• How do we communicate with each other?\n• What is the procedure if someone becomes unavailable?\nIn IT operations, we add a ﬁfth question:\n• What is the procedure if additional failures are detected?\nCreate a template that the IC can use to produce this document. Make the template\naccessible in a standard location so that everyone knows where to ﬁnd it. An\nexample can be found in Figure 15.2.\n15.5.4 Best Practices\nTo assure success, everyone should get practice using ICS procedures and being\nthe Incident Commander, new hires should be introduced to ICS procedures as\npart of their onboarding process, and escalation partners should be informed that\nICS procedures are in use.\nTeams should use ICS procedures for minor incidents to stay in practice so\nthat everyone is comfortable with the process when major situations develop. ICS\nprocedures should be used for non-emergencies such as datacenter moves and\nmajor upgrades as well as for security incidents and adversarial terminations.\n.\nWhat do we want to do?\nThe main web server is down. We must restore it to full functionality.\nWho is responsible for doing it?\nThe IC is Mary. The SRE team is responsible, with help from developers\nRosanne, Eddie, and Frances.\nHow do we communicate with each other?\nVideo chat room:\nhttps://plus.google.com/hangouts/_/example.com/outage\nWhat is the procedure if someone becomes unavailable?\nIf possible, they should report to the IC for relief via phone (201-555-1212),\ntext message, instant message, or video chat room. If they are unable to, or\nif someone suspects that someone else has become unavailable, report this\ninformation to the IC.\nWhat is the procedure if additional failures are detected?\nThe IC will be responsible for leading the triage effort, assessing the situation,\nand deciding how to move forward.\nFigure 15.2: A sample Incident Action Plan (IAP)\n",
      "content_length": 1896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "328\nChapter 15\nDisaster Preparedness\nICS training should be part of an operation team’s new-hire onboarding pro-\ncess. It is important that everyone understand the terminology and processes so\nthat the system works well.\nPeople on a team should rotate through the role of Incident Commander to\nget an idea of the scope and capabilities of the team as it works together. Selecting\nthe IC in non-emergencies should be done by picking the person who has gone the\nlongest without being in the role.\nEscalation partners should be informed about your team’s use of ICS pro-\ncedures and how they will interact with the ICS roles, such as Communications\nOfﬁcer. If they use ICS procedures internally as well, so much the better. Partner\nteams may have adapted ICS procedures differently for use in IT. This can lead to\nconfusion and conﬂicts. Therefore multi-team incidents should be practiced to ﬁnd\nand ﬁx problems. Both simulated problems and real, but non-emergency issues can\nbe used. A good opportunity for this type of practice is when triaging a major bug\nthat involves both teams.\n15.5.5 ICS Example\nAs an example use of ICS procedures, suppose a major outage is affecting XYZ\nCompany’s web operations. Bob, the oncall responder to the outage, activates the\nICS process and is the temporary Incident Commander. The ﬁrst thing Bob does is\nnotify his team about the outage and pass on the role of Incident Commander to\nhis team lead, Janet. Janet starts assembling an ICS team immediately, delegating\nOperations to Bob. She notiﬁes her manager Sandy about the outage and asks him\nto be the Public Information Ofﬁcer.\nBob and Janet start an Incident Action Plan by describing what they believe is\ngoing on and what they are doing about it. Bob is taking action and Janet is coor-\ndinating efforts. Bob explains that he will need certain resources, such as failover\nmachines, and that the team is likely to end up working late on this one. Janet,\nas Incident Commander, now reaches out to ﬁnd someone to take on the Logis-\ntics role. She picks Peter and directs him to get the requirements from Bob. The\nnew Logistics Section Chief delegates people to get the machine resources that\nOperations will need, and arranges for pizza to arrive at 6 .\nJanet identiﬁes someone from the Monitoring group to ask about getting\ndetailed information on the outage. Jyoti agrees to be the Planning Section Chief\nand starts working with other members of the Monitoring group to gather the\nrequested information. They are working on forecasting demand, such as know-\ning that web services are currently coming off daily peak demand in Europe and\nclimbing in North America.\nAs the outage continues, Janet’s manager Sandy (Public Information Ofﬁcer)\ninterfaces with Janet (Incident Command), Bob (Operations), Jyoti (Planning), and\n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "15.6\nSummary\n329\nPeter (Logistics) to keep track of what is going on and notify people about the\nprogress of the outage. Sandy delegates a person on the team to update status\npages for XYZ Company’s customers as well as an internal status page within the\ncompany.\nMeanwhile, Bob in Operations has determined that a database needs to be\nfailed over and replicas updated. Managing by objective, he asks Logistics for\nanother resource to do the replica and load balancing work. Logistics ﬁnds some-\none else in the company’s IT staff who has experience with the replica system and\ngets permission for that person to help Bob during this outage. Bob begins the\ndatabase failover and his new helper begins work on the load balancing and replica\nwork needed.\nThe new replica completes its initial copy and begins serving requests. Janet\nconﬁrms with all ICS section chiefs that the service’s status has returned to normal.\nThe event is declared resolved and the ICS process is explicitly terminated. Janet\ntakes the action item to lead the postmortem effort.\n15.6 Summary\nTo handle major outages and disasters well, we must prepare and practice. Igno-\nrance may be bliss, but practice makes progress. It is better to learn that a disaster\nrecovery process is broken by testing it in a controlled environment than to be\nsurprised when it breaks during an actual emergency.\nTo be prepared at every level, a strategy of practicing disaster recovery tech-\nniques at the individual, team, and organization levels is required. Each level is\ndependent on the competency achieved in the previous level.\nWheel of Misfortune is a game that trains individuals by talking through com-\nmon, and not so common, disaster scenarios. Fire drills are live tests performed\nto exercise a particular process. Fire drills should ﬁrst be performed on a process\nagain and again by the same people until the process works and can be performed\nsmoothly. Then the process should be done by each member of the team until\neveryone is conﬁdent in his or her ability to perform the task.\nTests involving shutting down randomly selected machines or servers can ﬁnd\nuntested failure scenarios. These tests can be done at designated times as a test, or\ncontinuously as part of production to ensure that systems that should be resilient\nto failure have not regressed.\nGame Day or DiRT exercises are organization-wide tests that ﬁnd gaps in\nprocesses that involve multiple teams. They often last multiple days and involve\ncutting off major systems or datacenters.\nDiRT events require a large amount of planning and coordination to reduce\nrisk. Tests should be approved by a central planning committee based on quality,\n",
      "content_length": 2677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "330\nChapter 15\nDisaster Preparedness\nimpact, and risk. Some tests are initiated by individual teams, whereas tests with\ncompany-wide impact are planned by the central committee.\nFire, police, medical, and other public safety organizations use the Incident\nCommand System (ICS) to coordinate efforts during disasters and other emergency\nsituations. The ICS provides standardized organizational structure, processes, and\nterminology. This system can be adapted for use in coordinating recovery from\nmajor IT-related outages. To stay in practice, teams should use ICS procedures for\nsmaller incidents as well.\nExercises\n1. What is an antifragile system? How is this different from fragile systems and\nresilient systems?\n2. What are some reasons to do ﬁre drills as part of reliability testing?\n3. Draw a graph that represents the ideal frequency of failures as discussed in\nSection 15.1.1.\n4. Describe a critical subsystem in your own environment that could beneﬁt from\nreliability testing, and explain how you would test it.\n5. Compare and contrast the traditional attitude toward failure with the DevOps\nattitude toward failure. Which lessons can you apply to your own environ-\nment?\n6. Why not ﬁre people who make mistakes? Wouldn’t this result in an organiza-\ntion that employs only perfect people?\n7. Which situations have you been part of that might have beneﬁted from using\nthe Incident Command System? Which speciﬁc beneﬁts would have occurred?\n8. Describe how the ICS process works in emergency services and public safety.\n9. Describe how the ICS process could be applied to IT.\n10. List the key concepts that go into an Incident Action Plan.\n11. How would you attempt to convince your management of the necessity of\nconducting ﬁre-drill exercises in your environment?\n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "Chapter 16\nMonitoring Fundamentals\nYou can observe a lot\nby just watching.\n—Yogi Berra\nMonitoring is the primary way we gain visibility into the systems we run. It is the\nprocess of observing information about the state of things for use in both short-term\nand long-term decision making. The operational goal of monitoring is to detect the\nprecursors of outages so they can be ﬁxed before they become actual outages, to\ncollect information that aids decision making in the future, and to detect actual\noutages. Monitoring is difﬁcult. Organizations often monitor the wrong things and\nsometimes do not monitor the important things.\nThe ideal monitoring system makes the operations team omniscient and\nomnipresent. Considering that having the root password makes us omnipotent,\nwe are quite the omniarchs.\nDistributed systems are complex. Being omniscient, all knowing, means our\nmonitoring system should give us the visibility into the system to ﬁnd out any-\nthing we need to know to do our job. We may not know everything the monitoring\nsystem knows, but we can look it up when we need it. Distributed systems are too\nlarge for any one person to know everything that is happening.\nThe large size of distributed systems means we must be omnipresent, existing\neverywhere at the same time. Monitoring systems permit us to do this even when\nour systems are distributed around the world. In a traditional system one could\nimagine a system administrator who knows enough about the system to keep an\neye on all the critical components. Whether or not this perception is accurate, we\nknow that in distributed systems it is deﬁnitely not true.\nMonitoring in distributed computing is different from monitoring in enter-\nprise computing. Monitoring is not just a system that wakes you up at night when\na service or site is down. Ideally, that should never happen. Choosing a strategy\n331\n",
      "content_length": 1877,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "332\nChapter 16\nMonitoring Fundamentals\n.\nTerms to Know\nServer: Software running to provide a function or API. (Not a piece of\nhardware.)\nService: A user-visible system or product composed of many servers.\nMachine: A virtual or physical machine.\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nDiurnal Cycles: Metrics that are high during the day and low at night.\nthat involves reacting to outages means that we have selected an operational strat-\negy with outages “baked in.” We can improve how fast we respond to an outage\nbut the outage still happened. That’s no way to run a reliable system.\nInstead, monitoring should be designed to detect the precursors of an outage\nin time for the problem to be prevented. A system must be instrumented and mon-\nitored so as to enable this strategy. This is more difﬁcult than detecting an outage,\nbut much better.\n16.1 Overview\nTo understanding monitoring you must ﬁrst understand its particular terminology.\nA measurement refers to a single point of data describing an aspect of a sys-\ntem, usually a value on which numerical operations make sense—for example,\n5, −25, 0, or Null. It can also be a string—for example, a version number or a\ncomma-separated list of currently mounted ﬁle systems.\nA metric is a measurement with a name and timestamp. For example:\nspellcheck:server5.demo.com:request-count@20140214T100000Z = 9566\nDifferent monitoring systems use different formats to label the data. In the\npreceding example, we have a metric from the spellcheck server that was running\non server5. The measurement is the request-count, presumably how many requests\nthe server has received since starting. After the @ symbol is the date stamp. The\nmeasured value is 9566.\nExamples of other metrics include the total RAM in use, the number of out-\nstanding requests in a queue, the build version of the code being run, the number\nof open bugs, the number of bugs, and the total cost on Amazon AWS.\nThere is also metadata associated with a metric. In particular, a numeric\nvalue usually has a unit associated with it. Knowing the unit permits automatic\n",
      "content_length": 2128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "16.1\nOverview\n333\nconversions, chart labeling, and so on. Many monitoring systems do not track\nthe units of a metric, requring people to guess based on the metric name or\nother context, and perform conversions manually. This process is notably prone to\nerror.\nMeasurement frequency is the rate at which new measurements are taken.\nDifferent metrics are collected at different frequencies. Many pieces of data are\ncollected every 5 minutes, but some metrics are collected many times a second and\nothers once a day or once a week.\nMonitoring perspective is the location of the monitoring application collect-\ning the measurement. The importance of perspective depends on the metric. The\ntotal number of packets sent on an interface is the same no matter the perspec-\ntive. In contrast, page load times and other timing data depend on the perspec-\ntive. One might collect the same measurement from different places around the\nworld to see how page load time is affected by distance or to detect problems\nwith a particular country or ISP’s connectivity. Alternatively, the measurement\nmay be collected directly on a machine if perspective is less important than\nconsistency.\nOne way to gather information from many perspectives is real user monitor-\ning (RUM). RUM collects actual application performance measurements from the\nweb browser. Code is inserted into the web page that collects metrics and transmits\nit back to the site that sent the original web page.\n16.1.1 Uses of Monitoring\nMonitoring is not just about collecting data. It is also used in various ways, and\nthere is speciﬁc terminology for those uses.\nVisualization is the assimilation of multiple measurements into a visual rep-\nresentation. These charts and graphs make it possible to ﬁnd trends and make\ncomparisons between systems and time ranges.\nA trend is the direction of a series of measurements on a metric. For example,\none might use visualization to see that use of a service is growing or shrinking.\nAlerting means to bring something to the attention of a person or another\nsystem. A sudden drop in QPS might result in alerting the oncall person.\nAlerts generally need to be acknowledged within a certain amount of time.\nIf the deadline is exceeded, someone else is alerted. This process is called\nescalation.\nVisualization is useful for gaining deeper insights into the system. It can help\nwith everything from design to planning to communication. Trends can be used\nfor capacity planning, which is discussed in more detail in Chapter 18. Alerting\nshould be used to warn people about situations that could result in an outage. It is\nalso used as a last resort to warn when an outage has occurred.\n",
      "content_length": 2673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "334\nChapter 16\nMonitoring Fundamentals\n16.1.2 Service Management\nLastly, here are some service management terms we will use occasionally. They\nmostly come from the business world:\n• Service Level Indicator (SLI): An agreement as to how a measurement will be\nmeasured. For example, it might deﬁne what is measured, how it is measured,\nand from what perspective.\n• Service Level Target (SLT): A target quality of service; in other words, an SLI’s\nexpected minimum or maximum. Until ITIL V3, this was called the Service\nLevel Objective (SLO). An example SLT might be a certain level of availability\nor the maximum permitted latency.\n• Service Level Agreement (SLA): The contract that states the SLIs, SLTs, and\nthe penalties if the SLTs are not met. For example, there may be a refund or\npenalty payment for any outage longer than one hour. The term SLA is often\noverused to mean SLT or any service level requirement.\n16.2 Consumers of Monitoring Information\nThere are many consumers of monitoring information, each of which has differ-\nent needs. People who monitor operational health need to know state changes\nimmediately for fast response—they need real-time monitoring. Capacity plan-\nners, product managers, and others need metrics collected over a long period of\ntime to spot trends—they need historical monitoring data.\nA more ﬁne-grained way to differentiate consumers is the Dickson model\n(Dickson 2013), which uses three characteristics: resolution, latency, and diversity.\nThese characteristics can be rated as high or low.\nResolution describes how frequently the metric is collected. High (R+) is many\ntimes a second, minute, or hour. Low (R−) is many times a day.\nLatency describes how long a period of time passes before the information is\nacted upon. Low (L+) is real-time response. High (L−) means data is stored and\nanalyzed later, perhaps used for daily, weekly, or monthly statistics. (Note that the\nuse of + and −are reversed from R and D. Think of + as the case that is more\ndifﬁcult to engineer.)\nDiversity describes how many metrics are being collected. High (D+) means\nmany metrics, perhaps many measurements about many different services. Low\n(D−) means there is a focus on a particular or small set of metrics.\nConsumers can be described by a 3-tuple. For example, (R+, L−, D+) describes\na high-resolution, high-latency, high-diversity consumer.\n",
      "content_length": 2370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "16.2\nConsumers of Monitoring Information\n335\nGiven these axes, we can describe the primary users of monitoring information\nas follows:\nOperational Health/Response (OH)\n(R+, L+, D+)\nHigh resolution, low latency, high diversity.\nSystem health. The things we get paged about.\nQuality Assurance/SLA (QA)\n(R+, L−, D+)\nHigh resolution, high latency, high diversity.\nLonger-term analysis of jitter, latency, and\nother quality-related factors.\nCapacity Planning (CP)\n(R−, L−, D+)\nLow resolution, high latency, high diversity.\nForecasting and purchasing more resources.\nProduct Management (PM)\n(R−, L−, D−)\nLow resolution, high latency, low diversity.\nDetermining the number of users, cost, and other\nresources.\nOperational Health is typical monitoring, where exceptional situations are\ndetected and alerts are generated. It is the most demanding use case. The resolu-\ntion and latency must be sufﬁcient to detect problems and respond to them within\nan SLA. This usually demands up-to-date access to all metrics, real-time computa-\ntion for high-speed analysis, and reliable alerting. The storage system must be high\nspeed and high volume at the same time. In fact, this kind of monitoring stresses\nevery part of the monitoring infrastructure.\nQuality Assurance usually involves medium- or long-term analysis for spe-\nciﬁc quality metrics such as variability. For example, some queries should always\ntake approximately the same amount of time with little variation. Quality assur-\nance detects this kind of variability just as an auto assembly line quality assurance\nteam looks for defects and unacceptable variations in the product being built. For\nthis reason, Quality Assurance needs high-resolution data but latency is not critical\nsince the data is often processed in batches after the fact.\nQuality Assurance also includes information required when ﬁnding and ﬁx-\ning bugs, such as debug logs, process traces, stack traces, coredumps, and proﬁler\noutput.\nCapacityPlanning(CP)istheprocessofpredictingresourceneedsinthefuture.\nThese predictions require coarse metrics such as the current number of machines,\namount of network bandwidth used, cost per user, and machine utilization and efﬁ-\nciency, as well as alerting when resources arerunninglow. CP is also concerned with\nhow resource use changes as the product changes—for example, if a new release\nrequires signiﬁcantly different resources. CP is the topic of Chapter 18.\n",
      "content_length": 2421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "336\nChapter 16\nMonitoring Fundamentals\nProduct Management (PM) requires very low-resolution data for calculating\nkey performance indicators (KPIs) such as conversion rates, counts of users, and\nanalysis of user retention (often called 7-day actives or 30-day actives). PM beneﬁts\nfrom large amounts of historic data for long-term views and visualizations to help\nunderstand trends.\n.\nMissing (R, L, D) Combinations\nA keen observer will notice that not every combination of R, L, and D\nappears in the model. Some do describe other operations-related functions.\n(R+, L+, D−) is what a load balancer requires to determine if it should send\ntrafﬁc to a particular backend. (R+, L−, D−) covers the kind of log anal-\nysis that is done in periodically in batches. This leaves (R−, L+, D−) and\n(R−, L+, D+), which we have not observed in the wild…yet.\n16.3 What to Monitor\nWhat should be monitored is different for every organization. The general strategy\nis to start with the business’s KPIs and collect related measurements.\nThe following are some example KPIs:\n• Availability: Have a web site that is up 99.99 percent of the time (measured\nfrom outside the datacenter).\n• Latency: The 90th percentile latency for the homepage should not exceed\n400 ms (request to render time).\n• Urgent Bug Count: There should be no more than n outstanding Severity-0\nbugs.\n• Urgent Bug Resolution: All Severity-0 bugs should be closed within 48 hours.\n• Major Bug Resolution: All Severity-1 bugs should be closed within 10 days.\n• Backend Server Stability: There should be no more than n percent queries\nreturning HTTP 5xx Server Error.\n• User Satisfaction: There should be no more than n percent abandoned carts.\n• Cart Size: The median number of items in a shopping card per order should\nbe n.\n• Finance: A total of n revenue this month.\nIf you do not know your organization’s KPIs, stop right now and ﬁnd out what\nthey are. If there aren’t any, prepare your resume because your company is a\n",
      "content_length": 1973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "16.3\nWhat to Monitor\n337\nrudderless ship. Alternatively, you can declare yourself the captain and invent the\nKPIs yourself.\nYou need to instrument your system enough so that you can see when things\nare going to fail. To improve on KPIs, we must measure more than just the end\nresult. By measuring things that are deeper in the pipeline, we can make better\ndecisions about how to achieve our KPIs. For example, to improve availability,\nwe need to measure the availability of the component systems that result in the\nﬁnal availability statistic. There may be systems that get overloaded, queues that\ngrow too long, or bottlenecks that start to choke. There may be a search tree that\nworks best when the tree is balanced; by monitoring how balanced it is, we can\ncorrelate performance issues with when it becomes imbalanced. To determine why\nshopping carts are being abandoned, we must know if there are problems with any\nof the web pages during the shopping experience.\nAll of the previously mentioned KPIs can be monitored by selecting the right\nmetrics, sometimes in combination with others. For example, determining the 90th\npercentile requires some calculation. Calculating the average number of items per\norder might require two metrics, the total number of items and the total number\nof orders.\nA diagnostic is a metric collected to aid technical processes such as debug-\nging and performance tuning. These metrics are not necessarily related to a KPI.\nFor example, we might collect a metric that helps us debug an ongoing technical\nissue that is intermittent but difﬁcult to ﬁnd. There is generally a minimal set of\nmetrics one collects from all machines: system metrics related to CPU, network\nbandwidth, disk space, disk access, and so on. Being consistent makes manage-\nment easier. Hand-crafting a bespoke list for each machine is rarely a good use of\nyour time.\nIt is conventional wisdom in our industry to “monitor everything” in hopes\nof preventing the situation where you suddenly realize you wish you had historic\ndata on a particular metric. If this dictate is taken literally, the metrics collection\ncan overwhelm the systems being monitored or the monitoring system itself. Find\na balance by focusing on KPIs ﬁrst and diagnostics as needed.\n.\nThe Minimum Monitor Problem\nA common interview question is “If you could monitor only three aspects of a\nweb server, what would they be?” This is an excellent test of technical knowl-\nedge and logical thinking. It requires you to use your technical knowledge to\nﬁnd one metric that can proxy for many possible problems.\nFor example, much can be learned by performing an HTTPS GET: We\nlearn whether the server is up, if the service is overloaded, and if there is\n",
      "content_length": 2722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "338\nChapter 16\nMonitoring Fundamentals\n.\nnetwork congestion. TCP timings indicate time to ﬁrst byte and time to full\npayload. The SSL transaction can be analyzed to monitor SSL certiﬁcate valid-\nity and expiration. The other two metrics can be used to differentiate between\nthose issues. Knowing CPU utilization can help differentiate between network\ncongestion and an overloaded system. Monitoring the amount of free disk\nspace can indicate runaway processes, logs ﬁlling the disk, and many other\nproblems.\nWe recommend that you blow the interviewer away by offering to do\nall that while measuring one metric and one metric only. Assuming it is an\ne-commerce site, simply measure revenue. If it drops in a way that is unchar-\nacteristic for that time of day, the site is overloaded. If it stops, the site is down\n(and if it isn’t down, there is reason to investigate anyway). If it ramps up,\nwe know we’re going to run out of capacity soon. It is the one KPI that ties\neverything together.\n16.4 Retention\nRetention is how long collected metric data is stored. After the retention time has\nelapsed, the old metric data is expired, downsampled, or deleted from the storage\nsystem.\nHow long monitoring data is retained differs for each organization and ser-\nvice. Generally there is a desire or temptation to store all metrics forever. This\navoids the problem of suddenly realizing the data you want was deleted. It is also\nsimpler than having to decide on a storage time for each metric.\nUnfortunately, storing data forever has a cost—not just in terms of hardware\nfor storage, but also in terms of backups, power, and complexity. Store enough\ndata, and it must be split between multiple storage systems, which is complex.\nThere may also be legal issues around how long the data should be retained.\nCreating your retention policy should start with collecting business require-\nments and goals, and translating them into requirements for the storage system.\nTwo years is considered to be the minimum storage period because it enables\nyear-over-year comparisons. More is better. It is likely that your next monitoring\nsystem will be unable to read the data collected by the previous system, and a\nconversion process is unlikely to be available. In such a case, if you build a new\nmonitoring system every ﬁve or six years, that may be an upper bound. Time-\nseries databases are becoming more standardized and easier to convert, however,\nmaking this upper bound likely to disappear.\nHaving the ability to retain decades of monitoring data at full resolution has\nbeneﬁts we are just beginning to understand. For example, Google’s paper “Failure\nTrends in a Large Disk Drive Population” (Pinheiro, Weber & Barroso 2007) was\n",
      "content_length": 2717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "16.5\nMeta-monitoring\n339\nable to bust many myths about hard disk reliability because the authors had\naccess to high-resolution monitoring data from hundreds of thousands of hard\ndrives’ self-monitoring facility (SMART) collected over ﬁve years. Of course, not\neveryone has seemingly inﬁnite data storage facilities. Some kind of consolidation\nor compaction is needed.\nThe easiest consolidation is to simply delete data that is no longer needed.\nWhile originally many metrics might be collected, many of them will turn out to\nbe irrelevant or unnecessary. It is better to collect too much when setting up the\nsystem than to wish you had data that you didn’t collect. After you run the service\nfor a while, certain metrics may be deemed unnecessary or may be useful only in\nthe short term. For example, there may be speciﬁc CPU-related metrics that are\nuseful when debugging current issues but whose utility expires after a year.\nAnother way to reduce storage needs is through summarization, or down-\nsampling. With this technique, recent data is kept at full ﬁdelity but older data is\nreplaced by averages or other form of summarization. For example, metrics might\nbe collected at 1- or 5-minute intervals. When data is more than 13 months old,\nhourly averages, percentiles, maximums, and minimums are calculated and the\nraw data is deleted. When the data is even older, perhaps 25–37 months, 4-hour\nor even daily summaries are calculated, reducing the storage requirements even\nmore. Again, the amount of summarization one can do depends on business needs.\nIf you need to know only the approximate bandwidth utilization, daily values may\nbe sufﬁcient.\n16.5 Meta-monitoring\nMonitoring the monitoring system is called meta-monitoring. How do you know if\nthe reason you haven’t been alerted today is because everything is ﬁne or because\nthe monitoring system has failed? Meta-monitoring detects situations where the\nmonitoring system itself is the problem.\nThe monitoring system needs to be more available and scalable than the\nservices being monitored. Every monitoring system should have some kind of\nmeta-monitoring. Even the smallest system needs a simple check to make sure it is\nstill running. Larger systems should be monitored for the same scale and capacity\nissues as any other service to prevent disk space, CPU, and network capacity from\nbecoming limiting factors. The accuracy and precision of collected data should be\nmonitored; one should monitor how often an attempt to collect a measurement\nfails. The display of information must be faster than the average person’s attention\nspan. The freshness of the data used to calculate a KPI should be monitored; know-\ning the age of data used to calculate a KPI may be as important as the KPI itself.\nKnowing how KPI latency is trending is important to understanding the health of\nthe monitoring system.\n",
      "content_length": 2858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "340\nChapter 16\nMonitoring Fundamentals\nOne meta-monitoring technique is to deploy a second monitoring system that\nmonitors the primary system. It should have as few common dependencies as\npossible. If different software is used, it is less likely that the same bug will take\ndown both systems. This technique, however, adds complexity, requires additional\ntraining, and necessitates maintenance.\nAnother technique is to divide the network into two parts, each with its own\nmonitoring system. The two monitoring systems can also monitor each other. For\nexample, a site with two datacenters might deploy a different monitoring system in\neach one that monitors local machines. This saves on inter-datacenter bandwidth\nand removes the interconnection as a source of failure.\nWith more than one datacenter, a similar arrangement can be used, with\npairs of datacenters monitoring each other, or each monitoring system monitoring\nanother system in a big circle.\n16.6 Logs\nAnother way of gaining visibility into the system is through analysis of logging\ndata. While not directly related to monitoring, we mention this capability here\nbecause of the visibility it brings. There are many kind of logs:\n• Web “Hit” Logs: Web servers generally log each HTTP access along with\nstatistics about performance, where the access came from, and if the access\nwas a success, error, redirect, and so on. This data can be used for a multitude\nof business purposes: determining page generation times, analyzing where\nusers come from, tracking the paths users take through the system, and more.\nTechnical operations can use this data to analyze and improve page load time\nand latency.\n• API Logs: Logging each API call generally involves storing who made the\ncall, the input parameters, and output results (often summarized as a simple\nsuccess or error code). API logs can be useful for billing, security forensics,\nand feature usage patterns. A team that is going to eliminate certain obsolete\nAPI calls can use logs to determine which users will be affected, if any.\n• System Logs: The operating system kernel, devices, and system services con-\ntribute to the system log. This is often useful for tracking hardware problems\nand system changes.\n• Application Logs: Each service or server generates logs of actions. This is use-\nful for studying errors and debugging problems, as well as providing business\ninformation such as which features are used the most.\n• Application Debug Logs: Applications often generate debugging informa-\ntion in a separate log. This type of log is often more verbose but is retained\nfor a shorter amount of time. Such logs are used for debugging problems by\ndevelopers and operations staff.\n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "16.6\nLogs\n341\n16.6.1 Approach\nWhile logs do not directly ﬁt the Dickson model, if they did they would be\n(R+, L−, D−) because they are high resolution (generally one log entry per action),\nhigh latency (often processed much later in batches), and low diversity (usually\ncollected about a particular topic, such a web server hits).\nThe architecture of a log processing system is similar to the monitoring archi-\ntecture discussed in the next chapter. Generally logs are collected from machines\nand services and kept in a central place for storage and analysis. Retention is often\na legal issue, as logging data often contains personal information that is regulated.\nThe market space for log analysis tools has been growing over the years as\nnew analysis methods are invented. Web logs can be analyzed to determine the\npath a user takes through the system. As a result one can ﬁnd user interface\n“dead ends” that leave users bafﬂed. Application logs can be used to ﬁnd ﬂaws\nin sales processes or to identify high-value customers who otherwise would not\nhave been discovered. System logs can be analyzed to ﬁnd anomalies and even\npredict hardware failures.\nThe consolidation of logging data can be rather complex, as different systems\ngenerate logs in different formats. It’s best to establish a single log format for all\nsystems. By providing a software library that generates logs that conform to the\nformat, you can make the path of least resistance (using the library) be the path\nthat has the desired behavior (conforms to the standard).\n16.6.2 Timestamps\nRecord timestamps the same way in all systems to make it easier to combine and\ncompare logs. In particular, all machines should use NTP or the equivalent to keep\nclocks synchronized, and timestamps should be stored in UTC rather than local\ntime zones.\nThis is important because often logs from different systems are compared\nwhen debugging or ﬁguring out what went wrong after an outage. For exam-\nple, when writing a postmortem one builds a timeline of events by collecting\nlogs from various machines and services, including chat room transcripts, instant\nmessage sessions, and email discussions. If each of these services records time-\nstamps in its local time zone, just ﬁguring out the order of what happened can\nbe a challenge, especially if such systems do not indicate which time zone was\nbeing used.\nConsolidating logs on a large scale is generally automated and the consolida-\ntion process normalizes all logs to UTC. Unfortunately, conﬁguration mistakes can\nresult in logging data being normalized incorrectly, something that is not noticed\nuntil it is too late. This problem can be avoided by using UTC for everything.\nGoogle famously timestamps logs using the U.S./Paciﬁc time zone, which\ncaused no end of frustration for Tom when he worked there. This time zone has\n",
      "content_length": 2837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "342\nChapter 16\nMonitoring Fundamentals\na different daylight savings time calendar than Europe, making log normalization\nextra complex two weeks each year, depending on the year. It also means that\nsoftware must be written to understand that one day each year is missing an hour,\nand another day each year has an extra hour. Legend has it that the U.S./Paciﬁc\ntime zone is used simply because the ﬁrst Google systems administrator did not\nconsider the ramiﬁcations of his decision. The time zone is embedded so deeply in\nGoogle’s many systems that there is little hope it will ever change.\n16.7 Summary\nMonitoring is the primary way we gain visibility into the systems we run. It\nincludes real-time monitoring, which is used to alert us to exceptional situations\nthat need attention, and long-term or historic data collection, which facilitates trend\nanalysis. Distributed systems are complex and require extensive monitoring. No\none person can watch over the system unaided or be expected to intuit what is\ngoing on.\nThe goal of monitoring is to detect problems before they turn into outages,\nnot to detect outages. If we simply detect outages, then our operating process has\ndowntime “baked in.”\nA measurement is a data point. It refers to a single point of data describing an\naspect of a system, usually a numerical value or a string. A metric is a measurement\nwith a name and a timestamp.\nDeciding what to monitor should begin with a top-down process. Identify the\nbusiness’s key performance indicators (KPIs) and then determine which metrics\ncan be collected to create those KPIs.\nMonitoring is particularly important for distributed systems. By instrument-\ning systems and servers and automatically collecting the exposed metrics, we\ncan become the omniscient, omnipresent, omnipotent system administrators that\nstakeholders assume we are.\nExercises\n1. What is the goal of monitoring?\n2. Why is monitoring important in distributed systems?\n3. Why should operations staff be omniscient, omnipresent, and omnipotent?\nWhy are these important characteristics?\n4. How is a measurement different from a metric?\n5. If you could monitor only three things about a web site, what would they be?\nJustify your answer.\n",
      "content_length": 2211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "Exercises\n343\n6. Section 16.2 mentioned that there are no situations where (R−, L+, D−) and\n(R−, L+, D+) had been observed. Why might this be?\n7. Most monitoring systems “ping” devices to determine if they are up. This\nentire chapter never mentions ping. Based on what you learned in this chap-\nter, why is pinging devices an insufﬁcient or failed operational strategy for\nmonitoring?\n",
      "content_length": 385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "Chapter 17\nMonitoring Architecture and\nPractice\nWhat is a cynic? A man who\nknows the price of everything\nand the value of nothing.\n—Oscar Wilde\nThis chapter discusses the anatomy of a monitoring system. Best practices for each\ncomponent are described along the way.\nA monitoring system has many different parts. Dickson (2013) decomposes\nmonitoring systems into the functional components depicted in Figure 17.1. A mea-\nsurement ﬂows through a pipeline of steps. Each step receives its conﬁguration\nfrom the conﬁguration base and uses the storage system to read and write metrics\nand results.\nThe sensing and measurement system gathers the measurements. The col-\nlection system transports them to the storage system. From there, one or more\nanalysis systems extract meaning from the raw data—for example, detecting prob-\nlems such as a server being down or anomalies such as a metric for one system\nbeing signiﬁcantly dissimilar from all the others. The alerting and escalation sys-\ntem communicates these conditions to interested parties. Visualization systems\ndisplay the data for human interpretation. Each of these components is told how\nto do its job via information from the configuration base.\nOver the years there have been many monitoring products, both commercial\nand open source. It seems like each one has been very good at two or three compo-\nnents, but left much to be desired with the others. Some systems have done many\ncomponents well, but none has done them all well.\nInterfaces between the components are starting to become standardized in\nways that let us mix and match components. This enables faster innovation as new\nsystems can be created without having to reinvent all the parts.\n345\n",
      "content_length": 1709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "346\nChapter 17\nMonitoring Architecture and Practice\nFigure 17.1: The components of a monitoring system\nWhat follows is a deeper discussion of each component, its purpose, and\nfeatures that we’ve found to be the most useful.\n17.1 Sensing and Measurement\nThe sensing and measurement component gathers the measurements. Measure-\nments can be categorized as blackbox or whitebox, depending on the amount of\nknowledge of the internals that is used. Measurements can be direct or synthesized,\ndepending on whether each item is separately counted, or totals are periodically\nretrieved and averaged. We can monitor the rate at which a particular operation is\nhappening, or the capability of the system to allow that operation to happen. Sys-\ntems may be instrumented to provide gauges, such as percentage CPU utilization,\nor counters, such as the number of times that something has occurred.\nLet’s look at each of those in more detail.\n17.1.1 Blackbox versus Whitebox Monitoring\nBlackbox monitoring means that measurements try to emulate a user. They treat\nthe system as a blackbox, whose contents are unknown. Users do not know how\na system’s internals work and can only examine the external properties of the\n",
      "content_length": 1203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "17.1\nSensing and Measurement\n347\nsystem. They may guess about the internals, but they cannot be sure. In other\nwords, these measurements are done at a high level of abstraction.\nDoing an HTTPS GET of a web site’s main page is an example of blackbox\nmonitoring. The measurement is unaware of any load balancing infrastructure,\ninternal server architecture, or which technologies are in use. Nevertheless, from\nthis one measurement, we can determine many things: whether the site is up, how\nfast is it responding, if the SSL certiﬁcate has expired, if the system is producing\na valid HTML document, and so on. Blackbox testing includes monitoring that\nattempts to do multi-step processes as a user, such as verifying that the purchasing\nprocess is working.\nIt is tempting to get a particular web page and verify that the HTML received\nis correct. However, then the monitoring system must be updated anytime the web\nsite changes. An alternative is to verify that the page contains a particular message.\nFor example, a search engine might periodically query for the name of a particular\ncelebrity and verify that the name of the person’s fan club is mentioned in the\nHTML. The measurement communicated to the monitoring system would be 1\nor 0, representing whether the string was or wasn’t found.\nBlackbox monitoring is often highly dependent on perspective. Monitoring\nmight be done from the machine itself, from within the datacenter or other failure\ndomain, or from different places around the world. Each of these tests answers\ndifferent questions about the system.\nA whitebox measurement has the beneﬁt of internal knowledge because it\nis a lower level of abstraction. For example, such a measure might monitor raw\ncounters of the number of times a particular API call was made, the number of\noutstanding items waiting in a queue, or latency information of internal processes.\nSuch measurements may disappear or change meaning as the system’s internals\nchange. In whitebox monitoring, perspective is less important. Usually such mea-\nsurements are done as directly as possible to be more efﬁcient and to prevent data\nloss that could occur by adding distance.\n17.1.2 Direct versus Synthesized Measurements\nSome measurements are direct, whereas others are synthesized. For example, if\nevery time a purchase is made the system sends a metric of the total money col-\nlected to the monitoring system, this is a direct measurement. Alternatively, if\nevery 5 minutes the monitoring system tallies the total money collected so far,\nthis is a synthesized metric. If all we have is a synthesized metric, we cannot\nback-calculate the individual amounts that created it.\nWhether direct measurements are available is generally a function of fre-\nquency of change. In an e-commerce site where one or two purchases are made\neach day, direct measurement is possible. By comparison, for a busy e-commerce\nsite such as eBay or Amazon, the monitoring system could not keep up with the\n",
      "content_length": 2967,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "348\nChapter 17\nMonitoring Architecture and Practice\nnumber of purchases made if each one resulted in a metric. In this case the synthe-\nsized metric of total revenue collected is all that is possible. Exact counts would be\nthe job of the accounting and billing system.\nA more obvious example is measuring how much disk space is in use versus\nbeing told about every block allocation and deallocation. There can be millions of\ndisk operations each minute. Collecting the details of each operation just to arrive\nat a metric of how much disk space is in use would be a waste of effort.\n17.1.3 Rate versus Capability Monitoring\nEvent frequency determines what to monitor. One wants to know that a customer\ncan make purchases and is making purchases. The rate at which purchases hap-\npen determines which of these to monitor. If one or two purchases are made each\nday, monitoring whether customers can make purchases is important because it\nhappens so infrequently that we need to verify that the purchasing ﬂow hasn’t\nsomehow broken. If thousands of purchases per minute are made, then we gener-\nally know purchases can be made and it is more important to measure the rate and\ntrends.\nIn short, rate metrics are more important when event frequency is high and\nthere are smooth, predictable trends. When there is low event frequency or an\nuneven rate, capability metrics are more important.\nThat said, never collect rates directly. Rates are lossy. Given two counts and\ntheir time delta, we can compute the rate. Given two rates and their time delta, we\ncan only surmise the count—and then only if the two measurements were collected\nat precise intervals that are in sync with the rate’s denominator.\nFor example, if we are measuring the count of how many API calls have been\nreceived, we might collect a metric of 10,000 at 12:10:00 and 16,000 at 12:15:00. We\ncould then calculate the rate by dividing the delta of the counts by the time delta:\n(16, 000−10, 000)/300,or20APIcallspersecond.Supposethatsecondpollingevent\nwas skipped, perhaps due to a brief network problem, the next polling event would\nbe at 12:20:00 and might collect the measurement of 21,000. We can still calculate\nthe rate: (21, 000 −10, 000)/600, or 18.2 API calls per second. However, if we were\ncollecting rates, that network blip would mean we would not be able to estimate\nwhat happened during the missing 5 minutes. We could take the average of the two\nadjacent rates, but if during that time there was a large spike, we would not know.\n17.1.4 Gauges versus Counters\nSome measurements are gauges, while others are counters.\nA gauge value is an amount that varies. It is analogous to a real-world gauge\nlike one that indicates barometric pressure. Examples include an indicator of how\nmuch unused disk space is available, a temperature reading, and the number\n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "17.1\nSensing and Measurement\n349\nof active processes on a system. A gauge’s value varies, going up and down as\nwhatever it measures increases or decreases.\n.\nHigh-Speed Network Counters\nThe frequency and potential rate of change should determine the size of the\ncounter. If the counter that stores the number of bits transmitted on a 10 Gbps\ninterface is 32 bits long, it will roll over to zero after 4,294,967,296 bits have\nbeen transmitted. If the interface is running at about 50 percent capacity, the\ncounter will roll over about every 8.5 seconds. It is not possible to correctly\nsynthesize the metric for trafﬁc across this interface unless you collect data\nfaster than the counter rolls over. Therefore a counter that changes this quickly\nrequires a 64-bit counter.\nA counter is a measurement that only increases—for example, a count of the\nnumber of API calls received by a service or the count of the number of packets\ntransmitted on a network interface. A counter does not decrease or run backward.\nYou can’t unsend a packet or deny that an API call was received after the fact.\nHowever, counters do reset to zero in two circumstances. Unless a counter’s value\nis in persistent storage, the value is reset to zero on reboot or restart. Also, like an\nautomobile odometer that rolls over at 99,999, a counter loops back to zero when\nit reaches its maximum value. Typically counters are 16-, 32-, or 64-bit integers\nthat have maximum values of 65,535, 4,294,967,295, and 18,446,744,073,709,551,615,\nrespectively (i.e., 2n −1, where n is the number of bits).\nDoing math on counter readings is rather complex. In our previous example,\nwe simply subtracted adjacent counter values to learn the delta. However, if the\nsecond reading is less than the ﬁrst, special care must be taken.\nIf the counter looped around to zero, the actual delta between two read-\nings includes the count up to the maximum value plus the newer counter value:\n(maxvalue −Rn) + Rn+1. However, if the counter was reset to zero due to a restart,\nthen the exact delta value is unobtainable because we do not know the value of\nthe counter when the restart occurred. We can make estimates using a little high\nschool calculus involving the past rate and how long ago the reset occured.\nDetermining whether the counter reset to zero due to a restart or reaching\nthe maximum value is another heuristic. If the last reading was near the maxi-\nmum value, we can assume the counter looped around to zero. We can improve\nthe accuracy of this guess, again by applying a little calculus.\nSuch great complexity still results in a margin of error and risks introducing\na loss of accuracy if either heuristic produces incorrect results. A much simpler\napproach is to ignore the problem. For 64-bit counters, wrap-arounds are rare or\n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "350\nChapter 17\nMonitoring Architecture and Practice\nnonexistent. For counter resets, if the monitoring frequency is fast enough, the mar-\ngin of error will be very small. Usually the desired end result is not the count itself,\nbut a rate. The calculation of the rate for the entire interval will not be dramatically\naffected. For example, if there are 10 counter readings, 9 deltas are produced. If\na delta is negative, it indicates there was a counter reset. Simply throw away any\nnegative deltas and calculate the rate with the remaining data.\n.\nJava Counters\nSystems written in the Java programming language use counters that are\nsigned integers. Signed integers roll over to negative numbers and have a\nmaximum value that is approximately half their signed counterparts.\n17.2 Collection\nOnce we have a measurement, we must transmit it to the storage system. Met-\nrics are collected from many places and brought to a storage system so that they\nmay be analyzed. The metric’s identity must be preserved through any and all\ntransmissions.\nThere are hundreds of ways to collect metrics. Most fall into one of two camps:\npush or pull. Irrespective of the choice of a push or pull mechanism, there is also\na choice of which protocols are used for data collection. Another aspect of data\ncollection is whether the server itself that communicates directly with the collector,\nor whether an external agent acts as an intermediary between the server and the\ncollector. A monitoring system may have a central collector or regional collectors\nthat consolidate data before passing it back to a central system.\n17.2.1 Push versus Pull\nPush means the sensor that took the measurement transmits it to the collection\nmechanism. Pull means an agent polls the object being monitored and requests\nthe data and stores it.\nFour myths in monitoring are that pull doesn’t scale, that push is horrible and\nshouldn’t be used, that pull is horrible and shouldn’t be used, and that push doesn’t\nscale. Dispelling these myths is easy.\nThe ability to scale is a matter of implementation, not push versus pull. Moni-\ntoring 10,000 items at least once every 5 minutes requires 33 connections per second\nif spread evenly. A web server can handle millions of inbound TCP connections.\nOutbound connections are no different. A monitoring system has the beneﬁt that\n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "17.2\nCollection\n351\nit can make certain optimizations. It can transmit multiple measurements over\neach TCP connection. It can simply create a long-lived TCP connection for every\nmachine. Scaling is a concern, but not an insurmountable obstacle.\nWhether push or pull is better is a matter of application. Pull is better for syn-\nthesized measurements, such as counters. It is also better for direct measurements\nwhen change is infrequent. For example, if a change happens a few times per hour,\nbeing polled every 5 minutes may be sufﬁcient resolution. Pull is also better if it\nwould be useful to have a variable measurement rate, such as a system that polls\nat a higher frequency when diagnosing a problem. This approach is analogous to\nincreasing the magniﬁcation on a microscope.\nPush is better when direct observation is required, such as when we need to\nknow when something happens rather than how many times it has happened. In\nthe event of a failure, it is easier for a push system to store measurements and\ntransmit the backlog when possible. If it is useful to observe each discrete or speciﬁc\naction, then push is more appropriate.\n17.2.2 Protocol Selection\nMany different protocols are used in monitoring. The Simple Network Manage-\nment Protocol (SNMP) is horrible and should be avoided. Sadly, if you deal with\nnetwork equipment, it is likely your only choice. If you manage services and\nmachines, there are many alternatives.\nMost of the alternatives are migrating to using JSON transmitted over HTTP.\nJSON is a text-based standard for human-readable data interchange. HTTP PUT\nis used for push and HTTP GET is used for pull. Because the same JSON data\nstructure is sent in either case, it simpliﬁes the processing of the data.\n.\nSNMP: Simply Not a Management Protocol\nWe dislike SNMP. Most implementations transmit passwords in clear text and\nare inefﬁcient because they transmit one measurement per transaction. Ver-\nsion 3 of the protocol can be secure and fast but few vendors implement it.\nThe protocol is complex enough that implementations are a constant source\nof security-related bugs. The adjective “simple” was intended to describe the\nprotocol, which makes us concerned what the complex protocol, if it had been\ninvented, would look like. Because it scales so badly, we wonder whether\n“simple” actually describes the kind of network it is good for monitoring.\nThe use of the word “management” in its name has hurt the industry by cre-\nating the misperception that managing a network is a matter of collecting\nmeasurements.\n",
      "content_length": 2548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "352\nChapter 17\nMonitoring Architecture and Practice\n17.2.3 Server Component versus Agent versus Poller\nCollectors can work in several different ways. The cleanest way to implement\nwhitebox monitoring is to have the running server gather its own measure-\nments and handle pushing or pulling the results to the collector. The soft-\nware that provides these functions can be put into a library for convenient\nuse by any program. For example, every time an API call is received, the\nsoftware might call metric.increment('api-calls-count') to increment the\ncount called api-calls-count. To collect how much bandwidth is being used\nin a video server, after each write to the network the software might call\nmetric.add('bandwidth-out', x), where x is the amount of data sent. The library\nwould maintain a running total under the name bandwidth-out. Functions like this\nare called throughout the program to count, total, or record measurements.\nIf the collection is done by push, the library spawns a thread that wakes up\nperiodically to push the metrics to the monitoring system. Pull is implemented by\nthe library spawning a thread that listens to a speciﬁc TCP port for polling events\nand replies with some or all of the metrics. If the server already processes HTTP\nrequests, it can simply attach the library to a particular path or route. For example,\nHTTP requests for /monitor would call the library, which would then reply to the\nrequest with a JSON representation of the metrics.\nSadly, we can’t always modify software to collect metrics this way. In that case\nwe run software on the machine that collects data by reading logs, status APIs, or\nsystem parameters and pulls or pushes the information. Such software is called an\nagent. For example, scollector is a software agent that runs on a Linux machine\nand calls any number of plug-ins to gather metrics, which are then pushed to the\nBosun monitoring system. Plug-ins are available that collect performance infor-\nmation about the operating system, disk systems, and many popular open source\nsystems such as MySQL and HBase.\nSometimes we cannot modify software or install software on the machine\nitself. For example, hardware devices such as network switches, storage area\nnetwork (SAN) hardware, and uninterruptible power supplies (UPSs) do not\npermit user-installed software. In these cases software is run on one machine\nthat polls the other devices, collects the information, and transmits it via push\nor pull to the monitoring system. This type of software is called a poller or a\nmonitoring proxy.\n17.2.4 Central versus Regional Collectors\nSome monitoring systems scale to global size by having a collector run in each\nregion and relay the data collected to the main monitoring system. This type of\ncollector is called a remote monitoring station or aggregator. An aggregator might\nbe placed in each datacenter or geographical region. It may receive metrics by push\nor pull, and it generally consolidates the information and transmits it to the main\n",
      "content_length": 3010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "17.3\nAnalysis and Computation\n353\nsystem in a more efﬁcient manner. This approach may be used to scale a system\nglobally, saving bandwidth between each datacenter. Alternatively, it may be done\nto scale a system up; each aggregator may be able to handle a certain number of\ndevices.\n17.3 Analysis and Computation\nOnce the data has been collected, it can be used and interpreted. Analysis extracts\nmeaning from raw data. Analysis is the most important component because it\nproduces the results that justify having a monitoring system in the ﬁrst place.\nReal-time analysis examines the data as it is collected. It is generally the\nmost computationally expensive analysis and is reserved for critical tasks such as\ndetermining conditions where someone should be alerted. To do this efﬁciently,\nmonitoring systems tee the data as it is collected and send one copy to the storage\nsystem and another to the real-time analysis system. Alternatively, storage systems\nmay hold copies of recently collected metrics in RAM. For example, by keeping the\nlast hour’s worth of metrics in RAM and constructing all alerting rules to refer to\nonly the last hour of history, hundreds or thousands of alert rules can be processed\nefﬁciently.\nTypically real-time analysis involves dozens or hundreds of alert rules that are\nsimultaneously processed to ﬁnd exceptional situations, called triggers. Sample\ntriggers include if a service is down, if HTTP responses from a server exceed n\nms for x minutes, or if the amount of free disk space drops below m gigabytes.\nThe real-time analysis includes a language for writing formulas that describe these\nsituations.\nThis analysis may also detect situations that are not so critical as to require\nimmediate attention, but if left unattended could create a more signiﬁcant prob-\nlem. Such situations should generate tickets rather than alerts. See Section 14.1.7\nfor problem classiﬁcations. Alerts should be reserved for problems that do require\nimmediate attention. When an alert is triggered, it prompts the alerting and\nescalation manager, described later in this chapter, to take action.\nShort-term analysis examines data that was collected in the last day, week, or\nmonth. Generally dashboards ﬁt into this category. They are updated infrequently,\noften every few minutes or on demand when someone calls up the speciﬁc web\npage. Short-term analysis usually queries the on-disk copy of the stored metrics.\nNear-term analysis is also used to generate tickets for problems that are not so\nurgent as to require immediate attention. See Section 14.1.7.\nDashboard systems generally include a template language that generates\nHTML pages and a language for describing data graphs. The data graph descrip-\ntions are encoded in URLs so they may be included as embedded images in the\nHTML pages. For example, one URL might specify a graph that compares the ratio\nof two metrics for the last month for a particular service. It may specify a histogram\n",
      "content_length": 2964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "354\nChapter 17\nMonitoring Architecture and Practice\nof latency for the 10 slowest web servers, after calculating latency for hundreds of\nweb servers.\nLong-term analysis generally examines data collected over large spans of time,\noften the entire history of a metric, to produce trend data. In many cases, this\ninvolves generating and storing summaries of data (averages, aggregates, and so\non) so that navigating the data can be done quickly, although at low resolution.\nBecause this type of analysis requires a large amount of processing, the results are\nusually stored permanently rather than regenerated as needed. Some systems also\nhandle situations where old data is stored on different media—for example, tape.\nAnomaly detection is the determination that a speciﬁc measurement is not\nwithin expectations. For example, one might examine all web servers of the same\ntype and detect if one is generating metrics that are signiﬁcantly different from the\nothers. This could imply that the one server is having difﬁculties that others are\nnot. Anomaly detection ﬁnds problems that you didn’t think to monitor for.\nAnomaly detection can also be predictive. Mathematical models can be cre-\nated that use last year’s data to predict what should be happening this year. One\ncan then detect when this year’s data deviates signiﬁcantly from the prediction. For\nexample, if you can predict how many QPS are expected from each country, iden-\ntifying a deviation of more than 10 percent from the prediction might be a good\nway to detect regional outages or just that an entire South American country stops\nto watch a particular sporting event.\nDoing anomaly detection in real time and across many systems can be com-\nputationally difﬁcult but systems for doing this are becoming more commonplace.\n17.4 Alerting and Escalation Manager\nThe alerting and escalation component manages the process of communicating to\noncall and other people when exceptional situations are detected. If the person can-\nnot be reached in a certain amount of time, this system attempts to contact others.\nSection 14.1.7 discusses alerting strategy and various communication technologies.\nThe ﬁrst job of the alerting component is to get the attention of the person\noncall, or his or her substitute. The next job is to communicate speciﬁc information.\nThe former is usually done by pager or text message. Since these systems permit\nonly short messages to be sent, there is usually a second method, such as email, to\ncommunicate the complete message.\nThe message should communicate the following information:\n• Failure Condition: A description of what is wrong in technical terms but in\nplain English. For example, “QPS too high on service XYZ” is clear. “Error 42”\nis not.\n",
      "content_length": 2739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "17.4\nAlerting and Escalation Manager\n355\n• Business Impact: The size and scope of the issue—for example, how many\nmachines or users this affects, and whether service is reduced or completely\nunavailable.\n• Escalation Chain: The escalation chain is who to contact, and who to contact\nif that person does not respond. Generally, one or two chains are deﬁned for\neach service or group of services.\n• Suggested Resolution: Concise instructions of what to do to resolve this issue.\nThis is best done with a link to the playbook entry related to this alert, as\ndescribed in Section 14.2.5.\nThe last two items may be difﬁcult to write at the time the alert rule is created. The\nspeciﬁc business impact may not be known, but at least you’ll know which service\nis affected, so that information can be used as a placeholder. When the alert rule is\ntriggered, the speciﬁcs will become clear. Take time to record your thoughts so as\nto not lose this critical information.\nUpdate the impact and resolution as part of the postmortem exercise. Bring all\nthe stakeholders together. Ask the affected stakeholders to explain how their busi-\nness was impacted in their own business terms. Ask the operational stakeholders\nto evaluate the steps taken, including what went well and what could have been\nimproved. Compare this information with what is in the playbook and update it\nas necessary.\n17.4.1 Alerting, Escalation, and Acknowledgments\nThe alert system is responsible for delivering the alert to to the right person and\nescalating to others if they do not respond. As described in Section 14.1.5, this\ninformation is encoded in the oncall calendar.\nIn most cases, the workﬂow involves communicating to the primary oncall\nperson or people. They acknowledge the alert by replying to the text message with\nthe word “ACK” or “YES,” clicking on a link, or other means. If there is no acknowl-\nedgment after a certain amount of time, the next person on the escalation list is\ntried.\nHaving the ability to negatively acknowledge (“NAK”) the alert saves time\nduring escalations. A NAK immediately escalates to the next person on the list. For\nexample, if the oncall person receives the alert but is unable to attend to the issue\nbecause his or her Internet connection has died, the individual could simply do\nnothing and in a few minutes the escalation will happen automatically. However,\nif the escalation schedule involves paging the oncall person every 5 minutes and\nnot escalating until three attempts have been made, this means delaying action\nfor 15 minutes. In this case, the person can NAK and the escalation will happen\nimmediately.\n",
      "content_length": 2621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "356\nChapter 17\nMonitoring Architecture and Practice\nInevitably, there are alert ﬂoods or “pager storms”—situations where dozens\nor hundreds of alerts are sent at the same time. This is usually due to one network\noutage that causes many alert rules to trigger. In most cases, there is a mechanism\nto suppress dependent alerts automatically, but ﬂoods may still happen despite the\norganization’s best efforts. For this reason, an alert system should have the ability\nto acknowledge all alerts at the same time. For example, by replying to the text\nmessage with the word “ALL” or “STFU,” all pending alerts for that particular\nperson are acknowledged as well as any alerts received in the next 5 minutes. The\nactual alerts can be seen at the alerting dashboard.\nSome alert managers have a two-stage acknowledgment. First the oncall per-\nson must acknowledge receiving the alert. This establishes that the person is\nworking on the issue. This disables alerts for that particular issue while person-\nnel are working on it. When the issue is resolved, the oncall person must “resolve”\nthe alert to indicate that the issue is ﬁxed. The beneﬁt of this system is that it makes\nit easier to generate metrics about how long it took to resolve the issue. But what if\nthe person forgets to mark the issue resolved? The system would need to send out\nreminder alerts periodically, which defeats the purpose of having two stages.\nFor this reason, we feel the two-stage acknowledgment provides little actual\nvalue. If there is a desire to record how long it takes to resolve an issue, have the\nsystem detect when the alert is no longer triggering. It will be more accurate and\nless annoying than requiring the operator to manually indicate that the issue is\nresolved.\n17.4.2 Silence versus Inhibit\nOperationally, there is a need to be able to silence an alert at will. For example,\nduring scheduled maintenance the person doing the maintenance does not need to\nreceive alerts that a system is down. Systems that depend on that system should not\ngenerate alerts because, in theory, their operations teams have been made aware of\nthe scheduled maintenance.\nThe mechanism for handling this is called a silence or sometimes a mainte-\nnance. A silence is speciﬁed as a start time, an end time, and a speciﬁcation of what\nto silence. When specifying what to silence, it can be useful to accept wildcards or\nregular expressions.\nBy implementing silences in the alert and escalation system, the alerts still\ntrigger but no action is taken. This is an important distinction. The alert is still trig-\ngering; we’re just not receiving notiﬁcations. The problem or outage is still really\nhappening and any dashboards or displays will reﬂect that fact.\nAlternatively, one can implement silences in the real-time analysis system.\nThis approach is called an inhibit. In this case the alert does not trigger and as\n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "17.4\nAlerting and Escalation Manager\n357\na result no alerts are sent. An inhibit can be implemented by a mechanism where\nan alert rule speciﬁes it should not be evaluated (calculated) if one or more pre-\nrequisite alert rules are currently triggering. Alternatively, the formula language\ncould include a Boolean function that returns true or false depending on whether\nthe alert is triggering. This function would be used to short-circuit the evaluation\nof the rule.\nFor example, an alert rule to warn of a high error rate might be inhibited if the\ndatabase in use is ofﬂine. There is no sense in getting alerted for something you\ncan’t do anything about. Monitoring the database is handled by another rule, or\nby another team. The alert rule might look like:\nIF http-500-rate > 1% THEN\nALERT(error-rate-too-high)\nUNLESS ACTIVE_ALERT(database-offline)\n.\nSilence Creation UI Advice\nWe learned the hard way that people are bad at doing math related to dates,\ntimes, and time zones, especially when stress is high and alerts are blaring.\nIt is also easy to make mistakes when wildcards or regular expressions can\nbe used to specify what to silence. Therefore we recommend the following UI\nfeatures:\n• The default start time should be “now.”\n• It should be possible to enter the end time as a duration in minutes, hours,\nor days, as well as a speciﬁc time and date in any time zone.\n• So that the user may check his or her work, the UI should display what\nwill be silenced and require conﬁrmation before it is activated.\nInhibits can cause confusion if an outage is happening and, due to inhibits, the\nmonitoring system says that everything is ﬁne. Teams that depend on the service\nwill be confused when their dashboards show it being up, yet their service is mal-\nfunctioning due to the outage. Therefore we recommend using inhibits sparingly\nand carefully.\nThe difference between a silence and an inhibit is very subtle. You silence an\nalert when you know it would be erroneous to page someone based on some condi-\ntion such as a planned outage or upgrade. You inhibit alerts to conditionally cause\none alert to not ﬁre when another condition is active.\n",
      "content_length": 2152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "358\nChapter 17\nMonitoring Architecture and Practice\n17.5 Visualization\nVisualization is the creation of a visual representation of one or more metrics.\nIt is more than just creating the pretty graphs that will impress management.\nVisualization helps ﬁnd meaning in large amounts of data.\nA well-designed monitoring system does not collect data twice, once for real-\ntime alerting and yet again for visualization. If you collect it for visualization, be\nable to alert on it. If you alert on it, store it so that you can visualize it.\nSimple graphs can display raw data, summarized data, or a comparison of two\nmore metrics. Visualization also involves synthesizing new metrics from others.\nFor example, a rate can be calculated as the function of a counter and time.\nRaw data graphed over time, either as a single point or as an aggregate of\nsimilar metrics, is useful for metrics that change slowly. For example, if the memory\nuse of a server over time, which usually remains nearly constant, changes after a\nparticular software release, it is something to investigate. If the usage went up and\ndevelopers did not expect it to, there may be a memory leak. If usage went down\nand developers did not expect it to, there may be a bug.\nCounters are best viewed as rates. A counter, visualized in its raw form, looks\nlike a line going up and to the right. If the rate over time is calculated, we will see\neither acceleration, deceleration, or an equilibrium.\nSome visualizations are less useful and, in fact, can be misleading. A pie chart\nof disk space used and unused is a common dashboard ﬁxture that we ﬁnd rather\npretty but pointless. Knowing that a disk has 30 percent space remaining isn’t use-\nful without knowing the size of the disk. Seeing two such pie charts, side by side,\nof two different-sized disks, is rarely a useful comparison.\nIn contrast, calculating the rate at which the disk is ﬁlling and displaying the\nderivative of that rate becomes an actionable metric. It helps predict when the\ndisk space will be exhausted. Graph this derivative against the derivative of other\nresource consumption rates, such as the number of new accounts created, and you\ncan see how resources are consumed in proportion to each other.\nThe only thing we dislike more than pie charts are averages, or the mathemat-\nical mean. Averages can be misleading in ways that encourage you to make bad\ndecisions. If half of your customers like hot tea and the other half like cold tea, none\nof them will be happy if you serve them all lukewarm tea even though you have\nserved the same average temperature. A network link that is overloaded during\nthe day and barely used at night will be, on average, half used. It would be a bad\nidea to make a decision based on the average utilization. Averages lie. If billionaire\nBill Gates walks into a homeless shelter, the average person in the building is a\nmultimillionaire but it doesn’t take a genius to realize that the homeless problem\nhasn’t been eliminated.\nThat said, averages aren’t inherently misleading and have a place in your\nstatistical toolbox. Like other statistical functions they need to be used wisely.\n",
      "content_length": 3147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "17.5\nVisualization\n359\n17.5.1 Percentiles\nPercentiles and medians are useful when analyzing utilization or quality. These\nterms are often misunderstood. Here’s a simple way to understand them.\nImagine a school has 400 students. Stand those students up along a 100-meter\nline in order of lowest GPA to highest. Stretch out the line evenly so that the student\nwith the lowest GPA is at 0 and the one with the highest GPA is at 100. The student\nstanding at the 90-meter mark is the 90th percentile. This means that 90 percent of\nthe student body did worse than this person. The student at the 50-meter mark,\nexactly in the middle, is known as the median.\nIf there are more students, if there are fewer students, or if the grading stan-\ndard changed, there will always be a student at (or closest to) the 90-meter mark.\nThe GPA of the 90th percentile student could be a C- or an A+. It all depends on the\nmakeup of the entire student body’s GPAs. The entire school could have excellent\ngrades but someone is still going to be at the 90th percentile. Trends can be ana-\nlyzed by, for example, graphing the GPA of the 50th and 90th percentile students\neach year.\nIt is common for colocation facilities to charge based on the 90th percentile\nof bandwidth used. This means they take measurements every 5 minutes of\nhow much bandwidth is being used at that moment. All these measurements\nare sorted, but duplicates are not removed. If you wrote all these measure-\nments, distributed evenly, along a 100-meter football ﬁeld, the measurement\nat the 90-meter mark would be the bandwidth rate you will be billed for.\nThis is a more fair billing method than charging for the maximum or average\nrate.\nIt is very smart of your colocation facility to charge you this way. If it charged\nbased on the average bandwidth used, the company would take the total amount of\nbytes transmitted in the day, divide by the number of seconds in a day, and use that\nrate. That rate would be misleadingly low for customers that consume very little\nbandwidth at night and a lot during the day, what is known as a diurnal usage\npattern. In contrast, the 90th percentile represents how heavily you use bandwidth\nwithout penalizing you for huge bursts that happen from time to time. In fact,\nit forgives you for your 30 most extreme 5-minute periods of utilization, nearly\n2.5 hours each day. More importantly for the colocation facility, it best represents\nthe cost of the bandwidth used.\nPercentiles are also commonly used when discussing latency or delay per unit.\nFor example, a site that is trying to improve its page-load time might set a goal of\nhaving the 80th percentile page load time be 300 ms or less. A particular API call\nmight have a lot of variation in how long it takes to execute due to caching, type of\nrequest, and so on. One might set a goal of having its 99th percentile be less than\nsome amount, such as 100 ms.\n",
      "content_length": 2898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "360\nChapter 17\nMonitoring Architecture and Practice\n17.5.2 Stack Ranking\nStack ranking makes it easy to compare data by sorting the elements by value.\nFigure 17.2 is an example of stack ranking. If the cities had been sorted alphabeti-\ncally, it would not be clear how, for example, Atlanta compares to New York. With\nthe stack rank we can clearly see the difference.\nStack ranking is best used when data is apples-to-apples comparable. One way\nto achieve this is to normalize the data to be per unit (e.g., per capita, per machine,\nper service, per thousand queries). Figure 17.2 would mean something very dif-\nferent if the y-axis was thousands of tacos eaten in total by all people in that city\nversus being normalized to be a per-person metric. The latter is a more comparable\nnumber.\nIf you do not have a base-line, a stack rank can help establish one. You might\nnot know if “55” is a lot or a little, a bad or good amount, but you know only\ntwo cities were bigger. Managers who are unskilled at determining whether an\nemployee is effective can instead rely on peer stack ranking rather than making\nthe decision themselves.\nFigure 17.2: Cities stack ranked by number of tacos consumed\n",
      "content_length": 1190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "17.5\nVisualization\n361\n17.5.3 Histograms\nHistograms can reveal a lot about datasets and are particularly useful when ana-\nlyzing latency. A histogram is a graph of the count of values in a dataset, possibly\nafter rounding. For example, if we take a set of data and round all the values to the\nnearest multiple of 5, we would graph the number of 0s, 5s, 10s, 15s, and so on. We\nmight simply round to the nearest integer. These ranges are often called buckets.\nEach data point is put in its most appropriate bucket and the graph visualizes how\nmany items are in each bucket.\nFigure 17.3a shows the page load measurements collected on a ﬁctional web\nsite. Graphing them as a line plot does not reveal much except that there is a lot of\nvariation. The average is 27, which is also not very actionable.\nHowever, if we take each measurement, round it to the nearest multiple of\n5, and graph the number of 0s, 5s, 10s, and so on, we end up with the graph in\nFigure 17.3b. This reveals that the majority of the data points are in the 10, 15,\n35, and 40 buckets. The graph has two humps, like a camel. This pattern is called\nbimodal. Knowing that the data has this shape gives us a basis for investigation.\nWe can separate the data points out into the two humps and look for similarities.\nWe might ﬁnd that most of the site’s pages load very quickly, but most of the data\npoints in the ﬁrst hump are from a particular page. Investigation shows that every\ntime this page is generated, a large amount of data must be sorted. By pre-sorting\nthat data, we can reduce the load time of that page. The other hump might be\nfrom a web page that requires a database lookup that is larger than the cache and\ntherefore performs badly. We can adjust the cache so as to improve performance of\nthat page.\nThere are many other ways to visualize data. More sophisticated visualiza-\ntions can draw out different trends or consolidate more information into a picture.\nColor can be used to add an additional dimension. There are many ﬁne books on\nFigure 17.3: Page load time recorded from a ﬁctional web site\n",
      "content_length": 2080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "362\nChapter 17\nMonitoring Architecture and Practice\nthis subject. We highly recommend The Visual Display of Quantitative Information by\nTufte (1986).\n17.6 Storage\nThe storage system holds the metrics collected and makes them accessible by the\nother modules.\nStorage is one of the most architecturally demanding parts of the monitoring\nsystem. New metrics arrive in a constant ﬂood, at high speed. Alerting requires\nfast, real-time, read access for recent data. At the same time, other analysis requires\niterations over large ranges, which makes caching difﬁcult. Typical SQL databases\nare bad at all of these things.\nMedium-sized systems often collect 25–200 metrics for each server. There are\nmultiple servers on each machine. A medium-sized system may need to store 400\nnew metrics each second. Larger systems typically store thousands of metrics per\nsecond. Globally distributed systems may store tens or hundreds of thousands of\nmetrics every second.\nAs a result, many storage systems handle either real-time data or long-term\nstorage but not both, or can’t do both at large scale. Recently a number of time-\nseries databases such as OpenTSDB have sprung up that are speciﬁcally designed\nto be good at both real-time and long-term storage. They achieve this by keeping\nrecent data in RAM, often up to an hour’s worth, as well as by using a highly tuned\nstorage format.\nOn-disk storage of time-series data is usually done one of two ways. One\nmethod achieves fast high-speed random access by using ﬁxed-size records. For\nexample, each metric might be stored in a 20-byte record. The system can efﬁciently\nﬁnd a metric at a particular time by using a modiﬁed binary search. This approach\nis most effective when real-time visualization is required. Another method is to\ncompress the data, taking advantage of the fact that deltas can be stored in very few\nbits. The result is often variable-length records, which means the time-series data\nmust be read from the start to ﬁnd the metric at a particular time. These systems\npermit much greater storage density. Some systems achieve a balance by storing\nﬁxed-size records on a ﬁle system with built-in compression.\n17.7 Configuration\nThe six monitoring components discussed so far all need conﬁguration information\nto direct their work. The sensing system needs to know which data to measure and\nhow often. The collection system needs to know what to collect and where to send\nit. The analysis system has a base of formulas to process. The alerting system needs\n",
      "content_length": 2510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "17.8\nSummary\n363\nto know who to alert, how, and who to escalate to. The visualization system needs\nto know which graphs to generate and how to do so. The storage system needs to\nknow how to store and access the data.\nThese conﬁgurations should be treated like any other software source code:\nkept under revision control, tested using both unit tests and system tests, and so\non. Revision control tracks changes of a ﬁle over time, enabling one to see what\na ﬁle looked like at any point in its history. A unit test framework would take as\ninput time-series data for one or more metrics and output whether the alert would\ntrigger. This permits one to validate alert formulas.\nIn distributed monitoring systems, each component may be separated out and\nperhaps replicated or sharded. Each piece needs a way to access the conﬁguration.\nA system like ZooKeeper, discussed in Section 11.7, can be used to distribute a\npointer to where the full conﬁguration can be found, which is often a source code\nor package repository.\nSome monitoring systems are multitenant. This is where a monitoring sys-\ntem permits many teams to independently control metric collection, alert rules,\nand so on. By centralizing the service but decentralizing the ability to use it, we\nempower service owners, developers, and others to do their own monitoring and\nbeneﬁt from the ability to automatically collect and analyze data. Other monitor-\ning systems achieve the same goal by making it easy for individuals to install their\nown instance of the monitoring system, or just their own sensing and collection\ncomponents while centralizing storage and other components.\n17.8 Summary\nMonitoring systems are complex, with many components working together.\nThe sensing and measurement component takes measurements. Whitebox\nmonitoring monitors the systems internals. Blackbox monitoring collects data from\nthe perspective of a user. Gauges measure an amount that varies. Counters are\nnon-decreasing indicators of how many times something has happened.\nThe collection system gathers the metrics. Metrics may be pushed (sent) to the\ncollection system or the collection system may pull (query) systems to gather the\nmetrics.\nThe storage system stores the metrics. Usually custom databases are used\nto handle the large volume of incoming data and take advantage of the unique\nqualities of time-series data.\nThe analysis system extracts meaning from the data. There may be many\ndifferent analysis systems, each providing services such as anomaly detection, fore-\ncasting, or data mining. Some analysis occurs in real time, happening as the data is\ngathered. Short-term analysis focuses on recent data or provides the random access\n",
      "content_length": 2692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "364\nChapter 17\nMonitoring Architecture and Practice\nneeded by speciﬁc applications such as dashboards. Long-term analysis examines\nlarge spans of data to detect trends over many years. It is often done in batch mode,\nstoring intermediate results for later use.\nAlerting and escalation systems reach out to ﬁnd people when manual inter-\nvention is needed, ﬁnding a substitute when a person doesn’t respond within a\ncertain amount of time.\nVisualization systems provide graphs and dashboards. They can combine\nand transform data and do operations such as calculating percentiles, building\nhistograms, and determining stack ranks.\nAll of this is tied together by a conﬁguration manager that directs all of the\nother components in their work. Changes to conﬁgurations can be distributed in\nmany ways, often by distributing conﬁguration ﬁles or more dynamic systems such\nas ZooKeeper.\nWhen monitoring systems are multitenant, we empower individual service\nteams to control their own monitoring conﬁgurations. They beneﬁt from central-\nized components, freeing them from having to worry about capacity planning and\nother operational duties.\nWhen all the components work together, we have a monitoring system that is\nscalable, reliable, and functional.\nExercises\n1. What are the components of the monitoring system?\n2. Pick three components of the monitoring system and describe them in detail.\n3. Do all monitoring systems have all the components described in this chapter?\nGive examples of why components may be optional.\n4. What is a pager storm, and what are the ways to deal with one?\n5. Research the JSON format for representing data. Design a JSON format for the\ncollection of metrics.\n6. Describe the monitoring system in use in your organization or one you’ve had\nexperience with in the past. How is it used? What does it monitor? Which\nproblems does it solve?\n7. Create one or more methods of calculating a rate for a counter metric. The\nmethod should work even if there is a counter reset. Can your method also\ncalculate a margin of error?\n8. Design a better monitoring system for your current environment.\n9. Why are averages discouraged? Present a metric misleadingly as an average,\nand non-misleadingly some other way.\n",
      "content_length": 2226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "Chapter 18\nCapacity Planning\nPlans are nothing,\nplanning is everything.\n—Philip Kotler\nCapacity planning means ensuring that there will be enough resources when they\nare needed. Optimally this is done such that the system is neither under capac-\nity nor over capacity. Resources include CPUs, memory, storage, server instances,\nnetwork bandwidth, switch ports, console connections, power, cooling, datacenter\nspace, and any other infrastructure components that are required to run a service.\nThere are two major objectives of capacity planning. First, we want to prevent\nservice interruptions due to lack of capacity. Second, we want to preserve capital\ninvestment by adding only the capacity required at any given time. Good capac-\nity planning provides a demonstrable return on investment (ROI) by showing the\nneed for resources and capping resource usage at a level that ensures good service.\nCapacity planning should be a data-driven process, using data collected about\nthe running system to forecast trends. It also should be informed by future business\nplans for growth. This chapter explains which data you need to collect and how to\nuse it to forecast your future capacity requirements.\nCapacity planning in large, fast-growing services becomes highly complex\nand relies on sophisticated mathematical models. Organizations often hire a full-\ntime statistician to develop and maintain these models. A statistician with a\ntechnical background may be difﬁcult to ﬁnd, but such a person is worth his or\nher weight in gold. This chapter introduces some mathematical models that were\ndeveloped for trading on the ﬁnancial markets, and shows how you can apply\nthem to capacity planning in a rapidly changing environment.\nThis chapter does not look at the enterprise-style capacity planning required\nto meet the day-to-day needs of the people who work for the service provider.\nInstead, this chapter focuses on capacity planning for the service itself.\n365\n",
      "content_length": 1957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "366\nChapter 18\nCapacity Planning\n.\nTerms to Know\nQPS: Queries per second. Usually how many web hits or API calls received\nper second.\nActive Users: The number of users who have accessed the service in the\nspeciﬁed timeframe.\nMAU: Monthly active users. The number of users who have accessed the\nservice in the last month.\nEngagement: How many times on average an active user performs a par-\nticular transaction.\nPrimary Resource: The one system-level resource that is the main limiting\nfactor for the service.\nCapacity Limit: The point at which performance starts to degrade rapidly\nor become unpredictable.\nCore Driver: A factor that strongly drives demand for a primary resource.\nTime Series: A sequence of data points measured at equally spaced time\nintervals. For example, data from monitoring systems.\n18.1 Standard Capacity Planning\nCapacity planning needs to provide answers to two questions: What are you going\nto need to buy in the coming year? and When are you going to need to buy it? To\nanswer those questions, you need to know the following information:\n• Current Usage: Which components can inﬂuence service capacity? How much\nof each do you use at the moment?\n• Normal Growth: What is the expected growth rate of the service, without\nthe inﬂuence of any speciﬁc business or marketing events? Sometimes this\nis called organic growth.\n• Planned Growth: Which business or marketing events are planned, when will\nthey occur, and what is the anticipated growth due to each of these events?\n• Headroom: Which kind of short-term usage spikes does your service\nencounter? Are there any particular events in the coming year, such as the\nOlympics or an election, that are expected to cause a usage spike? How much\nspare capacity do you need to handle these spikes gracefully? Headroom is\nusually speciﬁed as a percentage of current capacity.\n• Timetable: For each component, what is the lead time from ordering to deliv-\nery, and from delivery until it is in service? Are there speciﬁc constraints for\nbringing new capacity into service, such as change windows?\n",
      "content_length": 2066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "18.1\nStandard Capacity Planning\n367\n.\nMath Terms\nCorrelation Coefficient: Describes how strongly measurements for differ-\nent data sources resemble each other.\nMoving Average: A series of averages, each of which is taken across a short\ntime interval (window), rather than across the whole data set.\nRegression Analysis: A statistical method for analyzing relationships\nbetween different data sources to determine how well they correlate,\nand to predict changes in one based on changes in another.\nEMA: Exponential moving average. It applies a weight to each data point\nin the window, with the weight decreasing exponentially for older data\npoints.\nMACD: Moving average convergence/divergence. An indicator used to\nspot changes in strength, direction, and momentum of a metric. It mea-\nsures the difference between an EMA with a short window and an EMA\nwith a long window.\nZero Line Crossover: A crossing of the MACD line through zero happens\nwhen there is no difference between the short and long EMAs. A move\nfrom positive to negative shows a downward trend in the data, and a\nmove from negative to positive shows an upward trend.\nMACD Signal Line: An EMA of the MACD measurement.\nSignal Line Crossover: The MACD line crossing over the signal line indi-\ncates that the trend in the data is about to accelerate in the direction of\nthe crossover. It is an indicator of momentum.\nFrom that information, you can calculate the amount of capacity you expect to need\nfor each resource by the end of the following year with a simple formula:\nFuture Resources = Current Usage × (1 + Normal Growth\n+ Planned Growth) + Headroom\nYou can then calculate for each resource the additional capacity that you need\nto purchase:\nAdditional Resources = Future Resources −Current Resources\nPerform this calculation for each resource, whether or not you think you will\nneed more capacity. It is okay to reach the conclusion that you don’t need any\nmore network bandwidth in the coming year. It is not okay to be taken by sur-\nprise and run out of network bandwidth because you didn’t consider it in your\n",
      "content_length": 2082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "368\nChapter 18\nCapacity Planning\ncapacity planning. For shared resources, the data from many teams will need to be\ncombined to determine whether more capacity is needed.\n18.1.1 Current Usage\nBefore you can consider buying additional equipment, you need to understand\nwhat you currently have available and how much of it you are using. Before you\ncan assess what you have, you need a complete list of all the things that are required\nto provide the service. If you forget something, it won’t be included in your capac-\nity planning, and you may run out of that one thing later, and as a result be unable\nto grow the service as quickly as you need.\nWhat to Track\nThe two most obvious things that the provider of an Internet-based service needs\nare some machines to provide the service and a connection to the Internet. Some\nmachines may be generic machines that are later customized to perform given\ntasks, whereas others may be specialized appliances. Going deeper into these\nitems, machines have CPUs, caches, RAM, storage, and network. Connecting to\nthe Internet requires a local network, routers, switches, and a connection to at least\none ISP. Going deeper still, network cards, routers, switches, cables, and storage\ndevices all have bandwidth limitations. Some appliances may have higher-end\nnetwork cards that need special cabling and interfaces on the network gear. All net-\nworked devices need IP addresses. These are all resources that need to be tracked.\nTaking one step back, all devices run some sort of operating system, and some\nrun additional software. The operating systems and software may require licenses\nand maintenance contracts. Data and conﬁguration information on the devices\nmay need backing up to yet more systems. Stepping even farther back, machines\nneed to be installed in a datacenter that meets their power and environment needs.\nThe number and type of racks in the datacenter, the power and cooling capac-\nity, and the available ﬂoor space all need to be tracked. Datacenters may provide\nadditional per-machine services, such as console service. For companies that have\nmultiple datacenters and points of presence, there may be links between those sites\nthat also have capacity limits. These are all additional resources to track.\nOutside vendors may provide some services. The contracts covering those ser-\nvices specify cost or capacity limits. To make sure that you have covered avery\npossible aspect, talk to people in every department, and ﬁnd out what they do and\nhow it relates to the service. For everything that relates to the services, you need to\nunderstand what the limits are, how you can track them, and how you can measure\nhow much of the available capacity is used.\nHow Much Do You Have\nThere is no substitute for a good up-to-date inventory database for keeping track\nof your assets. The inventory database should be kept up-to-date by making it a\n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "18.1\nStandard Capacity Planning\n369\ncore component in the ordering, provisioning, and decommissioning processes. An\nup-to-date inventory system gives you the data you need to ﬁnd out how much of\neach resource you have. It should also be used to track the software license and\nmaintenance contract inventory, and the contracted amount of resources that are\navailable from third parties.\nUsing a limited number of standard machine conﬁgurations and having a set\nof standard appliances, storage systems, routers, and switches makes it easier to\nmap the number of devices to the lower-level resources, such as CPU and RAM,\nthat they provide.\nHow Much Are You Using Now\nIdentify the limiting resources for each service. Your monitoring system is likely\nalready collecting resource use data for CPU, RAM, storage, and bandwidth. Typi-\ncally it collects this data at a higher frequency than required for capacity planning.\nA summarization or statistical sample may be sufﬁcient for planning purposes and\nwill generally simplify calculations. Combining this data with the data from the\ninventory system will show how much spare capacity you currently have.\nTracking everything in the inventory database and using a limited set of stan-\ndard hardware conﬁgurations also makes it easy to specify how much space,\npower, cooling, and other datacenter resources are used per device. With all of\nthat data entered into the inventory system, you can automatically generate the\ndatacenter utilization rate.\n18.1.2 Normal Growth\nThe monitoring system directly provides data on current usage and current capac-\nity. It can also supply the normal growth rate for the preceding years. Look for any\nnoticeable step changes in usage, and see if these correspond to a particular event,\nsuch as the roll-out of a new product or a special marketing drive. If the offset due\nto that event persists for the rest of the year, calculate the change and subtract it\nfrom subsequent data to avoid including this event-driven change in the normal\ngrowth calculation. Plot the data from as many years as possible on a graph, to\ndetermine if the normal growth rate is linear or follows some other trend.\n18.1.3 Planned Growth\nThe second step is estimating additional growth due to marketing and business\nevents, such as new product launches or new features. For example, the marketing\ndepartment may be planning a major campaign in May that it predicts will increase\nthe customer base by 20 to 25 percent. Or perhaps a new product is scheduled to\nlaunch in August that relies on three existing services and is expected to increase\nthe load on each of those by 10 percent at launch, increasing to 30 percent by the\n",
      "content_length": 2679,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "370\nChapter 18\nCapacity Planning\nend of the year. Use the data from any changes detected in the ﬁrst step to validate\nthe assumptions about expected growth.\n18.1.4 Headroom\nHeadroom is the amount of excess capacity that is considered routine. Any service\nwill have usage spikes or edge conditions that require extended resource usage\noccasionally. To prevent these edge conditions from triggering outages, spare\nresources must be routinely available. How much headroom is needed for any\ngiven service is a business decision. Since excess capacity is largely unused capac-\nity, by its very nature it represents potentially wasted investment. Thus a ﬁnan-\ncially responsible company wants to balance the potential for service interruption\nwith the desire to conserve ﬁnancial resources.\nYour monitoring data should be picking up these resource spikes and provid-\ning hard statistical data on when, where, and how often they occur. Data on outages\nand postmortem reports are also key in determining reasonable headroom.\nAnother component in determining how much headroom is needed is the\namount of time it takes to have additional resources deployed into production from\nthe moment that someone realizes that additional resources are required. If it takes\nthree months to make new resources available, then you need to have more head-\nroom available than if it takes two weeks or one month. At a minimum, you need\nsufﬁcient headroom to allow for the expected growth during that time period.\n18.1.5 Resiliency\nReliable services also need additional capacity to meet their SLAs. The additional\ncapacity allows for some components to fail, without the end users experiencing an\noutage or service degradation. As discussed in Chapter 6, the additional capacity\nneeds to be in a different failure domain; otherwise, a single outage could take\ndown both the primary machines and the spare capacity that should be available\nto take over the load.\nFailure domains also should be considered at a large scale, typically at the\ndatacenter level. For example, facility-wide maintenance work on the power sys-\ntems requires the entire building to be shut down. If an entire datacenter is ofﬂine,\nthe service must be able to smoothly run from the other datacenters with no capac-\nity problems. Spreading the service capacity across many failure domains reduces\nthe additional capacity required for handling the resiliency requirements, which\nis the most cost-effective way to provide this extra capacity. For example, if a\nservice runs in one datacenter, a second datacenter is required to provide the addi-\ntional capacity, about 50 percent. If a service runs in nine datacenters, a tenth\nis required to provide the additional capacity; this conﬁguration requires only\n10 percent additional capacity.\n",
      "content_length": 2785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "18.2\nAdvanced Capacity Planning\n371\nAs discussed in Section 6.6.5, the gold standard is to provide enough capacity\nfor two datacenters to be down at the same time. This permits one to be down\nfor planned maintenance while the organization remains prepared for another\ndatacenter going down unexpectedly. Appendix B discusses the history of such\nresilient architectures.\n18.1.6 Timetable\nMost companies plan their budgets annually, with expenditures split into quarters.\nBased on your expected normal growth and planned growth bursts, you can map\nout when you need the resources to be available. Working backward from that date,\nyou need to ﬁgure out how long it takes from “go” until the resources are available.\nHow long does it take for purchase orders to be approved and sent to the\nvendor? How long does it take from receipt of a purchase order until the vendor has\ndelivered the goods? How long does it take from delivery until the resources are\navailable? Are there speciﬁc tests that need to be performed before the equipment\ncan be installed? Are there speciﬁc change windows that you need to aim for to\nturn on the extra capacity? Once the additional capacity is turned on, how long\ndoes it take to reconﬁgure the services to make use of it? Using this information,\nyou can provide an expenditures timetable.\nPhysical services generally have a longer lead time than virtual services. Part\nof the popularity of IaaS and PaaS offerings such as Amazon’s EC2 and Elas-\ntic Storage are that newly requested resources have virtually instant delivery\ntime.\nIt is always cost-effective to reduce resource delivery time because it means we\nare paying for less excess capacity to cover resource delivery time. This is a place\nwhere automation that prepares newly acquired resources for use has immediate\nvalue.\n18.2 Advanced Capacity Planning\nLarge, high-growth environments such as popular internet services require a dif-\nferent approach to capacity planning. Standard enterprise-style capacity planning\ntechniques are often insufﬁcient. The customer base may change rapidly in ways\nthat are hard to predict, requiring deeper and more frequent statistical analy-\nsis of the service monitoring data to detect signiﬁcant changes in usage trends\nmore quickly. This kind of capacity planning requires deeper technical knowledge.\nCapacity planners will need to be familiar with concepts such as QPS, active users,\nengagement, primary resources, capacity limit, and core drivers. The techniques\ndescribed in this section of the book were covered by Yan and Kejariwal (2013),\nwhose work inspired this section.\n",
      "content_length": 2604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "372\nChapter 18\nCapacity Planning\n18.2.1 Identifying Your Primary Resources\nEach service has one primary resource, such as CPU utilization, memory footprint\nor bandwidth, storage footprint or bandwidth, or network bandwidth, that is the\ndominant resource consumed by the service. For example, a service that does a\nlot of computation is usually limited by the available CPU resources—that is, it is\nCPU-bound. Capacity planning focuses on the primary resource.\nServices also have secondary resource needs. For example, a service that is\nCPU-bound may, to a lesser extent, use memory, storage, and network bandwidth.\nThe secondary resources are not interesting from a capacity planning point of view,\nwith the current software version and hardware models, but one of them may\nbecome the primary resource later as the code or hardware changes. Consequently,\nthese secondary resources should also be monitored and the usage trends tracked.\nDetecting a change in which resource is the primary resource for a given service\nis done by tracking the constraint ratio (disk:CPU:memory:network) between the\nprimary and secondary resources. Whenever a new software version is released or\na new hardware model is introduced, this ratio should be recalculated.\nPrimary and secondary resources are low-level resource units that drive the\nneed for ancillary resources such as server instances, switch ports, load balancers,\npower, and other datacenter infrastructure.\nThe ﬁrst step in capacity planning is to identify your primary resources, as\nthese are the resources that you will focus on for capacity planning. You must also\ndeﬁne a relationship between your primary and ancillary usage. When your capac-\nity planning indicates that you need more of your primary resources, you need to\nbe able to determine how much of the ancillary resources are required as a result.\nAs the hardware that you use changes, these mappings need to be updated.\n18.2.2 Knowing Your Capacity Limits\nThe capacity limit of any resource is the point at which performance starts to\ndegrade rapidly or become unpredictable, as shown in Figure 18.1. Capacity limits\nshould be determined by load testing. Load testing is normally performed in an\nisolated lab environment. It can be performed by synthetically generating trafﬁc\nor by replaying production trafﬁc.\nFor each primary and secondary resource, you need to know the capacity\nlimit. To generate such data for each resource in isolation, your lab setup should\nbe equipped with lots of excess capacity for all but one low-level resource. That\nresource will then be the limiting factor for the service, and response times can be\ngraphed against the percentage utilization of that resource and separately against\nthe absolute amount of that resource that is available. For completeness, it is best\nto repeat this test several times, each time scaling up the whole environment\nuniformly. This approach will enable you to determine if the limit is more closely\nrelated to percentage utilization or to quantity of remaining resources.\n",
      "content_length": 3045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "18.2\nAdvanced Capacity Planning\n373\nFigure 18.1: Response time starts to degrade beyond a certain percent\nutilization.\n18.2.3 Identifying Your Core Drivers\nCore drivers are factors that strongly drive demand for a primary resource. They\ntypically include values such as MAU, QPS, the size of the corpus, or other high-\nlevel metrics that represent well the factors that generate trafﬁc or load on the\nservice. These metrics also often have meaningful business implications, with links\nto sources of revenue, for example.\nA site may have millions of registered users, but it will typically have many\nfewer users who are active. For example, many people sign up for accounts and\nuse the service a few times, but never return. Counting these users in your planning\ncan be misleading. Many people register with social networking sites, but rarely\nuse their accounts. Some people who are registered with online shopping sites\nmay use the service only before birthdays and gift-buying holidays. A\nmore accurate representation of users may be how many were active in the last\n7 or 30 days. The number of users in the last 7 days is often called 7-day actives\n(7DA), while the term weekly active users (WAU) is used to indicate how many\nwere active in a speciﬁc calendar week. Likewise, 30-day actives (30DA) measures\nthe number of users in the last 30 days, with the term monthly active users (MAU)\nused if the measurement was bounded by a speciﬁc calendar month. These mea-\nsurements often reﬂect usage much more accurately than the number of registered\nusers.\n",
      "content_length": 1555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "374\nChapter 18\nCapacity Planning\nFor metrics such as active users that have a time component, different val-\nues of that time component may be appropriate to use in capacity planning for\ndifferent services. For example, for some services monthly active users may be the\nappropriate core driver, whereas for another service minutely active users may be a\nbetter indicator to use in capacity planning. For highly transactional systems that\nare driven by active users, smaller time scales like minutely active users may be\nappropriate. For storage-bound services that are driven by users, the total number\nof registered users (or total user corpus number) may be more appropriate.\nThe capacity model depicts the relationship between the core driver and the\nprimary resource. For a given service, the capacity model expresses how changes\nin the core driver affect that service’s need for its core driver.\nOnce you have identiﬁed the core drivers, and have determined the effect that\neach one has on each of the primary and secondary resources, you can quantify\nthe effect that each will have on your requirements for ancillary resources, such as\nservers. You can also analyze whether your ancillary resources are well balanced.\nIf servers run out of CPU cycles long before they run out of RAM, then it may\nbe more cost-effective to order servers with less RAM or more or faster CPUs, for\nexample. Or it may be possible to rework the service by taking advantage of the\nextra RAM or making it less CPU-intensive. Similarly, if your switches run out of\nports long before backplane bandwidth, perhaps a different switch model would\nbe more appropriate.\n18.2.4 Measuring Engagement\nIf the number of active users is growing rapidly, it might seem as if this number is\nthe only core driver that needs to be taken into consideration. However, that works\non the assumption that this growth will continue. Another factor can, and should,\nbe tracked separately—user engagement. Engagement is the number of times that\nthe average active user accesses the particular service in a given period of time.\nIn other words, it is a measure of the popularity of a particular feature. For exam-\nple, a site might ﬁnd that when it ﬁrst introduces a video-sharing functionality, not\nmany people are interested, and only about 1 percent of the monthly active users\naccess that feature. However, as time goes on, this feature may become more pop-\nular, and six months later 25 percent of the monthly active users may be accessing\nthat feature.\nIf the number of active users stays static but engagement increases, then the\nload on your service will also increase. Taken in combination, these two numbers\nindicate how often a service is accessed.\nThe engagement graph for each service will be different, and an increase in\nengagement for one service will have a different effect on resource requirements\nthan a change in engagement for another service. For example, an increase in\n",
      "content_length": 2950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "18.2\nAdvanced Capacity Planning\n375\nengagement for video uploads will have more of an impact on disk space and I/O\nbandwidth than an increase in engagement for the chat service will.\n18.2.5 Analyzing the Data\nOnce you have decided which metrics to collect and started gathering data, you\nneed to be able to process that data and generate useful inputs for your capacity\nplanning process. Standard capacity planning looks at year-to-year statistics and\nmakes projections based on those data. This approach is still necessary, but in a\nlarge-scale, rapidly changing environment, we need to augment it so that we are\nable to react more quickly to changes in demand.\nIn a large-scale environment, one of the goals is to be able to simply but accu-\nrately specify how many resources you need to have based on a measured core\ndriver. One way to meet this goal is to simplify the metrics down to blocks of\nusers based on the serving capacity of one rack or cluster of machines. With this\napproach, capacity planning is simpliﬁed into a more cookie-cutter approach. One\napplication might serve 100,000 active users per rack. Another application might\nserve 1000 active users per cluster, where each cluster is a standardized combi-\nnation of machines that act as an independent unit. Engineering can produce new\nratios as newer, faster hardware becomes available or new software designs are cre-\nated. Now capacity planning is simpliﬁed and resources can be managed based on\nblocks of active users. While this approach is less granular, it is sufﬁcient because\nit matches the deployment granularity. You can’t buy half a machine, so capacity\nplanning doesn’t need to be super precise.\nTo identify such a relationship between a core driver and resource consump-\ntion, you ﬁrst need to understand which core drivers inﬂuence which resources\nand how strongly. The way to do so is to correlate the resource usage metrics with\nthe core driver metrics.\nCorrelation\nCorrelation measures how closely data sources resemble each other. Visually, you\nmight see on a monitoring graph that an increase in CPU usage on a server matches\nup with a corresponding increase in network trafﬁc to the same server, which also\nmatches up with a spike in QPS. From these observations you might conclude that\nthese three measurements are related, although you cannot necessarily say that the\nchanges in one caused the changes in another.\nRegression analysis mathematically calculates how well time-series data\nsources match up. Regression analysis of your metrics can indicate how strongly\nchanges in a core driver affect the usage of a primary resource. It can also indicate\nhow strongly two core drivers are related.\n",
      "content_length": 2686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "376\nChapter 18\nCapacity Planning\nTo perform a regression analysis on time-series data, you ﬁrst need to deﬁne\na time interval, such as 1 day or 4 weeks. The number of data samples in that time\nperiod is n. If your core driver metric is x and your primary resource metric is y,\nyou ﬁrst calculate the sum of the last n values for x, x2, y, y2, and x times y, giving\n∑x, ∑x2, ∑y, ∑y2, and ∑xy. Then calculate SSxy, SSxx, SSyy, and R as follows:\nSSxy =\n∑\nxy −(∑x)(∑y)\nn\nSSxx =\n∑\nx2 −(∑x)2\nn\nSSyy =\n∑\ny2 −(∑y)2\nn\nr =\nSSxy\n√SSxxSSyy\nRegression analysis results in a correlation coefﬁcient R, which is a number\nbetween −1 and 1. Squaring this number and then multiplying by 100 gives the\npercentage match between the two data sources. For example, for the MAU and\nnetwork utilization ﬁgures shown in Figure 18.2, this calculation gives a very high\ncorrelation, between 96 percent and 100 percent, as shown in Figure 18.3, where R2\nis graphed.\nWhen changes in a core driver correlate well with changes in usage of a pri-\nmary resource, you can derive an equation of the form y = a + bx, which describes\nthe relationship between the two, known as the regression line. This equation\nenables you to calculate your primary resource requirements based on core driver\nFigure 18.2: The number of users correlates well with network trafﬁc.\n",
      "content_length": 1325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "18.2\nAdvanced Capacity Planning\n377\nFigure 18.3: The upper line shows a high correlation of over 96% between two data\nsources. The lower line shows low correlation—less than 60%.\nmeasurements. In other words, given a value for your core driver x, you can\ncalculate how much of your primary resource y you think you will need, with a\nconﬁdence of R2. To calculate a and b, ﬁrst calculate the moving average of the last\nn data points for x and y, giving ¯x and ¯y. Then:\nb = SSxx\nSSyy\na = ¯y −b¯x\nCorrelation between metrics changes over time, and should therefore be\ngraphed and tracked with a rolling correlation analysis, rather than assessed\nonce and assumed to be constant. Changes in the service can have a signiﬁcant\nimpact on correlation. Changes in the end-user demographic are usually slower,\nbut can also affect correlation by changing how the average customer uses the\nservice.\nFigure 18.4a shows a sharp drop in correlation corresponding to a change in\nresource usage patterns between one software release and the next. The change in\ncorrelation in the graph actually corresponds to a step-change in resource usage\npatterns from one release to the next. After the time interval chosen for the rolling\ncorrelation measurement elapses, correlation returns to normal.\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "378\nChapter 18\nCapacity Planning\nFigure 18.4: Change in correlation between MAU and bandwidth\nFigure 18.4b shows b for the same time interval. Notice that after the upgrade\nb changes signiﬁcantly during the time period chosen for the correlation analysis\nand then becomes stable again but at a higher value. The large ﬂuctuations in b for\nthe length of the correlation window are due to signiﬁcant changes in the moving\naverages from day to day, as the moving average has both pre- and post-upgrade\ndata. When sufﬁcient time has passed so that only post-upgrade data is used in\nthe moving average, b becomes stable and the correlation coefﬁcient returns to its\nprevious high levels.\nThe value of b corresponds to the slope of the line, or the multiplier in the\nequation linking the core driver and the usage of the primary resource. When corre-\nlation returns to normal, b is at a higher level. This result indicates that the primary\nresource will be consumed more rapidly with this software release than with the\nprevious one. Any marked change in correlation should trigger a reevaluation of\nthe multiplier b and corresponding resource usage predictions.\nForecasting\nForecasting attempts to predict future needs based on current and past measure-\nments. The most basic forecasting technique is to graph the 90th percentile of the\nhistorical usage and then ﬁnd an equation that best ﬁts this data. You can then\nuse that equation to predict future usage. Calculating percentiles was discussed in\nSection 17.5.1.\nOf course, growth rates change, usage patterns change, and application\nresource needs change. We need to be able to detect these changes and alter our\nresource planning accordingly. To detect changes in a trend, we use the moving\naverage convergence/divergence (MACD) metric. MACD measures the difference\nbetween a long-period (e.g., 3 months) and a short-period (e.g., 1 month) moving\naverage. However, a standard moving average tends to mask recent changes in\nmetrics. Since forecasting aims to provide early detection for such changes, MACD\n",
      "content_length": 2056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "18.2\nAdvanced Capacity Planning\n379\nuses an exponential moving average (EMA), which gives an average with a much\nheavier weighting for recent data and a very low weighting for old data. An EMA\nis calculated as follows, where n is the number of samples:\nk =\n2\nn + 1\nEMAx = Valuex × k + EMAx−1 × (1 −k)\nTo get started, the ﬁrst EMAx−1 value (actually EMAn) is just a straight average\nof the ﬁrst n data points.\nTo use MACD to give early warning of changes in behavior, you need to cal-\nculate and plot some additional data, called the MACD signal line, on the same\ngraph. The MACD signal is an EMA of the MACD. When the MACD line crosses\nthe MACD signal line, that is an indication that the trend is changing. When the\nMACD line crosses from below the signal line to above the signal line, it indi-\ncates an increase. For example, for an engagement metric, it would indicate an\nunexpected increase in engagement for a particular feature. When the MACD line\ncrosses from above the signal line to below it, it indicates a downward trend. For\nexample, in a memory usage graph, it might indicate that the most recent release is\nmore memory efﬁcient than the previous one. This may cause you to reassess the\nnumber of users a cluster can support.\nThe challenge in measuring and graphing MACD is to deﬁne the time scales\nto use for the long and short periods. If the periods are too short, the MACD will\nindicate changes in trends too frequently to be useful, as in Figure 18.5. The bar\nchart in the background of this ﬁgure is the 90th percentile data. The smoother\ngraph line is the MACD signal line, and the other line is the MACD.\nHowever, increasing the short period, in particular, will tend to delay trig-\ngering a change in trend event. In Figure 18.6, the downward trend is triggered\n2 days earlier with a 2-week short period (Figure 18.6a) than with the 4-week short\nperiod (Figure 18.6b), keeping the long period constant at 3 months. However, the\n2-week short period also has a little extra noise, with a downward trigger followed\n2 days later by an upward trigger, followed a week later by the ﬁnal downward\ntrigger.\nWhen choosing your long and short periods, you can validate the model by\nseeing if older data predicts more recent data. If it does, it is reasonable to conclude\nthat the model will be a good predictor in the near-term future. Start with existing\ndata, as far back as you have available. Try several different combinations of short\nand long periods, and see which combination best predicts trends observed in your\nhistorical datasets.\n",
      "content_length": 2557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "380\nChapter 18\nCapacity Planning\nFigure 18.5: Noisy data from a 1-week short period and a 4-week long period\nFigure 18.6: Effect of changing only the short period\n18.2.6 Monitoring the Key Indicators\nThe correlation coefﬁcients between core drivers and resources should be graphed\nand monitored. If there is a signiﬁcant change in correlation, the relationship\nbetween the core driver and resource consumption should be reassessed, and any\nchanges fed back into the capacity planning process.\nSimilarly, the MACD and MACD signal for each core driver and resource\nshould be monitored. Since these metrics can be used for early detection in changes\n",
      "content_length": 647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "18.3\nResource Regression\n381\nin trends, they are key elements in capacity management. Alerts generated by the\nMACD line and MACD signal line crossing should go straight to the team respon-\nsible for capacity planning. If the core drivers and relationships to the primary\nresources are well deﬁned, theoretically it should be necessary to monitor only\nthe core drivers and the correlation coefﬁcients. In reality, it is prudent to monitor\neverything to minimize the chance of surprises.\n18.2.7 Delegating Capacity Planning\nCapacity planning is often done by the technical staff. However, with good metrics\nand a clear understanding of how core drivers affect your resource requirements,\nyou can decouple the capacity planning from the deployment. A program man-\nager can do the capacity planning and ordering, while technical staff take care of\ndeployment.\nYou can enable non-technical staff to do the capacity planning by building\na capacity planning dashboard as part of your monitoring system. Create one or\nmore web pages with capacity data in a specialized view, ideally with the ability\nto create graphs automatically. Make this dashboard accessible within the organi-\nzation separately from the main monitoring dashboard. This way anyone in the\norganization can access the data in a reasonable form to justify capital expenditure\non additional capacity.\nSometimes the decision is not to buy more resources but rather to make more\nefﬁcient use of existing resources. Examples might include compressing data rather\nthan buying more storage or bandwidth, or using better algorithms rather than\nincreasing memory or CPU. Testing such scenarios relies on resource regression,\nwhich we will examine next.\n18.3 Resource Regression\nA resource regression is a calculation of the difference in resource usage between\none release or version and another. It is expected that each software release will\nhave slightly different resource needs. If a new software release uses signiﬁcantly\nmore resources, that discrepancy is often unintended; in other words, it is a bug\nthat should be reported. If it is intended, it means capacity planners need to adjust\ntheir models and purchasing plans.\nTo perform a resource regression, do workﬂow analysis based on the user\ntransactions associated with a release. This is simply a fancy way of saying that\nyou make lists of which capabilities are possible with the new release and which\nkind of resource usage goes with each capability. Then, for each capability, you\nmultiply by the number of projected transactions per customer and the number of\ncustomers active. This will give you a resource projection for the new release.\n",
      "content_length": 2659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "382\nChapter 18\nCapacity Planning\nFor example, suppose you have a photo sharing site. There are three main\ntransactions that your site has with a customer. First, the user can log in and edit\ntheir proﬁle data. Second, the user can upload a picture. Third, the user can view a\npicture. Each of these transactions has a resource cost that can be measured. How\nwould you measure it? The easiest way would be by analyzing data from a staging\nsystem with sample users whom you control.\nBy using automated testing scripts to simulate user login and transactions,\nyou can have a known number of users, each involved in a particular transaction.\nThe monitoring data you gather from that system can then be compared with data\nfrom the baseline (no simulated users) system. It is reasonable to assume that the\ndifferences in memory, disk, CPU, and network usage are due to the user trans-\nactions. Subtract the baseline footprint of each of those resources from the loaded\nfootprint. The difference can then be divided by the number of user transactions,\nallowing you to get a sense of the per-transaction resource usage. Be sure to load\ntest your predictions to see whether the resource usage scales linearly or whether\ninﬂection points appear as you add more transactions.\nWhen calculating workﬂow analysis, be sure to include infrastructure resource\ncosts as well. How many DNS lookups are required for this end-to-end transaction?\nHow many database calls? A single transaction may touch many services within\nyour system infrastructure, and those service resources must be assessed as well\nand scaled appropriately along with the transaction server scaling.\n18.4 Launching New Services\nNow that we understand how to plan for capacity on an existing system, let’s\ntalk about launching new services. Launching a new service is difﬁcult and risky\nbecause you have no prior experience or service metrics to plan the initial capacity\nrequired. For large services, testing may be unreliable, as there may be insufﬁcient\ncapacity on a staging environment for a true test.\nAdding to this risk, launch day is often when the service will have the most\nmedia scrutiny. If the system runs out of capacity and becomes unreliable, the\nmedia will have nothing to write about except what a terrible launch it was. This\nmakes a bad ﬁrst impression on customers.\nOne way to mitigate this risk is to ﬁnd a way to have a slow ramp-up in the\nnumber of active users. For example, enabling a product but not announcing it (a\nsoft launch) gives engineers time to ﬁnd and ﬁx problems as they happen. How-\never, if additional resources are needed and will require weeks of lead time, only\nthe slowest of ramp-ups will help.\nRelying on a slow ramp-up is not an option for well-known companies.\nGoogle and Facebook haven’t had a slow ramp-up on a new service, even with\na soft launch, for years. Any new service is immediately ﬂooded with new users.\n",
      "content_length": 2915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "18.4\nLaunching New Services\n383\nTherefore the ability to do accurate capacity planning on new services has become\na highly desired skill.\nFortunately, there is a technique that can be used to bridge the knowledge\ngap, and that technique is the dark launch. Facebook used a dark launch of its\nchat messaging system to ensure that the company would bring a reliable system\nto its user base.\nIn a dark launch, the new feature is released into production with simulated\ntrafﬁc, in effect treating the production environment as a well-controlled testbed for\nthe new feature’s resource needs. No user-visible information is created—service\nagents exercise a feature silently from the user perspective, but employ real user\nactivity to trigger simulated activity for the new feature.\nFor example, suppose we wanted to add a photo editing capability to our\nphoto sharing site from the earlier example. To do a dark launch, we might do\nthe following:\n• Create a software toggle for the photo editing feature: on or off. (Software\ntoggles were described in Section 2.1.9.)\n• Create a dark launch toggle for the photo editing feature: on or off.\n• Create a sample photo editing transaction that is saved to a dummy area. The\ncustomer’s photo is not changed, but behind the scenes the photo is edited\nand a modiﬁed version saved elsewhere.\n• Modify an existing transaction to behave differently when the dark launch\ntoggle is “on.” In this case, photo upload will be modiﬁed to run the sample\nphoto editing transaction 25 percent of the time that uploads occur, feeding\nthe uploaded photo into the editing transaction.\nThe 25 percent value is only an example—it could be any percentage. It simply\nrepresents a known quantity that can be calculated at a later time based on trans-\naction data. Starting with a more conservative number such as 5 percent is likely\nto be a good idea with an actual complex service. Use the resource regression anal-\nysis technique to see which kind of resource costs are associated with the new\nlaunch feature. This will give you a ﬁrst pass at capacity planning for the new fea-\nture launch, based on actual production environment usage. Adjust your capacity\naccordingly, and then continue the dark launch, ﬁxing any bugs that are found and\nadjusting capacity as needed. Gradually increase the percentage of sample dark\nlaunch transactions until you reach a level where real usage is likely to occur. Be\nsure to go beyond that by some percentage to give yourself headroom.\nFinally the dark launch is turned off and the feature is turned on for the cus-\ntomers. With the appropriate toggles, this can all be done without rolling a new\nrelease. In practice, enough bug ﬁxing happens during a dark launch that you will\nalmost certainly upgrade new releases during the dark launch period. However,\n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "384\nChapter 18\nCapacity Planning\nall of these bugs should be invisible to your customer community, and your actual\nlaunch will be the better for having done a dark launch.\n.\nCase Study: Facebook Chat’s Dark Launch\nThe term “dark launch” was coined in 2008 when Facebook revealed the\ntechnique was used to launch Facebook Chat. The launch raised an impor-\ntant issue: how to go from zero to 70 million users overnight without scaling\nissues. An outage would be highly visible. Long before the feature was visible\nto users, Facebook pages were programmed to make connections to the chat\nservers, query for presence information, and simulate message sends without\na single UI element drawn on the page (Letuchy 2008). This gave Facebook an\nopportunity to ﬁnd and ﬁx any issues ahead of time. If you were a Facebook\nuser back then, you had no idea your web browser was sending simulated chat\nmessages but the testing you provided was greatly appreciated. (Section 11.7\nhas a related case study.)\n18.5 Reduce Provisioning Time\nAs discussed in Section 18.1.4, one of the factors inﬂuencing your headroom\nrequirement is resource acquisition and provisioning time. If you can reduce the\nacquisition and provisioning time, you can reduce your headroom, which in turn\nreduces the amount of capital investment that is idle. In addition, faster acquisi-\ntion and provisioning enables faster response to changing demands and can be a\nsigniﬁcant competitive advantage.\nHowever, idle excess capacity also can be a result of being unable to reduce\nthe available resources quickly. Being able to jettison increased capacity, and its\nassociated costs, at will is also a competitive advantage. There are a number of\napproaches to reducing idle excess capacity, and they can often be combined to\nhave greater effect:\n• Lease computers rather than purchase them. There may be a shorter acqui-\nsition time depending on what is in stock, and the ability to terminate a lease\nand return the hardware may be useful. Leasing can be used to shift capacity\ncosts from capital expenditures into operational costs.\n• Use virtual resources that are allocated quickly and have little or no startup\ncosts associated with them. These resources can also be quickly terminated,\nand billing reﬂects only actual usage of the resources.\n",
      "content_length": 2298,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "18.6\nSummary\n385\n• Improve the ordering process for new hardware resources. Preapprove\nbudget decisions and create standardized ordering.\n• Improve installation time. Part of the provisioning time is the time from when\nhardware hits the loading dock to when it is actually in use. Find ways to\nstreamline the actual rack and burn-in.\n• Manage your time. Make it a priority to install new equipment the moment it\narrives. Have no idle hardware. Alternatively, dedicate staff to doing this. Hire\nnon-system administrators (“technicians”) to unbox and rack mount systems.\n• Work with vendors (supply chain management) to reduce ordering time.\n• Place many smaller orders rather than one huge order. This improves par-\nallelism of the system. Vendors may be able to chop up one big order into\nperiodic deliveries and monthly billing. A transition from one huge order\nevery 6 months to 6 monthly orders and deliveries may have billing, capital\ncost, and labor beneﬁts.\n• Automate configuration so that once new hardware is racked, it is soon\navailable for use.\n18.6 Summary\nCapacity planning is the process that ensures services have enough resources\nwhen they are needed. It is challenging to prevent service interruptions due to\nlack of capacity and, simultaneously, preserve capital by adding only the capacity\nrequired at any given time.\nStandard capacity planning is based on current usage and simple rates of\nchange. It assumes future resource needs will be similar to current usage plus two\nkinds of growth. Normal or organic growth is what is expected based on current\ntrends. Planned growth is what is expected due to new initiatives such as mar-\nketing plans. Additional capacity, called headroom, is added to handle short-term\nspikes. Based on the timetable showing lead time (i.e., how long it takes to acquire\nand conﬁgure new resources), capacity schedules can be determined. By reducing\nlead time, capacity planning can be more agile.\nStandard capacity planing is sufﬁcient for small sites, sites that grow slowly,\nand sites with simple needs. It is insufﬁcient for large, rapidly growing sites. They\nrequire more advanced techniques.\nAdvanced capacity planning is based on core drivers, capacity limits of indi-\nvidual resources, and sophisticated data analysis such as correlation, regression\nanalysis, and statistical models for forecasting. Regression analysis ﬁnds corre-\nlations between core drivers and resources. Forecasting uses past data to predict\nfuture needs.\n",
      "content_length": 2483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "386\nChapter 18\nCapacity Planning\nWith sufﬁciently large sites, capacity planning is a full-time job, often done\nby project managers with technical backgrounds. Some organizations employ\nfull-time statisticians to build complex models and dashboards that provide the\ninformation required by a project manager.\nGood capacity planning models can also detect unexpected changes in\nresource needs—for example, a new software release that unexpectedly requires\nmore resources.\nCapacity planning is highly data-driven and uses past data to predict future\nneeds. Launching a brand-new service, therefore, poses a special challenge. Dark\nlaunches and other techniques permit services to gather accurate data before the\nservice is visible to users.\nTo improve cost-effectiveness, reduce the time it takes to provision\nnew resources. Provisioning involves acquiring, conﬁguring, and putting new\nresources into production. Total provisioning time is called lead time. Long lead\ntimes tie up capital. Reducing lead time reduces idle capacity and improves\nﬁnancial efﬁciency.\nCapacity planning is a complex and important part of reliable operations.\nExercises\n1. Describe how standard capacity planning works.\n2. Describe how advanced capacity planning works.\n3. Compare and contrast standard and advanced capacity planning. When is\neach best used?\n4. What are the challenges of launching a new service?\n5. List the resources used by your main application.\n6. Use regression analysis to determine the correlation between two resources.\n7. Create a forecasting model for a service’s capacity needs.\n8. Describe how you would implement a dark launch of a new feature in your\nmain application.\n9. What is a resource regression and why is it important?\n10. Perform a resource regression between your current application release and a\nprevious release. Discuss what you ﬁnd and its implications for your capacity\nplanning.\n11. There are many ways to reduce provisioning time. Which three ways would\nhave the most impact in your environment? (Alternatively, group the methods\nlisted in this chapter by categories of your choosing.)\n12. Why is it desirable to reduce provisioning time?\n",
      "content_length": 2165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Chapter 19\nCreating KPIs\nThe only man I know who\nbehaves sensibly is my tailor;\nhe takes my measurements anew\neach time he sees me. The rest go on\nwith their old measurements and\nexpect me to ﬁt them.\n—George Bernard Shaw\nA startup decides that web site speed is important to the success of its business.\nManagement decides that page load time will be the key performance indicator\n(KPI) that determines employee pay raises at the end of the year. Soon services that\nshared machines are given dedicated resources to avoid any possibility of inter-\nprocess interference. Only the fastest machines are purchased. Many features are\ndelayed as time is dedicated to code optimization. By the end of the year, the KPI\nmeasurements conﬁrm the web pages have extremely fast load times. The goal has\nbeen reached. Sadly, the company has no money for raises because it has spent its\nway out of business.\nMeasurement affects behavior. People change their behavior when they know\nthey are being measured. People tend to ﬁnd the shortest path to meeting a goal.\nThis creates unintended side effects.\nIn this chapter we talk about smart ways to set goals and create KPIs. Managers\nneed to set goals that drive desired behavior to achieve desired results while min-\nimizing the unintended consequences. Done correctly, this enables us to manage\noperations in a way that is more efﬁcient, is fairer, and produces better results.\nSetting KPIs is quite possibly the most important thing that a manager does.\nIt is often said that a manager has two responsibilities: setting priorities and pro-\nviding the resources to get those priorities done. Setting KPIs is an important way\nto verify that those priorities are being met.\n387\n",
      "content_length": 1711,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "388\nChapter 19\nCreating KPIs\nThe effectiveness of the KPI itself must be evaluated by making measurements\nbefore and after introducing it and then observing the differences. This changes\nmanagement from a loose set of guesses into a set of scientiﬁc methods. We measure\nthe quality of our system, set or change policies, and then measure again to see their\neffect. This is more difﬁcult than it sounds.\n19.1 What Is a KPI?\nA key performance indicator is a type of performance measurement used to evalu-\nate the success of an organization or a particular activity. Generally KPIs are used\nto encourage an organization or team to reach a particular goal.\nKPIs should be directly tied to the organization’s strategy, vision, or mission.\nGenerally they come from executive management but often other levels of man-\nagement create KPIs for their own purposes, usually as a way of furthering the\nKPIs relevant to them.\nA well-deﬁned KPI follows the SMART criteria: Speciﬁc, Measurable,\nAchievable, Relevant, and Time-phrased. It is speciﬁc, unambiguously deﬁned\nand not overly broad. It is measurable so that success or failure can be objectively\nquantiﬁed. It is achievable under reasonable circumstances. It is relevant to the suc-\ncess of the organization as a whole, or the project as a whole. It is time-phrased,\nwhich means the relevant time period is speciﬁed.\nGoals should be measurable so that one can unambiguously determine what\nfraction of the goal was achieved, or if it was achieved at all. Things you can count,\nsuch as uptime, disk space, and the number of major features released this month,\nare all measurable. Example measurable goals include the following:\n• Provide 10T of disk space to each user.\n• Page load time less than 300 ms.\n• Fewer than 10 “severity 1” open bugs.\n• Launch 10 major features this month.\n• 99.99 percent service uptime.\nNon-measurable goals cannot be quantiﬁed or do not include a speciﬁc numerical\ngoal. Some examples are shown here:\n• Get better at writing Python code.\n• Get better at failing over to DR systems.\n• Provide more free disk space.\n• Make pages load faster.\n",
      "content_length": 2113,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "19.2\nCreating KPIs\n389\nThese are all good things, of course. They could all be turned into measurable goals.\nHowever, as stated, they are not measurable.\nKPIs go by many different names. Sometimes they are informally called “met-\nrics,” which is true in the sense that a KPI is a kind of metric—the kind used to drive\norganizational behavior. However, that would be like calling oranges “fruit” and\nexpecting people to know you mean a speciﬁc kind of fruit.\n.\nIntel’s OKR System\nIntel uses a related term called OKRs, which stands for “objectives and key\nresults.” OKRs are often used to set goals at the personal, team, division, and\ncompany levels. The key results are often measured via KPIs. The acronym\nOKR was popularized by venture capitalist John Doerr, who brought the\nconcept to Google. Rick Klau’s video “How Google Sets Goals: OKRs” is\nan excellent explanation of OKRs and serves as a tutorial on how to adopt\nGoogle’s OKR system to your team or business (Klau 2012).\n19.2 Creating KPIs\nCreating good KPIs requires serious time and effort. This process has many steps.\nFirst we envision what the world would look like if the goal was met. Next we\ndetermine ways to quantify how close we are to that ideal. This leads to one or\nmore potential KPIs. Then we consider all the ways that people could behave but\nstill match the incentive. Based on that information, we revise the potential KPIs.\nNow we repeat these steps until we have our ﬁnal KPI.\nWhen deﬁned correctly, KPIs can improve a team’s performance by 10 to\n30 percent. The total cost of a typical engineer, including salary, beneﬁts, and other\nexpenses, can be $200,000 or more per year. For an organization with 50 engineers,\nsuch an improvement is worth $1 million to $3 million per year. Most managers\nfacing a $3 million project would dedicate days or weeks of planning to assure its\nproper execution. In terms of return on investment, spending 10 hours to create\nsuch an improvement has a 1:1000 or 1:3000 payoff. Who would turn down such a\nreturn? Yet KPIs are often created with little forethought and the unintended side\neffects negate any beneﬁt.\nThese numbers should be your personal incentive to develop the skills\nrequired to create effective KPIs. Better KPIs are, quite possibly, more important\nthan anything else you say or do.\n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "390\nChapter 19\nCreating KPIs\n19.2.1 Step 1: Envision the Ideal\nPause to imagine what the world would be like if this goal was met perfectly. How\nwould it be different from what it is today? Think in terms of the end result, not\nhow we get there. Exercise your creativity. How would you describe the company,\nproject, or service? How would resources be used?\nOne place to look for inspiration is subteams that are doing well in this area.\nGenerally they won’t have a KPI you can use, but they often have qualities you\nwant to reproduce. Sometimes the quality may be their culture, not a particular\ntangible outcome.\nThe most common mistake we see is managers skipping this ﬁrst step and\ngoing directly to creating a KPI formula. Taking this shortcut is a bad idea. If you\ndon’t ﬁrst decide on the destination, it isn’t likely you’ll get there.\n19.2.2 Step 2: Quantify Distance to the Ideal\nAsk yourself, how can we measure how far we are now from this ideal? We create\nmany candidate KPIs that might measure this distance. The measurement might be\na time duration, a count, the number of times something happens or doesn’t hap-\npen, a quantity, or any other numerical quantiﬁcation. The measurement should\nbe real and repeatable.\nFor example, if the ideal is users having a web experience that is faster than\nwith native software applications on a PC, the way to measure our distance to the\nideal would involve measuring page load time. If the ideal envisions a world with\nno security breaches, the way to measure how far we are from that ideal would be\nthe count of known security breaches detected each month. The ideal might also be\nthat all detected intrusions are investigated within a certain time frame, in which\ncase we would quantify both the time from detection to start of investigation and\nthe duration of the investigation.\n19.2.3 Step 3: Imagine How Behavior Will Change\nFor each potential KPI, try to defeat it. What are all the ways that people could\nbehave but still match the incentive? How could a person maximize his or her\npersonal gain?\nSet aside hope that employees will “just behave.” If they are following your\nformula, they’re behaving. You can’t expect them to read your mind and under-\nstand your intention.\nSalespeople make a good example. If they are selling many products, they\nwill immediately analyze the commission structure and ﬁgure out which product\nto focus on to maximize the use of their time. If they sell two products with the\nsame price and same commission rate but one is easier to sell, the other product\nwill be starved for attention. If they sell 10 products, each with a different price,\n",
      "content_length": 2635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "19.2\nCreating KPIs\n391\ncommission, and likelihood of a successful sale, they will do the math and ﬁgure\nout which products to actively sell; the others will be ignored. This is not cheating,\nbut rather following the rules they’ve been given. This is why some companies\nhave separate sales teams for big customers and small customers. Who would\nspend time trying to make $1,000 deals with small customers when they can make\nmillion-dollar deals with big companies? The sales cycle would have to be 1000\ntimes longer for the big deals to be less efﬁcient. Without dedicated sales teams,\nthe company would miss out on any sales to small companies.\nSimilarly, engineers will examine the KPIs given and follow them as stated.\nAgain, this is not “gaming the system,” but simply following the rules. This is\ndoing what the engineers were told to do. Gaming the system would be falsifying\nlogs or going around the KPIs to beneﬁt themselves. Do not be upset or surprised\nwhen people conform to what you wrote, rather than what you intended.\nReturning to our earlier security example, the easiest way to achieve the goals\nas stated is to turn off any mechanisms that detect such breaches. This is probably\nnot the intended reaction. Thus, it should be revised.\n19.2.4 Step 4: Revise and Select\nBased on what we’ve learned in Step 3, we revise the KPI. We may select one KPI\nover another or loop back to Step 1 and start over.\nSeek conﬁrming and non-conﬁrming data. For example, calculate the KPI\nbased on past or current metrics. If such metrics are not available, test against\nsimulated data.\nSeek the opinions of others. Ask how they would behave under such KPIs. You\nwill get different answers depending on whether the person has a vested interest\nin the KPI. If the person’s work will be judged by the KPI, ﬁlter what he or she says\nbased on the individual’s potential bias toward personal gain or preferring to be\njudged less harshly. If the person would beneﬁt from the KPI’s goal succeeding, he\nor she will have the opposite bias but may not be in tune with the internal processes\nthat can be used to subvert the KPI.\nAsking for feedback on KPIs that have not been announced creates the\npotential that rumors will spread about the new KPI. Like the children’s game\n“Telephone,” where the message becomes more and more misconstrued as it gets\npassed along, the rumor of the new KPI will invariably portray it as harsher as\nit moves from person to person. This will hurt morale and will result in mis-\ninformation that confuses people when the real KPI is announced. The worst\nsituation is where the draft KPI is tossed out but the rumors persist. Therefore\ndiscussing potential KPIs should be done with an understanding of who they can\nbe shared with.\nIt is sometimes possible to test the KPI on one team or one project before apply-\ning it to all the others. This gives us real-world experience with how people react.\n",
      "content_length": 2915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "392\nChapter 19\nCreating KPIs\nAlternatively, it can be employed during a trial period so that people expect some\nrevision before it becomes policy.\nWe don’t have many chances to revise the KPI after it becomes policy. If a KPI\nis announced and has unintended negative side effects, you should modify the KPI\nto ﬁx any bugs. However, if the KPI changes again and again, management looks\nlike they don’t know what they are doing. Employees lose trust. Morale will suffer\ngreatly if the rules keep changing. People will feel like the rug is being pulled out\nfrom under their feet if they have just ﬁnished adjusting their behavior to align\nwith one KPI when it changes and requires opposite adjustments.\nBecause of this, great effort should be put into getting it right the ﬁrst time.\nBecause of the beneﬁts of a well-structured KPI, and the potential damage of a\nbadly engineered KPI, this effort should be given high priority and be taken seri-\nously. The ﬁnal KPI doesn’t have to be simple, but it has to be easy to understand.\nIf it is easy to understand, it will be easy to follow.\nReturning to our security breach example, our last draft contained a bug in\nthat it did not include both the coverage of the intrusion detection system and the\nnumber of intrusions detected. Thus, we revise it to be three KPIs: one that reﬂects\nthe percentage of subnets that are covered by the intrusion detection system, one\nthat reﬂects the total number of intrusions, and one that reﬂects the duration of\ninvestigations.\n19.2.5 Step 5: Deploy the KPI\nThe next step is to deploy the KPI, or institute it as policy. This is mostly a com-\nmunication function. The new KPI must be communicated to the team that is\nresponsible for the KPI, plus key stakeholders, management, and so on.\nDeploying the KPI means making people aware of it as well as putting into\nplace mechanisms to measure the KPI. The KPI should be deployed with the\nassumption that it may require revision in the future, but this should not be an\nexcuse to do a bad job in creating it.\nIf at all possible, the metrics that are used to calculate KPIs should be collected\nautomatically. This might occur via the monitoring system, or it might be done by\nextracting the information from logs. Either way, the process should not require\nhuman intervention. Even if you forget about them, the metrics you need should\nautomatically accumulate for you.\nIf the metrics cannot be collected automatically, there are two actions to take.\nOne is to ﬁnd a way to ensure that the metrics are collected manually. This could be\nan automated email reminder or a repeating calendar entry. The other is to develop\na mechanism so that the collection will be automated. There are no metrics that\ncan’t be collected automatically, only metrics whose collection has not yet been\nautomated.\n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "19.3\nExample KPI: Machine Allocation\n393\nOnce the data is collected, a dashboard should be created. A dashboard is a\nweb page that shows a visualization of the KPI. It may have related metrics and\ninformation useful for drilling down into the data.\nImportant KPIs should have a second dashboard, one that is appropriate for\nlarge displays. Install a large monitor in a hallway or other common area and dis-\nplay this dashboard continuously. Such displays should use large fonts and have\ngraphics that are easy to view from a distance. Such displays serve as an impor-\ntant reminder of the KPIs’ status and can become a point of pride when the KPIs\nare met.\n19.3 Example KPI: Machine Allocation\nSuppose we are developing a KPI to assess the quality of the process by which\nvirtual machines (VMs) are created. This KPI may apply to a public cloud service\nprovider, it may be geared toward a team that creates VMs for an internal cloud\nservice within a large company, or perhaps it is intended just to assess the process\nthat a team uses for creating its own VMs.\n19.3.1 The First Pass\nWe begin with Step 1, Envision the Ideal. In an ideal world, people would get the\nVMs they want as soon as they request them. There would be no delay.\nIn Step 2, Quantify Distance to the Ideal, we simply measure the duration of\ntime from the request to when the VM is created.\nIn Step 3, Imagine How Behavior Will Change, we brainstorm all the ways that\npeople could behave but still match the incentive. We foresee many challenges.\nOne challenge is that people could get very creative about the deﬁnition of\n“start time.” It could be the time when the request is received from the user, or it\ncould be when the creation process actually begins. If requests are made by creating\ntickets in a helpdesk request system, the delay before someone processes the ticket\ncould be very large. If the requests come in via a web portal or API, they may\nbe queued up and processed sequentially. If that wait time is not included in the\nmetric, it would make the team look good, but would not truly reﬂect the service\nusers receive.\nA more realistic indication comes from measuring the end-to-end result from\nthe customer’s perspective. Doing so might inspire the team to move from a ticket-\nbased request system to a self-service portal or API. This would not only replace\nthe human process of re-entering data from a ticket, but also ensure that all the\ninformation needed to complete the process is collected at the very beginning.\nThus it avoids the back-and-forth communication that might be required to collect\ninformation the user forgot or didn’t know to include in the ticket.\n",
      "content_length": 2656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "394\nChapter 19\nCreating KPIs\nPeople could also demonstrate creativity in how they deﬁne when the request\nis “complete.” The incentive is to interpret completion time as soon as possible. Is\nthe VM created when the VM Manager allocates RAM, disk, and other resources to\ncreate the empty virtual machine, or when that virtual machine has loaded its oper-\nating system? If a different team is responsible for the OS installation mechanism,\ncan we be “complete” once the OS installation starts, whether or not it ﬁnishes suc-\ncessfully? To the user, a failed, half-installed VM is worthless, but a mistated KPI\nmight permit it.\nThe team could rationalize any of these end points to be the end time. Thus,\nwe go back to Step 1 and use what we have learned to do better.\n19.3.2 The Second Pass\nOur original deﬁnition was a very narrow deﬁnition of the end result. Let’s broaden\nthe deﬁnition and focus on what the user sees as the desired result.\nTo the user, the end result is that the VM is ready for use for the purpose the\nuser intended.\nIdeally, the user always receives the VM that has been requested: it has the\nright name, size (amount of RAM, disk, and vCPU), operating system, and so on.\nThus, our ideal world implies that the right information is gathered from the user\nand the user receives a VM that works as requested.\nPerhaps some roadblocks might prevent the customer from using the machine\nright away. These should be included in the metric. For example, to use the machine\nrequires DNS changes to be propagated, access controls to be implemented, and\nso on.\nAlso consider the use of company resources. Can users request an inﬁnite\nnumber of VMs? Who pays for them? Billing for resources is often an effective\nway to lead users to restrain their use of a resource. We’ll use that.\nTherefore we revise our ideal-world deﬁnition as follows: Users get a usable\nVM, as they requested, as quickly as possible, with billing arrangements estab-\nlished.\nContinuing our example into Step 2, we deﬁne the start time as when the\nrequest is initially received from the user. The end time is when the VM is usable by\nthe requester. We can deﬁne “usable” as the user being able to log into the machine.\nThis automatically includes DNS propagation and access control as well as issues\nwe are unaware of.\nThe draft KPI becomes:\nThe 90th percentile creation time, which starts when the request is created, and ends\nwhen the user is able to log into the machine.\nWe use a percentile instead of an average because, as discussed in Section 17.5,\naverages can be misleading. A single long-delayed creation would unfairly ruin\nan average.\n",
      "content_length": 2628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "19.3\nExample KPI: Machine Allocation\n395\nAn ideal world has no resource shortages. Anytime a new VM is needed, there\nwould be capacity to allocate one. New capacity would come online just in time to\nfulﬁll any request. We could add a metric related to capacity planning but it is bet-\nter to keep the KPI at a high level of abstraction, rather than micro-managing how\nthe service is run. We are concerned with the end result. If capacity planning is not\ndone right, that fact will surface due to the KPI we have constructed as requests\nare held while waiting for new capacity to come on line. That said, if capacity plan-\nning becomes an issue, we can later establish KPIs speciﬁcally related to capacity\nplanning efﬁciency.\nIn Step 3, we imagine how behavior will change. If a cancelled request does\nnot count toward the KPI, we could simply cancel any request that takes too long.\nFor example, if requests are made via a helpdesk ticket, if the user doesn’t supply\nall the information required to complete the task, we could cancel the request and\nlet the user know what information we need when the user creates the new ticket.\nThis would be terrible behavior but would improve the KPI.\nIf the OS installation step is unreliable and requires multiple restarts, this\nwould delay the total creation time. To improve our KPI numbers, the operators\ncould simply cancel jobs that fail rather than retry them. Again, this would beneﬁt\nthe KPI but not the user.\nWe could work around this loophole by adding signiﬁcant complexity to the\nKPI. However, often it is easier to prevent bad behavior by letting people know\nthey are being watched. First, we can publish the number of customer-initiated\ncancellation requests. This permits management to quietly watch for shenanigans.\nSecond, we can privately agree with the team’s manager that such behavior will\nbe discouraged. Executive management can assist by creating an environment with\nhigh standards, including only hiring managers who wouldn’t tolerate such behav-\nior. Such managers would, for example, notice that the system doesn’t log who\ncancelled a request, and require that this be changed so that the KPI can properly\nmeasure customer-initiated cancellations versus cancellations from operational\nstaff.\nStep 4, Revise and Select, results in the following KPI:\nThe 90th percentile creation time, measured from when the request is received from\nthe user until the user is able to log into the machine. Manual and automatic retries\nafter a failure are counted as part of the original request, not as separate requests.\nRequests that are outright canceled will be logged and investigated if within a week\nthere are more than 5 operator-initiated cancellations or if more than 1 percent of all\nrequests end in customer-initiated cancellation.\nStep 5 deploys the KPI. The KPI is communicated to key stakeholders. Mea-\nsurements required to calculate the KPI plus the computation itself should be\nautomated and presented in a dashboard. In our example, the system might gener-\nate the metric we require or it might timestamp the request and completion times,\n",
      "content_length": 3108,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "396\nChapter 19\nCreating KPIs\nrequiring some post-processing to correlate the two and calculate the duration.\nHowever the metric is gathered, it should then be stored and made available to\nwhatever data visualization system is in use so that a graphical dashboard can be\ncreated.\n19.3.3 Evaluating the KPI\nA few weeks after deployment, the initial results should be audited for unintended\nnegative side effects.\nIn our example, depending on how often VMs are created, the resulting KPI\nmeasurements might be reviewed daily, weekly, or monthly. These results might\ninspire additional metrics to isolate problem areas. The VM creation process is\nmade up of many steps. By measuring the wait time before each step, as well as\nthe duration of each step, opportunities for improvement can be easily found.\nFor example, measuring lead time might reveal a long span of time between\nwhen a ticket is ﬁled and when the actual creation begins, indicating that a self-\nservice request system would have a large beneﬁt. Lead time might reveal that\nrequests are often delayed for weeks waiting for new resources to be installed,\nindicating that capacity planning needs improvement. Collecting data on how\nmany times a task fails and is retried might indicate that the OS installation pro-\ncess is unstable and should be improved. Perhaps DNS propagation delays can\nbe tightened or the OS installation process can be made faster using image-based\ninstallations. Monitoring network utilization might ﬁnd an overloaded link that, if\nimproved, could result in faster installation time in general. By analyzing the types\nof requests, it may be determined that a few standard-size VMs can be pre-created\nand simply handed out as needed.\nAll these changes are viable. By taking measurements, we can predict how\neach one might improve the KPI. By collecting KPI measurements before and after\nchanges, we can measure the actual improvement.\n19.4 Case Study: Error Budget\nBenjamin Treynor Sloss, Vice President of Engineering at Google, revealed a highly\nsuccessful KPI called Google Error Budget. The goal was to encourage high uptime\nwithout stiﬂing innovation, and to encourage innovation without encouraging\nundue risk.\n19.4.1 Conflicting Goals\nThere is a historic conﬂict between developers and operations teams. Developers\nwant to launch new features; operations teams want stability.\n",
      "content_length": 2367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "19.4\nCase Study: Error Budget\n397\nDevelopers are in the business of making change. They are rewarded for new\nfeatures, especially ones that are highly visible to the end customers. They would\nprefer to have each feature they create pushed into production as fast as possible\nso as not to delay gratiﬁcation. The question they get the most from management\nis likely to be, “When will it ship?”\nOperations people are in the business of stability. They want nothing to break\nso they don’t get paged or otherwise have a bad day. This makes them risk averse.\nIf they could, they would reject a developer’s request to push new releases into\nproduction. If it ain’t broke, don’t ﬁx it. The question they get the most from\nmanagement is likely to be, “Why was the system down?”\nOnce a system is stable, operations would prefer to reject new software\nreleases. However, it is culturally unacceptable to do so. Instead, rules are cre-\nated to prevent problems. They start as simple rules: no upgrades on Friday; if\nsomething goes wrong, we shouldn’t have to spend the weekend debugging it.\nThen Mondays are eliminated because human errors are perceived to increase then.\nThen early mornings are eliminated, as are late nights. More and more safeguards\nare added prior to release: 1 percent tests go from being optional to required. Basi-\ncally operations never says “no” directly but enough rules accumulate that “no” is\nvirtually enforced.\nNot to be locked out of shipping code, developers work around these rules.\nThey hide large amounts of untested code releases behind ﬂag ﬂips; they encode\nmajor features in conﬁguration ﬁles so that software upgrades aren’t required, just\nnew conﬁgurations. Workarounds like these circumvent operations’ approvals and\ndo so at great risk.\nThis situation is not the fault of the developers or the operations teams. It is\nthe fault of the manager who decreed that any outage is bad. One hundred percent\nuptime is for pacemakers, not web sites. The typical user is connecting to the web\nsite via WiFi, which has an availability of 99 percent, possibly less. This dwarfs any\ngoal of perfection demanded from on high.\n19.4.2 A Unified Goal\nTypically four 9s (99.99 percent) availability is sufﬁcient for a web site. That leaves\na “budget” of 0.01 percent downtime, a bit less than an hour each year (52.56\nminutes). Thus the Google Error Budget was created. Rather than seeking perfect\nuptime, a certain amount of imperfection is budgeted for each quarter. Without\npermission to fail, innovation is stiﬂed. The Error Budget encourages risk taking\nwithout encouraging carelessness.\nAt the start of each quarter, the budget is reset to 13 minutes, which is about\n0.01 percent of 90 days. Any unavailability subtracts from the budget. If the bud-\nget has not been exhausted, developers may release as often as they want. When\n",
      "content_length": 2848,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "398\nChapter 19\nCreating KPIs\nthe budget is exhausted, all launches stop. An exception is made for high-priority\nsecurity ﬁxes. The releases begin again when the counter resets and there is once\nagain a 13-minute budget in place.\nAs a result, operations is no longer put into the position of having to decide\nwhether to permit a launch. Being in such a position makes them “the bad guys”\nevery time they say no, and leads developers to think of them as “the enemy to be\ndefeated.” More importantly, it is unfair to put operations in this position because\nof the information asymmetry inherent in this relationship. Developers know the\ncode better than operations and therefore are in a better position to perform testing\nand judge the quality of the release. Operations staff, though they are unlikely to\nadmit it, are not mind readers.\n19.4.3 Everyone Benefits\nFor developers, the Error Budget creates incentives to improve reliability by offer-\ning them something they value highly: the opportunity to do more releases. This\nencourages them to test releases more thoroughly, to adopt better release prac-\ntices, and to invest effort in building frameworks that improve operations and\nreliability. Previously these tasks might have been considered distractions from\ncreating new features. Now these tasks create the ability to push more features.\nFor example, developers may create a framework that permits new code to\nbe tested better, or to perform 1 percent experiments with less effort. They are\nencouraged to take advantage of existing frameworks they may not have con-\nsidered before. For example, implementation of lame-duck mode, as described in\nSection 2.1.3, may be built into the web framework they use, but they have simply\nnot taken advantage of it.\nMore importantly, the budget creates peer pressure between developer teams\nto have high standards. Development for a given service is usually the result of\nmany subteams. Each team wants to launch frequently. Yet one team can blow the\nbudget for all teams if they are not careful. Nobody wants to be the last team to\nadopt a technology or framework that improves launch success. Also, there is less\ninformation asymmetry between developer teams. Therefore teams can set high\nstandards for code reviews and other such processes. (Code reviews are discussed\nin Section 12.7.6.)\nThis does not mean that Google considers it okay to be down for an hour\neach year. If you recall from Section 1.3, user-visible services are often composed\nof the output of many other services. If one of those services is not responding,\nthe composition can still succeed by replacing the missing part with generic ﬁller,\nby showing blank space, or by using other graceful degradation techniques as\ndescribed in Section 2.1.10.\n",
      "content_length": 2767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "Exercises\n399\nThis one KPI has succeeded in improving availability at Google and at the\nsame time has aligned developer and operations priorities, helping them work\ntogether. It removes operations from the “bad guy” role of having to refuse\nreleases, and it gives developers an incentive to balance time between adding new\nfeatures and improving operational processes. It is simple to explain and, since\navailability is already tightly monitored, easy to implement. As a result, all of\nGoogle’s services beneﬁt.\n19.5 Summary\nManaging by using KPIs is a radical departure from traditional IT management.\nIt is effective because it sets goals and permits the smart people whom you hired\nto work out how to achieve them. Those people are closer to the task and more\nknowledgable about the technical details at hand, making them better suited to\ninventing ways to achieve the goal.\nCreating effective KPIs is difﬁcult—and it should be. It yields a huge return on\ninvestment when done right. Something that has a 1:1000 payoff is not going to be\neasy. KPIs should be speciﬁc, measurable, achievable, relevant, and time-phrased.\nPoorly written KPIs have unintended consequences when people follow the rules\nas written rather than what you intended. It is important to think through all the\nways that the KPI may be interpreted, and the actions that people might take to\nimprove their performance on this metric.\nSpend time up front examining and amending the KPI to make sure that the\nKPI will most likely give the result you want. Do not over-complicate the KPI. If\nyou suspect that the KPI might trigger an adverse behavior, measure and publish\nthat behavior as well, and make sure that your managers know to watch out for\nand discourage it.\nRemember that the KPI should be achievable. One hundred percent\navailability is not achievable, unless the deﬁnition of “availability” is adjusted\nto cover more than you intended. However four 9s (99.99 percent) is achievable.\nGoogle’s Error Budget KPI successfully uses that target to achieve the desired\nresult: a stable service with innovative new features deployed frequently. It is an\nexcellent example of how a good KPI can beneﬁt everyone.\nExercises\n1. What is a KPI?\n2. What are the SMART criteria? Brieﬂy describe each one.\n3. Give examples of unintended side effects of KPIs.\n",
      "content_length": 2324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "400\nChapter 19\nCreating KPIs\n4. What are the steps for creating KPIs? Evaluate which steps are the most\ndifﬁcult and provide justiﬁcation as to why they are difﬁcult.\n5. Which KPIs do you track in your environment and why?\n6. Create an effective KPI for assessing a service in your environment. After\ncreating it, have three other people tell you how they would maximize their\npersonal gain. Revise the KPI. Report on the ideal (Step 1), the initial KPI(s),\npeople’s reactions, and the ﬁnal KPI.\n7. How does the KPI you created address each of the SMART criteria?\n8. When managed through KPIs, why do people follow what is written rather\nthan what was intended? Is this good or bad?\n9. The example in Section 19.3 always discusses measuring the end-to-end time\nbased on the customer’s perspective. Would there be value in measuring the\ntime from when the VM creation starts to when it is usable?\n10. How would you modify the KPI created in Section 19.3 if after the request was\nmade, the requester’s manager had to approve the request?\n",
      "content_length": 1036,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "Chapter 20\nOperational Excellence\nA company can seize\nextraordinary opportunities\nonly if it is very good at\nthe ordinary operations.\n—Marcel Telles\nThis chapter is about measuring or assessing the quality of service operations. It\nproposes an assessment tool and gives examples of how to use it to evaluate an\nindividual service, a team, or multiple teams.\nIn Chapter 19, we discussed how to create KPIs that drive desired behavior to\nachieve speciﬁc goals. This chapter describes an assessment system that evaluates\nthe degree of formality and optimization of processes—that is, whether processes\nare ad hoc, or formal, or actively optimized. This assessment is different than KPIs\nin that it gauges teams on a more generic level, one that is more comparable across\nteams or across services within a team.\nSuch assessments help identify areas of improvement. We can then make\nchanges, reassess, and measure the improvement. If we do this periodically, we\ncreate an environment of continuous improvement.\n20.1 What Does Operational Excellence Look Like?\nWhat does great system administration look like? Like art and literature, it is difﬁ-\ncult to deﬁne other than to say you know it when you see it. This ambiguity makes\nit difﬁcult to quantitatively measure how well or poorly a system administration\nteam is performing.\nHigh-performing organizations have smooth operations, well-designed poli-\ncies and practices, and discipline in what they do. They meet or exceed the needs\nof their customers and delight them with innovations that meet future needs often\nbefore such needs ever surface. The organization is transparent about how it plans,\n401\n",
      "content_length": 1650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "402\nChapter 20\nOperational Excellence\noperates, provides services, and handles costs or charge-backs to customers. The\nvast majority of customers are happy customers. Even dissatisﬁed customers feel\nthey have a voice, are heard, and have a channel to escalate their issues. Everyone\nfeels the operations organization moves the company forward. Its funding reﬂects\na reasonable budget for the work the organization does. The organization makes\nits successes visible and, more importantly, is honest and forthright when it comes\nto discussing its own faults. The organization is constantly improving. Outages\nand escalated issues result in action plans that reduce future occurrences of that\nproblem. The world is constantly changing, and the organization incorporates new\ntechnologies and techniques to improve its inner workings as well as the services\nit provides.\nOperations organizations seem to fall into three broad categories or strata: the\ngreat ones, the ones that want to be great, and the ones that don’t even know what\ngreat is.\nWe estimate that 5 to 10 percent of all operations teams fall into this ﬁrst cate-\ngory. They know and use the best practices of our industry. Some even invent new\nones. The next 25 to 30 percent know that the best practices exist but are strug-\ngling to adopt them. The remaining super-majority do not even know these best\npractices exist.\nScience ﬁction writer William Gibson famously said, “The future is already\nhere—it’s just not very evenly distributed.” Likewise, the knowledge of how to be\na great system administration team is here—it’s just not very evenly distributed.\n20.2 How to Measure Greatness\nMeasuring the quality of an operations team is extremely difﬁcult. Other aspects of\noperations are easy to measure. For example, size can be measured by counting the\nnumber of team members, the number of services provided, or the dollars spent\nper year. Scale can be measured by counting the number of machines, the amount\nof storage, the total bandwidth used, and so on. We can measure efﬁciency using\ncost ratios.\nAlas, quality is not so easy to measure.\nImagine for a moment that we could measure quality. Imagine we had a stan-\ndard way to rate the quality of an operations team with a simple value on a scale of\n0 to 1000. Also imagine that we could, possibly by some feat of magic, rank every\noperations organization in the world.\nIf we could do that, we could line all of these organizations up from “best”\nto “worst.” The potential for learning would be incredible. We could observe what\nthe top 50 percent do differently from the bottom 50 percent. Alternatively, a single\norganization could study the organizations ranked higher for inspiration on how\nto improve.\n",
      "content_length": 2724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "20.3\nAssessment Methodology\n403\nAlas, there is no such single measurement. Operations is just too complex.\nTherefore the measurement, or assessment, must reﬂect that complexity.\nAssessments sound a lot like the grades we received in school, but the concept\nis very different. A student assessment evaluates an individual student’s learning\nand performance. Grades assess learning, but they also incorporate attendance,\nparticipation, and effort. An assessment is more focused.\nAn assessment of a service is an evaluation based on speciﬁc criteria related\nto process maturity. It is not an evaluation of whether the service is popular, has\nhigh availability, or is fast. Not all services need to be popular, highly available, or\nfast. In contrast, all services need good processes to achieve whatever goals they do\nhave. Therefore we assess process because good processes are a roadmap to success.\n20.3 Assessment Methodology\nThis assessment methodology is a bottom-up assessment. A service is evaluated\non eight attributes, called operational responsibilities (OR). Each OR is assessed\nto be at one of ﬁve levels, with 5 being best. If assessment is done periodically, one\ncan see progress over time. A weighted average can be used to roll up the eight\nindividual assessments to arrive at a single number representing the service.\nA team performs this assessment on each service. A team can be assessed using\nthe weighted average of the services it provides. Teams can then be compared by\nstack rank. Teams can seek to improve their rank by identifying problem areas to\nwork on. Best practices of high-ranking teams can be identiﬁed and shared.\nThe eight core ORs are geared toward service management and do not ﬁt well\nfor transactional IT services such as a helpdesk or other front-of-house, tier 1, or\nother customer-facing service center.\n20.3.1 Operational Responsibilities\nWe have identiﬁed eight broad categories of operational responsibilities that most\nservices have. Some may be more or less important for a particular service. Your\nteam may emphasize or de-emphasize certain ORs by using a weighted average\nwhen performing roll-ups. Teams may also choose to add more ORs if need be.\nThe eight common operational responsibilities are as follows:\n• Regular Tasks (RT): How normal, non-emergency, operational duties are\nhandled; that is, how work is received, queued, distributed, processed, and\nveriﬁed, plus how periodic tasks are scheduled and performed.\n• Emergency Response (ER): How outages and disasters are handled. This\nincludes technical and non-technical processes performed during and after\noutages (response and remediation). This is the stuff of Chapters 6, 14, and 15.\n",
      "content_length": 2692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "404\nChapter 20\nOperational Excellence\n• Monitoring and Metrics (MM): Collecting and using data to make decisions.\nMonitoring collects data about a system. A metric uses that data to measure\na quantiﬁable component of performance. This is the stuff of Chapters 16, 17,\nand 19.\n• Capacity Planning (CP): Determining future resource needs. Capacity plan-\nning involves the technical work of understanding how many resources are\nneeded per unit of growth, plus non-technical aspects such as budgeting,\nforecasting, and supply chain management. This is the stuff of Chapter 18.\n• Change Management (CM): Managing how services are purposefully\nchanged over time. This includes the service delivery platform and how it is\nused to create, deliver, and push into production new application and infra-\nstructure software. This includes ﬁrmware upgrades, network changes, and\nOS conﬁguration management.\n• New Product Introduction and Removal (NPI/NPR): Determining how new\nproducts and services are introduced into the environment and how they\nare deprecated and removed. This is a coordination function. Introducing\nand removing a service or product from an environment touches on multiple\nteams. Removing it involves tracking down the current users and managing\nthe migration away from the service so it can be eliminated. For example, it\nmay involve coordinating all teams that are touched by introducing a new\nbrand of hardware into an ecosystem, launching an entirely new service, or\ndecommissioning all instances of an old hardware platform.\n• Service Deploy and Decommission (SDD): Determining how instances of an\nexisting service are created and how they are turned off (decommissioned).\nAfter a service is introduced to an environment, it is deployed many times.\nAfter serving their purpose, these deployments are decommissioned. Exam-\nples include turning up a new datacenter, adding a replica of a service, or\nadding a replica of a database.\n• Performance and Efficiency (PE): Measuring how well a service performs\nand how cost-effectively resources are used. A running service needs to have\ngood performance without wasting resources. Examples include managing\nutilization or power efﬁciency and related costs.\nThe difference between NPI, SDD, and CM is subtle. NPI is how something is\nlaunched for the ﬁrst time. It is the non-technical coordinating function for all\nrelated processes, many of which are technical. SDD is the technical process of\ndeploying new instances of an existing item, whether it is a machine, a server, or a\nservice. It may include non-technical processes such as budget approval, but these\nare in support of the technical goal. CM is how upgrades and changes are man-\naged, either using a software deployment platform or an enterprise-style change\nmanagement review board.\n",
      "content_length": 2801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "20.3\nAssessment Methodology\n405\nSee Appendix A for more detailed descriptions of these, plus additional ORs\nspeciﬁc to certain chapters of this book.\n20.3.2 Assessment Levels\nThe rating, or assessment, uses a scale of 1 to 5, based on the Capability Maturity\nModel (CMM). The CMM is a tool designed to measure the maturity of a capability\nor responsibility.\nThe term “maturity” indicates how formally or informally an operational\naspect is practiced. The formality ranges from ad hoc and improvised, to having a\ndocumented process, to measuring the results of the process, to actively improving\nthe system based on those measurements.\nThe CMM deﬁnes ﬁve levels:\n• Level 1, Initial: Sometimes called Chaotic. This is the starting point for a new\nor undocumented process. Processes are ad hoc and rely on individual heroics.\n• Level 2, Repeatable: The process is at least documented sufﬁciently such that\nit can be repeated with the same results.\n• Level 3, Defined: Roles and responsibilities of the process are deﬁned and\nconﬁrmed.\n• Level 4, Managed: The process is quantitatively managed in accordance with\nagreed-upon metrics.\n• Level 5, Optimizing: Process management includes deliberate process opti-\nmization/improvement.\nLevel 1: Initial\nAt this level, the process is ad hoc. Results are inconsistent. Different people do\ntasks in different ways, usually with slightly different results. Processes are not\ndocumented. Work is untracked and requests are often lost. The team is unable\nto accurately estimate how long a task will take. Customers and partners may be\nhappy with the service they receive, but there is no evidence-based determination\nat this time.\nLevel 2: Repeatable\nAt this level, the process has gone from being ad hoc to repeatable. The steps are\ndeﬁned in such a way that two different people can follow them and get the same\nresults. The process is documented with no missing steps. The end results are rel-\natively consistent. This is not to say that errors don’t happen; after all, nothing is\nperfect. Because there is little measurement at this level, we may not know how\noften work is defective.\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "406\nChapter 20\nOperational Excellence\nLevel 3: Defined\nAt this level, the roles and responsibilities of the process are deﬁned and conﬁrmed.\nAt the previous level, we learned what needed to be done. At this level, we know\nwho is responsible for doing it, and we have deﬁnitions of how to measure cor-\nrectness. The correctness measurements might not be collected, but at least the\npeople involved know what they are. Each step has a series of checks to ﬁnd errors\nas they happen rather than waiting until the end of the process to ﬁnd out what\nwent wrong. Duplication of effort is minimized. If we need to increase the rate of\nproduction, we can duplicate the process or add more capacity.\nLevel 4: Managed\nAt this level, aspects of the process are measured. How long each step takes is mea-\nsured, including what portion of that time involved waiting to begin the process\nitself. Measurements include how often the process is done each month, how often\nit is performed without error, and how many times an exceptional case arises that\nrequires special treatment and ad hoc processes. Variation is measured. These mea-\nsurements are collected automatically. There is a dashboard that shows all of these\nmeasurements. It is easy to spot bottlenecks. Postmortems are published within a\nspeciﬁc amount of time after an exception. Exceptions are cataloged and periodi-\ncally reviewed. Requests to change a process are justiﬁed using measurement data\nto show that there is a problem. Capacity needs are predicted ahead of time.\nLevel 5: Optimizing\nAt this level, the measurements and metrics are being used to optimize the entire\nsystem. Improvements have been made at all the previous levels but the improve-\nments made here are prompted by measurements and, after the change is made, the\nsuccess is evaluated based on new measurements. Analyzing the duration of each\nstep exposes bottlenecks and delays. Measurements indicate month-over-month\nimprovements. We can stress-test the system to see what breaks, ﬁx it, and then\nrun the system at the new level.\n.\nDefined versus Managed\nPeople often have a difﬁcult time conceptualizing the subtle difference\nbetween Levels 3 and 4. Consider the example of assessing the load bal-\nancer aspect of a service. Load balancers can be used to improve capacity or\nresiliency, or both. Resiliency requires the system to run with enough spare\ncapacity to withstand a failed backend. (See Section 6.3.)\nIf there is a written policy that the load balancer is used for resiliency, this\nis a Level 3 behavior. If there exists monitoring that determines the current\n",
      "content_length": 2594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "20.4\nService Assessments\n407\n.\nlevel of redundancy (N + 0, N + 1, and so on), this is Level 4 behavior. In this\ncase, Level 4 is signiﬁcantly more difﬁcult to achieve because it requires a lot\nof effort to accurately determine the maximum capacity of a backend, which\nis required to know how much spare capacity is available.\nSomewhat surprisingly, we have found many situations where there is\nno clear sense of how a load balancer is being used, or there is disagreement\namong team members about whether the role of a load balancer is to improve\ncapacity or to improve resiliency. This demonstrates Level 1 behavior.\n20.3.3 Assessment Questions and Look-For’s\nTo perform an assessment for an operational responsibility, describe the current\npractices in that area. Based on this description, evaluate which level describes the\ncurrent practices.\nTo help this process, we have developed a standard set of questions to ask to\nhelp form your description.\nWe’ve also developed a set of look-for’s for each level. A look-for is a behavior,\nindicator, or outcome common to a service or organization at a particular level. In\nother words, it is “what a level looks like.” If you read a description and it “sounds\nlike you’re talking about where I work,” then that’s a good indication that your\norganization is at that level for that service.\nFor example, in Regular Tasks, at Level 1 there is no playbook of common\noperational duties, nor is there a list of what those duties are. At Level 3 those\nduties have been deﬁned and documented.\nLook-for’s are not checklists. One does not have to demonstrate every look-\nfor to be assessed at that level. Some look-for’s are appropriate only for certain\nsituations or services. Look-for’s are simply signals and indicators, not steps to\nfollow or achievements to seek out.\nAppendix A lists operational responsibilities, questions, and look-for’s for\neach level. Take a moment to ﬂip there and review some of them. We’ll wait.\n20.4 Service Assessments\nTo put this evaluation process into action, staff must periodically perform an\nassessment on each major service or group of related services they provide.\nThe assessment is used to expose areas for improvement. The team brainstorms\nways to ﬁx these problems and chooses a certain number of projects to ﬁx the\nhighest-priority issues. These become projects for the new quarter.\nThe team then repeats the process. Services are assessed. The assessment\ninspires new projects. The projects are worked on. The services are assessed. The\n",
      "content_length": 2520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "408\nChapter 20\nOperational Excellence\nprocess then begins again. Teams that work this way beneﬁt from having a struc-\nture in place that determines projects. They always know what they should be\nworking on.\n20.4.1 Identifying What to Assess\nFirst, identify the major services that your organization provides. A web-based\nservice might identify the major components of the service (major feature groups\nserved by software components) and infrastructure services such as networks,\npower, cooling, and Internet access. An enterprise IT organization might iden-\ntify the major applications provided to the company (e.g., email, ﬁle storage,\ncentralized compute farm, desktop/laptop ﬂeet management), plus infrastructure\nservices such as DNS, DHCP, ActiveDirectory/LDAP, NTP, and so on. Smaller\nsites may group services together (DNS, DHCP, and ActiveDirectory/LDAP might\nbe “name services”), and larger sites may consider each individual component its\nown service. Either way, construct your list of major services.\nFor each service, assess the service’s eight operational responsibilities. Each\nsection in Appendix A lists questions that will help in this assessment. These\nquestions are generic and should apply to most services. You may wish to add\nadditional questions that are appropriate for your organization or for a particular\nservice. It is important to use the same questions for each assessment so that the\nnumbers are comparable. Make a reference document that lists which questions\nare used.\n20.4.2 Assessing Each Service\nDuring each assessment period, record the assessment number (1 through 5) along\nwith notes that justify the assessment. Generally these notes are in the form of\nanswers to the questions.\nFigure 20.1 shows an example spreadsheet that could be used to track the\nassessment of a service. The ﬁrst column lists the eight operational responsibilities.\nThe other columns each represent an assessment period. Use a different sub-sheet\nfor each service. Use the spreadsheet’s “insert comment” feature to record notes\nthat justify the assessment value.\nIt is a good idea to list the responsibilities in an order that indicates their impor-\ntance to that service. For example, a service that does not grow frequently is less\nconcerned with capacity planning, so that responsibility might be listed last.\nColor the squares red, orange, yellow, green, and blue for the values\n1 through 5, respectively. This gives a visual indication and creates a “heat map”\nshowing progress over time as the colors change.\nFor the person responsible for a service, this tool is a good way to help evaluate\nthe service as well as track progress.\n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "20.4\nService Assessments\n409\n.\nFigure 20.1: Assessment of a service\n.\nFigure 20.2: Roll-up to assess a team\n20.4.3 Comparing Results across Services\nFigure 20.2 shows an example spreadsheet being used to roll up the assessments\nto compare the services. The services should be listed by order of importance.\nEach number represents all eight assessments for that service. The number may\n",
      "content_length": 385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "410\nChapter 20\nOperational Excellence\nbe a weighted average or simply the mathematical “mode” (the most common\nnumber). Whatever method you use, be consistent.\nBoth of these spreadsheets should be ﬁlled out by the team as a group exercise.\nThe manager’s role is to hold everyone to high standards for accuracy. The man-\nager should not do this assessment on his or her own and surprise the team with\nthe result. A self-assessment has an inherent motivational factor; being graded by\na manager is demotivating at best.\n20.4.4 Acting on the Results\nWe can now determine which services need the most improvement and, within\neach service, identify the problem areas.\nDetermine which projects will ﬁx the areas that are both low assessment and\nfor which improvements would have a large impact. That is, focus on the most\nneeded improvements for the most important services.\nLevel 3 is often good enough for most services. Unless it is directly revenue\ngenerating or has highly demanding requirements, generally one should achieve\n“a solid 3” across most responsibilities before expending effort on projects that\nwould result in achieving Level 4 or 5.\nWork on the selected projects during the next quarter. At the start of the\nnext quarter, repeat the assessment, update the spreadsheet, and repeat the pro-\ncess.\n20.4.5 Assessment and Project Planning Frequencies\nThe frequencies of assessments and project planning cycles do not have to be the\nsame. Plan for monthly assessments and quarterly project cycles. Monthly assess-\nment is a good frequency to track progress. More-frequent assessments become a\nwaste of time if very little measurable progress will have happened in that interval.\nLess-frequent assessments may make it difﬁcult to spot new problems quickly.\nThe project cycle may be quarterly to sync up with performance review cycles\nor simply because three months is a reasonable amount of time for a project to be\ncompleted and show results. If the project selection occurs too infrequently, it may\nslow the cadence of change. If the project cycle is too frequent, it may discourage\nlarge, meaningful projects.\nThe longer the gap between assessments, the larger a burden they become. If\nthe cycle is monthly, a meeting that lasts an hour or two can generally complete the\nassessment. A yearly cycle makes assessment “a big deal” and there is a temptation\nto stop all work for a week to prepare for day-long assessment meetings that ana-\nlyze every aspect of the team. Such annual assessments become scary, unhelpful,\nand boring, and they usually turn into a bureaucratic waste of time. This is another\ncase of small batches being better.\n",
      "content_length": 2647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "20.5\nOrganizational Assessments\n411\n20.5 Organizational Assessments\nAs an executive or manager responsible for many services, this assessment system\nis a good way to guide the team’s direction and track progress. These assessments\nalso clearly communicate expectations to team members.\nThe people responsible for the service should do the assessments as a group.\nWhen people are allowed to do their own assessments, it motivates them to take on\nrelated improvement projects. You will be surprised at how often someone, seeing\na low assessment, jumps at the chance to start a project that ﬁxes related problems,\neven before you suggest it.\nYour role as manager is to hold the team to high standards for accuracy and\nconsistency as they complete the assessment. The rubric used should be consistent\nacross all services and across all teams. If you use the assessment questions and\nlook-for’s in Appendix A, revise them to be better suited to your organization.\nLarger organizations have many operational teams. Use this assessment tool\nwith all teams to spot problem areas needing resources and attention, track\nprogress, and motivate healthy competition. Senior managers responsible for\nmultiple teams should work together to assure that rubrics are used consistently.\nFigure 20.3 shows an example spreadsheet that could be used to roll up the\nassessments of many teams so they can be compared. Notice that teams 1, 4, and 5\nhave consistently improved over time; team 4’s progress has been slower. Team 3\nhad a bad year and is just starting to make progress. This team may need extra\nattention to assure the progress continues. Team 2’s progress is erratic, ﬂipping\nbetween Levels 3 and 4 with a small trend upward.\nFigure 20.3: Roll-up to compare teams\n",
      "content_length": 1753,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "412\nChapter 20\nOperational Excellence\nHigh-performing employees will see such comparisons as a source of pride\nor inspiration to do better. Teams with high assessments should be encouraged to\nshare their knowledge and best practices.\n20.6 Levels of Improvement\nEach CMM level builds on the next. Level 2 contains the seeds that enable Level 3;\nLevel 3 contains the seeds that enable Level 4. For this reason organizations pass\nthrough the levels in order and levels cannot be skipped.\nIt is not important to get to Level 5 for all operational responsibilities of all\nservices. It would be a waste of resources to achieve Level 5 for a service that is little\nused and has low priority. In fact, it would be professionally negligent to expend\nthe resources required to achieve a Level 5 assessment on a low-priority service\nwhen more important services need improvement.\nLevel 3 assessment is designed to be good enough for most services. Going\nabove that level should be reserved for high-priority services such as revenue-\ngenerating services or services that are particularly demanding.\nWhen setting organizational goals, focus on ﬁxing a problem, rather than\nachieving a particular assessment level. That is, never set a goal of improving the\nassessment of an operational responsibility for the sake of improving the assess-\nment. That is putting the cart before the horse. Instead, identify a problem, engineer\na solution, and measure the success or failure by whether the assessment improves.\nThis is a subtle but important difference.\nSetting the goal of an improved assessment drives the wrong behavior. It\nencourages people to create spot ﬁxes that do not solve the larger problem or to\nﬁx unimportant-but-easy issues just to get a better assessment. It would be like\npaying teachers based on the grades their students receive; such a system would\nsimply lead to all students receiving A+’s as exams become easier and easier.\nFor this reason it is equally wrong to set a goal of raising the assessment level\nof all services to a certain level. An engineer’s time is scarce. A push to have a\nhigh assessment for every organizational responsibility results in expending those\nscarce resources on low-priority services in the name of raising an average. Such\na pursuit creates bureaucratic handcuffs that strangle an organization. It is some-\nthing you should never do; indeed, it is a trap that you should only hope your\ncompetition falls into.\nThis point cannot be overstated. If your management ever sets a corporate,\ndivision, or organizational goal of achieving a certain assessment level on all orga-\nnizational responsibilities of all services, you are to show them this section of the\nbook and tell them that such a plan is a recipe for disaster.\nLikewise, one should never make raises and bonuses contingent on the results\nof an assessment. This discourages people from joining the projects that need the\n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "20.7\nGetting Started\n413\nmost help. Instead, encourage your best people to join projects where they can\ndo the most good. Engineers are highly motivated by the opportunity to do good\nwork. The best reward for an improved assessment is not money, but the oppor-\ntunity to work on the most interesting project, or to receive notoriety by sharing\ntheir knowledge with other teams. Rewards based on rising assessment levels are\nalso not recommended because often it is a major achievement to simply retain a\nparticular level in the face of challenging times.\n20.7 Getting Started\nIntroducing this assessment system to a team can be a challenge. Most teams are\nnot used to working in such a data-driven assessment environment. It should be\nintroduced and used as a self-improvement tool—a way to help teams aspire to do\ntheir very best.\nTo begin, the team should enumerate the major services it provides. For exam-\nple, a team that is responsible for a large web site might determine that each web\nproperty is a service, each internal API is a service, and the common platform used\nto provide the services is a service. There may be multiple platforms, in which case\neach is considered a service. Finally, the infrastructure itself is often counted as a\nservice as far as assessment is concerned.\nAssessments should be done on a periodic and repeatable schedule. The\nﬁrst Monday of each month is a common choice for frequency. The team meets\nand conducts a self-assessment of each service. Management’s role is to maintain\nhigh standards and to ensure consistency across the services and teams. Manage-\nment may set global standards for how certain ORs are evaluated. For example,\nthere may be a corporate change management policy; compliance with that policy\nshould be evaluated the same way for all services by all teams.\nEight core ORs should be used to assess all services. Appendix A includes\ndetails about these eight operational responsibilities, along with questions to ask\nduring assessment to aid the team’s understanding of the OR. The questions are\nfollowed by look-for’s describing behaviors typically seen at various levels. Look-\nfor’s are indicators that the service is operating at a particular level. They are not\nchecklists of behaviors to emulate. Do not attempt to achieve every look-for. They\nare indicators, not requirements or checklists. Not every look-for is appropriate for\nevery service.\nIn addition to the eight core ORs, Appendix A lists other ORs that are particu-\nlar to speciﬁc services. They are optional and serve as examples of service-speciﬁc\nORs that an organization may choose to add to the core eight. Organizations may\nalso choose to invent ORs for their own special needs.\nAs the monthly assessments progress, the changes over time should be appar-\nent. The results of the assessments will help teams determine project priorities.\n",
      "content_length": 2868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "414\nChapter 20\nOperational Excellence\nOver time, the roll-ups described in this chapter can be used to compare services\nor teams.\n20.8 Summary\nIn this chapter we discussed how to assess the quality of operations and how to\nuse this assessment to drive improvements.\nMeasuring the quality of system administration is complex. Therefore for each\nservice we assess eight different qualities, called operational responsibilities: reg-\nular tasks (how normal, nonemergency tasks are performed), emergency response\n(how outages and other emergencies are handled), monitoring and metrics (collect-\ning data used to make decisions), capacity planning (determining future resource\nneeds), change management (how services are purposefully changed from birth\nto end), new service introduction and removal (how new products, hardware, or\nservices are introduced into the environment and how they are removed), service\ndeployment and decommissioning (how instances of an existing service are cre-\nated and decommissioned), and performance and efﬁciency (how cost-effectively\nresources are used).\nEach operational responsibility is assessed as being at one of ﬁve levels, reﬂect-\ning the Capability Maturity Model (CMM) levels used in software engineering. The\nCMM is a set of maturity levels for assessing processes: Initial (ad hoc), Repeatable\n(documented, automated), Deﬁned (roles and responsibilities are agreed upon),\nManaged (decisions are data-driven), and Optimizing (improvements are made\nand the results measured). The ﬁrst three levels of the CMM are the most important;\nthe other levels are often attempted only for high-value services.\nThe eight responsibilities of a service are individually assessed. These num-\nbers are rolled up to create a single assessment of the service. Teams are typically\nresponsible for many services. The individual service assessments are rolled up to\nassess the team, usually via a weighted average since not all services are equally\nimportant. These assessments can be used to rank teams.\nBy doing assessments periodically, progress can be tracked on the service,\nteam, or organization level. High-ranking teams should be encouraged to share\ntheir best practices so others may adopt them.\nThe goal is to make improvements and measure their effectiveness by seeing\nif the assessment changes. The goal should not be to improve the assessment or to\nachieve an average assessment across a set of services.\nUsing assessments to drive decisions in IT brings us closer to a system of\nscientiﬁc management for system administration and moves us away from “gut\nfeelings” and intuition. The importance of the ﬁrst three levels of the CMM is\nthat they take us away from ad hoc processes and individual heroics and create\nrepeatable processes that are more efﬁcient and of higher quality.\n",
      "content_length": 2810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "Exercises\n415\nExercises\n1. Why is it difﬁcult to assess the quality of operations work, or to compare two\ndifferent operations teams?\n2. Describe the ﬁve Capability Maturity Model (CMM) levels.\n3. Rank the eight “operational responsibilities” from most important to least\nimportant. Justify your ranking.\n4. In what ways could assessments become too bureaucratic and how might one\nprevent this?\n5. Why is the mathematical mode recommended when rolling up a service’s\nassessments, and a weighted average used when rolling up a team’s assess-\nments?\n6. In which other ways could one roll up assessments? Discuss their pros\nand cons.\n7. Select a service you are responsible for. Assess it based on the CMM levels for\neach of the operational responsibilities.\n8. Perform an assessment on a service you are not responsible for but for which\nyou are familiar with its IT staff. Interview those personnel to assess the service\nbased on the CMM levels for each of the operational responsibilities.\n9. Compare your experience doing Exercises 7 and 8.\n10. The eight operational responsibilities may not be appropriate for all services\nin all companies. Which modiﬁcations, additions, or subtractions would you\npropose for a service you are involved in? Justify your answer.\n11. There are disadvantages to repeating the assessment cycle too frequently or\nnot frequently enough. What would be the advantages and disadvantages of\nusing a weekly cycle? A yearly cycle?\n12. This chapter advises against setting goals that specify achieving a particu-\nlar CMM level. Relate the reason given to a personal experience (inside or\noutside of IT).\n",
      "content_length": 1627,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "Epilogue\nWe look for things.\nThings that make us go.\n—Pakled Captain Grebnedlog\nWe want a better world.\nWe want a world where food is safer and tastes better.\nWe want a world where deliveries happen on time.\nWe want a world where visiting the Department of Motor Vehicles is a fast\nand efﬁcient process.\nWe want a world where relationships are stronger, more meaningful, and more\nloving.\nWe want a world where we are happier in our jobs.\nWe want a world where the gloriously fanciful ideas of visionary science\nﬁction become the real-life products and experiences available to everyone.\nWe want a world without war, hunger, poverty, and hate.\nWe want a world where optimism, science, and truth win out over pessimism,\nignorance, and lies.\nWe want a world where everyone works together for the betterment of all.\nWe, the authors of this book, were born when computers could not ﬁt on a\ndesktop, let alone in a pocket. As youths, we saw the rise of computers and felt\ndeep in our hearts that technology would save the world. As adults, we’ve seen\ntechnology transform the world, make it smaller, and enable us to do things that\nour parents would never have dreamed of.\nNow we realize that computers and software are just a small piece of the pic-\nture. To make the world a better place requires operational practices that bring all\nthe parts together, that bring all the people together, to run it and maintain it and\nkeep it going.\n417\n",
      "content_length": 1435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "418\nEpilogue\nPeople say that software “eats” one industry at a time, disrupting and\nrevolutionizing it. When that happens, it is the operational practices that determine\nsuccess or failure.\nThe logistics of producing and distributing food is now a software function.\nFrom the farm to the dinner table, better software makes it possible to grow more\nwith fewer resources, harvest it at the right time, and transport it. Because the oper-\national practices are successful, we eat a wider variety of food that is fresher and\nless expensive than ever before.\nWhat looks like the simple act of buying something online requires a chain\nof organizations working together: raw material suppliers, factories, distribution,\nsales, marketing, purchasing, logistics, and delivery. Each of these is embodied in\nsoftware and works well, or not, because of the operational practices involved.\nThe creation of new products is accelerated by the improved cycle time that\nbrings new ideas and technologies to market faster. New ideas breed new ideas.\nWho would have imagined that someday we’d use our phone to run apps that\nenable us to deposit checks by taking their picture, reserve a car by pressing a\nbutton, or toss disgruntled birds at unstable structures?\nWhen operations is done right, we are happier in our jobs. We eliminate the\nterror and uncertainty of major upgrades. We end the practice of requiring certain\ntasks to be done at odd hours that steal our sleep and distract us from pleasure.\nThe stress from our jobs that hurts our nonwork relationships goes away. We live\nhappier lives. We have more time for living and loving. We have more freedom to\nuse our time to help others. Happiness multiplies love and it overﬂows us, leading\nus to share it with others.\nThese are the early years of all this stuff whose names and deﬁnitions are\nstill evolving: cloud computing, distributed computing, DevOps, SRE, the web,\nthe internet of things. We are standing at the base of the mountain, looking up,\nwondering what the future holds.\nIf you follow every bit of advice in this book, it will not cure all the world’s\nills. It will not end poverty or make food taste better. The advice in this book is\nobsolete as we write it.\nBut it is a start.\nWe hope this book gave you good things to think about. Interesting ideas to\ntry. A starting place. We are merely collectors of the good ideas we’ve learned from\nothers, read and heard about, and often been lucky enough to have experienced. We\nare scribes. We hope we have translated these ideas and experiences into words that\nconvey their essence without misrepresenting them or leaving out the important\nbits. Where we haven’t, we apologize in advance.\nThese are the early years. This book is just one voice. The rest is up to you.\nTake what you’ve learned here and build a better world.\n",
      "content_length": 2824,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "Part III\nAppendices\n",
      "content_length": 20,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "Appendix A\nAssessments\nThis\nappendix\ncontains\nassessment\nquestions\nand\nlook-for’s\nfor\nvarious\noperational responsibilities (ORs). Chapter 20, “Operational Excellence,” is the\ninstruction manual for this appendix. Advice on getting started is in Section 20.7.\nAssessment Levels\n.\nLevel 1\nInitial/Chaotic\nAd hoc and relying on individual heroics\nLevel 2\nRepeatable\nRepeatable results\nLevel 3\nDeﬁned\nResponsibilities deﬁned/conﬁrmed\nLevel 4\nManaged\nQuantitatively managed metrics\nLevel 5\nOptimizing\nDeliberate optimization/improvement\nCore Operational Responsibilities\n.\nPage\nOperational Responsibility\nChapters\n423\nRegular Tasks (RT)\n12, 14\n426\nEmergency Response (ER)\n6, 14, 15\n428\nMonitoring and Metrics (MM)\n16, 17, 19\n431\nCapacity Planning (CP)\n18\n433\nChange Management (CM)\n435\nNew Product Introduction and Removal (NPI/NPR)\n437\nService Deployment and Decommissioning (SDD)\n439\nPerformance and Efﬁciency (PE)\n421\n",
      "content_length": 916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "422\nAppendix A\nAssessments\nAdditional Operational Responsibilities\n.\nPage\nOperational Responsibility\nChapters\n442\nService Delivery: The Build Phase\n9\n444\nService Delivery: The Deployment Phase\n10, 11\n446\nToil Reduction\n12\n448\nDisaster Preparedness\n15\n",
      "content_length": 251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "A.1\nRegular Tasks (RT)\n423\nA.1 Regular Tasks (RT)\nRegular Tasks include how normal, non-emergency, operational duties are\nhandled—that is, how work is received, queued, distributed, processed, and veri-\nﬁed, plus how periodic tasks are scheduled and performed. All services have some\nkind of normal, scheduled or unscheduled work that needs to be done. Often web\noperations teams do not perform direct customer support but there are interteam\nrequests, requests from stakeholders, and escalations from direct customer support\nteams. These topics are covered in Chapters 12 and 14.\nSample Assessment Questions\n• What are the common and periodic operational tasks and duties?\n• Is there a playbook for common operational duties?\n• What is the SLA for regular requests?\n• How is the need for new playbook entries identiﬁed? Who may write new\nentries? Edit existing ones?\n• How are requests from users received and tracked?\n• Is there a playbook for common user requests?\n• How often are user requests not covered by the playbook?\n• How do users engage us for support? (online and physical locations)\n• How do users know how to engage us for support?\n• How do users know what is supported and what isn’t?\n• How do we respond to requests for support of the unsupported?\n• What are the limits of regular support (hours of operation, remote or on-site)?\nHow do users know these limits?\n• Are different size categories handled differently? How is size determined?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• There is no playbook, or it is out of date and unused.\n• Results are inconsistent.\n• Different people do tasks differently.\n• Two users requesting the same thing usually get different results.\n• Processes aren’t documented.\n• The team can’t enumerate all the processes a team does (even at a high level).\n• Requests get lost or stalled indeﬁnitely.\n",
      "content_length": 1944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "424\nAppendix A\nAssessments\n• The organization cannot predict how long common tasks will take to\ncomplete.\n• Operational problems, if reported, don’t get attention.\nLevel 2: Repeatable\n• There is a ﬁnite list of which services are supported by the team.\n• Each end-to-end process has each step enumerated, with dependencies.\n• Each end-to-end process has each step’s process documented.\n• Different people do the tasks the same way.\n• Sadly, there is some duplication of effort seen in the ﬂow.\n• Sadly, some information needed by multiple tasks may be re-created by each\nstep that needs it.\nLevel 3: Defined\n• The team has an SLA deﬁned for most requests, though it may not be\nadhered to.\n• Each step has a QA checklist to be completed before handing off to next step.\n• Teams learn of process changes by other teams ahead of time.\n• Information or processing needed by multiple steps is created once.\n• There is no (or minimal) duplication of effort.\n• The ability to turn up new capacity is a repeatable process.\nLevel 4: Managed\n• The deﬁned SLA is measured.\n• There are feedback mechanisms for all steps.\n• There is periodic (weekly?) review of defects and reworks.\n• Postmortems are published for all to see, with a draft report available within\nx hours and a ﬁnal report completed within y days.\n• There is periodic review of alerts by the affected team. There is periodic review\nof alerts by a cross-functional team.\n• Process change requests require data to measure the problem being ﬁxed.\n• Dashboards report data in business terms (i.e., not just technical terms).\n• Every “failover procedure” has a “date of last use” dashboard.\n• Capacity needs are predicted ahead of need.\n",
      "content_length": 1686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "A.1\nRegular Tasks (RT)\n425\nLevel 5: Optimizing\n• After process changes are made, before/after data are compared to determine\nsuccess.\n• Process changes are reverted if before/after data shows no improvement.\n• Process changes that have been acted on come from a variety of sources.\n• At least one process change has come from every step (in recent history).\n• Cycle time enjoys month-over-month improvements.\n• Decisions are supported by modeling “what if” scenarios using extracted\nactual data.\n",
      "content_length": 496,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "426\nAppendix A\nAssessments\nA.2 Emergency Response (ER)\nEmergency Response covers how outages and disasters are handled. This includes\nengineering resilient systems that prevent outages plus technical and non-technical\nprocesses performed during and after outages (response and remediation). These\ntopics are covered in Chapters 6, 14, and 15.\nSample Assessment Questions\n• How are outages detected? (automatic monitoring? user complaints?)\n• Is there a playbook for common failover scenarios and outage-related duties?\n• Is there an oncall calendar?\n• How is the oncall calendar created?\n• Can the system withstand failures on the local level (component failure)?\n• Can the system withstand failures on the geographic level (alternative data-\ncenters)?\n• Are staff geographically distributed (i.e., can other regions cover for each other\nfor extended periods of time)?\n• Do you write postmortems? Is there a deadline for when a postmortem must\nbe completed?\n• Is there a standard template for postmortems?\n• Are postmortems reviewed to assure action items are completed?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Outages are reported by users rather than a monitoring system.\n• No one is ever oncall, a single person is always oncall, or everyone is always\noncall.\n• There is no oncall schedule.\n• There is no oncall calendar.\n• There is no playbook of what to do for various alerts.\nLevel 2: Repeatable\n• A monitoring system contacts the oncall person.\n• There is an oncall schedule with escalation plan.\n• There is a repeatable process for creating the next month’s oncall calendar.\n• A playbook item exists for any possible alert.\n• A postmortem template exists.\n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "A.2\nEmergency Response (ER)\n427\n• Postmortems are written occasionally but not consistently.\n• Oncall coverage is geographically diverse (multiple time zones).\nLevel 3: Defined\n• Outages are classiﬁed by size (i.e., minor, major, catastrophic).\n• Limits (and minimums) for how often people should be oncall are deﬁned.\n• Postmortems are written for all major outages.\n• There is an SLA deﬁned for alert response: initial, hands-on-keyboard, issue\nresolved, postmortem complete.\nLevel 4: Managed\n• The oncall pain is shared by the people most able to ﬁx problems.\n• How often people are oncall is veriﬁed against the policy.\n• Postmortems are reviewed.\n• There is a mechanism to triage recommendations in postmortems and assure\nthey are completed.\n• The SLA is actively measured.\nLevel 5: Optimizing\n• Stress testing and failover testing are done frequently (quarterly or monthly).\n• “Game Day” exercises (intensive, system-wide tests) are done periodically.\n• The monitoring system alerts before outages occur (indications of “sick”\nsystems rather than “down” systems).\n• Mechanisms exist so that any failover procedure not utilized in recent history\nis activated artiﬁcially.\n• Experiments are performed to improve SLA compliance.\n",
      "content_length": 1232,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "428\nAppendix A\nAssessments\nA.3 Monitoring and Metrics (MM)\nMonitoring and Metrics covers collecting and using data to make decisions. Moni-\ntoring collects data about a system. Metrics uses that data to measure a quantiﬁable\ncomponent of performance. This includes technical metrics such as bandwidth,\nspeed, or latency; derived metrics such as ratios, sums, averages, and percentiles;\nand business goals such as the efﬁcient use of resources or compliance with\na service level agreement (SLA). These topics are covered in Chapters 16, 17,\nand 19.\nSample Assessment Questions\n• Is the service level objective (SLO) documented? How do you know your SLO\nmatches customer needs?\n• Do you have a dashboard? Is it in technical or business terms?\n• How accurate are the collected data and the predictions? How do you know?\n• How efﬁcient is the service? Are machines over- or under-utilized? How is\nutilization measured?\n• How is latency measured?\n• How is availability measured?\n• How do you know if the monitoring system itself is down?\n• How do you know if the data used to calculate key performance indicators\n(KPIs) is fresh? Is there a dashboard that shows measurement freshness and\naccuracy?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• No SLOs are documented.\n• If there is monitoring, not everything is monitored, and there is no way to\ncheck completeness.\n• Systems and services are manually added to the monitoring system, if at all:\nthere is no process.\n• There are no dashboards.\n• Little or no measurement or metrics.\n• You think customers are happy but they aren’t.\n• It is common (and rewarded) to enact optimizations that beneﬁt a person or\nsmall group to the detriment of the larger organization or system.\n• Departmental goals emphasize departmental performance to the detriment of\norganizational performance.\n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "A.3\nMonitoring and Metrics (MM)\n429\nLevel 2: Repeatable\n• The process for creating machines/server instances assures they will be\nmonitored.\nLevel 3: Defined\n• SLOs are documented.\n• Business KPIs are deﬁned.\n• The freshness of business KPI data is deﬁned.\n• A system exists to verify that all services are monitored.\n• The monitoring system itself is monitored (meta-monitoring).\nLevel 4: Managed\n• SLOs are documented and monitored.\n• Deﬁned KPIs are measured.\n• Dashboards exist showing each step’s completion time; the lag time of each\nstep is identiﬁed.\n• Dashboards exist showing current bottlenecks, backlogs, and idle steps.\n• Dashboards show defect and rework counts.\n• Capacity planning is performed for the monitoring system and all analysis\nsystems.\n• The freshness of the data used to calculate KPIs is measured.\nLevel 5: Optimizing\n• The accuracy of collected data is veriﬁed through active testing.\n• KPIs are calculated using data that is less than a minute old.\n• Dashboards and other analysis displays are based on fresh data.\n• Dashboards and other displays load quickly.\n• Capacity planning for storage, CPU, and network of the monitoring system is\ndone with the same sophistication as any major service.\n.\nThe Unexpectedly Slow Cache\nStack Exchange purchased a product that would accelerate web page delivery\nto customers using a globally distributed cache. Most customers deploy this\nproduct and assume it has a “can’t lose” beneﬁt.\n",
      "content_length": 1455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "430\nAppendix A\nAssessments\n.\nBefore deploying it, Stack Exchange engineer Nick Craver created a\nframework for measuring end-to-end page load times. The goal was to pre-\ncisely know how much improvement was gained both globally and for\ncustomers in various geographic regions.\nWe were quite surprised to discover that the product degraded per-\nformance. It improved certain aspects but only at the detriment of others,\nresulting in a net performance loss.\nStack Exchange worked with the vendor to identify the problem. As a\nresult, a major design error was found and ﬁxed.\nIf care hadn’t been taken to measure performance before and after the\nchange, Stack Exchange’s efforts would have unknowingly made its service\nslower. One wonders how many other customers of this product did no\nsuch measurements and simply assumed performance was improved while\nin reality it was made worse.\n",
      "content_length": 881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "A.4\nCapacity Planning (CP)\n431\nA.4 Capacity Planning (CP)\nCapacity Planning covers determining future resource needs. All services require\nsome kind of planning for future resources. Services tend to grow. Capacity plan-\nning involves the technical work of understanding how many resources are needed\nper unit of growth, plus non-technical aspects such as budgeting, forecasting, and\nsupply chain management. These topics are covered in Chapter 18.\nSample Assessment Questions\n• How much capacity do you have now?\n• How much capacity do you expect to need three months from now? Twelve\nmonths from now?\n• Which statistical models do you use for determining future needs?\n• How do you load-test?\n• How much time does capacity planning take? What could be done to make it\neasier?\n• Are metrics collected automatically?\n• Are metrics available always or does their need initiate a process that collects\nthem?\n• Is capacity planning the job of no one, everyone, a speciﬁc person, or a team\nof capacity planners?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• No inventory is kept.\n• The system runs out of capacity from time to time.\n• Determining how much capacity to add is done by tradition, guessing, or luck.\n• Operations is reactive about capacity planning, often not being able to fulﬁll\nthe demand for capacity in time.\n• Capacity planning is everyone’s job, and therefore no one’s job.\n• No one is speciﬁcally assigned to handle CP duties.\n• A large amount of headroom exists rather than knowing precisely how much\nslack is needed.\nLevel 2: Repeatable\n• CP metrics are collected on demand, or only when needed.\n• The process for collecting CP metrics is written and repeatable.\n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "432\nAppendix A\nAssessments\n• Load testing is done occasionally, perhaps when a service is new.\n• Inventory of all systems is accurate, possibly due to manual effort.\nLevel 3: Defined\n• CP metrics are automatically collected.\n• Capacity required for a certain amount of growth is well deﬁned.\n• There is a dedicated CP person on the team.\n• CP requirements are deﬁned at a subsystem level.\n• Load testing is triggered by major software and hardware changes.\n• Inventory is updated as part of capacity changes.\n• The amount of headroom needed to survive typical surges is deﬁned.\nLevel 4: Managed\n• CP metrics are collected continuously (daily/weekly instead of monthly or\nquarterly).\n• Additional capacity is gained automatically, with human approval.\n• Performance regressions are detected during testing, involving CP if perfor-\nmance regression will survive into production (i.e., it is not a bug).\n• Dashboards include CP information.\n• Changes in correlation are automatically detected and raise a ticket for CP to\nverify and adjust relationships between core drivers and resource units.\n• Unexpected increases in demand are automatically detected using MACD\nmetrics or similar technique, which generates a ticket for the CP person or\nteam.\n• The amount of headroom in the system is monitored.\nLevel 5: Optimizing\n• Past CP projections are compared with actual results.\n• Load testing is done as part of a continuous test environment.\n• The team employs a statistician.\n• Additional capacity is gained automatically.\n• The amount of headroom is systematically optimized to reduce waste.\n",
      "content_length": 1591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": "A.5\nChange Management (CM)\n433\nA.5 Change Management (CM)\nChange Management covers how services are deliberately changed over time.\nThis includes the software delivery platform—the steps involved in a software\nrelease: develop, build, test, and push into production. For hardware, this includes\nﬁrmware upgrades and minor hardware revisions. These topics are covered in\nChapters 9, 10, and 11.\nSample Assessment Questions\n• How often are deployments (releases pushed into production)?\n• How much human labor does it take?\n• When a release is received, does the operations team need to change anything\nin it before it is pushed?\n• How does operations know if a release is major or minor, a big or small\nchange? How are these types of releases handled differently?\n• How does operations know if a release is successful?\n• How often have releases failed?\n• How does operations know that new releases are available?\n• Are there change-freeze windows?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Deployments are done sparingly, as they are very risky.\n• The deployment process is ad hoc and laborious.\n• Developers notify operations of new releases when a release is ready for\ndeployment.\n• Releases are not deployed until weeks or months after they are available.\n• Operations and developers bicker over when to deploy releases.\nLevel 2: Repeatable\n• The deployment is no longer ad hoc.\n• Deployment is manual but consistent.\n• Releases are deployed as delivered.\n• Deployments fail often.\n",
      "content_length": 1581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "434\nAppendix A\nAssessments\nLevel 3: Defined\n• What constitutes a successful deployment is deﬁned.\n• Minor and major releases are handled differently.\n• The expected time gap between release availability and deployment is\ndeﬁned.\nLevel 4: Managed\n• Deployment success/failure is measured against deﬁnitions.\n• Deployments fail rarely.\n• The expected time gap between release availability and deployment is\nmeasured.\nLevel 5: Optimizing\n• Continuous deployment is in use.\n• Failed deployments are extremely rare.\n• New releases are deployed with little delay.\n",
      "content_length": 558,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "A.6\nNew Product Introduction and Removal (NPI/NPR)\n435\nA.6 New Product Introduction and Removal\n(NPI/NPR)\nNew Product Introduction and Removal covers how new products and services are\nintroduced into the environment and how they are removed. This is a coordination\nfunction: introducing a new product or service requires a support infrastructure\nthat may touch multiple teams.\nFor example, before a new model of computer hardware is introduced into\nthe datacenter environment, certain teams must have access to sample hardware\nfor testing and qualiﬁcation, the purchasing department must have a process\nto purchase the machines, and datacenter technicians need documentation. For\nintroducing software and services, there should be tasks such as requirements\ngathering, evaluation and procurement, licensing, and creation of playbooks for\nthe helpdesk and operations.\nProduct removal might involve ﬁnding all machines with a particularly old\nrelease of an operating system and seeing that all of them get upgraded. Prod-\nuct removal requires identifying current users, agreeing on timelines for migrating\nthem away, updating documentation, and eventually decommissioning the prod-\nuct, any associated licenses, maintenance contracts, monitoring, and playbooks.\nThe majority of the work consists of communication and coordination between\nteams.\nSample Assessment Questions\n• How is new hardware introduced into the environment? Which teams are\ninvolved and how do they communicate? How long does the process take?\n• How is old hardware or software eliminated from the system?\n• What is the process for disposing of old hardware?\n• Which steps are taken to ensure disks and other storage are erased when\ndisposed?\n• How is new software or a new service brought into being? Which teams are\ninvolved and how do they communicate? How long does the process take?\n• What is the process for handoff between teams?\n• Which tools are used?\n• Is documentation current?\n• Which steps involve human interaction? How could it be eliminated?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\n",
      "content_length": 2146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "436\nAppendix A\nAssessments\nLevel 1: Initial\n• New products are introduced through ad hoc measures and individual\nheroics.\n• Teams are surprised by NPI, often learning they must deploy something into\nproduction with little notice.\n• NPI is delayed due to lack of capacity, miscommunication, or errors.\n• Deprecating old products is rarely done, resulting in operations having to\nsupport an “inﬁnite” number of hardware or software versions.\nLevel 2: Repeatable\n• The process used for NPI/NPR is repeatable.\n• The handoff between teams is written and agreed upon.\n• Each team has a playbook for tasks related to its involvement with NPR/NPI.\n• Equipment erasure and disposal is documented and veriﬁed.\nLevel 3: Defined\n• Expectations for how long NPI/NPR will take are deﬁned.\n• The handoff between teams is encoded in a machine-readable format.\n• Members of all teams understand their role as it ﬁts into the larger, overall\nprocess.\n• The maximum number of products supported by each team is deﬁned.\n• The list of each team’s currently supported products is available to all teams.\nLevel 4: Managed\n• There are dashboards for observing NPI and NPR progress.\n• The handoff between teams is actively revised and improved.\n• The number of no-longer-supported products is tracked.\n• Decommissioning no-longer-supported products is a high priority.\nLevel 5: Optimizing\n• NPI/NPR tasks have become API calls between teams.\n• NPI/NPR processes are self-service by the team responsible.\n• The handoff between teams is a linear ﬂow (or for very complex systems,\njoining multiple linear ﬂows).\n",
      "content_length": 1584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 468,
      "content": "A.7\nService Deployment and Decommissioning (SDD)\n437\nA.7 Service Deployment and Decommissioning\n(SDD)\nService Deployment and Decommissioning covers how instances of an existing\nservice are created and how they are turned off (decommissioned). After a service\nis designed, it is usually deployed repeatedly. Deployment may involve turning\nup satellite replicas in new datacenters or creating a development environment of\nan existing service. Decommissioning could be part of turning down a datacenter,\nreducing excess capacity, or turning down a particular service instance such as a\ndemo environment.\nSample Assessment Questions\n• What is the process for turning up a service instance?\n• What is the process for turning down a service instance?\n• How is new capacity added? How is unused capacity turned down?\n• Which steps involve human interaction? How could it be eliminated?\n• How many teams touch these processes?\n• Do all teams know how they ﬁt into the over all picture?\n• What is the workﬂow from team to team?\n• Which tools are used?\n• Is documentation current?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• The process is undocumented and haphazard. Results are inconsistent.\n• The process is deﬁned by who does something, not what is done.\n• Requests get delayed due to miscommunication, lack of resources, or other\navoidable reasons.\n• Different people do the tasks differently.\nLevel 2: Repeatable\n• The processes required to deploy or decommission a service are understood\nand documented.\n• The process for each step is documented and veriﬁed.\n• Each step has a QA checklist to be completed before handing off to the next\nstep.\n• Teams learn of process changes by other teams ahead of time.\n",
      "content_length": 1798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "438\nAppendix A\nAssessments\n• Information or processing needed by multiple steps is created once.\n• There is no (or minimal) duplication of effort.\n• The ability to turn up new capacity is a repeatable process.\n• Equipment erasure and disposal is documented and veriﬁed.\nLevel 3: Defined\n• The SLA for how long each step should take is deﬁned.\n• For physical deployments, standards for removal of waste material (boxes,\nwrappers, containers) are based on local environmental standards.\n• For physical decommissions, standards for disposing of old hardware are\nbased on local environmental standards as well as the organization’s own\nstandards for data erasure.\n• Tools exist to implement many of the steps and processes.\nLevel 4: Managed\n• The deﬁned SLA for each step is measured.\n• There are feedback mechanisms for all steps.\n• There is periodic review of defects and reworks.\n• Capacity needs are predicted ahead of need.\n• Equipment disposal compliance is measured against organization standards\nas well as local environmental law.\n• Waste material (boxes, wrappers, containers) involved in deployment is\nmeasured.\n• Quantity of equipment disposal is measured.\nLevel 5: Optimizing\n• After process changes are made, before/after data is compared to determine\nsuccess.\n• Process changes are reverted if before/after data shows no improvement.\n• Process changes that have been acted on come from a variety of sources.\n• Cycle time enjoys month-over-month improvements.\n• Decisions are supported by modeling “what if” scenarios using extracts from\nactual data.\n• Equipment disposal is optimized by the reduction of equipment deployment.\n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "A.8\nPerformance and Efﬁciency (PE)\n439\nA.8 Performance and Efficiency (PE)\nPerformance and Efﬁciency covers how cost-effectively resources are used and\nhow well the service performs. A running service needs to have good performance\nwithout wasting resources. We can generally improve performance by using more\nresources, or we may be able to improve efﬁciency to the detriment of performance.\nAchieving both requires a large effort to bring about equilibrium. Cost-efﬁciency\nis cost of resources divided by quantity of use. Resource efﬁciency is quantity of\nresources divided by quantity of use. To calculate these statistics, one must know\nhow many resources exist; thus some kind of inventory is required.\nSample Assessment Questions\n• What is the formula used to measure performance?\n• What is the formula used to determine utilization?\n• What is the formula used to determine resource efﬁciency?\n• What is the formula used to determine cost efﬁciency?\n• How is performance variation measured?\n• Are performance, utilization, and resource efﬁciency monitored automati-\ncally? Is there a dashboard for each?\n• Is there an inventory of the machines and servers used in this service?\n• How is the inventory kept up-to-date?\n• How would you know if something was missing from the inventory?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Performance and utilization are not consistently measured.\n• What is measured depends on who set up the systems and services.\n• Resource efﬁciency is not measured.\n• Performance problems often come as a surprise and are hard to diagnose and\nresolve because there is insufﬁcient data.\n• Inventory is not up-to-date.\n• Inventory may or may not be updated, depending on who is involved in\nreceiving or disposing of items.\n",
      "content_length": 1848,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "440\nAppendix A\nAssessments\nLevel 2: Repeatable\n• All metrics relevant to performance and utilization are collected across all\nsystems and services.\n• The process for bringing up new systems and services is documented and\neveryone follows the process.\n• Systems are associated with services when conﬁgured for use by a service, and\ndisassociated when released.\n• Inventory is up-to-date. The inventory process is well documented and every-\none follows the process.\nLevel 3: Defined\n• Performance and utilization monitoring is automatically conﬁgured for all\nsystems and services during installation and removed during decommission.\n• Performance targets for each service are deﬁned.\n• Resource usage targets for each service are deﬁned.\n• Formulas for service-oriented performance and utilization metrics are\ndeﬁned.\n• Performance of each service is monitored continuously.\n• Resource utilization of each service is monitored continuously.\n• Idle capacity that is not currently used by any service is monitored.\n• The desired amount of headroom is deﬁned.\n• The roles and responsibilities for keeping the inventory up-to-date are deﬁned.\n• Systems for tracking the devices that are connected to the network and their\nhardware conﬁgurations are in place.\nLevel 4: Managed\n• Dashboards track performance, utilization, and resource efﬁciency.\n• Minimum, maximum, and 90th percentile headroom are tracked and com-\npared to the desired headroom and are visible on a dashboard.\n• Goals for performance and efﬁciency are set and tracked.\n• There are periodic reviews of performance and efﬁciency goals and status for\neach service.\n• KPIs are used to set performance, utilization, and resource efﬁciency goals that\ndrive optimal behavior.\n• Automated systems track the devices that are on the network and their con-\nﬁgurations and compare them with the inventory system, ﬂagging problems\nwhen they are found.\n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "A.8\nPerformance and Efﬁciency (PE)\n441\nLevel 5: Optimizing\n• Bottlenecks are identiﬁed using the performance dashboard. Changes are\nmade as a result.\n• Services that use large amounts of resources are identiﬁed and changes are\nmade.\n• Changes are reverted if the changes do not have a positive effect.\n• Computer hardware models are regularly evaluated to ﬁnd models where\nutilization of the different resources is better balanced.\n• Other sources of hardware and other hardware models are regularly evaluated\nto determine if cost efﬁciency can improved.\n",
      "content_length": 555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "442\nAppendix A\nAssessments\nA.9 Service Delivery: The Build Phase\nService delivery is the technical process of how a service is created. It starts with\nsource code created by developers and ends with a service running in production.\nSample Assessment Questions\n• How is software built from source code to packages?\n• Is the ﬁnal package built from source or do developers deliver precompiled\nelements?\n• What percentage of code is covered by unit tests?\n• Which tests are fully automated?\n• Are metrics collected about bug lead time, code lead time, and patch lead time?\n• To build the software, do all raw source ﬁles come from version control\nrepositories?\n• To build the software, how many places (repositories or other sources) are\naccessed to attain all raw source ﬁles?\n• Is the resulting software delivered as a package or a set of ﬁles?\n• Is everything required for deployment delivered in the package?\n• Which package repository is used to hand off the results to the deployment\nphase?\n• Is there a single build console for status and control of all steps?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Each person builds in his or her own environment.\n• People check in code without checking that it builds.\n• Developers deliver precompiled elements to be packaged.\n• Little or no unit testing is performed.\n• No metrics are collected.\n• Version control systems are not used to store source ﬁles.\n• Building the software is a manual process or has manual steps.\n• The master copies of some source ﬁles are kept in personal home directories\nor computers.\nLevel 2: Repeatable\n• The build environment is deﬁned; everyone uses the same system for consis-\ntent results.\n• Building the software is still done manually.\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "A.9\nService Delivery: The Build Phase\n443\n• Testing is done manually.\n• Some unit tests exist.\n• Source ﬁles are kept in version-controlled repositories.\n• Software packages are used as the means of delivering the end result.\n• If multiple platforms are supported, each is repeatable, though possibly\nindependently.\nLevel 3: Defined\n• Building the software is automated.\n• Triggers for automated builds are deﬁned.\n• Expectations around unit test coverage are deﬁned; they are less than\n100 percent.\n• Metrics for bug lead time, code lead time, and patch lead time are deﬁned.\n• Inputs and outputs of each step are deﬁned.\nLevel 4: Managed\n• Success/fail build ratios are measured and tracked on a dashboard.\n• Metrics for bug lead time, code lead time, and patch lead time are collected\nautomatically.\n• Metrics are presented on a dashboard.\n• Unit test coverage is measured and tracked.\nLevel 5: Optimizing\n• Metrics are used to select optimization projects.\n• Attempts to optimize the process involve collecting before and after metrics.\n• Each developer can perform the end-to-end build process in his or her own\nsandbox before committing changes to a centralized repository.\n• Insufﬁcient unit test code coverage stops production.\n• If multiple platforms are supported, building for one is as easy as building for\nthem all.\n• The software delivery platform is used for building infrastructure as well as\napplications.\n",
      "content_length": 1423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "444\nAppendix A\nAssessments\nA.10 Service Delivery: The Deployment Phase\nThe goal of the deployment phase is to create a running environment. The deploy-\nment phase creates the service in one or more testing and production environ-\nments. This environment will then be used for testing or for live production\nservices.\nSample Assessment Questions\n• How are packages deployed in production?\n• How much downtime is required to deploy the service in production?\n• Are metrics collected about frequency of deployment, mean time to restore\nservice, and change success rate?\n• How is the decision made to promote a package from testing to production?\n• Which kind of testing is done (system, performance, load, user acceptance)?\n• How is deployment handled differently for small, medium, and large releases?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply?\nLevel 1: Initial\n• Deployment involves or requires manual steps.\n• Deployments into the testing and production environments are different\nprocesses, each with its own tools and procedures.\n• Different people on the team perform deployments differently.\n• Deployment requires downtime, and sometimes signiﬁcant downtime.\n• How a release is promoted to production is ad hoc or ill deﬁned.\n• Testing is manual, ill deﬁned, or not done.\nLevel 2: Repeatable\n• Deployment is performed in a documented, repeatable process.\n• If deployment requires downtime, it is a predictable.\n• Testing procedures are documented and repeatable.\nLevel 3: Defined\n• Metrics for frequency of deployment, mean time to restore service, and change\nsuccess rate are deﬁned.\n• How downtime due to deployments is to be measured is deﬁned; limits and\nexpectations are deﬁned.\n",
      "content_length": 1747,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "A.10\nService Delivery: The Deployment Phase\n445\n• How a release is promoted to production is deﬁned.\n• Testing results are clearly communicated to all stakeholders.\nLevel 4: Managed\n• Metrics for frequency of deployment, mean time to restore service, and change\nsuccess rate are collected automatically.\n• Metrics are presented on a dashboard.\n• Downtime due to deployments is measured automatically.\n• Reduced production capacity during deployment is measured.\n• Tests are fully automated.\nLevel 5: Optimizing\n• Metrics are used to select optimization projects.\n• Attempts to optimize the process involve collecting before and after metrics.\n• Deployment is fully automated.\n• Promotion decisions are fully automated, perhaps with a few speciﬁc excep-\ntions.\n• Deployment requires no downtime.\n",
      "content_length": 795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "446\nAppendix A\nAssessments\nA.11 Toil Reduction\nToil Reduction is the process by which we improve the use of people within our\nsystem. When we reduce toil (i.e., exhausting physical labor), we create a more\nsustainable working environment for operational staff. While reducing toil is not\na service per se, this OR can be used to assess the amount of toil and determine\nwhether practices are in place to limit the amount of toil.\nSample Assessment Questions\n• How many hours each week are spent on coding versus non-coding projects?\n• What percent of time is spent on project work versus manual labor that could\nbe automated?\n• What percentage of time spent on manual labor should raise a red ﬂag?\n• What is the process for detecting that the percentage of manual labor has\nexceeded the red ﬂag threshold?\n• What is the process for raising a red ﬂag? Whose responsibility is it?\n• What happens after a red ﬂag is raised? When is it lowered?\n• How are projects for reducing toil identiﬁed? How are they prioritized?\n• How is the effectiveness of those projects measured?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Toil is not measured and grows until no project work, or almost no project\nwork, can be accomplished.\n• There is no process for raising a red ﬂag.\n• Some individuals recognize when toil is becoming a problem and look for\nsolutions, but others are unaware of the problem.\n• Individuals choose to work on the projects that are the most interesting to\nthem, without looking at which projects will have the biggest impact.\nLevel 2: Repeatable\n• The amount of time spent on toil versus on projects is measured.\n• The percentage of time spent on toil that constitutes a problem is deﬁned and\ncommunicated.\n• The process for raising a red ﬂag is documented and communicated.\n• Individuals track their own toil to project work ratio, and are individually\nresponsible for raising a red ﬂag.\n• Red ﬂags may not always be raised when they should be.\n",
      "content_length": 2047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "A.11\nToil Reduction\n447\n• The process for identifying which projects will have the greatest impact on toil\nreduction is deﬁned.\n• The method for prioritizing projects is documented.\nLevel 3: Defined\n• For each team, the person responsible for tracking toil and raising a red ﬂag is\nidentiﬁed.\n• The people involved in identifying and prioritizing toil-reduction projects are\nknown.\n• Both a red ﬂag level of toil and a target level are deﬁned. The red ﬂag is lowered\nwhen toil reaches the target level.\n• During the red ﬂag period, the team works on only the highest-impact toil-\nreduction projects.\n• During the red ﬂag period, the team has management support for putting\nother projects on hold until toil is reduced to a target level.\n• After each step in a project, statistics on toil are closely monitored, providing\nfeedback on any positive or negative changes.\nLevel 4: Managed\n• Project time versus toil is tracked on a dashboard, and the amount of time\nspent on each individual project or manual task is also tracked.\n• Red ﬂags are raised automatically, and the dashboard gives an overview of\nwhere the problems lie.\n• The time-tracking data is monitored for trends that give an early alert for teams\nthat are showing an increase in toil in one or more areas.\n• KPIs are deﬁned and tracked to keep toil within the desired range and\nminimize the red ﬂag periods.\nLevel 5: Optimizing\n• The target and red ﬂag levels are adjusted and the results are monitored to the\neffect on overall ﬂow, performance, and innovation.\n• Changes to the main project prioritization process are introduced and evalu-\nated for positive or negative impact, including the impact on toil.\n• Changes to the red ﬂag toil-reduction task prioritization process are intro-\nduced and evaluated.\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "448\nAppendix A\nAssessments\nA.12 Disaster Preparedness\nAn operations organization needs to be able to handle outages well, and it must\nhave practices that reduce the chance of repeating past mistakes. Disasters and\nmajor outages happen. Everyone in the company from the top down needs to rec-\nognize that fact, and adopt a mind-set that accepts outages and learns from them.\nSystems should be designed to be resilient to failure.\nSample Assessment Questions\n• What is the SLA? Which tools and processes are in place to ensure that the\nSLA is met?\n• How complete are the playbooks?\n• When was each scenario in the playbooks last exercised?\n• What is the mechanism for exercising different failure modes?\n• How are new team members trained to be prepared to handle disasters?\n• Which roles and responsibilities apply during a disaster?\n• How do you prepare for disasters?\n• How are disasters used to improve future operations and disaster response?\n• If there is a corporate standard practice for this OR, what is it and how does\nthis service comply with the practice?\nLevel 1: Initial\n• Disasters are handled in an ad hoc manner, requiring individual heroics.\n• Playbooks do not exist, or do not cover all scenarios.\n• Little or no training exists.\n• Service resiliency and different failure scenarios are never tested.\nLevel 2: Repeatable\n• Playbooks exist for all failure modes, including large-scale disasters.\n• New team members receive on-the-job training.\n• Disasters are handled consistently, independent of who is responding.\n• If multiple team members respond, their roles, responsibilities, and handoffs\nare not clearly deﬁned, leading to some duplication of effort.\nLevel 3: Defined\n• The SLA is deﬁned, including dates for postmortem reports.\n• Handoff procedures are deﬁned, including checks to be performed and docu-\nmented.\n",
      "content_length": 1837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "A.12\nDisaster Preparedness\n449\n• How to scale the responding team to make efﬁcient use of more team members\nis deﬁned.\n• The roles and responsibilities of team members in a disaster are deﬁned.\n• Speciﬁc disaster preparedness training for new team members is deﬁned and\nimplemented.\n• The team has regular disaster preparedness exercises.\n• The exercises include ﬁre drills performed on the live service.\n• After every disaster, a postmortem report is produced and circulated.\nLevel 4: Managed\n• The SLA is tracked using dashboards.\n• The timing for every step in the process from the moment the event occurred\nis tracked on the dashboard.\n• A program for disaster preparedness training ensures that all aspects are\ncovered.\n• The disaster preparedness program measures the results of disaster prepared-\nness training.\n• As teams become better at handling disasters, the training expands to cover\nmore complex scenarios.\n• Teams are involved in cross-functional ﬁre drills that involve multiple teams\nand services.\n• Dates for publishing initial and ﬁnal postmortem reports are tracked and\nmeasured against the SLA.\nLevel 5: Optimizing\n• Areas for improvement are identiﬁed from the dashboards.\n• New techniques and processes are tested and the results measured and used\nfor further decision making.\n• Automated systems ensure that every failure mode is exercised within a\ncertain period, by artiﬁcially causing a failure if one has not occurred naturally.\n",
      "content_length": 1457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "Appendix B\nThe Origins and Future of\nDistributed Computing\nand Clouds\nTo me it seems quite clear\nthat it’s all just a little bit of\nhistory repeating\n—Propellerheads\nModern, large datacenters typically consist of rack after rack of pizza box–sized\ncomputers. This is a typical design pattern for any large cloud-scale, distributed\ncomputing environment. Why is this? How did so many companies arrive at the\nsame design pattern?\nThe answer is that this design pattern was inevitable. To manage enormous\ndatacenters, hardware and processes must be highly organized. Repeating the\nsame rack structure within a datacenter reduces complexity, which reduces man-\nagement and administrative overhead. Many other factors led to the choice of\nhardware to ﬁt into those racks.\nHowever, nothing is truly inevitable. Thus the appearance of massive data-\ncenters full of machines built from commodity hardware, distributed computing\nreplacing large computers, and the popularity of cloud computing were inevitable\nonly in the sense that necessity is the mother of invention.\nEarlier approaches to large web and e-commerce sites did not have the right\neconomies of scale. In fact, scaling became more expensive per user. This caused the\nﬁrst dot-com bubble to be unsustainable. What drove the distributed computing\nrevolution was the need to create the right economies of scale, where addtional\nusers become cheaper to support.\nEvery order of magnitude improvement in the cost of computing enables a\nnew era of applications, each of which was unimaginable just a few years before.\n451\n",
      "content_length": 1571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "452\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nEach requires new supporting infrastructure technology and operational method-\nologies. These technologies and methodologies do not always arrive in time, and\nsometimes they are ahead of their time.\nUnderstanding the historical context of all these changes gives context to the\ntools and techniques described in this book. If this doesn’t interest you, feel free to\nskip this appendix.\nThe history of the computer industry is quite long, starting with the abacus.\nThis appendix will skip ahead a few years and focus on a few speciﬁc periods:\n• The pre-web era: The years immediately prior to the web, from 1985 to 1994\n• The ﬁrst web era: “The dot-com bubble” from 1995 to 2000\n• The dot-bomb era: The economic downturn from 2000 to 2003\n• The second web era: The resurgence from 2003 to 2010\n• The cloud computing era: Where we are today\nEach period had different needs that drove the technology decisions of the day,\npushing evolution forward step by step. These needs changed over time due to\nthe ebb and ﬂow of economic prosperity, the march of technological improvement,\nand increasing expectations of reliability and speed. This appendix provides our\ninterpretation of how it happened.\nB.1 The Pre-Web Era (1985–1994)\nComputing was different prior to the web. Reliability and scale were important but\nnot in the same way they are today. The Internet was used only by a small group\nof technologists, and most people were unaware of its existence.\nAvailability Requirements\nFor most businesses, their internal operations were reliant on computers, but\noutages were largely invisible to external customers. The exceptions were com-\npanies whose customers included a large segment of the population and accessed\nthe services from outside the business premises—for example, telephone compa-\nnies whose customers were making phone calls and banks that provided ATMs.\nCustomers expect those services to be available on a 24 × 7 basis.\nIn this era most large businesses were already heavily reliant on computers\nfor the bulk of their work. Some employees had remote access from home, over a\ntelephony-based modem, sometimes with a dumb terminal, sometimes with a PC\nor Mac.\nMost businesses could schedule outages for maintenance—by looking at prod-\nuct release schedules and avoiding the end-of-quarter period. The customers of\n",
      "content_length": 2400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "B.1\nThe Pre-Web Era (1985–1994)\n453\nthe computer systems were internal to the company, and there were easy, deﬁned\nways of contacting them to schedule downtime.\nThe Internet was largely text-based, with email, Internet news, bulletin boards,\nand ﬁle transfer programs. Outages of an Internet service might cause a backlog\nof email or news, but went largely unnoticed by most people. For most people,\nInternet access was a perk, but it was not business-critical. Some companies offered\nanonymous-ftp drop-boxes for third parties, such as customers needing support.\nLittle other commercial business was carried out over the Internet. It was far from\nclear in the early days whether it was a potential acceptable use policy violation to\nuse the Internet for commercial purposes.\nDuring this era, 24 × 7 operation was important for some internal corporate\nsystems, but not for Internet services. Maintenance outages of internal systems\ncould be scheduled, because the user population was known and could be easily\ninformed.\n.\nDowntime Used to Be Normal\nTom’s ﬁrst computer experience was in 1980. Tom was in sixth grade and his\nschool gave a small group of students access to a terminal connected to a main-\nframe. When someone logged into the system, it would display a message that\n“the system will be down every day after 6 for taping.” Neither Tom nor\nhis teacher knew what that meant, but years later Tom realized that “taping”\nmust have meant “doing backups.” The system was down for hours each day\nto do backups and that was normal.\nTechnology\nBusiness applications and services ran on mainframes and minicomputers. These\ndevices often had megabytes of storage and could execute less than 1 million\ninstructions per second (MIPS). They were server-class machines, which were\nbuilt with high-quality components, high-speed technologies, and expansion capa-\nbilities.\nHome computers were a new thing. There was no Internet access and common\napplications (stand-alone games, word processing, spreadsheets, and tax pack-\nages) came on ﬂoppy disk. The hardware components were “consumer grade,”\nwhich meant cheap, underpowered, and unreliable. The computers often relied\non 8-bit and later 16-bit processors, with RAM and disk storage measured in\nkilobytes and megabytes, not gigabytes and terabytes. CPU speed was measured\nin MHz, and GHz was science ﬁction. Data was stored on slow ﬂoppy disks.\n",
      "content_length": 2397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "454\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nA 10MB hard drive was a luxury item, even though it was fragile and being\ndropped would destroy it.\nScaling\nA company’s computer systems served internal customers, applications, and busi-\nness processes. They needed to scale with the growth of the company. As the\ncompany’s business grew, so would the number of employees and the computing\nrequirements. Even for fast-growing companies, this growth was relatively pre-\ndictable and often bounded by the much slower rate at which new employees were\nhired.\nBusiness-critical applications ran on a small number of large, high-end com-\nputers. If a computer system ran out of capacity, it could be upgraded with\nadditional disks, memory, and CPUs. Upgrading further meant buying a big-\nger machine, so machines that were to be servers were typically purchased with\nplenty of spare upgrade capacity. Sometimes services were also scaled by deploy-\ning servers for the application into several geographic regions, or business units,\neach of which would then use its local server. For example, when Tom ﬁrst worked\nat AT&T, there was a different payroll processing center for each division of the\ncompany.\nHigh Availability\nApplications requiring high availability required “fault-tolerant” computers.\nThese computers had multiple CPUs, error-correcting RAM, and other technolo-\ngies that were extremely expensive at the time. Fault-tolerant systems were niche\nproducts. Generally only the military and Wall Street needed such systems. As a\nresult they were usually priced out of the reach of typical companies.\nCosts\nDuring this era the Internet was not business-critical, and outages for internal\nbusiness-critical systems could be scheduled because the customer base was a lim-\nited, known set of people. When necessary, advanced reliability and scaling needs\nwere addressed with very expensive specialized hardware. However, businesses\ncould easily calculate the costs of any outages, do a risk analysis to understand\nwhat the budget for providing higher reliability was, and make an informed\ndecision about how to proceed. These costs were under control.\nScaling was handled through hardware upgrades, but compute requirements\nscaled in predictable ways with the company’s business. As the business grew, the\nbudget for the compute infrastructure grew, and upgrades could be planned and\nscheduled.\n",
      "content_length": 2419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "B.2\nThe First Web Era: The Bubble (1995–2000)\n455\nB.2 The First Web Era: The Bubble (1995–2000)\nIn the beginning of the ﬁrst web era, web sites were relatively static reposito-\nries of linked documents. The ﬁrst corporate web sites were largely marketing\nliterature—information about the company and its products, press releases, job\nlistings, and contact information for various customer-facing groups, such as sales\nand customer support. But many businesses quickly realized the potential for\nconducting business over the Internet, and e-commerce was born.\nAvailability Requirements\nFor most companies, their Internet presence was initially treated in much the same\nway as their key internal systems. They used server-grade hardware with some\nhigh-availability options and additional capacity for scaling. For most companies,\ntheir Internet presence was just another part of the infrastructure, and not a par-\nticularly business-critical part at that. Under normal conditions the site should\nremain up, but system administrators could schedule outages for maintenance\nas needed. Usually a scheduled outage for the web service involved conﬁguring\nanother machine to respond with a static page letting people know that the site\nwas down for maintenance and to try again later.\nThen new startup companies appeared with business models that revolved\naround conducting business entirely on the web. These companies did not have\nexisting products, channels, and customers. They were not adapting existing busi-\nness processes to include the web. For these companies, their Internet presence was\nthe entire business. If their web sales channel failed, everything came to a standstill.\nThey did not have the luxury of contacting their customers to schedule a mainte-\nnance window, as they had no way to know who their customers might be during\nthat time period. Anyone with Internet access was a potential customer. These com-\npanies needed a highly reliable Internet presence in a way that no other companies\nhad before. Companies wanted 24 × 7 operations with no maintenance windows.\nTechnology\nDuring this era, home computers became more common, as did faster connections\nto the home with xDSL and Internet service from cable TV companies. Better graph-\nics and more powerful computers resulted in better games, many of which were\nnetworked, multi-user games. Voice over IP (VoIP) emerged, with associated new\nproducts and services. Disks became larger and cheaper, so people started digitiz-\ning and storing new types of data, such as music and videos. Inevitably companies\nbuilt products and Internet services around that data as well.\nOn the server side, companies looking to provide a more reliable Internet pres-\nence started buying RAID subsystems instead of plain disks, multiple high-end\n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "456\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nprocessors, and so on. While these technologies are common today, they were\nextremely expensive then. Vendors that previously sold to the very small set of\ncustomers who needed either very large or very reliable computers had an entirely\nnew group of customers who wanted machines that were both large and reliable.\nSun Microsystems’ E4500 server was the normative hardware of the dot-com era.\nLoad balancers also appeared on the market during this era. A load balancer\nsits in front of a group of machines that are all providing the same service. It contin-\nually tests to see if the service is available on each of the machines. Load balancers\ncan be conﬁgured in a primary/secondary setup—sending all trafﬁc to the primary\ndevice until it fails, and then automatically switching to the secondary device. They\ncan also be conﬁgured to load-share between machines that all provide the same\nservice. In this latter mode, when a machine stops responding correctly, the load\nbalancer stops directing queries to it until it resumes responding correctly. Load\nbalancers provide automatic failover in case a machine is down, making them a\nuseful tool for a service that needs high availability.\nScaling\nBusinesses needed servers to power their web sites. System administrators applied\ntheir old methods to the new requirements: one machine for each web site. As the\nsite got more popular, larger machines were used to meet the demand. To achieve\nbetter performance, custom CPUs and new internal architectures were developed,\nbut these machines were expensive. Software was also expensive. A typical web\nserver required an OS license, a web server license, and a database server license.\nEach was priced proportionately to the hardware.\nWith the web the requirements for scaling are not bound to the number of\nemployees in the company. The web introduces an environment where the users\nof the service can be anyone with an Internet connection. That is a very large and\nrapidly growing number. When a site introduces or successfully markets a web-\nbased service, the number of people accessing the servers can increase very rapidly\nover a short period of time. Scaling for sudden, unpredictable ﬂuctuations in ser-\nvice usage was a new challenge that was not well understood in this era, although\nit was something for which Internet startups tried to prepare. Internet startups\nplanned for success and purchased the biggest, most reliable systems available.\nThese were expensive times.\nHowever, the practice of growing web sites through bigger, more reliable, more\nexpensive hardware was not economically viable. Normally as a company grows,\neconomies of scale result in lower cost per unit. The dot-coms, however, required\ncomputers that were more expensive per unit of capacity as the company grew\nlarger. A computer that is 10 times more powerful than an average computer is\n",
      "content_length": 2945,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "B.2\nThe First Web Era: The Bubble (1995–2000)\n457\nmore than 10 times as expensive. These larger, more reliable systems used custom\nhardware and had a smaller market—two factors that drove up prices. For linear\nincreases in performance, the cost per unit of capacity was growing super-linearly.\nThe more users the web site gained, the more expensive it was to provide the ser-\nvice. This model is the opposite of what you want. Additional costs came from the\ntechniques used to ensure high availability, which are covered in the next section.\nOddly enough, these high costs were acceptable at the time. The dot-coms\nwere ﬂush with cash. Spending it on expensive machines was common, because\nit showed how optimistic you were about your company’s success. Also, the eco-\nnomics of scaling sites in this way was not well understood—the traditional model\nof increased sales yielding a lower cost per unit was assumed to still hold true. In\naddition, startup valuations were made in a rather strange manner in those days.\nIf a startup was achieving a small proﬁt, it got a low valuation. However, if it was\nshowing a loss, it got a high valuation. Moreover, the larger the loss, the higher\nthe valuation. When these startups were ﬂoated on the stock market, their stock\nprices went through the roof, and the investors made substantial proﬁts, even if\nthe business model made no sense.\nThe saying at the time was “We may lose money on every sale, but we’ll make\nit up in volume.” Behind the scenes was the thought that if the company could\ncorner the market, it could raise prices later. However, that assumption failed to\ntake into account that the next startup would immediately undercut the higher\nprices with its own loss-making corner-the-market scheme.\nThis gold rush mentality kept the economy buzzing for quite a while. Then\nin 2000 the bubble burst. The house of cards collapsed. This approach to scaling\nweb-based services was unsustainable. If the bubble had not burst due to issues\nof investment and cash ﬂow, it would have failed due to the bad economics of the\ntechnology being used.\nHigh Availability\nWith the advent of the web, the user base changed from known internal company\nemployees with predictable, cyclic availability (the 9-to-5 business day) require-\nments and access schedules, to unknown external Internet users who required\nconstant access. This change created the need for higher reliability. The ﬁrst\napproach to meeting these availability goals was to buy more expensive hardware\nwith built-in higher availability—for example, RAID and multiple CPUs.\nBut even the most reliable system fails occasionally. Traditional options for\ncritical systems were to have a service contract with a four-hour turnaround for\nreplacement parts, or to purchase spare parts to be stored near the machine. Either\nway, some downtime would be required in the event of a hardware failure.\n",
      "content_length": 2893,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "458\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nThere was also the issue of downtime to perform software upgrades. Apply-\ning the internal corporate approach of notifying the user base, it became common\npractice for web sites to pre-announce such downtime. There would be a warn-\ning that “This site will be down Saturday from noon to 5 PST for an upgrade.”\nRegular users could plan around this, but new or occasional customers would be\ncaught unaware, as there was no other way to notify them. Advertising upgrades\nin advance could also lead to adverse headlines if the upgrade went poorly, as\nsome people would watch to see how the upgrade went and to report on the new\nservice.\nN + 1 Configurations\nUsing two machines became the best practice. One would run the service and the\nother was idle but conﬁgured and ready to take over if the ﬁrst machine failed.\nUnless the site had a load balancer the “failover” usually required manual inter-\nvention but a good systems administrator could do the switch fast enough that the\nweb site would be down for less than an hour. This is called an N+1 conﬁguration\nsince there is one more device than required to provide the service. This technique\nis very expensive considering that at any given time 50 percent of your investment\nis sitting idle.\nSoftware upgrades could be done by upgrading the spare server and switching\nto it when it was time to unveil the new features. The downtime would only be\nminutes or seconds to perform the failover. Users might not even notice!\nN + 2 Configurations\nWhat if the primary machine failed while the spare was being upgraded? The half-\nconﬁgured machine would not be in a usable state. As software releases increased\nin frequency, the likelihood that the spare would not be in a usable state also\nincreased.\nThus, the best practice became having three machines, or an N + 2 conﬁgura-\ntion. Now systems administrators could safely perform upgrades but 66 percent\nof the hardware investment was idle at any given time. Imagine paying for three\nhouses but only living in one. Imagine being the person who had to tell the CEO\nhow much money was used on idle equipment!\nSome companies tried to optimize by load sharing between the machines.\nExtra software development or a load balancer was required to make this work\nbut it was possible. In an N + 1 conﬁguration, systems administrators could per-\nform software upgrades by taking one machine out of service and upgrading it\nwhile the other remained running. However, if both machines were at 80 percent\nutilization, the site now had a single machine that was 160 percent utilized, which\nwould make it unacceptably slow for the end users. The web site might as well be\ndown. The easy solution to that problem is to never let either machine get more\n",
      "content_length": 2806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "B.3\nThe Dot-Bomb Era (2000–2003)\n459\nthan 50 percent utilized—but that simply returns us to the situation where half the\ncapacity we paid for is idle. The idle capacity is just split between two machines!\nSome companies tried to do such upgrades only late at night when fewer users\nmeant that utilization had dipped below 50 percent. That left very little time to\ndo the upgrade, making large or complex upgrades extremely risky. Changes that\nrequired an operating system upgrade or extensive testing were not possible. If\nthe site became popular internationally and was busy during every time zone, this\noption disappeared. Also, no one can schedule hardware failures to happen only\nat night! Neither of these approaches was a viable option for better utilization of\nthe available resources.\nFor high availability, sites still needed three machines and the resulting 66\npercent idle capacity.\nCosts\nThe cost of providing a highly available and popular web service during this\nera was very high. It was run on expensive high-end hardware, with expensive\nreliability features such as RAID and multiple CPUs, in an N + 1 or N + 2 conﬁgu-\nration. That architecture meant that 50 to 66 percent of this expensive hardware was\nalways idle. To reduce downtime to next to nothing, sites might also use expensive\nload balancers.\nThe OS costs and support costs for this high-end hardware were also very\nhigh, as were the costs for the application software, such as the web server software\nand database software.\nScaling a site built in this way meant that the cost per unit of performance\nincreased as the site got bigger. Rather than more customers resulting in economies\nof scale, more customers meant a higher cost per unit. The cost model was the\nreverse of what a business would normally expect.\nB.3 The Dot-Bomb Era (2000–2003)\nThe dot-com bubble collapse started in 2000. By 2001, the bubble was deﬂating as\nfast as it could. Most of the dot-coms burned through their venture capital and\ntheir stock ceased trading, often never having reached proﬁtability.\nAfter the collapse came a period of calm. Things slowed down a bit. It was\npossible to pause and consider what had been learned from the past. Without all\nthe hype, marketing, and easy money, the better technologies survived. Without\npressure from investors wanting their cash spent quickly so that they would get\na quick return on their investment, people could take enough time to think and\ninvent new solutions. Silicon Valley is, for the most part, a meritocracy. Working\ntechnology rules; hype and self-promotion are ﬁltered out.\n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "460\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nAvailability Requirements\nDuring the dot-bomb era, there were no signiﬁcant changes in availability require-\nments. The Internet-based companies that had survived the crash developed a\nbetter understanding of their availability requirements and ﬁgured out how to\nmeet them without breaking the bank.\nTechnology\nThree trends enabled the next phase: surplus capacity left over from the previous\nboom years, the commoditization of hardware, and the maturation of open source\nsoftware.\nThe ﬁrst trend was short-lived but signiﬁcant. A lot of capacity had been built\nup in the previous boom years and suppliers were slashing prices. Millions of\nmiles of ﬁber had been laid in the ground and in the oceans to meet the predicted\nbandwidth needs of the world. With relatively few customers, telecommunications\nproviders were desperate to make deals. Similarly, huge datacenter “colocation\nfacilities” had been built. A colocation facility is a highly reliable datacenter facil-\nity that rents space to other companies. Many colocation providers went bankrupt\nafter building some of the world’s largest facilities. That space could now be rented\nvery inexpensively. While these surpluses would eventually be exhausted, the\ntemporarily depressed prices helped kickstart the era.\nThe second trend was the commoditization of hardware components used in\nhome computers, such as Intel x86 CPUs, low-end hard drives, and RAM. Before\nthe advent of the web, the average home did not have a computer. The popularity\nof the Internet created more demand for home computers, resulting in components\nbeing manufactured at a scale never before seen. In addition, the popularity of\ngames that required high-end graphics, lots of memory, and fast CPUs was one\nof the major drivers toward making increasingly higher-end devices available in\nthe consumer market. This mass production led to commoditization and, in turn,\nlower prices. The price of home PCs came down, but servers still used different\ncomponents and remained expensive.\nThe third trend was the maturity of open source projects such as Linux,\nApache, MySQL, and Perl. The rise of Linux brought a UNIX-like server operating\nsystem to the Intel x86 platform. Previously systems that used the Intel x86 chips\ncould not run server-class, UNIX and UNIX-like operating systems. SGI, IBM, Sun,\nand others did not make their operating systems available for the x86. Intel x86\ncomputers ran Windows 95 and variants that were not designed as server operat-\ning systems. Even Windows NT, which was designed as a server operating system,\ndid not achieve success as a web service platform.\nThere were also free versions of BSD UNIX available for x86-based comput-\ners at the same time. Eventually, however, Linux became the dominant x86 UNIX\n",
      "content_length": 2841,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "B.3\nThe Dot-Bomb Era (2000–2003)\n461\nbecause various companies like RedHat offered versions of Linux with support.\nCorporations had typically shied away from free open source software, because it\nwas not supported. These companies were slowly persuaded that commercial ver-\nsions of Linux were acceptable for servers, because they could still buy support.\nEven though they were still paying for the OS and support for the OS, they real-\nized signiﬁcant cost savings through the use of cheaper x86 hardware and reduced\nOS costs.\nAlthough Linux was available during the ﬁrst web era, it was not mature\nenough for production use. In fact, tools like Linux, Apache, MySQL, and Perl were\nconsidered toys compared to a Solaris OS, Netscape web server, Oracle database,\nJava “stack” or a Microsoft NT Server, IIS (web server), and SQL Server database\nand .NET environment. Even so, those open source projects were now creating soft-\nware that was reliable enough for production use. Linux matured. Apache proved\nto be faster, more stable, more feature rich, and easier to conﬁgure than commer-\ncial platforms. MySQL was easier to install and manage than Oracle. Oracle’s price\ntag was so high that the moment a free SQL database was available, the ﬂood-\ngates opened. Perl matured, added object-oriented features, and gained acceptance\noutside of its initial niche as a system administration language. All of this was\nunimaginable just a few years earlier. One combination of open source software\nwas so common that the acronym “LAMP” was coined, referring to the quartet of\nLinux, Apache, MySQL, and Perl. The ability to use commodity servers running a\nfree operating system was revolutionary.\nHigh Availability\nWhile a LAMP system was less expensive, it was slower and less reliable. It would\ntake many such servers to equal the aggregate processing capacity of the larger\nmachine being replaced. Reliability was more complex.\nResearchers started experimenting with using low-cost components to build\nservers. Research such as Recovery-Oriented Computing (ROC) at University of\nCalifornia–Berkeley (Patterson et al. 2002) discovered that many small servers\ncould work better and be cheaper than individual large servers. This was pre-\nviously unheard of. How could that underpowered CPU designed for desktops\ncompete with a custom-designed Sun SPARC chip? How could a cheap consumer-\ngrade hard disk compete with a fancy SCSI drive?\nThe conventional engineering wisdom is that more components means more\nfailures. If one computer is likely to fail every 100 days, then it is likely that a\nsystem with two computers will have a failure every 50 days. If you had enough\ncomputers, you were likely to experience failures every day.\nResearch projects like ROC at UC-Berkeley and others reversed that logic.\nMore computers could be more reliable when “distributed computing” techniques\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "462\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nwere employed. Rather than design a system where any one machine’s failure\nprevented the system from providing service, these researchers developed dis-\ntributed computing techniques that let many machines share the workload and\nhave enough spare capacity that a certain percentage of the machines could be\ndown and service would be unaffected.\nROC also observed that companies were paying for reliability at every level.\nThe load balancer provided reliability, sending trafﬁc to only the “up” server(s).\nThe servers provided reliability by using expensive custom CPUs. The storage sys-\ntems provided reliability by using expensive RAID controllers and high-end hard\ndrives. Yet, to avoid downtime due to applications that required manual failover,\ncompanies still had to write software to survive outages. Why pay for many layers\nof reliability and also pay to develop software that survives outages? ROC rea-\nsoned that if the software already survived outages, why pay for extra quality at\nevery level? Using the cheaper commodity components might result in less reliable\nservers, but software techniques would result in a system that was more reliable\nas a whole. Since the software had to be developed anyway, this made a lot of\nsense.\nDigital electronics either work or they don’t. The ability to provide service is\nbinary: the computer is on or off. This is called the “run run run dead” problem.\nDigital electronics typically run and run and run until they fail. At that point, they\nfail completely. The system stops working. Compare this to an analog system such\nas an old tube-based radio. When it is new, it works ﬁne. Over time components\nstart to wear out, reducing audio quality. The user hears more static, but the radio\nis usable. The amount of static increases slowly, giving the user months of warn-\ning before the radio is unusable. Instead of “run run run dead,” analog systems\ndegrade slowly.\nWith distributed computing techniques, each individual machine is still digi-\ntal: it is either running or not. However, the collection of machines is more like the\nanalog radio: the system works but performance drops as more and more machines\nfail. A single machine being down is not a cause for alarm but rather a signal that\nit must be repaired before too many other machines have also failed.\nScaling\nROC researchers demonstrated that distributed computing could be reliable\nenough to provide a service requiring high availability. But could it be fast enough?\nThe answer to this question was also “yes.” The computing power of a fully loaded\nSun E10K could be achieved with enough small, pizza box–sized machines based\non commodity hardware.\nWeb applications were particularly well suited for distributed computing.\nImagine a simple case where the contents of a web site can be stored on one\n",
      "content_length": 2885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "B.3\nThe Dot-Bomb Era (2000–2003)\n463\nmachine. A large server might be able to deliver 4000 queries per second (QPS) of\nservice. Suppose a commodity server could provide only 100 QPS. It would take\n40 such servers to equal the aggregate capacity of the larger machine. Distributed\ncomputing algorithms for load balancing can easily scale to 40 machines.\nData Scaling\nFor some applications, all the data for the service might not ﬁt on a single commod-\nity server. These commodity servers were too small to store a very large dataset.\nApplications such as web search have a dataset, or “corpus,” that could be quite\nlarge. Researchers found that they could resolve this issue by dividing the cor-\npus into many “fractions,” each stored on a different machine, or “leaf.” Other\nmachines (called “the root”) would receive requests and forward each one to the\nappropriate leaf.\nTo make the system more resilient to failures, each fraction could be stored on\ntwo different leaves. If there were 10 fractions, there would be 20 leaves. The root\nwould divide the trafﬁc for a particular fraction among the two leaves as long as\nboth were up. If one failed, the root would send all requests related to that fraction\nto the remaining leaf. The chance of a simultaneous failure by two leaves hold-\ning the same data was unlikely. Even if it did happen, users might not notice that\ntheir web searches returned slightly fewer results until the replacement algorithms\nloaded the missing data onto a spare machine.\nScaling was also achieved through replication. If the system did not process\nrequests fast enough, it could be scaled by adding leaves. A particular fraction\nmight be stored in three or more places.\nThe algorithms got more sophisticated over time. For example, rather than\nsplitting the corpus into 10 fractions, one for each machine, the corpus could be\nsplit into 100 fractions and each machine would store 10. If a particular fraction\nwas receiving a particularly large number of hits (it was “hot”), that fraction could\nbe placed on more machines, bumping out less popular fractions. Better algo-\nrithms resulted in better placement, diversity, and dynamically updatable corpus\ndata.\nApplicability\nThese algorithms were particularly well suited for web search and similar appli-\ncations where the data was mostly static (did not change) except for wholesale\nreplacements when a new corpus was produced. In contrast, they were inappropri-\nate for traditional applications. After all, you wouldn’t want your payroll system\nbuilt on a database that dealt with machine failures by returning partial results.\nAlso, these systems lacked many of the features of traditional databases related to\nconsistency and availability.\n",
      "content_length": 2722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "464\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nNew distributed computing algorithms enabled new applications one by one.\nFor example, the desire to provide email as a massive web-based service led\nto better storage systems. Over time more edge cases were conquered so that\ndistributed computing techniques could be applied to more applications.\nCosts\nIf distributed computing could be more reliable and faster, could it be more cost\neffective, too? The cost of one highly reliable and expensive machine compared\nvery well to the cost of enough commodity machines to provide equivalent capa-\ncity. In fact, the cost per unit of capacity was often 3 to 50 times less expensive\nusing distributed computing. Remember that previously we saw 50 to 66 percent\nidle capacity with the large servers when used in N + 1 and N + 2 conﬁgura-\ntions. Suppose an entire rack of machines was required to provide the equivalent\ncapacity. That would typically be 40 machines, with two held as spares. (N + 2\nredundancy). The “waste” now drops from 66 percent to 5 percent. That shrink-\ning of idle capacity gives the distributed computing design a head start. Factoring\nin the power of a commodity market to drive down the cost of components\nimproves the situation further. Moreover, one gets a volume discount when pur-\nchasing many computers—something you can’t do when buying one or two large\nmachines.\nIn the 2003 article “Web Search for a Planet: The Google Cluster Architecture,”\nBarroso et al. wrote:\nThe cost advantages of using inexpensive, PC-based clusters over high-end multipro-\ncessor servers can be quite substantial, at least for a highly parallelizable application\nlike ours. The example $278,000 rack contains 176 2-GHz Xeon CPUs, 176 Gbytes of\nRAM, and 7 Tbytes of disk space. In comparison, a typical x86-based server contains\neight 2-GHz Xeon CPUs, 64 Gbytes of RAM, and 8 Tbytes of disk space; it costs about\n$758,000. In other words, the multiprocessor server is about three times more expen-\nsive but has 22 times fewer CPUs, three times less RAM, and slightly more disk space.\n(Barroso, Dean & Hölzle 2003)\nFurther cost reductions came by stripping the machines down to the exact\ncomponents needed. These machines did not need video cards, audio cards, speak-\ners, keyboards, USB ports, or a fancy plastic bezel with a cool logo on the front.\nEven if eliminating an item saved just a few dollars, when buying thousands of\nmachines it added up. Some companies (Yahoo!) worked with vendors to build\ncustom computers to their exact speciﬁcations, while others grew large enough to\ndesign their own computers from scratch (Google).\nAll these changes altered the economics of computing. In fact, they are what\nenabled the second web era.\n",
      "content_length": 2766,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "B.4\nThe Second Web Era (2003–2010)\n465\nUnder the old economics, the larger the scale, the more disproportionately\nexpensive the system became. In the new economics, the larger the scale, the more\nopportunity for the economics to improve. Instead of super-linear cost curves, cost\ngrowth was closer to linear.\nB.4 The Second Web Era (2003–2010)\nIn the history of computing, every jump in improved cost led to a wave of new\napplications. Distributed computing’s better economics was one such leap. Web\nservices that had previously failed, unable to make a proﬁt, were now cheaper to\noperate and got a second chance.\nAvailability Requirements\nAs before, companies that conducted business entirely over the Internet typi-\ncally aimed at a global audience of Internet users. That meant that they required\n24 × 7 availability with no scheduled downtime, and as close to no unscheduled\ndowntime as possible. Companies started applying the newly developed dis-\ntributed computing techniques to their service offerings to meet these availability\nrequirements in a more cost-effective way.\nTechnology\nMany companies embraced distributed computing techniques. Google adopted\nthe techniques to provide web search that was faster than ever seen before. Previ-\nously web searches took many seconds and sometimes minutes or hours to show\nsearch results. Google was so proud of its ability to return results in less than half\na second that it listed the number of milliseconds your request took to process at\nthe bottom of every page.\nNew advertising systems like Google AdSense, which collects pennies per\nclick, could be proﬁtable now that the cost of computing was signiﬁcantly cheaper.\nEarlier business models that involved selling advertising on a web site required\nhuge sales forces to get advertisers to buy ads on their web site. AdSense and simi-\nlar systems were fully automated. Potential advertisers would bid in a huge, online\nauction for ad placement. This reduced and often eliminated the need for a sales\nforce. All the company needed to do was add some JavaScript code to its site. Soon\nthe advertisements would start appearing and money would start rolling in. Such\nadvertising systems created a new business model that enabled the development\nof entire new industries.\nWeb hosting became much cheaper. The software got much easier to use.\nThis led to the invention of “blogs” (originally from the term “web-log”), which\n",
      "content_length": 2423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "466\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nrequired very little technical knowledge to operate. Rather than needing a sales\nforce and a large IT department, a single person could run a successful blog.\nBloggers could focus on creating content and the advertising network would send\nthem checks.\nHigh Availability\nWith distributed computing techniques, high availability and scaling became\nclosely coupled. Techniques used to ensure high availability also aided scaling,\nand vice versa. Google published many of the distributed computing techniques\nit invented. Some of the earliest of these were the Google File System (GFS) and\nMapReduce.\nGFS was a ﬁle system that scaled to multiple terabytes of data. Files were\nstored as 64-megabyte chunks. Each chunk was stored on three or more commod-\nity servers. Accessing the data was done by talking directly to the machine with the\ndesired data rather than going through a mediator machine that could be a bottle-\nneck. GFS operations often happened in parallel. For example, copying a ﬁle was\nnot done one block at a time. Instead, each machine that stored a part of the source\nﬁle would be paired with another machine and all blocks would be transmitted\nin parallel. Resiliency could be improved by conﬁguring GFS to keep more than\nthree replicas of each block, decreasing the chance that all copies of the ﬁle would\nbe on a down machine at any given time. If one machine did fail, the GFS master\nwould use the remaining copies to populate the data elsewhere until the replication\nfactor was achieved. Increasing the replication level also improved performance.\nData access was load balanced among all the replicas. More replicas meant more\naggregate read performance.\n.\nCase Study: MapReduce\nMapReduce is a system for processing large batches of data in parallel. Sup-\npose you needed to process 100 terabytes of data. Reading that much data\nfrom start to ﬁnish would take a long time. Instead, dozens (or thousands) of\nmachines could be set up to process a portion of the data each in parallel. The\ndata they read would be processed (mapped) to create an intermediate result.\nThese intermediates would be processed and then summarized (reduced) to\ngain the ﬁnal result. MapReduce did all the difﬁcult work of dividing up the\ndata and delegating work to various machines. As a software developer using\nMapReduce, you just needed to write the “map” function and the “reduce”\nfunction. Everything else was taken care of.\n",
      "content_length": 2498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "B.4\nThe Second Web Era (2003–2010)\n467\n.\nBecause MapReduce was a centralized service, it could be improved and\nall users would beneﬁt from the changes. For example, there was a good prob-\nability that a machine would fail during a MapReduce run that takes days and\ninvolves thousands of machines. The logic for detecting a failed machine and\nsending its portion of data to another working machine is incorporated in the\nMapReduce system. The developer does not need to worry about it. Suppose a\nbetter way to detect and replace a failed machine is developed. This improve-\nment would be made to the central MapReduce system and all users would\nbeneﬁt.\nOne such improvement involved predicting which machines would fail.\nIf one machine was taking considerably longer to process its share of the data,\nthat slow performance could be a sign that it had a hard disk that was start-\ning to fail or that some other problem was present. MapReduce could direct\nanother machine to process the same portion of data. Whichever machine ﬁn-\nished ﬁrst would “win” and its results would be kept. The other machine’s\nprocessing would be aborted. This kind of preemptive redundancy could\nsave hours of processing, especially since a MapReduce run is only as fast\nas the slowest machine. Many such optimizations were developed over the\nyears.\nOpen source versions of MapReduce soon appeared. Now it was easy for\nother companies to adopt MapReduce’s techniques. The most popular imple-\nmentation is Hadoop. It includes a data storage system similar to GFS called\nHBase.\nScaling\nIf you’ve spent money to invent ways to create highly available distributed com-\nputing systems that scale well, the best way to improve your return on investment\n(ROI) is to use that technology on as many datacenters as possible.\nOnce you are maintaining hundreds of racks of similarly conﬁgured com-\nputers, it makes sense to manage them centrally. Centralized operations have\neconomic beneﬁts. For example, standardization makes it possible to automate\noperational tasks and therefore requires less labor. When labor is manual, larger\nsystems require more labor. When operations are automated, the cost of develop-\ning the software is spread across all the machines, so the cost per machine decreases\nas the number of machines increases.\nTo gain further efﬁciencies of scale, companies like Google and Facebook take\nthis one step further. Why not treat the entire datacenter as one giant computer?\nA computer has memory, disks, and CPUs that are connected by a communi-\ncation “bus.” A datacenter has a network that acts like a bus connecting units\n",
      "content_length": 2612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "468\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nof compute and storage. Why not design and operate datacenters as one large,\nwarehouse-sized computer? Barroso, Clidaras, and Hölzle (2013) coined the phrase\n“warehouse-scale computing” to capture this idea of treating the entire datacenter\nas one giant computer.\nCosts\nIn this era companies also competed to develop better and less expensive data-\ncenters. The open source movement was reducing the cost of software. Moore’s\nlaw was reducing the cost of hardware. The remaining costs were electricity and\noperations itself. Companies saw that improvements in these areas would give\nthem a competitive edge.\nPrior to the second web era, datacenters wasted electricity at unbelievable\nrates. According to the Uptime Institute, the typical datacenter has an average\npower usage effectiveness (PUE) of 2.5. This means that for every 2.5 watts in at\nthe utility meter, only 1 watt is used for the IT. By using the most efﬁcient equip-\nment and best practices, most facilities could achieve 1.6 PUE. The lack of efﬁciency\nprimarily comes from two places. First, some efﬁciency is lost every time there is\na power conversion. A UPS converts the power from alternating current (A/C) to\ndirect current (D/C), and back to A/C—two conversions. Power is converted from\nhigh-voltage lines to 110 VAC used in power outlets to 12 V and other voltage levels\nrequired by the components and chips in computers—four or ﬁve more conver-\nsions. In addition, cooling is a major factor. If 1 watt of power makes a computer\ngenerate a certain amount of heat, it takes at least 1 watt of power to remove that\nheat. You don’t actually “cool” a datacenter; you extract the heat. Companies seek-\ning to reduce their operational costs like Google, Microsoft, and Facebook achieved\ncloser to 1.2 PUE. In 2011, Google reported one datacenter achieved 1.08 PUE dur-\ning winter months (Google 2012). Every decimal improvement in PUE means a big\ncompetitive edge.\nOther things are even more important than power efﬁciency. Using less\npower—for example, by shutting off unused machines—is an improvement no\nmatter your PUE rating. Smart companies work toward achieving the best price\nper unit of computation. For example, for internet search, true efﬁciency comes\nfrom getting the most QPS for your money. This may mean skipping a certain gen-\neration of Intel CPU because, while faster, the CPU uses a disproportionate amount\nof power.\nThe new economics also made it more feasible to give away a service because\nadvertising revenue was sufﬁcient to make the business viable. Many sites were\nfree for most features but offered “premium” services at an additional cost. This\n“freemium” (free + premium) business model was a great way to attract large num-\nbers of customers. Being free meant there was very little barrier to entry. Once the\nuser was accustomed to using the service, selling the premium features was easier.\nIf advertising revenue was sufﬁcient, the free users were no burden.\n",
      "content_length": 3032,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "B.5\nThe Cloud Computing Era (2010–present)\n469\nSoftware, hardware, and power costs were all signiﬁcantly lower with\nthis new approach, enabling the emergence of new business models. Could\nthe price drop to the point that computing becomes cost free? The answer will\nsurprise you.\nB.5 The Cloud Computing Era (2010–present)\nThe trend so far as been that each era has computing capacity that is more econom-\nical than the previous era. Increases in capacity and reliability went from having\nsuper-linear cost growth to linear and sub-linear cost growth. Can it get any better?\nCould it become free?\nMany landlords live “rent free” by simply having enough proﬁtable tenants\nto cover the cost of the apartment they live in. This is the economic thinking that\nenables cloud computing: build more computing resources than you need and rent\nthe surplus.\nAvailability Requirements\nAround this time, mobile computing became more affordable and, in turn, more\naccessible. Cell phones became smartphones running operating systems as sophis-\nticated as those found on PCs. Mobile applications created demand for even better\nlatency and reliability.\nMobile applications create demand for lower-latency services—that is, ser-\nvices that respond faster. A mobile map application would not be useful if it\ntook an hour to calculate the best driving route to a location. It has to be fast\nenough to be usable in a real-world context, responding in a second or two. If such\ncalculations can be done hundreds of times a second, it opens the door to new\napplications: the ability to drag points in the map and see the route recalculated\nin real time. Now instead of one request, dozens are sent. Because the applica-\ntion is so much more usable this way, the number of users increases. Thus capacity\ndemands increase by multiple orders of magnitude, which requires quantum leaps\nin support infrastructure.\nMobile applications demand new levels of reliability. A map application is not\nvery useful if the map service it relies on is down when you need it. Yes, map tiles\ncan be cached but real-time trafﬁc reports less so. As mobile apps become more and\nmore life-critical, the reliability of their supporting services becomes more impor-\ntant. As reliability improvements leap forward, more life-critical applications\nbecome possible.\nCosts\nAs computing was done at even larger scales, new economics present themselves.\nIf computing becomes cheaper at larger scales, then it becomes advantageous to\n",
      "content_length": 2479,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "470\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nbuild a larger infrastructure. If you build an infrastructure larger than you need,\nyou simply have to develop the technology that lets you “rent” the spare capacity\nto others. The part you use is cheaper than it would have been otherwise; the part\nyou don’t use turns a proﬁt. If the proﬁt is small, it offsets the cost of your infra-\nstructure. If the proﬁt is large enough, it could pay for all of your infrastructure. At\nthat point your computing infrastructure becomes “free” for you. Do things right\nand you could have an infrastructure with a negative cost. Imagine running all of\nAmazon’s infrastructure and having someone else pay for it. Now imagine trying\nto start a new web site that sells books when your competition gets its infrastruc-\nture “for free.” These are the economic aspirations that drive the supplier side of\ncloud computing.\nIn the cloud computing era, the scale provides economics that make the cost\na new order less expensive. This frees up enough headroom to price the service\nat less than customers could do it themselves and delivers additional proﬁt that\nsubsidizes the provider’s infrastructure. Anything done to improve the efﬁciency\nof operations either adds to the service provider’s proﬁtability or enables it to offer\nservices at a lower cost than the competition.\nTo understand the consumer demand for cloud computing, we need to look\nat the costs associated with small-scale computing systems. At a small scale one\ncannot take advantage of the economics of distributed computing. Instead, one\nmust achieve reliability through more expensive hardware. Automation makes less\nsense when doing things at a small scale, which drives up the operational cost.\nWhen automation is created, it is more expensive because the cost is not amortized\nover as many uses. Many distributed computing technologies require people with\nspecialized knowledge that a small company does not possess, since at a small scale\none must hire generalists and can’t afford a full-time person (or team) to oversee\njust one aspect of the system. The use of external consultants when such expertise\nis needed can be expensive.\nMany of these problems are mitigated when small-scale computing is done\nby renting space on a cloud infrastructure. Customers get to take advantage of\nthe lower cost and greater power efﬁciency. Difﬁcult-to-maintain services such as\nspecialized storage and networking technology can be offered in a way that hides\nthe difﬁcult behind-the-scenes management such systems require.\nThere are also non-cost advantages for the customers. Elasticity is the ability\nto increase and decrease the amount of resources consumed dynamically. With\nmany applications, being able to spin up many servers for a short amount of\ntime is valuable. Suppose you have an advertising campaign that will last for just\none week. While the ads are running, you may need hundreds of web servers\nto handle the trafﬁc. Being able to acquire so many servers in hours is invalu-\nable. Most companies would need months or a year to set up so many servers.\n",
      "content_length": 3131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 502,
      "content": "B.5\nThe Cloud Computing Era (2010–present)\n471\nAn advertising agency previously would not have the ability to do so. Now,\nwithout even the knowledge of how to build a datacenter, an ad agency can have\nall the systems it needs. Possibly more important is that at the end of the campaign,\nthe servers can be “given back” to the cloud provider. Doing that the old way with\nphysical hardware would be impractical.\nAnother non-cost advantage for many companies is that cloud computing\nenabled other departments to make an end-run around IT departments that had\nbecome recalcitrant or difﬁcult to deal with. The ability to get the computing\nresources they need by clicking a mouse, instead of spending months of argu-\ning with an uncooperative and underfunded IT department, is appealing. We are\nashamed to admit that this is true but it is often cited as a reason people adopt\ncloud computing services.\nScaling and High Availability\nMeeting the new requirements of scaling and high availability in the cloud com-\nputing era requires new paradigms. Lower latency is achieved primarily through\nfaster storage technology and faster ways to move information around.\nIn this era SSDs have replaced disks. SSDs are faster because there are no mov-\ning parts. There is no wait for a read head to move to the right part of a disk platter,\nno wait for the platter to rotate to the right position. SSDs are more expensive per\ngigabyte but the total cost of ownership is lower. Suppose you require 10 database\nserver replicas to provide enough horsepower to provide a service at the latency\nrequired. While using SSDs would be more expensive, the same latency can be\nprovided with fewer machines, often just two or three machines in total. The SSDs\nare more expensive, it is true—but not as expensive as needing seven additional\nmachines.\nService latency is also reduced by reducing the latency of internal communi-\ncation. In the past, information sent between two machines went through many\nlayers of technology. The information was converted to a “wire format,” which\nmeant making a copy read for transmission and putting it in a packet. The packet\nthen went through the operating system’s TCP/IP layer and device layer, through\nthe network, and then reached the other machine, where the process was reversed.\nEach of these steps added latency. Most or all of this latency has now been removed\nthrough technologies that permit direct memory access between machines. Some-\ntimes these technologies even bypass the CPU of the source or destination machine.\nThe result is the ability to pass information between machines nearly as fast as\nreading local RAM. The latency is so low that it has caused underlying RPC\nmechanisms to be redesigned from scratch to fully take advantage of the new\ncapabilities.\n",
      "content_length": 2788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "472\nAppendix B\nThe Origins and Future of Distributed Computing and Clouds\nTechnology\nThe technology that drives cloud computing itself is the ability to segment infra-\nstructure such that one “tenant” cannot interfere with another. This requires net-\nworking technologies that allow segmenting computers from each other at a very\nﬁne granularity such as programmable VLANs and software-deﬁned networks. It\nrequires the ability to partition one large machine into many smaller ones—that\nis, virtualization technology. It requires control panels, APIs, and various self-\nservice administration capabilities so that customers support themselves and do\nnot add considerable labor cost to datacenter operations. It requires storage and\nother services to attract customers.\nAmazon was the ﬁrst company to develop such a system. Amazon Elastic\nCompute Cloud (Amazon EC2) was the ﬁrst major company to rent virtual\nmachines on its infrastructure to subsidize its cost. The term “elastic” comes from\nthe fact that this approach makes it so fast to expand and contract the amount\nof resources being used. One of the most attractive features is how quickly one\ncan spin up new machines. Even more attractive is how quickly one can dispose\nof machines when they aren’t needed. Suppose you needed 1000 machines for a\nmonth-long web advertising campaign. It would be an arduous task to set up that\nmany machines yourself and difﬁcult to get rid of them a month later. With EC2,\nyou can spin them all up with a mouse click or a few API calls. When the campaign\nis over, releasing the machines is just as simple.\nSuch “tenant” systems are just getting started. We are just now beginning to\nunderstand these new economics and their ramiﬁcations.\nB.6 Conclusion\nThe goal of this appendix was to explain the history of technologies used for pro-\nviding web services. The techniques described here form the basis for the platform\noptions introduced in Chapter 3, the web architectures presented in Chapter 4,\nand the scaling and resiliency techniques described in Chapters 5 and 6. The other\nchapters in this book reﬂect the operational practices that make all of the above\nwork.\nThe economics of computing change over time. Faster and more reliable com-\nputing technology had a super-linear cost curve in the pre-web and ﬁrst web eras.\nThe second web era was enabled by linear cost curves. Cloud computing gives\nus sub-linear cost curves. These changes happened by taking advantage of com-\nmoditization and standardization, shifting to open source software, building more\nreliability through software instead of hardware, and replacing labor-intensive\noperations with more software.\nEvery order-of-magnitude improvement in the cost of computing enables a\nnew era of applications, each of which was unimaginable just a few years before.\n",
      "content_length": 2817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "Exercises\n473\nCould the person who used an 8-bit computer to balance his or her checkbook in\n1983 ever have imagined Facebook or Google Glass?\nWhat will follow cloud computing is anyone’s guess. The applications of\ntomorrow will demand computing resources that are orders-of-magnitude faster,\nhave lower latency, and are less expensive. It will be very exciting to see what\ndevelops.\nExercises\n1. This appendix provides a history of ﬁve eras of computing technology: pre-\nweb era, ﬁrst web era, dot-bomb era, second web era, and cloud computing\nera. For each era, describe the level of economic prosperity, the technological\nimprovements, and the expectations for reliability and computing power.\n2. The practice of owning the entire process instead of using external providers\nfor certain steps or parts is called “vertical integration.” Which examples of\nvertical integration were described in this appendix? What were the beneﬁts?\n3. What role did open source software play in the maturation of the second\nweb era?\n4. Describe the redundant reliability levels replaced by ROC-style distributed\ncomputing.\n5. The history discussed in this appendix is described as inevitable. Do you agree\nor disagree? Support your case.\n6. What era will follow cloud computing? Identify the trends described in\nthis appendix and extrapolate them to predict the future based on past\nperformance.\n",
      "content_length": 1381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 505,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "Appendix C\nScaling Terminology and\nConcepts\nThe essence of life is statistical\nimprobability on a colossal scale.\n—Richard Dawkins\nYou’ve probably experienced software that works well with a small amount of data\nbut gets slower and slower as more data is added. Some systems get a little slower\nwith more data. With others, the slow-down is much bigger, often dramatically so.\nWhen discussing scalability, there is terminology that describes this phe-\nnomenon. This enables us to communicate with each other with great speciﬁcity.\nThis appendix describes some basic terminology, a more mathematical way of\ndescribing the same concepts, and ﬁnally some caveats for how modern systems\ndo or do not act as one would expect.\nC.1 Constant, Linear, and Exponential Scaling\nThere is a lot of terminology related to describing how systems perform and scale.\nThree terms are used so commonly that to be a professional system administrator\nyou should be able to use these terms conversationally. They describe how a system\nperforms as data size grows: the system is unaffected, gets slower, or gets much\nslower.\n• Constant Scaling: No matter the scale of the input, performance does not\nchange. For example, a hash-table lookup in RAM can always be done in con-\nstant time whether the table contains 100 items or 100 million items. It would\nbe nice if all systems were so fast, but such algorithms are rare. In fact, even a\nhash-table lookup is limited to situations where the data ﬁts in RAM.\n475\n",
      "content_length": 1488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "476\nAppendix C\nScaling Terminology and Concepts\n• Linear Scaling: As input size grows, the system slows down proportionately.\nFor example, twice as much data will require twice as much processing time.\nSuppose an authentication system for a service with 10,000 users takes 10 ms to\nauthenticate each request. When there are 20,000 users, the system might take\n60 ms. With 30,000 users, the system might take 110 ms. Each unit of growth\nresulted in a slowdown that was the same size (50 ms slower per 10,000 users\nadded). Therefore we can classify this as a system with linear performance\nwith respect to the size of the user database.\n• Exponential Scaling: As input size grows, the system slows down dispro-\nportionately. Continuing our authentication system example, if adding more\nusers resulted in response times of 10 ms, 100 ms, and 1000 ms, this would\nbe exponential scaling. A system that slows down at such a rate, if it inter-\nacts with all parts of our system, would be a disaster! It might be ﬁne if the\ninput size is not expected to change and, at the current size, the system is fast\nenough. This assumption carries a high risk.\nC.2 Big O Notation\nO\n()\nor “Big O notation” is used to classify a system based on how it responds\nto changes in input size. It is the more mathematical way of describing a sys-\ntem’s behavior. O\n()\nnotation comes from computer science. The letter “O” is used\nbecause the growth rate of an algorithm’s run-time is known as its “order.”\nHere is a list of common Big O terms that a system administrator should be\nfamiliar with. The ﬁrst three we’ve already described:\n• O\n(\n1\n)\n: Constant Scaling: No change in performance no matter the size of the\ninput.\n• O\n(\nn\n)\n: Linear Scaling: Gets slower in proportion to the size of the input.\n• O\n(\nnm)\n: Exponential Scaling: Performance worsens exponentially.\n• O\n(\nn2)\n: Quadratic Scaling: Performance worsens relative to the square of the\nsize of input. Quadratic scaling is a special case of exponential scaling, where m\nis 2. People tend to refer to systems as scaling “exponentially” when they actu-\nally mean “quadratically.” This is so common that one rarely hears the term\n“quadratic” anymore except when someone is being very speciﬁc (or pedantic)\nabout the performance curve.\n• O\n(\nlog n\n)\n: Logarithmic Scaling: Performance worsens proportional to the log2\nof the size of input. Performance asymptotically levels off as input size grows.\nFor example, a binary search grows slower as the size of the corpus being\nsearched grows, but less than linearly.\n• O\n(\nn log n\n)\n: Loglinear Scaling: Performance worsens more than linearly, with\nthe “more than” component being proportional to log2 of the size of input.\nThink of this as linear scaling with a small but ever-growing performance\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "C.2\nBig O Notation\n477\nsurcharge added as the input size grows. People often incorrectly use the term\n“logarithmic scaling” when they actually mean “loglinear scaling.” You can\nuse the term “loglinear” when you want to sound like a true expert.\n• O\n(\nn!\n)\n: Factorial Scaling: Performance worsens proportional to the factorial of\nthe size of input. In other words, performance gets bad so quickly that each\nadditional unit of size worsens performance by a leap as big as all previous\nleaps put together plus some more! O\n(\nn!\n)\nalgorithms are usually a worst-case\nscenario. For example, the breakthrough that permits Google PageRank and\nFacebook SocialRank to work so well came from computer science research\nthat invented replacements for O\n(\nn!\n)\nalgorithms.\nThe term sub-linear refers to anything that grows less than linearly, such as con-\nstant and logarithmic scaling. Super-linear refers to anything that grows faster,\nsuch as exponential and factorial scaling.\nIn addition to being used to describe scaling, these terms are often used to\ndescribe growth. One might describe the increase in customers being attracted to\nyour business as growing linearly or exponentially. The run-time of a system might\nbe described as growing in similar terms.\nSuper-linear systems sound awful compared to sub-linear systems. Why not\nalways use algorithms that are constant or linear? The simplest reason is that often\nalgorithms of that order don’t exist. Sorting algorithms have to touch every item\nat least once, eliminating the possibility of O\n(\n1\n)\nalgorithms. There is one O\n(\nn\n)\nsort\nalgorithm but it works on only certain kinds of data.\nAnother reason is that faster algorithms often require additional work ahead\nof time: for example, building an index makes future searches faster but requires\nthe overhead and complexity of building and maintaining the index. That effort of\ndeveloping, testing, and maintaining such indexing code may not be worth it if the\nsystem’s performance is sufﬁcient as is.\nAt small values, systems of different order may be equivalent. What’s big-\nger, x or x2? Your gut reaction may be that the square of something will be bigger\nthan the original number. However, if x is 0.5, then this is not true. 0.52 = 0.25,\nwhich is smaller than 0.5. Likewise, an O\n(\nn\n)\nalgorithm may be slower than an\nO\n(\nn2)\nalgorithm for very small inputs. Also an O\n(\nn2)\nalgorithm may be the easiest\nto implement, even though it wouldn’t be the most efﬁcient for larger-sized input.\nThus the more complex algorithm would be a waste of time to develop. It would\nalso be riskier. More complex code is more likely to have bugs.\nIt is important to remember that the concept of O\n()\nis a gross generaliza-\ntion that focuses on the trend as its input grows, not the speciﬁc run-time. For\nexample, two systems that are both O\n(\nn2)\nwill not to have the exact same per-\nformance. The shared order simply indicates that both scale worse than, say, an\nO\n(\nn\n)\nsystem.\nO\n()\nnotation is an indicator of the biggest factor, not all factors. A real sys-\ntem may be dominated by an O\n(\nn\n)\nlookup, but might also involve many O\n(\n1\n)\n",
      "content_length": 3130,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "478\nAppendix C\nScaling Terminology and Concepts\noperations and possibly even some O\n(\nn2)\noperations, albeit ones that are inconse-\nquential for other reasons.\nFor example, an API call might perform an O\n(\nn\n)\nlookup on a small list of\nauthorized users and then spend the majority of its time doing an O\n(\n1\n)\nlookup in\na large database. If the list of authorized users is tiny (say, it veriﬁes the user is one\nof three people permitted to use the system) and done in RAM, it is inconsequen-\ntial compared to the database lookup. Therefore this API call would be considered\nO\n(\n1\n)\n, not O\n(\nn\n)\n. Such a system may run at acceptable speed for years but then\nthe number of authorized users surges, possibly due to a management change\nthat authorizes thousands of additional users. Suddenly this O\n(\nn\n)\nlookup starts\nto dominate the run-time of the API call. Surprises like this happen all the time in\nproduction systems.\nWhile the “O” in Big O notation stands for “order,” conversationally you may\nhear the word “order” used as shorthand for order of magnitude. Order of mag-\nnitude is related but different. Order of magnitude means, essentially, how many\ndigits are in a number that describes a size (the magnitude of a digit literally means\nif it is in the 1s, 10s, 100s, or some other place). If you work for a company with\n100 employees and your friend works at a company with 1000 employees, then\nyour friend works at a company that is an order of magnitude larger. You may hear\nphrases like an algorithm working for situations “for 1 million users but breaks\ndown on larger order of magnitude.” You might hear references to a system pro-\ncessing “on the order of 2000 queries per second,” which is a fancy way of saying\n“approximately 2000” with the implication that this is a very coarse estimate.\nC.3 Limitations of Big O Notation\nO\n()\nnotation relates to the number of operations an algorithm will perform. How-\never, not all operations take the same amount of time. Reading two adjacent records\nfrom a database, if both ﬁt into the same disk block, is essentially one disk read fol-\nlowed by two reads from the RAM that caches the entire block. Two algorithms\nwith the same O\n()\norder, one that takes advantage of this fact and one that does\nnot, will have extremely different performance.\nIn Kamp’s (2010) description of the Varnish HTTP accelerator, he explains that\nby taking advantage of a deep understanding of how a modern OS handles disk\nI/O, signiﬁcantly better performance can be achieved than with naive O\n()\ncompar-\nisons. Varnish was able to replace 12 overloaded machines running a competing\nsoftware package with three machines that were 90 percent idle. He writes:\nWhat good is an O\n(\nlog2 n\n)\nalgorithm if those operations cause page faults and slow\ndisk operations? For most relevant datasets an O\n(\nn\n)\nor even an O\n(\nn2)\nalgorithm,\nwhich avoids page faults, will run circles around it.\n",
      "content_length": 2917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "C.3\nLimitations of Big O Notation\n479\nThe improvement came from structuring his heap in such a way that nodes\nwould avoid generating virtual memory page faults. Kamp arranged data struc-\ntures such that parent and child nodes tended to be on the same page in virtual\nmemory.\nA page fault occurs when a program accesses virtual memory that is not cur-\nrently in the computer’s RAM. The operating system pauses the process, reads the\ndata from disk, and unpauses the process when the read is complete. Since mem-\nory is handled in 4KB blocks, the ﬁrst access to a block may cause a page fault but\nlater accesses within that same 4KB area do not.\nA page fault takes about as long as 10 million instructions could run. In other\nwords, spending 10,000 instructions to avoid a page fault has a 1000-to-1 payback.\nBy carefully organizing where in memory objects are stored, Kamp was able to\neliminate many page faults.\nHis message to the industry, titled “You’re Doing It Wrong,” details in very\ntechnical terms how most computer science education is still teaching algorithm\ndesign as appropriate for computer systems that have not existed for 30 years.\nModern CPUs have many complexities that make performance non-intuitive.\nLinear access is signiﬁcantly faster than random access. Instructions execute in par-\nallel when they are provably independent. The performance of a CPU drops when\nmultiple CPUs are accessing the same part of memory.\nFor example, reading every element in an array in row order is signiﬁcantly\nfaster than reading the elements in column order. If the rows are bigger than the\nCPU’s memory cache, the latter is essentially going to be a cache “miss” for every\nread. The former would be linear memory reads, which CPUs are optimized to do\nquickly. Even though either way reads the same number of bytes from memory,\nthe former is faster.\nMore surprisingly, a loop that reads every element of an array takes approxi-\nmately the same amount of time to execute as a loop that reads every 16th element\nof the same array. Even though the former does 1/16th of the number of opera-\ntions, the number of RAM cache misses is the same for both loops. Reading blocks\nof memory from RAM to the CPU’s L1 (Level 1) cache is slow and dominates the\nrun-time of either algorithm. The fact that the former runs faster is due to the fact\nthat there are special instructions for sequentially reading memory.\nAbove and beyond RAM, virtual memory, and disk issues, we also have to\nconsider the effect of threads, multiple CPUs trying to access the same data, locks,\nand mutexes. All of these cloud the issue. You can’t really tell how fast an algo-\nrithm will be without benchmarking. O\n()\nnotation becomes a general guide, not\nan absolute.\nThat said, O\n()\nnotation is still useful when conversationally describing\nsystems and communicating with other system administrators. Therefore under-\nstanding the nomenclature is essential to being a system administrator today.\n",
      "content_length": 2965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "Appendix D\nTemplates and Examples\nD.1 Design Document Template\nBelow is a simple design document template as described in Section 13.2.\n.\nTitle:\nDate:\nAuthor(s): (add authors, please link to their email addresses)\nReviewers(s): (add reviewers, please link to their email addresses)\nApprovers(s): (add approvers, please link to their email addresses)\nRevision Number:\nStatus: (draft, in review, approved, in progress)\nExecutive Summary:\n(2–4 sentences explaining the project)\nGoals:\n(bullet list describing the problem being solved)\nNon-goals:\n(bullet list describing the limits of the project)\nBackground:\n(terminology and history one needs to know to understand the design)\nHigh-Level Design:\n(a brief, high-level description of the design; diagrams are helpful)\n481\n",
      "content_length": 768,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "482\nAppendix D\nTemplates and Examples\n.\nDetailed Design:\n(the design in detail; usually a top-down description)\nAlternatives Considered:\n(alternatives considered and rejected; include pros and cons of each)\nSecurity Concerns:\n(security/privacy ramiﬁcations and how those concerns are mitigated)\nD.2 Design Document Example\nBelow is an example design document as described in Section 13.2.\n.\nTo illustrate the principle of design documents, we have created a sample\ndesign document with most of the parts included:\nTitle: New Monitoring System\nDate: 2014-07-19\nAuthor(s): Strata Chalup <src@example.com>\nand Tom Limoncelli <tal@example.com>\nReviewers: Joe, Mary, Jane\nApprovers: Security Operations 2014-08-03, Chris (Build System Tech Lead)\n2014-08-04, Sara (Director of Ops) 2014-08-10\nRevision Number: 1.0\nStatus: Approved\nExecutive Summary:\nCreate a monitoring system for devices that will support real-time alerting via\npagers and deploy to production cluster.\nGoals:\nA monitoring system for our network:\n• Be able to monitor at least 10,000 devices, 500 attributes each, collected\neach minute.\n",
      "content_length": 1099,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "D.2\nDesign Document Example\n483\n.\n• Must support real-time alerting via SMS.\n• Must retain data for 30 years.\nNon-goals:\n• Monitoring external systems.\n• Building chat functionality into the system.\nBackground:\nHistorically we have used home-brew monitoring scripts, but we keep having\nto adjust them based on changing platforms. We want to have a standardized\nway of monitoring that doesn’t require development time on our part as we\nadd platforms.\nHigh-Level Design:\nThe plan is to use Bosun with remote collectors. The server will be the current\ncorporate standard conﬁguration for server hardware and Linux. The server\nwill be named bosun01.example.com. It will be in the Phoenix and Toronto\ndatacenters. Monitoring conﬁguration will be kept in the Git “sysadmin con-\nﬁgs” repo, in the top-level directory /monitor.\nDetailed Design:\nServer conﬁguration:\nDell 720XD with 64G RAM, 8 disks in RAID6 conﬁguration\nDebian Linux (standard corporate “server conﬁguration”)\nBosun package name is “bosun-server.”\n• Any system being monitored will have the “bosun-client” package\ninstalled.\n• Backups will be performed nightly using the standard corporate backup\nmechanism.\n• (The full document would include a description of operational tasks such\nas adding new monitoring collectors, and alerting rules.)\nCost Projections:\n• Initial cost [cost of server].\n• Software is open source.\n• One half-time FTE for 3 weeks to set up monitoring and launch.\n",
      "content_length": 1443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "484\nAppendix D\nTemplates and Examples\n.\nSchedule after approval:\n1. 1 week to procure hardware.\n2. 2 days of installation and testing.\n3. “Go live” on week 3.\nAlternatives Considered:\nZappix and SkunkStorm were both considered, but the feature set of Bosun is\nmore closely aligned with the project goals. [link to comparison charts]\nSpecial Constraints:\nBosun stores SNMP “community strings,” which are essentially passwords,\nin a password vault. The storage method has been reviewed and approved by\nSecurity Operations.\nD.3 Sample Postmortem Template\nBelow is a simple postmortem template as described in Section 14.3.2.\n.\nTitle:\nReport Status:\nExecutive Summary:\nList what happened, who was affected, and what are the key recommendations for\nprevention in the future (especially any that will require budget or executive approval).\nOutage Description:\nA general description of the outage, from a technical perspective of what hap-\npened.\nAffected users:\nWho was affected.\nStart Date/Time:\nEnd Date/Time:\nDuration:\nTimeline:\nA minute-by-minute timeline assembled from system logs, chat logs, emails, and\nwhatever other resources are available.\nContributing Conditions Analysis:\nWhat were the contributing causes that led to the outage?\n",
      "content_length": 1237,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "D.3\nSample Postmortem Template\n485\n.\nWhat went well?\nA bullet list of what went well. This is a good opportunity to thank anyone who\nwent above and beyond expectations to help out.\nWhat could have gone better?\nA bullet list of which actions could have been taken that would have improved\nhow fast we were back in service, the techniques used, and so on.\nRecommendations:\nA bullet list of recommendations that would prevent this outage in the future.\nEach should be actionable and measurable. Good example: “Monitor disk space for\ndatabase server and alert if less than 20 percent is available.” Bad example: “Improve\nmonitoring.” File a bug/feature request for each recommendation; list bug IDs here.\nNames of people involved:\nList of the people involved in the resolution of the outage.\n",
      "content_length": 788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "Appendix E\nRecommended Reading\nTo achieve great things, two things\nare needed; a plan, and\nnot quite enough time.\n—Leonard Bernstein\nDevOps:\n• The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win\n(Kim et al. 2013)\nA ﬁctional story that teaches the Three Ways of DevOps.\n• Building a DevOps Culture (Walls 2013)\nStrategies for building a DevOps culture, including aligning incentives,\ndeﬁning meaningful and achievable goals, and creating a supportive team\nenvironment.\n• Continuous Delivery: Reliable Software Releases through Build, Test, and Deploy-\nment Automation (Humble & Farley 2010)\nThe canonical book about service delivery platforms.\n• Release It!: Design and Deploy Production-Ready Software (Nygard 2007)\nDetailed coverage and examples of how to implement many of the ideas in\nChapter 11.\n• Blameless PostMortems and a Just Culture (Allspaw 2009)\nTheory and practice of postmortems.\n• A Mature Role for Automation (Allspaw 2012c)\nWhy “Automate Everything!” is bad, and what to do instead.\n• Each Necessary, But Only Jointly Sufﬁcient (Allspaw 2012a)\nMyths and limits of “root cause analysis” and “The Five Why’s.”\n487\n",
      "content_length": 1154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "488\nAppendix E\nRecommended Reading\nITIL:\n• Owning ITIL: A Skeptical Guide for Decision-Makers (England 2009)\nA gentle introduction to ITIL with honest advice about how to make it work.\nTheory:\n• In Search of Certainty: The Science of Our Information Infrastructure (Burgess 2013)\nThe physics of distributed systems.\n• Promise Theory: Principles and Applications (Burgess & Bergstra 2014)\nThe theory of conﬁguration management and change management.\nClassic Google Papers:\n• “The Anatomy of a Large-Scale Hypertextual Web Search Engine” (Brin &\nPage 1998)\nThe ﬁrst paper describing the Google search engine.\n• “Web Search for a Planet: The Google Cluster Architecture” (Barroso, Dean &\nHölzle 2003)\nGoogle’s ﬁrst paper revealing how clusters are designed using commodity\nclass PCs with fault-tolerant software.\n• “The Google File System” (Ghemawat, Gobioff & Leung 2003)\n• ”MapReduce: Simpliﬁed Data Processing on Large Clusters” (Dean &\nGhemawat 2004)\n• “Bigtable: A Distributed Storage System for Structured Data” (Chang et al.\n2006)\n• “The Chubby Lock Service for Loosely-Coupled Distributed Systems”\n(Burrows 2006)\n• “Spanner: Google’s Globally-Distributed Database” (Corbett et al. 2012)\n• “The Tail at Scale” (Dean & Barroso 2013)\nImproving latency means ﬁxing the last percentile.\n• “Failure Trends in a Large Disk Drive Population” (Pinheiro, Weber & Barroso\n2007)\nLongitudinal study of hard drive failures.\n• “DRAM Errors in the Wild: A Large-Scale Field Study” (Schroeder, Pinheiro &\nWeber 2009)\nLongitudinal study of DRAM failures.\nClassic Facebook Papers:\n• “Cassandra: A Decentralized Structured Storage System” (Lakshman & Malik\n2010)\n",
      "content_length": 1648,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "Recommended Reading\n489\nScalability:\n• The Art of Scalability: Scalable Web Architecture, Processes, and Organizations for\nthe Modern Enterprise (Abbott & Fisher 2009)\nAn extensive catalog of techniques and discussion of scalability of people,\nprocess, and technology.\n• Scalability Rules: 50 Principles for Scaling Web Sites (Abbott & Fisher 2011)\nA slimmer volume, focused on technical strategy and techniques.\nUNIX Internals:\n• The Design and Implementation of the FreeBSD Operating System (McKusick,\nNeville-Neil & Watson 2014)\nThis is the best deep-dive into how UNIX-like operating systems work.\nWhile the examples are all FreeBSD, Linux users will beneﬁt from the theory\nand technical details the book provides.\nUNIX Systems Programming:\n• The UNIX Programming Environment (Kernighan & Pike 1984)\n• Advanced Programming in the UNIX Environment (Stevens & Rago 2013)\n• UNIX Network Programming (Stevens 1998)\nNetwork Protocols:\n• TCP/IP Illustrated, Volume 1: The Protocols (Stevens & Fall 2011)\n• TCP/IP Illustrated, Volume 2: The Implementation (Wright & Stevens 1995)\n• TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP, and the UNIX\nDomain Protocols (Stevens & Wright 1996)\n",
      "content_length": 1197,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "Bibliography\nAbbott, M., & Fisher, M. (2009). The Art of Scalability: Scalable Web Architecture, Processes, and\nOrganizations for the Modern Enterprise, Pearson Education.\nAbbott, M., & Fisher, M. (2011). Scalability Rules: 50 Principles for Scaling Web Sites, Pearson\nEducation.\nAbts, D., & Felderman, B. (2012). A guided tour through data-center networking, Queue\n10(5): 10:10–10:23.\nhttp://queue.acm.org/detail.cfm?id=2208919\nAdya, A., Cooper, G., Myers, D., & Piatek, M. (2011). Thialﬁ: A client notiﬁcation service\nfor internet-scale applications, Proceedings of the 23rd ACM Symposium on Operating\nSystems Principles (SOSP), pp. 129–142.\nhttp://research.google.com/pubs/pub37474.html\nAllspaw, J. (2009). Blameless postmortems and a just culture.\nhttp://codeascraft.com/2012/05/22/blameless-postmortems.\nAllspaw, J. (2012a). Each necessary, but only jointly sufﬁcient.\nhttp://www.kitchensoap.com/2012/02/10/each-necessary-but-only-jointly-\nsufficient.\nAllspaw, J. (2012b). Fault injection in production, Queue 10(8): 30:30–30:35.\nhttp://queue.acm.org/detail.cfm?id=2353017\nAllspaw, J. (2012c). A mature role for automation.\nhttp://www.kitchensoap.com/2012/09/21/a-mature-role-for-automation-\npart-i\nAnderson, C. (2012). Idea ﬁve: Software will eat the world, Wired.\nhttp://www.wired.com/business/2012/04/ff_andreessen/5\nArmbrust, M., Fox, A., Grifﬁth, R., Joseph, A. D., Katz, R., Konwinski, A., Lee, G.,\nPatterson, D., Rabkin, A., Stoica, I., & Zaharia, M. (2010). A view of cloud computing,\nCommunications of the ACM 53(4): 50–58.\nhttp://cacm.acm.org/magazines/2010/4/81493-a-view-of-cloud-computing\nBarroso, L. A., Dean, J., & Hölzle, U. (2003). Web search for a planet: The Google cluster\narchitecture, IEEE Micro 23(2): 22–28.\nhttp://research.google.com/archive/googlecluster.html\n491\n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 523,
      "content": "492\nBibliography\nBarroso, L. A., Clidaras, J., & Hölzle, U. (2013). The Datacenter as a Computer: An Introduction\nto the Design of Warehouse-Scale Machines, 2nd ed., Morgan and Claypool Publishers.\nhttp://research.google.com/pubs/pub41606.html\nBeck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M.,\nGrenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R. C.,\nMellor, S., Schwaber, K., Sutherland, J., & Thomas, D. (2001). Manifesto for agile\nsoftware development.\nhttp://agilemanifesto.org\nBlack, B. (2009). EC2 origins.\nhttp://blog.b3k.us/2009/01/25/ec2-origins.html\nBrandt, K. (2014). OODA for sysadmins.\nhttp://blog.serverfault.com/2012/07/18/ooda-for-sysadmins\nBrin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine,\nProceedings of the Seventh International Conference on World Wide Web 7, WWW7,\nElsevier Science Publishers, Amsterdam, Netherlands, pp. 107–117.\nhttp://dl.acm.org/citation.cfm?id=297805.297827\nBurgess, M. (2013). In Search of Certainty: The Science of Our Information Infrastructure,\nCreatespace Independent Publishing.\nBurgess, M., & Bergstra, J. (2014). Promise Theory: Principles and Applications, On Demand\nPublishing, Create Space.\nBurrows, M. (2006). The Chubby lock service for loosely-coupled distributed systems,\nProceedings of the 7th Symposium on Operating Systems Design and Implementation,\nOSDI ’06, USENIX Association, Berkeley, CA, pp. 335–350.\nhttp://research.google.com/archive/chubby.html\nCandea, G., & Fox, A. (2003). Crash-only software, HotOS, pp. 67–72.\nhttps://www.usenix.org/conference/hotos-ix/crash-only-software\nChang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T.,\nFikes, A., & Gruber, R. E. (2006). Bigtable: A distributed storage system for structured\ndata, Proceedings of the 7th USENIX Symposium on Operating Systems Design and\nImplementation, pp. 205–218.\nChapman, B. (2005). Incident command for IT: What we can learn from the ﬁre\ndepartment., LISA, USENIX.\nhttp://www.greatcircle.com/presentations\nCheswick, W. R., Bellovin, S. M., & Rubin, A. D. (2003). Firewalls and Internet Security:\nRepelling the Wiley Hacker, Addison-Wesley.\nhttp://books.google.com/books?id=_ZqIh0IbcrgC\nClos, C. (1953). A study of non-blocking switching networks, The Bell System Technical\nJournal 32(2): 406–424.\nhttp://www.alcatel-lucent.com/bstj/vol32-1953/articles/bstj32-2-406.pdf\nCorbett, J. C., Dean, J., Epstein, M., Fikes, A., Frost, C., Furman, J. J., Ghemawat, S.,\nGubarev, A., Heiser, C., Hochschild, P., Hsieh, W., Kanthak, S., Kogan, E., Li, H.,\nLloyd, A., Melnik, S., Mwaura, D., Nagle, D., Quinlan, S., Rao, R., Rolig, L., Saito, Y.,\n",
      "content_length": 2709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 524,
      "content": "Bibliography\n493\nSzymaniak, M., Taylor, C., Wang, R., & Woodford, D. (2012). Spanner: Google’s\nglobally-distributed database, Proceedings of the 10th USENIX Conference on Operating\nSystems Design and Implementation, OSDI’12, USENIX Association, Berkeley, CA,\npp. 251–264.\nhttp://research.google.com/archive/spanner.html\nDean, J. (2009). Designs, lessons and advice from building large distributed systems.\nhttp://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-\nladis2009.pdf\nDean, J., & Barroso, L. A. (2013). The tail at scale, Communications of the ACM 56(2): 74–80.\nhttp://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale\nDean, J., & Ghemawat, S. (2004). MapReduce: Simpliﬁed data processing on large clusters,\nOSDI’04: Proceedings of the 6th USENIX Symposium on Operating Systems Design and\nImplementation, USENIX Association.\nDebois, P. (2010a). Jedi4Ever blog.\nhttp://www.jedi.be/blog\nDebois, P. (2010b). DevOps is a verb. Slideshare of talk.\nhttp://www.slideshare.net/jedi4ever/devops-is-a-verb-its-all-about-\nfeedback-13174519\nDeming, W. (2000). Out of the Crisis, Massachusetts Institute of Technology, Center for\nAdvanced Engineering Study.\nDevOps-Toolchain. (2010). A set of best practices useful to those practicing DevOps\nmethodology.\nhttp://code.google.com/p/devops-toolchain/wiki/BestPractices\nDickson, C. (2013). A working theory of monitoring, presented as part of the 27th Large\nInstallation System Administration Conference, USENIX, Berkeley, CA.\nhttps://www.usenix.org/conference/lisa13/working-theory-monitoring\nEdwards, D. (2010). DevOps is not a technology problem.\nhttp://dev2ops.org/2010/11/devops-is-not-a-technology-problem-devops-\nis-a-business-problem\nEdwards, D. (2012). Use DevOps to turn IT into a strategic weapon.\nhttp://dev2ops.org/2012/09/use-devops-to-turn-it-into-a-strategic-\nweapon\nEdwards, D., & Shortland, A. (2012). Integrating DevOps tools into a service delivery\nplatform.\nhttp://dev2ops.org/2012/07/integrating-devops-tools-into-a-service-\ndelivery-platform-video\nEngland, R. (2009). Owning ITIL: A Skeptical Guide for Decision-Makers, Two Hills.\nFitts, P. (1951). Human engineering for an effective air navigation and trafﬁc-control\nsystem, Technical Report ATI-133954, Ohio State Research Foundation.\nhttp://www.skybrary.aero/bookshelf/books/355.pdf\nFlack, M., & Wiese, K. (1977). The Story about Ping, Picture Pufﬁn Books, Pufﬁn\nBooks.\n",
      "content_length": 2401,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 525,
      "content": "494\nBibliography\nGallagher, S. (2012). Built to win: Deep inside Obama’s campaign tech.\nhttp://arstechnica.com/information-technology/2012/11/built-to-win-\ndeep-inside-obamas-campaign-tech\nGhemawat, S., Gobioff, H., & Leung, S.-T. (2003). The Google ﬁle system, Proceedings of the\nNineteenth ACM Symposium on Operating Systems Principles, SOSP ’03, ACM,\nNew York, NY, pp. 29–43.\nhttp://doi.acm.org/10.1145/945445.945450\nGilbert, S., & Lynch, N. (2002). Brewer’s conjecture and the feasibility of consistent,\navailable, partition-tolerant web services, SIGACT News 33(2): 51–59.\nhttp://doi.acm.org/10.1145/564585.564601\nGoogle. (2012). Efﬁciency: How we do it.\nhttp://www.google.com/about/datacenters/efficiency/internal\nGruver, G., Young, M., & Fulghum, P. (2012). A Practical Approach to Large-Scale Agile\nDevelopment: How HP Transformed HP LaserJet FutureSmart Firmware, Addison-Wesley.\nHaynes, D. (2013). Understanding CPU steal time: When should you be worried.\nhttp://blog.scoutapp.com/articles/2013/07/25/understanding-cpu-steal-\ntime-when-should-you-be-worried.\nHickstein, J. (2007). Sysadmin slogans.\nhttp://www.jxh.com/slogans.html\nHohpe, G., & Woolf, B. (2003). Enterprise Integration Patterns: Designing, Building, and\nDeploying Messaging Solutions, Addison-Wesley Longman, Boston, MA.\nHumble, J., & Farley, D. (2010). Continuous Delivery: Reliable Software Releases through Build,\nTest, and Deployment Automation, Addison-Wesley Professional.\nJacob, A. (2010). Choose your own adventure: Adam Jacob on DevOps.\nhttp://www.youtube.com/watch?v=Fx8OBeNmaWw\nKamp, P.-H. (2010). You’re doing it wrong, Communications of the ACM 53(7): 55–59.\nhttp://queue.acm.org/detail.cfm?id=1814327\nKartar, J. (2010). What DevOps means to me.\nhttp://www.kartar.net/2010/02/what-devops-means-to-me\nKernighan, B., & Pike, R. (1984). The UNIX Programming Environment, Prentice Hall.\nKernighan, B., & Plauger, P. (1978). The Elements of Programming Style, McGraw-Hill.\nKim, G., Behr, K., & Spafford, G. (2013). The Phoenix Project: A Novel about IT, DevOps, and\nHelping Your Business Win, IT Revolution Press.\nhttp://books.google.com/books?id=mqXomAEACAAJ\nKlau, R. (2012). How Google sets goals: OKRs.\nhttp://gv.com/1322\nKrishnan, K. (2012). Weathering the unexpected, Queue 10(9): 30:30–30:37.\nhttp://queue.acm.org/detail.cfm?id=2371516\nLakshman, A., & Malik, P. (2010). Cassandra: A decentralized structured storage system,\nSIGOPS Operating Systems Review 44(2): 35–40.\nhttp://doi.acm.org/10.1145/1773912.1773922\n",
      "content_length": 2503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 526,
      "content": "Bibliography\n495\nLamport, L. (2001). Paxos made simple, SIGACT News 32(4): 51–58.\nhttp://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf\nLamport, L., & Marzullo, K. (1998). The part-time parliament, ACM Transactions on\nComputer Systems 16: 133–169.\nLee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance,\nHuman Factors 46(1): 50–80.\nLetuchy, E. (2008). Facebook Chat.\nhttps://www.facebook.com/note.php?note_id=14218138919\nLevinson, M. (2008). The Box: How the Shipping Container Made the World Smaller and the\nWorld Economy Bigger, Princeton University Press.\nLevy, S. (2012). Google throws open doors to its top-secret data center, Wired 20(11).\nhttp://www.wired.com/wiredenterprise/2012/10/ff-inside-google-data-\ncenter/all\nLimoncelli, T. A. (2005). Time Management for System Administrators, O’Reilly and\nAssociates.\nLimoncelli, T. (2012). Google DiRT: The view from someone being tested, Queue\n10(9): 35:35–35:37.\nhttp://queue.acm.org/detail.cfm?id=2371516#sidebar\nLimoncelli, T. A., Hogan, C., & Chalup, S. R. (2015). The Practice of System and Network\nAdministration, 3rd ed., Pearson Education.\nLink, D. (2013). Netﬂix and stolen time.\nhttp://blog.sciencelogic.com/netflix-steals-time-in-the-cloud-and-from-\nusers/03/2011\nMadrigal, A. C. (2012). When the nerds go marching in: How a dream team of engineers\nfrom Facebook, Twitter, and Google built the software that drove Barack Obama’s\nreelection.\nhttp://www.theatlantic.com/technology/archive/2012/11/when-the-nerds-\ngo-marching-in/265325\nMcKinley, D. (2012). Why MongoDB never worked out at Etsy.\nhttp://mcfunley.com/why-mongodb-never-worked-out-at-etsy\nMcKusick, M. K., Neville-Neil, G., & Watson, R. N. (2014). The Design and Implementation of\nthe FreeBSD Operating System, Prentice Hall.\nMegiddo, N., & Modha, D. S. (2003). ARC: A self-tuning, low overhead replacement cache,\nProceedings of the 2nd USENIX Conference on File and Storage Technologies, FAST ’03,\nUSENIX Association, Berkeley, CA, pp. 115–130.\nhttps://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-\nreplacement-cache\nMetz, C. (2013). Return of the Borg: How Twitter rebuilt Google’s secret weapon, Wired.\nhttp://www.wired.com/wiredenterprise/2013/03/google-borg-twitter-\nmesos/all\nNygard, M. T. (2007). Release It!: Design and Deploy Production-Ready Software, Pragmatic\nBookshelf.\n",
      "content_length": 2371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 527,
      "content": "496\nBibliography\nOppenheimer, D. L., Ganapathi, A., & Patterson, D. A. (2003). Why do Internet services\nfail, and what can be done about it?, USENIX Symposium on Internet Technologies and\nSystems.\nParasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of\nhuman interaction with automation, Transactions on Systems, Man, and Cybernetics\nPart A 30(3): 286–297.\nhttp://dx.doi.org/10.1109/3468.844354\nPatterson, D., Brown, A., Broadwell, P., Candea, G., Chen, M., Cutler, J., Enriquez, P., Fox,\nA., Kiciman, E., Merzbacher, M., Oppenheimer, D., Sastry, N., Tetzlaff, W.,\nTraupman, J., & Treuhaft, N. (2002). Recovery Oriented Computing (ROC):\nMotivation, deﬁnition, techniques, and case studies, Technical Report\nUCB/CSD-02-1175, EECS Department, University of California, Berkeley, CA.\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2002/5574.html\nPfeiffer, T. (2012). Why waterfall was a big misunderstanding from the beginning: Reading\nthe original paper.\nhttp://pragtob.wordpress.com/2012/03/02/why-waterfall-was-a-big-\nmisunderstanding-from-the-beginning-reading-the-original-paper\nPinheiro, E., Weber, W.-D., & Barroso, L. A. (2007). Failure trends in a large disk drive\npopulation, 5th USENIX Conference on File and Storage Technologies (FAST 2007),\npp. 17–29.\nRachitsky, L. (2010). A guideline for postmortem communication.\nhttp://www.transparentuptime.com/2010/03/guideline-for-postmortem-\ncommunication.html\nRichard, D. (2013). Gamedays on the Obama campaign.\nhttp://velocityconf.com/velocity2013/public/schedule/detail/28444;\nhttp://www.youtube.com/watch?v=LCZT_Q3z520\nRobbins, J., Krishnan, K., Allspaw, J., & Limoncelli, T. (2012). Resilience engineering:\nLearning to embrace failure, Queue 10(9): 20:20–20:28.\nhttp://queue.acm.org/detail.cfm?id=2371297\nRockwood, B. (2013). Why SysAdmin’s can’t code.\nhttp://cuddletech.com/?p=817\nRossi, C. (2011). Facebook: Pushing millions of lines of code ﬁve days a week.\nhttps://www.facebook.com/video/video.php?v=10100259101684977\nRoyce, D. W. W. (1970). Managing the development of large software systems: Concepts\nand techniques.\nSchlossnagle, T. (2011). Career development.\nhttp://www.youtube.com/watch?v=y0mHo7SMCQk\nSchroeder, B., Pinheiro, E., & Weber, W.-D. (2009). DRAM errors in the wild: A large-scale\nﬁeld study, SIGMETRICS.\nSchwarzkopf, M., Konwinski, A., Abd-El-Malek, M., & Wilkes, J. (2013). Omega: Flexible,\nscalable schedulers for large compute clusters, SIGOPS European Conference on\nComputer Systems (EuroSys), Prague, Czech Republic, pp. 351–364.\nhttp://research.google.com/pubs/pub41684.html\n",
      "content_length": 2590,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 528,
      "content": "Bibliography\n497\nSeven, D. (2014). Knightmare: A DevOps cautionary tale.\nhttp://dougseven.com/2014/04/17/knightmare-a-devops-cautionary-tale\nSiegler, M. (2011). The next 6 months worth of features are in Facebook’s code right now\n(but we can’t see).\nhttp://techcrunch.com/2011/05/30/facebook-source-code\nSpear, S., & Bowen, H. K. (1999). Decoding the DNA of the Toyota production system,\nHarvard Business Review.\nSpolsky, J. (2004). Things you should never do, Part I, Joel on Software, Apress.\nhttp://www.joelonsoftware.com/articles/fog0000000069.html\nStevens, W. (1998). UNIX Network Programming: Interprocess Communications, UNIX\nNetworking Reference Series, Vol. 2, Prentice Hall.\nStevens, W. R., & Fall, K. (2011). TCP/IP Illustrated, Volume 1: The Protocols, Pearson\nEducation.\nhttp://books.google.com/books?id=a23OAn5i8R0C\nStevens, W., & Rago, S. (2013). Advanced Programming in the UNIX Environment, Pearson\nEducation.\nStevens, W., & Wright, G. (1996). TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP,\nNNTP, and the UNIX Domain Protocols, Addison-Wesley.\nTseitlin, A. (2013). The antifragile organization, Queue 11(6): 20:20–20:26.\nhttp://queue.acm.org/detail.cfm?id=2499552\nTufte, E. R. (1986). The Visual Display of Quantitative Information, Graphics Press,\nCheshire, CT.\nWalls, M. (2013). Building a DevOps Culture, O’Reilly Media.\nWillis, J., & Edwards, D. (2011). Interview with Jesse Robbins.\nhttp://www.opscode.com/blog/2011/09/22/jesse-robbins-interview-on-\ndevops-cafe-19-w-full-transcript\nWillis, J., Edwards, D., & Humble, J. (2012). DevOps Cafe Episode 33: Finally it’s Jez.\nhttp://devopscafe.org/show/2012/9/26/devops-cafe-episode-33.html\nWright, G., & Stevens, W. (1995). TCP/IP Illustrated, Volume 2: The Implementations, Pearson\nEducation.\nYan, B., & Kejariwal, A. (2013). A systematic approach to capacity planning in the real\nworld.\nhttp://www.slideshare.net/arunkejariwal/a-systematic-approach-to-\ncapacity-planning-in-the-real-world\n",
      "content_length": 1970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 529,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 530,
      "content": "Index\nA-B testing, 232–233\nAAA (authentication, authorization, and\naccounting), 222\nAbbott, M., 99–100\nAbstracted administration in\nDevOps, 185\nAbstraction in loosely coupled\nsystems, 24\nAbts, D., 137\nAccess Control List (ACL) mechanisms\ndescription, 40\nGoogle, 41\nAccess controls in design for operations,\n40–41\nAccount creation automation example,\n251–252\nAccuracy, automation for, 253\nACID databases, 24\nAcknowledgments for alert messages,\n355–356\nACL (Access Control List) mechanisms\ndescription, 40\nGoogle, 41\nActive-active pairs, 126\nActive users, 366\nAdaptive Replacement Cache (ARC)\nalgorithm, 107\nAdopting design documents, 282–283\nAdvertising systems in second web\nera, 465\nAfter-hours oncall maintenance\ncoordination, 294\nAgents in collections, 352\nAggregators, 352\nAgile Alliance, 189\nAgile Manifesto, 189\nAgile techniques, 180\ncontinuous delivery, 188–189\nfeature requests, 264\nAKF Scaling Cube, 99\ncombinations, 104\nfunctional and service splits,\n101–102\nhorizontal duplication, 99–101\nlookup-oriented splits, 102–104\nAlerts, 163, 285\nalerting and escalation systems, 345,\n354–357\nmonitoring, 333\noncall for. See Oncall\nrules, 353\nthresholds, 49\nAlexander, Christopher, 69\nAllen, Woody, 285\nAllspaw, John\nautomation, 249\ndisaster preparedness tests, 318–320\noutage factors, 302\npostmortems, 301\nAlternatives in design documents, 278\nAmazon\ndesign process, 276\nGame Day, 318\nSimple Queue Service, 85\nAmazon Elastic Compute Cloud\n(Amazon EC2), 472\nAmazon Web Services (AWS), 59\nAnalysis\nin capacity planning, 375–376\ncausal, 301–302\n499\n",
      "content_length": 1549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 531,
      "content": "500\nIndex\nAnalysis (continued)\ncrash reports, 129\nin monitoring, 345, 353–354\nAncillary resources in capacity\nplanning, 372\nAndreessen, Marc, 181\nAnomaly detection, 354\n“Antifragile Organization” article,\n315, 320\nAntifragile systems, 308–310\nApache systems\nHadoop, 132, 467\nMesos, 34\nweb server forking, 114\nZookeeper, 231, 363\nAPI (Application Programming\nInterface)\ndeﬁned, 10\nlogs, 340\nApplicability in dot-bomb era,\n463–464\nApplication architectures, 69\ncloud-scale service, 80–85\nexercises, 93\nfour-tier web service, 77–80\nmessage bus, 85–90\nreverse proxy service, 80\nservice-oriented, 90–92\nsingle-machine web servers, 70–71\nsummary, 92–93\nthree-tier web service, 71–77\nApplication debug logs, 340\nApplication logs, 340\nApplication Programming Interface\n(API)\ndeﬁned, 10\nlogs, 340\nApplication servers in four-tier web\nservice, 79\nApprovals\ncode, 47–48\ndeployment phase, 214, 216–217\ndesign documents, 277, 281\nservice launches, 159\nArbitrary groups, segmentation\nby, 104\nARC (Adaptive Replacement Cache)\nalgorithm, 107\nArchitecture factors in service launches,\n157\nArchives\ndesign documents, 279–280\nemail, 277\nArt of Scalability, Scalable Web\nArchitecture, Processes, and\nOrganizations for the Modern\nEnterprise, 100\nArtifacts\nartifact-scripted database changes,\n185\ndeﬁned, 196\nAssessments, 421–422\nCapacity Planning, 431–432\nChange Management, 433–434\nDisaster Preparedness, 448–450\nEmergency Response, 426–428\nlevels, 405–407\nmethodology, 403–407\nMonitoring and Metrics, 428–430\nNew Product Introduction and\nRemoval, 435–436\noperational excellence, 405–407\norganizational, 411–412\nPerformance and Efﬁciency, 439–441\nquestions, 407\nRegular Tasks, 423–425\nService Delivery: The Build Phase,\n442–443\nService Delivery: The Deployment\nPhase, 444–445\nService Deployment and\nDecommissioning, 437–438\nservices, 407–410\nToil Reduction, 446–447\nAsynchronous design, 29\nAtlassian Bamboo tool, 205\nAtomicity\nACID term, 24\nrelease, 240–241\nAttack surface area, 79\nAuditing operations design, 42–43\nAugmentation ﬁles, 41–42\nAuthentication, authorization, and\naccounting (AAA), 222\nAuthentication in deployment phase,\n222\nAuthors in design documents, 277, 282\nAuto manufacturing automation\nexample, 251\n",
      "content_length": 2199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 532,
      "content": "Index\n501\nAutomation, 243–244\napproaches, 244–245\nbaking, 219\nbeneﬁts, 154\ncode amount, 269–270\ncode reviews, 268–269\ncompensatory principle, 246–247\ncomplementarity principle, 247–248\ncontinuous delivery, 190\ncrash data collection and analysis,\n129\ncreating, 255–258\nDevOps, 182, 185–186\nexercises, 272–273\ngoals, 252–254\nhidden costs, 250\ninfrastructure strategies, 217–220\nissue tracking systems, 263–265\nlanguage tools, 258–262\nleft-over principle, 245–246\nlessons learned, 249–250\nmultitenant systems, 270–271\nprioritizing, 257–258\nrepair life cycle, 254–255\nsoftware engineering tools and\ntechniques, 262–270\nsoftware packaging, 266\nsoftware restarts and escalation,\n128–129\nsteps, 258\nstyle guides, 266–267, 270\nsummary, 271–272\nsystem administration, 248–249\ntasks, 153–155\ntest-driven development, 267–268\ntoil reduction, 257\nvs. tool building, 250–252\nversion control systems, 265–266\nAvailability\nCAP Principle, 21–22\nmonitoring, 336\nAvailability and partition tolerance\n(AP), 24\nAvailability requirements\ncloud computing era, 469\ndot-bomb era, 460\nﬁrst web era, 455\npre-web era, 452–453\nsecond web era, 465\nAverages in monitoring, 358\nAWS (Amazon Web Services), 59\nBackend replicas, load balancers with,\n12–13\nBackends\nmultiple, 14–15\nserver stability, 336\nBackground in design documents,\n277–278\nBackground processes for containers, 61\nBackups in design for operations, 36\nBaked images for OS installation,\n219–220\nBanned query lists, 130\nBare metal clouds, 68\nBarroso, L. A.\ncanary requests, 131\ncost comparisons, 464\ndisk drive failures, 133, 338\nBASE (Basically Available Soft-state\nservices) databases, 24\nBaseboard Management Controller\n(BMC), 218\nBasecamp application, 55\nbash (Bourne Again Shell), 259\nBatch size in DevOps, 178–179\nBathtub failure curve, 133\nBeck, K., 189\nBehaviors in KPIs, 390–391\nBehr, K., 172\nBellovin, S. M., 79\nBenchmarks in service platform\nselection, 53\nBernstein, Leonard, 487\nBerra, Yogi, 331\nBibliography, 491–497\nBidirectional learning in code review\nsystems, 269\nBig O notation, 476–479\nBigtable storage system, 24\nBimodal patterns in histograms, 361\nBIOS settings in deployment phase,\n218\nBlackbox monitoring, 346–347\nBlacklists, 40–42\nBlade servers, 217–218\n“Blameless Postmortems and a Just\nCulture” article, 301\n",
      "content_length": 2266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 533,
      "content": "502\nIndex\nBlog Search, upgrading, 226\nBlue-green deployment, 230\nBMC (Baseboard Management\nController), 218\nBotnets, 140\nBots in virtual ofﬁces, 166–167\nBottlenecks\nautomation for, 257\nDevOps, 179\nidentifying, 96\nBourne Again Shell (bash), 259\nBowen, H. K., 172\nBoyd, John, 296\nBSD UNIX, 460\nBuckets in histograms, 361\nBuffer thrashing, 71\nBugs\ncode review systems, 269\nvs. feature requests, 263\nﬂag ﬂips for, 232\nlead time, 201\nmonitoring, 336\nnew releases, 178\npriority for, 150\nunused code, 270\nBuilds\nassessments, 442–443\nbuild console, 205\nbuild step, 203–204\ncommit step, 202–203\ncontinuous deployment, 237\ncontinuous integration, 205–207\ndevelop step, 202\nDevOps, 185–186\nexercises, 209\noverview, 195–196\npackage step, 204\npackages as handoff interface, 207–208\nregister step, 204\nservice delivery strategies, 197–200\nsteps overview, 202–204\nsummary, 208–209\nversion-controlled, 191\nvirtuous cycle of quality, 200–201\nwaterfall methodology, 199\n“Built to Win: Deep Inside Obama’s\nCampaign Tech” article, 307\nBusiness impact in alert messages, 355\nBusiness listings in Google Maps, 42\nc-SOX requirements, 43\nCache hit ratio, 105, 109\nCache hits, 104\nCache misses, 104\nCaches, 104–105\neffectiveness, 105\npersistence, 106\nplacement, 106\nreplacement algorithms, 107\nsize, 108–110\nCalendar documents for oncall\nschedules, 290–291\nCanary process for upgrading services,\n227–228\nCanary requests, 131\nCandea, G., 35\nCAP Principle, 21–24\nCapability Maturity Model (CMM),\n405–407\nCapability monitoring, 348\nCapacity factors in service launches, 158\nCapacity models, 374\nCapacity planning (CP), 365\nadvanced, 371–381\nassessments, 431–432\ncapacity limits, 366, 372–373\ncore drivers, 373–374\ncurrent usage, 368–369\ndata analysis, 375–380\ndelegating, 381\nengagement measuring, 374–375\nexercises, 386\nheadroom, 370\nkey indicators, 380–381\nlaunching new services, 382–384\nmonitoring, 335\nnormal growth, 369\noperational responsibility, 404\nplanned growth, 369–370\nprimary resources, 372\nprovisioning time, 384–385\nresiliency, 370–371\nresource regression, 381–382\nstandard, 366–371\nsummary, 385–386\ntimetables, 371\nCart size monitoring, 336\nCascade load balancing, 74\nCassandra system, 24\n",
      "content_length": 2178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 534,
      "content": "Index\n503\nCausal analysis, 301–302\nCCA (contributing conditions analysis),\n301\nCDNs (content delivery networks),\n114–116\nCentral collectors, 352–353\nCertiﬁcate management, 79\nCFEngine system\nconﬁguration management, 261\ndeployment phase, 213\nChalup, S. R., 204\nChange\ndocumenting, 276\nlimits, 41\nvs. stability, 149–151\nsuccess rate, 201\nversion control systems, 265\nChange-instability cycles, 150\nChange Management (CM)\nassessments, 433–434\noperational responsibility, 404\nChannels in message bus architectures,\n86\nChaos Gorilla, 315\nChaos Monkey, 315\nChapman, Brent, 323\nChat room bots for alerts, 293\nChat rooms for virtual ofﬁces, 166–167\nChecklists\noncall pre-shift responsibilities, 294\nservice launches, 157, 159\nChef software framework, 213\nCheswick, W. R., 79\n“Choose Your Own Adventure” talk, 173\nChubby system, 231, 314\nChurchill, Winston, 119\nClassiﬁcation systems for oncall, 292\nClos networking, 137\nCloud computing era (2010-present),\n469–472\nCloud-scale service, 80–81\nglobal load balancing methods, 82,\n83–85\ninternal backbones, 83–84\npoints of presence, 83–85\nCM (conﬁguration management)\nlanguages, 260–262\nCMDB (Conﬁguration Management\nDatabase), 222\nCMM (Capability Maturity Model),\n405–407\nCNN.com web site, 13–14\nCode\napproval process, 47–48\nautomated reviews, 268–269\nlead time, 201\nlive changes, 236\nsufﬁcient amount, 269–270\nCode latency in DevOps, 178–179\nCode pushes\ndescription, 225, 226\nfailed, 239–240\nCode review system (CRS), 268–269\nCognitive systems engineering (CSE)\napproach, 248\nCold caches, 106\nCold storage factor in service platform\nselection, 54\nCollaboration in DevOps, 183\nCollection systems, 345\ncentral vs. regional collectors,\n352–353\nmonitoring, 349–353\nprotocol selection, 351\npush and pull, 350–351\nserver component vs. agents vs.\npollers, 352\nColocation\nCDNs, 114\nservice platform selection, 65–66\nCommand-line ﬂags, 231\nComments in style guides, 267\nCommit step in build phase, 202–203\nCommodity servers, 463\nCommunication\nemergency plans, 317–318\npostmortems, 302\nvirtual ofﬁces, 166–167\nCompensation in oncall schedules, 290\nCompensatory automation principle,\n244, 246–247\nCompiled languages, 260\nComplementarity principle, 244, 247–248\nCompliance in platform selection, 63\nComprehensiveness in continuous\ndeployment, 237\nComputation, monitoring, 353–354\nConﬁdence in service delivery, 200\n",
      "content_length": 2343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 535,
      "content": "504\nIndex\nConﬁguration\nautomating, 254\ndeployment phase, 213–214\nin designing for operations, 33–34\nDevOps, 185\nfour-tier web service, 80\nmonitoring, 345–346, 362–363\nConﬁguration management (CM)\nlanguages, 260–262\nConﬁguration Management Database\n(CMDB), 222\nConﬁguration management strategy in\nOS installation, 219\nConﬁguration packages, 220\nConﬂicting goals, 396–397\nCongestion problems, 15\nConsistency\nACID term, 24\nCAP Principle, 21\nConsistency and partition tolerance\n(CP), 24\nConstant scaling, 475–476\nContainers, 60–62\nContent delivery networks (CDNs),\n114–116\nContent distribution servers, 83\nContinuous builds in DevOps, 186\nContinuous Delivery, 223\nContinuous delivery (CD)\ndeployment phase, 221\nDevOps, 189–192\npractices, 191\nprinciples, 190–191\nContinuous deployment\nDevOps, 186\nupgrading live services, 236–239\nContinuous improvement technique\nDevOps, 153, 183\nservice delivery, 201\nContinuous integration (CI) in build\nphase, 205–207\nContinuous tests, 186\nContract questions for hosting\nproviders, 64–65\nContributing conditions analysis (CCA),\n301\nControl in platform selection, 64\nConvergent orchestration, 213–214\nCookies, 76–78\nCoordination for oncall schedules, 290\nCore drivers\ncapacity planning, 373–374\ndeﬁned, 366\nCoredumps, 129\nCorporate emergency communications\nplans, 317–318\nCorpus, 16–17\nCorrelation coefﬁcient, 367\nCorrelation in capacity planning,\n375–378\nCosts\ncaches, 105\ncloud computing era, 469–470\ndot-bomb era, 464–465\nﬁrst web era, 459\nplatform selection, 63–64\npre-web era, 454\nsecond web era, 468–469\nservice platform selection, 66–67\nTCO, 172\nCounters in monitoring, 348–350, 358\nCPU core sharing, 59\nCrash-only software, 35\nCrashes\nautomated data collection and\nanalysis, 129\nsoftware, 128–129\nCraver, Nick, 430\nCRS (code review system), 268–269\nCSE (cognitive systems engineering)\napproach, 248\nCurrent usage in capacity planning,\n368–369\nCustomer functionality, segmentation\nby, 103\nCustomers in DevOps, 177\nCycle time, 196\nDaemons for containers, 61\nDaily oncall schedules, 289\nDark launches, 233, 383–384\nDashboards for alerts, 293\nData analysis in capacity planning,\n375–380\nData import controls, 41–42\nData scaling in dot-bomb era, 463\nData sharding, 110–112\nDatabase-driven dynamic content, 70\n",
      "content_length": 2242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 536,
      "content": "Index\n505\nDatabase views in live schema\nchanges, 234\nDatacenter failures, 137–138\nDates in design documents, 277, 282\nDawkins, Richard, 475\nDDoS (distributed denial-of-service)\nattacks, 140\nDeallocation of resources, 160\nDean, Jeff\ncanary requests, 131\nscaling information, 27\nDebois, Patrick, 180\nDebug instrumentation, 43\nDecommissioning services, 404\nassessments, 437–438\ndescription, 156\noverview, 160\nDedicated wide area network\nconnections, 83\nDefault policies, 40\nDefense in depth, 119\nDeﬁned level in CMM, 406–407\nDegradation, graceful, 39–40, 119\nDelays in continuous deployment,\n238\nDelegating capacity planning, 381\nDelegations of authority in Incident\nCommand System, 324\nDeming, W. Edwards, 172\nDenial-of-service (DoS) attacks, 140\nDependencies\ncontainers, 60–61\nservice launches, 158\nDeployment and deployment phase,\n195, 197, 211\napprovals, 216–217\nassessments, 444–445\nconﬁguration step, 213–214\ncontinuous delivery, 221\ndeﬁned, 196\nDevOps, 185\nexercises, 223\nfrequency in service delivery, 201\ninfrastructure as code, 221–222\ninfrastructure automation strategies,\n217–220\ninstallation step, 212–213\ninstalling OS and services, 219–220\nKPIs, 392–393\noperations console, 217\nphysical machines, 217–218\nplatform services, 222\npromotion step, 212\nsummary, 222–223\ntesting, 215–216\nvirtual machines, 218\nDescriptions of outages, 301\nDescriptive failure domains, 127\nDesign documents, 275\nadopting, 282–283\nanatomy, 277–278\narchive, 279–280\nchanges and rationale, 276\nexercises, 284\noverview, 275–276\npast decisions, 276–277\nreview workﬂows, 280–282\nsummary, 283\ntemplates, 279, 282, 481–484\nDesign for operations, 31\naccess controls and rate limits,\n40–41\nauditing, 42–43\nbackups and restores, 36\nconﬁguration, 33–34\ndata import controls, 41–42\ndebug instrumentation, 43\ndocumentation, 43–44\nexception collection, 43–44\nexercises, 50\nfeatures, 45–48\ngraceful degradation, 39–40\nhot swaps, 38–39\nimplementing, 45–48\nimproving models, 48–49\nmonitoring, 42\noperational requirements, 31–32\nqueue draining, 35–36\nredundancy, 37\nreplicated databases, 37–38\nsoftware upgrades, 36\nstartup and shutdown, 34–35\nsummary, 49–50\nthird-party vendors, 48\ntoggles for features, 39\nDesign patterns for resiliency. See\nResiliency\nDesign patterns for scaling. See Scaling\n",
      "content_length": 2265,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 537,
      "content": "506\nIndex\nDetails\ndesign documents, 278\npostmortems, 302\nDevelop step in build phase, 202\nDevelopers for oncall, 287\nDevOps, 171–172\nAgile, 188–189\napproach, 175–176\nautomation, 182, 185–186\nbatch size, 178–179\nbuild phase, 197–198\nbusiness level, 187–188\ncontinuous delivery, 189–192\ncontinuous improvement, 183\nconverting to, 186–188\ndescription, 172–173\nexercises, 193\nexperimentation and learning,\n178\nfeedback, 177–178\nhistory, 180–181\nintegration, 182\nnontechnical practices, 183–184\nrecommended reading, 487\nrelationships, 182\nrelease engineering practices, 186\nSRE, 181\nstarting, 187\nstrategy adoption, 179–180\nsummary, 192\nvs. traditional approach, 173–175\nvalues and principles, 181–186\nworkﬂow, 176–177\nDevOps Cafe Podcast, 180, 200\nDevOps culture, 171\n“DevOps Days” conferences, 180\nDiagnostics, monitoring, 337\nDickson, C., 345\nDickson model, 334\ndiff tool, 33\nDifferentiated services, 233\nDirect measurements, 347–348\nDirect orchestration, 213–214\nDiRT (Disaster Recovery Testing), 316,\n318, 320–323\nDisaster preparedness, 307, 448–450\nantifragile systems, 308–309\nDiRT tests, 320–323\nexercises, 330\nﬁre drills, 312–313\nimplementation and logistics, 318–320\nincident Command System, 323–329\nmindset, 308–310\nrandom testing, 314–315\nrisk reduction, 309–311\nscope, 317–318\nservice launches, 158\nservice testing, 313–314\nstarting, 316–317\nsummary, 329–330\ntraining for individuals, 311–312\ntraining for organizations, 315–317\nDisaster Recovery Testing (DiRT), 316,\n318, 320–323\nDisks\naccess time, 26\ncaches, 106–107\nfailures, 132–133\nDistributed computing and clouds\ncloud computing era, 469–472\nconclusion, 472–473\ndot-bomb era, 459–465\nexercises, 473\nﬁrst web era, 455–459\norigins overview, 451–452\npre-web era, 452–455\nsecond web era, 465–469\nDistributed computing overview, 9–10\nCAP Principle, 21–24\ndistributed state, 17–20\nexercises, 30\nload balancer with multiple backend\nreplicas, 12–13\nloosely coupled systems, 24–25\nserver trees, 16–17\nservers with multiple backends, 14–15\nsimplicity importance, 11\nspeed issues, 26–29\nsummary, 29–30\nvisibility at scale, 10–11\nDistributed denial-of-service (DDoS)\nattacks, 140\nDistributed state, 17–20\nDistributed version control systems\n(DVCSs), 265\nDiurnal cycles, 332\nDiurnal usage patterns, 359\nDiversity, monitoring, 334\n",
      "content_length": 2282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 538,
      "content": "Index\n507\nDNS\ndeployment phase, 222\nround robin, 72–73\nDocker system, 61, 219\nDocumentation\ndesign documents. See Design\ndocuments\ndesign for operations, 43–44\nservice launches, 158\nstakeholder interactions, 154\nDoerr, John, 389\nDomains, failure, 126–128\nDomain-speciﬁc languages (DSLs), 244\nDoS (denial-of-service) attacks, 140\nDot-bomb era (2000–2003), 459–465\nDownsampling, monitoring, 339\nDowntime\ncontainers, 61\npre-web era, 453\nin upgrading live services, 225\nDrain tool, 254\nDraining process, 112\nDrains, queue, 35–36\n“DRAM Errors in the Wild: A\nLarge-Scale Field Study” article, 134\nDSLs (domain-speciﬁc languages), 244\nDual load balancers, 76\nDurability in ACID term, 24\nDVCSs (distributed version control\nsystems), 265\nDynamic content with web servers, 70\nDynamic resource allocation, 138\nDynamic roll backs, 232\nDynamo system, 24\n“Each Necessary, But Only Jointly\nSufﬁcient” article, 302\nECC (error-correcting code) memory,\n131–132\nEdge cases, 153\nEdwards, Damon\nDevOps beneﬁts, 172-173\nDevOps Cafe podcast, 180, 188, 200\nEffectiveness of caches, 105\n80/20 rule for operational features, 47\nElements of Programming Style, 11\nEliminating tasks, 155\nEMA (exponential moving average),\n367, 379\nEmail\nalerts, 292–293\narchives, 277\nEmbedded knowledge in DevOps,\n177–178, 187\nEmergency hotﬁxes, 240\nEmergency issues, 160\nEmergency Response (ER), 403, 426–428\nEmergency tasks, 156\nEmployee human resources data\nupdates example, 89–90\nEmpowering users, automation for, 253\nEmptying queues, 35\nEncryption in four-tier web service, 79\nEnd-of-shift oncall responsibilities, 299\nEnd-to-end process in service delivery,\n200\nEngagement\ndeﬁned, 366\nmeasuring, 374–375\nEnterprise Integration Practices: Designing,\nBuilding, and Deploying Messaging\nSolutions, 87\nEnvironment-related ﬁles, 220\nEphemeral computing, 67\nEphemeral machines, 58\nErlang language, 236\nError Budgets, 152\ncase study, 396–399\nDevOps, 184\nError-correcting code (ECC) memory,\n131–132\nEscalation\nalert messages, 345, 354–357\nautomated, 128–129\nmonitoring, 333\nthird-party, 298\nEtsy blog, 256\nEU Data Protection Directive\nplatform selection factor, 63\nrequirements, 43\nEventual consistency, 21\nException collection, 43–44\nExceptional situations. See Oncall\nExecution in service delivery, 201\nExecutive summaries in design\ndocuments, 277, 282\nExpand/contract technique, 234–235\nExperimentation in DevOps, 178\n",
      "content_length": 2372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 539,
      "content": "508\nIndex\nExpertise of cloud providers factor in\nservice platform selection, 66\nExplicit oncall handoffs, 299\nExponential moving average (EMA),\n367, 379\nExponential scaling, 476\nFace-to-face discussions in DevOps, 187\nFacebook\nchat dark launch, 384\nrecommended reading, 488\nFactorial scaling, 477\nFail closed actions, 40\nFail open actions, 40\nFailed code pushes, 239–240\nFailed RAM chips, 123\nFailure condition in alert messages, 354\n“Failure Trends in a Large Disk Drive\nPopulation,” 133, 338\nFailures, 10, 120. See also Resiliency\noverload, 138–141\nphysical. See Physical failures\nFair queueing, 113\nFan in, 15\nFan out, 15\nFarley, D., 190, 223\n“Fault Injection in Production,” 320\nFeature requests vs. bugs, 263\nFeatures in design for operations, 46\nbuilding, 45\ntoggles, 39, 230–232\nwriting, 47–48\nFederal Emergency Management\nAdministration (FEMA) web site,\n324\nFeedback\ndesign for operations, 47–48\nDevOps, 177–178, 186–187\nFeeding from queues, 113\nFelderman, B., 137\nFEMA (Federal Emergency Management\nAdministration) web site, 324\nFiles, environment-related, 220\nFinance monitoring, 336\nFire drills, 312–313\nFirst web era: bubble (1995-2000),\n455–459\nFisher, M., 99–100\nFitts, P., 246\nFitts’s list, 246\nFix-it days, 166\nFIXME comments in style guides, 267\nFlag ﬂipping, 39, 230–232\nFlexibility in service-oriented\narchitectures, 91\nFlows\nbuild phase, 197\ndeﬁned, 196\nFocus in operational teams, 165–166\nFollow-up oncall work, 296\nForecasting in capacity planning,\n378–380\nFormal workﬂows, 280\nFour-tier web service, 77–78\napplication servers, 79\nconﬁguration options, 80\nencryption and certiﬁcate\nmanagement, 79\nfrontends, 78–79\nsecurity, 79\nFox, A., 35\nFreeBSD containers, 60\nFrequency\ndeployment, 201\nmeasurement, 333\noncall, 291–292\nFrontends in four-tier web service, 78–79\nFrying images, 219–220\nFulghum, P., 188, 215\nFunctional splits in AKF Scaling Cube,\n101–102\nFuture capacity planning, 49\nGallagher, S., 307\nGame Day exercises\nAmazon, 318\nDevOps, 184\ndisaster preparedness, 315, 318–320\n“Gamedays on the Obama Campaign”\narticle, 320\nGanapathi, A., 141\nGaneti system, 240, 254\nGatekeeper tool, 233\nGates, 196\nGauges in monitoring, 348–350\nGeographic diversity factor in service\nplatform selection, 54\nGeolocation in cloud-scale service, 81\n",
      "content_length": 2256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 540,
      "content": "Index\n509\nGFS (Google File System), 466\nGibson, William, 402\nGilbert, S., 23\nGlobal load balancing (GLB), 82–83,\n83–85\nGo Continuous Delivery tool, 205\nGoals\nautomation, 252–254\nconﬂicting, 396–397\ndesign documents, 277, 282\nuniﬁed, 397–398\nGoogle\nACLs, 41\nAdSense, 465\nAppEngine, 54\nBlog Search upgrading, 226\nbots, 167\nChubby system, 231, 314\nDisaster Recovery Testing, 316, 318\ngraceful degradation of apps, 40\nmessage bus architectures, 86\noncall, 291\noutages, 119\npostmortem reports, 301\nrecommended reading, 488\nself-service launches at, 159\nSRE model, 181\ntimestamps, 341–342\n“Google DiRT: The View from Someone\nBeing Tested” article, 320–323\nGoogle Error Budget KPI, 396–399\nGoogle File System (GFS), 466\nGoogle Maps\nload balancing, 83–84\nlocal business listings, 42\nGoogle Omega system, 34\n“Google Throws Open Doors to Its\nTop-Secret Data Center” article, 320\nGraceful degradation, 39–40, 119\nGraphical user interfaces (GUIs) for\nconﬁguration, 34\nGraphs, monitoring, 358\nGreatness, measuring, 402–403\nGruver, G., 188, 215\n“Guided Tour through Data-center\nNetworking” article, 137\n“Guideline for Postmortem\nCommunication” blog post, 302\nGUIs (graphical user interfaces) for\nconﬁguration, 34\nHadoop system, 132, 467\nHandoff interface, packages as, 207–208\nHandoffs, oncall, 299\nHangs, software, 129–130\nHardware components\ndot-bomb era, 460\nload balancers, 136\nresiliency, 120–121\nHardware output factor in service\nplatform selection, 67–68\nHardware qualiﬁcation, 156\nHardware virtual machines (HVMs), 58\nHash functions, 110\nHash preﬁx, segmentation by, 103\nHbase storage system, 23–24\nHead of line blocking, 112\nHeadroom in capacity planning, 370\nHealth checks\ndeployment phase, 213\nqueries, 12\nHeartbeat requests, 129\nHelp\noncall. See Oncall\nscaling, 252\nHidden costs of automation, 250\nHierarchy, segmentation by, 104\nHigh availability\ncloud computing era, 471\ndot-bomb era, 461–462\nﬁrst web era, 457–458\npre-web era, 454\nsecond web era, 466–467\nHigh-level design, 278\nHigh-speed network counters, 349\nHistograms, 361–362\n“Hit” logs, 340\nHits, cache, 104\nHoare, C. A. R., 9\nHogan, C., 204\nHohpe, G., 87\nHome computers in dot-bomb era, 460\nHorizontal duplication in AKF Scaling\nCube, 99–101\nHosting providers, contract questions\nfor, 64–65\nHot-pluggable devices, 38–39\nHot spares vs. load sharing, 126\nHot-swappable devices, 38–39\nHotﬁxes, 240\n",
      "content_length": 2353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 541,
      "content": "510\nIndex\n“How Google Sets Goals: OKRs” video,\n389\nHTTP (Hyper-Text Transfer Protocol)\nload balancing, 75\noverview, 69\nHudson tool, 205\nHuman error, 141–142\nHuman processes, automating, 154\nHuman resources data updates example,\n89–90\nHumble, J.\ncontinuous delivery, 190, 223\nDevOps Cafe Podcast, 188, 200\nHVMs (hardware virtual machines), 58\nHybrid load balancing strategy, 75\nHyper-Text Transfer Protocol (HTTP)\nload balancing, 75\noverview, 69\nIaaS (Infrastructure as a Service), 51–54\nIAPs (Incident Action Plans), 326–327\nIdeals for KPIs, 390\nImage method of OS installation,\n219–220\nImpact focus for feature requests, 46\nImplementation of disaster\npreparedness, 318–320\nImport controls, 41–42\nImprovement levels in operational\nexcellence, 412–413\nImproving models in design for\noperations, 48–49\nIn-house service provider factor in\nservice platform selection, 67\nIncident Action Plans (IAPs), 326–327\n“Incident Command for IT: What We\nCan Learn from the Fire\nDepartment” talk, 323\nIncident Command System, 323–324\nbest practices, 327–328\nexample use, 328–329\nIncident Action Plan, 326–327\nIT operations arena, 326\npublic safety arena, 325\nIncident Commanders, 324–325, 328\nIndex lookup speed, 28\nIndividual training for disaster\npreparedness, 311–312\nInformal review workﬂows, 280\nInfrastructure\nautomation strategies, 217–220\nDevOps, 185\nservice platform selection, 67\nInfrastructure as a Service (IaaS), 51–54\nInfrastructure as code, 221–222\nInhibiting alert messages, 356–357\nInitial level in CMM, 405\nInnovating, 148\nInput/output (I/O)\noverload, 13\nvirtual environments, 58–59\nInstallation\nin deployment phase, 212–213\nOS and services, 219–220\nIntegration in DevOps, 182\nIntel OKR system, 389\nIntentional delays in continuous\ndeployment, 238\nIntermodal shipping, 62\nInternal backbones in cloud-scale\nservice, 83–85\nInternet Protocol (IP) addresses\ndeployment phase, 222\nload balancers, 72–73\nrestrictions on, 40\nIntroducing new features, ﬂag ﬂips for,\n232\nIntrospection, 10\nInvalidation of cache entry, 108\nInvolvement in DevOps, 183\nIP (Internet Protocol) addresses\ndeployment phase, 222\nload balancers, 72–73\nrestrictions on, 40\nIsolation in ACID term, 24\nISPs for cloud-scale service, 83\nIssues\nnaming standards, 264\ntracking systems, 263–265\nIT operations arena in Incident\nCommand System, 326\nITIL recommended reading, 488\nj-SOX requirements, 43\nJacob, Adam, 173\nJails\ncontainers, 60\nprocesses, 55\n",
      "content_length": 2411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 542,
      "content": "Index\n511\nJava counters, 350\nJCS (joint cognitive system), 248\nJenkins CI tool, 205\nJob satisfaction in service delivery, 201\nJoint cognitive system (JCS), 248\nJSON transmitted over HTTP, 351\nKamp, P.-H., 478–479\nKartar, J., 183\nKeeven, T., 99\nKejariwal, A., 371\nKernighan, B., 11\nKey indicators in capacity planning,\n380–381\nKey performance indicators (KPIs),\n387–388\ncreating, 389–390\nError Budget case study, 396–399\nevaluating, 396\nexercises, 399–400\nmachine allocation example,\n393–396\nmonitoring example, 336–337\noverview, 388–389\nsummary, 399\nKeywords in alerts, 304\nKim, Gene, 171–172\nKlau, Rick, 389\nKotler, Philip, 365\nKPIs. See Key performance indicators\n(KPIs)\nKrishnan, Kripa, 319–320\nLabor laws, 43\nLame-duck mode, 35\n“LAMP” acronym, 461\nLanguage tools for automation, 258–262\nLatency\ncloud computing era, 471\ncloud-scale service, 81–82\ncode, 178–179\nmonitoring, 334, 336\nservice platform selection, 54\nSRE vs. traditional enterprise IT, 149\nLatency load balancing, 74\nLatency Monkey, 315\nLaunch leads, 159\nLaunch Readiness Engineers (LREs),\n157–158\nLaunch Readiness Reviews (LRRs), 159\nLaunches\ndark, 233\nservices, 156–160, 382–384\nLayer 3 and 4 load balancers, 73\nLayer 7 load balancers, 73\nLead times in service delivery, 201\nLeaf servers, 17\n“Lean Manufacturing,” 172\nLearning DevOps, 178\nLeast Frequently Used (LFU) algorithm,\n107\nLeast Loaded (LL) algorithm, 13\nload balancing, 74\nproblems, 13–14\nLeast Loaded with Slow Start load\nbalancing, 74\nLeast Recently Used (LRU) algorithm,\n107\nLee, J. D., 249\nLeft-over automation principle, 244–246\nLegacy system backups and restores, 36\nLessons learned in automation, 249–250\nLevel of service abstraction in service\nplatform selection, 52–56\nLevels of improvement in operational\nexcellence, 412–413\nLevy, Steven, 320\nLFU (Least Frequently Used) algorithm,\n107\nLimoncelli, T. A.\nDiRT tests, 320\nmeta-work, 162\npackages, 204\ntest event planning, 319\ntime management, 256\nLinear scaling, 476–477\nLink-shortening site example, 87–89\nLinking tickets to subsystems, 263–264\nLinux in dot-bomb era, 460–461\nLive code changes, 236\nLive restores, 36\nLive schema changes, 234–236\nLive service upgrades. See Upgrading\nlive services\nLoad balancers\nfailures, 134–136\nﬁrst web era, 456\nwith multiple backend replicas, 12–13\n",
      "content_length": 2273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 543,
      "content": "512\nIndex\nLoad balancers (continued)\nwith shared state, 75\nthree-tier web service, 72–74\nLoad sharing vs. hot spares, 126\nLoad shedding, 139\nLoad testing, 215\nLocal business listings in Google Maps,\n42\nLocal labor laws, 43\nLogarithmic scaling, 476\nLogistics in disaster preparedness,\n318–320\nLogistics team in Incident Command\nSystem, 326\nLoglinear scaling, 476–477\nLogs, 11, 340\napproach, 341\ndesign documents, 282\ntimestamps, 341–342\nLong-term analysis, 354\nLong-term ﬁxes\noncall, 299–300\nvs. quick, 295–296\nLongitudinal hardware failure study,\n133–134\nLook-for’s, 407\nLookup-oriented splits in AKF Scaling\nCube, 102–104\nLoosely coupled systems, 24–25\nLower-latency services in cloud\ncomputing era, 469\nLREs (Launch Readiness Engineers),\n157–158\nLRRs (Launch Readiness Reviews), 159\nLRU (Least Recently Used) algorithm,\n107\nLynch, N., 23\nMACD (moving average\nconvergence/divergence) metric,\n367, 378–379\nMACD signal line, 367\nMachine Learning service, 55\nMachines\nautomated conﬁguration example, 251\ndeﬁned, 10\nfailures, 134\nKPI example, 393–396\nMadrigal, A. C., 316\nMain threads, 112\nMaintenance alert messages, 356\nMajor bug resolution monitoring, 336\nMajor outages, 307\nMalfunctions, 121\ndeﬁned, 120\ndistributed computing approach, 123\nMTBF, 121–122\ntraditional approach, 122–123\nManaged level in CMM, 406–407\nManagement support in design\ndocuments, 282\nManual scaling, 153\nManual stop lists, 238\nMany-to-many communication in\nmessage bus architectures, 85–86\nMany-to-one communication in message\nbus architectures, 85–86\nMapReduce system, 466–467\nMaster-master pairs, 126\nMaster of Disaster (MoD) in Wheel of\nMisfortune game, 311–312\nMaster servers, 20\nMaster-slave split-brain relationship, 22\nMath terms, 367\n“Mature Role for Automation” blog\npost, 249\nMAUs (monthly active users), 366, 373\nMcHenry, Stephen, 234\nMcHenry Technique, 234–235\nMcKinley, D., 256\nMcLean, Malcom, 62\nMCollective service, 86\nMD5 algorithm, 110\nMean time between failures (MTBF),\n120–122, 125\nMean time to repair (MTTR), 125\nMean time to restore service in service\ndelivery, 201\nMeasurements, 332\nengagement, 374–375\nfrequency, 333\ngreatness, 402–403\nmonitoring. See Monitoring\nscaling, 97\nMedians, monitoring, 359\nMemory\ncaches, 104–106\nECC, 131–132\nfailures, 123, 131–132\n",
      "content_length": 2257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 544,
      "content": "Index\n513\nvirtual machines, 59\nweb servers, 71\nMesos system, 34\nMessage bus architectures, 85–86\ndesigns, 86\nemployee human resources data\nupdates example, 89–90\nlink-shortening site example, 87–89\nreliability, 87\nMessages, alert. See Alerts\nMeta-monitoring, 339–340\nMeta-processes, automating, 155\nMeta-work, 162\nMetrics, 11, 332\nMindset in disaster preparedness,\n308–310\nMinimum monitor problem, 337–338\nMisses, cache, 104\n“Model for Types and Levels of Human\nInteraction with Automation”\narticle, 248\nMonitoring, 331–332\nalerting and escalation management,\n354–357\nanalysis and computation,\n353–354\nassessments, 428–430\nblackbox vs. whitebox, 346–347\ncollections, 350–353\ncomponents, 345–346\nconﬁguration, 362–363\nconsumers, 334–336\ndesign for operations, 42\nexercises, 342–343, 364\nhistograms, 361–362\nkey indicators, 336–337, 380–381\nlogs, 340–342\nmeta-monitoring, 339–340\noverview, 332–333\npercentiles, 359\nprotocol selection, 351\npush vs. pull, 350–351\nretention, 338–339\nsensing and measurement, 345–350\nservice launches, 158\nstack ranking, 360\nstorage, 362\nsummary, 342, 363–364\nsystem integration, 250\nuses, 333\nvisualization, 358–362\nMonitoring and Metrics (MM), 404,\n428–430\nMonthly active users (MAUs), 366, 373\nMoving average\nconvergence/divergence (MACD)\nmetric, 367, 378–379\nMoving averages\ncapacity planning, 377\ndeﬁned, 367\nMTBF (mean time between failures),\n120–122, 125\nMTTR (mean time to repair), 125\nMultiple backend replicas, load\nbalancers with, 12–13\nMultiple backends, servers with, 14–15\nMultiple data stores in three-tier web\nservice, 77\nMultitenant systems, automation,\n270–271\nMultithreaded code, 112\nN + 1 conﬁgurations, 458\nN + 2 conﬁgurations, 458–459\nN + M redundancy, 124–125\n“NAK” (negatively acknowledge) alerts,\n355\nNaming standards, 264\nNative URLs, 115\nNatural disasters factor in service\nplatform selection, 53\nNearest load balancing, 81\nNearest by other metric load balancing,\n81\nNearest with limits load balancing, 81\nNegatively acknowledge (“NAK”)\nalerts, 355\nNetﬂix\ndisaster preparedness, 315\nvirtual machines, 59\nNetﬂix Aminator framework, 219\nNetﬂix Simian Army, 315\nNetworks\naccess speed, 26–27\ncounters, 349\ninterface failures, 133\nprotocols, 489\nNew feature reviews in DevOps, 183\n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 545,
      "content": "514\nIndex\nNew Product Introduction and Removal\n(NPI/NPR)\nassessments, 435–436\noperational responsibility, 404\nNew services, launching, 382–384\nNielsen, Jerri, 225\nNon-blocking bandwidth, 137\nNon-functional requirements term, 32\nNon-goals in design documents, 277\nNonemergency tasks, 156\nNontechnical DevOps practices, 183–184\nNormal growth in capacity planning, 369\nNormal requests, 161\nNoSQL databases, 24\nNotiﬁcation types in oncall, 292–293\nObjectives in Incident Command\nSystem, 324\nObserve, Orient, Decide, Act (OODA)\nloop, 296–297\nO’Dell’s Axiom, 95\nOKR system, 389\nOn-premises, externally run services\nfactor in service platform\nselection, 67\nOncall, 285\nafter-hours maintenance coordination,\n294\nalert responsibilities, 295–296\nalert reviews, 302–304\nbeneﬁts, 152\ncalendar, 290–291, 355\ncontinuous deployment, 238\ndeﬁned, 148\ndesigning, 285–286\nDevOps, 183\nend-of-shift responsibilities, 299\nexcessive paging, 304–305\nexercises, 306\nfrequency, 291–292\nlong-term ﬁxes, 299–300\nnotiﬁcation types, 292–293\nonduty, 288\nOODA, 296–297\noperational rotation, 161–162\noverview, 163–164\nplaybooks, 297–298\npostmortems, 300–302\npre-shift responsibilities, 294\nregular responsibilities, 294–295\nrosters, 287\nschedule design, 288–290\nservice launches, 158\nSLAs, 286–287\nsummary, 305–306\nthird-party escalation, 298\nOnduty, 288\nOne percent testing, 233\nOne-to-many communication in\nmessage bus architectures, 85–86\nOne-way pagers, 293\nOODA (Observe, Orient, Decide, Act)\nloop, 296–297\nOpen source projects in dot-bomb era,\n460\nOpenStack system, 240\nOperating system installation, 219–220\nOperational excellence, 401\nassessment levels, 405–407\nassessment methodology, 403–407\nassessment questions and look-for’s,\n407\nexercises, 415\ngreatness measurement, 402–403\nimprovement levels, 412–413\norganizational assessments, 411–412\noverview, 401–402\nservice assessments, 407–410\nstarting, 413–414\nsummary, 414\nOperational Health, monitoring, 335\nOperational hygiene in service launches,\n158–159\nOperational requirements in designing\nfor operations, 31–32\nOperational responsibilities (OR),\n403–404\nOperational teams, 160–162\nﬁx-it days, 166\nfocus, 165–166\nin Incident Command System, 326\noncall days, 163–164\norganizing strategies, 160–166\nproject-focused days, 162–163\nticket duty days, 164–165\ntoil reduction, 166\nOperations, 147–148\nchange vs. stability, 149–151\n",
      "content_length": 2354,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 546,
      "content": "Index\n515\ndesign for. See Design for operations\nexercises, 168–169\norganizing strategies for operational\nteams, 160–166\nat scale, 152–155\nservice life cycle, 155–160\nSRE overview, 151–152\nSRE vs. traditional enterprise IT,\n148–149\nsummary, 167–168\nvirtual ofﬁces, 166–167\nOperations console in deployment\nphase, 217\nOperator errors, 171\nOppenheimer, D. L., 141, 171\nOpportunity costs in service platform\nselection, 66–67\nOptimizing level in CMM, 406\nOrders of magnitude, 478\nOrganizational assessments in\noperational excellence, 411–412\nOrganizational divisions, segmentation\nby, 103\nOrganizational memory, 157\nOrganizational training for disaster\npreparedness, 315–317\nOS installation, 219–220\nOutages\ncode review systems, 269\ndeﬁned, 120\ndisasters. See Disaster preparedness\nOverﬂow capacity factor in service\nplatform selection, 67\nOverload failures\nDoS and DDoS attacks, 139\nload shedding, 139\nscraping attacks, 140–141\ntrafﬁc surges, 138–139\nOversubscribed systems\ndeﬁned, 53\nspare capacity, 125\nPaaS (Platform as a Service), 51, 54–55\nPackages\nbuild phase, 204\nconﬁguration, 220\ncontinuous delivery, 190\ndeployment phase, 213\ndistributing, 266\nas handoff interface, 207–208\npinning, 212\nregistering, 206\nPager storms, 356\nPagers for alerts, 293, 304–305\nPanics, 128\nParasuraman, R., 248\nParavirtualization (PV), 58–59\nParity bits, 131–132\nPartition tolerance in CAP Principle,\n22–24\nParts and components failures, 131–134\nPast decisions, documenting, 276–277\nPatch lead time in service delivery, 201\nPatterson, D. A., 141\nPCI DSS requirements, 43\nPercentiles in monitoring, 359\nPerformance\ncaches, 105\ntesting, 215\nPerformance and Efﬁciency (PE)\nassessments, 439–441\noperational responsibility, 404\nPerformance regressions, 156, 215\nPerformant systems, 10\nPerl language, 259–260, 262\nPersistence in caches, 106\nPerspective in monitoring, 333\nPhased roll-outs, 229\nPhoenix Project, 172\nPhysical failures, 131\ndatacenters, 137–138\nload balancers, 134–136\nmachines, 134\nparts and components, 131–134\nracks, 136–137\nPhysical machines\ndeployment phase, 217–218\nfailures, 134\nservice platform selection, 57\nPie charts, 358\nPinheiro, E.\ndrive failures, 133, 338\nmemory errors, 134\nPinning packages, 212\nPKI (public key infrastructure), 40\nPlanned growth in capacity planning,\n369–370\nPlanning in disaster preparedness,\n318–319\n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 547,
      "content": "516\nIndex\nPlanning team in Incident Command\nSystem, 326\nPlatform as a Service (PaaS), 51, 54–55\nPlatform selection. See Service platform\nselection\nPlauger, P., 11\nPlaybooks\noncall, 297–298\nprocess, 153\nPods, 137\nPoints of presence (POPs), 83–85\nPollers, 352\nPost-crash recovery, 35\nPostmortems, 152\ncommunication, 302\nDevOps, 184\noncall, 291, 300–302\npurpose, 300–301\nreports, 301–302\ntemplates, 484–485\nPower failures, 34, 133\nPower of 2 mapping process, 110–111\nPractical Approach to Large-Scale Agile\nDevelopment: How HP Transformed\nHP LaserJet FutureSmart Firmware,\n188\nPractice of System and Network\nAdministration, 132, 204\nPre-checks, 141\nPre-shift oncall responsibilities, 294\nPre-submit checks in build phase,\n202–203\nPre-submit tests, 267\nPre-web era (1985-1994), 452–455\nPrefork processing module, 114\nPremature optimization, 96\nPrescriptive failure domains, 127\nPrimary resources\ncapacity planning, 372\ndeﬁned, 366\nPrioritizing\nautomation, 257–258\nfeature requests, 46\nfor stability, 150\nPrivacy in platform selection, 63\nPrivate cloud factor in platform\nselection, 62\nPrivate sandbox environments, 197\nProactive scaling solutions, 97–98\nProblems to solve in DevOps, 187\nProcess watchers, 128\nProcesses\nautomation beneﬁts, 253\ncontainers, 60\ninstead of threads, 114\nProctors for Game Day, 318\nProduct Management (PM) monitoring,\n336\nProduction candidates, 216\nProduction health in continuous\ndeployment, 237\nProject-focused days, 162–163\nProject planning frequencies, 410\nProject work, 161–162\nPromotion step in deployment phase,\n212\nPropellerheads, 451\nProportional shedding, 230\nProtocols\ncollections, 351\nnetwork, 489\nPrototyping, 258\nProvider comparisons in service\nplatform selection, 53\nProvisional end-of-shift reports, 299\nProvisioning\nin capacity planning, 384–385\nin DevOps, 185–186\nProxies\nmonitoring, 352\nreverse proxy service, 80\nPublic cloud factor in platform\nselection, 62\nPublic Information Ofﬁcers in Incident\nCommand System, 325–326\nPublic key infrastructure (PKI), 40\nPublic safety arena in Incident\nCommand System, 325\nPublishers in message bus architectures,\n85\nPublishing postmortems, 302\nPubSub2 system, 86\nPull monitoring, 350–351\nPuppet systems\nconﬁguration management, 261\ndeployment phase, 213\nmultitenant, 271\nPush conﬂicts in continuous\ndeployment, 238\n",
      "content_length": 2295,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 548,
      "content": "Index\n517\nPush monitoring, 350–351\n“Pushing Millions of Lines of Code Five\nDays a Week” presentation, 233\nPV (paravirtualization), 58–59\nPython language\nlibraries, 55\noverview, 259–261\nQPS (queries per second)\ndeﬁned, 10\nlimiting, 40–41\nQuadratic scaling, 476\nQuality Assurance monitoring, 335\nQuality assurance (QA) engineers, 199\nQuality measurements, 402\nQueries in HTTP, 69\nQueries of death, 130–131\nQueries per second (QPS)\ndeﬁned, 10\nlimiting, 40–41\nQueues, 113\nbeneﬁts, 113\ndraining, 35–36\nissue tracking systems, 263\nmessages, 86\nvariations, 113–114\nQuick ﬁxes vs. long-term, 295–296\nRabbitMQ service, 86\nRachitsky, L., 302\nRack diversity, 136\nRacks\nfailures, 136\nlocality, 137\nRAID systems, 132\nRAM\nfor caching, 104–106\nfailures, 123, 131–132\nRandom testing for disaster\npreparedness, 314–315\nRapid development, 231–232\nRate limits in design for operations,\n40–41\nRate monitoring, 348\nRationale, documenting, 276\nRe-assimilate tool, 255\nRead-only replica support, 37\nReal-time analysis, 353\nReal user monitoring (RUM), 333\nReboots, 34\nRecommendations in postmortem\nreports, 301\nRecommended reading, 487–489\nRecovery-Oriented Computing (ROC),\n461\nRecovery tool, 255\nRedis storage system, 24, 106\nReduced risk factor in service delivery,\n200\nReducing risk, 309–311\nReducing toil, automation for, 257\nRedundancy\ndesign for operations, 37\nﬁle chunks, 20\nfor resiliency, 124–125\nservers, 17\nReengineering components, 97\nRefactoring, 97\nRegional collectors, 352–353\nRegistering packages, 204, 206\nRegression analysis, 375–376\nRegression lines, 376\nRegression tests for performance, 156,\n215\nRegular meetings in DevOps, 187\nRegular oncall responsibilities, 294–295\nRegular software crashes, 128\nRegular Tasks (RT)\nassessments, 423–425\noperational responsibility, 403\nRegulating system integration, 250\nRelationships in DevOps, 182\nRelease atomicity, 240–241\nRelease candidates, 197\nRelease engineering practice in DevOps,\n186\nRelease vehicle packaging in DevOps,\n185\nReleases\ndeﬁned, 196\nDevOps, 185\nReliability\nautomation for, 253\nmessage bus architectures, 87\nReliability zones in service platform\nselection, 53–54\nRemote hands, 163\nRemote monitoring stations, 352\n",
      "content_length": 2169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 549,
      "content": "518\nIndex\nRemote Procedure Call (RPC) protocol,\n41\nRepair life cycle, 254–255\nRepeatability\nautomation for, 253\ncontinuous delivery, 190\nRepeatable level in CMM, 405\nReplacement algorithms for caches, 107\nReplicas, 124\nin design for operations, 37–38\nload balancers with, 12–13\nthree-tier web service, 76\nupdating, 18\nReports for postmortems, 301–302\nRepositories in build phase, 197\nReproducibility in continuous\ndeployment, 237\nRequests in updating state, 18\n“Resilience Engineering: Learning to\nEmbrace Failure” article, 320\nResiliency, 119–120\ncapacity planning, 370–371\nDevOps, 178\nexercises, 143\nfailure domains, 126–128\nhuman error, 141–142\nmalfunctions, 121–123\noverload failures, 138–141\nphysical failures. See Physical failures\nsoftware failures, 128–131\nsoftware vs. hardware, 120–121\nspare capacity for, 124–126\nsummary, 142\nResolution\nalert messages, 355\nmonitoring, 334\nResource pools, 99\nResource regression in capacity\nplanning, 381–382\nResource sharing\nservice platform selection, 62–65\nvirtual machines, 59\nResources\ncontention, 59, 238\ndeallocation, 160\ndynamic resource allocation, 138\nResponsibilities for oncall, 294–296\nRestarts, automated, 128–129\nRestores in design for operations, 36\nRetention monitoring, 338–339\nReverse proxy service, 80\nReview workﬂows in design documents,\n280–282\nReviewers in design documents, 277, 281\nRevising KPIs, 391–392\nRevision numbers in design documents,\n277, 282\nRework time factor in service delivery,\n201\nRich, Amy, 51\nRichard, Dylan, 320\nRisk reduction, 309–311\nRisk system, 24\nRisk taking in DevOps, 178\nRituals in DevOps, 178\nRobbins, Jesse\ncommunication beneﬁts, 186\nDiRT tests, 320\ntest planning, 319–320\nROC (Recovery-Oriented Computing),\n461\nRoll back, 239\nRoll forward, 239\nRolling upgrades, 226\nRoosevelt, Franklin D., 275\nRoosevelt, Theodore, 307\nRoot cause analysis, 301–302\nRoot servers, 17\nRossi, Chuck, 233\nRosters, oncall, 287\nRound-robin for backends, 12\nRound robin (RR) load balancing, 72–74\nRoyce, D. W. W., 175\nRPC (Remote Procedure Call) protocol,\n41\nRSS feeds of build status, 205\nRubin, A. D., 79\nRuby language, 259–260\nRUM (real user monitoring), 333\n“Run run run dead” problem, 462\nSaaS (Software as a Service), 51, 55–56\nSafeguards, automation for, 253\nSafety for automation, 249\nSalesforce.com, 55–56\nSample launch readiness review survey,\n157–158\nSandbox environments, 197\n",
      "content_length": 2361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 550,
      "content": "Index\n519\nSatellites in cloud-scale service, 83\nScalability Rules: 50 Principles for Scaling\nWeb Sites, 100\nScale\noperations at, 152–155\nvisibility at, 10–11\nScaling, 95, 475\nAKF Scaling Cube, 99–104\nautomation for, 252\nBig O notation, 476–479\ncaching, 104–110\ncloud computing era, 471\nconstant, linear, and exponential,\n475–476\ncontent delivery networks, 114–116\ndata sharding, 110–112\ndatabase access, 37\ndot-bomb era, 462–463\nexercises, 116–117\nﬁrst web era, 456–457\ngeneral strategy, 96–98\nmonitoring, 350\nPaaS services, 54\npre-web era, 454\nqueueing, 113–114\nrecommended reading, 489\nin resiliency, 135\nscaling out, 99–101\nscaling up, 98–99\nsecond web era, 467–468\nsmall-scale computing systems, 470\nsummary, 116\nthreading, 112–113\nthree-tier web service, 76\nSchedules\ncontinuous deployment, 238\noncall, 288–291\nSchema changes, 234–236\nSchlossnagle, Theo, 31, 172\nSchroeder, B., 134\nScope in disaster preparedness, 317–318\nScraping attacks, 140–141\nScripting languages, 259–260\nSDLC (Software Development Life\nCycle), 184–185\nSeaLand company, 62\nSecond web era (2003-2010), 465–469\nSecondary resources in capacity\nplanning, 372\nSecurity in four-tier web service, 79\nSee, K. A., 249\nSegments in lookup-oriented splits,\n102–103\nSelenium WebDriver project, 215\nSelf-service launches at Google, 159\nSelf-service requests, 154\nSend to Repairs tool, 255\nSenge, Peter, 147\nSensing and measurement systems,\n345–350\nServer trees, 16–17, 80\nServerFault.com, 102\nServers\ncollections, 352\ndeﬁned, 10\nwith multiple backends, 14–15\nService assessments, operational\nexcellence, 407–410\nService delivery\nassessments, 442–445\nbuild phase. See Builds\ndeployment phase. See Deployment\nand deployment phase\nﬂow, 196\nService Deployment and\nDecommissioning (SDD), 404,\n437–438\nService latency in cloud computing era,\n471\nService level agreements (SLAs)\nError Budgets, 152\nload shedding, 139\nmonitoring, 334\noncall, 286–287\nService Level Indicators (SLIs), 334\nService Level Objectives (SLOs), 334\nService Level Targets (SLTs), 334\nService life cycle, 155\ndecommissioning services, 160\nstages, 156–160\nService management monitoring, 334\nService-oriented architecture (SOA),\n90–91\nbest practices, 91–92\nﬂexibility, 91\nsupport, 91\nService platform selection, 51–52\ncolocation, 65–66\ncontainers, 60–61\n",
      "content_length": 2280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 551,
      "content": "520\nIndex\nService platform selection (continued)\nexercises, 68\nlevel of service abstraction, 52–56\nmachine overview, 56\nphysical machines, 57\nresource sharing levels, 62–65\nstrategies, 66–68\nsummary, 68\nvirtual machines, 57–60\nService splits in AKF Scaling Cube,\n101–102\nService testing in disaster preparedness,\n313–314\nServices\nassessing, 407–410\ndecommissioning, 160\ndeﬁned, 10\ninstalling, 219–220\nrestart, 34\nSRE vs. traditional enterprise IT, 148–149\nSession IDs, 76\n7-day actives (7DA), 373\nSharding, 110–112\nShards, 16, 18–19\nShared oncall responsibilities, 183\nShared pools, 138\nShared state in load balancing, 75\nShaw, George Bernard, 387\nShedding, proportional, 230\nShell scripting languages, 259\nSheridan, T. B., 248\n“Shewhart cycle,” 172\nShifts, oncall, 164–165, 291–292\nShipping containers, 62\nShort-lived machines, 58\nShort-term analysis, 353\nShutdown in design for operations,\n34–35\nSign-off for design documents, 281–282\nSignal line crossover, 367\nSilencing alert messages, 356–357\nSilos, 174\nSimian Army, 315\nSimple Network Management Protocol\n(SNMP), 351\nSimple Queue Service (SQS), 86\nSimplicity\nimportance, 11\nreview workﬂows, 280\nSingapore MAS requirements, 43\nSingle-machine web servers, 70–71\nSite Reliability Engineering (SRE),\n147–148\nDevOps, 181\noverview, 151–152\nvs. traditional enterprise IT, 148–149\nSite reliability practices, 151–152\nSize\nbatches, 178–179\ncaches, 108–110\nSLAs (service level agreements)\nError Budgets, 152\nload shedding, 139\nmonitoring, 334\noncall, 286–287\nSLIs (Service Level Indicators), 334\nSLOs (Service Level Objectives), 334\nSloss, Benjamin Treynor\nGoogle Error Budget, 396\nsite reliability practices, 151\nSlow start algorithm, 13\nSLTs (Service Level Targets), 334\nSmall-scale computing systems, scaling,\n470\nSMART (Speciﬁc, Measurable,\nAchievable, Relevant, and\nTime-phrased) criteria, 388\nSmart phone apps for alerts, 293\nSmoke tests, 192\nSMS messages for alerts, 293\nSNMP (Simple Network Management\nProtocol), 351\nSOA (service-oriented architecture),\n90–91\nbest practices, 91–92\nﬂexibility, 91\nsupport, 91\nSoft launches, 148, 382\nSoftware as a Service (SaaS), 51, 55–56\nSoftware Development Life Cycle\n(SDLC), 184–185\nSoftware engineering tools and\ntechniques in automation, 262–263\ncode reviews, 268–269\nissue tracking systems, 263–265\npackages, 266\nstyle guides, 266–267, 270\nsufﬁcient code, 269–270\ntest-driven development, 267–268\nversion control systems, 265–266\n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 552,
      "content": "Index\n521\nSoftware engineers (SWEs), 199\nSoftware failures, 128\ncrashes, 128–129\nhangs, 129–130\nqueries of death, 130–131\nSoftware load balancers, 136\nSoftware packages. See Packages\nSoftware resiliency, 120–121\nSoftware upgrades in design for\noperations, 36\nSolaris containers, 60\nSolid-state drives (SSDs)\nfailures, 132\nspeed, 26\nSource control systems, 206\nSOX requirements, 43\nSpafford, G., 172\nSpare capacity, 124–125\nload sharing vs. hot spares, 126\nneed for, 125–126\nSpear, S., 172\nSpecial constraints in design documents,\n278\nSpecial notations in style guides, 267\nSpeciﬁc, Measurable, Achievable,\nRelevant, and Time-phrased\n(SMART) criteria, 388\nSpeed\nimportance, 10\nissues, 26–29\nSpell check services, abstraction in, 24\nSpindles, 26\nSplit brain, 23\nSplit days oncall schedules, 289\nSpolsky, J., 121\nSprints, 189\nSQS (Simple Queue Service), 86\nSRE (Site Reliability Engineering),\n147–148\nDevOps, 181\noverview, 151–152\nvs. traditional enterprise IT, 148–149\nSSDs (solid-state drives)\nfailures, 132\nspeed, 26\nStability vs. change, 149–151\nStack Exchange, 167\nStack ranking, 360\nStakeholders, 148\nStandard capacity planning, 366–368\nStandardized shipping containers, 62\nStartup in design for operations, 34–35\nStates, distributed, 17–20\nStatic content on web servers, 70\nStatus of design documents, 277, 282\nSteal time, 59\nStickiness in load balancing, 75\nStorage systems, monitoring, 345, 362\nStranded capacity, 57\nStranded resources in containers, 61\nStyle guides\nautomation, 266–267, 270\ncode review systems, 269\nSub-linear scaling, 477\nSubscribers in message bus\narchitectures, 86\nSubsystems, linking tickets to, 263–264\nSuggested resolution in alert messages,\n355\nSummarization, monitoring, 339\nSuper-linear scaling, 477\nSurvivable systems, 120\nSWEs (software engineers), 199\nSynthesized measurements, 347–348\nSystem administration, automating,\n248–249, 253\nSystem logs, 340\nSystem testing\nin build phase, 203\nvs. canarying, 228–229\noverview, 215\nT-bird database system, 103\nTags in repositories, 208\nTaking down services for upgrading,\n225–226\nTargeting in system integration, 250\nTasks\nassessments, 423–425\nautomating, 153–155\nTCO (total cost of ownership), 172\nTDD (test-driven development), 267–268\nTeam managers in operational rotation,\n162\nTeamCity tool, 205\nTeams, 160–162\nautomating processes, 155\nﬁx-it days, 166\nfocus, 165–166\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 553,
      "content": "522\nIndex\nTeams (continued)\nin Incident Command System, 326\noncall days, 163–164\norganizing strategies, 160–166\nproject-focused days, 162–163\nticket duty days, 164–165\ntoil reduction, 166\nvirtual ofﬁces, 166–167\nTechnical debt, 166\nTechnical practices in DevOps, 184–185\nTechnology\ncloud computing era, 472\ndot-bomb era, 460–461\nﬁrst web era, 455–456\npre-web era, 453–454\nsecond web era, 465–466\nTelles, Marcel, 401\nTemplates\ndesign documents, 279, 282, 481–484\npostmortem, 484–485\nTerminology for Incident Command\nSystem, 324\nTest-driven development (TDD),\n267–268\nTests\nvs. canarying, 228–229\ncontinuous deployment, 237\ndeployment phase, 215–216\nDevOps, 186\ndisaster preparedness. See Disaster\npreparedness\nearly and fast, 195\nenvironments, 197\nﬂag ﬂips for, 232–233\nText-chat, 167\nText ﬁles for conﬁguration, 33\nText messages for alerts, 293\nTheme for operational teams, 165–166\nTheory, recommended reading for, 488\nThialﬁsystem, 86\n“Things You Should Never Do” essay,\n121\nThird-party vendors\ndesign for operations, 48\noncall escalation, 298\n30-day actives (30DA), 373\nThompson, Ken, 245\nThreading, 112–113\nThree-tier web service, 71–72\nload balancer methods, 74\nload balancer types, 72–73\nload balancing with shared state,\n75\nscaling, 76–77\nuser identity, 76\nTicket duty\ndescription, 161–162\nticket duty days, 164–165\nTime Management for System\nAdministrators, 162, 256\nTime savings, automation for, 253\nTime series, 366\nTime to live (TTL) value for caches,\n108\nTime zones in oncall schedules, 289\nTimed release dates, 232\nTimelines of events, 301\nTimestamps in logs, 341–342\nTimetables in capacity planning, 371\nTitles in design documents, 277, 282\nTODO comments in style guides, 267,\n270\nToggling features, 39, 230–233\nToil\ndeﬁned, 244\nreducing, 166, 257, 446–447\nTool building vs. automation, 250–252\nTorvalds, Linus, 276\nTotal cost of ownership (TCO), 172\nTracebacks, 129\nTracking system integration, 250\nTrafﬁc\ndeﬁned, 10\nsurges, 138–139\nTrailing averages, 13\nTraining\ndisaster preparedness, 311–312,\n315–317\noncall, 287\nTransit ISPs, 83\nTrends, monitoring, 333\nTriggers, 353\nTrustworthiness of automation, 249\nTseitlin, A., 315, 320\nTTL (time to live) value for caches, 108\nTufte, E. R., 362\nTumblr Invisible Touch system, 218\nTwain, Mark, 243\nTwitter, 103\nTwo-way pagers, 293\n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 554,
      "content": "Index\n523\nUAT (User Acceptance Testing), 216\nUbuntu Upstart system, 34\nUnaligned failure domains, 127\nUndersubscribed systems, 53\nUniﬁed goals, 397–398\nUninterrupted time, 164\nUninterruptible power supply (UPS)\nsystems, 34\nUnit tests in build phase, 203\nUNIX, recommended reading, 489\nUnused code, bugs in, 270\nUpdating state, 18\nUpgrades\nBlog Search, 226\noverview, 156\nsoftware, 36\nUpgrading live services, 225–226\nblue-green deployment, 230\ncanary process, 227–228\ncontinuous deployment, 236–239\nexercises, 241–242\nfailed code pushes, 239–240\nlive code changes, 236\nlive schema changes, 234–236\nphased roll-outs, 229\nproportional shedding, 230\nrelease atomicity, 240–241\nrolling upgrades, 226–227\nsummary, 241\ntaking down services for, 225–226\ntoggling features, 230–233\nUploading to CDNs, 115\nUPS (uninterruptible power supply)\nsystems, 34\nUptime in SRE vs. traditional enterprise\nIT, 149\nUrgent bug count, monitoring, 336\nUrgent bug resolution, monitoring, 336\nURLs for CDNs, 115\nU.S. Federal Emergency Management\nAdministration web site, 324\nUser Acceptance Testing (UAT), 216\nUser identity in three-tier web service,\n76\nUser satisfaction, monitoring, 336\nUser-speciﬁc data, global load balancing\nwith, 82–83\nUser stories, 189\nUser wait time, automation for, 253\nUtilization, segmentation by, 103\nUtilization Limit load balancing, 74\nVagrant framework, 219\nValue streams in DevOps, 176\nVarnish HTTP accelerator, 478\nVCSs (version control systems), 265–266\nVelocity in DevOps, 179\nVendor lock-in, 56\nVendors\ndesign for operations, 48\noncall escalations, 298\nVersion conﬂicts in containers, 60–61\nVersion control systems (VCSs), 265–266\nVersion-controlled builds, 191\nVertical integration, 64\nViews in live schema changes, 234\nVirtual machine monitor (VMM), 58–59\nVirtual machines\nbeneﬁts, 58\ndeployment phase, 218\ndisadvantages, 59–60\nIaaS, 52\nI/O, 58–59\noverview, 57\nservice platform selection, 66\nVirtual ofﬁces, 166–167\nVirtuous cycle of quality, 200–201\nVisibility at scale, 10–11\nVisual Display of Quantitative Information,\n362\nVisualization, monitoring, 333, 358–362\nVMM (virtual machine monitor), 58–59\nVoice calls for alerts, 293\nVolatile data in OS installation, 219–220\nWait time\nautomation for, 253\nservice delivery, 201\nWAN (wide area network) connections,\n83\nWarmed caches, 106\nWatchdog timers, 130\nWaterfall methodology\noverview, 173–175\nphases, 199\nWAUs (weekly active users), 373\n“Weathering the Unexpected” article,\n320\n",
      "content_length": 2441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 555,
      "content": "524\nIndex\nWeb “Hit” logs, 340\n“Web Search for a Planet: The Google\nCluster Architecture” article, 464\nWeb servers, single-machine, 70–71\nWeb services\nfour-tier, 77–80\nthree-tier, 71–77\nWeber, W.-D.\ndrive errors, 133, 338\nmemory errors, 134\nWeekly active users (WAUs), 373\nWeekly oncall schedules, 288–289\nWeighted RR load balancing, 74\nWheel of Misfortune game, 311–312\n“When the Nerds Go Marching in”\narticle, 316, 320\nWhitebox monitoring, 346–347\nWhitelists, 40–42\n“Why Do Internet Services Fail, and\nWhat Can Be Done about It?”\npaper, 141, 171\nWickens, C. D., 248\nWide area network (WAN) connections,\n83\nWilde, Oscar, 345\nWillis, John, 180, 200\nWilly Wonka, 195\nWoolf, B., 87\nWorker threads, 112\nWorkﬂows\ndesign documents, 280–282\nDevOps, 176–177\nWorking from home, 166–167\nWrites in updating state, 18\nX-axes in AKF Scaling Cube, 99–101\nX-Forwarded-For headers, 73\nY-axes in AKF Scaling Cube, 99,\n101–102\nYan, B., 371\n“You’re Doing It Wrong” article, 479\nYoung, M., 188, 215\nZ-axes in AKF Scaling Cube, 99, 102–104\nZero line crossover, 367\nZones, Solaris, 60\nZooKeeper system, 231, 363\n",
      "content_length": 1090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 556,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 557,
      "content": "Essential Practices Previously Handed\nDown Only from Mentor to Protégé\ninformit.com/tposa\nWhether you use Linux, Unix, or Windows, this \nwonderfully lucid, often funny cornucopia of \ninformation introduces beginners to advanced \nframeworks valuable for their entire career, yet is \nstructured to help even the most advanced experts \nthrough difﬁcult projects.\nWith 28 new chapters, the third edition has been \nrevised with thousands of updates and clariﬁca-\ntions based on reader feedback. This new edition \nalso incorporates DevOps strategies even for \nnon-DevOps environments.\nFor more information and sample content visit informit.com/tposa.\neBook and print formats available.\n",
      "content_length": 680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 558,
      "content": "Register the Addison-Wesley, Exam \nCram, Prentice Hall, Que, and \nSams products you own to unlock \ngreat beneﬁ ts. \nTo begin the registration process, \nsimply go to informit.com/register \nto sign in or create an account. \nYou will then be prompted to enter \nthe 10- or 13-digit ISBN that appears \non the back cover of your product.\ninformIT.com \nTHE TRUSTED TECHNOLOGY LEARNING SOURCE\nAddison-Wesley  |  Cisco Press  |  Exam Cram  \nIBM Press   |   Que   |   Prentice Hall   |   Sams \nSAFARI BOOKS ONLINE\nAbout InformIT — THE TRUSTED TECHNOLOGY LEARNING SOURCE\nINFORMIT IS HOME TO THE LEADING TECHNOLOGY PUBLISHING IMPRINTS \nAddison-Wesley Professional, Cisco Press, Exam Cram, IBM Press, Prentice Hall \nProfessional, Que, and Sams. Here you will gain access to quality and trusted content and \nresources from the authors, creators, innovators, and leaders of technology. Whether you’re \nlooking for a book on a new technology, a helpful article, timely newsletters, or access to \nthe Safari Books Online digital library, InformIT has a solution for you.\nRegistering your products can unlock \nthe following beneﬁ ts:\n•  Access to supplemental content, \nincluding bonus chapters, \nsource code, or project ﬁ les. \n•  A coupon to be used on your \nnext purchase.\nRegistration beneﬁ ts vary by product.  \nBeneﬁ ts will be listed on your Account \npage under Registered Products.\ninformit.com/register\nTHIS PRODUCT\n",
      "content_length": 1407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 559,
      "content": " InformIT is a brand of Pearson and the online presence \nfor the world’s leading technology publishers. It’s your source \nfor reliable and qualified content and knowledge, providing \naccess to the top brands, authors, and contributors from \nthe tech community.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nLearnIT at InformIT\nLooking for a book, eBook, or training video on a new technology? Seek-\ning timely and relevant information and tutorials? Looking for expert opin-\nions, advice, and tips?  InformIT has the solution.\n•  Learn about new releases and special promotions by \nsubscribing to a wide variety of newsletters. \nVisit informit.com/newsletters.\n•   Access FREE podcasts from experts at informit.com/podcasts.\n•   Read the latest author articles and sample chapters at \ninformit.com/articles.\n•  Access thousands of books and videos in the Safari Books \nOnline digital library at safari.informit.com.\n• Get tips from expert blogs at informit.com/blogs.\nVisit informit.com/learn to discover all the ways you can access the \nhottest technology content.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nAre You Part of the IT Crowd?\nConnect with Pearson authors and editors via RSS feeds, Facebook, \nTwitter, YouTube, and more! Visit informit.com/socialconnect.\n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}