{
  "metadata": {
    "title": "Software Architecture in Practice 3rd Edition",
    "author": "Len Bass,Paul Clements,Rick Kazman",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 620,
    "conversion_date": "2025-12-19T17:45:22.790454",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Software Architecture in Practice 3rd Edition.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "T\nhe SEI Series in Software Engineering represents is a collaborative \nundertaking of the Carnegie Mellon Software Engineering Institute (SEI) and \nAddison-Wesley to develop and publish books on software engineering and \nrelated topics. The common goal of the SEI and Addison-Wesley is to provide \nthe most current information on these topics in a form that is easily usable by \npractitioners and students.\nBooks in the series describe frameworks, tools, methods, and technologies \ndesigned to help organizations, teams, and individuals improve their technical \nor management capabilities. Some books describe processes and practices for \ndeveloping higher-quality software, acquiring programs for complex systems, or \ndelivering services more effectively. Other books focus on software and system \narchitecture and product-line development. Still others, from the SEI’s CERT \nProgram, describe technologies and practices needed to manage software \nand network security risk. These and all books in the series address critical \nproblems in software engineering for which practical solutions are available. \nVisit informit.com/sei for a complete list of available products.\nThe SEI Series in \nSoftware Engineering\n\n\nSoftware\nArchitecture\nin Practice\nThird Edition \nLen Bass\nPaul Clements\nRick Kazman\n▼\n▲\n▼ Addison-Wesley\nUpper Saddle River, NJ  •  Boston  •  Indianapolis  •  San Francisco\nNew York  •  Toronto  •  Montreal  •  London  •  Munich  •  Paris  •  Madrid\nCapetown  •  Sydney  •  Tokyo  •  Singapore  •  Mexico City\n\n\nThe SEI Series in Software Engineering\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nCMM, CMMI, Capability Maturity Model, Capability Maturity Modeling, Carnegie Mellon, CERT, \nand CERT Coordination Center are registered in the U.S. Patent and Trademark Office by Carnegie \nMellon University. \nATAM; Architecture Tradeoff Analysis Method; CMM Integration; COTS Usage-Risk Evaluation; \nCURE; EPIC; Evolutionary Process for Integrating COTS Based Systems; Framework for Software \nProduct Line Practice; IDEAL; Interim Profile; OAR; OCTAVE; Operationally Critical Threat, Asset, \nand Vulnerability Evaluation; Options Analysis for Reengineering; Personal Software Process; PLTP; \nProduct Line Technical Probe; PSP; SCAMPI; SCAMPI Lead Appraiser; SCAMPI Lead Assessor; \nSCE; SEI; SEPG; Team Software Process; and TSP are service marks of Carnegie Mellon University. \nSpecial permission to reproduce portions of works copyright by Carnegie Mellon University, as listed \non page 588, is granted by the Software Engineering Institute.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or \nimplied warranty of any kind and assume no responsibility for errors or omissions. No liability is \nassumed for incidental or consequential damages in connection with or arising out of the use of the \ninformation or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales opportunities (which may \ninclude electronic versions; custom cover designs; and content particular to your business, training \ngoals, marketing focus, or branding interests), please contact our corporate sales department at corp-\nsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the U.S., please contact international@pearsoned.com.\nVisit us on the Web: informit.com/aw\nLibrary of Congress Cataloging-in-Publication Data\nBass, Len.\n    Software architecture in practice / Len Bass, Paul Clements, Rick Kazman.—3rd ed.\n    p.  cm.—(SEI series in software engineering)\n    Includes bibliographical references and index.\n    ISBN 978-0-321-81573-6 (hardcover : alk. paper) 1. Software architecture. 2. System  \ndesign. I. Clements, Paul, 1955– II. Kazman, Rick. III. Title.\n    QA76.754.B37 2012\n    005.1—dc23\n\t\n2012023744\nCopyright © 2013 Pearson Education, Inc.\nAll rights reserved. Printed in the United States of America. This publication is protected by copy-\nright, and permission must be obtained from the publisher prior to any prohibited reproduction, stor-\nage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, pho-\ntocopying, recording, or likewise. To obtain permission to use material from this work, please submit \na written request to Pearson Education, Inc., Permissions Department, 200 Old Tappan Road, Old \nTappan, New Jersey 07657, or you may fax your request to (201) 236-3290.\nISBN-13: 978-0-321-81573-6 \nISBN-10: 0-321-81573-4\nText printed in the United States on recycled paper at Courier in Westford, Massachusetts. \nFifth printing, September 2015\n\n\nv\nContents\nPreface    xv\nReader’s Guide    xvii\nAcknowledgments    xix\n\tPart ONE\t Introduction    1\nCHAPTER  1\nWhat Is Software Architecture?    3\n1.1\tWhat Software Architecture Is and What It \nIsn’t        4\n1.2  Architectural Structures and Views        9\n1.3  Architectural Patterns        18\n1.4  What Makes a “Good” Architecture?        19\n1.5  Summary        21\n1.6  For Further Reading        22\n1.7  Discussion Questions        23\nCHAPTER  2\nWhy Is Software Architecture Important?    25\n2.1  Inhibiting or Enabling a System’s Quality \nAttributes        26\n2.2  \u0007Reasoning About and Managing \nChange        27\n2.3  Predicting System Qualities         28\n2.4  \u0007Enhancing Communication among \nStakeholders        29\n2.5  Carrying Early Design Decisions        31\n2.6  \u0007Defining Constraints on an \nImplementation        32\n2.7  Influencing the Organizational Structure         33\n2.8  Enabling Evolutionary Prototyping        33\n\n\nvi \nContents\t\n2.9  Improving Cost and Schedule Estimates        34\n2.10  Supplying a Transferable, Reusable \nModel        35\n2.11  \u0007Allowing Incorporation of Independently \nDeveloped Components        35\n2.12  Restricting the Vocabulary of Design \nAlternatives        36\n2.13  Providing a Basis for Training        37\n2.14  Summary        37\n2.15  For Further Reading        38\n2.16  Discussion Questions        38\nCHAPTER  3\nThe Many Contexts of Software \nArchitecture    39\n3.1  Architecture in a Technical Context        40\n3.2  Architecture in a Project Life-Cycle \nContext        44\n3.3  Architecture in a Business Context        49\n3.4  Architecture in a Professional Context        51\n3.5  Stakeholders        52\n3.6  How Is Architecture Influenced?        56\n3.7  What Do Architectures Influence?         57\n3.8  Summary         59\n3.9  For Further Reading        59\n3.10  Discussion Questions        60\n\tPart TWO\t Quality Attributes    61\nCHAPTER  4\nUnderstanding Quality Attributes    63\n4.1  Architecture and Requirements        64\n4.2  Functionality        65\n4.3  Quality Attribute Considerations         65\n4.4  \tSpecifying Quality Attribute \nRequirements        68\n4.5  \tAchieving Quality Attributes through \nTactics        70\n4.6  Guiding Quality Design Decisions        72\n4.7  Summary        76\n\n\nContents\nvii\n4.8  For Further Reading        77\n4.9  Discussion Questions        77\nCHAPTER  5\nAvailability    79\n5.1  Availability General Scenario        85\n5.2  Tactics for Availability        87\n5.3  A Design Checklist for Availability        96\n5.4  Summary        98\n5.5  For Further Reading        99\n5.6  Discussion Questions        100\nCHAPTER  6\nInteroperability    103\n6.1  Interoperability General Scenario        107\n6.2  Tactics for Interoperability        110\n6.3  A Design Checklist for Interoperability        114\n6.4  Summary        115\n6.5  For Further Reading        116\n6.6  Discussion Questions        116\nCHAPTER  7\nModifiability    117\n7.1 Modifiability General Scenario        119\n7.2  Tactics for Modifiability        121\n7.3  A Design Checklist for Modifiability        125\n7.4  Summary        128\n7.5  For Further Reading        128\n7.6  Discussion Questions        128\nCHAPTER  8\nPerformance    131\n8.1  Performance General Scenario        132\n8.2  Tactics for Performance        135\n8.3  A Design Checklist for Performance        142\n8.4  Summary        145\n8.5  For Further Reading        145\n8.6  Discussion Questions        145\nCHAPTER  9\nSecurity    147\n9.1  Security General Scenario        148\n9.2  Tactics for Security        150\n\n\nviii \nContents\t\n9.3  A Design Checklist for Security        154\n9.4  Summary        156\n9.5  For Further Reading        157\n9.6  Discussion Questions        158\nCHAPTER  10\nTestability    159\n10.1  Testability General Scenario        162\n10.2  Tactics for Testability        164\n10.3  A Design Checklist for Testability        169\n10.4  Summary        172\n10.5  For Further Reading        172\n10.6  Discussion Questions        173\nCHAPTER  11\nUsability    175\n11.1  Usability General Scenario        176\n11.2  Tactics for Usability        177\n11.3  A Design Checklist for Usability        181\n11.4  Summary        183\n11.5  For Further Reading        183\n11.6  Discussion Questions        183\nCHAPTER  12\nOther Quality Attributes    185\n12.1  Other Important Quality Attributes        185\n12.2  Other Categories of Quality Attributes        189\n12.3  \u0007Software Quality Attributes and System \nQuality Attributes        190\n12.4  \u0007Using Standard Lists of Quality Attributes— \nor Not         193\n12.5  \u0007Dealing with “X-ability”: Bringing a New \nQuality Attribute into the Fold        196\n12.6  For Further Reading        200\n12.7  Discussion Questions        201\nCHAPTER  13\nArchitectural Tactics and Patterns    203\n13.1  Architectural Patterns        204\n13.2  Overview of the Patterns Catalog        205\n13.3  \u0007Relationships between Tactics and \nPatterns         238\n\n\nContents\nix\n13.4  Using Tactics Together         242\n13.5  Summary        247\n13.6  For Further Reading        248\n13.7  Discussion Questions        249\nCHAPTER  14\nQuality Attribute Modeling and Analysis    251\n14.1  Modeling Architectures to Enable Quality \nAttribute Analysis        252\n14.2  Quality Attribute Checklists        260\n14.3  \u0007Thought Experiments and  \nBack-of-the-Envelope Analysis        262\n14.4  Experiments, Simulations, and \nPrototypes        264\n14.5  Analysis at Different Stages of the Life \nCycle        265\n14.6  Summary        266\n14.7  For Further Reading        267\n14.8  Discussion Questions        269\n\t Part THREE\t Architecture in the Life \nCycle    271\nCHAPTER  15\nArchitecture in Agile Projects    275\n15.1  How Much Architecture?        277\n15.2  Agility and Architecture Methods        281\n15.3  A Brief Example of Agile Architecting        283\n15.4  Guidelines for the Agile Architect        286\n15.5  Summary        287\n15.6  For Further Reading        288\n15.7  Discussion Questions        289\nCHAPTER  16\nArchitecture and Requirements    291\n16.1  Gathering ASRs from Requirements \nDocuments        292\n16.2  Gathering ASRs by Interviewing \nStakeholders        294\n16.3  \u0007Gathering ASRs by Understanding the \nBusiness Goals        296\n",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "x \nContents\t\n16.4  Capturing ASRs in a Utility Tree        304\n16.5  Tying the Methods Together        308\n16.6  Summary        308\n16.7  For Further Reading        309\n16.8  Discussion Questions        309\nCHAPTER  17\nDesigning an Architecture    311\n17.1  Design Strategy        311\n17.2  The Attribute-Driven Design Method        316\n17.3  The Steps of ADD        318\n17.4  Summary        325\n17.5  For Further Reading        325\n17.6  Discussion Questions        326\nCHAPTER  18\nDocumenting Software Architectures    327\n18.1  \u0007Uses and Audiences for Architecture \nDocumentation        328\n18.2  Notations for Architecture \nDocumentation        329\n18.3  Views        331\n18.4  Choosing the Views        341\n18.5  Combining Views         343\n18.6  Building the Documentation Package        345\n18.7  Documenting Behavior        351\n18.8  Architecture Documentation and Quality \nAttributes         354\n18.9  \u0007Documenting Architectures That Change \nFaster Than You Can Document Them        355\n18.10  \u0007Documenting Architecture in an Agile \nDevelopment Project        356\n18.11  Summary        359\n18.12  For Further Reading        360\n18.13  Discussion Questions        360\nCHAPTER  19\nArchitecture, Implementation, and \nTesting    363\n19.1  Architecture and Implementation        363\n19.2  Architecture and Testing        370\n\n\nContents\nxi\n19.3  Summary        376\n19.4  For Further Reading        376\n19.5  Discussion Questions        377\nCHAPTER  20\nArchitecture Reconstruction and \nConformance    379\n20.1  Architecture Reconstruction Process         381\n20.2  Raw View Extraction        382\n20.3  Database Construction        386\n20.4  View Fusion        388\n20.5  Architecture Analysis: Finding \nViolations        389\n20.6  Guidelines        392\n20.7  Summary        393\n20.8  For Further Reading        394\n20.9  Discussion Questions         395\nCHAPTER  21\nArchitecture Evaluation    397\n21.1  Evaluation Factors        397\n21.2  The Architecture Tradeoff Analysis \nMethod        400\n21.3  Lightweight Architecture Evaluation        415\n21.4  Summary        417\n21.5  For Further Reading        417\n21.6  Discussion Questions         418\nCHAPTER  22\nManagement and Governance    419\n22.1  Planning        420\n22.2  Organizing        422\n22.3  Implementing        427\n22.4  Measuring        429\n22.5  Governance        430\n22.6  Summary        432\n22.7  For Further Reading        432\n22.8  Discussion Questions        433\n\n\nxii \nContents\t\n\t\nPart FOUR\t Architecture and \nBusiness    435\nCHAPTER  23\nEconomic Analysis of Architectures    437\n23.1  Decision-Making Context        438\n23.2  The Basis for the Economic Analyses        439\n23.3  Putting Theory into Practice:  \nThe CBAM        442\n23.4  Case Study: The NASA ECS Project        450\n23.5  Summary        457\n23.6  For Further Reading        458\n23.7  Discussion Questions        458\nCHAPTER  24\nArchitecture Competence    459\n24.1  \u0007Competence of Individuals: Duties, Skills, and \nKnowledge of Architects        460\n24.2  Competence of a Software Architecture \nOrganization        467\n24.3  Summary        475\n24.4  For Further Reading        475\n24.5  Discussion Questions        477\nCHAPTER  25\nArchitecture and Software Product Lines    479\n25.1  An Example of Product Line \nVariability        482\n25.2  What Makes a Software Product Line \nWork?        483\n25.3  Product Line Scope        486\n25.4  The Quality Attribute of Variability        488\n25.5  The Role of a Product Line \nArchitecture        488\n25.6  Variation Mechanisms        490\n25.7  Evaluating a Product Line \nArchitecture        493\n25.8  Key Software Product Line Issues        494\n25.9  Summary        497\n25.10  For Further Reading        498\n25.11  Discussion Questions        498\n\n\nContents\nxiii\n\tPart FIVE\t The Brave New World    501\nCHAPTER  26\nArchitecture in the Cloud    503\n26.1  Basic Cloud Definitions        504\n26.2  Service Models and Deployment \nOptions        505\n26.3  Economic Justification        506\n26.4  Base Mechanisms        509\n26.5  Sample Technologies        514\n26.6  Architecting in a Cloud Environment        520\n26.7  Summary        524\n26.8  For Further Reading        524\n26.9  Discussion Questions        525\nCHAPTER  27\nArchitectures for the Edge    527\n27.1  The Ecosystem of Edge-Dominant \nSystems        528\n27.2  Changes to the Software Development Life \nCycle        530\n27.3  Implications for Architecture        531\n27.4  Implications of the Metropolis Model        533\n27.5  Summary        537\n27.6  For Further Reading        538\n27.7  Discussion Questions        538\nCHAPTER  28\nEpilogue    541\nReferences    547\nAbout the Authors    561\nIndex    563\n\n\nThis page intentionally left blank \n\n\nxv\nPreface\nI should have no objection to go over the same \nlife from its beginning to the end: requesting only \nthe advantage authors have, of correcting in a \n[third] edition the faults of the first [two].\n— Benjamin Franklin\nIt has been a decade since the publication of the second edition of this book. \nDuring that time, the field of software architecture has broadened its focus \nfrom being primarily internally oriented—How does one design, evaluate, \nand document software?—to including external impacts as well—a deeper \nunderstanding of the influences on architectures and a deeper understanding of \nthe impact architectures have on the life cycle, organizations, and management.\nThe past ten years have also seen dramatic changes in the types of systems \nbeing constructed. Large data, social media, and the cloud are all areas that, at \nmost, were embryonic ten years ago and now are not only mature but extremely \ninfluential.\nWe listened to some of the criticisms of the previous editions and have \nincluded much more material on patterns, reorganized the material on quality \nattributes, and made interoperability a quality attribute worthy of its own chapter. \nWe also provide guidance about how you can generate scenarios and tactics for \nyour own favorite quality attributes.\nTo accommodate this plethora of new material, we had to make difficult \nchoices. In particular, this edition of the book does not include extended \ncase studies as the prior editions did. This decision also reflects the maturing \nof the field, in the sense that case studies about the choices made in software \narchitectures are more prevalent than they were ten years ago, and they are less \nnecessary to convince readers of the importance of software architecture. The \ncase studies from the first two editions are available, however, on the book’s \nwebsite, at www.informit.com/title/9780321815736. In addition, on the same \nwebsite, we have slides that will assist instructors in presenting this material.\nWe have thoroughly reworked many of the topics covered in this edition. \nIn particular, we realize that the methods we present—for architecture design, \nanalysis, and documentation—are one version of how to achieve a particular \ngoal, but there are others. This led us to separate the methods that we present \n\n\nxvi \nPreface\t\nin detail from their underlying theory. We now present the theory first with \nspecific methods given as illustrations of possible realizations of the theories. \nThe new topics in this edition include architecture-centric project management; \narchitecture competence; requirements modeling and analysis; Agile methods; \nimplementation and testing; the cloud; and the edge.\nAs with the prior editions, we firmly believe that the topics are best discussed \nin either reading groups or in classroom settings, and to that end we have included \na collection of discussion questions at the end of each chapter. Most of these \nquestions are open-ended, with no absolute right or wrong answers, so you, as a \nreader, should emphasize how you justify your answer rather than just answer the \nquestion itself.\n\n\nxvii\nReader’s Guide\nWe have structured this book into five distinct portions. Part One introduces \narchitecture and the various contextual lenses through which it could be viewed. \nThese are the following:\n■\n■Technical. What technical role does the software architecture play in the \nsystem or systems of which it’s a part? \n■\n■Project. How does a software architecture relate to the other phases of a \nsoftware development life cycle?\n■\n■Business. How does the presence of a software architecture affect an \norganization’s business environment?\n■\n■Professional. What is the role of a software architect in an organization or a \ndevelopment project?\nPart Two is focused on technical background. Part Two describes how \ndecisions are made. Decisions are based on the desired quality attributes for a \nsystem, and Chapters 5–11 describe seven different quality attributes and the \ntechniques used to achieve them. The seven are availability, interoperability, \nmaintainability, performance, security, testability, and usability. Chapter 12 \ntells you how to add other quality attributes to our seven, Chapter 13 discusses \npatterns and tactics, and Chapter 14 discusses the various types of modeling and \nanalysis that are possible.\nPart Three is devoted to how a software architecture is related to the other \nportions of the life cycle. Of special note is how architecture can be used in Agile \nprojects. We discuss individually other aspects of the life cycle: requirements, \ndesign, implementation and testing, recovery and conformance, and evaluation.\nPart Four deals with the business of architecting from an economic \nperspective, from an organizational perspective, and from the perspective of \nconstructing a series of similar systems.\nPart Five discusses several important emerging technologies and how \narchitecture relates to these technologies.\n",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-26)",
      "start_page": 18,
      "end_page": 26,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\nxix\nAcknowledgments\nWe had a fantastic collection of reviewers for this edition, and their assistance \nhelped make this a better book. Our reviewers were Muhammad Ali Babar, Felix \nBachmann, Joe Batman, Phil Bianco, Jeromy Carriere, Roger Champagne, Steve \nChenoweth, Viktor Clerc, Andres Diaz Pace, George Fairbanks, Rik Farenhorst, \nIan Gorton, Greg Hartman, Rich Hilliard, James Ivers, John Klein, Philippe \nKruchten, Phil Laplante, George Leih, Grace Lewis, John McGregor, Tommi \nMikkonen, Linda Northrop, Ipek Ozkaya, Eltjo Poort, Eelco Rommes, Nick \nRozanski, Jungwoo Ryoo, James Scott, Antony Tang, Arjen Uittenbogaard, Hans \nvan Vliet, Hiroshi Wada, Rob Wojcik, Eoin Woods, and Liming Zhu.\nIn addition, we had significant contributions from Liming Zhu, Hong-\nMei Chen, Jungwoo Ryoo, Phil Laplante, James Scott, Grace Lewis, and Nick \nRozanski that helped give the book a richer flavor than one written by just the \nthree of us.\nThe issue of build efficiency in Chapter 12 came from Rolf Siegers and John \nMcDonald of Raytheon. John Klein and Eltjo Poort contributed the “abstract \nsystem clock” and “sandbox mode” tactics, respectively, for testability. The list \nof stakeholders in Chapter 3 is from Documenting Software Architectures: Views \nand Beyond, Second Edition. Some of the material in Chapter 28 was inspired by a \ntalk given by Anthony Lattanze called “Organizational Design Thinking” in 2011.\nJoe Batman was instrumental in the creation of the seven categories of design \ndecisions we describe in Chapter 4. In addition, the descriptions of the security \nview, communications view, and exception view in Chapter 18 are based on material \nthat Joe wrote while planning the documentation for a real system’s architecture. \nMuch of the new material on modifiability tactics was based on the work of Felix \nBachmann and Rod Nord. James Ivers helped us with the security tactics.\nBoth Paul Clements and Len Bass have taken new positions since the \nlast edition was published, and we thank their new respective managements \n(BigLever Software for Paul and NICTA for Len) for their willingness to support \nour work on this edition. We would also like to thank our (former) colleagues at \nthe Software Engineering Institute for multiple contributions to the evolution of \nthe ideas expressed in this edition.\nFinally, as always, we thank our editor at Addison-Wesley, Peter Gordon, \nfor providing guidance and support during the writing and production processes.\n\n\nThis page intentionally left blank \n\n\n1\nPart  O N E\nIntroduction\nWhat is a software architecture? What is it good for? How does it come to be? \nWhat effect does its existence have? These are the questions we answer in Part I.\nChapter 1 deals with a technical perspective on software architecture. We \ndefine it and relate it to system and enterprise architectures. We discuss how the \narchitecture can be represented in different views to emphasize different perspec-\ntives on the architecture. We define patterns and discuss what makes a “good” \narchitecture.\nIn Chapter 2, we discuss the uses of an architecture. You may be surprised \nthat we can find so many—ranging from a vehicle for communication among \nstakeholders to a blueprint for implementation, to the carrier of the system’s \nquality attributes. We also discuss how the architecture provides a reasoned basis \nfor schedules and how it provides the foundation for training new members on a \nteam.\nFinally, in Chapter 3, we discuss the various contexts in which a software ar-\nchitecture exists. It exists in a technical context, in a project life-cycle context, in \na business context, and in a professional context. Each of these contexts defines a \nrole for the software architecture to play, or an influence on it. These impacts and \ninfluences define the Architecture Influence Cycle.\n\n\nThis page intentionally left blank \n\n\n3\n1\nWhat Is Software \nArchitecture?\nGood judgment is usually the result of experience. \nAnd experience is frequently the result of bad \njudgment. But to learn from the experience of \nothers requires those who have the experience to \nshare the knowledge with those who follow.\n—Barry LePatner\nWriting (on our part) and reading (on your part) a book about software architec-\nture, which distills the experience of many people, presupposes that \n1.\t\nhaving a software architecture is important to the successful development \nof a software system and \n2.\t\nthere is a sufficient, and sufficiently generalizable, body of knowledge \nabout software architecture to fill up a book.\nOne purpose of this book is to convince you that both of these assumptions are \ntrue, and once you are convinced, give you a basic knowledge so that you can \napply it yourself.\nSoftware systems are constructed to satisfy organizations’ business goals. \nThe architecture is a bridge between those (often abstract) business goals and \nthe final (concrete) resulting system. While the path from abstract goals to con-\ncrete systems can be complex, the good news is that software architectures can be \ndesigned, analyzed, documented, and implemented using known techniques that \nwill support the achievement of these business and mission goals. The complex-\nity can be tamed, made tractable.\nThese, then, are the topics for this book: the design, analysis, documenta-\ntion, and implementation of architectures. We will also examine the influences, \nprincipally in the form of business goals and quality attributes, which inform \nthese activities. \n\n\n4 \nPart One  Introduction\t\n1—What Is Software Architecture?\nIn this chapter we will focus on architecture strictly from a software engineer-\ning point of view. That is, we will explore the value that a software architecture \nbrings to a development project. (Later chapters will take a business and organi-\nzational perspective.)\n1.1  What Software Architecture Is and What It Isn’t\nThere are many definitions of software architecture, easily discoverable with \na web search, but the one we like is this one:\nThe software architecture of a system is the set of structures needed to \nreason about the system, which comprise software elements, relations \namong them, and properties of both. \nThis definition stands in contrast to other definitions that talk about the sys-\ntem’s “early” or “major” design decisions. While it is true that many architectural \ndecisions are made early, not all are—especially in Agile or spiral-development \nprojects. It’s also true that very many decisions are made early that are not archi-\ntectural. Also, it’s hard to look at a decision and tell whether or not it’s “major.” \nSometimes only time will tell. And since writing down an architecture is one of \nthe architect’s most important obligations, we need to know now which decisions \nan architecture comprises.\nStructures, on the other hand, are fairly easy to identify in software, and they \nform a powerful tool for system design. \nLet us look at some of the implications of our definition. \nArchitecture Is a Set of Software Structures\nThis is the first and most obvious implication of our definition. A structure is sim-\nply a set of elements held together by a relation. Software systems are composed \nof many structures, and no single structure holds claim to being the architecture. \nThere are three categories of architectural structures, which will play an import-\nant role in the design, documentation, and analysis of architectures:\n1.\t\nFirst, some structures partition systems into implementation units, which \nin this book we call modules. Modules are assigned specific computational \nresponsibilities, and are the basis of work assignments for programming \nteams (Team A works on the database, Team B works on the business rules, \nTeam C works on the user interface, etc.). In large projects, these elements \n(modules) are subdivided for assignment to subteams. For example, the da-\ntabase for a large enterprise resource planning (ERP) implementation might \nbe so complex that its implementation is split into many parts. The structure \nthat captures that decomposition is a kind of module structure, the module \n\n\n1.1  What Software Architecture Is and What It Isn’t\n5\ndecomposition structure in fact. Another kind of module structure emerges \nas an output of object-oriented analysis and design—class diagrams. If you \naggregate your modules into layers, you’ve created another (and very use-\nful) module structure. Module structures are static structures, in that they \nfocus on the way the system’s functionality is divided up and assigned to \nimplementation teams. \n2.\t\nOther structures are dynamic, meaning that they focus on the way the el-\nements interact with each other at runtime to carry out the system’s func-\ntions. Suppose the system is to be built as a set of services. The services, \nthe infrastructure they interact with, and the synchronization and interaction \nrelations among them form another kind of structure often used to describe \na system. These services are made up of (compiled from) the programs in \nthe various implementation units that we just described. In this book we \nwill call runtime structures component-and-connector (C&C) structures. \nThe term component is overloaded in software engineering. In our use, a \ncomponent is always a runtime entity.\n3.\t\nA third kind of structure describes the mapping from software structures \nto the system’s organizational, developmental, installation, and execution \nenvironments. For example, modules are assigned to teams to develop, and \nassigned to places in a file structure for implementation, integration, and \ntesting. Components are deployed onto hardware in order to execute. These \nmappings are called allocation structures.\nAlthough software comprises an endless supply of structures, not all of them \nare architectural. For example, the set of lines of source code that contain the let-\nter “z,” ordered by increasing length from shortest to longest, is a software struc-\nture. But it’s not a very interesting one, nor is it architectural. A structure is archi-\ntectural if it supports reasoning about the system and the system’s properties. The \nreasoning should be about an attribute of the system that is important to some \nstakeholder. These include functionality achieved by the system, the availability \nof the system in the face of faults, the difficulty of making specific changes to the \nsystem, the responsiveness of the system to user requests, and many others. We \nwill spend a great deal of time in this book on the relationship between architec-\nture and quality attributes like these.\nThus, the set of architectural structures is not fixed or limited. What is archi-\ntectural is what is useful in your context for your system.\nArchitecture Is an Abstraction\nBecause architecture consists of structures and structures consist of elements1 \nand relations, it follows that an architecture comprises software elements and \n1.  In this book we use the term “element” when we mean either a module or a component, and don’t \nwant to distinguish.\n\n\n6 \nPart One  Introduction\t\n1—What Is Software Architecture?\nhow the elements relate to each other. This means that architecture specifically \nomits certain information about elements that is not useful for reasoning about \nthe system—in particular, it omits information that has no ramifications outside \nof a single element. Thus, an architecture is foremost an abstraction of a system \nthat selects certain details and suppresses others. In all modern systems, elements \ninteract with each other by means of interfaces that partition details about an el-\nement into public and private parts. Architecture is concerned with the public \nside of this division; private details of elements—details having to do solely with \ninternal implementation—are not architectural. Beyond just interfaces, though, \nthe architectural abstraction lets us look at the system in terms of its elements, \nhow they are arranged, how they interact, how they are composed, what their \nproperties are that support our system reasoning, and so forth. This abstraction \nis essential to taming the complexity of a system—we simply cannot, and do not \nwant to, deal with all of the complexity all of the time. \nEvery Software System Has a Software Architecture\nEvery system can be shown to comprise elements and relations among them to \nsupport some type of reasoning. In the most trivial case, a system is itself a single \nelement—an uninteresting and probably non-useful architecture, but an architec-\nture nevertheless. \nEven though every system has an architecture, it does not necessarily follow \nthat the architecture is known to anyone. Perhaps all of the people who designed \nthe system are long gone, the documentation has vanished (or was never pro-\nduced), the source code has been lost (or was never delivered), and all we have is \nthe executing binary code. This reveals the difference between the architecture of \na system and the representation of that architecture. Because an architecture can \nexist independently of its description or specification, this raises the importance \nof architecture documentation, which is described in Chapter 18, and architec-\nture reconstruction, discussed in Chapter 20.\nArchitecture Includes Behavior\nThe behavior of each element is part of the architecture insofar as that behavior \ncan be used to reason about the system. This behavior embodies how elements \ninteract with each other, which is clearly part of our definition of architecture. \nThis tells us that box-and-line drawings that are passed off as architectures \nare in fact not architectures at all. When looking at the names of the boxes (da-\ntabase, graphical user interface, executive, etc.), a reader may well imagine the \nfunctionality and behavior of the corresponding elements. This mental image \napproaches an architecture, but it springs from the imagination of the observ-\ner’s mind and relies on information that is not present. This does not mean that \nthe exact behavior and performance of every element must be documented in \nall circumstances—some aspects of behavior are fine-grained and below the \n",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 27-36)",
      "start_page": 27,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "1.1  What Software Architecture Is and What It Isn’t\n7\narchitect’s level of concern. But to the extent that an element’s behavior influ-\nences another element or influences the acceptability of the system as a whole, \nthis behavior must be considered, and should be documented, as part of the soft-\nware architecture. \nNot All Architectures Are Good Architectures\nThe definition is indifferent as to whether the architecture for a system is a good \none or a bad one. An architecture may permit or preclude a system’s achievement \nof its behavioral, quality attribute, and life-cycle requirements. Assuming that we \ndo not accept trial and error as the best way to choose an architecture for a sys-\ntem—that is, picking an architecture at random, building the system from it, and \nthen hacking away and hoping for the best—this raises the importance of archi-\ntecture design, which is treated in Chapter 17, and architecture evaluation, which \nwe deal with in Chapter 21.\nSystem and Enterprise Architectures\nTwo disciplines related to software architecture are system architecture \nand enterprise architecture. Both of these disciplines have broader con-\ncerns than software and affect software architecture through the estab-\nlishment of constraints within which a software system must live. In both \ncases, the software architect for a system should be on the team that pro-\nvides input into the decisions made about the system or the enterprise. \nSystem architecture\nA system’s architecture is a representation of a system in which there \nis a mapping of functionality onto hardware and software components, \na mapping of the software architecture onto the hardware architecture, \nand a concern for the human interaction with these components. That is, \nsystem architecture is concerned with a total system, including hardware, \nsoftware, and humans.\nA system architecture will determine, for example, the functionality that \nis assigned to different processors and the type of network that connects \nthose processors. The software architecture on each of those processors \nwill determine how this functionality is implemented and how the various \nprocessors interact through the exchange of messages on the network.\nA description of the software architecture, as it is mapped to hardware \nand networking components, allows reasoning about qualities such as per-\nformance and reliability. A description of the system architecture will allow \nreasoning about additional qualities such as power consumption, weight, \nand physical footprint.\nWhen a particular system is designed, there is frequently negotiation be-\ntween the system architect and the software architect as to the distribution \n\n\n8 \nPart One  Introduction\t\n1—What Is Software Architecture?\nof functionality and, consequently, the constraints placed on the software \narchitecture.\nEnterprise architecture\nEnterprise architecture is a description of the structure and behavior of an \norganization’s processes, information flow, personnel, and organizational \nsubunits, aligned with the organization’s core goals and strategic direction. \nAn enterprise architecture need not include information systems—clearly \norganizations had architectures that fit the preceding definition prior to the \nadvent of computers—but these days, enterprise architectures for all but the \nsmallest businesses are unthinkable without information system support. \nThus, a modern enterprise architecture is concerned with how an enter-\nprise’s software systems support the business processes and goals of the \nenterprise. Typically included in this set of concerns is a process for deciding \nwhich systems with which functionality should be supported by an enterprise.\nAn enterprise architecture will specify the data model that various sys-\ntems use to interact, for example. It will specify rules for how the enter-\nprise’s systems interact with external systems.\nSoftware is only one concern of enterprise architecture. Two other com-\nmon concerns addressed by enterprise architecture are how the software \nis used by humans to perform business processes, and the standards that \ndetermine the computational environment.\nSometimes the software infrastructure that supports communication \namong systems and with the external world is considered a portion of the \nenterprise architecture; other times, this infrastructure is considered one \nof the systems within an enterprise. (In either case, the architecture of that \ninfrastructure is a software architecture!) These two views will result in dif-\nferent management structures and spheres of influence for the individuals \nconcerned with the infrastructure.\nThe system and the enterprise provide environments for, and constraints \non, the software architecture. The software architecture must live within \nthe system and enterprise, and increasingly it is the focus for achieving the \norganization’s business goals. But all three forms of architecture share im-\nportant commonalities: They are concerned with major elements taken as \nabstractions, the relationships among the elements, and how the elements \ntogether meet the behavioral and quality goals of the thing being built.\nAre these in scope for this book? Yes! (Well, no.)\nSystem and enterprise architectures share a great deal with software ar-\nchitectures. All can be designed, evaluated, and documented; all answer \nto requirements; all are intended to satisfy stakeholders; all consist of \nstructures, which in turn consist of elements and relationships; all have a \nrepertoire of patterns and styles at their respective architects’ disposal; \nand the list goes on. So to the extent that these architectures share com-\nmonalities with software architecture, they are in the scope of this book. \nBut like all technical disciplines, each has its own specialized vocabulary \nand techniques, and we won’t cover those. Copious other sources do.\n\n\n1.2  Architectural Structures and Views\n9\n1.2  Architectural Structures and Views\nThe neurologist, the orthopedist, the hematologist, and the dermatologist all have \ndifferent views of the structure of a human body. Ophthalmologists, cardiolo-\ngists, and podiatrists concentrate on specific subsystems. And the kinesiologist \nand psychiatrist are concerned with different aspects of the entire arrangement’s \nbehavior. Although these views are pictured differently and have very different \nproperties, all are inherently related, interconnected: together they describe the \narchitecture of the human body. Figure 1.1 shows several different views of the \nhuman body: the skeletal, the vascular, and the X-ray.\nFIGURE 1.1  Physiological structures (Getty images: Brand X Pictures [skeleton], \nDon Farrall [woman], Mads Abildgaard [man])\nSo it is with software. Modern systems are frequently too complex to grasp \nall at once. Instead, we restrict our attention at any one moment to one (or a \nsmall number) of the software system’s structures. To communicate meaningfully \nabout an architecture, we must make clear which structure or structures we are \ndiscussing at the moment—which view we are taking of the architecture. \n\n\n10 \nPart One  Introduction\t\n1—What Is Software Architecture?\nStructures and Views\nWe will be using the related terms structure and view when discussing architec-\nture representation. \n■\n■A view is a representation of a coherent set of architectural elements, as \nwritten by and read by system stakeholders. It consists of a representation \nof a set of elements and the relations among them. \n■\n■A structure is the set of elements itself, as they exist in software or \nhardware.\nIn short, a view is a representation of a structure. For example, a module \nstructure is the set of the system’s modules and their organization. A module view \nis the representation of that structure, documented according to a template in a \nchosen notation, and used by some system stakeholders. \nSo: Architects design structures. They document views of those structures.\nThree Kinds of Structures\nAs we saw in the previous section, architectural structures can be divided into \nthree major categories, depending on the broad nature of the elements they show. \nThese correspond to the three broad kinds of decisions that architectural design \ninvolves: \n1.\t\nModule structures embody decisions as to how the system is to be struc-\ntured as a set of code or data units that have to be constructed or procured. \nIn any module structure, the elements are modules of some kind (perhaps \nclasses, or layers, or merely divisions of functionality, all of which are units \nof implementation). Modules represent a static way of considering the sys-\ntem. Modules are assigned areas of functional responsibility; there is less \nemphasis in these structures on how the resulting software manifests itself \nat runtime. Module structures allow us to answer questions such as these: \n■\n■What is the primary functional responsibility assigned to each module? \n■\n■What other software elements is a module allowed to use? \n■\n■What other software does it actually use and depend on? \n■\n■What modules are related to other modules by generalization or special-\nization (i.e., inheritance) relationships? \nModule structures convey this information directly, but they can also be \nused by extension to ask questions about the impact on the system when the \nresponsibilities assigned to each module change. In other words, examining \na system’s module structures—that is, looking at its module views—is an \nexcellent way to reason about a system’s modifiability. \n2.\t\nComponent-and-connector structures embody decisions as to how the \nsystem is to be structured as a set of elements that have runtime behav-\nior (components) and interactions (connectors). In these structures, the \n\n\n1.2  Architectural Structures and Views\n11\nelements are runtime components (which are the principal units of compu-\ntation and could be services, peers, clients, servers, filters, or many other \ntypes of runtime elements) and connectors (which are the communication \nvehicles among components, such as call-return, process synchronization \noperators, pipes, or others). Component-and-connector views help us an-\nswer questions such as these: \n■\n■What are the major executing components and how do they interact at \nruntime? \n■\n■What are the major shared data stores? \n■\n■Which parts of the system are replicated? \n■\n■How does data progress through the system? \n■\n■What parts of the system can run in parallel? \n■\n■Can the system’s structure change as it executes and, if so, how? \nBy extension, component-and-connector views are crucially important \nfor asking questions about the system’s runtime properties such as \nperformance, security, availability, and more.\n3.\t\nAllocation structures embody decisions as to how the system will relate \nto nonsoftware structures in its environment (such as CPUs, file systems, \nnetworks, development teams, etc.). These structures show the relationship \nbetween the software elements and elements in one or more external envi-\nronments in which the software is created and executed. Allocation views \nhelp us answer questions such as these: \n■\n■What processor does each software element execute on? \n■\n■In what directories or files is each element stored during development, \ntesting, and system building? \n■\n■What is the assignment of each software element to development teams?\nStructures Provide Insight\nStructures play such an important role in our perspective on software architec-\nture because of the analytical and engineering power they hold. Each structure \nprovides a perspective for reasoning about some of the relevant quality attributes. \nFor example:\n■\n■The module “uses” structure, which embodies what modules use what other \nmodules, is strongly tied to the ease with which a system can be extended \nor contracted. \n■\n■The concurrency structure, which embodies parallelism within the system, \nis strongly tied to the ease with which a system can be made free of \ndeadlock and performance bottlenecks. \n■\n■The deployment structure is strongly tied to the achievement of \nperformance, availability, and security goals. \n\n\n12 \nPart One  Introduction\t\n1—What Is Software Architecture?\nAnd so forth. Each structure provides the architect with a different insight \ninto the design (that is, each structure can be analyzed for its ability to deliver a \nquality attribute). But perhaps more important, each structure presents the archi-\ntect with an engineering leverage point: By designing the structures appropri-\nately, the desired quality attributes emerge.\nScenarios, described in Chapter 4, are useful for exercising a given structure \nas well as its connections to other structures. For example, a software engineer \nwanting to make a change to the concurrency structure of a system would need \nto consult the concurrency and deployment views, because the affected mecha-\nnisms typically involve processes and threads, and physical distribution might \ninvolve different control mechanisms than would be used if the processes were \nco-located on a single machine. If control mechanisms need to be changed, the \nmodule decomposition would need to be consulted to determine the extent of the \nchanges.\nSome Useful Module Structures\nUseful module structures include the following:\n■\n■Decomposition structure. The units are modules that are related to each \nother by the is-a-submodule-of relation, showing how modules are decom-\nposed into smaller modules recursively until the modules are small enough \nto be easily understood. Modules in this structure represent a common \nstarting point for design, as the architect enumerates what the units of \nsoftware will have to do and assigns each item to a module for subsequent \n(more detailed) design and eventual implementation. Modules often have \nproducts (such as interface specifications, code, test plans, etc.) associated \nwith them. The decomposition structure determines, to a large degree, the \nsystem’s modifiability, by assuring that likely changes are localized. That \nis, changes fall within the purview of at most a few (preferably small) mod-\nules. This structure is often used as the basis for the development project’s \norganization, including the structure of the documentation, and the project’s \nintegration and test plans. The units in this structure tend to have names that \nare organization-specific such as “segment” or “subsystem.”\n■\n■Uses structure. In this important but overlooked structure, the units here are \nalso modules, perhaps classes. The units are related by the uses relation, \na specialized form of dependency. A unit of software uses another if the \ncorrectness of the first requires the presence of a correctly functioning \nversion (as opposed to a stub) of the second. The uses structure is used to \nengineer systems that can be extended to add functionality, or from which \nuseful functional subsets can be extracted. The ability to easily create a \nsubset of a system allows for incremental development.\n\n\n1.2  Architectural Structures and Views\n13\n■\n■Layer structure. The modules in this structure are called layers. A layer \nis an abstract “virtual machine” that provides a cohesive set of services \nthrough a managed interface. Layers are allowed to use other layers in a \nstrictly managed fashion; in strictly layered systems, a layer is only allowed \nto use the layer immediately below. This structure is used to imbue a system \nwith portability, the ability to change the underlying computing platform. \n■\n■Class (or generalization) structure. The module units in this structure are \ncalled classes. The relation is inherits from or is an instance of. This view \nsupports reasoning about collections of similar behavior or capability (e.g., \nthe classes that other classes inherit from) and parameterized differences. \nThe class structure allows one to reason about reuse and the incremental \naddition of functionality. If any documentation exists for a project that has \nfollowed an object-oriented analysis and design process, it is typically this \nstructure.\n■\n■Data model. The data model describes the static information structure in \nterms of data entities and their relationships. For example, in a banking \nsystem, entities will typically include Account, Customer, and Loan. \nAccount has several attributes, such as account number, type (savings or \nchecking), status, and current balance. A relationship may dictate that one \ncustomer can have one or more accounts, and one account is associated to \none or two customers.\nSome Useful C&C Structures\nComponent-and-connector structures show a runtime view of the system. In these \nstructures the modules described above have all been compiled into executable \nforms. All component-and-connector structures are thus orthogonal to the mod-\nule-based structures and deal with the dynamic aspects of a running system. The \nrelation in all component-and-connector structures is attachment, showing how \nthe components and the connectors are hooked together. (The connectors them-\nselves can be familiar constructs such as “invokes.”) Useful C&C structures in-\nclude the following:\n■\n■Service structure. The units here are services that interoperate with each \nother by service coordination mechanisms such as SOAP (see Chapter 6). \nThe service structure is an important structure to help engineer a system \ncomposed of components that may have been developed anonymously and \nindependently of each other. \n■\n■Concurrency structure. This component-and-connector structure allows the \narchitect to determine opportunities for parallelism and the locations where \nresource contention may occur. The units are components and the connec-\ntors are their communication mechanisms. The components are arranged \ninto logical threads; a logical thread is a sequence of computations that \n\n\n14 \nPart One  Introduction\t\n1—What Is Software Architecture?\ncould be allocated to a separate physical thread later in the design process. \nThe concurrency structure is used early in the design process to identify the \nrequirements to manage the issues associated with concurrent execution.\nSome Useful Allocation Structures\nAllocation structures define how the elements from C&C or module structures \nmap onto things that are not software: typically hardware, teams, and file sys-\ntems. Useful allocation structures include these:\n■\n■Deployment structure. The deployment structure shows how software is \nassigned to hardware processing and communication elements. The ele-\nments are software elements (usually a process from a C&C view), hard-\nware entities (processors), and communication pathways. Relations are \nallocated-to, showing on which physical units the software elements reside, \nand migrates-to if the allocation is dynamic. This structure can be used to \nreason about performance, data integrity, security, and availability. It is of \nparticular interest in distributed and parallel systems. \n■\n■Implementation structure. This structure shows how software elements \n(usually modules) are mapped to the file structure(s) in the system’s devel-\nopment, integration, or configuration control environments. This is critical \nfor the management of development activities and build processes. (In prac-\ntice, a screenshot of your development environment tool, which manages \nthe implementation environment, often makes a very useful and sufficient \ndiagram of your implementation view.)\n■\n■Work assignment structure. This structure assigns responsibility for im-\nplementing and integrating the modules to the teams who will carry it out. \nHaving a work assignment structure be part of the architecture makes it \nclear that the decision about who does the work has architectural as well as \nmanagement implications. The architect will know the expertise required \non each team. Also, on large multi-sourced distributed development proj-\nects, the work assignment structure is the means for calling out units of \nfunctional commonality and assigning those to a single team, rather than \nhaving them implemented by everyone who needs them. This structure will \nalso determine the major communication pathways among the teams: regu-\nlar teleconferences, wikis, email lists, and so forth.\nTable 1.1 summarizes these structures. The table lists the meaning of the \nelements and relations in each structure and tells what each might be used for. \nRelating Structures to Each Other\nEach of these structures provides a different perspective and design handle on a \nsystem, and each is valid and useful in its own right. Although the structures give \n\n\n1.2  Architectural Structures and Views\n15\nTABLE 1.1  Useful Architectural Structures\nSoftware \nStructure\nElement  \nTypes\n \nRelations\n \nUseful For\nQuality Attributes \nAffected\nModule \nStructures\nDecomposition\nModule\nIs a submodule of\nResource allocation and project structuring and \nplanning; information hiding, encapsulation; \nconfiguration control\nModifiability\nUses\nModule\nUses (i.e., requires the correct \npresence of)\nEngineering subsets, engineering extensions\n“Subsetability,” \nextensibility\nLayers\nLayer\nRequires the correct presence \nof, uses the services of, \nprovides abstraction to\nIncremental development, implementing systems \non top of “virtual machines”\nPortability\nClass\nClass, object\nIs an instance of, shares access \nmethods of\nIn object-oriented design systems, factoring out \ncommonality; planning extensions of functionality\nModifiability, \nextensibility\nData model\nData entity\n{one, many}-to-{one, many}, \ngeneralizes, specializes\nEngineering global data structures for consistency \nand performance\nModifiability, \nperformance\nC&C \nStructures\nService\nService, ESB, registry, \nothers\nRuns concurrently with, may \nrun concurrently with, excludes, \nprecedes, etc.\nScheduling analysis, performance analysis\nInteroperability, \nmodifiability\nConcurrency\nProcesses, threads\nCan run in parallel\nIdentifying locations where resource contention \nexists, or where threads may fork, join, be created, \nor be killed\nPerformance, \navailability\nAllocation  \nStructures\nDeployment\nComponents, hardware \nelements\nAllocated to, migrates to\nPerformance, availability, security analysis\nPerformance, \nsecurity, availability\nImplementation\nModules, file structure\nStored in\nConfiguration control, integration, test activities\nDevelopment \nefficiency\nWork assignment Modules, organizational \nunits\nAssigned to\nProject management, best use of expertise and \navailable resources, management of commonality\nDevelopment \nefficiency \n\n\n16 \nPart One  Introduction\t\n1—What Is Software Architecture?\ndifferent system perspectives, they are not independent. Elements of one structure \nwill be related to elements of other structures, and we need to reason about these \nrelations. For example, a module in a decomposition structure may be manifested \nas one, part of one, or several components in one of the component-and-con-\nnector structures, reflecting its runtime alter ego. In general, mappings between \nstructures are many to many. \nFigure 1.2 shows a very simple example of how two structures might relate \nto each other. The figure on the left shows a module decomposition view of a \ntiny client-server system. In this system, two modules must be implemented: The \nclient software and the server software. The figure on the right shows a compo-\nnent-and-connector view of the same system. At runtime there are ten clients run-\nning and accessing the server. Thus, this little system has two modules and eleven \ncomponents (and ten connectors).\nWhereas the correspondence between the elements in the decomposition \nstructure and the client-server structure is obvious, these two views are used for \nvery different things. For example, the view on the right could be used for perfor-\nmance analysis, bottleneck prediction, and network traffic management, which \nwould be extremely difficult or impossible to do with the view on the left.\n(In Chapter 13 we’ll learn about the map-reduce pattern, in which copies \nof simple, identical functionality are distributed across hundreds or thousands \nof processing nodes—one module for the whole system, but one component per \nnode.) \nIndividual projects sometimes consider one structure dominant and cast \nother structures, when possible, in terms of the dominant structure. Often the \ndominant structure is the module decomposition structure. This is for a good \nClient\nServer\nModule\nSystem\nDecomposition View\nKey:\nClient-Server View\nKey:\nComponent\nRequest-Reply\nC7\nC8\nC2\nC3\nC1\nC4\nC6\nC9\nC10\nC5\nS1\nFIGURE 1.2  Two views of a client-server system\n",
      "page_number": 27
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-44)",
      "start_page": 37,
      "end_page": 44,
      "detection_method": "topic_boundary",
      "content": "1.2  Architectural Structures and Views\n17\nreason: it tends to spawn the project structure, because it mirrors the team struc-\nture of development. In other projects, the dominant structure might be a C&C \nstructure that shows how the system’s functionality and/or critical quality attri-\nbutes are achieved.\nFewer Is Better\nNot all systems warrant consideration of many architectural structures. The larger \nthe system, the more dramatic the difference between these structures tends to be; \nbut for small systems we can often get by with fewer. Instead of working with \neach of several component-and-connector structures, usually a single one will do. \nIf there is only one process, then the process structure collapses to a single node \nand need not be explicitly represented in the design. If there is to be no distribu-\ntion (that is, if the system is implemented on a single processor), then the deploy-\nment structure is trivial and need not be considered further. In general, design and \ndocument a structure only if doing so brings a positive return on the investment, \nusually in terms of decreased development or maintenance costs.\nWhich Structures to Choose?\nWe have briefly described a number of useful architectural structures, and there \nare many more. Which ones shall an architect choose to work on? Which ones \nshall the architect choose to document? Surely not all of them. Chapter 18 will \ntreat this topic in more depth, but for now a good answer is that you should think \nabout how the various structures available to you provide insight and leverage \ninto the system’s most important quality attributes, and then choose the ones that \nwill play the best role in delivering those attributes. \nAsk Cal\nMore than a decade ago I went to a customer site to do an architecture \nevaluation—one of the first instances of the Architecture Tradeoff Analysis \nMethod (ATAM) that I had ever performed (you can read about the ATAM, \nand other architecture evaluation topics, in Chapter 21). In those early \ndays, we were still figuring out how to make architecture evaluations re-\npeatable and predictable, and how to guarantee useful outcomes from \nthem. One of the ways that we ensured useful outcomes was to enforce \ncertain preconditions on the evaluation. A precondition that we figured out \nrather quickly was this: if the architecture has not been documented, we \nwill not proceed with the evaluation. The reason for this precondition was \nsimple: we could not evaluate the architecture by reading the code—we \ndidn’t have the time for that—and we couldn’t just ask the architect to \n\n\n18 \nPart One  Introduction\t\n1—What Is Software Architecture?\nsketch the architecture in real time, since that would produce vague and \nvery likely erroneous representations.\nOkay, it’s not completely true to say that they had no architecture docu-\nmentation. They did produce a single-page diagram, with a few boxes and \nlines. Some of those boxes were, however, clouds. Yes, they actually used \na cloud as one of their icons. When I pressed them on the meaning of this \nicon—Was it a process? A class? A thread?—they waffled. This was not, in \nfact, architecture documentation. It was, at best, “marketecture.”\nBut in those early days we had no preconditions and so we didn’t stop \nthe evaluation there. We just blithely waded in to whatever swamp we \nfound, and we enforced nothing. As I began this evaluation, I interviewed \nsome of the key project stakeholders: the project manager and several of \nthe architects (this was a large project with one lead architect and several \nsubordinates). As it happens, the lead architect was away, and so I spent \nmy time with the subordinate architects. Every time I asked the subor-\ndinates a tough question—“How do you ensure that you will meet your \nlatency goal along this critical execution path?” or “What are your rules for \nlayering?”—they would answer: “Ask Cal. Cal knows that.” Cal was the lead \narchitect. Immediately I noted a risk for this system: What if Cal gets hit by \na bus? What then?\nIn the end, because of my pestering, the architecture team did in fact \nproduce respectable architecture documentation. About halfway through \nthe evaluation, the project manager came up to me and shook my hand \nand thanked me for the great job I had done. I was dumbstruck. In my \nmind I hadn’t done anything, at that point; the evaluation was only partially \ncomplete and I hadn’t produced a single report or finding. I said that to the \nmanager and he said: “You got those guys to document the architecture. \nI’ve never been able to get them to do that. So . . . thanks!”\nIf Cal had been hit by a bus or just left the company, they would have \nhad a serious problem on their hands: all of that architectural knowledge \nlocated in one guy’s head and he is no longer with the organization. In can \nhappen. It does happen.\nThe moral of this story? An architecture that is not documented, and not \ncommunicated, may still be a good architecture, but the risks surrounding it \nare enormous.\n—RK\n1.3  Architectural Patterns\nIn some cases, architectural elements are composed in ways that solve particular \nproblems. The compositions have been found useful over time, and over many \ndifferent domains, and so they have been documented and disseminated. These \ncompositions of architectural elements, called architectural patterns, provide \npackaged strategies for solving some of the problems facing a system.\n\n\n1.4  What Makes a “Good” Architecture?\n19\nAn architectural pattern delineates the element types and their forms of in-\nteraction used in solving the problem. Patterns can be characterized according to \nthe type of architectural elements they use. For example, a common module type \npattern is this:\n■\n■Layered pattern. When the uses relation among software elements is \nstrictly unidirectional, a system of layers emerges. A layer is a coherent \nset of related functionality. In a strictly layered structure, a layer can only \nuse the services of the layer immediately below it. Many variations of this \npattern, lessening the structural restriction, occur in practice. Layers are \noften designed as abstractions (virtual machines) that hide implementation \nspecifics below from the layers above, engendering portability. \nCommon component-and-connector type patterns are these:\n■\n■Shared-data (or repository) pattern. This pattern comprises components \nand connectors that create, store, and access persistent data. The repository \nusually takes the form of a (commercial) database. The connectors are \nprotocols for managing the data, such as SQL. \n■\n■Client-server pattern. The components are the clients and the servers, and \nthe connectors are protocols and messages they share among each other to \ncarry out the system’s work. \nCommon allocation patterns include the following:\n■\n■Multi-tier pattern, which describes how to distribute and allocate the \ncomponents of a system in distinct subsets of hardware and software, \nconnected by some communication medium. This pattern specializes the \ngeneric deployment (software-to-hardware allocation) structure.\n■\n■Competence center and platform, which are patterns that specialize a \nsoftware system’s work assignment structure. In competence center, work \nis allocated to sites depending on the technical or domain expertise located \nat a site. For example, user-interface design is done at a site where usability \nengineering experts are located. In platform, one site is tasked with \ndeveloping reusable core assets of a software product line (see Chapter 25), \nand other sites develop applications that use the core assets.\nArchitectural patterns will be investigated much further in Chapter 13. \n1.4  What Makes a “Good” Architecture?\nThere is no such thing as an inherently good or bad architecture. Architectures \nare either more or less fit for some purpose. A three-tier layered service-oriented \narchitecture may be just the ticket for a large enterprise’s web-based B2B system \n\n\n20 \nPart One  Introduction\t\n1—What Is Software Architecture?\nbut completely wrong for an avionics application. An architecture carefully \ncrafted to achieve high modifiability does not make sense for a throwaway proto-\ntype (and vice versa!). One of the messages of this book is that architectures can \nin fact be evaluated—one of the great benefits of paying attention to them—but \nonly in the context of specific stated goals. \nNevertheless, there are rules of thumb that should be followed when design-\ning most architectures. Failure to apply any of these does not automatically mean \nthat the architecture will be fatally flawed, but it should at least serve as a warn-\ning sign that should be investigated.\nWe divide our observations into two clusters: process recommendations and \nproduct (or structural) recommendations. Our process recommendations are the \nfollowing:\n1.\t\nThe architecture should be the product of a single architect or a small \ngroup of architects with an identified technical leader. This approach \ngives the architecture its conceptual integrity and technical consistency. \nThis recommendation holds for Agile and open source projects as well \nas “traditional” ones. There should be a strong connection between the \narchitect(s) and the development team, to avoid ivory tower designs that are \nimpractical.\n2.\t\nThe architect (or architecture team) should, on an ongoing basis, base the \narchitecture on a prioritized list of well-specified quality attribute require-\nments. These will inform the tradeoffs that always occur. Functionality mat-\nters less.\n3.\t\nThe architecture should be documented using views. The views should \naddress the concerns of the most important stakeholders in support of the \nproject timeline. This might mean minimal documentation at first, elaborat-\ned later. Concerns usually are related to construction, analysis, and main-\ntenance of the system, as well as education of new stakeholders about the \nsystem. \n4.\t\nThe architecture should be evaluated for its ability to deliver the system’s \nimportant quality attributes. This should occur early in the life cycle, when \nit returns the most benefit, and repeated as appropriate, to ensure that \nchanges to the architecture (or the environment for which it is intended) \nhave not rendered the design obsolete. \n5.\t\nThe architecture should lend itself to incremental implementation, to avoid \nhaving to integrate everything at once (which almost never works) as well \nas to discover problems early. One way to do this is to create a “skeletal” \nsystem in which the communication paths are exercised but which at first \nhas minimal functionality. This skeletal system can be used to “grow” the \nsystem incrementally, refactoring as necessary.\nOur structural rules of thumb are as follows:\n1.\t\nThe architecture should feature well-defined modules whose functional \nresponsibilities are assigned on the principles of information hiding and \n\n\n1.5  Summary\n21\nseparation of concerns. The information-hiding modules should encapsulate \nthings likely to change, thus insulating the software from the effects of \nthose changes. Each module should have a well-defined interface that \nencapsulates or “hides” the changeable aspects from other software \nthat uses its facilities. These interfaces should allow their respective \ndevelopment teams to work largely independently of each other. \n2.\t\nUnless your requirements are unprecedented—possible, but unlikely—your \nquality attributes should be achieved using well-known architectural pat-\nterns and tactics (described in Chapter 13) specific to each attribute. \n3.\t\nThe architecture should never depend on a particular version of a commer-\ncial product or tool. If it must, it should be structured so that changing to a \ndifferent version is straightforward and inexpensive. \n4.\t\nModules that produce data should be separate from modules that consume \ndata. This tends to increase modifiability because changes are frequently \nconfined to either the production or the consumption side of data. If new \ndata is added, both sides will have to change, but the separation allows for a \nstaged (incremental) upgrade. \n5.\t\nDon’t expect a one-to-one correspondence between modules and compo-\nnents. For example, in systems with concurrency, there may be multiple in-\nstances of a component running in parallel, where each component is built \nfrom the same module. For systems with multiple threads of concurrency, \neach thread may use services from several components, each of which was \nbuilt from a different module.\n6.\t\nEvery process should be written so that its assignment to a specific proces-\nsor can be easily changed, perhaps even at runtime. \n7.\t\nThe architecture should feature a small number of ways for components \nto interact. That is, the system should do the same things in the same way \nthroughout. This will aid in understandability, reduce development time, \nincrease reliability, and enhance modifiability. \n8.\t\nThe architecture should contain a specific (and small) set of resource con-\ntention areas, the resolution of which is clearly specified and maintained. \nFor example, if network utilization is an area of concern, the architect \nshould produce (and enforce) for each development team guidelines that \nwill result in a minimum of network traffic. If performance is a concern, the \narchitect should produce (and enforce) time budgets for the major threads. \n1.5  Summary\nThe software architecture of a system is the set of structures needed to reason \nabout the system, which comprise software elements, relations among them, and \nproperties of both.\n\n\n22 \nPart One  Introduction\t\n1—What Is Software Architecture?\nA structure is a set of elements and the relations among them.\nA view is a representation of a coherent set of architectural elements, as \nwritten by and read by system stakeholders. A view is a representation of one or \nmore structures. \nThere are three categories of structures:\n■\n■Module structures show how a system is to be structured as a set of code or \ndata units that have to be constructed or procured. \n■\n■Component-and-connector structures show how the system is to be \nstructured as a set of elements that have runtime behavior (components) and \ninteractions (connectors). \n■\n■Allocation structures show how the system will relate to nonsoftware \nstructures in its environment (such as CPUs, file systems, networks, \ndevelopment teams, etc.). \nStructures represent the primary engineering leverage points of an architec-\nture. Each structure brings with it the power to manipulate one or more quality \nattributes. They represent a powerful approach for creating the architecture (and \nlater, for analyzing it and explaining it to its stakeholders). And as we will see \nin Chapter 18, the structures that the architect has chosen as engineering lever-\nage points are also the primary candidates to choose as the basis for architecture \ndocumentation. \nEvery system has a software architecture, but this architecture may be docu-\nmented and disseminated, or it may not be.\nThere is no such thing as an inherently good or bad architecture. Architec-\ntures are either more or less fit for some purpose. \n1.6  For Further Reading\nThe early work of David Parnas laid much of the conceptual foundation for what \nbecame the study of software architecture. A quintessential Parnas reader would \ninclude his foundational article on information hiding [Parnas 72] as well as his \nworks on program families [Parnas 76], the structures inherent in software sys-\ntems [Parnas 74], and introduction of the uses structure to build subsets and sup-\nersets of systems [Parnas 79]. All of these papers can be found in the more easily \naccessible collection of his important papers [Hoffman 00].\nAn early paper by Perry and Wolf [Perry 92] drew an analogy between soft-\nware architecture views and structures and the structures one finds in a house \n(plumbing, electrical, and so forth). \nSoftware architectural patterns have been extensively catalogued in the se-\nries Pattern-Oriented Software Architecture [Buschmann 96] and others. Chapter \n13 of this book also deals with architectural patterns.\n\n\n1.7  Discussion Questions\n23\nEarly papers on architectural views as used in industrial development proj-\nects are [Soni 95] and [Kruchten 95]. The former grew into a book [Hofmeister \n00] that presents a comprehensive picture of using views in development and \nanalysis. The latter grew into the Rational Unified Process, about which there is \nno shortage of references, both paper and online. A good one is [Kruchten 03].\nCristina Gacek and her colleagues discuss the process issues surrounding \nsoftware architecture in [Gacek 95].\nGarlan and Shaw’s seminal work on software architecture [Garlan 93] \nprovides many excellent examples of architectural styles (a concept similar to \npatterns).\nIn [Clements 10a] you can find an extended discussion on the difference be-\ntween an architectural pattern and an architectural style. (It argues that a pattern \nis a context-problem-solution triple; a style is simply a condensation that focuses \nmost heavily on the solution part.)\nSee [Taylor 09] for a definition of software architecture based on decisions \nrather than on structure.\n1.7  Discussion Questions\n1.\t\nSoftware architecture is often compared to the architecture of buildings as a \nconceptual analogy. What are the strong points of that analogy? What is the \ncorrespondence in buildings to software architecture structures and views? \nTo patterns? What are the weaknesses of the analogy? When does it break \ndown?\n2.\t\nDo the architectures you’ve been exposed to document different structures \nand relations like those described in this chapter? If so, which ones? If not, \nwhy not? \n3.\t\nIs there a different definition of software architecture that you are familiar \nwith? If so, compare and contrast it with the definition given in this chapter. \nMany definitions include considerations like “rationale” (stating the reasons \nwhy the architecture is what it is) or how the architecture will evolve over \ntime. Do you agree or disagree that these considerations should be part of \nthe definition of software architecture?\n4.\t\nDiscuss how an architecture serves as a basis for analysis. What about \ndecision-making? What kinds of decision-making does an architecture \nempower? \n5.\t\nWhat is architecture’s role in project risk reduction?\n\n\n24 \nPart One  Introduction\t\n1—What Is Software Architecture?\n6.\t\nFind a commonly accepted definition of system architecture and discuss \nwhat it has in common with software architecture. Do the same for enter-\nprise architecture.\n7.\t\nFind a published example of an architecture. What structure or structures \nare shown? Given its purpose, what structure or structures should have \nbeen shown? What analysis does the architecture support? Critique it: What \nquestions do you have that the representation does not answer?\n8.\t\nSailing ships have architectures, which means they have “structures” that \nlend themselves to reasoning about the ship’s performance and other qual-\nity attributes. Look up the technical definitions for barque, brig, cutter, \nfrigate, ketch, schooner, and sloop. Propose a useful set of “structures” for \ndistinguishing and reasoning about ship architectures.\n",
      "page_number": 37
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 45-53)",
      "start_page": 45,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "25\n2\nWhy Is Software \nArchitecture Important?\nSoftware architecture is the set of design \ndecisions which, if made incorrectly, may \ncause your project to be cancelled.\n—Eoin Woods\nIf architecture is the answer, what was the question?\nWhile Chapter 3 will cover the business importance of architecture to an \nenterprise, this chapter focuses on why architecture matters from a technical per-\nspective. We will examine a baker’s dozen of the most important reasons.\n1.\t An architecture will inhibit or enable a system’s driving quality attributes.\n2.\t The decisions made in an architecture allow you to reason about and man-\nage change as the system evolves.\n3.\t The analysis of an architecture enables early prediction of a system’s \nqualities.\n4.\t A documented architecture enhances communication among stakeholders.\n5.\t The architecture is a carrier of the earliest and hence most fundamental, \nhardest-to-change design decisions.\n6.\t An architecture defines a set of constraints on subsequent implementation.\n7.\t The architecture dictates the structure of an organization, or vice versa.\n8.\t An architecture can provide the basis for evolutionary prototyping.\n9.\t An architecture is the key artifact that allows the architect and project man-\nager to reason about cost and schedule.\n10.\t An architecture can be created as a transferable, reusable model that forms \nthe heart of a product line.\n11.\t Architecture-based development focuses attention on the assembly of com-\nponents, rather than simply on their creation.\n12.\t By restricting design alternatives, architecture channels the creativity of \ndevelopers, reducing design and system complexity.\n13.\t An architecture can be the foundation for training a new team member.\n\n\n26 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nEven if you already believe us that architecture is important and don’t need the \npoint hammered thirteen more times, think of these thirteen points (which form \nthe outline for this chapter) as thirteen useful ways to use architecture in a project.\n2.1  \u0007Inhibiting or Enabling a System’s Quality \nAttributes\nWhether a system will be able to exhibit its desired (or required) quality attri-\nbutes is substantially determined by its architecture. \nThis is such an important message that we’ve devoted all of Part 2 of this \nbook to expounding that message in detail. Until then, keep these examples in \nmind as a starting point:\n■\n■If your system requires high performance, then you need to pay attention \nto managing the time-based behavior of elements, their use of shared \nresources, and the frequency and volume of inter-element communication.\n■\n■If modifiability is important, then you need to pay careful attention to \nassigning responsibilities to elements so that the majority of changes to the \nsystem will affect a small number of those elements. (Ideally each change \nwill affect just a single element.)\n■\n■If your system must be highly secure, then you need to manage and protect \ninter-element communication and control which elements are allowed to \naccess which information; you may also need to introduce specialized \nelements (such as an authorization mechanism) into the architecture.\n■\n■If you believe that scalability will be important to the success of your \nsystem, then you need to carefully localize the use of resources to facilitate \nintroduction of higher-capacity replacements, and you must avoid hard-\ncoding in resource assumptions or limits.\n■\n■If your projects need the ability to deliver incremental subsets of the \nsystem, then you must carefully manage intercomponent usage.\n■\n■If you want the elements from your system to be reusable in other systems, \nthen you need to restrict inter-element coupling, so that when you extract \nan element, it does not come out with too many attachments to its current \nenvironment to be useful.\nThe strategies for these and other quality attributes are supremely architectural. \nBut an architecture alone cannot guarantee the functionality or quality required of \na system. Poor downstream design or implementation decisions can always under-\nmine an adequate architectural design. As we like to say (mostly in jest): The archi-\ntecture giveth and the implementation taketh away. Decisions at all stages of the \nlife cycle—from architectural design to coding and implementation—affect system \nquality. Therefore, quality is not completely a function of an architectural design. \n\n\n2.2  Reasoning About and Managing Change\n27\nA good architecture is necessary, but not sufficient, to ensure quality. Achiev-\ning quality attributes must be considered throughout design, implementation, and \ndeployment. No quality attribute is entirely dependent on design, nor is it entirely \ndependent on implementation or deployment. Satisfactory results are a matter of \ngetting the big picture (architecture) as well as the details (implementation) correct. \nFor example, modifiability is determined by how functionality is divided \nand coupled (architectural) and by coding techniques within a module (nonar-\nchitectural). Thus, a system is typically modifiable if changes involve the fewest \npossible number of distinct elements. In spite of having the ideal architecture, \nhowever, it is always possible to make a system difficult to modify by writing \nobscure, tangled code. \n2.2  Reasoning About and Managing Change\nThis point is a corollary to the previous point.\nModifiability—the ease with which changes can be made to a system—is \na quality attribute (and hence covered by the arguments in the previous section), \nbut it is such an important quality that we have awarded it its own spot in the List \nof Thirteen. The software development community is coming to grips with the \nfact that roughly 80 percent of a typical software system’s total cost occurs after \ninitial deployment. A corollary of this statistic is that most systems that people \nwork on are in this phase. Many programmers and software designers never get \nto work on new development; they work under the constraints of the existing \narchitecture and the existing body of code. Virtually all software systems change \nover their lifetime, to accommodate new features, to adapt to new environments, \nto fix bugs, and so forth. But these changes are often fraught with difficulty. \nEvery architecture partitions possible changes into three categories: local, \nnonlocal, and architectural. \n■\n■A local change can be accomplished by modifying a single element. For \nexample, adding a new business rule to a pricing logic module. \n■\n■A nonlocal change requires multiple element modifications but leaves \nthe underlying architectural approach intact. For example, adding a new \nbusiness rule to a pricing logic module, then adding new fields to the \ndatabase that this new business rule requires, and then revealing the results \nof the rule in the user interface. \n■\n■An architectural change affects the fundamental ways in which the \nelements interact with each other and will probably require changes all \nover the system. For example, changing a system from client-server to \npeer-to-peer. \n\n\n28 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nObviously, local changes are the most desirable, and so an effective architec-\nture is one in which the most common changes are local, and hence easy to make. \nDeciding when changes are essential, determining which change paths have \nthe least risk, assessing the consequences of proposed changes, and arbitrating \nsequences and priorities for requested changes all require broad insight into rela-\ntionships, performance, and behaviors of system software elements. These activ-\nities are in the job description for an architect. Reasoning about the architecture \nand analyzing the architecture can provide the insight necessary to make deci-\nsions about anticipated changes. \n2.3  Predicting System Qualities \nThis point follows from the previous two. Architecture not only imbues systems \nwith qualities, but it does so in a predictable way. \nWere it not possible to tell that the appropriate architectural decisions have \nbeen made (i.e., if the system will exhibit its required quality attributes) without \nwaiting until the system is developed and deployed, then choosing an architec-\nture would be a hopeless task—randomly making architecture selections would \nperform as well as any other method. Fortunately, it is possible to make quality \npredictions about a system based solely on an evaluation of its architecture. If we \nknow that certain kinds of architectural decisions lead to certain quality attributes \nin a system, then we can make those decisions and rightly expect to be rewarded \nwith the associated quality attributes. After the fact, when we examine an archi-\ntecture, we can look to see if those decisions have been made, and confidently \npredict that the architecture will exhibit the associated qualities.\nThis is no different from any mature engineering discipline, where design \nanalysis is a standard part of the development process. The earlier you can find \na problem in your design, the cheaper, easier, and less disruptive it will be to fix.\nEven if you don't do the quantitative analytic modeling sometimes necessary \nto ensure that an architecture will deliver its prescribed benefits, this principle of \nevaluating decisions based on their quality attribute implications is invaluable for \nat least spotting potential trouble spots early.\nThe architecture modeling and analysis techniques described in Chap-\nter 14, as well as the architecture evaluation techniques covered in Chapter 21, \nallow early insight into the software product qualities made possible by software \narchitectures.\n\n\n2.4  Enhancing Communication among Stakeholders\n29\n2.4  Enhancing Communication among Stakeholders\nSoftware architecture represents a common abstraction of a system that most, \nif not all, of the system’s stakeholders can use as a basis for creating mutual under-\nstanding, negotiating, forming consensus, and communicating with each other. The \narchitecture—or at least parts of it—is sufficiently abstract that most nontechnical \npeople can understand it adequately, particularly with some coaching from the archi-\ntect, and yet that abstraction can be refined into sufficiently rich technical specifica-\ntions to guide implementation, integration, test, and deployment.\nEach stakeholder of a software system—customer, user, project manager, \ncoder, tester, and so on—is concerned with different characteristics of the system \nthat are affected by its architecture. For example:\n■\n■The user is concerned that the system is fast, reliable, and available when \nneeded. \n■\n■The customer is concerned that the architecture can be implemented on \nschedule and according to budget.\n■\n■The manager is worried (in addition to concerns about cost and schedule) \nthat the architecture will allow teams to work largely independently, \ninteracting in disciplined and controlled ways.\n■\n■The architect is worried about strategies to achieve all of those goals. \nArchitecture provides a common language in which different concerns can \nbe expressed, negotiated, and resolved at a level that is intellectually manageable \neven for large, complex systems. Without such a language, it is difficult to under-\nstand large systems sufficiently to make the early decisions that influence both \nquality and usefulness. Architectural analysis, as we will see in Chapter 21, both \ndepends on this level of communication and enhances it.\nSection 3.5 covers stakeholders and their concerns in greater depth.\n“What Happens When I Push This Button?” Architecture as a \nVehicle for Stakeholder Communication\nThe project review droned on and on. The government-sponsored devel-\nopment was behind schedule and over budget and was large enough that \nthese lapses were attracting congressional attention. And now the govern-\nment was making up for past neglect by holding a marathon come-one-\ncome-all review session. The contractor had recently undergone a buyout, \nwhich hadn’t helped matters. It was the afternoon of the second day, and \nthe agenda called for the software architecture to be presented. The young \narchitect—an apprentice to the chief architect for the system—was bravely \nexplaining how the software architecture for the massive system would \nenable it to meet its very demanding real-time, distributed, high-reliability \n\n\n30 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nrequirements. He had a solid presentation and a solid architecture to pres-\nent. It was sound and sensible. But the audience—about 30 government \nrepresentatives who had varying roles in the management and oversight of \nthis sticky project—was tired. Some of them were even thinking that per-\nhaps they should have gone into real estate instead of enduring another one \nof these marathon let’s-finally-get-it-right-this-time reviews. \nThe viewgraph showed, in semiformal box-and-line notation, what the \nmajor software elements were in a runtime view of the system. The names \nwere all acronyms, suggesting no semantic meaning without explanation, \nwhich the young architect gave. The lines showed data flow, message \npassing, and process synchronization. The elements were internally re-\ndundant, the architect was explaining. “In the event of a failure,” he began, \nusing a laser pointer to denote one of the lines, “a restart mechanism \ntriggers along this path when—”\n“What happens when the mode select button is pushed?” interrupted \none of the audience members. He was a government attendee represent-\ning the user community for this system.\n“Beg your pardon?” asked the architect. \n“The mode select button,” he said. “What happens when you push it?”\n“Um, that triggers an event in the device driver, up here,” began the \narchitect, laser-pointing. “It then reads the register and interprets the event \ncode. If it’s mode select, well, then, it signals the blackboard, which in turns \nsignals the objects that have subscribed to that event. . . . ”\n“No, I mean what does the system do,” interrupted the questioner. “Does \nit reset the displays? And what happens if this occurs during a system \nreconfiguration?”\nThe architect looked a little surprised and flicked off the laser pointer. \nThis was not an architectural question, but since he was an architect and \ntherefore fluent in the requirements, he knew the answer. “If the command \nline is in setup mode, the displays will reset,” he said. “Otherwise an error \nmessage will be put on the control console, but the signal will be ignored.” \nHe put the laser pointer back on. “Now, the restart mechanism that I was \ntalking about—”\n“Well, I was just wondering,” said the users’ delegate. “Because I see \nfrom your chart that the display console is sending signal traffic to the \ntarget location module.”\n“What should happen?” asked another member of the audience, \naddressing the first questioner. “Do you really want the user to get mode \ndata during its reconfiguring?” And for the next 45 minutes, the architect \nwatched as the audience consumed his time slot by debating what the cor-\nrect behavior of the system was supposed to be in various esoteric states. \nThe debate was not architectural, but the architecture (and the graphical \nrendition of it) had sparked debate. It is natural to think of architecture as \nthe basis for communication among some of the stakeholders besides the \narchitects and developers: Managers, for example, use the architecture to \ncreate teams and allocate resources among them. But users? The architec-\nture is invisible to users, after all; why should they latch on to it as a tool for \nunderstanding the system?\n\n\n2.5  Carrying Early Design Decisions\n31\nThe fact is that they do. In this case, the questioner had sat through two \ndays of viewgraphs all about function, operation, user interface, and testing. \nBut it was the first slide on architecture that—even though he was tired and \nwanted to go home—made him realize he didn’t understand something. \nAttendance at many architecture reviews has convinced me that seeing \nthe system in a new way prods the mind and brings new questions to the \nsurface. For users, architecture often serves as that new way, and the \nquestions that a user poses will be behavioral in nature. In a memorable \narchitecture evaluation exercise a few years ago, the user representatives \nwere much more interested in what the system was going to do than in how \nit was going to do it, and naturally so. Up until that point, their only contact \nwith the vendor had been through its marketers. The architect was the first \nlegitimate expert on the system to whom they had access, and they didn’t \nhesitate to seize the moment.\nOf course, careful and thorough requirements specifications would ame-\nliorate this situation, but for a variety of reasons they are not always created \nor available. In their absence, a specification of the architecture often \nserves to trigger questions and improve clarity. It is probably more prudent \nto recognize this reality than to resist it. \nSometimes such an exercise will reveal unreasonable requirements, \nwhose utility can then be revisited. A review of this type that emphasizes \nsynergy between requirements and architecture would have let the young \narchitect in our story off the hook by giving him a place in the overall review \nsession to address that kind of information. And the user representative \nwouldn’t have felt like a fish out of water, asking his question at a clearly \ninappropriate moment. \n—PCC\n2.5  Carrying Early Design Decisions\nSoftware architecture is a manifestation of the earliest design decisions about a \nsystem, and these early bindings carry enormous weight with respect to the sys-\ntem’s remaining development, its deployment, and its maintenance life. It is also \nthe earliest point at which these important design decisions affecting the system \ncan be scrutinized.\nAny design, in any discipline, can be viewed as a set of decisions. When \npainting a picture, an artist decides on the material for the canvas, on the media \nfor recording—oil paint, watercolor, crayon—even before the picture is begun. \nOnce the picture is begun, other decisions are immediately made: Where is the \nfirst line? What is its thickness? What is its shape? All of these early design de-\ncisions have a strong influence on the final appearance of the picture. Each deci-\nsion constrains the many decisions that follow. Each decision, in isolation, might \nappear innocent enough, but the early ones in particular have disproportionate \nweight simply because they influence and constrain so much of what follows.\n\n\n32 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nSo it is with architecture design. An architecture design can also be viewed \nas a set of decisions. The early design decisions constrain the decisions that fol-\nlow, and changing these decisions has enormous ramifications. Changing these \nearly decisions will cause a ripple effect, in terms of the additional decisions that \nmust now be changed. Yes, sometimes the architecture must be refactored or re-\ndesigned, but this is not a task we undertake lightly (because the “ripple” might \nturn into a tsunami).\nWhat are these early design decisions embodied by software architecture? \nConsider:\n■\n■Will the system run on one processor or be distributed across multiple \nprocessors?\n■\n■Will the software be layered? If so, how many layers will there be? What \nwill each one do?\n■\n■Will components communicate synchronously or asynchronously? Will \nthey interact by transferring control or data or both?\n■\n■Will the system depend on specific features of the operating system or \nhardware? \n■\n■Will the information that flows through the system be encrypted or not?\n■\n■What operating system will we use?\n■\n■What communication protocol will we choose?\nImagine the nightmare of having to change any of these or a myriad other \nrelated decisions. Decisions like these begin to flesh out some of the structures of \nthe architecture and their interactions. In Chapter 4, we describe seven categories \nof these early design decisions. In Chapters 5–11 we show the implications of \nthese design decision categories on achieving quality attributes.\n2.6  Defining Constraints on an Implementation\nAn implementation exhibits an architecture if it conforms to the design decisions \nprescribed by the architecture. This means that the implementation must be im-\nplemented as the set of prescribed elements, these elements must interact with \neach other in the prescribed fashion, and each element must fulfill its responsibil-\nity to the other elements as dictated by the architecture. Each of these prescrip-\ntions is a constraint on the implementer.\nElement builders must be fluent in the specifications of their individual ele-\nments, but they may not be aware of the architectural tradeoffs—the architecture \n(or architect) simply constrains them in such a way as to meet the tradeoffs. A \nclassic example of this phenomenon is when an architect assigns performance \nbudget to the pieces of software involved in some larger piece of functionality. \nIf each software unit stays within its budget, the overall transaction will meet its \n\n\n2.8  Enabling Evolutionary Prototyping\n33\nperformance requirement. Implementers of each of the constituent pieces may \nnot know the overall budget, only their own.\nConversely, the architects need not be experts in all aspects of algorithm \ndesign or the intricacies of the programming language—although they should \ncertainly know enough not to design something that is difficult to build—but they \nare the ones responsible for establishing, analyzing, and enforcing the architec-\ntural tradeoffs. \n2.7  Influencing the Organizational Structure \nNot only does architecture prescribe the structure of the system being developed, \nbut that structure becomes engraved in the structure of the development project (and \nsometimes the structure of the entire organization). The normal method for divid-\ning up the labor in a large project is to assign different groups different portions of \nthe system to construct. This is called the work-breakdown structure of a system. \nBecause the architecture includes the broadest decomposition of the system, it is \ntypically used as the basis for the work-breakdown structure. The work-breakdown \nstructure in turn dictates units of planning, scheduling, and budget; interteam com-\nmunication channels; configuration control and file-system organization; integration \nand test plans and procedures; and even project minutiae such as how the project \nintranet is organized and who sits with whom at the company picnic. Teams commu-\nnicate with each other in terms of the interface specifications for the major elements. \nThe maintenance activity, when launched, will also reflect the software structure, \nwith teams formed to maintain specific structural elements from the architecture: the \ndatabase, the business rules, the user interface, the device drivers, and so forth.\nA side effect of establishing the work-breakdown structure is to freeze some \naspects of the software architecture. A group that is responsible for one of the \nsubsystems will resist having its responsibilities distributed across other groups. \nIf these responsibilities have been formalized in a contractual relationship, chang-\ning responsibilities could become expensive or even litigious. \nThus, once the architecture has been agreed on, it becomes very costly—for \nmanagerial and business reasons—to significantly modify it. This is one argu-\nment (among many) for carrying out extensive analysis before settling on the \nsoftware architecture for a large system—because so much depends on it.\n2.8  Enabling Evolutionary Prototyping\nOnce an architecture has been defined, it can be analyzed and prototyped \nas a skeletal system. A skeletal system is one in which at least some of the \n",
      "page_number": 45
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 54-61)",
      "start_page": 54,
      "end_page": 61,
      "detection_method": "topic_boundary",
      "content": "34 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\ninfrastructure—how the elements initialize, communicate, share data, access re-\nsources, report errors, log activity, and so forth—is built before much of the sys-\ntem’s functionality has been created. (The two can go hand in hand: build a little \ninfrastructure to support a little end-to-end functionality; repeat until done.) \nFor example, systems built as plug-in architectures are skeletal systems: the \nplug-ins provide the actual functionality. This approach aids the development \nprocess because the system is executable early in the product’s life cycle. The \nfidelity of the system increases as stubs are instantiated, or prototype parts are \nreplaced with complete versions of these parts of the software. In some cases the \nprototype parts can be low-fidelity versions of the final functionality, or they can \nbe surrogates that consume and produce data at the appropriate rates but do little \nelse. Among other things, this approach allows potential performance problems \nto be identified early in the product’s life cycle.\nThese benefits reduce the potential risk in the project. Furthermore, if the ar-\nchitecture is part of a family of related systems, the cost of creating a framework \nfor prototyping can be distributed over the development of many systems.\n2.9  Improving Cost and Schedule Estimates\nCost and schedule estimates are important tools for the project manager both to \nacquire the necessary resources and to monitor progress on the project, to know \nif and when a project is in trouble. One of the duties of an architect is to help \nthe project manager create cost and schedule estimates early in the project life \ncycle. Although top-down estimates are useful for setting goals and apportion-\ning budgets, cost estimations that are based on a bottom-up understanding of the \nsystem’s pieces are typically more accurate than those that are based purely on \ntop-down system knowledge. \nAs we have said, the organizational and work-breakdown structure of a proj-\nect is almost always based on its architecture. Each team or individual responsi-\nble for a work item will be able to make more-accurate estimates for their piece \nthan a project manager and will feel more ownership in making the estimates \ncome true. But the best cost and schedule estimates will typically emerge from a \nconsensus between the top-down estimates (created by the architect and project \nmanager) and the bottom-up estimates (created by the developers). The discus-\nsion and negotiation that results from this process creates a far more accurate \nestimate than either approach by itself.\nIt helps if the requirements for a system have been reviewed and validated. \nThe more up-front knowledge you have about the scope, the more accurate the \ncost and schedule estimates will be.\nChapter 22 delves into the use of architecture in project management.\n\n\n2.11  Allowing Incorporation of Independently Developed Components\n35\n2.10  Supplying a Transferable, Reusable Model\nThe earlier in the life cycle that reuse is applied, the greater the benefit that can \nbe achieved. While code reuse provides a benefit, reuse of architectures provides \ntremendous leverage for systems with similar requirements. Not only can code be \nreused, but so can the requirements that led to the architecture in the first place, \nas well as the experience and infrastructure gained in building the reused archi-\ntecture. When architectural decisions can be reused across multiple systems, all \nof the early-decision consequences we just described are also transferred.\nA software product line or family is a set of software systems that are all \nbuilt using the same set of reusable assets. Chief among these assets is the archi-\ntecture that was designed to handle the needs of the entire family. Product-line \narchitects choose an architecture (or a family of closely related architectures) that \nwill serve all envisioned members of the product line. The architecture defines \nwhat is fixed for all members of the product line and what is variable. Software \nproduct lines represent a powerful approach to multi-system development that \nis showing order-of-magnitude payoffs in time to market, cost, productivity, \nand product quality. The power of architecture lies at the heart of the paradigm. \nSimilar to other capital investments, the architecture for a product line becomes \na developing organization’s core asset. Software product lines are explained in \nChapter 25. \n2.11  \u0007Allowing Incorporation of Independently \nDeveloped Components\nWhereas earlier software paradigms have focused on programming as the prime \nactivity, with progress measured in lines of code, architecture-based development \noften focuses on composing or assembling elements that are likely to have been \ndeveloped separately, even independently, from each other. This composition is \npossible because the architecture defines the elements that can be incorporated \ninto the system. The architecture constrains possible replacements (or additions) \naccording to how they interact with their environment, how they receive and re-\nlinquish control, what data they consume and produce, how they access data, and \nwhat protocols they use for communication and resource sharing. \nIn 1793, Eli Whitney’s mass production of muskets, based on the principle \nof interchangeable parts, signaled the dawn of the industrial age. In the days be-\nfore physical measurements were reliable, manufacturing interchangeable parts \nwas a daunting notion. Today in software, until abstractions can be reliably de-\nlimited, the notion of structural interchangeability is just as daunting and just as \nsignificant. \n\n\n36 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nCommercial off-the-shelf components, open source software, publicly avail-\nable apps, and networked services are all modern-day software instantiations of \nWhitney’s basic idea. Whitney’s musket parts had “interfaces” (having to do with \nfit and durability) and so do today’s interchangeable software components.\nFor software, the payoff can be \n■\n■Decreased time to market (it should be easier to use someone else’s ready \nsolution than build your own)\n■\n■Increased reliability (widely used software should have its bugs ironed out \nalready)\n■\n■Lower cost (the software supplier can amortize development cost across \ntheir customer base)\n■\n■Flexibility (if the component you want to buy is not terribly special-\npurpose, it’s likely to be available from several sources, thus increasing \nyour buying leverage)\n2.12  Restricting the Vocabulary of Design Alternatives\nAs useful architectural patterns are collected, it becomes clear that although soft-\nware elements can be combined in more or less infinite ways, there is something \nto be gained by voluntarily restricting ourselves to a relatively small number of \nchoices of elements and their interactions. By doing so we minimize the design \ncomplexity of the system we are building. \nA software engineer is not an artiste, whose creativity and freedom are \nparamount. Engineering is about discipline, and discipline comes in part by re-\nstricting the vocabulary of alternatives to proven solutions. Advantages of this \napproach include enhanced reuse, more regular and simpler designs that are more \neasily understood and communicated, more capable analysis, shorter selection \ntime, and greater interoperability. Architectural patterns guide the architect and \nfocus the architect on the quality attributes of interest in large part by restricting \nthe vocabulary of design alternatives to a relatively small number.\nProperties of software design follow from the choice of an architectural pat-\ntern. Those patterns that are more desirable for a particular problem should im-\nprove the implementation of the resulting design solution, perhaps by making it \neasier to arbitrate conflicting design constraints, by increasing insight into poorly \nunderstood design contexts, or by helping to surface inconsistencies in require-\nments. We will discuss architectural patterns in more detail in Chapter 13.\n\n\n2.14  Summary\n37\n2.13  Providing a Basis for Training\nThe architecture, including a description of how the elements interact with each \nother to carry out the required behavior, can serve as the first introduction to the \nsystem for new project members. This reinforces our point that one of the im-\nportant uses of software architecture is to support and encourage communication \namong the various stakeholders. The architecture is a common reference point.\nModule views are excellent for showing someone the structure of a project: \nWho does what, which teams are assigned to which parts of the system, and so \nforth. Component-and-connector views are excellent for explaining how the sys-\ntem is expected to work and accomplish its job.\nWe will discuss these views in more detail in Chapter 18.\n2.14  Summary\nSoftware architecture is important for a wide variety of technical and nontechni-\ncal reasons. Our list includes the following:\n1.\t An architecture will inhibit or enable a system’s driving quality attributes.\n2.\t The decisions made in an architecture allow you to reason about and man-\nage change as the system evolves.\n3.\t The analysis of an architecture enables early prediction of a system’s \nqualities.\n4.\t A documented architecture enhances communication among stakeholders.\n5.\t The architecture is a carrier of the earliest and hence most fundamental, \nhardest-to-change design decisions.\n6.\t An architecture defines a set of constraints on subsequent implementation.\n7.\t The architecture dictates the structure of an organization, or vice versa.\n8.\t An architecture can provide the basis for evolutionary prototyping.\n9.\t An architecture is the key artifact that allows the architect and project man-\nager to reason about cost and schedule.\n10.\t An architecture can be created as a transferable, reusable model that forms \nthe heart of a product line.\n11.\t Architecture-based development focuses attention on the assembly of com-\nponents, rather than simply on their creation.\n12.\t An architecture channels the creativity of developers, reducing design and \nsystem complexity.\n13.\t An architecture can be the foundation for training of a new team member.\n\n\n38 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\n2.15  For Further Reading\nRebecca Grinter has observed architects from a sociological standpoint. In \n[Grinter 99] she argues eloquently that the architect’s primary role is to facilitate \nstakeholder communication. The way she puts it is that architects enable com-\nmunication among parties who would otherwise not be able to talk to each other.\nThe granddaddy of papers about architecture and organization is [Conway \n68]. Conway’s law states that “organizations which design systems . . . are con-\nstrained to produce designs which are copies of the communication structures of \nthese organizations.” \nThere is much about software development through composition that re-\nmains unresolved. When the components that are candidates for importation and \nreuse are distinct subsystems that have been built with conflicting architectural \nassumptions, unanticipated complications can increase the effort required to inte-\ngrate their functions. David Garlan and his colleagues coined the term architec-\ntural mismatch to describe this situation, and their paper on it is worth reading \n[Garlan 95].\nPaulish [Paulish 02] discusses architecture-based project management, and \nin particular the ways in which an architecture can help in the estimation of proj-\nect cost and schedule.\n2.16  Discussion Questions\n1.\t\nFor each of the thirteen reasons articulated in this chapter why architecture \nis important, take the contrarian position: Propose a set of circumstances \nunder which architecture is not necessary to achieve the result indicated. \nJustify your position. (Try to come up with different circumstances for each \nof the thirteen.)\n2.\t\nThis chapter argues that architecture brings a number of tangible benefits. \nHow would you measure the benefits, on a particular project, of each of the \nthirteen points?\n3.\t\nSuppose you want to introduce architecture-centric practices to your orga-\nnization. Your management is open to the idea, but wants to know the ROI \nfor doing so. How would you respond?\n4.\t\nPrioritize the list of thirteen points in this chapter according to some criteria \nmeaningful to you. Justify your answer. Or, if you could choose only two \nor three of the reasons to promote the use of architecture in a project, which \nwould you choose and why?\n\n\n39\n3\nThe Many Contexts of \nSoftware Architecture\nPeople in London think of London as the center \nof the world, whereas New Yorkers think the \nworld ends three miles outside of Manhattan.\n—Toby Young \nIn 1976, a New Yorker magazine cover featured a cartoon by Saul Steinberg \nshowing a New Yorker’s view of the world. You’ve probably seen it; if not, you \ncan easily find it online. Looking to the west from 9th Avenue in Manhattan, the \nillustration shows 10th Avenue, then the wide Hudson River, then a thin strip \nof completely nondescript land called “Jersey,” followed by a somewhat thicker \nstrip of land representing the entire rest of the United States. The mostly empty \nUnited States has a cartoon mountain or two here and there and a few cities hap-\nhazardly placed “out there,” and is flanked by featureless “Canada” on the right \nand “Mexico” on the left. Beyond is the Pacific Ocean, only slightly wider than \nthe Hudson, and beyond that lie tiny amorphous shapes for Japan and China and \nRussia, and that’s pretty much the world from a New Yorker’s perspective.\nIn a book about architecture, it is tempting to view architecture in the same \nway, as the most important piece of the software universe. And in some chapters, \nwe unapologetically will do exactly that. But in this chapter we put software ar-\nchitecture in its place, showing how it supports and is informed by other critical \nforces and activities in the various contexts in which it plays a role. \nThese contexts, around which we structured this book, are as follows:\n■\n■Technical. What technical role does the software architecture play in the \nsystem or systems of which it’s a part? \n■\n■Project life cycle. How does a software architecture relate to the other \nphases of a software development life cycle?\n■\n■Business. How does the presence of a software architecture affect an orga-\nnization’s business environment?\n\n\n40 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n■\n■Professional. What is the role of a software architect in an organization or a \ndevelopment project? \nThese contexts all play out throughout the book, but this chapter introduces each \none. Although the contexts are unchanging, the specifics for your system may \nchange over time. One challenge for the architect is to envision what in their \ncontext might change and to adopt mechanisms to protect the system and its de-\nvelopment if the envisioned changes come to pass.\n3.1  Architecture in a Technical Context\nArchitectures inhibit or enable the achievement of quality attributes, and one use \nof an architecture is to support reasoning about the consequences of change in the \nparticular quality attributes important for a system at its inception.\nArchitectures Inhibit or Enable the \nAchievement of Quality Attributes\nChapter 2 listed thirteen reasons why software architecture is important and mer-\nits study. Several of those reasons deal with exigencies that go beyond the bounds \nof a particular development project (such as communication among stakehold-\ners, many of whom may reside outside the project’s organization). Others deal \nwith nontechnical aspects of a project (such as the architecture’s influence on a \nproject’s team structure, or its contribution to accurate budget and schedule esti-\nmation). The first three reasons in that List of Thirteen deal specifically with an \narchitecture’s technical impact on every system that uses it:\n1.\t\nAn architecture will inhibit or enable the achievement of a system’s quality \nattributes.\n2.\t\nYou can predict many aspects of a system’s qualities by studying its \narchitecture.\n3.\t\nAn architecture makes it easier for you to reason about and manage change.\nThese are all about the architecture’s effect on a system’s quality attributes, \nalthough the first one states it the most explicitly. While all of the reasons enu-\nmerated in Chapter  2 are valid statements of the contribution of architecture, \nprobably the most important reason that it warrants attention is its critical effect \non quality attributes.\nThis is such a critical point that, with your indulgence, we’ll add a few more \npoints to the bullet list that we gave in Section 2.1. Remember? The one that \nstarted like this:\n\n\n3.1  Architecture in a Technical Context\n41\n■\n■If your system requires high performance, then you need to pay attention \nto managing the time-based behavior of elements, their use of shared \nresources, and the frequency and volume of interelement communication.\nTo that list, we’ll add the following:\n■\n■If you care about a system’s availability, you have to be concerned with \nhow components take over for each other in the event of a failure, and how \nthe system responds to a fault.\n■\n■If you care about usability, you have to be concerned about isolating the \ndetails of the user interface and those elements responsible for the user \nexperience from the rest of the system, so that those things can be tailored \nand improved over time.\n■\n■If you care about the testability of your system, you have to be concerned \nabout the testability of individual elements, which means making their state \nobservable and controllable, plus understanding the emergent behavior of \nthe elements working together.\n■\n■If you care about the safety of your system, you have to be concerned about \nthe behavioral envelope of the elements and the emergent behavior of the \nelements working in concert.\n■\n■If you care about interoperability between your system and another, you \nhave to be concerned about which elements are responsible for external \ninteractions so that you can control those interactions.\nThese and other representations are all saying the same thing in different \nways: If you care about this quality attribute, you have to be concerned with these \ndecisions, all of which are thoroughly architectural in nature. An architecture in-\nhibits or enables a system’s quality attributes. And conversely, nothing else influ-\nences an architecture more than the quality attribute requirements it must satisfy.\nIf you care about architecture for no other reason, you should care about it for \nthis one. We feel so strongly about architecture’s importance with respect to achiev-\ning system quality attributes that all of Part II of this book is devoted to the topic.\nWhy is functionality missing from the preceding list? It is missing because \nthe architecture mainly provides containers into which the architect places func-\ntionality. Functionality is not so much a driver for the architecture as it is a conse-\nquence of it. We return to this point in more detail in Part II.\nArchitectures and the Technical Environment\nThe technical environment that is current when an architecture is designed will \ninfluence that architecture. It might include standard industry practices or soft-\nware engineering techniques prevalent in the architect’s professional community. \nIt is a brave architect who, in today’s environment, does not at least consider \na web-based, object-oriented, service-oriented, mobility-aware, cloud-based, \n",
      "page_number": 54
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 62-69)",
      "start_page": 62,
      "end_page": 69,
      "detection_method": "topic_boundary",
      "content": "42 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nsocial-networking-friendly design for an information system. It wasn’t always so, \nand it won’t be so ten years from now when another crop of technological trends \nhas come to the fore.\nThe Swedish Ship Vasa\nIn the 1620s, Sweden and Poland were at war. The king of Sweden, \nGustavus Adolphus, was determined to put a swift and favorable end to it \nand commissioned a new warship the likes of which had never been seen \nbefore. The Vasa, shown in Figure 3.1, was to be the world’s most formi-\ndable instrument of war: 70 meters long, able to carry 300 soldiers, and \nwith an astonishing 64 heavy guns mounted on two gun decks. Seeking to \nadd overwhelming firepower to his navy to strike a decisive blow, the king \ninsisted on stretching the Vasa’s armaments to the limits. Her architect, \nHenrik Hybertsson, was a seasoned Dutch shipbuilder with an impeccable \nreputation, but the Vasa was beyond even his broad experience. Two-\ngun-deck ships were rare, and none had been built of the Vasa’s size and \narmament. \nLike all architects of systems that push the envelope of experience, \nHybertsson had to balance many concerns. Swift time to deployment was \ncritical, but so were performance, functionality, safety, reliability, and cost. \nFigure 3.1  The warship. Used with permission of The Vasa Museum, \nStockholm, Sweden.\n\n\n3.1  Architecture in a Technical Context\n43\nHe was also responsible to a variety of stakeholders. In this case, the \nprimary customer was the king, but Hybertsson also was responsible to \nthe crew that would sail his creation. Also like all architects, Hybertsson \nbrought his experience with him to the task. In this case, his experience \ntold him to design the Vasa as though it were a single-gun-deck ship and \nthen extrapolate, which was in accordance with the technical environment \nof the day. Faced with an impossible task, Hybertsson had the good sense \nto die about a year before the ship was finished.\nThe project was completed to his specifications, however, and on \nSunday morning, August 10, 1628, the mighty ship was ready. She set her \nsails, waddled out into Stockholm’s deep-water harbor, fired her guns in sa-\nlute, and promptly rolled over. Water poured in through the open gun ports, \nand the Vasa plummeted. A few minutes later her first and only voyage \nended 30 meters beneath the surface. Dozens among her 150-man crew \ndrowned.\nInquiries followed, which concluded that the ship was well built but “badly \nproportioned.” In other words, its architecture was flawed. Today we know \nthat Hybertsson did a poor job of balancing all of the conflicting constraints \nlevied on him. In particular, he did a poor job of risk management and a \npoor job of customer management (not that anyone could have fared bet-\nter). He simply acquiesced in the face of impossible requirements. \nThe story of the Vasa, although more than 375 years old, well illustrates \nthe Architecture Influence Cycle: organization goals beget requirements, \nwhich beget an architecture, which begets a system. The architecture flows \nfrom the architect’s experience and the technical environment of the day. \nHybertsson suffered from the fact that neither of those were up to the task \nbefore him. \nIn this book, we provide three things that Hybertsson could have used:\n1.\t\nExamples of successful architectural practices that work under \ndemanding requirements, so as to help set the technical \nplaying field of the day.\n2.\t\nMethods to assess an architecture before any system is built \nfrom it, so as to mitigate the risks associated with launching \nunprecedented designs.\n3.\t\nTechniques for incremental architecture-based development, \nso as to uncover design flaws before it is too late to correct \nthem.\nOur goal is to give architects another way out of their design dilemmas \nthan the one that befell the ill-fated Dutch ship designer. Death before de-\nployment is not nearly so admired these days. \n—PCC\n\n\n44 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.2  Architecture in a Project Life-Cycle Context\nSoftware development processes are standard approaches for developing software \nsystems. They impose a discipline on software engineers and, more important, \nteams of software engineers. They tell the members of the team what to do next. \nThere are four dominant software development processes, which we describe in \nroughly the order in which they came to prominence: \n1.\t\nWaterfall. For many years the Waterfall model dominated the field of \nsoftware development. The Waterfall model organized the life cycle into a \nseries of connected sequential activities, each with entry and exit conditions \nand a formalized relationship with its upstream and downstream neighbors. \nThe process began with requirements specification, followed by design, \nthen implementation, then integration, then testing, then installation, \nall followed by maintenance. Feedback paths from later to earlier steps \nallowed for the revision of artifacts (requirements documents, design \ndocuments, etc.) on an as-needed basis, based on the knowledge acquired \nin the later stage. For example, designers might push back against overly \nstringent requirements, which would then be reworked and flow back down. \nTesting that uncovered defects would trigger reimplementation (and maybe \neven redesign). And then the cycle continued.\n2.\t\nIterative. Over time the feedback paths of the Waterfall model became \nso pronounced that it became clear that it was better to think of software \ndevelopment as a series of short cycles through the steps—some \nrequirements lead to some design, which can be implemented and tested \nwhile the next cycle’s worth of requirements are being captured and \ndesigned. These cycles are called iterations, in the sense of iterating toward \nthe ultimate software solution for the given problem. Each iteration should \ndeliver something working and useful. The trick here is to uncover early \nthose requirements that have the most far-reaching effect on the design; the \ncorresponding danger is to overlook requirements that, when discovered \nlater, will capsize the design decisions made so far. An especially well-\nknown iterative process is called the Unified Process (originally named the \nRational Unified Process, after Rational Software, which originated it). It \ndefines four phases of each iteration: inception, elaboration, construction, \nand transition. A set of chosen use cases defines the goals for each iteration, \nand the iterations are ordered to address the greatest risks first.\n3.\t\nAgile. The term “Agile software development” refers to a group of \nsoftware development methodologies, the best known of which include \nScrum, Extreme Programming, and Crystal Clear. These methodologies \nare all incremental and iterative. As such, one can consider some iterative \n\n\n3.2  Architecture in a Project Life-Cycle Context\n45\nmethodologies as Agile. What distinguishes Agile practices is early \nand frequent delivery of working software, close collaboration between \ndevelopers and customers, self-organizing teams, and a focus on adaptation \nto changing circumstances (such as late-arriving requirements). All Agile \nmethodologies focus on teamwork, adaptability, and close collaboration \n(both within the team and between team members and customers/end \nusers). These methodologies typically eschew substantial up-front work, \non the assumption that requirements always change, and they continue to \nchange throughout the project’s life cycle. As such, it might seem that Agile \nmethodologies and architecture cannot happily coexist. As we will show in \nChapter 15, this is not so.\n4.\t\nModel-driven development. Model-driven development is based on the \nidea that humans should not be writing code in programming languages, \nbut they should be creating models of the domain, from which code is \nautomatically generated. Humans create a platform-independent model \n(PIM), which is combined with a platform-definition model (PDM) to \ngenerate running code. In this way the PIM is a pure realization of the \nfunctional requirements while the PDM addresses platform specifics and \nquality attributes.\nAll of these processes include design among their obligations, and because \narchitecture is a special kind of design, architecture finds a home in each one. \nChanging from one development process to another in the middle of a project re-\nquires the architect to save useful information from the old process and determine \nhow to integrate it into the new process.\nNo matter what software development process or life-cycle model you’re \nusing, there are a number of activities that are involved in creating a software \narchitecture, using that architecture to realize a complete design, and then imple-\nmenting or managing the evolution of a target system or application. The process \nyou use will determine how often and when you revisit and elaborate each of \nthese activities. These activities include: \n1.\t\nMaking a business case for the system \n2.\t\nUnderstanding the architecturally significant requirements \n3.\t\nCreating or selecting the architecture\n4.\t\nDocumenting and communicating the architecture \n5.\t\nAnalyzing or evaluating the architecture\n6.\t\nImplementing and testing the system based on the architecture\n7.\t\nEnsuring that the implementation conforms to the architecture\nEach of these activities is covered in a chapter in Part III of this book, and \ndescribed briefly below.\n\n\n46 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nMaking a Business Case for the System\nA business case is, briefly, a justification of an organizational investment. It is a \ntool that helps you make business decisions by predicting how they will affect \nyour organization. Initially, the decision will be a go/no-go for pursuing a new \nbusiness opportunity or approach. After initiation, the business case is reviewed \nto assess the accuracy of initial estimates and then updated to examine new or al-\nternative angles on the opportunity. By documenting the expected costs, benefits, \nand risks, the business case serves as a repository of the business and market-\ning data. In this role, management uses the business case to determine possible \ncourses of action. \nKnowing the business goals for the system—Chapter 16 will show you how \nto elicit and capture them in a systematic way—is also critical in the creation of a \nbusiness case for a system. \nCreating a business case is broader than simply assessing the market need \nfor a system. It is an important step in shaping and constraining any future re-\nquirements. How much should the product cost? What is its targeted market? \nWhat is its targeted time to market and lifetime? Will it need to interface with \nother systems? Are there system limitations that it must work within?\nThese are all questions about which the system’s architects have specialized \nknowledge; they must contribute to the answers. These questions cannot be de-\ncided solely by an architect, but if an architect is not consulted in the creation of \nthe business case, the organization may be unable to achieve its business goals. \nTypically, a business case is created prior to the initiation of a project, but it also \nmay be revisited during the course of the project for the organization to deter-\nmine whether to continue making investments in the project. If the circumstances \nassumed in the initial version of the business case change, the architect may be \ncalled upon to establish how the system will change to reflect the new set of \ncircumstances.\nUnderstanding the Architecturally Significant Requirements\nThere are a variety of techniques for eliciting requirements from the stakeholders. \nFor example, object-oriented analysis uses use cases and scenarios to embody \nrequirements. Safety-critical systems sometimes use more rigorous approaches, \nsuch as finite-state-machine models or formal specification languages. In Part II \nof this book, which covers quality attributes, we introduce a collection of quality \nattribute scenarios that aid in the brainstorming, discussion, and capture of qual-\nity attribute requirements for a system. \nOne fundamental decision with respect to the system being built is the extent \nto which it is a variation on other systems that have been constructed. Because \nit is a rare system these days that is not similar to other systems, requirements \n\n\n3.2  Architecture in a Project Life-Cycle Context\n47\nelicitation techniques involve understanding these prior systems’ characteristics. \nWe discuss the architectural implications of software product lines in Chapter 25. \nAnother technique that helps us understand requirements is the creation of \nprototypes. Prototypes may help to model and explore desired behavior, design \nthe user interface, or analyze resource utilization. This helps to make the system \n“real” in the eyes of its stakeholders and can quickly build support for the project \nand catalyze decisions on the system’s design and the design of its user interface. \nCreating or Selecting the Architecture\nIn the landmark book The Mythical Man-Month, Fred Brooks argues forcefully \nand eloquently that conceptual integrity is the key to sound system design and \nthat conceptual integrity can only be had by a small number of minds coming \ntogether to design the system’s architecture. We firmly believe this as well. Good \narchitecture almost never results as an emergent phenomenon. \nChapters 5–12 and 17 will provide practical techniques that will aid you in \ncreating an architecture to achieve its behavioral and quality requirements. \nDocumenting and Communicating the Architecture\nFor the architecture to be effective as the backbone of the project’s design, it \nmust be communicated clearly and unambiguously to all of the stakeholders. De-\nvelopers must understand the work assignments that the architecture requires of \nthem, testers must understand the task structure that the architecture imposes on \nthem, management must understand the scheduling implications it contains, and \nso forth. \nToward this end, the architecture’s documentation should be informative, \nunambiguous, and readable by many people with varied backgrounds. Architec-\ntural documentation should also be minimal and aimed at the stakeholders who \nwill use it; we are no fans of documentation for documentation’s sake. We dis-\ncuss the documentation of architectures and provide examples of good documen-\ntation practices in Chapter 18. We will also discuss keeping the architecture up to \ndate when there is a change in something on which the architecture documenta-\ntion depends.\nAnalyzing or Evaluating the Architecture\nIn any design process there will be multiple candidate designs considered. Some \nwill be rejected immediately. Others will contend for primacy. Choosing among \nthese competing designs in a rational way is one of the architect’s greatest \nchallenges. \n\n\n48 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nEvaluating an architecture for the qualities that it supports is essential to \nensuring that the system constructed from that architecture satisfies its stake-\nholders’ needs. Analysis techniques to evaluate the quality attributes that an ar-\nchitecture imparts to a system have become much more widespread in the past \ndecade. Scenario-based techniques provide one of the most general and effective \napproaches for evaluating an architecture. The most mature methodological ap-\nproach is found in the Architecture Tradeoff Analysis Method (ATAM) of Chap-\nter 21, while the economic implications of architectural decisions are explored in \nChapter 23.\nImplementing and Testing the System \nBased on the Architecture\nIf the architect designs and analyzes a beautiful, conceptually sound architec-\nture which the implementers then ignore, what was the point? If architecture is \nimportant enough to devote the time and effort of your best minds to, then it is \njust as important to keep the developers faithful to the structures and interaction \nprotocols constrained by the architecture. Having an explicit and well-commu-\nnicated architecture is the first step toward ensuring architectural conformance. \nHaving an environment or infrastructure that actively assists developers in creat-\ning and maintaining the architecture (as opposed to just the code) is better. \nThere are many reasons why developers might not be faithful to the archi-\ntecture: It might not have been properly documented and disseminated. It might \nbe too confusing. It might be that the architect has not built ground-level support \nfor the architecture (particularly if it presents a different way of “doing business” \nthan the developers are used to), and so the developers resist it. Or the developers \nmay sincerely want to implement the architecture but, being human, they occa-\nsionally slip up. This is not to say that the architecture should not change, but it \nshould not change purely on the basis of the whims of the developers, because \nthey may not have the overall picture.\nEnsuring That the Implementation \nConforms to the Architecture\nFinally, when an architecture is created and used, it goes into a maintenance \nphase. Vigilance is required to ensure that the actual architecture and its repre-\nsentation remain faithful to each other during this phase. And when they do get \nsignificantly out of sync, effort must be expended to either fix the implementation \nor update the architectural documentation.\nAlthough work in this area is still relatively immature, it has been an area of \nintense activity in recent years. Chapter 20 will present the current state of recov-\nering an architecture from an existing system and ensuring that it conforms to the \nspecified architecture. \n\n\n3.3  Architecture in a Business Context\n49\n3.3  Architecture in a Business Context\nArchitectures and systems are not constructed frivolously. They serve some business \npurposes, although as mentioned before, these purposes may change over time. \nArchitectures and Business Goals\nSystems are created to satisfy the business goals of one or more organizations. \nDevelopment organizations want to make a profit, or capture market, or stay in \nbusiness, or help their customers do their jobs better, or keep their staff gainfully \nemployed, or make their stockholders happy, or a little bit of each. Customers \nhave their own goals for acquiring a system, usually involving some aspect of \nmaking their lives easier or more productive. Other organizations involved in a \nproject’s life cycle, such as subcontractors or government regulatory agencies, \nhave their own goals dealing with the system.\nArchitects need to understand who the vested organizations are and what their \ngoals are. Many of these goals will have a profound influence on the architecture. \nMany business goals will be manifested as quality attribute requirements. \nIn fact, every quality attribute—such as a user-visible response time or platform \nflexibility or ironclad security or any of a dozen other needs—should originate \nfrom some higher purpose that can be described in terms of added value. If we \nask, for example, “Why do you want this system to have a really fast response \ntime?” we might hear that this will differentiate the product from its competition \nand let the developing organization capture market share. \nSome business goals, however, will not show up in the form of requirements. \nWe know of one software architect who was informed by his manager that the \narchitecture should include a database. The architect was perplexed, because the \nrequirements for the system really didn’t warrant a database and the architect’s \ndesign had nicely avoided putting one in, thereby simplifying the design and \nlowering the cost of the product. The architect was perplexed, that is, until the \nmanager reminded the architect that the company’s database department was cur-\nrently overstaffed and underworked. They needed something to do! The architect \nput in the database, and all was well. That kind of business goal—keeping staff \ngainfully employed—is not likely to show up in any requirements document, but \nif the architect had failed to meet it, the manager would have considered the ar-\nchitecture as unacceptable, just as the customer would have if it failed to provide \na key piece of functionality.\nStill other business goals have no effect on the architecture whatsoever. A \nbusiness goal to lower costs might be realized by asking employees to work from \nhome, or turn the office thermostats down in the winter, or using less paper in the \nprinters. Chapter 16 will deal with uncovering business goals and the require-\nments they lead to.\n",
      "page_number": 62
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 70-80)",
      "start_page": 70,
      "end_page": 80,
      "detection_method": "topic_boundary",
      "content": "50 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nFigure 3.2 illustrates the major points from the preceding discussion. In the \nfigure, the arrows mean “leads to.” The solid arrows highlight the relationships of \nmost interest to us.\nArchitectures and the Development Organization\nA development organization contributes many of the business goals that influ-\nence an architecture. For example, if the organization has an abundance of ex-\nperienced and idle programmers skilled in peer-to-peer communications, then \na peer-to-peer architecture might be the approach supported by management. If \nnot, it may well be rejected. This would support the business goal, perhaps left \nimplicit, of not wanting to hire new staff or lay off existing staff, or not wanting \nto invest significantly in the retraining of existing staff. \nMore generally, an organization often has an investment in assets, such as \nexisting architectures and the products based on them. The foundation of a de-\nvelopment project may be that the proposed system is the next in a sequence of \nsimilar systems, and the cost estimates assume a high degree of asset reuse and a \nhigh degree of skill and productivity from the programmers. \nAdditionally, an organization may wish to make a long-term business in-\nvestment in an infrastructure to pursue strategic goals and may view the proposed \nsystem as one means of financing and extending that infrastructure. For example, \nan organization may decide that it wants to develop a reputation for supporting \nsolutions based on cloud computing or service-oriented architecture or high-per-\nformance real-time computing. This long-term goal would be supported, in part, \nby infrastructural investments that will affect the developing organization: a \ncloud-computing group needs to be hired or grown, infrastructure needs to be \npurchased, or perhaps training needs to be planned.\nBusiness Goals\nQuality Attributes\nArchitecture\nNonarchitectural Solutions\nFigure 3.2  Some business goals may lead to quality attribute requirements \n(which lead to architectures), or lead directly to architectural decisions, or lead to \nnonarchitectural solutions.\n\n\n3.4  Architecture in a Professional Context\n51\nFinally, the organizational structure can shape the software architecture, and \nvice versa. Organizations are often organized around technology and application \nconcepts: a database group, a networking group, a business rules team, a user-in-\nterface group. So the explicit identification of a distinct subsystem in the archi-\ntecture will frequently lead to the creation of a group with the name of the sub-\nsystem. Furthermore, if the user-interface team frequently needs to communicate \nwith the business rules team, these teams will need to either be co-located or they \nwill need some regular means of communicating and coordinating. \n3.4  Architecture in a Professional Context\nWhat do architects do? How do you become an architect? In this section we talk \nabout the many facets of being an architect that go beyond what you learned in a \nprogramming or software engineering course.\nYou probably know by now that architects need more than just technical \nskills. Architects need to explain to one stakeholder or another the chosen prior-\nities of different properties, and why particular stakeholders are not having all of \ntheir expectations fulfilled. To be an effective architect, then, you will need diplo-\nmatic, negotiation, and communication skills.\nYou will perform many activities beyond directly producing an architecture. \nThese activities, which we call duties, form the backbone of individual architec-\nture competence. We surveyed the broad body of information aimed at architects \n(such as websites, courses, books, and position descriptions for architects), as \nwell as practicing architects, and duties are but one aspect. Writers about archi-\ntects also speak of skills and knowledge. For example, architects need the ability \nto communicate ideas clearly and need to have up-to-date knowledge about (for \nexample) patterns, or database platforms, or web services standards. \nDuties, skills, and knowledge form a triad on which architecture compe-\ntence rests. You will need to be involved in supporting management and deal-\ning with customers. You will need to manage a diverse workload and be able to \nswitch contexts frequently. You will need to know business considerations. You \nwill need to be a leader in the eyes of developers and management. In Chapter 24 \nwe examine at length the architectural competence of organizations and people.\nArchitects’ Background and Experience\nWe are all products of our experiences, architects included. If you have had good \nresults using a particular architectural approach, such as three-tier client-server \nor publish-subscribe, chances are that you will try that same approach on a new \ndevelopment effort. Conversely, if your experience with an approach was disas-\ntrous, you may be reluctant to try it again. \n\n\n52 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nArchitectural choices may also come from your education and training, \nexposure to successful architectural patterns, or exposure to systems that have \nworked particularly poorly or particularly well. You may also wish to experiment \nwith an architectural pattern or technique learned from a book (such as this one) \nor a training course. \nWhy do we mention this? Because you (and your organization) must be \naware of this influence, so that you can manage it to the best of your abilities. This \nmay mean that you will critically examine proposed architectural solutions, to \nensure that they are not simply the path of least resistance. It may mean that you \nwill take training courses in interesting new technologies. It may mean that you \nwill invest in exploratory projects, to “test the water” of a new technology. Each \nof these steps is a way to proactively manage your background and experience.\n3.5  Stakeholders\nMany people and organizations are interested in a software system. We call these \nentities stakeholders. A stakeholder is anyone who has a stake in the success of \nthe system: the customer, the end users, the developers, the project manager, the \nmaintainers, and even those who market the system, for example. But stakehold-\ners, despite all having a shared stake in the success of the system, typically have \ndifferent specific concerns that they wish the system to guarantee or optimize. \nThese concerns are as diverse as providing a certain behavior at runtime, perform-\ning well on a particular piece of hardware, being easy to customize, achieving \nshort time to market or low cost of development, gainfully employing program-\nmers who have a particular specialty, or providing a broad range of functions. \nFigure 3.3 shows the architect receiving a few helpful stakeholder “suggestions.” \nYou will need to know and understand the nature, source, and priority of \nconstraints on the project as early as possible. Therefore, you must identify and \nactively engage the stakeholders to solicit their needs and expectations. Early en-\ngagement of stakeholders allows you to understand the constraints of the task, \nmanage expectations, negotiate priorities, and make tradeoffs. Architecture eval-\nuation (covered in Part III of this book) and iterative prototyping are two means \nfor you to achieve stakeholder engagement.\nHaving an acceptable system involves appropriate performance, reliability, \navailability, platform compatibility, memory utilization, network usage, security, \nmodifiability, usability, and interoperability with other systems as well as behav-\nior. All of these qualities, and others, affect how the delivered system is viewed \nby its eventual recipients, and so such quality attributes will be demanded by one \nor more of the system’s stakeholders. \nThe underlying problem, of course, is that each stakeholder has different \nconcerns and goals, some of which may be contradictory. It is a rare requirements \n\n\n3.5  Stakeholders\n53\ndocument that does a good job of capturing all of a system’s quality requirements \nin testable detail (a property is testable if it is falsifiable; “make the system easy \nto use” is not falsifiable but “deliver audio packets with no more than 10 ms. \njitter” is falsifiable). The architect often has to fill in the blanks—the quality attri-\nbute requirements that have not been explicitly stated—and mediate the conflicts \nthat frequently emerge.\nTherefore, one of the best pieces of advice we can give to architects is this: \nKnow your stakeholders. Talk to them, engage them, listen to them, and put your-\nself in their shoes. Table 3.1 enumerates a set of stakeholders. Notice the remark-\nable variety and length of this set, but remember that not every stakeholder named \nin this list may play a role in every system, and one person may play many roles. \nArchitect\nDeveloping\nOrganization’s\nManagement\nStakeholder\nMarketing\nStakeholder\nEnd User\nStakeholder\nMaintenance\nOrganization\nStakeholder\nCustomer\nStakeholder\nlow cost,\nkeeping people\nemployed!\nbehavior,\nperformance,\nsecurity,\nreliability,\nusability!\nNeat features,\nshort time to market,\nlow cost, parity with\ncompeting products!\nModifiability!\nlow cost, timely\ndelivery, not changed\nvery often!\nOhhhhhh...\nFigure 3.3  Influence of stakeholders on the architect\n\n\n54 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nTable 3.1  Stakeholders for a System and Their Interests\nName\nDescription\nInterest in Architecture\nAnalyst\nResponsible for analyzing the architecture to make sure it meets certain \ncritical quality attribute requirements. Analysts are often specialized; for \ninstance, performance analysts, safety analysts, and security analysts \nmay have well-defined positions in a project.\nAnalyzing satisfaction of quality attribute requirements of the system \nbased on its architecture.\nArchitect\nResponsible for the development of the architecture and its \ndocumentation. Focus and responsibility is on the system.\nNegotiating and making tradeoffs among competing requirements \nand design approaches. A vessel for recording design decisions. \nProviding evidence that the architecture satisfies its requirements.\nBusiness  \nManager\nResponsible for the functioning of the business/organizational entity \nthat owns the system. Includes managerial/executive responsibility, \nresponsibility for defining business processes, etc. \nUnderstanding the ability of the architecture to meet business goals.\nConformance \nChecker\nResponsible for assuring conformance to standards and processes to \nprovide confidence in a product’s suitability.\nBasis for conformance checking, for assurance that implementations \nhave been faithful to the architectural prescriptions.\nCustomer\nPays for the system and ensures its delivery. The customer often speaks \nfor or represents the end user, especially in a government acquisition \ncontext. \nAssuring required functionality and quality will be delivered; gauging \nprogress; estimating cost; and setting expectations for what will be \ndelivered, when, and for how much.\nDatabase \nAdministrator\nInvolved in many aspects of the data stores, including database design, \ndata analysis, data modeling and optimization, installation of database \nsoftware, and monitoring and administration of database security.\nUnderstanding how data is created, used, and updated by other \narchitectural elements, and what properties the data and database \nmust have for the overall system to meet its quality goals.\nDeployer\nResponsible for accepting the completed system from the development \neffort and deploying it, making it operational, and fulfilling its allocated \nbusiness function.\nUnderstanding the architectural elements that are delivered and \nto be installed at the customer or end user’s site, and their overall \nresponsibility toward system function.\nDesigner\nResponsible for systems and/or software design downstream of the \narchitecture, applying the architecture to meet specific requirements of the \nparts for which they are responsible.\nResolving resource contention and establishing performance and \nother kinds of runtime resource consumption budgets. Understand-\ning how their part will communicate and interact with other parts of \nthe system.\nEvaluator\nResponsible for conducting a formal evaluation of the architecture (and its \ndocumentation) against some clearly defined criteria.\nEvaluating the architecture’s ability to deliver required behavior and \nquality attributes.\n\n\n3.5  Stakeholders\n55\nName\nDescription\nInterest in Architecture\nImplementer\nResponsible for the development of specific elements according to \ndesigns, requirements, and the architecture.\nUnderstanding inviolable constraints and exploitable freedoms on \ndevelopment activities.\nIntegrator\nResponsible for taking individual components and integrating them, \naccording to the architecture and system designs.\nProducing integration plans and procedures, and locating the source \nof integration failures.\nMaintainer\nResponsible for fixing bugs and providing enhancements to the system \nthroughout its life (including adaptation of the system for uses not originally \nenvisioned).\nUnderstanding the ramifications of a change.\nNetwork \nAdministrator\nResponsible for the maintenance and oversight of computer hardware \nand software in a computer network. This may include the deployment, \nconfiguration, maintenance, and monitoring of network components.\nDetermining network loads during various use profiles, understanding \nuses of the network.\nProduct-Line \nManager\nResponsible for development of an entire family of products, all built using \nthe same core assets (including the architecture).\nDetermining whether a potential new member of a product family is in \nor out of scope and, if out, by how much.\nProject Manager\nResponsible for planning, sequencing, scheduling, and allocating \nresources to develop software components and deliver components to \nintegration and test activities.\nHelping to set budget and schedule, gauging progress against \nestablished budget and schedule, identifying and resolving \ndevelopment-time resource contention.\nRepresentative of \nExternal Systems \nResponsible for managing a system with which this one must interoperate, \nand its interface with our system.\nDefining the set of agreement between the systems. \nSystem Engineer\nResponsible for design and development of systems or system \ncomponents in which software plays a role.\nAssuring that the system environment provided for the software is \nsufficient.\nTester\nResponsible for the (independent) test and verification of the system or its \nelements against the formal requirements and the architecture.\nCreating tests based on the behavior and interaction of the software \nelements. \nUser\nThe actual end users of the system. There may be distinguished kinds of \nusers, such as administrators, superusers, etc.\nUsers, in the role of reviewers, might use architecture documentation \nto check whether desired functionality is being delivered. Users might \nalso use the documentation to understand what the major system \nelements are, which can aid them in emergency field maintenance.\n\n\n56 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.6  How Is Architecture Influenced?\nFor decades, software designers have been taught to build systems based on the \nsoftware’s technical requirements. In the older Waterfall model, the requirements \ndocument is “tossed over the wall” into the designer’s cubicle, and the designer \nmust come forth with a satisfactory design. Requirements beget design, which \nbegets system. In an iterative or Agile approach to development, an increment of \nrequirements begets an increment of design, and so forth.\nThis vision of software development is short-sighted. In any development \neffort, the requirements make explicit some—but only some—of the desired \nproperties of the final system. Not all requirements are focused directly on de-\nsired system properties; some requirements might mandate a development pro-\ncess or the use of a particular tool. Furthermore, the requirements specification \nonly begins to tell the story. Failure to satisfy other constraints may render the \nsystem just as problematic as if it functioned poorly. \nWhat do you suppose would happen if two different architects, working in two \ndifferent organizations, were given the same requirements specification for a sys-\ntem? Do you think they would produce the same architecture or different ones? \nThe answer is that they would very likely produce different ones, which im-\nmediately belies the notion that requirements determine architecture. Other fac-\ntors are at work. \nA software architecture is a result of business and social influences, as well \nas technical ones. The existence of an architecture in turn affects the technical, \nbusiness, and social environments that subsequently influence future architec-\ntures. In particular, each of the contexts for architecture that we just covered—\ntechnical, project, business, and professional—plays a role in influencing an ar-\nchitect and the architecture, as shown in Figure 3.4. \nArchitect’s Influences\nArchitect\nBusiness\nTechnical\nProject\nProfessional\nStakeholders\nArchitecture\nSystem\nFigure 3.4  Influences on the architect\n\n\n3.7  What Do Architectures Influence? \n57\nAn architect designing a system for which the real-time deadlines are tight \nwill make one set of design choices; the same architect, designing a similar sys-\ntem in which the deadlines can be easily satisfied, will make different choices. \nAnd the same architect, designing a non-real-time system, is likely to make quite \ndifferent choices still. Even with the same requirements, hardware, support soft-\nware, and human resources available, an architect designing a system today is \nlikely to design a different system than might have been designed five years ago. \n3.7  What Do Architectures Influence? \nThe story about contexts influencing architectures has a flip side. It turns out that \narchitectures have an influence on the very factors that influence them. Specifi-\ncally, the existence of an architecture affects the technical, project, business, and \nprofessional contexts that subsequently influence future architectures.\nHere is how the cycle works:\n■\n■Technical context. The architecture can affect stakeholder requirements \nfor the next system by giving the customer the opportunity to receive a \nsystem (based on the same architecture) in a more reliable, timely, and \neconomical manner than if the subsequent system were to be built from \nscratch, and typically with fewer defects. A customer may in fact be willing \nto relax some of their requirements to gain these economies. Shrink-\nwrapped software has clearly affected people’s requirements by providing \nsolutions that are not tailored to any individual’s precise needs but are \ninstead inexpensive and (in the best of all possible worlds) of high quality. \nSoftware product lines have the same effect on customers who cannot be so \nflexible with their requirements.\n■\n■Project context. The architecture affects the structure of the developing \norganization. An architecture prescribes a structure for a system; as we will \nsee, it particularly prescribes the units of software that must be implemented \n(or otherwise obtained) and integrated to form the system. These units \nare the basis for the development project’s structure. Teams are formed \nfor individual software units; and the development, test, and integration \nactivities all revolve around the units. Likewise, schedules and budgets \nallocate resources in chunks corresponding to the units. If a company \nbecomes adept at building families of similar systems, it will tend to invest \nin each team by nurturing each area of expertise. Teams become embedded \nin the organization’s structure. This is feedback from the architecture to \nthe developing organization. In any design undertaken by the organization \nat large, these groups have a strong voice in the system’s decomposition, \npressuring for the continued existence of the portions they control.\n\n\n58 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n■\n■Business context. The architecture can affect the business goals of the \ndeveloping organization. A successful system built from an architecture can \nenable a company to establish a foothold in a particular market segment—\nthink of the iPhone or Android app platforms as examples. The architecture \ncan provide opportunities for the efficient production and deployment of \nsimilar systems, and the organization may adjust its goals to take advantage \nof its newfound expertise to plumb the market. This is feedback from the \nsystem to the developing organization and the systems it builds.\n■\n■Professional context. The process of system building will affect the \narchitect’s experience with subsequent systems by adding to the corporate \nexperience base. A system that was successfully built around a particular \ntechnical approach will make the architect more inclined to build systems \nusing the same approach in the future. On the other hand, architectures that \nfail are less likely to be chosen for future projects.\nThese and other feedback mechanisms form what we call the Architecture \nInfluence Cycle, or AIC, illustrated in Figure 3.5, which depicts the influences of \nthe culture and business of the development organization on the software archi-\ntecture. That architecture is, in turn, a primary determinant of the properties of \nthe developed system or systems. But the AIC is also based on a recognition that \nshrewd organizations can take advantage of the organizational and experiential \neffects of developing an architecture and can use those effects to position their \nbusiness strategically for future projects.\nArchitect’s Influences\nArchitect\nBusiness\nTechnical\nProject\nProfessional\nStakeholders\nArchitecture\nSystem\nFigure 3.5  Architecture Influence Cycle\n\n\n3.9  For Further Reading\n59\n3.8  Summary \nArchitectures exist in four different contexts.\n1.\t\nTechnical. The technical context includes the achievement of quality \nattribute requirements. We spend Part II discussing how to do this. The \ntechnical context also includes the current technology. The cloud (discussed \nin Chapter 26) and mobile computing (discussed in Chapter 27) are \nimportant current technologies.\n2.\t\nProject life cycle. Regardless of the software development methodology \nyou use, you must make a business case for the system, understand the \narchitecturally significant requirements, create or select the architecture, \ndocument and communicate the architecture, analyze or evaluate the archi-\ntecture, implement and test the system based on the architecture, and ensure \nthat the implementation conforms to the architecture.\n3.\t\nBusiness. The system created from the architecture must satisfy the busi-\nness goals of a wide variety of stakeholders, each of whom has different \nexpectations for the system. The architecture is also influenced by and in-\nfluences the structure of the development organization.\n4.\t\nProfessional. You must have certain skills and knowledge to be an architect, \nand there are certain duties that you must perform as an architect. These \nare influenced not only by coursework and reading but also by your \nexperiences.\nAn architecture has some influences that lead to its creation, and its exis-\ntence has an impact on the architect, the organization, and, potentially, the indus-\ntry. We call this cycle the Architecture Influence Cycle.\n3.9  For Further Reading\nThe product line framework produced by the Software Engineering Institute in-\ncludes a discussion of business cases from which we drew [SEI 12].\nThe SEI has also published a case study of Celsius Tech that includes an ex-\nample of how organizations and customers change over time [Brownsword 96]. \nSeveral other SEI reports discuss how to find business goals and the busi-\nness goals that have been articulated by certain organizations [Kazman 05, Cle-\nments 10b]. \nRuth Malan and Dana Bredemeyer provide a description of how an architect \ncan build buy-in within an organization [Malan 00].\n\n\n60 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.10  Discussion Questions\n1.\t\nEnumerate six different software systems used by your organization. For \neach of these systems:\na.\t What are the contextual influences?\nb.\t Who are the stakeholders?\nc.\t How do these systems reflect or impact the organizational structure?\n2.\t\nWhat kinds of business goals have driven the construction of the following:\na.\t The World Wide Web\nb.\t Amazon’s EC2 cloud infrastructure\nc.\t Google’s Android platform\n3.\t\nWhat mechanisms are available to improve your skills and knowledge? \nWhat skills are you lacking?\n4.\t\nDescribe a system you are familiar with and place it into the AIC. Specifi-\ncally, identify the forward and reverse influences on contextual factors.\n",
      "page_number": 70
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 81-88)",
      "start_page": 81,
      "end_page": 88,
      "detection_method": "topic_boundary",
      "content": "61\nPart  T WO\nQuality Attributes\nIn Part II, we provide the technical foundations for you to design or analyze an \narchitecture to achieve particular quality attributes. We do not discuss design or \nanalysis processes here; we cover those topics in Part III. It is impossible, how-\never, to understand how to improve the performance of a design, for example, \nwithout understanding something about performance. \nIn Chapter 4 we describe how to specify a quality attribute requirement and \nmotivate design techniques called tactics to enable you to achieve a particular qual-\nity attribute requirement. We also enumerate seven categories of design decisions. \nThese are categories of decisions that are universally important, and so we provide \nmaterial to help an architect focus on these decisions. In Chapter 4, we describe \nthese categories, and in each of the following chapters devoted to a particular quality \nattribute—Chapters 5–11—we use those categories to develop a checklist that tells \nyou how to focus your attention on the important aspects associated with that quality \nattribute. Many of the items in our checklists may seem obvious, but the purpose of \na checklist is to help ensure the completeness of your design and analysis process.\nIn addition to providing a treatment of seven specific quality attributes \n(availability, interoperability, modifiability, performance, security, testability, and \nusability), we also describe how you can generate the material provided in Chap-\nters 5–11 for other quality attributes that we have not covered.\nArchitectural patterns provide known solutions to a number of common \nproblems in design. In Chapter 13, we present some of the most important pat-\nterns and discuss the relationship between patterns and tactics.\nBeing able to analyze a design for a particular quality attribute is a key skill \nthat you as an architect will need to acquire. In Chapter 14, we discuss modeling \ntechniques for some of the quality attributes.\n\n\nThis page intentionally left blank \n\n\n63\n4\nUnderstanding Quality \nAttributes\nBetween stimulus and response, there is a space. In \nthat space is our power to choose our response. In \nour response lies our growth and our freedom.\n— Viktor E. Frankl, Man’s Search for Meaning\nAs we have seen in the Architecture Influence Cycle (in Chapter 3), many fac-\ntors determine the qualities that must be provided for in a system’s architecture. \nThese qualities go beyond functionality, which is the basic statement of the sys-\ntem’s capabilities, services, and behavior. Although functionality and other qual-\nities are closely related, as you will see, functionality often takes the front seat in \nthe development scheme. This preference is shortsighted, however. Systems are \nfrequently redesigned not because they are functionally deficient—the replace-\nments are often functionally identical—but because they are difficult to maintain, \nport, or scale; or they are too slow; or they have been compromised by hackers. \nIn Chapter 2, we said that architecture was the first place in software creation in \nwhich quality requirements could be addressed. It is the mapping of a system’s \nfunctionality onto software structures that determines the architecture’s support \nfor qualities. In Chapters 5–11 we discuss how various qualities are supported by \narchitectural design decisions. In Chapter 17 we show how to integrate all of the \nquality attribute decisions into a single design. \nWe have been using the term “quality attribute” loosely, but now it is time to \ndefine it more carefully. A quality attribute (QA) is a measurable or testable prop-\nerty of a system that is used to indicate how well the system satisfies the needs of \nits stakeholders. You can think of a quality attribute as measuring the “goodness” \nof a product along some dimension of interest to a stakeholder.\nIn this chapter our focus is on understanding the following:\n■\n■How to express the qualities we want our architecture to provide to the sys-\ntem or systems we are building from it \n\n\n64 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n■\n■How to achieve those qualities \n■\n■How to determine the design decisions we might make with respect to those \nqualities \nThis chapter provides the context for the discussion of specific quality attributes \nin Chapters 5–11.\n4.1  Architecture and Requirements\nRequirements for a system come in a variety of forms: textual requirements, \nmockups, existing systems, use cases, user stories, and more. Chapter 16 dis-\ncusses the concept of an architecturally significant requirement, the role such re-\nquirements play in architecture, and how to identify them. No matter the source, \nall requirements encompass the following categories: \n1.\t\nFunctional requirements. These requirements state what the system must \ndo, and how it must behave or react to runtime stimuli. \n2.\t\nQuality attribute requirements. These requirements are qualifications of \nthe functional requirements or of the overall product. A qualification of a \nfunctional requirement is an item such as how fast the function must be \nperformed, or how resilient it must be to erroneous input. A qualification \nof the overall product is an item such as the time to deploy the product or a \nlimitation on operational costs.\n3.\t\nConstraints. A constraint is a design decision with zero degrees of freedom. \nThat is, it’s a design decision that’s already been made. Examples include \nthe requirement to use a certain programming language or to reuse a certain \nexisting module, or a management fiat to make your system service ori-\nented. These choices are arguably in the purview of the architect, but ex-\nternal factors (such as not being able to train the staff in a new language, or \nhaving a business agreement with a software supplier, or pushing business \ngoals of service interoperability) have led those in power to dictate these \ndesign outcomes.\nWhat is the “response” of architecture to each of these kinds of requirements?\n1.\t\nFunctional requirements are satisfied by assigning an appropriate sequence \nof responsibilities throughout the design. As we will see later in this chap-\nter, assigning responsibilities to architectural elements is a fundamental \narchitectural design decision.\n2.\t\nQuality attribute requirements are satisfied by the various structures de-\nsigned into the architecture, and the behaviors and interactions of the ele-\nments that populate those structures. Chapter 17 will show this approach in \nmore detail. \n\n\n4.3  Quality Attribute Considerations \n65\n3.\t\nConstraints are satisfied by accepting the design decision and reconciling it \nwith other affected design decisions.\n4.2  Functionality\nFunctionality is the ability of the system to do the work for which it was in-\ntended. Of all of the requirements, functionality has the strangest relationship to \narchitecture.\nFirst of all, functionality does not determine architecture. That is, given a \nset of required functionality, there is no end to the architectures you could create \nto satisfy that functionality. At the very least, you could divide up the function-\nality in any number of ways and assign the subpieces to different architectural \nelements. \nIn fact, if functionality were the only thing that mattered, you wouldn’t have \nto divide the system into pieces at all; a single monolithic blob with no internal \nstructure would do just fine. Instead, we design our systems as structured sets \nof cooperating architectural elements—modules, layers, classes, services, data-\nbases, apps, threads, peers, tiers, and on and on—to make them understandable \nand to support a variety of other purposes. Those “other purposes” are the other \nquality attributes that we’ll turn our attention to in the remaining sections of this \nchapter, and the remaining chapters of Part II. \nBut although functionality is independent of any particular structure, func-\ntionality is achieved by assigning responsibilities to architectural elements, re-\nsulting in one of the most basic of architectural structures.\nAlthough responsibilities can be allocated arbitrarily to any modules, soft-\nware architecture constrains this allocation when other quality attributes are im-\nportant. For example, systems are frequently divided so that several people can \ncooperatively build them. The architect’s interest in functionality is in how it in-\nteracts with and constrains other qualities. \n4.3  Quality Attribute Considerations \nJust as a system’s functions do not stand on their own without due consideration of \nother quality attributes, neither do quality attributes stand on their own; they pertain \nto the functions of the system. If a functional requirement is “When the user presses \nthe green button, the Options dialog appears,” a performance QA annotation might \ndescribe how quickly the dialog will appear; an availability QA annotation might \ndescribe how often this function will fail, and how quickly it will be repaired; a us-\nability QA annotation might describe how easy it is to learn this function.\n\n\n66 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\nFunctional Requirements\nAfter more than 15 years of writing and discussing the distinction between \nfunctional requirements and quality requirements, the definition of func-\ntional requirements still eludes me. Quality attribute requirements are well \ndefined: performance has to do with the timing behavior of the system, \nmodifiability has to do with the ability of the system to support changes in \nits behavior or other qualities after initial deployment, availability has to do \nwith the ability of the system to survive failures, and so forth.\nFunction, however, is much more slippery. An international standard \n(ISO 25010) defines functional suitability as “the capability of the software \nproduct to provide functions which meet stated and implied needs when \nthe software is used under specified conditions.” That is, functionality is the \nability to provide functions. One interpretation of this definition is that func-\ntionality describes what the system does and quality describes how well \nthe system does its function. That is, qualities are attributes of the system \nand function is the purpose of the system.\nThis distinction breaks down, however, when you consider the nature \nof some of the “function.” If the function of the software is to control engine \nbehavior, how can the function be correctly implemented without consid-\nering timing behavior? Is the ability to control access through requiring a \nuser name/password combination not a function even though it is not the \npurpose of any system?\nI like much better the use of the word “responsibility” to describe com-\nputations that a system must perform. Questions such as “What are the \ntiming constraints on that set of responsibilities?”, “What modifications are \nanticipated with respect to that set of responsibilities?”, and “What class of \nusers is allowed to execute that set of responsibilities?” make sense and \nare actionable.\nThe achievement of qualities induces responsibility; think of the user \nname/password example just mentioned. Further, one can identify respon-\nsibilities as being associated with a particular set of requirements.\nSo does this mean that the term “functional requirement” shouldn’t be \nused? People have an understanding of the term, but when precision is \ndesired, we should talk about sets of specific responsibilities instead.\nPaul Clements has long ranted against the careless use of the term \n“nonfunctional,” and now it’s my turn to rant against the careless use of the \nterm “functional”—probably equally ineffectually.\n—LB\nQuality attributes have been of interest to the software community at least \nsince the 1970s. There are a variety of published taxonomies and definitions, and \nmany of them have their own research and practitioner communities. From an \n\n\n4.3  Quality Attribute Considerations \n67\narchitect’s perspective, there are three problems with previous discussions of sys-\ntem quality attributes: \n1.\t\nThe definitions provided for an attribute are not testable. It is meaningless \nto say that a system will be “modifiable.” Every system may be modifiable \nwith respect to one set of changes and not modifiable with respect to an-\nother. The other quality attributes are similar in this regard: a system may \nbe robust with respect to some faults and brittle with respect to others. And \nso forth.\n2.\t\nDiscussion often focuses on which quality a particular concern belongs to. \nIs a system failure due to a denial-of-service attack an aspect of availability, \nan aspect of performance, an aspect of security, or an aspect of usability? \nAll four attribute communities would claim ownership of a system failure \ndue to a denial-of-service attack. All are, to some extent, correct. But this \ndoesn’t help us, as architects, understand and create architectural solutions \nto manage the attributes of concern.\n3.\t\nEach attribute community has developed its own vocabulary. The perfor-\nmance community has “events” arriving at a system, the security com-\nmunity has “attacks” arriving at a system, the availability community has \n“failures” of a system, and the usability community has “user input.” All \nof these may actually refer to the same occurrence, but they are described \nusing different terms.\nA solution to the first two of these problems (untestable definitions and \noverlapping concerns) is to use quality attribute scenarios as a means of charac-\nterizing quality attributes (see the next section). A solution to the third problem \nis to provide a discussion of each attribute—concentrating on its underlying con-\ncerns—to illustrate the concepts that are fundamental to that attribute community.\nThere are two categories of quality attributes on which we focus. The first is \nthose that describe some property of the system at runtime, such as availability, \nperformance, or usability. The second is those that describe some property of the \ndevelopment of the system, such as modifiability or testability. \nWithin complex systems, quality attributes can never be achieved in isola-\ntion. The achievement of any one will have an effect, sometimes positive and \nsometimes negative, on the achievement of others. For example, almost every \nquality attribute negatively affects performance. Take portability. The main tech-\nnique for achieving portable software is to isolate system dependencies, which \nintroduces overhead into the system’s execution, typically as process or proce-\ndure boundaries, and this hurts performance. Determining the design that sat-\nisfies all of the quality attribute requirements is partially a matter of making the \nappropriate tradeoffs; we discuss design in Chapter 17. Our purpose here is to \nprovide the context for discussing each quality attribute. In particular, we focus \non how quality attributes can be specified, what architectural decisions will en-\nable the achievement of particular quality attributes, and what questions about \nquality attributes will enable the architect to make the correct design decisions.\n\n\n68 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n4.4  Specifying Quality Attribute Requirements\nA quality attribute requirement should be unambiguous and testable. We use a \ncommon form to specify all quality attribute requirements. This has the advantage \nof emphasizing the commonalities among all quality attributes. It has the disad-\nvantage of occasionally being a force-fit for some aspects of quality attributes.\nOur common form for quality attribute expression has these parts:\n■\n■Stimulus. We use the term “stimulus” to describe an event arriving at the \nsystem. The stimulus can be an event to the performance community, a \nuser operation to the usability community, or an attack to the security \ncommunity. We use the same term to describe a motivating action for de-\nvelopmental qualities. Thus, a stimulus for modifiability is a request for \na modification; a stimulus for testability is the completion of a phase of \ndevelopment.\n■\n■Stimulus source. A stimulus must have a source—it must come from some-\nwhere. The source of the stimulus may affect how it is treated by the sys-\ntem. A request from a trusted user will not undergo the same scrutiny as a \nrequest by an untrusted user.\n■\n■Response. How the system should respond to the stimulus must also be \nspecified. The response consists of the responsibilities that the system \n(for runtime qualities) or the developers (for development-time qualities) \nshould perform in response to the stimulus. For example, in a performance \nscenario, an event arrives (the stimulus) and the system should process \nthat event and generate a response. In a modifiability scenario, a request \nfor a modification arrives (the stimulus) and the developers should imple-\nment the modification—without side effects—and then test and deploy the \nmodification.\n■\n■Response measure. Determining whether a response is satisfactory—\nwhether the requirement is satisfied—is enabled by providing a response \nmeasure. For performance this could be a measure of latency or throughput; \nfor modifiability it could be the labor or wall clock time required to make, \ntest, and deploy the modification.\nThese four characteristics of a scenario are the heart of our quality attribute \nspecifications. But there are two more characteristics that are important: environ-\nment and artifact.\n■\n■Environment. The environment of a requirement is the set of circumstances \nin which the scenario takes place. The environment acts as a qualifier on \nthe stimulus. For example, a request for a modification that arrives after \nthe code has been frozen for a release may be treated differently than one \nthat arrives before the freeze. A failure that is the fifth successive failure \n",
      "page_number": 81
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 89-96)",
      "start_page": 89,
      "end_page": 96,
      "detection_method": "topic_boundary",
      "content": "4.4  Specifying Quality Attribute Requirements\n69\nof a component may be treated differently than the first failure of that \ncomponent.\n■\n■Artifact. Finally, the artifact is the portion of the system to which the \nrequirement applies. Frequently this is the entire system, but occasion-\nally specific portions of the system may be called out. A failure in a \ndata store may be treated differently than a failure in the metadata store. \nModifications to the user interface may have faster response times than \nmodifications to the middleware. \nTo summarize how we specify quality attribute requirements, we capture \nthem formally as six-part scenarios. While it is common to omit one or more of \nthese six parts, particularly in the early stages of thinking about quality attributes, \nknowing that all parts are there forces the architect to consider whether each part \nis relevant. \nIn summary, here are the six parts:\n1.\t\nSource of stimulus. This is some entity (a human, a computer system, or \nany other actuator) that generated the stimulus.\n2.\t\nStimulus. The stimulus is a condition that requires a response when it ar-\nrives at a system.\n3.\t\nEnvironment. The stimulus occurs under certain conditions. The system \nmay be in an overload condition or in normal operation, or some other rele-\nvant state. For many systems, “normal” operation can refer to one of a num-\nber of modes. For these kinds of systems, the environment should specify in \nwhich mode the system is executing.\n4.\t\nArtifact. Some artifact is stimulated. This may be a collection of systems, \nthe whole system, or some piece or pieces of it.\n5.\t\nResponse. The response is the activity undertaken as the result of the arrival \nof the stimulus. \n6.\t\nResponse measure. When the response occurs, it should be measurable in \nsome fashion so that the requirement can be tested. \nWe distinguish general quality attribute scenarios (which we call “general \nscenarios” for short)—those that are system independent and can, potentially, \npertain to any system—from concrete quality attribute scenarios (concrete sce-\nnarios)—those that are specific to the particular system under consideration. \nWe can characterize quality attributes as a collection of general scenarios. \nOf course, to translate these generic attribute characterizations into requirements \nfor a particular system, the general scenarios need to be made system specific. \nDetailed examples of these scenarios will be given in Chapters 5–11. Figure 4.1 \nshows the parts of a quality attribute scenario that we have just discussed. Fig-\nure 4.2 shows an example of a general scenario, in this case for availability.\n\n\n70 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n4.5  Achieving Quality Attributes through Tactics\nThe quality attribute requirements specify the responses of the system that, with a \nbit of luck and a dose of good planning, realize the goals of the business. We now \nturn to the techniques an architect can use to achieve the required quality attri-\nbutes. We call these techniques architectural tactics. A tactic is a design decision \nthat influences the achievement of a quality attribute response—tactics directly \naffect the system’s response to some stimulus. Tactics impart portability to one \ndesign, high performance to another, and integrability to a third.\nStimulus\nResponse\nResponse\nMeasure\nSource\nof Stimulus\nArtifact\nEnvironment\n3\n2\n1\n4\nFigure 4.1  The parts of a quality attribute scenario\nFigure 4.2  A general scenario for availability\nStimulus\nResponse\nResponse\nMeasure\nSource\nof Stimulus\n3\n2\n1\n4\nInternal/External: \npeople, hardware, \nsoftware, physical \ninfrastructure, \nphysical \nenvironment\nFault: \nomission, \ncrash, \nincorrect \ntiming, \nincorrect \nresponse\nPrevent fault from \nbecoming failure\nDetect fault: log, notify \nRecover from fault:\ndisable event source, \nbe unavailable, \nfix/mask, degraded \nmode\nTime or time interval \nsystem must be available\nAvailability percentage \nTime in degraded mode\nTime to detect fault \nRepair time\nProportion of faults \nsystem handles\nArtifact\nProcessors, \ncommunication \nchannels, persistent \nstorage, processes\nEnvironment\nNormal operation, \nstartup, shutdown, \nrepair mode, \ndegraded \noperation, \noverloaded \noperation\n\n\n4.5  Achieving Quality Attributes through Tactics\n71\nNot My Problem\nOne time I was doing an architecture analysis on a complex system cre-\nated by and for Lawrence Livermore National Laboratory. If you visit their \nwebsite (www.llnl.gov) and try to figure out what Livermore Labs does, you \nwill see the word “security” mentioned over and over. The lab focuses on \nnuclear security, international and domestic security, and environmental \nand energy security. Serious stuff . . .\nKeeping this emphasis in mind, I asked them to describe the quality \nattributes of concern for the system that I was analyzing. I’m sure you can \nimagine my surprise when security wasn’t mentioned once! The system \nstakeholders mentioned performance, modifiability, evolvability, interoper-\nability, configurability, and portability, and one or two more, but the word \nsecurity never passed their lips. \nBeing a good analyst, I questioned this seemingly shocking and obvious \nomission. Their answer was simple and, in retrospect, straightforward: “We \ndon’t care about it. Our systems are not connected to any external net-\nwork and we have barbed-wire fences and guards with machine guns.” Of \ncourse, someone at Livermore Labs was very interested in security. But it \nwas clearly not the software architects.\n—RK\nThe focus of a tactic is on a single quality attribute response. Within a tactic, \nthere is no consideration of tradeoffs. Tradeoffs must be explicitly considered \nand controlled by the designer. In this respect, tactics differ from architectural \npatterns, where tradeoffs are built into the pattern. (We visit the relation between \ntactics and patterns in Chapter 14. Chapter 13 explains how sets of tactics for a \nquality attribute can be constructed, which are the steps we used to produce the \nset in this book.)\nA system design consists of a collection of decisions. Some of these deci-\nsions help control the quality attribute responses; others ensure achievement of \nsystem functionality. We represent the relationship between stimulus, tactics, and \nresponse in Figure 4.3. The tactics, like design patterns, are design techniques \nthat architects have been using for years. Our contribution is to isolate, catalog, \nand describe them. We are not inventing tactics here, we are just capturing what \narchitects do in practice. \nWhy do we do this? There are three reasons: \n1.\t\nDesign patterns are complex; they typically consist of a bundle of design \ndecisions. But patterns are often difficult to apply as is; architects need to \nmodify and adapt them. By understanding the role of tactics, an architect \ncan more easily assess the options for augmenting an existing pattern to \nachieve a quality attribute goal. \n\n\n72 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n2.\t\nIf no pattern exists to realize the architect’s design goal, tactics allow the \narchitect to construct a design fragment from “first principles.” Tactics give \nthe architect insight into the properties of the resulting design fragment. \n3.\t\nBy cataloging tactics, we provide a way of making design more systematic \nwithin some limitations. Our list of tactics does not provide a taxonomy. We \nonly provide a categorization. The tactics will overlap, and you frequently \nwill have a choice among multiple tactics to improve a particular quality at-\ntribute. The choice of which tactic to use depends on factors such as tradeoffs \namong other quality attributes and the cost to implement. These consider-\nations transcend the discussion of tactics for particular quality attributes. \nChapter 17 provides some techniques for choosing among competing tactics.\nThe tactics that we present can and should be refined. Consider perfor-\nmance: Schedule resources is a common performance tactic. But this tactic needs \nto be refined into a specific scheduling strategy, such as shortest-job-first, round-\nrobin, and so forth, for specific purposes. Use an intermediary is a modifiability \ntactic. But there are multiple types of intermediaries (layers, brokers, and prox-\nies, to name just a few). Thus there are refinements that a designer will employ to \nmake each tactic concrete. \nIn addition, the application of a tactic depends on the context. Again consid-\nering performance: Manage sampling rate is relevant in some real-time systems \nbut not in all real-time systems and certainly not in database systems.\n4.6  Guiding Quality Design Decisions\nRecall that one can view an architecture as the result of applying a collection of \ndesign decisions. What we present here is a systematic categorization of these \nFigure 4.3  Tactics are intended to control responses to stimuli.\nStimulus\nResponse\nTactics\nto Control\nResponse\n\n\n4.6  Guiding Quality Design Decisions\n73\ndecisions so that an architect can focus attention on those design dimensions \nlikely to be most troublesome. \nThe seven categories of design decisions are\n1.\t\nAllocation of responsibilities\n2.\t\nCoordination model\n3.\t\nData model\n4.\t\nManagement of resources\n5.\t\nMapping among architectural elements\n6.\t\nBinding time decisions\n7.\t\nChoice of technology\nThese categories are not the only way to classify architectural design deci-\nsions, but they do provide a rational division of concerns. These categories might \noverlap, but it’s all right if a particular decision exists in two different categories, \nbecause the concern of the architect is to ensure that every important decision is \nconsidered. Our categorization of decisions is partially based on our definition \nof software architecture in that many of our categories relate to the definition of \nstructures and the relations among them.\nAllocation of Responsibilities\nDecisions involving allocation of responsibilities include the following:\n■\n■Identifying the important responsibilities, including basic system functions, \narchitectural infrastructure, and satisfaction of quality attributes. \n■\n■Determining how these responsibilities are allocated to non-runtime and \nruntime elements (namely, modules, components, and connectors). \nStrategies for making these decisions include functional decomposition, \nmodeling real-world objects, grouping based on the major modes of system oper-\nation, or grouping based on similar quality requirements: processing frame rate, \nsecurity level, or expected changes.\nIn Chapters 5–11, where we apply these design decision categories to a \nnumber of important quality attributes, the checklists we provide for the alloca-\ntion of responsibilities category is derived systematically from understanding the \nstimuli and responses listed in the general scenario for that QA.\nCoordination Model\nSoftware works by having elements interact with each other through designed \nmechanisms. These mechanisms are collectively referred to as a coordination \nmodel. Decisions about the coordination model include these:\n\n\n74 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n■\n■Identifying the elements of the system that must coordinate, or are prohib-\nited from coordinating.\n■\n■Determining the properties of the coordination, such as timeliness, cur-\nrency, completeness, correctness, and consistency.\n■\n■Choosing the communication mechanisms (between systems, between our \nsystem and external entities, between elements of our system) that realize \nthose properties. Important properties of the communication mechanisms \ninclude stateful versus stateless, synchronous versus asynchronous, guar-\nanteed versus nonguaranteed delivery, and performance-related properties \nsuch as throughput and latency.\nData Model\nEvery system must represent artifacts of system-wide interest—data—in some \ninternal fashion. The collection of those representations and how to interpret \nthem is referred to as the data model. Decisions about the data model include the \nfollowing:\n■\n■Choosing the major data abstractions, their operations, and their properties. \nThis includes determining how the data items are created, initialized, ac-\ncessed, persisted, manipulated, translated, and destroyed.\n■\n■Compiling metadata needed for consistent interpretation of the data.\n■\n■Organizing the data. This includes determining whether the data is going \nto be kept in a relational database, a collection of objects, or both. If both, \nthen the mapping between the two different locations of the data must be \ndetermined.\nManagement of Resources\nAn architect may need to arbitrate the use of shared resources in the architec-\nture. These include hard resources (e.g., CPU, memory, battery, hardware buffers, \nsystem clock, I/O ports) and soft resources (e.g., system locks, software buffers, \nthread pools, and non-thread-safe code). \nDecisions for management of resources include the following:\n■\n■Identifying the resources that must be managed and determining the limits \nfor each.\n■\n■Determining which system element(s) manage each resource. \n■\n■Determining how resources are shared and the arbitration strategies em-\nployed when there is contention.\n■\n■Determining the impact of saturation on different resources. For example, \nas a CPU becomes more heavily loaded, performance usually just degrades \nfairly steadily. On the other hand, when you start to run out of memory, at \n\n\n4.6  Guiding Quality Design Decisions\n75\nsome point you start paging/swapping intensively and your performance \nsuddenly crashes to a halt.\nMapping among Architectural Elements \nAn architecture must provide two types of mappings. First, there is mapping \nbetween elements in different types of architecture structures—for example, \nmapping from units of development (modules) to units of execution (threads or \nprocesses). Next, there is mapping between software elements and environment \nelements—for example, mapping from processes to the specific CPUs where \nthese processes will execute.\nUseful mappings include these:\n■\n■The mapping of modules and runtime elements to each other—that is, the \nruntime elements that are created from each module; the modules that con-\ntain the code for each runtime element.\n■\n■The assignment of runtime elements to processors.\n■\n■The assignment of items in the data model to data stores.\n■\n■The mapping of modules and runtime elements to units of delivery.\nBinding Time Decisions\nBinding time decisions introduce allowable ranges of variation. This variation \ncan be bound at different times in the software life cycle by different entities—\nfrom design time by a developer to runtime by an end user. A binding time de-\ncision establishes the scope, the point in the life cycle, and the mechanism for \nachieving the variation. \nThe decisions in the other six categories have an associated binding time \ndecision. Examples of such binding time decisions include the following:\n■\n■For allocation of responsibilities, you can have build-time selection of mod-\nules via a parameterized makefile. \n■\n■For choice of coordination model, you can design runtime negotiation of \nprotocols.\n■\n■For resource management, you can design a system to accept new periph-\neral devices plugged in at runtime, after which the system recognizes them \nand downloads and installs the right drivers automatically.\n■\n■For choice of technology, you can build an app store for a smartphone that \nautomatically downloads the version of the app appropriate for the phone of \nthe customer buying the app.\nWhen making binding time decisions, you should consider the costs to im-\nplement the decision and the costs to make a modification after you have im-\nplemented the decision. For example, if you are considering changing platforms \n\n\n76 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\nat some time after code time, you can insulate yourself from the effects caused \nby porting your system to another platform at some cost. Making this decision \ndepends on the costs incurred by having to modify an early binding compared to \nthe costs incurred by implementing the mechanisms involved in the late binding. \nChoice of Technology\nEvery architecture decision must eventually be realized using a specific tech-\nnology. Sometimes the technology selection is made by others, before the in-\ntentional architecture design process begins. In this case, the chosen technology \nbecomes a constraint on decisions in each of our seven categories. In other cases, \nthe architect must choose a suitable technology to realize a decision in every one \nof the categories.\nChoice of technology decisions involve the following:\n■\n■Deciding which technologies are available to realize the decisions made in \nthe other categories.\n■\n■Determining whether the available tools to support this technology choice \n(IDEs, simulators, testing tools, etc.) are adequate for development to \nproceed.\n■\n■Determining the extent of internal familiarity as well as the degree of exter-\nnal support available for the technology (such as courses, tutorials, exam-\nples, and availability of contractors who can provide expertise in a crunch) \nand deciding whether this is adequate to proceed.\n■\n■Determining the side effects of choosing a technology, such as a required \ncoordination model or constrained resource management opportunities.\n■\n■Determining whether a new technology is compatible with the existing \ntechnology stack. For example, can the new technology run on top of or \nalongside the existing technology stack? Can it communicate with the exist-\ning technology stack? Can the new technology be monitored and managed?\n4.7  Summary\nRequirements for a system come in three categories:\n1.\t\nFunctional. These requirements are satisfied by including an appropriate set \nof responsibilities within the design.\n2.\t\nQuality attribute. These requirements are satisfied by the structures and \nbehaviors of the architecture.\n3.\t\nConstraints. A constraint is a design decision that’s already been made.\n",
      "page_number": 89
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 97-110)",
      "start_page": 97,
      "end_page": 110,
      "detection_method": "topic_boundary",
      "content": "4.9  Discussion Questions\n77\nTo express a quality attribute requirement, we use a quality attribute sce-\nnario. The parts of the scenario are these:\n1.\t\nSource of stimulus\n2.\t\nStimulus\n3.\t\nEnvironment\n4.\t\nArtifact\n5.\t\nResponse \n6.\t\nResponse measure\nAn architectural tactic is a design decision that affects a quality attribute \nresponse. The focus of a tactic is on a single quality attribute response. Architec-\ntural patterns can be seen as “packages” of tactics.\nThe seven categories of architectural design decisions are these:\n1.\t\nAllocation of responsibilities\n2.\t\nCoordination model\n3.\t\nData model\n4.\t\nManagement of resources\n5.\t\nMapping among architectural elements\n6.\t\nBinding time decisions\n7.\t\nChoice of technology\n4.8  For Further Reading\nPhilippe Kruchten [Kruchten 04] provides another categorization of design \ndecisions.\nPena [Pena 87] uses categories of Function/Form/Economy/Time as a way \nof categorizing design decisions. \nBinding time and mechanisms to achieve different types of binding times \nare discussed in [Bachmann 05].\nTaxonomies of quality attributes can be found in [Boehm 78], [McCall 77], \nand [ISO 11].\nArguments for viewing architecture as essentially independent from func-\ntion can be found in [Shaw 95].\n4.9  Discussion Questions\n1.\t\nWhat is the relationship between a use case and a quality attribute scenario? \nIf you wanted to add quality attribute information to a use case, how would \nyou do it?\n\n\n78 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n2.\t\nDo you suppose that the set of tactics for a quality attribute is finite or in-\nfinite? Why?\n3.\t\nDiscuss the choice of programming language (an example of choice of \ntechnology) and its relation to architecture in general, and the design \ndecisions in the other six categories? For instance, how can certain pro-\ngramming languages enable or inhibit the choice of particular coordination \nmodels?\n4.\t\nWe will be using the automatic teller machine as an example throughout \nthe chapters on quality attributes. Enumerate the set of responsibilities that \nan automatic teller machine should support and propose an initial design to \naccommodate that set of responsibilities. Justify your proposal.\n5.\t\nThink about the screens that your favorite automatic teller machine uses. \nWhat do those screens tell you about binding time decisions reflected in the \narchitecture?\n6.\t\nConsider the choice between synchronous and asynchronous communica-\ntion (a choice in the coordination mechanism category). What quality attri-\nbute requirements might lead you to choose one over the other?\n7.\t\nConsider the choice between stateful and stateless communication (a choice \nin the coordination mechanism category). What quality attribute require-\nments might lead you to choose one over the other?\n8.\t\nMost peer-to-peer architecture employs late binding of the topology. What \nquality attributes does this promote or inhibit?\n\n\n79\n5\nAvailability\nWith James Scott\nNinety percent of life is just showing up.\n—Woody Allen\nAvailability refers to a property of software that it is there and ready to carry \nout its task when you need it to be. This is a broad perspective and encompasses \nwhat is normally called reliability (although it may encompass additional con-\nsiderations such as downtime due to periodic maintenance). In fact, availability \nbuilds upon the concept of reliability by adding the notion of recovery—that is, \nwhen the system breaks, it repairs itself. Repair may be accomplished by various \nmeans, which we’ll see in this chapter. More precisely, Avižienis and his col-\nleagues have defined dependability:\nDependability is the ability to avoid failures that are more frequent and \nmore severe than is acceptable.\nOur definition of availability as an aspect of dependability is this: “Availabil-\nity refers to the ability of a system to mask or repair faults such that the cumula-\ntive service outage period does not exceed a required value over a specified time \ninterval.” These definitions make the concept of failure subject to the judgment of \nan external agent, possibly a human. They also subsume concepts of reliability, \nconfidentiality, integrity, and any other quality attribute that involves a concept of \nunacceptable failure. \nAvailability is closely related to security. A denial-of-service attack is ex-\nplicitly designed to make a system fail—that is, to make it unavailable. Availabil-\nity is also closely related to performance, because it may be difficult to tell when \na system has failed and when it is simply being outrageously slow to respond. \nFinally, availability is closely allied with safety, which is concerned with keeping \n\n\n80 \nPart Two  Quality Attributes\t\n5—Availability\nthe system from entering a hazardous state and recovering or limiting the damage \nwhen it does.\nFundamentally, availability is about minimizing service outage time by mit-\nigating faults. Failure implies visibility to a system or human observer in the en-\nvironment. That is, a failure is the deviation of the system from its specification, \nwhere the deviation is externally visible. One of the most demanding tasks in \nbuilding a high-availability, fault-tolerant system is to understand the nature of \nthe failures that can arise during operation (see the sidebar “Planning for Fail-\nure”). Once those are understood, mitigation strategies can be designed into the \nsoftware.\nA failure’s cause is called a fault. A fault can be either internal or external to \nthe system under consideration. Intermediate states between the occurrence of a \nfault and the occurrence of a failure are called errors. Faults can be prevented, tol-\nerated, removed, or forecast. In this way a system becomes “resilient” to faults.\nAmong the areas with which we are concerned are how system faults are \ndetected, how frequently system faults may occur, what happens when a fault \noccurs, how long a system is allowed to be out of operation, when faults or fail-\nures may occur safely, how faults or failures can be prevented, and what kinds of \nnotifications are required when a failure occurs. \nBecause a system failure is observable by users, the time to repair is the time \nuntil the failure is no longer observable. This may be a brief delay in the response \ntime or it may be the time it takes someone to fly to a remote location in the An-\ndes to repair a piece of mining machinery (as was recounted to us by a person \nresponsible for repairing the software in a mining machine engine). The notion \nof “observability” can be a tricky one: the Stuxnet virus, as an example, went un-\nobserved for a very long time even though it was doing damage. In addition, we \nare often concerned with the level of capability that remains when a failure has \noccurred—a degraded operating mode.\nThe distinction between faults and failures allows discussion of automatic \nrepair strategies. That is, if code containing a fault is executed but the system is \nable to recover from the fault without any deviation from specified behavior be-\ning observable, there is no failure. \nThe availability of a system can be calculated as the probability that it will \nprovide the specified services within required bounds over a specified time inter-\nval. When referring to hardware, there is a well-known expression used to derive \nsteady-state availability:\nMTBF\n(MTBF + MTTR)\nwhere MTBF refers to the mean time between failures and MTTR refers to the \nmean time to repair. In the software world, this formula should be interpreted \nto mean that when thinking about availability, you should think about what will \nmake your system fail, how likely that is to occur, and that there will be some \ntime required to repair it.\n\n\n\t\n5—Availability  81\nFrom this formula it is possible to calculate probabilities and make claims \nlike “99.999 percent availability,” or a 0.001 percent probability that the system \nwill not be operational when needed. Scheduled downtimes (when the system is \nintentionally taken out of service) may not be considered when calculating avail-\nability, because the system is deemed “not needed” then; of course, this depends \non the specific requirements for the system, often encoded in service-level agree-\nments (SLAs). This arrangement may lead to seemingly odd situations where the \nsystem is down and users are waiting for it, but the downtime is scheduled and so \nis not counted against any availability requirements. \nIn operational systems, faults are detected and correlated prior to being re-\nported and repaired. Fault correlation logic will categorize a fault according to \nits severity (critical, major, or minor) and service impact (service-affecting or \nnon-service-affecting) in order to provide the system operator with timely and ac-\ncurate system status and allow for the appropriate repair strategy to be employed. \nThe repair strategy may be automated or may require manual intervention.\nThe availability provided by a computer system or hosting service is fre-\nquently expressed as a service-level agreement. This SLA specifies the availabil-\nity level that is guaranteed and, usually, the penalties that the computer system or \nhosting service will suffer if the SLA is violated. The SLA that Amazon provides \nfor its EC2 cloud service is\nAWS will use commercially reasonable efforts to make Amazon EC2 \navailable with an Annual Uptime Percentage [defined elsewhere] of at \nleast 99.95% during the Service Year. In the event Amazon EC2 does \nnot meet the Annual Uptime Percentage commitment, you will be \neligible to receive a Service Credit as described below.\nTable 5.1 provides examples of system availability requirements and associated \nthreshold values for acceptable system downtime, measured over observation pe-\nriods of 90 days and one year. The term high availability typically refers to de-\nsigns targeting availability of 99.999 percent (“5 nines”) or greater. By definition \nor convention, only unscheduled outages contribute to system downtime.\nTable 5.1  System Availability Requirements\nAvailability\nDowntime/90 Days\nDowntime/Year\n99.0%\n21 hours, 36 minutes\n3 days, 15.6 hours\n99.9%\n2 hours, 10 minutes\n8 hours, 0 minutes, 46 seconds\n99.99%\n12 minutes, 58 seconds\n52 minutes, 34 seconds\n99.999%\n1 minute, 18 seconds\n5 minutes, 15 seconds\n99.9999%\n8 seconds\n32 seconds\n\n\n82 \nPart Two  Quality Attributes\t\n5—Availability\nPlanning for Failure\nWhen designing a high-availability or safety-critical system, it’s tempting to \nsay that failure is not an option. It’s a catchy phrase, but it’s a lousy design \nphilosophy. In fact, failure is not only an option, it’s almost inevitable. What \nwill make your system safe and available is planning for the occurrence of \nfailure or (more likely) failures, and handling them with aplomb. The first \nstep is to understand what kinds of failures your system is prone to, and \nwhat the consequences of each will be. Here are three well-known tech-\nniques for getting a handle on this.\nHazard analysis\nHazard analysis is a technique that attempts to catalog the hazards that \ncan occur during the operation of a system. It categorizes each hazard \naccording to its severity. For example, the DO-178B standard used in the \naeronautics industry defines these failure condition levels in terms of their \neffects on the aircraft, crew, and passengers:\n■\n■\nCatastrophic. This kind of failure may cause a crash. This failure represents \nthe loss of critical function required to safely fly and land aircraft.\n■\n■\nHazardous. This kind of failure has a large negative impact on safety or \nperformance, or reduces the ability of the crew to operate the aircraft due \nto physical distress or a higher workload, or causes serious or fatal injuries \namong the passengers. \n■\n■\nMajor. This kind of failure is significant, but has a lesser impact than a \nHazardous failure (for example, leads to passenger discomfort rather than \ninjuries) or significantly increases crew workload to the point where safety \nis affected.\n■\n■\nMinor. This kind of failure is noticeable, but has a lesser impact than a Ma-\njor failure (for example, causing passenger inconvenience or a routine flight \nplan change).\n■\n■\nNo effect. This kind of failure has no impact on safety, aircraft operation, or \ncrew workload.\nOther domains have their own categories and definitions. Hazard anal-\nysis also assesses the probability of each hazard occurring. Hazards for \nwhich the product of cost and probability exceed some threshold are then \nmade the subject of mitigation activities.\nFault tree analysis\nFault tree analysis is an analytical technique that specifies a state of the \nsystem that negatively impacts safety or reliability, and then analyzes the \nsystem’s context and operation to find all the ways that the undesired state \ncould occur. The technique uses a graphic construct (the fault tree) that \nhelps identify all sequential and parallel sequences of contributing faults \nthat will result in the occurrence of the undesired state, which is listed at \nthe top of the tree (the “top event”). The contributing faults might be hard-\nware failures, human errors, software errors, or any other pertinent events \nthat can lead to the undesired state. \n\n\nPart Two  Quality Attributes\t\n5—Availability  83\nFigure 5.1, taken from a NASA handbook on fault tree analysis, shows \na very simple fault tree for which the top event is failure of component D. It \nshows that component D can fail if A fails and either B or C fails.\nThe symbols that connect the events in a fault tree are called gate symbols, \nand are taken from Boolean logic diagrams. Figure 5.2 illustrates the notation.\nA fault tree lends itself to static analysis in various ways. For example, a \n“minimal cut set” is the smallest combination of events along the bottom of \nthe tree that together can cause the top event. The set of minimal cut sets \nshows all the ways the bottom events can combine to cause the overarch-\ning failure. Any singleton minimal cut set reveals a single point of failure, \nwhich should be carefully scrutinized. Also, the probabilities of various con-\ntributing failures can be combined to come up with a probability of the top \nevent occurring. Dynamic analysis occurs when the order of contributing \nfailures matters. In this case, techniques such as Markov analysis can be \nused to calculate probability of failure over different failure sequences. \nFault trees aid in system design, but they can also be used to diagnose \nfailures at runtime. If the top event has occurred, then (assuming the fault \ntree model is complete) one or more of the contributing failures has oc-\ncurred, and the fault tree can be used to track it down and initiate repairs.\nFailure Mode, Effects, and Criticality Analysis (FMECA) catalogs the \nkinds of failures that systems of a given type are prone to, along with how \nsevere the effects of each one can be. FMECA relies on the history of\nD Fails\nA Fails\nB or C Fail\nB Fails\nC Fails\nG1\nA\nG2\nC\nB\nFigure 5.1  A simple fault tree. D fails if A fails and either B or C fails.\n\n\n84 \nPart Two  Quality Attributes\t\n5—Availability\nfailure of similar systems in the past. Table 5.2, also taken from the NASA \nhandbook, shows the data for a system of redundant amplifiers. Historical \ndata shows that amplifiers fail most often when there is a short circuit or \nthe circuit is left open, but there are several other failure modes as well \n(lumped together as “Other”).\nn\n \nGATE SYMBOLS\nAND   Output fault occurs if all of the input faults occur\nOR   Output fault occurs if a least one of the input faults occurs\nCOMBINATION   Output fault occurs if n of the input faults occur\nEXCLUSIVE OR   Output fault occurs if exactly one of the input \nfaults occurs\nPRIORITY AND   Output fault occurs if all of the input faults occur in a \nspecific sequence (the sequence is represented by a CONDITIONING \nEVENT drawn to the right of the gate)\nINHIBIT   Output fault occurs if the (single) input fault occurs in the \npresence of an enabling condition (the enabling condition is represented \nby a CONDITIONING EVENT drawn to the right of the gate)\nFigure 5.2  Fault tree gate symbols\nTable 5.2  Failure Probabilities and Effects\n \nComponent\nFailure \nProbability\nFailure \nMode\n% Failures \nby Mode\nEffects\nCritical\nNoncritical\nA\n1 × 10–3\nOpen\n90\nX\nShort\n5\nX (5 × 10–5)\nOther\n5\nX (5 × 10–5)\nB\n1 × 10–3\nOpen\n90\nX\nShort\n5\nX (5 × 10–5)\nOther\n5\nX (5 × 10–5)\n\n\n5.1  Availability General Scenario\n85\nAdding up the critical column gives us the probability of a critical system \nfailure: 5 × 10–5 + 5 × 10–5 + 5 × 10–5 + 5 × 10–5 = 2 × 10–4. \nThese techniques, and others, are only as good as the knowledge and \nexperience of the people who populate their respective data structures. \nOne of the worst mistakes you can make, according to the NASA hand-\nbook, is to let form take priority over substance. That is, don’t let safety \nengineering become a matter of just filling out the tables. Instead, keep \npressing to find out what else can go wrong, and then plan for it.\n5.1  Availability General Scenario\nFrom these considerations we can now describe the individual portions of an \navailability general scenario. These are summarized in Table 5.3: \n■\n■Source of stimulus. We differentiate between internal and external origins of \nfaults or failure because the desired system response may be different. \n■\n■Stimulus. A fault of one of the following classes occurs: \n■\n■Omission. A component fails to respond to an input.\n■\n■Crash. The component repeatedly suffers omission faults.\n■\n■Timing. A component responds but the response is early or late.\n■\n■Response. A component responds with an incorrect value.\n■\n■Artifact. This specifies the resource that is required to be highly available, \nsuch as a processor, communication channel, process, or storage.\n■\n■Environment. The state of the system when the fault or failure occurs may \nalso affect the desired system response. For example, if the system has al-\nready seen some faults and is operating in other than normal mode, it may \nbe desirable to shut it down totally. However, if this is the first fault ob-\nserved, some degradation of response time or function may be preferred. \n■\n■Response. There are a number of possible reactions to a system fault. \nFirst, the fault must be detected and isolated (correlated) before any other \nresponse is possible. (One exception to this is when the fault is prevented \nbefore it occurs.) After the fault is detected, the system must recover from \nit. Actions associated with these possibilities include logging the failure, \nnotifying selected users or other systems, taking actions to limit the damage \ncaused by the fault, switching to a degraded mode with either less capacity \nor less function, shutting down external systems, or becoming unavailable \nduring repair.\n■\n■Response measure. The response measure can specify an availability per-\ncentage, or it can specify a time to detect the fault, time to repair the fault, \ntimes or time intervals during which the system must be available, or the \nduration for which the system must be available.\n\n\n86 \nPart Two  Quality Attributes\t\n5—Availability\nFigure 5.3 shows a concrete scenario generated from the general scenario: The \nheartbeat monitor determines that the server is nonresponsive during normal opera-\ntions. The system informs the operator and continues to operate with no downtime.\nTable 5.3  Availability General Scenario \nPortion of \nScenario\nPossible Values\nSource\nInternal/external: people, hardware, software, physical infrastructure, \nphysical environment\nStimulus\nFault: omission, crash, incorrect timing, incorrect response\nArtifact\nProcessors, communication channels, persistent storage, processes\nEnvironment Normal operation, startup, shutdown, repair mode, degraded operation, \noverloaded operation\nResponse\nPrevent the fault from becoming a failure\nDetect the fault:\n■\n■\nLog the fault\n■\n■\n\u0007Notify appropriate entities (people or systems)\nRecover from the fault:\n■\n■\nDisable source of events causing the fault\n■\n■\nBe temporarily unavailable while repair is being effected\n■\n■\nFix or mask the fault/failure or contain the damage it causes\n■\n■\nOperate in a degraded mode while repair is being effected\nResponse \nMeasure\nTime or time interval when the system must be available\nAvailability percentage (e.g., 99.999%)\nTime to detect the fault\nTime to repair the fault\nTime or time interval in which system can be in degraded mode\nProportion (e.g., 99%) or rate (e.g., up to 100 per second) of a certain \nclass of faults that the system prevents, or handles without failing\nStimulus:\nServer\nUnresponsive\nResponse:\nInform \nOperator\nContinue\nto Operate\nResponse \nMeasure:\nNo Downtime\nSource:\nHeartbeat\nMonitor\nArtifact:\nProcess\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nFigure 5.3  Sample concrete availability scenario\n\n\n5.2  Tactics for Availability\n87\n5.2  Tactics for Availability\nA failure occurs when the system no longer delivers a service that is consistent \nwith its specification; this failure is observable by the system’s actors. A fault \n(or combination of faults) has the potential to cause a failure. Availability tac-\ntics, therefore, are designed to enable a system to endure system faults so that a \nservice being delivered by the system remains compliant with its specification. \nThe tactics we discuss in this section will keep faults from becoming failures or \nat least bound the effects of the fault and make repair possible. We illustrate this \napproach in Figure 5.4.\nAvailability tactics may be categorized as addressing one of three catego-\nries: fault detection, fault recovery, and fault prevention. The tactics categoriza-\ntion for availability is shown in Figure 5.5 (on the next page). Note that it is often \nthe case that these tactics will be provided for you by a software infrastructure, \nsuch as a middleware package, so your job as an architect is often one of choos-\ning and assessing (rather than implementing) the right availability tactics and the \nright combination of tactics. \nFault\nFault Masked\nor Repair Made\nTactics\nto Control\nAvailability\nFigure 5.4  Goal of availability tactics\nDetect Faults\nBefore any system can take action regarding a fault, the presence of the fault \nmust be detected or anticipated. Tactics in this category include the following:\n■\n■Ping/echo refers to an asynchronous request/response message pair ex-\nchanged between nodes, used to determine reachability and the round-trip \ndelay through the associated network path. But the echo also determines \nthat the pinged component is alive and responding correctly. The ping is \n\n\n88 \nPart Two  Quality Attributes\t\n5—Availability\noften sent by a system monitor. Ping/echo requires a time threshold to be \nset; this threshold tells the pinging component how long to wait for the \necho before considering the pinged component to have failed (“timed out”). \nStandard implementations of ping/echo are available for nodes intercon-\nnected via IP.\n■\n■Monitor. A monitor is a component that is used to monitor the state of \nhealth of various other parts of the system: processors, processes, I/O, \nmemory, and so on. A system monitor can detect failure or congestion in \nthe network or other shared resources, such as from a denial-of-service \nattack. It orchestrates software using other tactics in this category to detect \nAvailability Tactics\nDetect Faults\nPrevent Faults\nPing / Echo\nRemoval from\nService\nMonitor\nTransactions\nPredictive\nModel\nRecover from Faults\nHeartbeat\nPreparation\nand Repair\nReintroduction\nActive\nRedundancy\nPassive\nRedundancy\nSpare\nEscalating\nRestart\nException\nHandling\nShadow\nNon-Stop\nForwarding\nState\nResynchronization\nException\nPrevention\nFault\nFault\nMasked\nor\nRepair\nMade\nTimestamp\nSanity\nChecking\nCondition\nMonitoring\nVoting\nException\nDetection\nSelf-Test\nRollback\nSoftware\nUpgrade\nRetry\nIgnore Faulty\nBehavior\nDegradation\nReconfiguration\nIncrease\nCompetence Set\nFigure 5.5  Availability tactics\n\n\n5.2  Tactics for Availability\n89\nmalfunctioning components. For example, the system monitor can initiate \nself-tests, or be the component that detects faulty time stamps or missed \nheartbeats.1\n■\n■Heartbeat is a fault detection mechanism that employs a periodic message \nexchange between a system monitor and a process being monitored. A \nspecial case of heartbeat is when the process being monitored periodically \nresets the watchdog timer in its monitor to prevent it from expiring and thus \nsignaling a fault. For systems where scalability is a concern, transport and \nprocessing overhead can be reduced by piggybacking heartbeat messages \non to other control messages being exchanged between the process being \nmonitored and the distributed system controller. The big difference between \nheartbeat and ping/echo is who holds the responsibility for initiating the \nhealth check—the monitor or the component itself.\n■\n■Time stamp. This tactic is used to detect incorrect sequences of events, pri-\nmarily in distributed message-passing systems. A time stamp of an event \ncan be established by assigning the state of a local clock to the event imme-\ndiately after the event occurs. Simple sequence numbers can also be used \nfor this purpose, if time information is not important.\n■\n■Sanity checking checks the validity or reasonableness of specific operations \nor outputs of a component. This tactic is typically based on a knowledge of \nthe internal design, the state of the system, or the nature of the information \nunder scrutiny. It is most often employed at interfaces, to examine a specific \ninformation flow. \n■\n■Condition monitoring involves checking conditions in a process or device, \nor validating assumptions made during the design. By monitoring condi-\ntions, this tactic prevents a system from producing faulty behavior. The \ncomputation of checksums is a common example of this tactic. However, \nthe monitor must itself be simple (and, ideally, provable) to ensure that it \ndoes not introduce new software errors. \n■\n■Voting. The most common realization of this tactic is referred to as triple \nmodular redundancy (TMR), which employs three components that do the \nsame thing, each of which receives identical inputs, and forwards their out-\nput to voting logic, used to detect any inconsistency among the three output \nstates. Faced with an inconsistency, the voter reports a fault. It must also \ndecide what output to use. It can let the majority rule, or choose some com-\nputed average of the disparate outputs. This tactic depends critically on the \nvoting logic, which is usually realized as a simple, rigorously reviewed and \ntested singleton so that the probability of error is low. \n1.  When the detection mechanism is implemented using a counter or timer that is periodically reset, \nthis specialization of system monitor is referred to as a “watchdog.” During nominal operation, the \nprocess being monitored will periodically reset the watchdog counter/timer as part of its signal that \nit’s working correctly; this is sometimes referred to as “petting the watchdog.”\n\n\n90 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Replication is the simplest form of voting; here, the components are exact \nclones of each other. Having multiple copies of identical components can \nbe effective in protecting against random failures of hardware, but this \ncannot protect against design or implementation errors, in hardware or \nsoftware, because there is no form of diversity embedded in this tactic. \n■\n■Functional redundancy is a form of voting intended to address the issue \nof common-mode failures (design or implementation faults) in hardware \nor software components. Here, the components must always give the \nsame output given the same input, but they are diversely designed and \ndiversely implemented. \n■\n■Analytic redundancy permits not only diversity among components’ pri-\nvate sides, but also diversity among the components’ inputs and outputs. \nThis tactic is intended to tolerate specification errors by using separate \nrequirement specifications. In embedded systems, analytic redundancy \nalso helps when some input sources are likely to be unavailable at times. \nFor example, avionics programs have multiple ways to compute aircraft \naltitude, such as using barometric pressure, the radar altimeter, and geo-\nmetrically using the straight-line distance and look-down angle of a point \nahead on the ground. The voter mechanism used with analytic redun-\ndancy needs to be more sophisticated than just letting majority rule or \ncomputing a simple average. It may have to understand which sensors are \ncurrently reliable or not, and it may be asked to produce a higher-fidelity \nvalue than any individual component can, by blending and smoothing \nindividual values over time. \n■\n■Exception detection refers to the detection of a system condition that alters \nthe normal flow of execution. The exception detection tactic can be further \nrefined:\n■\n■System exceptions will vary according to the processor hardware architec-\nture employed and include faults such as divide by zero, bus and address \nfaults, illegal program instructions, and so forth. \n■\n■The parameter fence tactic incorporates an a priori data pattern (such as \n0xDEADBEEF) placed immediately after any variable-length parameters \nof an object. This allows for runtime detection of overwriting the memory \nallocated for the object’s variable-length parameters. \n■\n■Parameter typing employs a base class that defines functions that add, \nfind, and iterate over type-length-value (TLV) formatted message param-\neters. Derived classes use the base class functions to implement functions \nthat provide parameter typing according to each parameter’s structure. \nUse of strong typing to build and parse messages results in higher avail-\nability than implementations that simply treat messages as byte buckets. \nOf course, all design involves tradeoffs. When you employ strong typing, \nyou typically trade higher availability against ease of evolution.\n",
      "page_number": 97
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 111-119)",
      "start_page": 111,
      "end_page": 119,
      "detection_method": "topic_boundary",
      "content": "5.2  Tactics for Availability\n91\n■\n■Timeout is a tactic that raises an exception when a component detects \nthat it or another component has failed to meet its timing constraints. For \nexample, a component awaiting a response from another component can \nraise an exception if the wait time exceeds a certain value. \n■\n■Self-test. Components (or, more likely, whole subsystems) can run proce-\ndures to test themselves for correct operation. Self-test procedures can be \ninitiated by the component itself, or invoked from time to time by a system \nmonitor. These may involve employing some of the techniques found in \ncondition monitoring, such as checksums.\nRecover from Faults\nRecover-from-faults tactics are refined into preparation-and-repair tactics and \nreintroduction tactics. The latter are concerned with reintroducing a failed (but \nrehabilitated) component back into normal operation.\nPreparation-and-repair tactics are based on a variety of combinations of re-\ntrying a computation or introducing redundancy. They include the following:\n■\n■Active redundancy (hot spare). This refers to a configuration where all of \nthe nodes (active or redundant spare) in a protection group2 receive and \nprocess identical inputs in parallel, allowing the redundant spare(s) to main-\ntain synchronous state with the active node(s). Because the redundant spare \npossesses an identical state to the active processor, it can take over from a \nfailed component in a matter of milliseconds. The simple case of one active \nnode and one redundant spare node is commonly referred to as 1+1 (“one \nplus one”) redundancy. Active redundancy can also be used for facilities \nprotection, where active and standby network links are used to ensure high-\nly available network connectivity. \n■\n■Passive redundancy (warm spare). This refers to a configuration where \nonly the active members of the protection group process input traffic; \none of their duties is to provide the redundant spare(s) with periodic state \nupdates. Because the state maintained by the redundant spares is only \nloosely coupled with that of the active node(s) in the protection group \n(with the looseness of the coupling being a function of the checkpointing \nmechanism employed between active and redundant nodes), the redundant \nnodes are referred to as warm spares. Depending on a system’s availability \nrequirements, passive redundancy provides a solution that achieves a bal-\nance between the more highly available but more compute-intensive (and \nexpensive) active redundancy tactic and the less available but significantly \nless complex cold spare tactic (which is also significantly cheaper). (For an \n2.  A protection group is a group of processing nodes where one or more nodes are “active,” with the \nremaining nodes in the protection group serving as redundant spares.\n\n\n92 \nPart Two  Quality Attributes\t\n5—Availability\nexample of implementing passive redundancy, see the section on code tem-\nplates in Chapter 19.) \n■\n■Spare (cold spare). Cold sparing refers to a configuration where the re-\ndundant spares of a protection group remain out of service until a fail-over \noccurs, at which point a power-on-reset procedure is initiated on the re-\ndundant spare prior to its being placed in service. Due to its poor recovery \nperformance, cold sparing is better suited for systems having only high-re-\nliability (MTBF) requirements as opposed to those also having high-avail-\nability requirements.\n■\n■Exception handling. Once an exception has been detected, the system must \nhandle it in some fashion. The easiest thing it can do is simply to crash, but \nof course that’s a terrible idea from the point of availability, usability, test-\nability, and plain good sense. There are much more productive possibilities. \nThe mechanism employed for exception handling depends largely on the \nprogramming environment employed, ranging from simple function return \ncodes (error codes) to the use of exception classes that contain information \nhelpful in fault correlation, such as the name of the exception thrown, the \norigin of the exception, and the cause of the exception thrown. Software \ncan then use this information to mask the fault, usually by correcting the \ncause of the exception and retrying the operation.\n■\n■Rollback. This tactic permits the system to revert to a previous known good \nstate, referred to as the “rollback line”—rolling back time—upon the detec-\ntion of a failure. Once the good state is reached, then execution can contin-\nue. This tactic is often combined with active or passive redundancy tactics \nso that after a rollback has occurred, a standby version of the failed compo-\nnent is promoted to active status. Rollback depends on a copy of a previous \ngood state (a checkpoint) being available to the components that are rolling \nback. Checkpoints can be stored in a fixed location and updated at regular \nintervals, or at convenient or significant times in the processing, such as at \nthe completion of a complex operation. \n■\n■Software upgrade is another preparation-and-repair tactic whose goal is to \nachieve in-service upgrades to executable code images in a non-service-af-\nfecting manner. This may be realized as a function patch, a class patch, \nor a hitless in-service software upgrade (ISSU). A function patch is used \nin procedural programming and employs an incremental linker/loader to \nstore an updated software function into a pre-allocated segment of target \nmemory. The new version of the software function will employ the entry \nand exit points of the deprecated function. Also, upon loading the new \nsoftware function, the symbol table must be updated and the instruction \ncache invalidated. The class patch tactic is applicable for targets executing \nobject-oriented code, where the class definitions include a back-door mech-\nanism that enables the runtime addition of member data and functions. Hit-\nless in-service software upgrade leverages the active redundancy or passive \n\n\n5.2  Tactics for Availability\n93\nredundancy tactics to achieve non-service-affecting upgrades to software \nand associated schema. In practice, the function patch and class patch are \nused to deliver bug fixes, while the hitless in-service software upgrade is \nused to deliver new features and capabilities.\n■\n■Retry. The retry tactic assumes that the fault that caused a failure is tran-\nsient and retrying the operation may lead to success. This tactic is used in \nnetworks and in server farms where failures are expected and common. \nThere should be a limit on the number of retries that are attempted before a \npermanent failure is declared.\n■\n■Ignore faulty behavior. This tactic calls for ignoring messages sent from a \nparticular source when we determine that those messages are spurious. For \nexample, we would like to ignore the messages of an external component \nlaunching a denial-of-service attack by establishing Access Control List \nfilters, for example.\n■\n■The degradation tactic maintains the most critical system functions in the \npresence of component failures, dropping less critical functions. This is \ndone in circumstances where individual component failures gracefully re-\nduce system functionality rather than causing a complete system failure. \n■\n■Reconfiguration attempts to recover from component failures by reassign-\ning responsibilities to the (potentially restricted) resources left functioning, \nwhile maintaining as much functionality as possible.\nReintroduction is where a failed component is reintroduced after it has been \ncorrected. Reintroduction tactics include the following:\n■\n■The shadow tactic refers to operating a previously failed or in-service up-\ngraded component in a “shadow mode” for a predefined duration of time \nprior to reverting the component back to an active role. During this duration \nits behavior can be monitored for correctness and it can repopulate its state \nincrementally.\n■\n■State resynchronization is a reintroduction partner to the active redun-\ndancy and passive redundancy preparation-and-repair tactics. When used \nalongside the active redundancy tactic, the state resynchronization occurs \norganically, because the active and standby components each receive and \nprocess identical inputs in parallel. In practice, the states of the active and \nstandby components are periodically compared to ensure synchronization. \nThis comparison may be based on a cyclic redundancy check calculation \n(checksum) ‎or, for systems providing safety-critical services, a message \ndigest calculation (a one-way hash function). When used alongside the pas-\nsive redundancy (warm spare) tactic, state resynchronization is based solely \non periodic state information transmitted from the active component(s) to \nthe standby component(s), typically via checkpointing. A special case of \nthis tactic is found in stateless services, whereby any resource can handle a \nrequest from another (failed) resource.\n\n\n94 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Escalating restart is a reintroduction tactic that allows the system to recov-\ner from faults by varying the granularity of the component(s) restarted and \nminimizing the level of service affected. For example, consider a system \nthat supports four levels of restart, as follows. The lowest level of restart \n(call it Level 0), and hence having the least impact on services, employs \npassive redundancy (warm spare), where all child threads of the faulty \ncomponent are killed and recreated. In this way, only data associated with \nthe child threads is freed and reinitialized. The next level of restart (Level \n1) frees and reinitializes all unprotected memory (protected memory would \nremain untouched). The next level of restart (Level 2) frees and reinitializes \nall memory, both protected and unprotected, forcing all applications to re-\nload and reinitialize. And the final level of restart (Level 3) would involve \ncompletely reloading and reinitializing the executable image and associated \ndata segments. Support for the escalating restart tactic is particularly useful \nfor the concept of graceful degradation, where a system is able to degrade \nthe services it provides while maintaining support for mission-critical or \nsafety-critical applications.\n■\n■Non-stop forwarding (NSF) is a concept that originated in router design. In \nthis design functionality is split into two parts: supervisory, or control plane \n(which manages connectivity and routing information), and data plane \n(which does the actual work of routing packets from sender to receiver). If \na router experiences the failure of an active supervisor, it can continue for-\nwarding packets along known routes—with neighboring routers—while the \nrouting protocol information is recovered and validated. When the control \nplane is restarted, it implements what is sometimes called “graceful restart,” \nincrementally rebuilding its routing protocol database even as the data \nplane continues to operate.\nPrevent Faults\nInstead of detecting faults and then trying to recover from them, what if your sys-\ntem could prevent them from occurring in the first place? Although this sounds \nlike some measure of clairvoyance might be required, it turns out that in many \ncases it is possible to do just that.3\n■\n■Removal from service. This tactic refers to temporarily placing a system \ncomponent in an out-of-service state for the purpose of mitigating potential \nsystem failures. One example involves taking a component of a system out \nof service and resetting the component in order to scrub latent faults (such \n3.  These tactics deal with runtime means to prevent faults from occurring. Of course, an excellent \nway to prevent faults—at least in the system you’re building, if not in systems that your system must \ninteract with—is to produce high-quality code. This can be done by means of code inspections, pair \nprogramming, solid requirements reviews, and a host of other good engineering practices. \n\n\n5.2  Tactics for Availability\n95\nas memory leaks, fragmentation, or soft errors in an unprotected cache) be-\nfore the accumulation of faults affects service (resulting in system failure). \nAnother term for this tactic is software rejuvenation.\n■\n■Transactions. Systems targeting high-availability services leverage transac-\ntional semantics to ensure that asynchronous messages exchanged between \ndistributed components are atomic, consistent, isolated, and durable. These \nfour properties are called the “ACID properties.” The most common realiza-\ntion of the transactions tactic is “two-phase commit” (a.k.a. 2PC) protocol. \nThis tactic prevents race conditions caused by two processes attempting to \nupdate the same data item.\n■\n■Predictive model. A predictive model, when combined with a monitor, is \nemployed to monitor the state of health of a system process to ensure that \nthe system is operating within its nominal operating parameters, and to take \ncorrective action when conditions are detected that are predictive of likely \nfuture faults. The operational performance metrics monitored are used to \npredict the onset of faults; examples include session establishment rate (in \nan HTTP server), threshold crossing (monitoring high and low water marks \nfor some constrained, shared resource), or maintaining statistics for process \nstate (in service, out of service, under maintenance, idle), message queue \nlength statistics, and so on.\n■\n■Exception prevention. This tactic refers to techniques employed for the pur-\npose of preventing system exceptions from occurring. The use of exception \nclasses, which allows a system to transparently recover from system excep-\ntions, was discussed previously. Other examples of exception prevention \ninclude abstract data types, such as smart pointers, and the use of wrappers \nto prevent faults, such as dangling pointers and semaphore access violations \nfrom occurring. Smart pointers prevent exceptions by doing bounds check-\ning on pointers, and by ensuring that resources are automatically deallocat-\ned when no data refers to it. In this way resource leaks are avoided.\n■\n■Increase competence set. A program’s competence set is the set of states in \nwhich it is “competent” to operate. For example, the state when the denom-\ninator is zero is outside the competence set of most divide programs. When \na component raises an exception, it is signaling that it has discovered itself \nto be outside its competence set; in essence, it doesn’t know what to do and \nis throwing in the towel. Increasing a component’s competence set means \ndesigning it to handle more cases—faults—as part of its normal operation. \nFor example, a component that assumes it has access to a shared resource \nmight throw an exception if it discovers that access is blocked. Another \ncomponent might simply wait for access, or return immediately with an \nindication that it will complete its operation on its own the next time it does \nhave access. In this example, the second component has a larger compe-\ntence set than the first. \n\n\n96 \nPart Two  Quality Attributes\t\n5—Availability\n5.3  A Design Checklist for Availability\nTable 5.4 is a checklist to support the design and analysis process for availability.\nTable 5.4  Checklist to Support the Design and Analysis Process for \nAvailability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine the system responsibilities that need to be highly \navailable. Within those responsibilities, ensure that additional \nresponsibilities have been allocated to detect an omission, \ncrash, incorrect timing, or incorrect response. Additionally, \nensure that there are responsibilities to do the following:\n■\n■\nLog the fault\n■\n■\nNotify appropriate entities (people or systems)\n■\n■\nDisable the source of events causing the fault\n■\n■\nBe temporarily unavailable\n■\n■\nFix or mask the fault/failure\n■\n■\nOperate in a degraded mode\nCoordination Model\nDetermine the system responsibilities that need to be highly \navailable. With respect to those responsibilities, do the \nfollowing: \n■\n■\nEnsure that coordination mechanisms can detect an \nomission, crash, incorrect timing, or incorrect response. \nConsider, for example, whether guaranteed delivery is \nnecessary. Will the coordination work under conditions of \ndegraded communication?\n■\n■\nEnsure that coordination mechanisms enable the logging \nof the fault, notification of appropriate entities, disabling of \nthe source of the events causing the fault, fixing or masking \nthe fault, or operating in a degraded mode.\n■\n■\nEnsure that the coordination model supports the replace-\nment of the artifacts used (processors, communications \nchannels, persistent storage, and processes). For exam-\nple, does replacement of a server allow the system to \ncontinue to operate? \n■\n■\nDetermine if the coordination will work under conditions \nof degraded communication, at startup/shutdown, in re-\npair mode, or under overloaded operation. For example, \nhow much lost information can the coordination model \nwithstand and with what consequences?\nData Model\nDetermine which portions of the system need to be highly \navailable. Within those portions, determine which data \nabstractions, along with their operations or their properties, \ncould cause a fault of omission, a crash, incorrect timing \nbehavior, or an incorrect response.\nFor those data abstractions, operations, and properties, \nensure that they can be disabled, be temporarily unavailable, \nor be fixed or masked in the event of a fault.\nFor example, ensure that write requests are cached if a \nserver is temporarily unavailable and performed when the \nserver is returned to service.\n\n\n5.3  A Design Checklist for Availability\n97\nCategory\nChecklist\nMapping among \nArchitectural Elements\nDetermine which artifacts (processors, communication \nchannels, persistent storage, or processes) may produce \na fault: omission, crash, incorrect timing, or incorrect \nresponse. \nEnsure that the mapping (or remapping) of architectural \nelements is flexible enough to permit the recovery from the \nfault. This may involve a consideration of the following:\n■\n■\nWhich processes on failed processors need to be reas-\nsigned at runtime \n■\n■\nWhich processors, data stores, or communication chan-\nnels can be activated or reassigned at runtime\n■\n■\nHow data on failed processors or storage can be served \nby replacement units\n■\n■\nHow quickly the system can be reinstalled based on the \nunits of delivery provided\n■\n■\nHow to (re)assign runtime elements to processors, com-\nmunication channels, and data stores\n■\n■\nWhen employing tactics that depend on redundancy of \nfunctionality, the mapping from modules to redundant \ncomponents is important. For example, it is possible to \nwrite one module that contains code appropriate for both \nthe active component and backup components in a pro-\ntection group. \nResource  \nManagement\nDetermine what critical resources are necessary to \ncontinue operating in the presence of a fault: omission, \ncrash, incorrect timing, or incorrect response. Ensure \nthere are sufficient remaining resources in the event of a \nfault to log the fault; notify appropriate entities (people or \nsystems); disable the source of events causing the fault; \nbe temporarily unavailable; fix or mask the fault/failure; \noperate normally, in startup, shutdown, repair mode, \ndegraded operation, and overloaded operation.\nDetermine the availability time for critical resources, what \ncritical resources must be available during specified time \nintervals, time intervals during which the critical resources \nmay be in a degraded mode, and repair time for critical \nresources. Ensure that the critical resources are available \nduring these time intervals.\nFor example, ensure that input queues are large enough \nto buffer anticipated messages if a server fails so that the \nmessages are not permanently lost.\ncontinues\n\n\n98 \nPart Two  Quality Attributes\t\n5—Availability\nTable 5.4  Checklist to Support the Design and Analysis Process for \nAvailability, continued\nCategory\nChecklist\nBinding Time\nDetermine how and when architectural elements are bound. \nIf late binding is used to alternate between components \nthat can themselves be sources of faults (e.g., processes, \nprocessors, communication channels), ensure the chosen \navailability strategy is sufficient to cover faults introduced by \nall sources. For example:\n■\n■\nIf late binding is used to switch between artifacts such \nas processors that will receive or be the subject of faults, \nwill the chosen fault detection and recovery mechanisms \nwork for all possible bindings?\n■\n■\nIf late binding is used to change the definition or toler-\nance of what constitutes a fault (e.g., how long a process \ncan go without responding before a fault is assumed), \nis the recovery strategy chosen sufficient to handle all \ncases? For example, if a fault is flagged after 0.1 millisec-\nonds, but the recovery mechanism takes 1.5 seconds to \nwork, that might be an unacceptable mismatch.\n■\n■\nWhat are the availability characteristics of the late bind-\ning mechanism itself? Can it fail?\nChoice of Technology\nDetermine the available technologies that can (help) detect \nfaults, recover from faults, or reintroduce failed components. \nDetermine what technologies are available that help the \nresponse to a fault (e.g., event loggers). \nDetermine the availability characteristics of chosen \ntechnologies themselves: What faults can they recover \nfrom? What faults might they introduce into the system? \n5.4  Summary\nAvailability refers to the ability of the system to be available for use, especially \nafter a fault occurs. The fault must be recognized (or prevented) and then the \nsystem must respond in some fashion. The response desired will depend on the \ncriticality of the application and the type of fault and can range from “ignore it” \nto “keep on going as if it didn’t occur.”\nTactics for availability are categorized into detect faults, recover from faults \nand prevent faults. Detection tactics depend, essentially, on detecting signs of life \nfrom various components. Recovery tactics are some combination of retrying an \noperation or maintaining redundant data or computations. Prevention tactics de-\npend either on removing elements from service or utilizing mechanisms to limit \nthe scope of faults.\n\n\n5.5  For Further Reading\n99\nAll of the availability tactics involve the coordination model because the \ncoordination model must be aware of faults that occur to generate an appropriate \nresponse. \n5.5  For Further Reading\nPatterns for availability:\n■\n■You can find patterns for fault tolerance in [Hanmer 07]. \nTactics for availability, overall:\n■\n■A more detailed discussion of some of the availability tactics in this chapter is \ngiven in [Scott 09]. This is the source of much of the material in this chapter.\n■\n■The Internet Engineering Task Force has promulgated a number of stan-\ndards supporting availability tactics. These standards include non-stop for-\nwarding [IETF 04], ping/echo ICMPv6 [IETF 06b], echo request/response), \nand MPLS (LSP Ping) networks [IETF 06a].\nTactics for availability, fault detection:\n■\n■The parameter fence tactic was first used (to our knowledge) in the Control \nData Series computers of the late 1960s. \n■\n■Triple modular redundancy (TMR), part of the voting tactic, was developed \nin the early 1960s by Lyons [Lyons 62].\n■\n■The fault detection tactic of voting is based on the fundamental contribu-\ntions to automata theory by Von Neumann, who demonstrated how systems \nhaving a prescribed reliability could be built from unreliable components \n[Von Neumann 56]. \nTactics for availability, fault recovery:\n■\n■Standards-based realizations of active redundancy exist for protecting net-\nwork links (i.e., facilities) at both the physical layer [Bellcore 99, Telcordia \n00] and the network/link layer [IETF 05].\n■\n■Exception handlinghas been written about by [Powel Douglass 99]. Soft-\nware can then use this information to mask the fault, usually by correcting \nthe cause of the exception and retrying the operation.\n■\n■[Morelos-Zaragoza 06] and [Schneier 96] have written about the compari-\nson of state during resynchronization. \n■\n■Some examples of how a system can degrade through use (degradation) are \ngiven in [Nygard 07].\n■\n■[Utas 05] has written about escalating restart. \n",
      "page_number": 111
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 120-127)",
      "start_page": 120,
      "end_page": 127,
      "detection_method": "topic_boundary",
      "content": "100 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Mountains of papers have been written about parameter typing, but [Utas \n05] writes about it in the context of availability (as opposed to bug preven-\ntion, its usual context).\n■\n■Hardware engineers often use preparation-and-repair tactics. Examples in-\nclude error detection and correction (EDAC) coding, forward error correction \n(FEC), and temporal redundancy. EDAC coding is typically used to protect \ncontrol memory structures in high-availability distributed real-time embedded \nsystems [Hamming 80]. Conversely, FEC coding is typically employed to \nrecover from physical-layer errors occurring on external network links More-\nlos-Zaragoza 06]. Temporal redundancy involves sampling spatially redundant \nclock or data lines at time intervals that exceed the pulse width of any transient \npulse to be tolerated, and then voting out any defects detected [Mavis 02].\nTactics for availability, fault prevention:\n■\n■Parnas and Madey have written about increasing an element’s competence \nset [Parnas 95].\n■\n■The ACID properties, important in the transactions tactic, were introduced \nby Gray in the 1970s and discussed in depth in [Gray 93]. \nAnalysis:\n■\n■Fault tree analysis dates from the early 1960s, but the granddaddy of re-\nsources for it is the U.S. Nuclear Regulatory Commission’s “Fault Tree \nHandbook,” published in 1981 [Vesely 81]. NASA’s 2002 “Fault Tree \nHandbook with Aerospace Applications” [Vesely 02] is an updated compre-\nhensive primer of the NRC handbook, and the source for the notation used \nin this chapter. Both are available online as downloadable PDF files. \n5.6  Discussion Questions\n1.\t\nWrite a set of concrete scenarios for availability using each of the possible \nresponses in the general scenario.\n2.\t\nWrite a concrete availability scenario for the software for a (hypothetical) \npilotless passenger aircraft.\n3.\t\nWrite a concrete availability scenario for a program like Microsoft Word.\n4.\t\nRedundancy is often cited as a key strategy for achieving high availability. \nLook at the tactics presented in this chapter and decide how many of them \nexploit some form of redundancy and how many do not.\n5.\t\nHow does availability trade off against modifiability? How would you make \na change to a system that is required to have “24/7” availability (no sched-\nuled or unscheduled downtime, ever)?\n\n\n5.6  Discussion Questions\n101\n6.\t\nCreate a fault tree for an automatic teller machine. Include faults dealing \nwith hardware component failure, communications failure, software failure, \nrunning out of supplies, user errors, and security attacks. How would you \nmodify your automatic teller machine design to accommodate these faults?\n7.\t\nConsider the fault detection tactics (ping/echo, heartbeat, system monitor, \nvoting, and exception detection). What are the performance implications of \nusing these tactics?\n\n\nThis page intentionally left blank \n\n\n103\n6\nInteroperability\nWith Liming Zhu\nThe early bird (A) arrives and catches worm (B), pulling \nstring (C) and shooting off pistol (D). Bullet (E) bursts \nballoon (F), dropping brick (G) on bulb (H) of atomizer \n(I) and shooting perfume (J) on sponge (K). As sponge \ngains in weight, it lowers itself and pulls string (L), \nraising end of board (M). Cannon ball (N) drops on nose \nof sleeping gentleman. String tied to cannon ball releases \ncork (O) of vacuum bottle (P) and ice water falls on \nsleeper’s face to assist the cannon ball in its good work.\n—Rube Goldberg, instructions for “a simple alarm clock”\nInteroperability is about the degree to which two or more systems can usefully \nexchange meaningful information via interfaces in a particular context. The defi-\nnition includes not only having the ability to exchange data (syntactic interoper-\nability) but also having the ability to correctly interpret the data being exchanged \n(semantic interoperability). A system cannot be interoperable in isolation. Any \ndiscussion of a system’s interoperability needs to identify with whom, with what, \nand under what circumstances—hence, the need to include the context.\nInteroperability is affected by the systems expected to interoperate. If we \nalready know the interfaces of external systems with which our system will in-\nteroperate, then we can design that knowledge into the system. Or we can design \nour system to interoperate in a more generic fashion, so that the identity and the \nservices that another system provides can be bound later in the life cycle, at build \ntime or runtime.\nLike all quality attributes, interoperability is not a yes-or-no proposition but \nhas shades of meaning. There are several characterizing frameworks for interop-\nerability, all of which seem to define five levels of interoperability “maturity” \n(see the “For Further Reading” section at the end of this chapter for a pointer). \nThe lowest level signifies systems that do not share data at all, or do not do so \n\n\n104 \nPart Two  Quality Attributes\t\n6—Interoperability\nwith any success. The highest level signifies systems that work together seam-\nlessly, never make any mistakes interpreting each other’s communications, and \nshare the same underlying semantic model of the world in which they work. \n“Exchanging Information via Interfaces”\nInteroperability, as we said, is about two or more systems exchanging \ninformation via interfaces. \nAt this point, we need to clarify two critical concepts central to this dis-\ncussion and emphasize that we are taking a broad view of each.\nThe first is what it means to “exchange information.” This can mean \nsomething as simple as program A calling program B with some param-\neters. However, two systems (or parts of a system) can exchange infor-\nmation even if they never communicate directly with each other. Did you \never have a conversation like the following in junior high school? “Charlene \nsaid that Kim told her that Trevor heard that Heather wants to come to \nyour party.” Of course, junior high school protocol would preclude the \npossibility of responding directly to Heather. Instead, your response (if you \nlike Heather) might be, “Cool,” which would make its way back through \nCharlene, Kim, and Trevor. You and Heather exchanged information, but \nnever talked to each other. (We hope you got to talk to each other at the \nparty.)\nEntities can exchange information in even less direct ways. If I have an \nidea of a program’s behavior, and I design my program to work assuming \nthat behavior, the two programs have also exchanged information—just not \nat runtime.\nOne of the more infamous software disasters in history occurred when \nan antimissile system failed to intercept an incoming ballistic rocket in \nOperation Desert Storm in 1991, resulting in 28 fatalities. One of the mis-\nsile’s software components “expected” to be shut down and restarted peri-\nodically, so it could recalibrate its orientation framework from a known initial \npoint. The software had been running for some 100 hours when the missile \nwas launched, and calculation errors had accumulated to the point where \nthe software component’s idea of its orientation had wandered hopelessly \naway from truth.\nSystems (or components within systems) often have or embody ex-\npectations about the behaviors of its “information exchange” partners. \nThe assumption of everything interacting with the errant component in the \npreceding example was that its accuracy did not degrade over time. The \nresult was a system of parts that did not work together correctly to solve \nthe problem they were supposed to.\nThe second concept we need to stress is what we mean by “interface.” \nOnce again, we mean something beyond the simple case—a syntactic \ndescription of a component’s programs and the type and number of their \nparameters, most commonly realized as an API. That’s necessary for \n\n\n\t\n6—Interoperability  105\ninteroperability—heck, it’s necessary if you want your software to compile \nsuccessfully—but it’s not sufficient. To illustrate this concept, we’ll use an-\nother “conversation” analogy. Has your partner or spouse ever come home, \nslammed the door, and when you ask what’s wrong, replied “Nothing!”? \nIf so, then you should be able to appreciate the keen difference between \nsyntax and semantics and the role of expectations in understanding how an \nentity behaves. Because we want interoperable systems and components, \nand not simply ones that compile together nicely, we require a higher bar \nfor interfaces than just a statement of syntax. By “interface,” we mean the \nset of assumptions that you can safely make about an entity. For example, \nit’s a safe assumption that whatever’s wrong with your spouse/partner, \nit’s not “Nothing,” and you know that because that “interface” extends way \nbeyond just the words they say. And it’s also a safe assumption that nothing \nabout our missile component’s accuracy degradation over time was in its \nAPI, and yet that was a critical part of its interface.\n—PCC\nHere are some of the reasons you might want systems to interoperate:\n■\n■Your system provides a service to be used by a collection of unknown \nsystems. These systems need to interoperate with your system even though \nyou may know nothing about them. An example is a service such as Google \nMaps.\n■\n■You are constructing capabilities from existing systems. For example, one \nof the existing systems is responsible for sensing its environment, another \none is responsible for processing the raw data, a third is responsible for \ninterpreting the data, and a final one is responsible for producing and \ndistributing a representation of what was sensed. An example is a traffic \nsensing system where the input comes from individual vehicles, the raw \ndata is processed into common units of measurement, is interpreted and \nfused, and traffic congestion information is broadcast.\nThese examples highlight two important aspects of interoperability:\n1.\t\nDiscovery. The consumer of a service must discover (possibly at runtime, \npossibly prior to runtime) the location, identity, and the interface of the \nservice.\n2.\t\nHandling of the response. There are three distinct possibilities:\n■\n■The service reports back to the requester with the response.\n■\n■The service sends its response on to another system. \n■\n■The service broadcasts its response to any interested parties.\nThese elements, discovery and disposition of response, along with management \nof interfaces, govern our discussion of scenarios and tactics for interoperability.\n\n\n106 \nPart Two  Quality Attributes\t\n6—Interoperability\nSystems of Systems\nIf you have a group of systems that are interoperating to achieve a joint \npurpose, you have what is called a system of systems (SoS). An SoS is \nan arrangement of systems that results when independent and useful sys-\ntems are integrated into a larger system that delivers unique capabilities. \nTable 6.1 shows a categorization of SoSs.\nTable 6.1  Taxonomy of Systems of Systems*\nDirected\nSoS objectives, centralized management, funding, and \nauthority for the overall SoS are in place. Systems are \nsubordinated to the SoS.\nAcknowledged\nSoS objectives, centralized management, funding, and \nauthority in place. However, systems retain their own \nmanagement, funding, and authority in parallel with the \nSoS.\nCollaborative\nThere are no overall objectives, centralized \nmanagement, authority, responsibility, or funding at the \nSoS level. Systems voluntarily work together to address \nshared or common interests.\nVirtual\nLike collaborative, but systems don’t know about each \nother.\n*  The taxonomy shown is an extension of work done by Mark Maier in 1998.\nIn directed and acknowledged SoSs, there is a deliberate attempt to \ncreate an SoS. The key difference is that in the former, there is SoS-level \nmanagement that exercises control over the constituent systems, while in \nthe latter, the constituent systems retain a high degree of autonomy in their \nown evolution. Collaborative and virtual systems of systems are more ad \nhoc, absent an overarching authority or source of funding and, in the case \nof a virtual SoS, even absent the knowledge about the scope and member-\nship of the SoS.\nThe collaborative case is quite common. Consider the Google Maps ex-\nample from the introduction. Google is the manager and funding authority \nfor the map service. Each use of the maps in an application (an SoS) has \nits own management and funding authority, and there is no overall manage-\nment of all of the applications that use Google Maps. The various organiza-\ntions involved in the applications collaborate (either explicitly or implicitly) to \nenable the applications to work correctly.\nA virtual SoS involves large systems and is much more ad hoc. For \nexample, there are over 3,000 electric companies in the U.S. electric grid, \neach state has a public utility commission that oversees the utility companies \noperating in its state, and the federal Department of Energy provides some \nlevel of policy guidance. Many of the systems within the electric grid must \ninteroperate, but there is no management authority for the overall system.\n\n\n6.1  Interoperability General Scenario\n107\n6.1  Interoperability General Scenario\nThe following are the portions of an interoperability general scenario:\n■\n■Source of stimulus. A system that initiates a request. \n■\n■Stimulus. A request to exchange information among systems.\n■\n■Artifacts. The systems that wish to interoperate.\n■\n■Environment. The systems that wish to interoperate are discovered at run-\ntime or are known prior to runtime.\n■\n■Response. The request to interoperate results in the exchange of informa-\ntion. The information is understood by the receiving party both syntactical-\nly and semantically. Alternatively, the request is rejected and appropriate \nentities are notified. In either case, the request may be logged.\n■\n■Response measure. The percentage of information exchanges correctly \nprocessed or the percentage of information exchanges correctly rejected.\nFigure 6.1 gives an example: Our vehicle information system sends our cur-\nrent location to the traffic monitoring system. The traffic monitoring system com-\nbines our location with other information, overlays this information on a Goo-\ngle Map, and broadcasts it. Our location information is correctly included with a \nprobability of 99.9%.\nTable 6.2 presents the possible values for each portion of an interoperability \nscenario.\nStimulus:\nResponse:\nEnvironment:\nSystems known\nprior to run-time\nArtifact:\nResponse\nMeasure:\nSource\nof Stimulus:\n3\n2\n1\n4\nOur Vehicle \nInformation \nSystem\nCurrent \nLocation \nSent\nTraffic Monitor \nCombines Current \nLocation with Other \nInformation, \nOverlays on Google \nMaps, and \nBroadcasts\nOur Information \nIncluded Correctly \n99.9% of the Time\nTraffic Monitoring \nSystem\nFigure 6.1  Sample concrete interoperability scenario\n",
      "page_number": 120
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 128-137)",
      "start_page": 128,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "108 \nPart Two  Quality Attributes\t\n6—Interoperability\nTable 6.2  General Interoperability Scenario\nPortion of Scenario Possible Values\nSource\nA system initiates a request to interoperate with another \nsystem.\nStimulus\nA request to exchange information among system(s).\nArtifact\nThe systems that wish to interoperate.\nEnvironment\nSystem(s) wishing to interoperate are discovered at runtime or \nknown prior to runtime.\nResponse\nOne or more of the following:\n■\n■\nThe request is (appropriately) rejected and appropriate \nentities (people or systems) are notified.\n■\n■\nThe request is (appropriately) accepted and information is \nexchanged successfully.\n■\n■\nThe request is logged by one or more of the involved \nsystems.\nResponse Measure\nOne or more of the following:\n■\n■\nPercentage of information exchanges correctly processed \n■\n■\nPercentage of information exchanges correctly rejected \nSOAP vs. REST\nIf you want to allow web-based applications to interoperate, you have \ntwo major off-the-shelf technology options today: (1) WS* and SOAP \n(which once stood for “Simple Object Access Protocol,” but that acronym \nis no longer blessed) and (2) REST (which stands for “Representation \nState Transfer,” and therefore is sometimes spelled ReST). How can we \ncompare these technologies? What is each good for? What are the road \nhazards you need to be aware of? This is a bit of an apples-and-oranges \ncomparison, but I will try to sketch the landscape. \nSOAP is a protocol specification for XML-based information that distrib-\nuted applications can use to exchange information and hence interoperate. \nIt is most often accompanied by a set of SOA middleware interoperability \nstandards and compliant implementations, referred to (collectively) as WS*. \nSOAP and WS* together define many standards, including the following:\n■\n■\nAn infrastructure for service composition. SOAP can employ the Business \nProcess Execution Language (BPEL) as a way to let developers express \nbusiness processes that are implemented as WS* services. \n■\n■\nTransactions. There are several web-service standards for ensuring \nthat transactions are properly managed: WS-AT, WS-BA, WS-CAF, and \nWS-Transaction.\n■\n■\nService discovery. The Universal Description, Discovery and Integration \n(UDDI) language enables businesses to publish service listings and \ndiscover each other.\n\n\n6.1  Interoperability General Scenario\n109\n■\n■\nReliability. SOAP, by itself, does not ensure reliable message delivery. \nApplications that require such guarantees must use services compliant with \nSOAP’s reliability standard: WS-Reliability.\nSOAP is quite general and has its roots in a remote procedure call \n(RPC) model of interacting applications, although other models are cer-\ntainly possible. SOAP has a simple type system, comparable to that found \nin the major programming languages. SOAP relies on HTTP and RPC for \nmessage transmission, but it could, in theory, be implemented on top of \nany communication protocol. SOAP does not mandate a service’s method \nnames, addressing model, or procedural conventions. Thus, choosing \nSOAP buys little actual interoperability between applications—it is just \nan information exchange standard. The interacting applications need to \nagree on how to interpret the payload, which is where you get semantic \ninteroperability. \nREST, on the other hand, is a client-server-based architectural style that \nis structured around a small set of create, read, update, delete (CRUD) op-\nerations (called POST, GET, PUT, DELETE respectively in the REST world) \nand a single addressing scheme (based on a URI, or uniform resource \nidentifier). REST imposes few constraints on an architecture: SOAP offers \ncompleteness; REST offers simplicity. \nREST is about state and state transfer and views the web (and the ser-\nvices that service-oriented systems can string together) as a huge network \nof information that is accessible by a single URI-based addressing scheme. \nThere is no notion of type and hence no type checking in REST—it is up to \nthe applications to get the semantics of interaction right.\nBecause REST interfaces are so simple and general, any HTTP client \ncan talk to any HTTP server, using the REST operations (POST, GET, PUT, \nDELETE) with no further configuration. That buys you syntactic interopera-\nbility, but of course there must be organization-level agreement about what \nthese programs actually do and what information they exchange. That is, \nsemantic interoperability is not guaranteed between services just because \nboth have REST interfaces.\nREST, on top of HTTP, is meant to be self-descriptive and in the best \ncase is a stateless protocol. Consider the following example, in REST, of a \nphone book service that allows someone to look up a person, given some \nunique identifier for that person:\nhttp://www.XYZdirectory.com/phonebook/UserInfo/99999\nThe same simple lookup, implemented in SOAP, would be specified as \nsomething like the following:\n<?xml version=”1.0”?>\n<soap:Envelope xmlns:soap=http://www.w3.org/2001/  \n\t\n\t\n12/soap-envelope \n soap:encodingStyle=”http://www.w3.org/2001/12/ \n\t\n\t\nsoap-encoding”>\n\t\n<soap:Body pb=”http://www.XYZdirectory.com/ \n\t\n\t\nphonebook”>\n\n\n110 \nPart Two  Quality Attributes\t\n6—Interoperability\n\t\n\t\n<pb:GetUserInfo>\n\t\n\t\n\t\n<pb:UserIdentifier>99999</pb:UserIdentifier>\n\t\n\t\n</pb:GetUserInfo>\n\t\n</soap:Body>\n</soap:Envelope>\nOne aspect of the choice between SOAP and REST is whether you \nwant to accept the complexity and restrictions of SOAP+WSDL (the Web \nServices Description Language) to get more standardized interoperability \nor if you want to avoid the overhead by using REST, but perhaps benefit \nfrom less standardization. What are the other considerations? \nA message exchange in REST has somewhat fewer characters than a \nmessage exchange in SOAP. So one of the tradeoffs in the choice between \nREST and SOAP is the size of the individual messages. For systems \nexchanging a large number of messages, another tradeoff is between per-\nformance (favoring REST) and structured messages (favoring SOAP).\nThe decision to implement WS* or REST will depend on aspects such \nas the quality of service (QoS) required—WS* implementation has greater \nsupport for security, availability, and so on—and type of functionality. A \nRESTful implementation, because of its simplicity, is more appropriate for \nread-only functionality, typical of mashups, where there are minimal QoS \nrequirements and concerns.\nOK, so if you are building a service-based system, how do you choose? \nThe truth is, you don’t have to make a single choice, once and for all time; \neach technology is reasonably easy to use, at least for simple applications. \nAnd each has its strengths and weaknesses. Like everything else in archi-\ntecture, it’s all about the tradeoffs; your decision will likely hinge on the way \nthose tradeoffs affect your system in your context.\n—RK\n6.2  Tactics for Interoperability\nFigure 6.2 shows the goal of the set of interoperability tactics.\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nTactics\nto Control\nInteroperability\nFigure 6.2  Goal of interoperability tactics \n\n\n6.2  Tactics for Interoperability\n111\nWe identify two categories of interoperability tactics: locate and manage \ninterfaces. \nLocate\nThere is only one tactic in this category: discover service. It is used when the \nsystems that interoperate must be discovered at runtime.\n■\n■Discover service. Locate a service through searching a known directory ser-\nvice. (By “service,” we simply mean a set of capabilities that is accessible \nvia some kind of interface.) There may be multiple levels of indirection in \nthis location process—that is, a known location points to another location \nthat in turn can be searched for the service. The service can be located by \ntype of service, by name, by location, or by some other attribute.\nManage Interfaces\nManaging interfaces consists of two tactics: orchestrate and tailor interface. \n■\n■Orchestrate. Orchestrate is a tactic that uses a control mechanism to \ncoordinate and manage and sequence the invocation of particular services \n(which could be ignorant of each other). Orchestration is used when the \ninteroperating systems must interact in a complex fashion to accomplish a \ncomplex task; orchestration “scripts” the interaction. Workflow engines are \nan example of the use of the orchestrate tactic. The mediator design pattern \ncan serve this function for simple orchestration. Complex orchestration can \nbe specified in a language such as BPEL.\n■\n■Tailor interface. Tailor interface is a tactic that adds or removes capabilities \nto an interface. Capabilities such as translation, adding buffering, or \nsmoothing data can be added. Capabilities may be removed as well. An \nexample of removing capabilities is to hide particular functions from \nuntrusted users. The decorator pattern is an example of the tailor interface \ntactic. \nThe enterprise service bus that underlies many service-oriented architec-\ntures combines both of the manage interface tactics.\nFigure 6.3 shows a summary of the tactics to achieve interoperability.\n\n\n112 \nPart Two  Quality Attributes\t\n6—Interoperability\nInteroperability Tactics\nLocate\nManage Interfaces\nDiscover\nService\nOrchestrate\nTailor Interface\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nFigure 6.3  Summary of interoperability tactics\nWhy Standards Are Not Enough to Guarantee Interoperability \nBy Grace Lewis\nDeveloper of System A needs to exchange product data with System B. \nDeveloper A finds that there is an existing WS* web service interface for \nsending product data that among other fields contains price expressed \nin XML Schema as a decimal with two fraction digits. Developer A writes \ncode to interact with the web service and the system works perfectly. \nHowever, after two weeks of operation, there is a huge discrepancy be-\ntween the totals reported by System A and the totals reported by System \nB. After conversations between the two developers, they discover that \nSystem B expected to receive a price that included tax and System A was \nsending it without tax. \nThis is a simple example of why standards are not enough. The sys-\ntems exchanged data perfectly because they both agreed that the price \nwas a decimal with two fractions digits expressed in XML Schema and the \nmessage was sent via SOAP over HTTP (syntax)—standards used in the \nimplementation of WS* web services—but they did not agree on whether \nthe price included tax or not (semantics).\nOf course, the only realistic approach to getting diverse applications to \nshare information is by reaching agreements on the structure and func-\ntion of the information to be shared. These agreements are often reflected \nin standards that provide a common interface that multiple vendors and \napplication builders support. Standards have indeed been instrumental \n\n\n6.2  Tactics for Interoperability\n113\nin achieving a significant level of interoperability that we rely on in almost \nevery domain. However, while standards are useful and in many ways in-\ndispensable, expectations of what can be achieved through standards are \nunrealistic. Here are some of the challenges that organizations face related \nto standards and interoperability:\n1.\t Ideally, every implementation of a standard should be identical \nand thus completely interoperable with any other implementation. \nHowever, this is far from reality. Standards, when incorporated into \nproducts, tools, and services, undergo customizations and exten-\nsions because every vendor wants to create a unique selling point as \na competitive advantage.\n2.\t Standards are often deliberately open-ended and provide exten-\nsion points. The actual implementation of these extension points \nis left to the discretion of implementers, leading to proprietary \nimplementations.\n3.\t Standards, like any technology, have a life cycle of their own and \nevolve over time in compatible and noncompatible ways. Deciding \nwhen to adopt a new or revised standard is a critical decision for or-\nganizations. Committing to a new standard that is not ready or even-\ntually not adopted by the community is a big risk for organizations. \nOn the other hand, waiting too long may also become a problem, \nwhich can lead to unsupported products, incompatibilities, and work-\narounds, because everyone else is using the standard.\n4.\t Within the software community, there are as many bad standards as \nthere are engineers with opinions. Bad standards include underspe-\ncified, overspecified, inconsistently specified, unstable, or irrelevant \nstandards. \n5.\t It is quite common for standards to be championed by competing \norganizations, resulting in conflicting standards due to overlap or mu-\ntual exclusion.\n6.\t For new and rapidly emerging domains, the argument often made is \nthat standardization will be destructive because it will hinder flexibil-\nity: premature standardization will force the use of an inadequate ap-\nproach and lead to abandoning other presumably better approaches. \nSo what do organizations do in the meantime?\nWhat these challenges illustrate is that because of the way in which \nstandards are usually created and evolved, we cannot let standards drive \nour architectures. We need to architect systems first and then decide which \nstandards can support desired system requirements and qualities. This ap-\nproach allows standards to change and evolve without affecting the overall \narchitecture of the system. \nI once heard someone in a keynote address say that “The nice thing \nabout standards is that there are so many to choose from.”\n\n\n114 \nPart Two  Quality Attributes\t\n6—Interoperability\n6.3  A Design Checklist for Interoperability\nTable 6.3 is a checklist to support the design and analysis process for inter­operability.\nTable 6.3  Checklist to Support the Design and Analysis Process for \nInteroperability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which of your system responsibilities will need to \ninteroperate with other systems.\nEnsure that responsibilities have been allocated to detect \na request to interoperate with known or unknown external \nsystems. \nEnsure that responsibilities have been allocated to carry out the \nfollowing tasks: \n■\n■\nAccept the request\n■\n■\nExchange information\n■\n■\nReject the request\n■\n■\nNotify appropriate entities (people or systems)\n■\n■\nLog the request (for interoperability in an untrusted environ-\nment, logging for nonrepudiation is essential) \nCoordination Model\nEnsure that the coordination mechanisms can meet the critical \nquality attribute requirements. Considerations for performance \ninclude the following:\n■\n■\nVolume of traffic on the network both created by the sys-\ntems under your control and generated by systems not \nunder your control\n■\n■\nTimeliness of the messages being sent by your systems\n■\n■\nCurrency of the messages being sent by your systems\n■\n■\nJitter of the messages’ arrival times\n■\n■\nEnsure that all of the systems under your control make as-\nsumptions about protocols and underlying networks that are \nconsistent with the systems not under your control.\nData Model\nDetermine the syntax and semantics of the major data \nabstractions that may be exchanged among interoperating \nsystems.\nEnsure that these major data abstractions are consistent with \ndata from the interoperating systems. (If your system’s data \nmodel is confidential and must not be made public, you may \nhave to apply transformations to and from the data abstractions \nof systems with which yours interoperates.)\nMapping among \nArchitectural \nElements\nFor interoperability, the critical mapping is that of components \nto processors. Beyond the necessity of making sure that \ncomponents that communicate externally are hosted \non processors that can reach the network, the primary \nconsiderations deal with meeting the security, availability, and \nperformance requirements for the communication. These will \nbe dealt with in their respective chapters.\n\n\n6.4  Summary\n115\nCategory\nChecklist\nResource \nManagement\nEnsure that interoperation with another system (accepting a \nrequest and/or rejecting a request) can never exhaust critical \nsystem resources (e.g., can a flood of such requests cause \nservice to be denied to legitimate users?).\nEnsure that the resource load imposed by the communication \nrequirements of interoperation is acceptable.\nEnsure that if interoperation requires that resources be shared \namong the participating systems, an adequate arbitration policy \nis in place.\nBinding Time\nDetermine the systems that may interoperate, and when they \nbecome known to each other. For each system over which you \nhave control:\n■\n■\nEnsure that it has a policy for dealing with binding to both \nknown and unknown external systems. \n■\n■\nEnsure that it has mechanisms in place to reject unaccept-\nable bindings and to log such requests.\n■\n■\nIn the case of late binding, ensure that mechanisms will \nsupport the discovery of relevant new services or protocols, \nor the sending of information using chosen protocols.\nChoice of \nTechnology\nFor any of your chosen technologies, are they “visible” at the \ninterface boundary of a system? If so, what interoperability \neffects do they have? Do they support, undercut, or have \nno effect on the interoperability scenarios that apply to your \nsystem? Ensure the effects they have are acceptable.\nConsider technologies that are designed to support \ninteroperability, such as web services. Can they be used to \nsatisfy the interoperability requirements for the systems under \nyour control?\n6.4  Summary\nInteroperability refers to the ability of systems to usefully exchange information. \nThese systems may have been constructed with the intention of exchanging infor-\nmation, they may be existing systems that are desired to exchange information, \nor they may provide general services without knowing the details of the systems \nthat wish to utilize those services.\nThe general scenario for interoperability provides the details of these dif-\nferent cases. In any interoperability case, the goal is to intentionally exchange \ninformation or reject the request to exchange information.\nAchieving interoperability involves the relevant systems locating each other \nand then managing the interfaces so that they can exchange information.\n\n\n116 \nPart Two  Quality Attributes\t\n6—Interoperability\n6.5  For Further Reading\nAn SEI report gives a good overview of interoperability, and it highlights some of \nthe “maturity frameworks” for interoperability [Brownsword 04].\nThe various WS* services are being developed under the auspices of the \nWorld Wide Web Consortium (W3C) and can be found at www.w3.org/2002/ws.\nSystems of systems are of particular interest to the U.S. Department of De-\nfense. An engineering guide can be found at [ODUSD 08].\n6.6  Discussion Questions\n1.\t\nFind a web service mashup. Write several concrete interoperability scenari-\nos for this system.\n2.\t\nWhat is the relationship between interoperability and the other quality \nattributes highlighted in this book? For example, if two systems fail to ex-\nchange information properly, could a security flaw result? What other quali-\nty attributes seem strongly related (at least potentially) to interoperability? \n3.\t\nIs a service-oriented system a system of systems? If so, describe a ser-\nvice-oriented system that is directed, one that is acknowledged, one that is \ncollaborative, and one that is virtual.\n4.\t\nUniversal Description, Discovery, and Integration (UDDI) was touted as a \ndiscovery service, but commercial support for UDDI is being withdrawn. \nWhy do you suppose this is? Does it have anything to do with the quality \nattributes delivered or not delivered by UDDI solutions?\n5.\t\nWhy has the importance of orchestration grown in recent years?\n6.\t\nIf you are a technology producer, what are the advantages and disadvan-\ntages of adhering to interoperability standards? Why would a producer not \nadhere to a standard?\n7.\t\nWith what other systems will an automatic teller machine need to interoper-\nate? How would you change your automatic teller system design to accom-\nmodate these other systems?\n\n\n117\n7\nModifiability\nAdapt or perish, now as ever, is \nnature’s inexorable imperative.\n—H.G. Wells\nChange happens.\nStudy after study shows that most of the cost of the typical software system \noccurs after it has been initially released. If change is the only constant in the uni-\nverse, then software change is not only constant but ubiquitous. Changes happen \nto add new features, to change or even retire old ones. Changes happen to fix de-\nfects, tighten security, or improve performance. Changes happen to enhance the \nuser’s experience. Changes happen to embrace new technology, new platforms, \nnew protocols, new standards. Changes happen to make systems work together, \neven if they were never designed to do so. \nModifiability is about change, and our interest in it centers on the cost and \nrisk of making changes. To plan for modifiability, an architect has to consider \nfour questions: \n■\n■What can change? A change can occur to any aspect of a system: the \nfunctions that the system computes, the platform (the hardware, operating \nsystem, middleware), the environment in which the system operates \n(the systems with which it must interoperate, the protocols it uses to \ncommunicate with the rest of the world), the qualities the system exhibits \n(its performance, its reliability, and even its future modifications), and its \ncapacity (number of users supported, number of simultaneous operations). \n■\n■What is the likelihood of the change? One cannot plan a system for all \npotential changes—the system would never be done, or if it was done \nit would be far too expensive and would likely suffer quality attribute \nproblems in other dimensions. Although anything might change, the \narchitect has to make the tough decisions about which changes are likely, \nand hence which changes are to be supported, and which are not.\n",
      "page_number": 128
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "118 \nPart Two  Quality Attributes\t\n7—Modifiability\n■\n■When is the change made and who makes it? Most commonly in the \npast, a change was made to source code. That is, a developer had to make \nthe change, which was tested and then deployed in a new release. Now, \nhowever, the question of when a change is made is intertwined with the \nquestion of who makes it. An end user changing the screen saver is clearly \nmaking a change to one of the aspects of the system. Equally clear, it is \nnot in the same category as changing the system so that it can be used \nover the web rather than on a single machine. Changes can be made to the \nimplementation (by modifying the source code), during compile (using \ncompile-time switches), during build (by choice of libraries), during \nconfiguration setup (by a range of techniques, including parameter setting), \nor during execution (by parameter settings, plugins, etc.). A change can also \nbe made by a developer, an end user, or a system administrator.\n■\n■What is the cost of the change? Making a system more modifiable involves \ntwo types of cost:\n■\n■The cost of introducing the mechanism(s) to make the system more \nmodifiable\n■\n■The cost of making the modification using the mechanism(s) \nFor example, the simplest mechanism for making a change is to wait for \na change request to come in, then change the source code to accommodate the \nrequest. The cost of introducing the mechanism is zero; the cost of exercising \nit is the cost of changing the source code and revalidating the system. At the \nother end of the spectrum is an application generator, such as a user interface \nbuilder. The builder takes as input a description of the designer user interface \nproduced through direct manipulation techniques and produces (usually) source \ncode. The cost of introducing the mechanism is the cost of constructing the UI \nbuilder, which can be substantial. The cost of using the mechanism is the cost of \nproducing the input to feed the builder (cost can be substantial or negligible), the \ncost of running the builder (approximately zero), and then the cost of whatever \ntesting is performed on the result (usually much less than usual).\nFor N similar modifications, a simplified justification for a change mecha-\nnism is that \nN × Cost of making the change without the mechanism <_  \nCost of installing the mechanism +  \n(N × Cost of making the change using the mechanism).\nN is the anticipated number of modifications that will use the modifiability \nmechanism, but N is a prediction. If fewer changes than expected come in, then \nan expensive modification mechanism may not be warranted. In addition, the cost \nof creating the modifiability mechanism could be applied elsewhere—in adding \nfunctionality, in improving the performance, or even in nonsoftware investments \nsuch as buying tech stocks. Also, the equation does not take time into account. It \n\n\n7.1  Modifiability General Scenario\n119\nmight be cheaper in the long run to build a sophisticated change-handling mecha-\nnism, but you might not be able to wait for that.\n7.1  Modifiability General Scenario\nFrom these considerations, we can see the portions of the modifiability general \nscenario:\n■\n■Source of stimulus. This portion specifies who makes the change: the \ndeveloper, a system administrator, or an end user. \n■\n■Stimulus. This portion specifies the change to be made. A change can be \nthe addition of a function, the modification of an existing function, or the \ndeletion of a function. (For this categorization, we regard fixing a defect \nas changing a function, which presumably wasn’t working correctly as \na result of the defect.) A change can also be made to the qualities of the \nsystem: making it more responsive, increasing its availability, and so forth. \nThe capacity of the system may also change. Accommodating an increasing \nnumber of simultaneous users is a frequent requirement. Finally, changes \nmay happen to accommodate new technology of some sort, the most \ncommon of which is porting the system to a different type of computer or \ncommunication network.\n■\n■Artifact. This portion specifies what is to be changed: specific components \nor modules, the system’s platform, its user interface, its environment, or \nanother system with which it interoperates.\n■\n■Environment. This portion specifies when the change can be made: design \ntime, compile time, build time, initiation time, or runtime. \n■\n■Response. Make the change, test it, and deploy it. \n■\n■Response measure. All of the possible responses take time and cost money; \ntime and money are the most common response measures. Although both \nsound simple to measure, they aren’t. You can measure calendar time or \nstaff time. But do you measure the time it takes for the change to wind its \nway through configuration control boards and approval authorities (some \nof whom may be outside your organization), or merely the time it takes \nyour engineers to make the change? Cost usually means direct outlay, but \nit might also include opportunity cost of having your staff work on changes \ninstead of other tasks. Other measures include the extent of the change \n(number of modules or other artifacts affected) or the number of new \ndefects introduced by the change, or the effect on other quality attributes. If \nthe change is being made by a user, you may wish to measure the efficacy \nof the change mechanisms provided, which somewhat overlaps with \nmeasures of usability (see Chapter 11). \n\n\n120 \nPart Two  Quality Attributes\t\n7—Modifiability\nFigure 7.1 illustrates a concrete modifiability scenario: The developer \nwishes to change the user interface by modifying the code at design time. The \nmodifications are made with no side effects within three hours.\nTable 7.1 enumerates the elements of the general scenario that characterize \nmodifiability.\nStimulus:\nWishes\nto Change\nthe UI\nResponse:\nChange Made \nand Unit Tested \nSource:\nDeveloper\nArtifact:\nCode\nEnvironment:\n \nDesign\nTime\nResponse\nMeasure:\nIn Three\nHours\n3\n2\n1\n4\nFigure 7.1  Sample concrete modifiability scenario\nTable 7.1  Modifiability General Scenario\nPortion of Scenario Possible Values\nSource\nEnd user, developer, system administrator\nStimulus\nA directive to add/delete/modify functionality, or change a \nquality attribute, capacity, or technology\nArtifacts\nCode, data, interfaces, components, resources, configurations, \n. . . \nEnvironment\nRuntime, compile time, build time, initiation time, design time\nResponse\nOne or more of the following:\n■\n■\nMake modification \n■\n■\nTest modification\n■\n■\nDeploy modification\nResponse Measure\nCost in terms of the following:\n■\n■\nNumber, size, complexity of affected artifacts\n■\n■\nEffort\n■\n■\nCalendar time\n■\n■\nMoney (direct outlay or opportunity cost)\n■\n■\nExtent to which this modification affects other functions or \nquality attributes\n■\n■\nNew defects introduced\n\n\n7.2  Tactics for Modifiability\n121\n7.2  Tactics for Modifiability\nTactics to control modifiability have as their goal controlling the complexity of \nmaking changes, as well as the time and cost to make changes. Figure 7.2 shows \nthis relationship.\nTo understand modifiability, we begin with coupling and cohesion.\nModules have responsibilities. When a change causes a module to be modi-\nfied, its responsibilities are changed in some way. Generally, a change that affects \none module is easier and less expensive than if it changes more than one mod-\nule. However, if two modules’ responsibilities overlap in some way, then a single \nchange may well affect them both. We can measure this overlap by measuring the \nprobability that a modification to one module will propagate to the other. This is \ncalled coupling, and high coupling is an enemy of modifiability.\nCohesion measures how strongly the responsibilities of a module are re-\nlated. Informally, it measures the module’s “unity of purpose.” Unity of purpose \ncan be measured by the change scenarios that affect a module. The cohesion of a \nmodule is the probability that a change scenario that affects a responsibility will \nalso affect other (different) responsibilities. The higher the cohesion, the lower \nthe probability that a given change will affect multiple responsibilities. High co-\nhesion is good; low cohesion is bad. The definition allows for two modules with \nsimilar purposes each to be cohesive.\nGiven this framework, we can now identify the parameters that we will use \nto motivate modifiability tactics:\n■\n■Size of a module. Tactics that split modules will reduce the cost of making \na modification to the module that is being split as long as the split is chosen \nto reflect the type of change that is likely to be made.\nChange \nArrives\nChange Made within\nTime and Budget\nTactics\nto Control\nModifiability\nFigure 7.2  The goal of modifiability tactics\n\n\n122 \nPart Two  Quality Attributes\t\n7—Modifiability\n■\n■Coupling. Reducing the strength of the coupling between two modules A \nand B will decrease the expected cost of any modification that affects A. \nTactics that reduce coupling are those that place intermediaries of various \nsorts between modules A and B.\n■\n■Cohesion. If module A has a low cohesion, then cohesion can be improved \nby removing responsibilities unaffected by anticipated changes. \nFinally we need to be concerned with when in the software development \nlife cycle a change occurs. If we ignore the cost of preparing the architecture for \nthe modification, we prefer that a change is bound as late as possible. Changes \ncan only be successfully made (that is, quickly and at lowest cost) late in the \nlife cycle if the architecture is suitably prepared to accommodate them. Thus the \nfourth and final parameter in a model of modifiability is this:\n■\n■Binding time of modification. An architecture that is suitably equipped to \naccommodate modifications late in the life cycle will, on average, cost less \nthan an architecture that forces the same modification to be made earlier. \nThe preparedness of the system means that some costs will be zero, or very \nlow, for late life-cycle modifications. This, however, neglects the cost of \npreparing the architecture for the late binding.\nNow we may understand tactics and their consequences as affecting one or \nmore of the previous parameters: reducing the size of a module, increasing cohe-\nsion, reducing coupling, and deferring binding time. These tactics are shown in \nFigure 7.3.\nModifiability Tactics\nIncrease\nCohesion\nReduce\nCoupling\nSplit Module\nEncapsulate\nUse an\nIntermediary\nChange\nArrives\nChange Made\nwithin Time \nand Budget\nReduce Size\nof a Module\nIncrease\nSemantic\nCoherence\nRestrict\nDependencies\nRefactor\nAbstract Common\nServices\nDefer\nBinding\nFigure 7.3  Modifiability tactics\n\n\n7.2  Tactics for Modifiability\n123\nReduce the Size of a Module\n■\n■Split module. If the module being modified includes a great deal of capa-\nbility, the modification costs will likely be high. Refining the module into \nseveral smaller modules should reduce the average cost of future changes. \nIncrease Cohesion\nSeveral tactics involve moving responsibilities from one module to another. The \npurpose of moving a responsibility from one module to another is to reduce the \nlikelihood of side effects affecting other responsibilities in the original module.\n■\n■Increase semantic coherence. If the responsibilities A and B in a module \ndo not serve the same purpose, they should be placed in different modules. \nThis may involve creating a new module or it may involve moving a re-\nsponsibility to an existing module. One method for identifying responsibil-\nities to be moved is to hypothesize likely changes that affect a module. If \nsome responsibilities are not affected by these changes, then those responsi-\nbilities should probably be removed.\nReduce Coupling\nWe now turn to tactics that reduce the coupling between modules.\n■\n■Encapsulate. Encapsulation introduces an explicit interface to a module. \nThis interface includes an application programming interface (API) and its \nassociated responsibilities, such as “perform a syntactic transformation on \nan input parameter to an internal representation.” Perhaps the most common \nmodifiability tactic, encapsulation reduces the probability that a change to \none module propagates to other modules. The strengths of coupling that \npreviously went to the module now go to the interface for the module. \nThese strengths are, however, reduced because the interface limits the ways \nin which external responsibilities can interact with the module (perhaps \nthrough a wrapper). The external responsibilities can now only directly in-\nteract with the module through the exposed interface (indirect interactions, \nhowever, such as dependence on quality of service, will likely remain un-\nchanged). Interfaces designed to increase modifiability should be abstract \nwith respect to the details of the module that are likely to change—that is, \nthey should hide those details. \n■\n■Use an intermediary breaks a dependency. Given a dependency between re-\nsponsibility A and responsibility B (for example, carrying out A first requires \ncarrying out B), the dependency can be broken by using an intermediary. \nThe type of intermediary depends on the type of dependency. For example, \na publish-subscribe intermediary will remove the data producer’s knowledge \n\n\n124 \nPart Two  Quality Attributes\t\n7—Modifiability\nof its consumers. So will a shared data repository, which separates readers of \na piece of data from writers of that data. In a service-oriented architecture in \nwhich services discover each other by dynamic lookup, the directory service \nis an intermediary.\n■\n■Restrict dependencies is a tactic that restricts the modules that a given mod-\nule interacts with or depends on. In practice this tactic is achieved by re-\nstricting a module’s visibility (when developers cannot see an interface, they \ncannot employ it) and by authorization (restricting access to only authorized \nmodules). This tactic is seen in layered architectures, in which a layer is only \nallowed to use lower layers (sometimes only the next lower layer) and in the \nuse of wrappers, where external entities can only see (and hence depend on) \nthe wrapper and not the internal functionality that it wraps.\n■\n■Refactor is a tactic undertaken when two modules are affected by the same \nchange because they are (at least partial) duplicates of each other. Code re-\nfactoring is a mainstay practice of Agile development projects, as a cleanup \nstep to make sure that teams have not produced duplicative or overly com-\nplex code; however, the concept applies to architectural elements as well. \nCommon responsibilities (and the code that implements them) are “factored \nout” of the modules where they exist and assigned an appropriate home of \ntheir own. By co-locating common responsibilities—that is, making them \nsubmodules of the same parent module—the architect can reduce coupling. \n■\n■Abstract common services. In the case where two modules provide not-\nquite-the-same but similar services, it may be cost-effective to implement \nthe services just once in a more general (abstract) form. Any modification \nto the (common) service would then need to occur just in one place, reduc-\ning modification costs. A common way to introduce an abstraction is by pa-\nrameterizing the description (and implementation) of a module’s activities. \nThe parameters can be as simple as values for key variables or as complex \nas statements in a specialized language that are subsequently interpreted. \nDefer Binding\nBecause the work of people is almost always more expensive than the work of \ncomputers, letting computers handle a change as much as possible will almost \nalways reduce the cost of making that change. If we design artifacts with built-in \nflexibility, then exercising that flexibility is usually cheaper than hand-coding a \nspecific change.\nParameters are perhaps the best-known mechanism for introducing \nflexibility, and that is reminiscent of the abstract common services tactic. A \nparameterized function f(a, b) is more general than the similar function f(a) that \nassumes b = 0. When we bind the value of some parameters at a different phase \nin the life cycle than the one in which we defined the parameters, we are applying \nthe defer binding tactic.\n\n\n7.3  A Design Checklist for Modifiability\n125\nIn general, the later in the life cycle we can bind values, the better. However, \nputting the mechanisms in place to facilitate that late binding tends to be more \nexpensive—yet another tradeoff. And so the equation on page 118 comes into \nplay. We want to bind as late as possible, as long as the mechanism that allows it \nis cost-effective.\nTactics to bind values at compile time or build time include these:\n■\n■Component replacement (for example, in a build script or makefile)\n■\n■Compile-time parameterization\n■\n■Aspects\nTactics to bind values at deployment time include this:\n■\n■Configuration-time binding\nTactics to bind values at startup or initialization time include this:\n■\n■Resource files\nTactics to bind values at runtime include these:\n■\n■Runtime registration\n■\n■Dynamic lookup (e.g., for services)\n■\n■Interpret parameters\n■\n■Startup time binding\n■\n■Name servers\n■\n■Plug-ins\n■\n■Publish-subscribe\n■\n■Shared repositories\n■\n■Polymorphism\nSeparating building a mechanism for modifiability from using the \nmechanism to make a modification admits the possibility of different stakeholders \nbeing involved—one stakeholder (usually a developer) to provide the mechanism \nand another stakeholder (an installer, for example, or a user) to exercise it later, \npossibly in a completely different life-cycle phase. Installing a mechanism so that \nsomeone else can make a change to the system without having to change any \ncode is sometimes called externalizing the change.\n7.3  A Design Checklist for Modifiability\nTable 7.2 is a checklist to support the design and analysis process for modifiability.\n",
      "page_number": 138
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 146-153)",
      "start_page": 146,
      "end_page": 153,
      "detection_method": "topic_boundary",
      "content": "126 \nPart Two  Quality Attributes\t\n7—Modifiability\nTable 7.2  Checklist to Support the Design and Analysis Process for Modifiability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which changes or categories of changes are likely to \noccur through consideration of changes in technical, legal, social, \nbusiness, and customer forces. For each potential change or \ncategory of changes: \n■\n■\nDetermine the responsibilities that would need to be added, \nmodified, or deleted to make the change. \n■\n■\nDetermine what  responsibilities are impacted by the change.\n■\n■\nDetermine an allocation of responsibilities to modules that \nplaces, as much as possible, responsibilities that will be \nchanged (or impacted by the change) together in the same \nmodule, and places responsibilities that will be changed at \ndifferent times in separate modules.\nCoordination \nModel\nDetermine which functionality or quality attribute can change at \nruntime and how this affects coordination; for example, will the \ninformation being communicated change at runtime, or will the \ncommunication protocol change at runtime? If so, ensure that such \nchanges affect a small number set of modules.\nDetermine which devices, protocols, and communication paths \nused for coordination are likely to change. For those devices, \nprotocols, and communication paths, ensure that the impact of \nchanges will be limited to a small set of modules.\nFor those elements for which modifiability is a concern, use \na coordination model that reduces coupling such as publish-\nsubscribe, defers bindings such as enterprise service bus, or \nrestricts dependencies such as broadcast.\nData Model\nDetermine which changes (or categories of changes) to the data \nabstractions, their operations, or their properties are likely to \noccur. Also determine which changes or categories of changes \nto these data abstractions will involve their creation, initialization, \npersistence, manipulation, translation, or destruction.\nFor each change or category of change, determine if the \nchanges will be made by an end user, a system administrator, or \na developer. For those changes to be made by an end user or \nsystem administrator, ensure that the necessary attributes are \nvisible to that user and that the user has the correct privileges to \nmodify the data, its operations, or its properties.\nFor each potential change or category of change:\n■\n■\nDetermine which data abstractions would need to be added, \nmodified, or deleted to make the change.\n■\n■\nDetermine whether there would be any changes to the \ncreation, initialization, persistence, manipulation, translation, or \ndestruction of these data abstractions.\n■\n■\nDetermine which other data abstractions are impacted \nby the change. For these additional data abstractions, \ndetermine whether the impact would be on the operations, \ntheir properties, their creation, initialization, persistence, \nmanipulation, translation, or destruction.\n■\n■\nEnsure an allocation of data abstractions that minimizes the \nnumber and severity of modifications to the abstractions by the \npotential changes.\nDesign your data model so that items allocated to each element of \nthe data model are likely to change together.\n\n\n7.3  A Design Checklist for Modifiability\n127\nCategory\nChecklist\nMapping among \nArchitectural \nElements\nDetermine if it is desirable to change the way in which functionality \nis mapped to computational elements (e.g., processes, threads, \nprocessors) at runtime, compile time, design time, or build time.\nDetermine the extent of modifications necessary to accommodate \nthe addition, deletion, or modification of a function or a quality \nattribute. This might involve a determination of the following, for \nexample:\n■\n■\nExecution dependencies\n■\n■\nAssignment of data to databases\n■\n■\nAssignment of runtime elements to processes, threads, or \nprocessors\nEnsure that such changes are performed with mechanisms that \nutilize deferred binding of mapping decisions.\nResource \nManagement\nDetermine how the addition, deletion, or modification of a \nresponsibility or quality attribute will affect resource usage. This \ninvolves, for example:\n■\n■\nDetermining what changes might introduce new resources or \nremove old ones or affect existing resource usage\n■\n■\nDetermining what resource limits will change and how\nEnsure that the resources after the modification are sufficient to \nmeet the system requirements.\nEncapsulate all resource managers and ensure that the policies \nimplemented by those resource managers are themselves \nencapsulated and bindings are deferred to the extent possible.\nBinding Time\nFor each change or category of change:\n■\n■\nDetermine the latest time at which the change will need to be \nmade. \n■\n■\nChoose a defer-binding mechanism (see Section 7.2) that \ndelivers the appropriate capability at the time chosen.\n■\n■\nDetermine the cost of introducing the mechanism and the cost \nof making changes using the chosen mechanism. Use the \nequation on page 118 to assess your choice of mechanism.\n■\n■\nDo not introduce so many binding choices that change is \nimpeded because the dependencies among the choices are \ncomplex and unknown. \nChoice of \nTechnology\nDetermine what modifications are made easier or harder by your \ntechnology choices. \n■\n■\nWill your technology choices help to make, test, and deploy \nmodifications?\n■\n■\nHow easy is it to modify your choice of technologies (in case \nsome of these technologies change or become obsolete)?\nChoose your technologies to support the most likely modifications. \nFor example, an enterprise service bus makes it easier to change \nhow elements are connected but may introduce vendor lock-in.\n\n\n128 \nPart Two  Quality Attributes\t\n7—Modifiability\n7.4  Summary\nModifiability deals with change and the cost in time or money of making a \nchange, including the extent to which this modification affects other functions or \nquality attributes. \nChanges can be made by developers, installers, or end users, and these \nchanges need to be prepared for. There is a cost of preparing for change as well \nas a cost of making a change. The modifiability tactics are designed to prepare for \nsubsequent changes.\nTactics to reduce the cost of making a change include making modules \nsmaller, increasing cohesion, and reducing coupling. Deferring binding will also \nreduce the cost of making a change.\nReducing coupling is a standard category of tactics that includes encapsulat-\ning, using an intermediary, restricting dependencies, co-locating related responsi-\nbilities, refactoring, and abstracting common services.\nIncreasing cohesion is another standard tactic that involves separating re-\nsponsibilities that do not serve the same purpose.\nDefer binding is a category of tactics that affect build time, load time, ini-\ntialization time, or runtime.\n7.5  For Further Reading\nSerious students of software engineering should read two early papers about \ndesigning for modifiability. The first is Edsger Dijkstra’s 1968 paper about the \nT.H.E. operating system [Dijkstra 68], which is the first paper that talks about de-\nsigning systems to be layered, and the modifiability benefits it brings. The second \nis David Parnas’s 1972 paper that introduced the concept of information hiding \n[Parnas 72]. Parnas prescribed defining modules not by their functionality but by \ntheir ability to internalize the effects of changes.\nThe tactics that we have presented in this chapter are a variant on those in-\ntroduced by [Bachmann 07].\nAdditional tactics for modifiability within the avionics domain can be found \nin [EOSAN 07], published by the European Organization for the Safety of Air \nNavigation. \n7.6  Discussion Questions\n1.\t\nModifiability comes in many flavors and is known by many names. Find \none of the IEEE or ISO standards dealing with quality attributes and \n\n\n7.6  Discussion Questions\n129\ncompile a list of quality attributes that refer to some form of modifiability. \nDiscuss the differences.\n2.\t\nFor each quality attribute that you discovered as a result of the previous \nquestion, write a modifiability scenario that expresses it.\n3.\t\nIn a certain metropolitan subway system, the ticket machines accept cash \nbut do not give change. There is a separate machine that dispenses change \nbut does not sell tickets. In an average station there are six or eight ticket \nmachines for every change machine. What modifiability tactics do you see \nat work in this arrangement? What can you say about availability?\n4.\t\nFor the subway system in the previous question, describe the specific form \nof modifiability (using a modifiability scenario) that seems to be the aim of \narranging the ticket and change machines as described.\n5.\t\nA wrapper is a common aid to modifiability. A wrapper for a component \nis the only element allowed to use that component; every other piece of \nsoftware uses the component’s services by going through the wrapper. The \nwrapper transforms the data or control information for the component it \nwraps. For example, a component may expect input using English measures \nbut find itself in a system in which all of the other components produce \nmetric measures. A wrapper could be employed to translate. What modifi-\nability tactics does a wrapper embody?\n6.\t\nOnce an intermediary has been introduced into an architecture, some mod-\nules may attempt to circumvent it, either inadvertently (because they are \nnot aware of the intermediary) or intentionally (for performance, for conve-\nnience, or out of habit). Discuss some architectural means to prevent inad-\nvertent circumvention of an intermediary.\n7.\t\nIn some projects, deployability is an important quality attribute that mea-\nsures how easy it is to get a new version of the system into the hands of its \nusers. This might mean a trip to your auto dealer or transmitting updates \nover the Internet. It also includes the time it takes to install the update once \nit arrives. In projects that measure deployability separately, should the cost \nof a modification stop when the new version is ready to ship? Justify your \nanswer.\n8.\t\nThe abstract common services tactic is intended to reduce coupling, but it \nalso might reduce cohesion. Discuss.\n9.\t\nIdentify particular change scenarios for an automatic teller machine. What \nmodifications would you make to your automatic teller machine design to \naccommodate these changes?\n\n\nThis page intentionally left blank \n\n\n131\n8\nPerformance\nAn ounce of performance is worth pounds of promises.\n—Mae West\nIt’s about time.\nPerformance, that is: It’s about time and the software system’s ability to meet \ntiming requirements. When events occur—interrupts, messages, requests from \nusers or other systems, or clock events marking the passage of time—the system, \nor some element of the system, must respond to them in time. Characterizing \nthe events that can occur (and when they can occur) and the system or element’s \ntime-based response to those events is the essence is discussing performance.\nWeb-based system events come in the form of requests from users (num-\nbering in the tens or tens of millions) via their clients such as web browsers. In \na control system for an internal combustion engine, events come from the opera-\ntor’s controls and the passage of time; the system must control both the firing of \nthe ignition when a cylinder is in the correct position and the mixture of the fuel \nto maximize power and efficiency and minimize pollution. \nFor a web-based system, the desired response might be expressed as number \nof transactions that can be processed in a minute. For the engine control system, \nthe response might be the allowable variation in the firing time. In each case, the \npattern of events arriving and the pattern of responses can be characterized, and \nthis characterization forms the language with which to construct performance \nscenarios.\nFor much of the history of software engineering, performance has been the \ndriving factor in system architecture. As such, it has frequently compromised the \nachievement of all other qualities. As the price/performance ratio of hardware \ncontinues to plummet and the cost of developing software continues to rise, other \nqualities have emerged as important competitors to performance.\nNevertheless, all systems have performance requirements, even if they are \nnot expressed. For example, a word processing tool may not have any explicit \nperformance requirement, but no doubt everyone would agree that waiting an \n\n\n132 \nPart Two  Quality Attributes\t\n8—Performance\nhour (or a minute, or a second) before seeing a typed character appear on the \nscreen is unacceptable. Performance continues to be a fundamentally important \nquality attribute for all software.\nPerformance is often linked to scalability—that is, increasing your system’s \ncapacity for work, while still performing well. Technically, scalability is making \nyour system easy to change in a particular way, and so is a kind of modifiability. \nIn addition, we address scalability explicitly in Chapter 12.\n8.1  Performance General Scenario\nA performance scenario begins with an event arriving at the system. Responding \ncorrectly to the event requires resources (including time) to be consumed. While \nthis is happening, the system may be simultaneously servicing other events. \nConcurrency\nConcurrency is one of the more important concepts that an architect must \nunderstand and one of the least-taught in computer science courses. \nConcurrency refers to operations occurring in parallel. For example, sup-\npose there is a thread that executes the statements \nx := 1;\nx++;\nand another thread that executes the same statements. What is the value \nof x after both threads have executed those statements? It could be either \n2 or 3. I leave it to you to figure out how the value 3 could occur—or \nshould I say I interleave it to you?\nConcurrency occurs any time your system creates a new thread, be-\ncause threads, by definition, are independent sequences of control. Multi-\ntasking on your system is supported by independent threads. Multiple users \nare simultaneously supported on your system through the use of threads. \nConcurrency also occurs any time your system is executing on more than \none processor, whether the processors are packaged separately or as \nmulti-core processors. In addition, you must consider concurrency when \nparallel algorithms, parallelizing infrastructures such as map-reduce, or \nNoSQL databases are used by your system, or you utilize one of a variety \nof concurrent scheduling algorithms. In other words, concurrency is a tool \navailable to you in many ways.\nConcurrency, when you have multiple CPUs or wait states that can \nexploit it, is a good thing. Allowing operations to occur in parallel improves \nperformance, because delays introduced in one thread allow the processor \n\n\n8.1  Performance General Scenario\n133\nto progress on another thread. But because of the interleaving phenome-\nnon just described (referred to as a race condition), concurrency must also \nbe carefully managed by the architect.\nAs the example shows, race conditions can occur when there are two \nthreads of control and there is shared state. The management of con-\ncurrency frequently comes down to managing how state is shared. One \ntechnique for preventing race conditions is to use locks to enforce sequen-\ntial access to state. Another technique is to partition the state based on the \nthread executing a portion of code. That is, if there are two instances of x in \nour example, x is not shared by the two threads and there will not be a race \ncondition.\nRace conditions are one of the hardest types of bugs to discover; the \noccurrence of the bug is sporadic and depends on (possibly minute) differ-\nences in timing. I once had a race condition in an operating system that I \ncould not track down. I put a test in the code so that the next time the race \ncondition occurred, a debugging process was triggered. It took over a year \nfor the bug to recur so that the cause could be determined.\nDo not let the difficulties associated with concurrency dissuade you from \nutilizing this very important technique. Just use it with the knowledge that \nyou must carefully identify critical sections in your code and ensure that \nrace conditions will not occur in those sections.\n—LB\nEvents can arrive in predictable patterns or mathematical distributions, or be \nunpredictable. An arrival pattern for events is characterized as periodic, stochastic, \nor sporadic:\n■\n■Periodic events arrive predictably at regular time intervals. For instance, an \nevent may arrive every 10 milliseconds. Periodic event arrival is most often \nseen in real-time systems. \n■\n■Stochastic arrival means that events arrive according to some probabilistic \ndistribution. \n■\n■Sporadic events arrive according to a pattern that is neither periodic \nnor stochastic. Even these can be characterized, however, in certain \ncircumstances. For example, we might know that at most 600 events will \noccur in a minute, or that there will be at least 200 milliseconds between \nthe arrival of any two events. (This might describe a system in which events \ncorrespond to keyboard strokes from a human user.) These are helpful \ncharacterizations, even though we don’t know when any single event will \narrive. \nThe response of the system to a stimulus can be measured by the following: \n■\n■Latency. The time between the arrival of the stimulus and the system’s \nresponse to it. \n",
      "page_number": 146
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 154-162)",
      "start_page": 154,
      "end_page": 162,
      "detection_method": "topic_boundary",
      "content": "134 \nPart Two  Quality Attributes\t\n8—Performance\n■\n■Deadlines in processing. In the engine controller, for example, the fuel \nshould ignite when the cylinder is in a particular position, thus introducing \na processing deadline. \n■\n■The throughput of the system, usually given as the number of transactions \nthe system can process in a unit of time. \n■\n■The jitter of the response—the allowable variation in latency. \n■\n■The number of events not processed because the system was too busy to \nrespond. \nFrom these considerations we can now describe the individual portions of a \ngeneral scenario for performance:\n■\n■Source of stimulus. The stimuli arrive either from external (possibly \nmultiple) or internal sources. \n■\n■Stimulus. The stimuli are the event arrivals. The arrival pattern can be peri-\nodic, stochastic, or sporadic, characterized by numeric parameters. \n■\n■Artifact. The artifact is the system or one or more of its components.\n■\n■Environment. The system can be in various operational modes, such as nor-\nmal, emergency, peak load, or overload.\n■\n■Response. The system must process the arriving events. This may cause a \nchange in the system environment (e.g., from normal to overload mode). \n■\n■Response measure. The response measures are the time it takes to process \nthe arriving events (latency or a deadline), the variation in this time (jitter), \nthe number of events that can be processed within a particular time interval \n(throughput), or a characterization of the events that cannot be processed \n(miss rate).\nThe general scenario for performance is summarized in Table 8.1.\nFigure 8.1 gives an example concrete performance scenario: Users initiate \ntransactions under normal operations. The system processes the transactions with \nan average latency of two seconds.\nTable 8.1  Performance General Scenario\nPortion of Scenario\nPossible Values\nSource\nInternal or external to the system \nStimulus\nArrival of a periodic, sporadic, or stochastic event\nArtifact\nSystem or one or more components in the system\nEnvironment\nOperational mode: normal, emergency, peak load, overload\nResponse\nProcess events, change level of service\nResponse Measure\nLatency, deadline, throughput, jitter, miss rate\n\n\n8.2  Tactics for Performance\n135\n8.2  Tactics for Performance\nThe goal of performance tactics is to generate a response to an event arriving \nat the system within some time-based constraint. The event can be single or a \nstream and is the trigger to perform computation. Performance tactics control the \ntime within which a response is generated, as illustrated in Figure 8.2. \nAt any instant during the period after an event arrives but before the sys-\ntem’s response t`o it is complete, either the system is working to respond to that \nevent or the processing is blocked for some reason. This leads to the two basic \ncontributors to the response time: processing time (when the system is working to \nrespond) and blocked time (when the system is unable to respond).\nStimulus:\nInitiate\nTransactions\nresponse:\nTransactions\nAre Processed\nresponse\nMeasure:\nAverage\nLatency\nof Two \nSource:\nUsers\nartifact:\nSystem\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSeconds\nFigure 8.1  Sample concrete performance scenario\nEvent\nArrives\nResponse\nGenerated\nwithin Time\nConstraints\nTactics\nto Control\nPerformance\nFigure 8.2  The goal of performance tactics\n\n\n136 \nPart Two  Quality Attributes\t\n8—Performance\n■\n■Processing time. Processing consumes resources, which takes time. Events \nare handled by the execution of one or more components, whose time \nexpended is a resource. Hardware resources include CPU, data stores, \nnetwork communication bandwidth, and memory. Software resources \ninclude entities defined by the system under design. For example, buffers \nmust be managed and access to critical sections1 must be made sequential. \nFor example, suppose a message is generated by one component. It \nmight be placed on the network, after which it arrives at another compo-\nnent. It is then placed in a buffer; transformed in some fashion; processed \naccording to some algorithm; transformed for output; placed in an output \nbuffer; and sent onward to another component, another system, or some \nactor. Each of these steps consumes resources and time and contributes to \nthe overall latency of the processing of that event.\nDifferent resources behave differently as their utilization approaches \ntheir capacity—that is, as they become saturated. For example, as a CPU \nbecomes more heavily loaded, performance usually degrades fairly steadily. \nOn the other hand, when you start to run out of memory, at some point the \npage swapping becomes overwhelming and performance crashes suddenly.\n■\n■Blocked time. A computation can be blocked because of contention for some \nneeded resource, because the resource is unavailable, or because the compu-\ntation depends on the result of other computations that are not yet available:\n■\n■Contention for resources. Many resources can only be used by a single \nclient at a time. This means that other clients must wait for access to \nthose resources. Figure 8.2 shows events arriving at the system. These \nevents may be in a single stream or in multiple streams. Multiple streams \nvying for the same resource or different events in the same stream vying \nfor the same resource contribute to latency. The more contention for a \nresource, the more likelihood of latency being introduced. \n■\n■Availability of resources. Even in the absence of contention, computation \ncannot proceed if a resource is unavailable. Unavailability may be caused \nby the resource being offline or by failure of the component or for some \nother reason. In any case, you must identify places where resource un-\navailability might cause a significant contribution to overall latency. Some \nof our tactics are intended to deal with this situation.\n■\n■Dependency on other computation. A computation may have to wait \nbecause it must synchronize with the results of another computation or \nbecause it is waiting for the results of a computation that it initiated. If a \ncomponent calls another component and must wait for that component to \nrespond, the time can be significant if the called component is at the other \nend of a network (as opposed to co-located on the same processor).\n1.   A critical section is a section of code in a multi-threaded system in which at most one thread may \nbe active at any time.\n\n\n8.2  Tactics for Performance\n137\nWith this background, we turn to our tactic categories. We can either reduce \ndemand for resources or make the resources we have handle the demand more \neffectively: \n■\n■Control resource demand. This tactic operates on the demand side to \nproduce smaller demand on the resources that will have to service the \nevents.\n■\n■Manage resources. This tactic operates on the response side to make the re-\nsources at hand work more effectively in handling the demands put to them.\nControl Resource Demand\nOne way to increase performance is to carefully manage the demand for re-\nsources. This can be done by reducing the number of events processed by en-\nforcing a sampling rate, or by limiting the rate at which the system responds to \nevents. In addition, there are a number of techniques for ensuring that the re-\nsources that you do have are applied judiciously:\n■\n■Manage sampling rate. If it is possible to reduce the sampling frequency \nat which a stream of environmental data is captured, then demand can be \nreduced, typically with some attendant loss of fidelity. This is common \nin signal processing systems where, for example, different codecs can be \nchosen with different sampling rates and data formats. This design choice \nis made to maintain predictable levels of latency; you must decide whether \nhaving a lower fidelity but consistent stream of data is preferable to losing \npackets of data.\n■\n■Limit event response. When discrete events arrive at the system (or element) \ntoo rapidly to be processed, then the events must be queued until they can \nbe processed. Because these events are discrete, it is typically not desirable \nto “downsample” them. In such a case, you may choose to process events \nonly up to a set maximum rate, thereby ensuring more predictable process-\ning when the events are actually processed. This tactic could be triggered \nby a queue size or processor utilization measure exceeding some warning \nlevel. If you adopt this tactic and it is unacceptable to lose any events, then \nyou must ensure that your queues are large enough to handle the worst case. \nIf, on the other hand, you choose to drop events, then you need to choose a \npolicy for handling this situation: Do you log the dropped events, or simply \nignore them? Do you notify other systems, users, or administrators?\n■\n■Prioritize events. If not all events are equally important, you can impose a \npriority scheme that ranks events according to how important it is to service \nthem. If there are not enough resources available to service them when they \narise, low-priority events might be ignored. Ignoring events consumes min-\nimal resources (including time), and thus increases performance compared \nto a system that services all events all the time. For example, a building \n\n\n138 \nPart Two  Quality Attributes\t\n8—Performance\nmanagement system may raise a variety of alarms. Life-threatening alarms \nsuch as a fire alarm should be given higher priority than informational \nalarms such as a room is too cold.\n■\n■Reduce overhead. The use of intermediaries (so important for modifiability, \nas we saw in Chapter 7) increases the resources consumed in processing \nan event stream, and so removing them improves latency. This is a clas-\nsic modifiability/performance tradeoff. Separation of concerns, another \nlinchpin of modifiability, can also increase the processing overhead nec-\nessary to service an event if it leads to an event being serviced by a chain \nof components rather than a single component. The context switching and \nintercomponent communication costs add up, especially when the compo-\nnents are on different nodes on a network. A strategy for reducing compu-\ntational overhead is to co-locate resources. Co-location may mean hosting \ncooperating components on the same processor to avoid the time delay of \nnetwork communication; it may mean putting the resources in the same \nruntime software component to avoid even the expense of a subroutine call. \nA special case of reducing computational overhead is to perform a periodic \ncleanup of resources that have become inefficient. For example, hash tables \nand virtual memory maps may require recalculation and reinitialization. \nAnother common strategy is to execute single-threaded servers (for simplic-\nity and avoiding contention) and split workload across them. \n■\n■Bound execution times. Place a limit on how much execution time is used to \nrespond to an event. For iterative, data-dependent algorithms, limiting the \nnumber of iterations is a method for bounding execution times. The cost is \nusually a less accurate computation. If you adopt this tactic, you will need \nto assess its effect on accuracy and see if the result is “good enough.” This \nresource management tactic is frequently paired with the manage sampling \nrate tactic.\n■\n■Increase resource efficiency. Improving the algorithms used in critical areas \nwill decrease latency. \nManage Resources\nEven if the demand for resources is not controllable, the management of these re-\nsources can be. Sometimes one resource can be traded for another. For example, \nintermediate data may be kept in a cache or it may be regenerated depending on \ntime and space resource availability. This tactic is usually applied to the proces-\nsor but is also effective when applied to other resources such as a disk. Here are \nsome resource management tactics:\n■\n■Increase resources. Faster processors, additional processors, additional \nmemory, and faster networks all have the potential for reducing latency. \n\n\n8.2  Tactics for Performance\n139\nCost is usually a consideration in the choice of resources, but increasing the \nresources is definitely a tactic to reduce latency and in many cases is the \ncheapest way to get immediate improvement. \n■\n■Introduce concurrency. If requests can be processed in parallel, the blocked \ntime can be reduced. Concurrency can be introduced by processing differ-\nent streams of events on different threads or by creating additional threads \nto process different sets of activities. Once concurrency has been intro-\nduced, scheduling policies can be used to achieve the goals you find desir-\nable. Different scheduling policies may maximize fairness (all requests get \nequal time), throughput (shortest time to finish first), or other goals. (See \nthe sidebar.)\n■\n■Maintain multiple copies of computations. Multiple servers in a client-serv-\ner pattern are replicas of computation. The purpose of replicas is to reduce \nthe contention that would occur if all computations took place on a single \nserver. A load balancer is a piece of software that assigns new work to one \nof the available duplicate servers; criteria for assignment vary but can be as \nsimple as round-robin or assigning the next request to the least busy server. \n■\n■Maintain multiple copies of data. Caching is a tactic that involves keeping \ncopies of data (possibly one a subset of the other) on storage with different \naccess speeds. The different access speeds may be inherent (memory versus \nsecondary storage) or may be due to the necessity for network communica-\ntion. Data replication involves keeping separate copies of the data to reduce \nthe contention from multiple simultaneous accesses. Because the data being \ncached or replicated is usually a copy of existing data, keeping the copies \nconsistent and synchronized becomes a responsibility that the system must \nassume. Another responsibility is to choose the data to be cached. Some \ncaches operate by merely keeping copies of whatever was recently request-\ned, but it is also possible to predict users’ future requests based on patterns \nof behavior, and begin the calculations or prefetches necessary to comply \nwith those requests before the user has made them.\n■\n■Bound queue sizes. This controls the maximum number of queued arrivals \nand consequently the resources used to process the arrivals. If you adopt \nthis tactic, you need to adopt a policy for what happens when the queues \noverflow and decide if not responding to lost events is acceptable. This tac-\ntic is frequently paired with the limit event response tactic.\n■\n■Schedule resources. Whenever there is contention for a resource, the \nresource must be scheduled. Processors are scheduled, buffers are \nscheduled, and networks are scheduled. Your goal is to understand the \ncharacteristics of each resource’s use and choose the scheduling strategy \nthat is compatible with it. (See the sidebar.)\nThe tactics for performance are summarized in Figure 8.3.\n\n\n140 \nPart Two  Quality Attributes\t\n8—Performance\nScheduling Policies\nA scheduling policy conceptually has two parts: a priority assignment \nand dispatching. All scheduling policies assign priorities. In some cases \nthe assignment is as simple as first-in/first-out (or FIFO). In other cases, \nit can be tied to the deadline of the request or its semantic importance. \nCompeting criteria for scheduling include optimal resource usage, request \nimportance, minimizing the number of resources used, minimizing latency, \nmaximizing throughput, preventing starvation to ensure fairness, and so \nforth. You need to be aware of these possibly conflicting criteria and the \neffect that the chosen tactic has on meeting them.\nA high-priority event stream can be dispatched only if the resource to \nwhich it is being assigned is available. Sometimes this depends on pre-\nempting the current user of the resource. Possible preemption options are \nas follows: can occur anytime, can occur only at specific preemption points, \nand executing processes cannot be preempted. Some common scheduling \npolicies are these:\n■\n■\nFirst-in/first-out. FIFO queues treat all requests for resources as equals \nand satisfy them in turn. One possibility with a FIFO queue is that one \nrequest will be stuck behind another one that takes a long time to gener-\nate a response. As long as all of the requests are truly equal, this is not \na problem, but if some requests are of higher priority than others, it is \nproblematic.\n■\n■\nFixed-priority scheduling. Fixed-priority scheduling assigns each source \nof resource requests a particular priority and assigns the resources in \nthat priority order. This strategy ensures better service for higher priority \nrequests. But it admits the possibility of a lower priority, but important, \nrequest taking an arbitrarily long time to be serviced, because it is stuck \nbehind a series of higher priority requests. Three common prioritization \nstrategies are these:\n■\n■\nSemantic importance. Each stream is assigned a priority statically \naccording to some domain characteristic of the task that generates it. \n■\n■\nDeadline monotonic. Deadline monotonic. Deadline monotonic is a \nstatic priority assignment that assigns higher priority to streams with \nshorter deadlines. This scheduling policy is used when streams of \ndifferent priorities with real-time deadlines are to be scheduled. \n■\n■\nRate monotonic. Rate monotonic is a static priority assignment \nfor periodic streams that assigns higher priority to streams with \nshorter periods. This scheduling policy is a special case of deadline \nmonotonic but is better known and more likely to be supported by the \noperating system. \n■\n■\nDynamic priority scheduling. Strategies include these:\n■\n■\nRound-robin. Round-robin is a scheduling strategy that orders \nthe requests and then, at every assignment possibility, assigns \nthe resource to the next request in that order. A special form of \n\n\n8.2  Tactics for Performance\n141\nround-robin is a cyclic executive, where assignment possibilities are \nat fixed time intervals.\n■\n■\nEarliest-deadline-first. Earliest-deadline-first. Earliest-deadline-first \nassigns priorities based on the pending requests with the earliest \ndeadline.\n■\n■\nLeast-slack-first. This strategy assigns the highest priority to the job \nhaving the least “slack time,” which is the difference between the exe-\ncution time remaining and the time to the job’s deadline.\nFor a single processor and processes that are preemptible (that is, it is \npossible to suspend processing of one task in order to service a task \nwhose deadline is drawing near), both the earliest-deadline and least-\nslack scheduling strategies are optimal. That is, if the set of processes can \nbe scheduled so that all deadlines are met, then these strategies will be \nable to schedule that set successfully.\n■\n■\nStatic scheduling. A cyclic executive schedule is a scheduling strategy \nwhere the preemption points and the sequence of assignment to the \nresource are determined offline. The runtime overhead of a scheduler is \nthereby obviated.\nPerformance Tactics\nControl Resource Demand\nManage Resources\nManage Sampling Rate\nLimit Event Response\nPrioritize Events\nReduce Overhead\nBound Execution Times\nIncrease Resource\nEfficiency\nEvent \nArrives\nResponse\nGenerated within\nTime Constraints\nIncrease Resources\nIntroduce Concurrency\nMaintain Multiple\nCopies of Computations\nMaintain Multiple\nCopies of Data\nBound Queue Sizes\nSchedule Resources\nFigure 8.3  Performance tactics\n\n\n142 \nPart Two  Quality Attributes\t\n8—Performance\nPerformance Tactics on the Road\nTactics are generic design principles. To exercise this point, think about \nthe design of the systems of roads and highways where you live. Traffic \nengineers employ a bunch of design “tricks” to optimize the performance \nof these complex systems, where performance has a number of mea-\nsures, such as throughput (how many cars per hour get from the suburbs \nto the football stadium), average-case latency (how long it takes, on aver-\nage, to get from your house to downtown), and worst-case latency (how \nlong does it take an emergency vehicle to get you to the hospital). What \nare these tricks? None other than our good old buddies, tactics.\nLet’s consider some examples:\n■\n■\nManage event rate. Lights on highway entrance ramps let cars onto the \nhighway only at set intervals, and cars must wait (queue) on the ramp for \ntheir turn.\n■\n■\nPrioritize events. Ambulances and police, with their lights and sirens \ngoing, have higher priority than ordinary citizens; some highways have \nhigh-occupancy vehicle (HOV) lanes, giving priority to vehicles with two \nor more occupants.\n■\n■\nMaintain multiple copies. Add traffic lanes to existing roads, or build \nparallel routes.\nIn addition, there are some tricks that users of the system can employ:\n■\n■\nIncrease resources. Buy a Ferrari, for example. All other things being \nequal, the fastest car with a competent driver on an open road will get \nyou to your destination more quickly.\n■\n■\nIncrease efficiency. Find a new route that is quicker and/or shorter than \nyour current route.\n■\n■\nReduce computational overhead. You can drive closer to the car in \nfront of you, or you can load more people into the same vehicle (that is, \ncarpooling).\nWhat is the point of this discussion? To paraphrase Gertrude Stein: per-\nformance is performance is performance. Engineers have been analyzing \nand optimizing systems for centuries, trying to improve their performance, \nand they have been employing the same design strategies to do so. So \nyou should feel some comfort in knowing that when you try to improve the \nperformance of your computer-based system, you are applying tactics that \nhave been thoroughly “road tested.” \n—RK\n8.3  A Design Checklist for Performance\nTable 8.2 is a checklist to support the design and analysis process for performance.\n",
      "page_number": 154
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 163-171)",
      "start_page": 163,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": "8.3  A Design Checklist for Performance\n143\nTable 8.2  Checklist to Support the Design and Analysis Process for \nPerformance\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine the system’s responsibilities that will involve heavy \nloading, have time-critical response requirements, are heavily \nused, or impact portions of the system where heavy loads or \ntime-critical events occur. \nFor those responsibilities, identify the processing requirements \nof each responsibility, and determine whether they may cause \nbottlenecks.\nAlso, identify additional responsibilities to recognize and process \nrequests appropriately, including\n■\n■\nResponsibilities that result from a thread of control crossing \nprocess or processor boundaries\n■\n■\nResponsibilities to manage the threads of control—allocation \nand deallocation of threads, maintaining thread pools, and so \nforth\n■\n■\nResponsibilities for scheduling shared resources or \nmanaging performance-related artifacts such as queues, \nbuffers, and caches\nFor the responsibilities and resources you identified, ensure that \nthe required performance response can be met (perhaps by \nbuilding a performance model to help in the evaluation).\nCoordination  \nModel\nDetermine the elements of the system that must coordinate with \neach other—directly or indirectly—and choose communication \nand coordination mechanisms that do the following:\n■\n■\nSupport any introduced concurrency (for example, is it thread \nsafe?), event prioritization, or scheduling strategy\n■\n■\nEnsure that the required performance response can be \ndelivered\n■\n■\nCan capture periodic, stochastic, or sporadic event arrivals, \nas needed \n■\n■\nHave the appropriate properties of the communication \nmechanisms; for example, stateful, stateless, synchronous, \nasynchronous, guaranteed delivery, throughput, or latency\nData Model\nDetermine those portions of the data model that will be heavily \nloaded, have time-critical response requirements, are heavily \nused, or impact portions of the system where heavy loads or \ntime-critical events occur. \nFor those data abstractions, determine the following:\n■\n■\nWhether maintaining multiple copies of key data would \nbenefit performance\n■\n■\nWhether partitioning data would benefit performance\n■\n■\nWhether reducing the processing requirements for the \ncreation, initialization, persistence, manipulation, translation, \nor destruction of the enumerated data abstractions is \npossible\n■\n■\nWhether adding resources to reduce bottlenecks for the \ncreation, initialization, persistence, manipulation, translation, \nor destruction of the enumerated data abstractions is feasible\ncontinues\n\n\n144 \nPart Two  Quality Attributes\t\n8—Performance\nTable 8.2  Checklist to Support the Design and Analysis Process for \nPerformance, continued\nCategory\nChecklist\nMapping among \nArchitectural \nElements\nWhere heavy network loading will occur, determine whether \nco-locating some components will reduce loading and improve \noverall efficiency.\nEnsure that components with heavy computation requirements \nare assigned to processors with the most processing capacity. \nDetermine where introducing concurrency (that is, allocating \na piece of functionality to two or more copies of a component \nrunning simultaneously) is feasible and has a significant positive \neffect on performance.\nDetermine whether the choice of threads of control and their \nassociated responsibilities introduces bottlenecks. \nResource \nManagement\nDetermine which resources in your system are critical for \nperformance. For these resources, ensure that they will be \nmonitored and managed under normal and overloaded system \noperation. For example: \n■\n■\nSystem elements that need to be aware of, and manage, \ntime and other performance-critical resources\n■\n■\nProcess/thread models \n■\n■\nPrioritization of resources and access to resources \n■\n■\nScheduling and locking strategies \n■\n■\nDeploying additional resources on demand to meet increased \nloads\nBinding Time\nFor each element that will be bound after compile time, \ndetermine the following:\n■\n■\nTime necessary to complete the binding\n■\n■\nAdditional overhead introduced by using the late binding \nmechanism\nEnsure that these values do not pose unacceptable performance \npenalties on the system.\nChoice of \nTechnology\nWill your choice of technology let you set and meet hard, real-\ntime deadlines? Do you know its characteristics under load and \nits limits?\nDoes your choice of technology give you the ability to set the \nfollowing:\n■\n■\nScheduling policy\n■\n■\nPriorities\n■\n■\nPolicies for reducing demand\n■\n■\nAllocation of portions of the technology to processors\n■\n■\nOther performance-related parameters\nDoes your choice of technology introduce excessive overhead \nfor heavily used operations?\n\n\n8.6  Discussion Questions\n145\n8.4  Summary\nPerformance is about the management of system resources in the face of partic-\nular types of demand to achieve acceptable timing behavior. Performance can be \nmeasured in terms of throughput and latency for both interactive and embedded \nreal-time systems, although throughput is usually more important in interactive \nsystems, and latency is more important in embedded systems.\nPerformance can be improved by reducing demand or by managing re-\nsources more appropriately. Reducing demand will have the side effect of re-\nducing fidelity or refusing to service some requests. Managing resources more \nappropriately can be done through scheduling, replication, or just increasing the \nresources available.\n8.5  For Further Reading\nPerformance has a rich body of literature. Here are some books we recommend:\n■\n■Software Performance and Scalability: A Quantitative Approach [Liu 09]. \nThis books covers performance geared toward enterprise applications, with \nan emphasis on queuing theory and measurement.\n■\n■Performance Solutions: A Practical Guide to Creating Responsive, Scal-\nable Software [Smith 01]. This book covers designing with performance in \nmind, with emphasis on building (and populating with real data) practical \npredictive performance models.\n■\n■Real-Time Design Patterns: Robust Scalable Architecture for Real-Time \nSystems [Douglass 99].\n■\n■Real-Time Systems [Liu 00]. \n■\n■Pattern-Oriented Software Architecture Volume 3: Patterns for Resource \nManagement [Kircher 03].\n8.6  Discussion Questions\n1.\t\n“Every system has real-time performance constraints.” Discuss. Or provide \na counterexample.\n2.\t\nWrite a performance scenario that describes the average on-time flight ar-\nrival performance for an airline.\n\n\n146 \nPart Two  Quality Attributes\t\n8—Performance\n3.\t\nWrite several performance scenarios for an automatic teller machine. Think \nabout whether your major concern is worst-case latency, average-case la-\ntency, throughput, or some other response measure. How would you modify \nyour automatic teller machine design to accommodate these scenarios?\n4.\t\nWeb-based systems often use proxy servers, which are the first element of \nthe system to receive a request from a client (such as your browser). Proxy \nservers are able to serve up often-requested web pages, such as a company’s \nhome page, without bothering the real application servers that carry out \ntransactions. There may be many proxy servers, and they are often located \ngeographically close to large user communities, to decrease response time \nfor routine requests. What performance tactics do you see at work here?\n5.\t\n A fundamental difference between coordination mechanisms is whether \ninteraction is synchronous or asynchronous. Discuss the advantages and \ndisadvantages of each with respect to each of the performance responses: \nlatency, deadline, throughput, jitter, miss rate, data loss, or any other re-\nquired performance-related response you may be used to.\n6.\t\nFind real-world (that is, nonsoftware) examples of applying each of the \nmanage-resources tactics. For example, suppose you were managing a \nbrick-and-mortar big-box retail store. How would you get people through \nthe checkout lines faster using these tactics?\n7.\t\nUser interface frameworks typically are single-threaded. Why is this so and \nwhat are the performance implications of this single-threading?\n\n\n147\n9\nSecurity\nWith Jungwoo Ryoo and Phil Laplante \nYour personal identity isn’t worth quite as much as \nit used to be—at least to thieves willing to swipe it. \nAccording to experts who monitor such markets, the \nvalue of stolen credit card data may range from $3 to \nas little as 40 cents. That’s down tenfold from a decade \nago—even though the cost to an individual who has a \ncredit card stolen can soar into the hundreds of dollars.\n—Forbes.com (Taylor Buley. “Hackonomics,” Forbes.com, \nOctober 27, 2008, www.forbes.com/2008/10/25/credit-card-\ntheft-tech-security-cz_tb1024theft.html)\nSecurity is a measure of the system’s ability to protect data and information from \nunauthorized access while still providing access to people and systems that are \nauthorized. An action taken against a computer system with the intention of do-\ning harm is called an attack and can take a number of forms. It may be an un-\nauthorized attempt to access data or services or to modify data, or it may be in-\ntended to deny services to legitimate users.\nThe simplest approach to characterizing security has three characteristics: \nconfidentiality, integrity, and availability (CIA):\n1.\t\nConfidentiality is the property that data or services are protected from \nunauthorized access. For example, a hacker cannot access your income tax \nreturns on a government computer.\n2.\t\nIntegrity is the property that data or services are not subject to unauthorized \nmanipulation. For example, your grade has not been changed since your \ninstructor assigned it.\n3.\t\nAvailability is the property that the system will be available for legitimate \nuse. For example, a denial-of-service attack won’t prevent you from order-\ning  book from an online bookstore.\n\n\n148 \nPart Two  Quality Attributes\t\n9—Security\nOther characteristics that are used to support CIA are these:\n4.\t\nAuthentication verifies the identities of the parties to a transaction and \nchecks if they are truly who they claim to be. For example, when you get \nan email purporting to come from a bank, authentication guarantees that it \nactually comes from the bank.\n5.\t\nNonrepudiation guarantees that the sender of a message cannot later deny \nhaving sent the message, and that the recipient cannot deny having received \nthe message. For example, you cannot deny ordering something from the \nInternet, or the merchant cannot disclaim getting your order.\n6.\t\nAuthorization grants a user the privileges to perform a task. For example, an \nonline banking system authorizes a legitimate user to access his account.\nWe will use these characteristics in our general scenarios for security. Approaches \nto achieving security can be characterized as those that detect attacks, those that \nresist attacks, those that react to attacks, and those that recover from successful \nattacks. The objects that are being protected from attacks are data at rest, data in \ntransit, and computational processes.\n9.1  Security General Scenario\nOne technique that is used in the security domain is threat modeling. An “attack \ntree,” similar to a fault tree discussed in Chapter 5, is used by security engineers \nto determine possible threats. The root is a successful attack and the nodes are \npossible direct causes of that successful attack. Children nodes decompose the \ndirect causes, and so forth. An attack is an attempt to break CIA, and the leaves of \nattack trees would be the stimulus in the scenario. The response to the attack is to \npreserve CIA or deter attackers through monitoring of their activities. From these \nconsiderations we can now describe the individual portions of a security general \nscenario. These are summarized in Table 9.1, and an example security scenario is \ngiven in Figure 9.1.\n■\n■Source of stimulus. The source of the attack may be either a human or \nanother system. It may have been previously identified (either correctly or \nincorrectly) or may be currently unknown. A human attacker may be from \noutside the organization or from inside the organization. \n■\n■Stimulus. The stimulus is an attack. We characterize this as an unauthorized \nattempt to display data, change or delete data, access system services, \nchange the system’s behavior, or reduce availability.\n■\n■Artifact. The target of the attack can be either the services of the system, \nthe data within it, or the data produced or consumed by the system. Some \nattacks are made on particular components of the system known to be \nvulnerable. \n\n\n9.1  Security General Scenario\n149\n■\n■Environment. The attack can come when the system is either online or \noffline, either connected to or disconnected from a network, either behind a \nfirewall or open to a network, fully operational, partially operational, or not \noperational.\n■\n■Response. The system should ensure that transactions are carried out in a \nfashion such that data or services are protected from unauthorized access; \ndata or services are not being manipulated without authorization; parties \nto a transaction are identified with assurance; the parties to the transaction \ncannot repudiate their involvements; and the data, resources, and system \nservices will be available for legitimate use. \nThe system should also track activities within it by recording access \nor modification; attempts to access data, resources, or services; and noti-\nfying appropriate entities (people or systems) when an apparent attack is \noccurring.\n■\n■Response measure. Measures of a system’s response include how much \nof a system is compromised when a particular component or data value is \ncompromised, how much time passed before an attack was detected, how \nmany attacks were resisted, how long it took to recover from a successful \nattack, and how much data was vulnerable to a particular attack.\nTable 9.1 enumerates the elements of the general scenario, which charac-\nterize security, and Figure 9.1 shows a sample concrete scenario: A disgruntled \nemployee from a remote location attempts to modify the pay rate table during \nnormal operations. The system maintains an audit trail, and the correct data is \nrestored within a day. \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nNormal\nOperations\n3\n2\n1\n4\nDisgruntled \nEmployee from \nRemote Location\nAttempts to \nModify Pay \nRate\nSystem \nMaintains \nAudit Trail\nCorrect Data Is \nRestored within a\nDay and Source \nof Tampering \nIdentified\nArtifact:\nData within\nthe System\nFigure 9.1  Sample concrete security scenario \n\n\n150 \nPart Two  Quality Attributes\t\n9—Security\nTable 9.1  Security General Scenario\nPortion of \nScenario\nPossible Values\nSource\nHuman or another system which may have been previously \nidentified (either correctly or incorrectly) or may be currently \nunknown. A human attacker may be from outside the organization or \nfrom inside the organization.\nStimulus\nUnauthorized attempt is made to display data, change or delete \ndata, access system services, change the system’s behavior, or \nreduce availability.\nArtifact\nSystem services, data within the system, a component or resources \nof the system, data produced or consumed by the system\nEnvironment\nThe system is either online or offline; either connected to or \ndisconnected from a network; either behind a firewall or open to a \nnetwork; fully operational, partially operational, or not operational.\nResponse\nTransactions are carried out in a fashion such that \n■\n■\nData or services are protected from unauthorized access. \n■\n■\nData or services are not being manipulated without authorization.\n■\n■\nParties to a transaction are identified with assurance. \n■\n■\nThe parties to the transaction cannot repudiate their \ninvolvements. \n■\n■\nThe data, resources, and system services will be available for \nlegitimate use. \nThe system tracks activities within it by\n■\n■\nRecording access or modification \n■\n■\nRecording attempts to access data, resources, or services \n■\n■\nNotifying appropriate entities (people or systems) when an \napparent attack is occurring\nResponse \nMeasure\nOne or more of the following:\n■\n■\nHow much of a system is compromised when a particular \ncomponent or data value is compromised\n■\n■\nHow much time passed before an attack was detected \n■\n■\nHow many attacks were resisted \n■\n■\nHow long does it take to recover from a successful attack \n■\n■\nHow much data is vulnerable to a particular attack\n9.2  Tactics for Security\nOne method for thinking about how to achieve security in a system is to think \nabout physical security. Secure installations have limited access (e.g., by using \nsecurity checkpoints), have means of detecting intruders (e.g., by requiring le-\ngitimate visitors to wear badges), have deterrence mechanisms such as armed \nguards, have reaction mechanisms such as automatic locking of doors, and have \nrecovery mechanisms such as off-site backup. These lead to our four categories \nof tactics: detect, resist, react, and recover. Figure 9.2 shows these categories as \nthe goal of security tactics.\n\n\n9.2  Tactics for Security\n151\nAttack\nSystem Detects, Resists,\nReacts, or Recovers \nTactics\nto Control\nSecurity\nFigure 9.2  The goal of security tactics \nDetect Attacks\nThe detect attacks category consists of four tactics: detect intrusion, detect service \ndenial, verify message integrity, and detect message delay.\n■\n■Detect intrusion is the comparison of network traffic or service request \npatterns within a system to a set of signatures or known patterns of \nmalicious behavior stored in a database. The signatures can be based on \nprotocol, TCP flags, payload sizes, applications, source or destination \naddress, or port number.\n■\n■Detect service denial is the comparison of the pattern or signature of \nnetwork traffic coming into a system to historic profiles of known denial-of-\nservice attacks. \n■\n■Verify message integrity. This tactic employs techniques such as \nchecksums or hash values to verify the integrity of messages, resource \nfiles, deployment files, and configuration files. A checksum is a validation \nmechanism wherein the system maintains redundant information for \nconfiguration files and messages, and uses this redundant information \nto verify the configuration file or message when it is used. A hash value \nis a unique string generated by a hashing function whose input could be \nconfiguration files or messages. Even a slight change in the original files or \nmessages results in a significant change in the hash value.\n■\n■Detect message delay is intended to detect potential man-in-the-middle \nattacks, where a malicious party is intercepting (and possibly modifying) \nmessages. By checking the time that it takes to deliver a message, it is \npossible to detect suspicious timing behavior, where the time it takes to \ndeliver a message is highly variable.\n",
      "page_number": 163
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 172-179)",
      "start_page": 172,
      "end_page": 179,
      "detection_method": "topic_boundary",
      "content": "152 \nPart Two  Quality Attributes\t\n9—Security\nResist Attacks\nThere are a number of well-known means of resisting an attack:\n■\n■Identify actors. Identifying “actors” is really about identifying the source of \nany external input to the system. Users are typically identified through user \nIDs. Other systems may be “identified” through access codes, IP addresses, \nprotocols, ports, and so on.\n■\n■Authenticate actors. Authentication means ensuring that an actor (a user or \na remote computer) is actually who or what it purports to be. Passwords, \none-time passwords, digital certificates, and biometric identification \nprovide a means for authentication.\n■\n■Authorize actors. Authorization means ensuring that an authenticated actor \nhas the rights to access and modify either data or services. This mechanism \nis usually enabled by providing some access control mechanisms within \na system. Access control can be by an actor or by an actor class. Classes \nof actors can be defined by actor groups, by actor roles, or by lists of \nindividuals.\n■\n■Limit access. Limiting access involves controlling what and who may access \nwhich parts of a system. This may include limiting access to resources such \nas processors, memory, and network connections, which may be achieved \nby using process management, memory protection, blocking a host, closing \na port, or rejecting a protocol. For example, a firewall is a single point of \naccess to an organization’s intranet. A demilitarized zone (DMZ) is a subnet \nbetween the Internet and an intranet, protected by two firewalls: one facing \nthe Internet and the other the intranet. A DMZ is used when an organization \nwants to let external users access services that should be publicly available \noutside the intranet. This way the number of open ports in the internal firewall \ncan be minimized. This tactic also limits access for actors (by identifying, \nauthenticating, and authorizing them).\n■\n■Limit exposure. Limiting exposure refers to ultimately and indirectly \nreducing the probability of a successful attack, or restricting the amount of \npotential damage. This can be achieved by concealing facts about a system \nto be protected (“security by obscurity”) or by dividing and distributing \ncritical resources so that the exploitation of a single weakness cannot fully \ncompromise any resource (“don’t put all your eggs in one basket”). For \nexample, a design decision to hide how many entry points a system has is a \nway of limiting exposure. A decision to distribute servers amongst several \ngeographically dispersed data centers is also a way of limiting exposure.\n■\n■Encrypt data. Data should be protected from unauthorized access. \nConfidentiality is usually achieved by applying some form of encryption \nto data and to communication. Encryption provides extra protection to \npersistently maintained data beyond that available from authorization. \nCommunication links, on the other hand, may not have authorization \ncontrols. In such cases, encryption is the only protection for passing data \nover publicly accessible communication links. The link can be implemented \nby a virtual private network (VPN) or by a Secure Sockets Layer (SSL) for \n\n\n9.2  Tactics for Security\n153\na web-based link. Encryption can be symmetric (both parties use the same \nkey) or asymmetric (public and private keys).\n■\n■Separate entities. Separating different entities within the system can be \ndone through physical separation on different servers that are attached \nto different networks; the use of virtual machines (see Chapter 26 for \na discussion of virtual machines); or an “air gap,” that is, by having no \nconnection between different portions of a system. Finally, sensitive \ndata is frequently separated from nonsensitive data to reduce the attack \npossibilities from those who have access to nonsensitive data.\n■\n■Change default settings. Many systems have default settings assigned \nwhen the system is delivered. Forcing the user to change those settings will \nprevent attackers from gaining access to the system through settings that \nare, generally, publicly available.\nReact to Attacks\nSeveral tactics are intended to respond to a potential attack: \n■\n■Revoke access. If the system or a system administrator believes that \nan attack is underway, then access can be severely limited to sensitive \nresources, even for normally legitimate users and uses. For example, if your \ndesktop has been compromised by a virus, your access to certain resources \nmay be limited until the virus is removed from your system.\n■\n■Lock computer. Repeated failed login attempts may indicate a potential \nattack. Many systems limit access from a particular computer if there \nare repeated failed attempts to access an account from that computer. \nLegitimate users may make mistakes in attempting to log in. Therefore, the \nlimited access may only be for a certain time period.\n■\n■Inform actors. Ongoing attacks may require action by operators, other \npersonnel, or cooperating systems. Such personnel or systems—the set of \nrelevant actors—must be notified when the system has detected an attack.\nRecover from Attacks\nOnce a system has detected and attempted to resist an attack, it needs to recover. \nPart of recovery is restoration of services. For example, additional servers or net-\nwork connections may be kept in reserve for such a purpose. Since a successful \nattack can be considered a kind of failure, the set of availability tactics (from \nChapter 5) that deal with recovering from a failure can be brought to bear for this \naspect of security as well. \nIn addition to the availability tactics that permit restoration of services, we \nneed to maintain an audit trail. We audit—that is, keep a record of user and sys-\ntem actions and their effects—to help trace the actions of, and to identify, an at-\ntacker. We may analyze audit trails to attempt to prosecute attackers, or to create \nbetter defenses in the future.\nThe set of security tactics is shown in Figure 9.3.\n\n\n154 \nPart Two  Quality Attributes\t\n9—Security\nSecurity Tactics\nResist Attacks\nEncrypt Data\nAttack\nSystem Detects,\nResists, Reacts,\nor Recovers\nDetect Attacks\nMaintain\nAudit Trail\nLimit Exposure\nRecover\nfrom Attacks\nReact to\nAttacks\nRevoke\nAccess\nLock\nComputer\nDetect\nIntrusion\nDetect Service\nDenial\nVerify Message\nIntegrity\nDetect Message\nDelay\nChange Default\nSettings\nSeparate\nEntities\nRestore\nSee\nAvailability\nIdentify\nActors\nAuthenticate\nActors\nAuthorize\nActors\nLimit Access\nInform\nActors\nFIGURE 9.3  Security tactics\n9.3  A Design Checklist for Security\nTable 9.2 is a checklist to support the design and analysis process for security.\nTABLE 9.2  Checklist to Support the Design and Analysis Process for Security\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which system responsibilities need to be secure. \nFor each of these responsibilities, ensure that additional \nresponsibilities have been allocated to do the following:\n■\n■\nIdentify the actor\n■\n■\nAuthenticate the actor\n■\n■\nAuthorize actors\n■\n■\nGrant or deny access to data or services\n■\n■\nRecord attempts to access or modify data or services\n■\n■\nEncrypt data\n■\n■\nRecognize reduced availability for resources or services and \ninform appropriate personnel and restrict access\n■\n■\nRecover from an attack\n■\n■\nVerify checksums and hash values\n\n\n9.3  A Design Checklist for Security\n155\nCategory\nChecklist\nCoordination \nModel\nDetermine mechanisms required to communicate and coordinate \nwith other systems or individuals. For these communications, \nensure that mechanisms for authenticating and authorizing the \nactor or system, and encrypting data for transmission across \nthe connection, are in place. Ensure also that mechanisms exist \nfor monitoring and recognizing unexpectedly high demands for \nresources or services as well as mechanisms for restricting or \nterminating the connection.\nData Model\nDetermine the sensitivity of different data fields. For each data \nabstraction:\n■\n■\nEnsure that data of different sensitivity is separated.\n■\n■\nEnsure that data of different sensitivity has different access \nrights and that access rights are checked prior to access.\n■\n■\nEnsure that access to sensitive data is logged and that the log \nfile is suitably protected.\n■\n■\nEnsure that data is suitably encrypted and that keys are \nseparated from the encrypted data.\n■\n■\nEnsure that data can be restored if it is inappropriately \nmodified.\nMapping among \nArchitectural \nElements\nDetermine how alternative mappings of architectural elements \nthat are under consideration may change how an individual or \nsystem may read, write, or modify data; access system services or \nresources; or reduce availability to system services or resources. \nDetermine how alternative mappings may affect the recording \nof access to data, services or resources and the recognition of \nunexpectedly high demands for resources.\nFor each such mapping, ensure that there are responsibilities to do \nthe following:\n■\n■\nIdentify an actor\n■\n■\nAuthenticate an actor\n■\n■\nAuthorize actors\n■\n■\nGrant or deny access to data or services\n■\n■\nRecord attempts to access or modify data or services\n■\n■\nEncrypt data\n■\n■\nRecognize reduced availability for resources or services, inform \nappropriate personnel, and restrict access\n■\n■\nRecover from an attack\nResource \nManagement\nDetermine the system resources required to identify and monitor \na system or an individual who is internal or external, authorized or \nnot authorized, with access to specific resources or all resources.\nDetermine the resources required to authenticate the actor, grant \nor deny access to data or resources, notify appropriate entities \n(people or systems), record attempts to access data or resources, \nencrypt data, recognize inexplicably high demand for resources, \ninform users or systems, and restrict access. \nFor these resources consider whether an external entity can \naccess a critical resource or exhaust a critical resource; how to \nmonitor the resource; how to manage resource utilization; how \nto log resource utilization; and ensure that there are sufficient \nresources to perform the necessary security operations.\nEnsure that a contaminated element can be prevented from \ncontaminating other elements.\nEnsure that shared resources are not used for passing sensitive \ndata from an actor with access rights to that data to an actor \nwithout access rights to that data.\ncontinues\n\n\n156 \nPart Two  Quality Attributes\t\n9—Security\nTable 9.2  Checklist to Support the Design and Analysis Process for Security, \ncontinued\nCategory\nChecklist\nBinding Time\nDetermine cases where an instance of a late-bound component \nmay be untrusted. For such cases ensure that late-bound \ncomponents can be qualified; that is, if ownership certificates \nfor late-bound components are required, there are appropriate \nmechanisms to manage and validate them; that access to \nlate-bound data and services can be managed; that access by \nlate-bound components to data and services can be blocked; that \nmechanisms to record the access, modification, and attempts to \naccess data or services by late-bound components are in place; \nand that system data is encrypted where the keys are intentionally \nwithheld for late-bound components\nChoice of \nTechnology\nDetermine what technologies are available to help user \nauthentication, data access rights, resource protection, and data \nencryption.\nEnsure that your chosen technologies support the tactics relevant \nfor your security needs.\n9.4  Summary\nAttacks against a system can be characterized as attacks against the confidential-\nity, integrity, or availability of a system or its data. Confidentiality means keeping \ndata away from those who should not have access while granting access to those \nwho should. Integrity means that there are no unauthorized modifications to or \ndeletion of data, and availability means that the system is accessible to those who \nare entitled to use it.\nThe emphasis of distinguishing various classes of actors in the characteri-\nzation leads to many of the tactics used to achieve security. Identifying, authen-\nticating, and authorizing actors are tactics intended to determine which users or \nsystems are entitled to what kind of access to a system.\nAn assumption is made that no security tactic is foolproof and that systems \nwill be compromised. Hence, tactics exist to detect an attack, limit the spread of \nany attack, and to react and recover from an attack.\nRecovering from an attack involves many of the same tactics as availability \nand, in general, involves returning the system to a consistent state prior to any attack.\n\n\n9.5  For Further Reading\n157\n9.5  For Further Reading\nThe architectural tactics that we have described in this chapter are only one as-\npect of making a system secure. Other aspects are these:\n■\n■Coding. Secure Coding in C and C++ [Seacord 05] describes how to code \nsecurely. The Common Weakness Enumeration [CWE 12] is a list of the \nmost common vulnerabilities discovered in systems. \n■\n■Organizational processes. Organizations must have processes that provide \nfor responsibility for various aspects of security, including ensuring that \nsystems are patched to put into place the latest protections. The National \nInstitute of Standards and Technology (NIST) provides an enumeration of \norganizational processes [NIST 09]. [Cappelli 12] discusses insider threats.\n■\n■Technical processes. Microsoft has a life-cycle development process (The \nSecure Development Life Cycle) that includes modeling of threats. Four \ntraining classes are publicly available. www.microsoft.com/download/en/\ndetails.aspx?id=16420\nNIST has several volumes that give definitions of security terms [NIST 04], \ncategories of security controls [NIST 06], and an enumeration of security con-\ntrols that an organization could employ [NIST 09]. A security control could be a \ntactic, but it could also be organizational, coding-related, or a technical process.\nThe attack surface of a system is the code that can be run by unauthorized \nusers. A discussion of how to minimize the attack surface for a system can be \nfound at [Howard 04].\nEncryption and certificates of various types and strengths are commonly \nused to resist certain types of attacks. Encryption algorithms are particularly dif-\nficult to code correctly. A document produced by NIST [NIST 02] gives require-\nments for these algorithms.\nGood books on engineering systems for security have been written by Ross \nAnderson [Anderson 08] and Bruce Schneier [Schneier 08].\nDifferent domains have different specific sets of practices. The Payment \nCard Industry (PCI) has a set of standards intended for those involved in credit \ncard processing (www.pcisecuritystandards.org). There is also a set of recom-\nmendations for securing various portions of the electric grid (www.smartgridipe-\ndia.org/index.php/ASAP-SG).\nData on the various sources of data breaches can be found in the Verizon \n2012 Data Breach Investigations Report [Verizon 12].\nJohn Viega has written several books about secure software development in \nvarious environments. See, for example, [Viega 01].\n\n\n158 \nPart Two  Quality Attributes\t\n9—Security\n9.6  Discussion Questions\n1.\t\nWrite a set of concrete scenarios for security for an automatic teller ma-\nchine. How would you modify your design for the automatic teller machine \nto satisfy these scenarios?\n2.\t\nOne of the most sophisticated attacks on record was carried out by a virus \nknown as Stuxnet. Stuxnet first appeared in 2009 but became widely known \nin 2011 when it was revealed that it had apparently severely damaged or \nincapacitated the high-speed centrifuges involved in Iran’s uranium en-\nrichment program. Read about Stuxnet and see if you can devise a defense \nstrategy against it based on the tactics in this chapter.\n3.\t\nSome say that inserting security awareness into the software develop-\nment life cycle is at least as important as designing software with security \ncountermeasures. What are some examples of software development pro-\ncesses that can lead to more-secure systems?\n4.\t\nSecurity and usability are often seen to be at odds with each other. Security \noften imposes procedures and processes that seem like needless overhead to \nthe casual user. But some say that security and usability go (or should go) \nhand in hand and argue that making the system easy to use securely is the \nbest way to promote security to the user. Discuss.\n5.\t\nList some examples of critical resources for security that might become \nexhausted.\n6.\t\nList an example of a mapping of architectural elements that has strong se-\ncurity implications. Hint: think of where data is stored.\n7.\t\nWhich of the tactics in our list will protect against an insider threat? Can \nyou think of any that should be added?\n8.\t\nIn the United States, Facebook can account for more than 5 percent of all \nInternet traffic in a given week. How would you recognize a denial-of-ser-\nvice attack on Facebook.com?\n9.\t\nThe public disclosure of vulnerabilities in production systems is a matter of \ncontroversy. Discuss why this is so and the pros and cons of public disclo-\nsure of vulnerabilities.\n\n\n159\n10\nTestability\nTesting leads to failure, and failure \nleads to understanding\n—Burt Rutan\nIndustry estimates indicate that between 30 and 50 percent (or in some cases, \neven more) of the cost of developing well-engineered systems is taken up by test-\ning. If the software architect can reduce this cost, the payoff is large.\nSoftware testability refers to the ease with which software can be made to \ndemonstrate its faults through (typically execution-based) testing. Specifically, \ntestability refers to the probability, assuming that the software has at least one \nfault, that it will fail on its next test execution. Intuitively, a system is testable if it \n“gives up” its faults easily. If a fault is present in a system, then we want it to fail \nduring testing as quickly as possible. Of course, calculating this probability is not \neasy and, as you will see when we discuss response measures for testability, other \nmeasures will be used.\nFigure 10.1 shows a model of testing in which a program processes input \nand produces output. An oracle is an agent (human or mechanical) that decides \nwhether the output is correct or not by comparing the output to the program’s \nspecification. Output is not just the functionally produced value, but it also can \ninclude derived measures of quality attributes such as how long it took to produce \nthe output. Figure 10.1 also shows that the program’s internal state can also be \nshown to the oracle, and an oracle can decide whether that is correct or not—that \nis, it can detect whether the program has entered an erroneous state and render a \njudgment as to the correctness of the program. \nSetting and examining a program’s internal state is an aspect of testing that \nwill figure prominently in our tactics for testability.\n",
      "page_number": 172
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 180-187)",
      "start_page": 180,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": "160 \nPart Two  Quality Attributes\t\n10—Testability\nProgram\nOracle\n{ \n}\ninput\noutput\napproved\nrejected\ninternal state\nFigure 10.1  A model of testing\nFor a system to be properly testable, it must be possible to control each compo-\nnent’s inputs (and possibly manipulate its internal state) and then to observe its \noutputs (and possibly its internal state, either after or on the way to computing \nthe outputs). Frequently this control and observation is done through the use of a \ntest harness, which is specialized software (or in some cases, hardware) designed \nto exercise the software under test. Test harnesses come in various forms, such \nas a record-and-playback capability for data sent across various interfaces, or a \nsimulator for an external environment in which a piece of embedded software is \ntested, or even during production (see sidebar). The test harness can provide as-\nsistance in executing the test procedures and recording the output. A test harness \ncan be a substantial piece of software in its own right, with its own architecture, \nstakeholders, and quality attribute requirements. \nTesting is carried out by various developers, users, or quality assurance per-\nsonnel. Portions of the system or the entire system may be tested. The response \nmeasures for testability deal with how effective the tests are in discovering faults \nand how long it takes to perform the tests to some desired level of coverage. Test \ncases can be written by the developers, the testing group, or the customer. The \ntest cases can be a portion of acceptance testing or can drive the development as \nthey do in certain types of Agile methodologies.\nNetflix’s Simian Army\nNetflix distributes movies and television shows both via DVD and via \nstreaming video. Their streaming video service has been extremely suc-\ncessful. In May 2011 Netflix streaming video accounted for 24 percent of the \nInternet traffic in North America. Naturally, high availability is important to \nNetflix.\nNetflix hosts their computer services in the Amazon EC2 cloud, and they \nutilize what they call a “Simian Army” as a portion of their testing process. \nThey began with a Chaos Monkey, which randomly kills processes in the \n\n\n\t\n10—Testability  161\nrunning system. This allows the monitoring of the effect of failed processes \nand gives the ability to ensure that the system does not fail or suffer serious \ndegradation as a result of a process failure. \nRecently, the Chaos Monkey got some friends to assist in the testing. \nCurrently, the Netflix Simian Army includes these:\n■\n■\nThe Latency Monkey induces artificial delays in the client-server \ncommunication layer to simulate service degradation and measures if \nupstream services respond appropriately. \n■\n■\nThe Conformity Monkey finds instances that don’t adhere to best \npractices and shuts them down. For example, if an instance does not \nbelong to an auto-scaling group, it will not appropriately scale when \ndemand goes up.\n■\n■\nThe Doctor Monkey taps into health checks that run on each instance as \nwell as monitors other external signs of health (e.g., CPU load) to detect \nunhealthy instances. \n■\n■\nThe Janitor Monkey ensures that the Netflix cloud environment is \nrunning free of clutter and waste. It searches for unused resources and \ndisposes of them.\n■\n■\nThe Security Monkey is an extension of Conformity Monkey. It finds \nsecurity violations or vulnerabilities, such as improperly configured \nsecurity groups, and terminates the offending instances. It also ensures \nthat all the SSL and digital rights management (DRM) certificates are \nvalid and are not coming up for renewal.\n■\n■\nThe 10-18 Monkey (localization-internationalization) detects \nconfiguration and runtime problems in instances serving customers in \nmultiple geographic regions, using different languages and character \nsets. The name 10-18 comes from L10n-i18n, a sort of shorthand for the \nwords localization and internationalization.\nSome of the members of the Simian Army use fault injection to place \nfaults into the running system in a controlled and monitored fashion. \nOther members monitor various specialized aspects of the system and its \nenvironment. Both of these techniques have broader applicability than just \nNetflix.\nNot all faults are equal in terms of severity. More emphasis should be \nplaced on finding the most severe faults than on finding other faults. The \nSimian Army reflects a determination by Netflix that the faults they look for \nare the most serious in terms of their impact.\nThis strategy illustrates that some systems are too complex and adap-\ntive to be tested fully, because some of their behaviors are emergent. An \naspect of testing in that arena is logging of operational data produced by \nthe system, so that when failures occur, the logged data can be analyzed in \nthe lab to try to reproduce the faults. Architecturally this can require mecha-\nnisms to access and log certain system state. The Simian Army is one way \nto discover and log behavior in systems of this ilk.\n—LB\n\n\n162 \nPart Two  Quality Attributes\t\n10—Testability\nTesting of code is a special case of validation, which is making sure that an \nengineered artifact meets the needs of its stakeholders or is suitable for use. In \nChapter 21 we will discuss architectural design reviews. This is another kind of \nvalidation, where the artifact being tested is the architecture. In this chapter we \nare concerned only with the testability of a running system and of its source code. \n10.1  Testability General Scenario\nWe can now describe the general scenario for testability.\n■\n■Source of stimulus. The testing is performed by unit testers, integration \ntesters, or system testers (on the developing organization side), or \nacceptance testers and end users (on the customer side). The source could \nbe human or an automated tester.\n■\n■Stimulus. A set of tests is executed due to the completion of a coding incre-\nment such as a class layer or service, the completed integration of a subsys-\ntem, the complete implementation of the whole system, or the delivery of \nthe system to the customer.\n■\n■Artifact. A unit of code (corresponding to a module in the architecture), a \nsubsystem, or the whole system is the artifact being tested. \n■\n■Environment. The test can happen at development time, at compile time, at \ndeployment time, or while the system is running (perhaps in routine use). The \nenvironment can also include the test harness or test environments in use.\n■\n■Response. The system can be controlled to perform the desired tests and the \nresults from the test can be observed. \n■\n■Response measure. Response measures are aimed at representing how eas-\nily a system under test “gives up” its faults. Measures might include the \neffort involved in finding a fault or a particular class of faults, the effort \nrequired to test a given percentage of statements, the length of the longest \ntest chain (a measure of the difficulty of performing the tests), measures of \neffort to perform the tests, measures of effort to actually find faults, esti-\nmates of the probability of finding additional faults, and the length of time \nor amount of effort to prepare the test environment. \nMaybe one measure is the ease at which the system can be brought into \na specific state. In addition, measures of the reduction in risk of the remain-\ning errors in the system can be used. Not all faults are equal in terms of \ntheir possible impact. Measures of risk reduction attempt to rate the severity \nof faults found (or to be found). \nFigure 10.2 shows a concrete scenario for testability. The unit tester com-\npletes a code unit during development and performs a test sequence whose results \nare captured and that gives 85 percent path coverage within three hours of testing. \n\n\n10.1  Testability General Scenario\n163\nTable 10.1 enumerates the elements of the general scenario that characterize \ntestability.\nTable 10.1  Testability General Scenario\nPortion of Scenario\nPossible Values\nSource\nUnit testers, integration testers, system testers, acceptance \ntesters, end users, either running tests manually or using \nautomated testing tools\nStimulus \nA set of tests is executed due to the completion of a coding \nincrement such as a class layer or service, the completed \nintegration of a subsystem, the complete implementation of the \nwhole system, or the delivery of the system to the customer.\nEnvironment \nDesign time, development time, compile time, integration time, \ndeployment time, run time\nArtifacts \nThe portion of the system being tested\nResponse \nOne or more of the following: execute test suite and capture \nresults, capture activity that resulted in the fault, control and \nmonitor the state of the system \nResponse Measure \nOne or more of the following: effort to find a fault or class of \nfaults, effort to achieve a given percentage of state space \ncoverage, probability of fault being revealed by the next \ntest, time to perform tests, effort to detect faults, length of \nlongest dependency chain in test, length of time to prepare \ntest environment, reduction in risk exposure (size(loss) × \nprob(loss))\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nDevelopment\n3\n2\n1\n4\nUnit Tester\nCode Unit \nCompleted\nResults Captured\n85% Path Coverage \nin Three Hours\nArtifact:\nCode Unit\nFigure 10. 2  Sample concrete testability scenario\n\n\n164 \nPart Two  Quality Attributes\t\n10—Testability\n10.2  Tactics for Testability\nThe goal of tactics for testability is to allow for easier testing when an increment \nof software development is completed. Figure 10.3 displays the use of tactics for \ntestability. Architectural techniques for enhancing the software testability have not \nreceived as much attention as more mature quality attribute disciplines such as \nmodifiability, performance, and availability, but as we stated before, anything the \narchitect can do to reduce the high cost of testing will yield a significant benefit. \nThere are two categories of tactics for testability. The first category deals \nwith adding controllability and observability to the system. The second deals \nwith limiting complexity in the system’s design. \nControl and Observe System State\nControl and observation are so central to testability that some authors even define \ntestability in those terms. The two go hand-in-hand; it makes no sense to control \nsomething if you can’t observe what happens when you do. The simplest form of \ncontrol and observation is to provide a software component with a set of inputs, \nlet it do its work, and then observe its outputs. However, the control and observe \nsystem state category of testability tactics provides insight into software that goes \nbeyond its inputs and outputs. These tactics cause a component to maintain some \nsort of state information, allow testers to assign a value to that state information, \nand/or make that information accessible to testers on demand. The state infor-\nmation might be an operating state, the value of some key variable, performance \nload, intermediate process steps, or anything else useful to re-creating component \nbehavior. Specific tactics include the following:\nTests\nExecuted\nFaults\nDetected\nTactics\nto Control\nTestability\nFigure 10.3  The goal of testability tactics\n\n\n10.2  Tactics for Testability\n165\n■\n■Specialized interfaces. Having specialized testing interfaces allows you \nto control or capture variable values for a component either through a test \nharness or through normal execution. Examples of specialized test routines \ninclude these:\n■\n■A set and get method for important variables, modes, or attributes \n(methods that might otherwise not be available except for testing \npurposes)\n■\n■A report method that returns the full state of the object \n■\n■A reset method to set the internal state (for example, all the attributes of a \nclass) to a specified internal state\n■\n■A method to turn on verbose output, various levels of event logging, \nperformance instrumentation, or resource monitoring\nSpecialized testing interfaces and methods should be clearly identified or \nkept separate from the access methods and interfaces for required function-\nality, so that they can be removed if needed. (However, in performance-crit-\nical and some safety-critical systems, it is problematic to field different \ncode than that which was tested. If you remove the test code, how will you \nknow the code you field has the same behavior, particularly the same timing \nbehavior, as the code you tested? For other kinds of systems, however, this \nstrategy is effective.)\n■\n■Record/playback. The state that caused a fault is often difficult to re-create. \nRecording the state when it crosses an interface allows that state to be used \nto “play the system back” and to re-create the fault. Record/playback refers \nto both capturing information crossing an interface and using it as input for \nfurther testing. \n■\n■Localize state storage. To start a system, subsystem, or module in an arbi-\ntrary state for a test, it is most convenient if that state is stored in a single \nplace. By contrast, if the state is buried or distributed, this becomes difficult \nif not impossible. The state can be fine-grained, even bit-level, or coarse-\ngrained to represent broad abstractions or overall operational modes. The \nchoice of granularity depends on how the states will be used in testing. A \nconvenient way to “externalize” state storage (that is, to make it able to be \nmanipulated through interface features) is to use a state machine (or state \nmachine object) as the mechanism to track and report current state.\n■\n■Abstract data sources. Similar to controlling a program’s state, easily con-\ntrolling its input data makes it easier to test. Abstracting the interfaces lets \nyou substitute test data more easily. For example, if you have a database of \ncustomer transactions, you could design your architecture so that it is easy \nto point your test system at other test databases, or possibly even to files of \ntest data instead, without having to change your functional code.\n■\n■Sandbox. “Sandboxing” refers to isolating an instance of the system from \nthe real world to enable experimentation that is unconstrained by the worry \n\n\n166 \nPart Two  Quality Attributes\t\n10—Testability\nabout having to undo the consequences of the experiment. Testing is helped \nby the ability to operate the system in such a way that it has no permanent \nconsequences, or so that any consequences can be rolled back. This can \nbe used for scenario analysis, training, and simulation. (The Spring frame-\nwork, which is quite popular in the Java community, comes with a set of \ntest utilities that support this. Tests are run as a “transaction,” which is \nrolled back at the end.)\nA common form of sandboxing is to virtualize resources. Testing a \nsystem often involves interacting with resources whose behavior is outside \nthe control of the system. Using a sandbox, you can build a version of the \nresource whose behavior is under your control. For example, the system \nclock’s behavior is typically not under our control—it increments one \nsecond each second—which means that if we want to make the system \nthink it’s midnight on the day when all of the data structures are supposed \nto overflow, we need a way to do that, because waiting around is a poor \nchoice. By having the capability to abstract system time from clock time, \nwe can allow the system (or components) to run at faster than wall-clock \ntime, and to allow the system (or components) to be tested at critical time \nboundaries (such as the next shift on or off Daylight Savings Time). Similar \nvirtualizations could be done for other resources, such as memory, battery, \nnetwork, and so on. Stubs, mocks, and dependency injection are simple but \neffective forms of virtualization.\n■\n■Executable assertions. Using this tactic, assertions are (usually) hand-coded \nand placed at desired locations to indicate when and where a program is in \na faulty state. The assertions are often designed to check that data values \nsatisfy specified constraints. Assertions are defined in terms of specific data \ndeclarations, and they must be placed where the data values are referenced \nor modified. Assertions can be expressed as pre- and post-conditions for \neach method and also as class-level invariants. This results in increasing \nobservability, when an assertion is flagged as having failed. Assertions \nsystematically inserted where data values change can be seen as a manual \nway to produce an “extended” type. Essentially, the user is annotating \na type with additional checking code. Any time an object of that type is \nmodified, the checking code is automatically executed, and warnings are \ngenerated if any conditions are violated. To the extent that the assertions \ncover the test cases, they effectively embed the test oracle in the code—\nassuming the assertions are correct and correctly coded.\nAll of these tactics add capability or abstraction to the software that (were we \nnot interested in testing) otherwise would not be there. They can be seen as replac-\ning bare-bones, get-the-job-done software with more elaborate software that has \nbells and whistles for testing. There are a number of techniques for effecting this \nreplacement. These are not testability tactics, per se, but techniques for replacing \none component with a different version of itself. They include the following:\n\n\n10.2  Tactics for Testability\n167\n■\n■Component replacement, which simply swaps the implementation of a \ncomponent with a different implementation that (in the case of testability) \nhas features that facilitate testing. Component replacement is often \naccomplished in a system’s build scripts.\n■\n■Preprocessor macros that, when activated, expand to state-reporting code or \nactivate probe statements that return or display information, or return con-\ntrol to a testing console.\n■\n■Aspects (in aspect-oriented programs) that handle the cross-cutting concern \nof how state is reported.\nLimit Complexity\nComplex software is harder to test. This is because, by the definition of complex-\nity, its operating state space is very large and (all else being equal) it is more dif-\nficult to re-create an exact state in a large state space than to do so in a small state \nspace. Because testing is not just about making the software fail but about finding \nthe fault that caused the failure so that it can be removed, we are often concerned \nwith making behavior repeatable. This category has three tactics:\n■\n■Limit structural complexity. This tactic includes avoiding or resolving \ncyclic dependencies between components, isolating and encapsulating \ndependencies on the external environment, and reducing dependencies \nbetween components in general (for example, reduce the number of \nexternal accesses to a module’s public data). In object-oriented systems, \nyou can simplify the inheritance hierarchy: Limit the number of classes \nfrom which a class is derived, or the number of classes derived from a \nclass. Limit the depth of the inheritance tree, and the number of children of \na class. Limit polymorphism and dynamic calls. One structural metric that \nhas been shown empirically to correlate to testability is called the response \nof a class. The response of class C is a count of the number of methods \nof C plus the number of methods of other classes that are invoked by the \nmethods of C. Keeping this metric low can increase testability.\nHaving high cohesion, loose coupling, and separation of concerns—all \nmodifiability tactics (see Chapter 7)—can also help with testability. They \nare a form of limiting the complexity of the architectural elements by \ngiving each element a focused task with limited interaction with other ele-\nments. Separation of concerns can help achieve controllability and observ-\nability (as well as reducing the size of the overall program’s state space). \nControllability is critical to making testing tractable, as Robert Binder has \nnoted: “A component that can act independently of others is more readily \ncontrollable. . . . With high coupling among classes it is typically more \ndifficult to control the class under test, thus reducing testability. . . . If user \ninterface capabilities are entwined with basic functions it will be more \ndifficult to test each function” [Binder 94].\n",
      "page_number": 180
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 188-195)",
      "start_page": 188,
      "end_page": 195,
      "detection_method": "topic_boundary",
      "content": "168 \nPart Two  Quality Attributes\t\n10—Testability\nAlso, systems that require complete data consistency at all times are of-\nten more complex than those that do not. If your requirements allow it, con-\nsider building your system under the “eventual consistency” model, where \nsooner or later (but maybe not right now) your data will reach a consistent \nstate. This often makes system design simpler, and therefore easier to test.\nFinally, some architectural styles lend themselves to testability. In a \nlayered style, you can test lower layers first, then test higher layers with \nconfidence in the lower layers. \n■\n■Limit nondeterminism. The counterpart to limiting structural complexity \nis limiting behavioral complexity, and when it comes to testing, \nnondeterminism is a very pernicious form of complex behavior. \nNondeterministic systems are harder to test than deterministic systems. \nThis tactic involves finding all the sources of nondeterminism, such as \nunconstrained parallelism, and weeding them out as much as possible. \nSome sources of nondeterminism are unavoidable—for instance, in multi-\nthreaded systems that respond to unpredictable events—but for such \nsystems, other tactics (such as record/playback) are available. \nFigure 10.4 provides a summary of the tactics used for testability.\nTestability Tactics\nControl and Observe\nSystem State\nLimit Complexity\nSpecialized\nInterfaces\nLimit Structural\nComplexity\nLimit\nNondeterminism\nTests\nExecuted\nFaults\nDetected\nRecord/\nPlayback\nLocalize State\nStorage\nSandbox\nExecutable\nAssertions\nAbstract Data\nSources\nFigure 10.4  Testability tactics\n\n\n10.3  A Design Checklist for Testability\n169\n10.3  A Design Checklist for Testability\nTable 10.2 is a checklist to support the design and analysis process for testability.\nTable 10.2  Checklist to Support the Design and Analysis Process for Testability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which system responsibilities are most critical \nand hence need to be most thoroughly tested.\nEnsure that additional system responsibilities have been \nallocated to do the following:\n■\n■\nExecute test suite and capture results (external test or \nself-test)\n■\n■\nCapture (log) the activity that resulted in a fault or that \nresulted in unexpected (perhaps emergent) behavior \nthat was not necessarily a fault\n■\n■\nControl and observe relevant system state for testing\nMake sure the allocation of functionality provides high \ncohesion, low coupling, strong separation of concerns, and \nlow structural complexity.\nCoordination Model\nEnsure the system’s coordination and communication \nmechanisms:\n■\n■\nSupport the execution of a test suite and capture the \nresults within a system or between systems\n■\n■\nSupport capturing activity that resulted in a fault within \na system or between systems\n■\n■\nSupport injection and monitoring of state into the \ncommunication channels for use in testing, within a \nsystem or between systems\n■\n■\nDo not introduce needless nondeterminism\nData Model\nDetermine the major data abstractions that must be tested \nto ensure the correct operation of the system.\n■\n■\nEnsure that it is possible to capture the values of \ninstances of these data abstractions \n■\n■\nEnsure that the values of instances of these data \nabstractions can be set when state is injected into the \nsystem, so that system state leading to a fault may be \nre-created\n■\n■\nEnsure that the creation, initialization, persistence, \nmanipulation, translation, and destruction of instances \nof these data abstractions can be exercised and \ncaptured \nMapping among \nArchitectural Elements\nDetermine how to test the possible mappings of \narchitectural elements (especially mappings of processes \nto processors, threads to processes, and modules to \ncomponents) so that the desired test response is achieved \nand potential race conditions identified.\nIn addition, determine whether it is possible to test for \nillegal mappings of architectural elements.\ncontinues\n\n\n170 \nPart Two  Quality Attributes\t\n10—Testability\nTable 10.2  Checklist to Support the Design and Analysis Process for \nTestability, continued\nCategory\nChecklist\nResource Management\nEnsure there are sufficient resources available to execute \na test suite and capture the results. Ensure that your test \nenvironment is representative of (or better yet, identical to) \nthe environment in which the system will run. Ensure that \nthe system provides the means to do the following:\n■\n■\nTest resource limits \n■\n■\nCapture detailed resource usage for analysis in the \nevent of a failure\n■\n■\nInject new resource limits into the system for the \npurposes of testing\n■\n■\nProvide virtualized resources for testing\nBinding Time\nEnsure that components that are bound later than compile \ntime can be tested in the late-bound context. \nEnsure that late bindings can be captured in the event of a \nfailure, so that you can re-create the system’s state leading \nto the failure. \nEnsure that the full range of binding possibilities can be \ntested.\nChoice of Technology\nDetermine what technologies are available to help achieve \nthe testability scenarios that apply to your architecture. Are \ntechnologies available to help with regression testing, fault \ninjection, recording and playback, and so on?\nDetermine how testable the technologies are that you have \nchosen (or are considering choosing in the future) and \nensure that your chosen technologies support the level of \ntesting appropriate for your system. For example, if your \nchosen technologies do not make it possible to inject state, \nit may be difficult to re-create fault scenarios.\nNow That Your Architecture Is Set to Help You Test . . . \nBy Nick Rozanski, coauthor (with Eoin Woods) of Software Systems \nArchitecture: Working With Stakeholders Using Viewpoints and \nPerspectives\nIn addition to architecting your system to make it amenable to testing, \nyou will need to overcome two more specific and daunting challenges \nwhen testing very large or complex systems, namely test data and test \nautomation.\nTest Data\nYour first challenge is how to create large, consistent and useful test \ndata sets. This is a significant problem in my experience, particularly for \nintegration testing (that is, testing a number of components to confirm that \nthey work together correctly) and performance testing (confirming that \n\n\n10.3  A Design Checklist for Testability\n171\nthe system meets it requirements for throughput, latency, and response \ntime). For unit tests, and usually for user acceptance tests, the test data is \ntypically created by hand.\nFor example, you might need 50 products, 100 customers, and 500 \norders in your test database, so that you can test the functional steps \ninvolved in creating, amending, or deleting orders. This data has to be \nsufficiently varied to make testing worthwhile, it has to conform to all the \nreferential integrity rules and other constraints of your data model, and you \nneed to be able to calculate and specify the expected results of the tests.\nI’ve seen—and been involved in—two ways of doing this: you either \nwrite a system to generate your test data, or you capture a representative \ndata set from the production environment and anonymize it as necessary. \n(Anonymizing test data involves removing any sensitive information, such as \npersonal data about people or organizations, financial details, and so on.)\nCreating your own test data is the ideal, because you know what data \nyou are using and can ensure that it covers all of your edge cases, but it is \na lot of effort. Capturing data from the live environment is easier, assum-\ning that there is a system there already, but you don’t know what data and \nhence what coverage you’re going to get, and you may have to take extra \ncare to conform to privacy and data protection legislation.\nThis can have an impact on the system’s architecture in a number of \nways, and should be given due consideration early on by the architect. For \nexample, the system may need to be able to capture live transactions, or \ntake “snapshots” of live data, which can be used to generate test data. In ad-\ndition, the test-data-generation system may need an architecture of its own.\nTest Automation\nYour second challenge is around test automation. In practice it is not pos-\nsible to test large systems by hand because of the number of tests, their \ncomplexity, and the amount of checking of results that’s required. In the \nideal world, you create a test automation framework to do this automati-\ncally, which you feed with test data, and set running every night, or even \nrun every time you check in something (the continuous integration model).\nThis is an area that is given too little attention on many large software \ndevelopment projects. It is often not budgeted for in the project plan, with \nan unwritten assumption that the effort needed to build it can be somehow \n“absorbed” into the development costs. A test automation framework can \nbe a significantly complex thing in its own right (which raises the question \nof how you test it!). It should be scoped and planned like any other project \ndeliverable.\nDue consideration should be given to how the framework will invoke \nfunctions on the system under test, particularly for testing user interfaces, \nwhich is almost without exception a nightmare. (The execution of a UI test \nis highly dependent on the layout of the windows, the ordering of fields, \nand so on, which usually changes a lot in heavily user-focused systems. \nIt is sometimes possible to execute window controls programmatically, but \nin the worst case you may have to record and replay keystrokes or mouse \nmovements.)\n\n\n172 \nPart Two  Quality Attributes\t\n10—Testability\nThere are lots of tools to help with this nowadays, such as Quick Test \nPro, TestComplete, or Selenium for testing, and CruiseControl, Hudson, \nand TeamCity for continuous integration. A comprehensive list on the web \ncan be found here: en.wikipedia.org/wiki/Test_automation.\n10.4  Summary\nEnsuring that a system is easily testable has payoffs both in terms of the cost of \ntesting and the reliability of the system. A vehicle often used to execute the tests \nis the test harness. Test harnesses are software systems that encapsulate test re-\nsources such as test cases and test infrastructure so that it is easy to reapply tests \nacross iterations and it is easy to apply the test infrastructure to new increments \nof the system. Another vehicle is the creation of test cases prior to the develop-\nment of a component, so that developers know which tests their component must \npass.\nControlling and observing the system state is a major class of testability \ntactics. Providing the ability to do fault injection, to record system state at key \nportions of the system, to isolate the system from its environment, and to abstract \nvarious resources are all different tactics to support the control and observation of \na system and its components.\nComplex systems are difficult to test because of the large state space in \nwhich their computations take place, and because of the larger number of inter-\nconnections among the elements of the system. Consequently, keeping the sys-\ntem simple is another class of tactics that supports testability.\n10.5  For Further Reading\nAn excellent general introduction to software testing is [Beizer 90]. For a more \nmodern take on testing, and from the software developer’s perspective rather than \nthe tester’s, Freeman and Pryce cover test-driven development in the object-ori-\nented realm [Freeman 09].\nBertolino and Strigini [Bertolino 96] are the developers of the model of test-\ning shown in Figure 10.1. \nYin and Bieman [Yin 94] have written about executable assertions. Hartman \n[Hartman 10] describes a technique for using executable assertions as a means \nfor detecting race conditions.\nBruntink and van Deursen [Bruntink 06] write about the impact of structure \non testing. \n\n\n10.6  Discussion Questions\n173\nJeff Voas’s foundational work on testability and the relationship between \ntestability and reliability is worthwhile. There are several papers to choose from, \nbut [Voas 95] is a good start that will point you to others.\n10.6  Discussion Questions\n1.\t\nA testable system is one that gives up its faults easily. That is, if a system \ncontains a fault, then it doesn’t take long or much effort to make that fault \nshow up. On the other hand, fault tolerance is all about designing systems \nthat jealously hide their faults; there, the whole idea is to make it very diffi-\ncult for a system to reveal its faults. Is it possible to design a system that is \nboth highly testable and highly fault tolerant, or are these two design goals \ninherently incompatible? Discuss.\n2.\t\n“Once my system is in routine use by end users, it should not be highly \ntestable, because if it still contains faults—and all systems probably do—\nthen I don’t want them to be easily revealed.” Discuss.\n3.\t\nMany of the tactics for testability are also useful for achieving modifiabili-\nty. Why do you think that is?\n4.\t\nWrite some concrete testability scenarios for an automatic teller machine. \nHow would you modify your design for the automatic teller machine to ac-\ncommodate these scenarios?\n5.\t\nWhat other quality attributes do you think testability is most in conflict \nwith? What other quality attributes do you think testability is most compati-\nble with?\n6.\t\nOne of our tactics is to limit nondeterminism. One method is to use locking \nto enforce synchronization. What impact does the use of locks have on oth-\ner quality attributes?\n7.\t\nSuppose you’re building the next great social networking system. You antic-\nipate that within a month of your debut, you will have half a million users. \nYou can’t pay half a million people to test your system, and yet it has to be \nrobust and easy to use when all half a million are banging away at it. What \nshould you do? What tactics will help you? Write a testability scenario for \nthis social networking system.\n8.\t\nSuppose you use executable assertions to improve testability. Make a case \nfor, and then a case against, allowing the assertions to run in the production \nsystem as opposed to removing them after testing.\n\n\nThis page intentionally left blank \n\n\n175\n11\nUsability\nAny darn fool can make something complex; it \ntakes a genius to make something simple.\n—Albert Einstein\nUsability is concerned with how easy it is for the user to accomplish a desired \ntask and the kind of user support the system provides. Over the years, a focus on \nusability has shown itself to be one of the cheapest and easiest ways to improve a \nsystem’s quality (or more precisely, the user’s perception of quality). \nUsability comprises the following areas:\n■\n■Learning system features. If the user is unfamiliar with a particular system \nor a particular aspect of it, what can the system do to make the task of \nlearning easier? This might include providing help features.\n■\n■Using a system efficiently. What can the system do to make the user more \nefficient in its operation? This might include the ability for the user to redi-\nrect the system after issuing a command. For example, the user may wish to \nsuspend one task, perform several operations, and then resume that task.\n■\n■Minimizing the impact of errors. What can the system do so that a user \nerror has minimal impact? For example, the user may wish to cancel a com-\nmand issued incorrectly.\n■\n■Adapting the system to user needs. How can the user (or the system itself) \nadapt to make the user’s task easier? For example, the system may automat-\nically fill in URLs based on a user’s past entries.\n■\n■Increasing confidence and satisfaction. What does the system do to give the \nuser confidence that the correct action is being taken? For example, pro-\nviding feedback that indicates that the system is performing a long-running \ntask and the extent to which the task is completed will increase the user’s \nconfidence in the system.\n",
      "page_number": 188
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 196-203)",
      "start_page": 196,
      "end_page": 203,
      "detection_method": "topic_boundary",
      "content": "176 \nPart Two  Quality Attributes\t\n11—Usability\n11.1  Usability General Scenario\nThe portions of the usability general scenarios are these:\n■\n■Source of stimulus. The end user (who may be in a specialized role, such as \na system or network administrator) is always the source of the stimulus for \nusability.\n■\n■Stimulus. The stimulus is that the end user wishes to use a system efficient-\nly, learn to use the system, minimize the impact of errors, adapt the system, \nor configure the system. \n■\n■Environment. The user actions with which usability is concerned always \noccur at runtime or at system configuration time. \n■\n■Artifact. The artifact is the system or the specific portion of the system with \nwhich the user is interacting.\n■\n■Response. The system should either provide the user with the features need-\ned or anticipate the user’s needs. \n■\n■Response measure. The response is measured by task time, number of \nerrors, number of tasks accomplished, user satisfaction, gain of user \nknowledge, ratio of successful operations to total operations, or amount of \ntime or data lost when an error occurs. \nTable 11.1 enumerates the elements of the general scenario that characterize \nusability.\nFigure 11.1 gives an example of a concrete usability scenario that you could \ngenerate using Table 11.1: The user downloads a new application and is using it \nproductively after two minutes of experimentation.\nTable 11.1  Usability General Scenario\nPortion of Scenario\nPossible Values\nSource \nEnd user, possibly in a specialized role\nStimulus\nEnd user tries to use a system efficiently, learn to use the \nsystem, minimize the impact of errors, adapt the system, or \nconfigure the system.\nEnvironment\nRuntime or configuration time \nArtifacts\nSystem or the specific portion of the system with which the \nuser is interacting\nResponse\nThe system should either provide the user with the features \nneeded or anticipate the user’s needs.\nResponse Measure \nOne or more of the following: task time, number of errors, \nnumber of tasks accomplished, user satisfaction, gain of user \nknowledge, ratio of successful operations to total operations, \nor amount of time or data lost when an error occurs \n\n\n11.2  Tactics for Usability\n177\n11.2  Tactics for Usability\nRecall that usability is concerned with how easy it is for the user to accomplish \na desired task, as well as the kind of support the system provides to the user. \nResearchers in human-computer interaction have used the terms user initiative, \nsystem initiative, and mixed initiative to describe which of the human-computer \npair takes the initiative in performing certain actions and how the interaction pro-\nceeds. Usability scenarios can combine initiatives from both perspectives. For \nexample, when canceling a command, the user issues a cancel—user initiative—\nand the system responds. During the cancel, however, the system may put up a \nprogress indicator—system initiative. Thus, cancel may demonstrate mixed ini-\ntiative. We use this distinction between user and system initiative to discuss the \ntactics that the architect uses to achieve the various scenarios.\nFigure 11.2 shows the goal of the set of runtime usability tactics.\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nRuntime\n3\n2\n1\n4\nUser\nDownloads \na New \nApplication\nUser Uses\nApplication\nProductively\nWithin Two\nMinutes of\nExperimentation\nArtifact:\nSystem\nFigure 11.1  Sample concrete usability scenario \nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nTactics\nto Control\nUsability\nFigure 11.2  The goal of runtime usability tactics\n\n\n178 \nPart Two  Quality Attributes\t\n11—Usability\nSeparate the User Interface!\nOne of the most helpful things an architect can do to make a system \nusable is to facilitate experimentation with the user interface via the con-\nstruction of rapid prototypes. Building a prototype, or several prototypes, \nto let real users experience the interface and give their feedback pays \nenormous dividends. The best way to do this is to design the software so \nthat the user interface can be quickly changed.\nTactics for modifiability that we saw in Chapter 7 support this goal per-\nfectly well, especially these:\n■\n■\nIncrease semantic coherence, encapsulate, and co-locate related re-\nsponsibilities, which localize user interface responsibilities to a single \nplace\n■\n■\nRestrict dependencies, which minimizes the ripple effect to other soft-\nware when the user interface changes\n■\n■\nDefer binding, which lets you make critical user interface choices without \nhaving to recode\nDefer binding is especially helpful here, because you can expect that \nyour product’s user interface will face pressure to change during testing \nand even after it goes to market.\nUser interface generation tools are consistent with these tactics; most \nproduce a single module with an abstract interface to the rest of the soft-\nware. Many provide the capability to change the user interface after compile \ntime. You can do your part by restricting dependencies on the generated \nmodule, should you later decide to adopt a different tool.\nMuch work in different user interface separation patterns occurred in the \n1980s and 90s. With the advent of the web and the modernization of the \nmodel-view-controller (MVC) pattern to reflect web interfaces, MVC has \nbecome the dominant separation pattern. Now the MVC pattern is built into \na wide variety of different frameworks. (See Chapter 14 for a discussion of \nMVC.) MVC makes it easy to provide multiple views of the data, supporting \nuser initiative, as we discuss next.\nMany times quality attributes are in conflict with each other. Usability \nand modifiability, on the other hand, often complement each other, \nbecause one of the best ways to make a system more usable is to make \nit modifiable. However, this is not always the case. In many systems busi-\nness rules drive the UI—for example, specifying how to validate input. To \nrealize this validation, the UI may need to call a server (which can neg-\natively affect performance). To get around this performance penalty, the \narchitect may choose to duplicate these rules in the client and the server, \nwhich then makes evolution difficult. Alas, the architect’s life is never easy!\n\n\n11.2  Tactics for Usability\n179\nThere is a connection between the achievement of usability and modifiabil-\nity. The user interface design process consists of generating and then testing a \nuser interface design. Deficiencies in the design are corrected and the process \nrepeats. If the user interface has already been constructed as a portion of the sys-\ntem, then the system must be modified to reflect the latest design. Hence the con-\nnection with modifiability. This connection has resulted in standard patterns to \nsupport user interface design (see sidebar).\nSupport User Initiative\nOnce a system is executing, usability is enhanced by giving the user feed-\nback as to what the system is doing and by allowing the user to make appro-\npriate responses. For example, the tactics described next—cancel, undo, pause/ \nresume, and aggregate—support the user in either correcting errors or being more \nefficient.\nThe architect designs a response for user initiative by enumerating and al-\nlocating the responsibilities of the system to respond to the user command. Here \nare some common examples of user initiative: \n■\n■Cancel. When the user issues a cancel command, the system must be \nlistening for it (thus, there is the responsibility to have a constant listener \nthat is not blocked by the actions of whatever is being canceled); the \ncommand being canceled must be terminated; any resources being \nused by the canceled command must be freed; and components that are \ncollaborating with the canceled command must be informed so that they \ncan also take appropriate action.\n■\n■Undo. To support the ability to undo, the system must maintain a sufficient \namount of information about system state so that an earlier state may be \nrestored, at the user’s request. Such a record may be in the form of state \n“snapshots”—for example, checkpoints—or as a set of reversible oper-\nations. Not all operations can be easily reversed: for example, changing \nall occurrences of the letter “a” to the letter “b” in a document cannot be \nreversed by changing all instances of “b” to “a”, because some of those in-\nstances of “b” may have existed prior to the original change. In such a case \nthe system must maintain a more elaborate record of the change. Of course, \nsome operations, such as ringing a bell, cannot be undone. \n■\n■Pause/resume. When a user has initiated a long-running operation—say, \ndownloading a large file or set of files from a server—it is often useful to \nprovide the ability to pause and resume the operation. Effectively pausing a \nlong-running operation requires the ability to temporarily free resources so \nthat they may be reallocated to other tasks. \n\n\n180 \nPart Two  Quality Attributes\t\n11—Usability\n■\n■Aggregate. When a user is performing repetitive operations, or operations \nthat affect a large number of objects in the same way, it is useful to provide \nthe ability to aggregate the lower-level objects into a single group, so that \nthe operation may be applied to the group, thus freeing the user from the \ndrudgery (and potential for mistakes) of doing the same operation repeated-\nly. For example, aggregate all of the objects in a slide and change the text to \n14-point font.\nSupport System Initiative\nWhen the system takes the initiative, it must rely on a model of the user, the \ntask being undertaken by the user, or the system state itself. Each model requires \nvarious types of input to accomplish its initiative. The support system initiative \ntactics are those that identify the models the system uses to predict either its \nown behavior or the user’s intention. Encapsulating this information will make \nit easier for it to be tailored or modified. Tailoring and modification can be either \ndynamically based on past user behavior or offline during development. These \ntactics are the following:\n■\n■Maintain task model. The task model is used to determine context so the \nsystem can have some idea of what the user is attempting and provide \nassistance. For example, knowing that sentences start with capital letters \nwould allow an application to correct a lowercase letter in that position.\n■\n■Maintain user model. This model explicitly represents the user’s knowledge \nof the system, the user’s behavior in terms of expected response time, and \nother aspects specific to a user or a class of users. For example, maintaining \na user model allows the system to pace mouse selection so that not all of \nthe document is selected when scrolling is required. Or a model can control \nthe amount of assistance and suggestions automatically provided to a user. \nA special case of this tactic is commonly found in user interface customiza-\ntion, wherein a user can explicitly modify the system’s user model.\n■\n■Maintain system model. Here the system maintains an explicit model \nof itself. This is used to determine expected system behavior so that \nappropriate feedback can be given to the user. A common manifestation of \na system model is a progress bar that predicts the time needed to complete \nthe current activity.\nFigure 11.3 shows a summary of the tactics to achieve usability.\n\n\n11.3  A Design Checklist for Usability\n181\nUsability Tactics\nSupport User\nInitiative\nSupport System\nInitiative\nCancel\nMaintain User\nModel\nMaintain System\nModel\nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nUndo\nPause/Resume\nAggregate\nMaintain Task\nModel\nFigure 11.3  Usability tactics\n11.3  A Design Checklist for Usability\nTable 11.2 is a checklist to support the design and analysis process for usability.\nTable 11.2  Checklist to Support the Design and Analysis Process for Usability\nCategory\nChecklist\nAllocation of \nResponsibilities\nEnsure that additional system responsibilities have been \nallocated, as needed, to assist the user in the following:\n■\n■\nLearning how to use the system\n■\n■\nEfficiently achieving the task at hand \n■\n■\nAdapting and configuring the system\n■\n■\nRecovering from user and system errors\nCoordination Model\nDetermine whether the properties of system elements’ \ncoordination—timeliness, currency, completeness, \ncorrectness, consistency—affect how a user learns to use \nthe system, achieves goals or completes tasks, adapts \nand configures the system, recovers from user and system \nerrors, and gains increased confidence and satisfaction. \nFor example, can the system respond to mouse events \nand give semantic feedback in real time? Can long-running \nevents be canceled in a reasonable amount of time?\ncontinues\n\n\n182 \nPart Two  Quality Attributes\t\n11—Usability\nTable 11.2  Checklist to Support the Design and Analysis Process for Usability, \ncontinued\nCategory\nChecklist\nData Model\nDetermine the major data abstractions that are involved \nwith user-perceivable behavior. Ensure these major data \nabstractions, their operations, and their properties have \nbeen designed to assist the user in achieving the task at \nhand, adapting and configuring the system, recovering from \nuser and system errors, learning how to use the system, and \nincreasing satisfaction and user confidence.\nFor example, the data abstractions should be designed \nto support undo and cancel operations: the transaction \ngranularity should not be so great that canceling or undoing \nan operation takes an excessively long time.\nMapping among \nArchitectural  \nElements\nDetermine what mapping among architectural elements is \nvisible to the end user (for example, the extent to which the \nend user is aware of which services are local and which \nare remote). For those that are visible, determine how this \naffects the ways in which, or the ease with which, the user \nwill learn how to use the system, achieve the task at hand, \nadapt and configure the system, recover from user and \nsystem errors, and increase confidence and satisfaction.\nResource  \nManagement\nDetermine how the user can adapt and configure the \nsystem’s use of resources. Ensure that resource limitations \nunder all user-controlled configurations will not make users \nless likely to achieve their tasks. For example, attempt to \navoid configurations that would result in excessively long \nresponse times. Ensure that the level of resources will not \naffect the users’ ability to learn how to use the system, or \ndecrease their level of confidence and satisfaction with the \nsystem.\nBinding Time\nDetermine which binding time decisions should be under \nuser control and ensure that users can make decisions \nthat aid in usability. For example, if the user can choose, at \nruntime, the system’s configuration, or its communication \nprotocols, or its functionality via plug-ins, you need to ensure \nthat such choices do not adversely affect the user’s ability to \nlearn system features, use the system efficiently, minimize \nthe impact of errors, further adapt and configure the system, \nor increase confidence and satisfaction.\nChoice of Technology\nEnsure the chosen technologies help to achieve the usability \nscenarios that apply to your system. For example, do these \ntechnologies aid in the creation of online help, the production \nof training materials, and the collection of user feedback? \nHow usable are any of your chosen technologies? Ensure \nthe chosen technologies do not adversely affect the usability \nof the system (in terms of learning system features, using the \nsystem efficiently, minimizing the impact of errors, adapting/\nconfiguring the system, and increasing confidence and \nsatisfaction).\n\n\n11.6  Discussion Questions\n183\n11.4  Summary\nArchitectural support for usability involves both allowing the user to take the ini-\ntiative—in circumstances such as canceling a long-running command or undoing \na completed command—and aggregating data and commands. \nTo be able to predict user or system responses, the system must keep an ex-\nplicit model of the user, the system, and the task.\nThere is a strong relationship between supporting the user interface design \nprocess and supporting modifiability; this relation is promoted by patterns that \nenforce separation of the user interface from the rest of the system, such as the \nMVC pattern.\n11.5  For Further Reading\nClaire Marie Karat has investigated the relation between usability and business \nadvantage [Karat 94].\nJakob Nielsen has also written extensively on this topic, including a calcula-\ntion on the ROI of usability [Nielsen 08].\nBonnie John and Len Bass have investigated the relation between usabil-\nity and software architecture. They have enumerated around two dozen usability \nscenarios that have architectural impact and given associated patterns for these \nscenarios [Bass 03].\nGreg Hartman has defined attentiveness as the ability of the system to sup-\nport user initiative and allow cancel or pause/resume [Hartman 10].\nSome of the patterns for separating the user interface are Arch/Slinky, See-\nheim, and PAC. These are discussed in Chapter 8 of Human-Computer Interac-\ntion [Dix 04].\n11.6  Discussion Questions\n1.\t\nWrite a concrete usability scenario for your automobile that specifies how \nlong it takes you to set your favorite radio stations? Now consider another \npart of the driver experience and create scenarios that test other aspects of \nthe response measures from the general scenario table.\n2.\t\nWrite a concrete usability scenario for an automatic teller machine. How \nwould your design be modified to satisfy these scenarios?\n",
      "page_number": 196
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 204-211)",
      "start_page": 204,
      "end_page": 211,
      "detection_method": "topic_boundary",
      "content": "184 \nPart Two  Quality Attributes\t\n11—Usability\n3.\t\nHow might usability trade off against security? How might it trade off \nagainst performance? \n4.\t\nPick a few of your favorite web sites that do similar things, such as social \nnetworking or online shopping. Now pick one or two appropriate responses \nfrom the usability general scenario (such as “achieve the task at hand”) and \na correspondingly appropriate response measure. Using the response and \nresponse measure you chose, compare the web sites’ usability.\n5.\t\nSpecify the data model for a four-function calculator that allows undo.\n6.\t\nWhy is it that in so many systems, the cancel button in a dialog box appears \nto be unresponsive? What architectural principles do you think were ig-\nnored in these systems?\n7.\t\nWhy do you think that progress bars frequently behave erratically, moving \nfrom 10 to 90 percent in one step and then getting stuck on 90 percent?\n8.\t\nResearch the crash of Air France Flight 296 into the forest at Habsheim, \nFrance, on June 26, 1988. The pilots said they were unable to read the dig-\nital display of the radio altimeter or hear its audible readout. If they could \nhave, do you believe the crash would have been averted? In this context, \ndiscuss the relationship between usability and safety.\n\n\n185\n12\nOther Quality Attributes\nQuality is not an act, it is a habit.\n—Aristotle\nChapters 5–11 each dealt with a particular quality attribute important to software \nsystems. Each of those chapters discussed how its particular quality attribute is \ndefined, gave a general scenario for that quality attribute, and showed how to \nwrite specific scenarios to express precise shades of meaning concerning that \nquality attribute. And each gave a collection of techniques to achieve that quality \nattribute in an architecture. In short, each chapter presented a kind of portfolio for \nspecifying and designing to achieve a particular quality attribute.\nThose seven chapters covered seven of the most important quality attributes, in \nterms of their occurrence in modern software-reliant systems. However, as is no \ndoubt clear, seven only begins to scratch the surface of the quality attributes that \nyou might find needed in a software system you’re working on. \nIs cost a quality attribute? It is not a technical quality attribute, but it certainly \naffects fitness for use. We consider economic factors in Chapter 23. \nThis chapter will give a brief introduction to a few other quality attributes—a \nsort of “B list” of quality attributes—but, more important, show how to build the \nsame kind of specification or design portfolio for a quality attribute not covered \nin our list.\n12.1  Other Important Quality Attributes\nBesides the quality attributes we’ve covered in depth in Chapters 5–11, some oth-\ners that arise frequently are variability, portability, development distributability, \nscalability and elasticity, deployability, mobility, and monitorability. We discuss \n“green” computing in Section 12.3.\n\n\n186 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nVariability\nVariability is a special form of modifiability. It refers to the ability of a system \nand its supporting artifacts such as requirements, test plans, and configuration \nspecifications to support the production of a set of variants that differ from each \nother in a preplanned fashion. Variability is an especially important quality at-\ntribute in a software product line (this will be explored in depth in Chapter 25), \nwhere it means the ability of a core asset to adapt to usages in the different prod-\nuct contexts that are within the product line scope. The goal of variability in a \nsoftware product line is to make it easy to build and maintain products in the \nproduct line over a period of time. Scenarios for variability will deal with the \nbinding time of the variation and the people time to achieve it.\nPortability\nPortability is also a special form of modifiability. Portability refers to the ease \nwith which software that was built to run on one platform can be changed to \nrun on a different platform. Portability is achieved by minimizing platform de-\npendencies in the software, isolating dependencies to well-identified locations, \nand writing the software to run on a “virtual machine” (such as a Java Virtual \nMachine) that encapsulates all the platform dependencies within. Scenarios de-\nscribing portability deal with moving software to a new platform by expending \nno more than a certain level of effort or by counting the number of places in the \nsoftware that would have to change.\nDevelopment Distributability\nDevelopment distributability is the quality of designing the software to support \ndistributed software development. Many systems these days are developed using \nglobally distributed teams. One problem that must be overcome when develop-\ning with distributed teams is coordinating their activities. The system should be \ndesigned so that coordination among teams is minimized. This minimal coor-\ndination needs to be achieved both for the code and for the data model. Teams \nworking on modules that communicate with each other may need to negotiate \nthe interfaces of those modules. When a module is used by many other mod-\nules, each developed by a different team, communication and negotiation become \nmore complex and burdensome. Similar considerations apply for the data model. \nScenarios for development distributability will deal with the compatibility of the \ncommunication structures and data model of the system being developed and the \ncoordination mechanisms of the organizations doing the development.\n\n\n12.1  Other Important Quality Attributes\n187\nScalability \nTwo kinds of scalability are horizontal scalability and vertical scalability. Hori-\nzontal scalability (scaling out) refers to adding more resources to logical units, \nsuch as adding another server to a cluster of servers. Vertical scalability (scaling \nup) refers to adding more resources to a physical unit, such as adding more mem-\nory to a single computer. The problem that arises with either type of scaling is \nhow to effectively utilize the additional resources. Being effective means that the \nadditional resources result in a measurable improvement of some system quality, \ndid not require undue effort to add, and did not disrupt operations. In cloud en-\nvironments, horizontal scalability is called elasticity. Elasticity is a property that \nenables a customer to add or remove virtual machines from the resource pool (see \nChapter 26 for further discussion of such environments). These virtual machines \nare hosted on a large collection of upwards of 10,000 physical machines that are \nmanaged by the cloud provider. Scalability scenarios will deal with the impact of \nadding or removing resources, and the measures will reflect associated availabil-\nity and the load assigned to existing and new resources. \nDeployability\nDeployability is concerned with how an executable arrives at a host platform and \nhow it is subsequently invoked. Some of the issues involved in deploying soft-\nware are: How does it arrive at its host (push, where updates are sent to users un-\nbidden, or pull, where users must explicitly request updates)? How is it integrated \ninto an existing system? Can this be done while the existing system is executing? \nMobile systems have their own problems in terms of how they are updated, be-\ncause of concerns about bandwidth. Deployment scenarios will deal with the type \nof update (push or pull), the form of the update (medium, such as DVD or Inter-\nnet download, and packaging, such as executable, app, or plug-in), the resulting \nintegration into an existing system, the efficiency of executing the process, and \nthe associated risk.\nMobility\nMobility deals with the problems of movement and affordances of a platform \n(e.g., size, type of display, type of input devices, availability and volume of \nbandwidth, and battery life). Issues in mobility include battery management, \nreconnecting after a period of disconnection, and the number of different user \ninterfaces necessary to support multiple platforms. Scenarios will deal with spec-\nifying the desired effects of mobility or the various affordances. Scenarios may \nalso deal with variability, where the same software is deployed on multiple (per-\nhaps radically different) platforms.\n\n\n188 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nMonitorability\nMonitorability deals with the ability of the operations staff to monitor the system \nwhile it is executing. Items such as queue lengths, average transaction processing \ntime, and the health of various components should be visible to the operations \nstaff so that they can take corrective action in case of potential problems. Sce-\nnarios will deal with a potential problem and its visibility to the operator, and \npotential corrective action. \nSafety\nIn 2009 an employee of the Shushenskaya hydroelectric power station in Siberia \nsent commands over a network to remotely, and accidentally, activate an unused \nturbine. The offline turbine created a “water hammer” that flooded and then de-\nstroyed the plant and killed dozens of workers.\nThe thought that software could kill people used to belong in the realm of \nkitschy computers-run-amok science fiction. Sadly, it didn’t stay there. As soft-\nware has come to control more and more of the devices in our lives, software \nsafety has become a critical concern.\nSafety is not purely a software concern, but a concern for any system that \ncan affect its environment. As such it receives mention in Section 12.3, where we \ndiscuss system quality attributes. But there are means to address safety that are \nwholly in the software realm, which is why we discuss it here as well. \nSoftware safety is about the software’s ability to avoid entering states that \ncause or lead to damage, injury, or loss of life to actors in the software’s envi-\nronment, and to recover and limit the damage when it does enter into bad states. \nAnother way to put this is that safety is concerned with the prevention of and \nrecovery from hazardous failures. Because of this, the architectural concerns with \nsafety are almost identical to those for availability, which is also about avoiding \nand recovering from failures. Tactics for safety, then, overlap with those for avail-\nability to a large degree. Both comprise tactics to prevent failures and to detect \nand recover from failures that do occur.\nSafety is not the same as reliability. A system can be reliable (consistent \nwith its specification) but still unsafe (for example, when the specification ig-\nnores conditions leading to unsafe action). In fact, paying careful attention to the \nspecification for safety-critical software is perhaps the most powerful thing you \ncan do to produce safe software. Failures and hazards cannot be detected, pre-\nvented, or ameliorated if the software has not been designed with them in mind. \nSafety is frequently engineered by performing failure mode and effects analy-\nsis, hazard analysis, and fault tree analysis. (These techniques are discussed in \nChapter 5.) These techniques are intended to discover possible hazards that could \nresult from the system’s operation and provide plans to cope with these hazards.\n\n\n12.2  Other Categories of Quality Attributes\n189\n12.2  Other Categories of Quality Attributes\nWe have primarily focused on product qualities in our discussions of quality at-\ntributes, but there are other types of quality attributes that measure “goodness” of \nsomething other than the final product. Here are three: \nConceptual Integrity of the Architecture\nConceptual integrity refers to consistency in the design of the architecture, and it \ncontributes to the understandability of the architecture and leads to fewer errors \nof confusion. Conceptual integrity demands that the same thing is done in the \nsame way through the architecture. In an architecture with conceptual integrity, \nless is more. For example, there are countless ways that components can send \ninformation to each other: messages, data structures, signaling of events, and so \nforth. An architecture with conceptual integrity would feature one way only, and \nonly provide alternatives if there was a compelling reason to do so. Similarly, \ncomponents should all report and handle errors in the same way, log events or \ntransactions in the same way, interact with the user in the same way, and so forth.\nQuality in Use\nISO/IEC 25010, which we discuss in Section 12.4, has a category of qualities that \npertain to the use of the system by various stakeholders. For example, time-to-\nmarket is an important characteristic of a system, but it is not discernible from an \nexamination of the product itself. Some of the qualities in this category are these:\n■\n■Effectiveness. This refers to the distinction between building the system \ncorrectly (the system performs according to its requirements) and building \nthe correct system (the system performs in the manner the user wishes). \nEffectiveness is a measure of whether the system is correct.\n■\n■Efficiency. The effort and time required to develop a system. Put another \nway, what is the architecture’s impact on the project’s cost and schedule? \nWould a different set of architectural choices have resulted in a system \nthat would be faster or cheaper to bring to fruition? Efficiency can include \ntraining time for developers; an architecture that uses technology unfamiliar \nto the staff on hand is less buildable. Is the architecture appropriate for the \norganization in terms of its experience and its available supporting infra-\nstructure (such as test facilities or development environments)?\n■\n■Freedom from risk. The degree to which a product or system affects \neconomic status, human life, health, or the environment.\n\n\n190 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nA special case of efficiency is how easy it is to build (that is, compile and \nassemble) the system after a change. This becomes critical during testing. A \nrecompile process that takes hours or overnight is a schedule-killer. Architects \nhave control over this by managing dependencies among modules. If the archi-\ntect doesn’t do this, then what often happens is that some bright-eyed developer \nwrites a makefile early on, it works, and people add to it and add to it. Eventually \nthe project ends up with a seven-hour compile step and very unhappy integrators \nand testers who are already behind schedule (because they always are).\nMarketability\nAn architecture’s marketability is another quality attribute of concern. Some sys-\ntems are well known by their architectures, and these architectures sometimes \ncarry a meaning all their own, independent of what other quality attributes they \nbring to the system. The current craze in building cloud-based systems has taught \nus that the perception of an architecture can be more important than the qualities \nthe architecture brings. Many organizations have felt they had to build cloud-\nbased systems (or some other technology du jour) whether or not that was the \ncorrect technical choice. \n12.3  \u0007Software Quality Attributes and \nSystem Quality Attributes\nPhysical systems, such as aircraft or automobiles or kitchen appliances, that rely \non software embedded within are designed to meet a whole other litany of qual-\nity attributes: weight, size, electric consumption, power output, pollution output, \nweather resistance, battery life, and on and on. For many of these systems, safety \ntops the list (see the sidebar).\nSometimes the software architecture can have a surprising effect on the sys-\ntem’s quality attributes. For example, software that makes inefficient use of com-\nputing resources might require additional memory, a faster processor, a bigger \nbattery, or even an additional processor. Additional processors can add to a sys-\ntem’s power consumption, weight, required cabinet space, and of course expense.\nGreen computing is an issue of growing concern. Recently there was a con-\ntroversy about how much greenhouse gas was pumped into the atmosphere by \nGoogle’s massive processor farms. Given the daily output and the number of \ndaily requests, it is possible to estimate how much greenhouse gas you cause to be \nemitted each time you ask Google to perform a search. (Current estimates range \nfrom 0.2 grams to 7 grams of CO2.) Green computing is all the rage. Eve Troeh, \non the American Public Media show “Marketplace” (July 5, 2011), reports:\n\n\n12.3  Software Quality Attributes and System Quality Attributes\n191\nTwo percent of all U.S. electricity now goes to data centers, according \nto the Environmental Protection Agency. Electricity has become the \nbiggest cost for processing data—more than the equipment to do it, \nmore than the buildings to house that equipment. . . . Google’s making \ndata servers that can float offshore, cooled by ocean breezes. HP has \nplans to put data servers near farms, and power them with methane gas \nfrom cow pies.\nThe lesson here is that if you are the architect for software that resides in a \nlarger system, you will need to understand the quality attributes that are import-\nant for the containing system to achieve, and work with the system architects and \nengineers to see how your software architecture can contribute to achieving them.\nThe Vanishing Line between Software and System Qualities\nThis is a book about software architecture, and so we treat quality attri-\nbutes from a software architect’s perspective. But you may have already \nnoticed that the quality attributes that the software architect can bring to \nthe party are limited by the architecture of the system in which the soft-\nware runs. \nFor example: \n■\n■\nThe performance of a piece of software is fundamentally constrained \nby the performance of the computer that runs it. No matter how well you \ndesign the software, you just can’t run the latest whole-earth weather \nforecasting models on Grampa’s Commodore 64 and hope to know if it’s \ngoing to rain tomorrow.\n■\n■\nPhysical security is probably more important and more effective than \nsoftware security at preventing fraud and theft. If you don’t believe this, \nwrite your laptop’s password on a slip of paper, tape it to your laptop, \nand leave it in an unlocked car with the windows down. (Actually, don’t \nreally do that. Consider this a thought experiment.) \n■\n■\nIf we’re being perfectly honest here, how usable is a device for web \nbrowsing that has a screen smaller than a credit card and keys the size \nof a raisin?\nFor me, nowhere is the barrier between software and system more \nnebulous than in the area of safety. The thought that software—strings \nof 0’s and 1’s—can kill or maim or destroy is still an unnatural notion. Of \ncourse, it’s not the 0’s and 1’s that wreak havoc. At least, not directly. It’s \nwhat they’re connected to. Software, and the system in which it runs, has \nto be connected to the outside world in some way before it can do damage. \nThat’s the good news. The bad news is that the good news isn’t all that \ngood. Software is connected to the outside world, always. If your program \n",
      "page_number": 204
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 212-220)",
      "start_page": 212,
      "end_page": 220,
      "detection_method": "topic_boundary",
      "content": "192 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nhas no effect whatsoever that is observable outside of itself, it probably \nserves no purpose.\nThere are notorious examples of software-related failures. The Siberian \nhydroelectric plant catastrophe mentioned in the text, the Therac-25 fatal \nradiation overdose, the Ariane 5 explosion, and a hundred lesser known \naccidents all caused harm because the software was part of a system that \nincluded a turbine, an X-ray emitter, or a rocket’s steering controls, in the \nexamples just cited. In these cases, flawed software commanded some \nhardware in the system to take a disastrous action, and the hardware sim-\nply obeyed. Actuators are devices that connect hardware to software; they \nare the bridge between the world of 0’s and 1’s and the world of motion and \ncontrol. Send a digital value to an actuator (or write a bit string in the hard-\nware register corresponding to the actuator) and that value is translated to \nsome mechanical action, for better or worse. \nBut connection to an actuator is not required for software-related disas-\nters. Sometimes all the computer has to do is send erroneous information \nto its human operators. In September 1983, a Soviet satellite sent data \nto its ground system computer, which interpreted that data as a missile \nlaunched from the United States aimed at Moscow. Seconds later, the \ncomputer reported a second missile in flight. Soon, a third, then a fourth, \nand then a fifth appeared. Soviet Strategic Rocket Forces lieutenant colonel \nStanislav Yevgrafovich Petrov made the astonishing decision to ignore the \nwarning system, believing it to be in error. He thought it extremely unlikely \nthat the U.S. would have fired just a few missiles, thereby inviting total \nretaliatory destruction. He decided to wait it out, to see if the missiles were \nreal—that is, to see if his country’s capital city was going to be incinerated. \nAs we know, it wasn’t. The Soviet system had mistaken a rare sunlight con-\ndition for missiles in flight. Similar mistakes have occurred on the U.S. side.\nOf course, the humans don’t always get it right. On the dark and stormy \nnight of June 1, 2009, Air France flight 447 from Rio de Janeiro to Paris \nplummeted into the Atlantic Ocean, killing all on board. The Airbus A-330’s \nflight recorders were not recovered until May 2011, and as this book goes \nto publication it appears that the pilots never knew that the aircraft had en-\ntered a high-altitude stall. The sensors that measure airspeed had become \nclogged with ice and therefore unreliable. The software was required to dis-\nengage the autopilot in this situation, which it did. The human pilots thought \nthe aircraft was going too fast (and in danger of structural failure) when in \nfact it was going too slow (and falling). During the entire three-minute-plus \nplunge from 38,000 feet, the pilots kept trying to pull the nose up and throt-\ntles back to lower the speed. It’s a good bet that adding to the confusion \nwas the way the A-330’s stall warning system worked. When the system \ndetects a stall, it emits a loud audible alarm. The computers deactivate the \nstall warning when they “think” that the angle of attack measurements are \ninvalid. This can occur when the airspeed readings are very low. That is ex-\nactly what happened with Air France 447: Its forward speed dropped below \n60 knots, and the angle of attack was extremely high. As a consequence \nof a rule in the flight control software, the stall warning stopped and started \n\n\n12.4  Using Standard Lists of Quality Attributes—or Not \n193\nseveral times. Worse, it came on whenever the pilot let the nose fall a bit \n(increasing the airspeed and taking the readings into the “valid” range, but \nstill in stall) and then stopped when he pulled back. That is, doing the right \nthing resulted in the wrong feedback and vice versa. \nWas this an unsafe system, or a safe system unsafely operated? \nUltimately the courts will decide. \nSoftware that can physically harm us is a fact of our modern life. \nSometimes the link between software and physical harm is direct, as in \nthe Ariane example, and sometimes it’s much more tenuous, as in the Air \nFrance 447 example. But as software professionals, we cannot take refuge \nin the fact that our software can’t actually inflict harm any more than the \nperson who shouts “Fire!” in a crowded theater can claim it was the stam-\npede, not the shout, that caused injury.\n—PCC\n12.4  Using Standard Lists of Quality Attributes—or Not \nArchitects have no shortage of lists of quality attributes for software systems at \ntheir disposal. The standard with the pause-and-take-a-breath title of “ISO/IEC \nFCD 25010: Systems and software engineering—Systems and software product \nQuality Requirements and Evaluation (SQuaRE)—System and software quality \nmodels,” is a good example. The standard divides quality attributes into those \nsupporting a “quality in use” model and those supporting a “product quality” \nmodel. That division is a bit of a stretch in some places, but nevertheless begins \na divide-and-conquer march through a breathtaking array of qualities. See Figure \n12.1 for this array.\nThe standard lists the following quality attributes that deal with product \nquality:\n■\n■Functional suitability. The degree to which a product or system provides \nfunctions that meet stated and implied needs when used under specified \nconditions\n■\n■Performance efficiency. Performance relative to the amount of resources \nused under stated conditions\n■\n■Compatibility. The degree to which a product, system, or component can \nexchange information with other products, systems, or components, and/or \nperform its required functions, while sharing the same hardware or software \nenvironment\n■\n■Usability. The degree to which a product or system can be used by specified \nusers to achieve specified goals with effectiveness, efficiency, and satisfac-\ntion in a specified context of use\n\n\n194 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nSystem Software \nProduct Quality\n \nFunctional \nsuitability\nFunctional \ncompleteness\nFunctional \ncorrectness\nFunctional \nappropriateness\nPerformance \nefficiency\nTime behavior\n \nResource \nutilization\nCapacity\n \nCompatibility\n \nCoexistence\n \nInteroperability\n \nLearnability\n \nOperability\nUser interface \naesthetics\nAccessibility\n \nReliability\n \nMaturity\n \nAvailability\n \n \nFault tolerance\n \nRecoverability\n \nSecurity\n \nConfidentiality\n \nIntegrity\n \n \nNonrepudiation\n \nAccountability\n \nAuthenticity\n \nMaintainability\n \nModularity\n \nReusability\n \nAnalyzability\n \nModifiability\n \nTestability\n \nUsability\n \nAppropriateness \nrecognizability\nUser error \nprediction\nPortability\n \nAdaptability\n \nInstallability\n \nReplaceability\nFigure 12.1  The ISO/IEC FCD 25010 product quality standard\n\n\n12.4  Using Standard Lists of Quality Attributes—or Not \n195\n■\n■Reliability. The degree to which a system, product, or component performs \nspecified functions under specified conditions for a specified period of time\n■\n■Security. The degree to which a product or system protects information and \ndata so that persons or other products or systems have the degree of data \naccess appropriate to their types and levels of authorization\n■\n■Maintainability. The degree of effectiveness and efficiency with which a \nproduct or system can be modified by the intended maintainers\n■\n■Portability. The degree of effectiveness and efficiency with which a system, \nproduct, or component can be transferred from one hardware, software, or \nother operational or usage environment to another\nIn ISO 25010, these “quality characteristics” are each composed of “qual-\nity subcharacteristics” (for example, nonrepudiation is a subcharacteristic of se-\ncurity). The standard slogs through almost five dozen separate descriptions of \nquality subcharacteristics in this way. It defines for us the qualities of “pleasure” \nand “comfort.” It distinguishes among “functional correctness” and “functional \ncompleteness,” and then adds “functional appropriateness” for good measure. To \nexhibit “compatibility,” systems must either have “interoperability” or just plain \n“coexistence.” “Usability” is a product quality, not a quality-in-use quality, al-\nthough it includes “satisfaction,” which is a quality-in-use quality. “Modifiabil-\nity” and “testability” are both part of “maintainability.” So is “modularity,” which \nis a strategy for achieving a quality rather than a goal in its own right. “Avail-\nability” is part of “reliability.” “Interoperability” is part of “compatibility.” And \n“scalability” isn’t mentioned at all.\nGot all that?\nLists like these—and there are many—do serve a purpose. They can be help-\nful checklists to assist requirements gatherers in making sure that no important \nneeds were overlooked. Even more useful than standalone lists, they can serve \nas the basis for creating your own checklist that contains the quality attributes \nof concern in your domain, your industry, your organization, and your products. \nQuality attribute lists can also serve as the basis for establishing measures. If \n“pleasure” turns out to be an important concern in your system, how do you mea-\nsure it to know if your system is providing enough of it?\nHowever, general lists like these also have drawbacks. First, no list will ever \nbe complete. As an architect, you will be called upon to design a system to meet \na stakeholder concern not foreseen by any list-maker. For example, some writers \nspeak of “manageability,” which expresses how easy it is for system administra-\ntors to manage the application. This can be achieved by inserting useful instru-\nmentation for monitoring operation and for debugging and performance tuning. \nWe know of an architecture that was designed with the conscious goal of retain-\ning key staff and attracting talented new hires to a quiet region of the American \nMidwest. That system’s architects spoke of imbuing the system with “Iowabil-\nity.” They achieved it by bringing in state-of-the-art technology and giving their \ndevelopment teams wide creative latitude. Good luck finding “Iowability” in any \n\n\n196 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nstandard list of quality attributes, but that QA was as important to that organiza-\ntion as any other.\nSecond, lists often generate more controversy than understanding. You \nmight argue persuasively that “functional correctness” should be part of “reliabil-\nity,” or that “portability” is just a kind of “modifiability,” or that “maintainability” \nis a kind of “modifiability” (not the other way around). The writers of ISO 25010 \napparently spent time and effort deciding to make security its own characteristic, \ninstead of a subcharacteristic of functionality, which it was in a previous version. \nWe believe that effort in making these arguments could be better spent elsewhere.\nThird, these lists often purport to be taxonomies, which are lists with the \nspecial property that every member can be assigned to exactly one place. Quality \nattributes are notoriously squishy in this regard. We discussed denial of service as \nbeing part of security, availability, performance, and usability in Chapter 4.\nFinally, these lists force architects to pay attention to every quality attribute \non the list, even if only to finally decide that the particular quality attribute is ir-\nrelevant to their system. Knowing how to quickly decide that a quality attribute is \nirrelevant to a specific system is a skill gained over time.\nThese observations reinforce the lesson introduced in Chapter 4 that quality \nattribute names, by themselves, are largely useless and are at best invitations to \nbegin a conversation; that spending time worrying about what qualities are sub-\nqualities of what other qualities is also almost useless; and that scenarios provide \nthe best way for us to specify precisely what we mean when we speak of a quality \nattribute. \nUse standard lists of quality attributes to the extent that they are helpful as \nchecklists, but don’t feel the need to slavishly adhere to their terminology. \n12.5  \u0007Dealing with “X-ability”: Bringing a New \nQuality Attribute into the Fold\nSuppose, as an architect, you must deal with a quality attribute for which there \nis no compact body of knowledge, no “portfolio” like Chapters 5–11 provided \nfor those seven QAs? Suppose you find yourself having to deal with a quality \nattribute like “green computing” or “manageability” or even “Iowability”? What \ndo you do?\nCapture Scenarios for the New Quality Attribute\nThe first thing to do is interview the stakeholders whose concerns have led to the \nneed for this quality attribute. You can work with them, either individually or as \na group, to build a set of attribute characterizations that refine what is meant by \n\n\n12.5  Dealing with “X-ability”: Bringing a New Quality Attribute into the Fold\n197\nthe QA. For example, security is often decomposed into concerns such as confi-\ndentiality, integrity, availability, and others. After that refinement, you can work \nwith the stakeholders to craft a set of specific scenarios that characterize what is \nmeant by that QA. \nOnce you have a set of specific scenarios, then you can work to generalize \nthe collection. Look at the set of stimuli you’ve collected, the set of responses, \nthe set of response measures, and so on. Use those to construct a general scenario \nby making each part of the general scenario a generalization of the specific in-\nstances you collected. \nIn our experience, the steps described so far tend to consume about half a day.\nAssemble Design Approaches for the New Quality Attribute\nAfter you have a set of guiding scenarios for the QA, you can assemble a set of \ndesign approaches for dealing with it. You can do this by \n1.\t\nRevisiting a body of patterns you’re familiar with and asking yourself how \neach one affects the QA of interest.\n2.\t\nSearching for designs that have had to deal with this QA. You can search on \nthe name you’ve given the QA itself, but you can also search for the terms \nyou chose when you refined the QA into subsidiary attribute characteriza-\ntions (such as “confidentiality” for the QA of security). \n3.\t\nFinding experts in this area and interviewing them or simply writing and \nasking them for advice. \n4.\t\nUsing the general scenario to try to catalog a list of design approaches to \nproduce the responses in the response category.\n5.\t\nUsing the general scenario to catalog a list of ways in which a problematic \narchitecture would fail to produce the desired responses, and thinking of \ndesign approaches to head off those cases.\nModel the New Quality Attribute\nIf you can build a conceptual model of the quality attribute, this can be helpful in \ncreating a set of design approaches for it. By “model,” we don’t mean anything \nmore than understanding the set of parameters to which the quality attribute is \nsensitive. For example, a model of modifiability might tell us that modifiability \nis a function of how many places in a system have to be changed in response to \na modification, and the interconnectedness of those places. A model for perfor-\nmance might tell us that throughput is a function of transactional workload, the \ndependencies among the transactions, and the number of transactions that can be \nprocessed in parallel. \n\n\n198 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nOnce you have a model for your QA, then you can work to catalog the ar-\nchitectural approaches (tactics and patterns) open to you for manipulating each of \nthe relevant parameters in your favor.\nAssemble a Set of Tactics for the New Quality Attribute\nThere are two sources that can be used to derive tactics for any quality attribute: \nmodels and experts. \nFigure 12.2 shows a queuing model for performance. Such models are \nwidely used to analyze the latency and throughput of various types of queuing \nsystems, including manufacturing and service environments, as well as computer \nsystems. \nWithin this model, there are seven parameters that can affect the latency that \nthe model predicts: \n■\n■Arrival rate\n■\n■Queuing discipline\n■\n■Scheduling algorithm\n■\n■Service time\n■\n■Topology \n■\n■Network bandwidth\n■\n■Routing algorithm\nResults\nRouting of \nmessages\nArrivals\nQueue\nServer\nScheduling \nalgorithm\nFigure 12.2  A generic queuing model\n\n\n12.5  Dealing with “X-ability”: Bringing a New Quality Attribute into the Fold\n199\nThese are the only parameters that can affect latency within this model. This \nis what gives the model its power. Furthermore, each of these parameters can be \naffected by various architectural decisions. This is what makes the model useful \nfor an architect. For example, the routing algorithm can be fixed or it could be a \nload-balancing algorithm. A scheduling algorithm must be chosen. The topology \ncan be affected by dynamically adding or removing new servers. And so forth.\nThe process of generating tactics based on a model is this:\n■\n■Enumerate the parameters of the model\n■\n■For each parameter, enumerate the architectural decisions that can affect \nthis parameter\nWhat results is a list of tactics to, in the example case, control performance \nand, in the more general case, to control the quality attribute that the model is \nconcerned with. This makes the design problem seem much more tractable. This \nlist of tactics is finite and reasonably small, because the number of parameters of \nthe model is bounded, and for each parameter, the number of architectural deci-\nsions to affect the parameter is limited.\nDeriving tactics from models is fine as long as the quality attribute in ques-\ntion has a model. Unfortunately, the number of such models is limited and is a \nsubject of active research. There are no good architectural models for usability or \nsecurity, for example. In the cases where we had no model to work from, we did \nfour things to catalog the tactics: \n1.\t\nWe interviewed experts in the field, asking them what they do as architects \nto improve the quality attribute response. \n2.\t\nWe examined systems that were touted as having high usability (or testabil-\nity, or whatever tactic we were focusing on). \n3.\t\nWe scoured the relevant design literature looking for common themes in \ndesign. \n4.\t\nWe examined documented architectural patterns to look for ways they \nachieved the quality attribute responses touted for them.\nConstruct Design Checklists for the New Quality Attribute\nFinally, examine the seven categories of design decisions in Chapter 4 and ask \nyourself (or your experts) how to specialize your new quality of interest to these \ncategories. In particular, think about reviewing a software architecture and trying \nto figure out how well it satisfies your new qualities in these seven categories. \nWhat questions would you ask the architect of that system to understand how \nthe design attempts to achieve the new quality? These are the basis for the design \nchecklist.\n\n\n200 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\n12.6  For Further Reading\nFor most of the quality attributes we discussed in this chapter, the Internet is your \nfriend. You can find reasonable discussions of scalability, portability, and deploy-\nment strategies using your favorite search engine. Mobility is harder to find be-\ncause it has so many meanings, but look under “mobile computing” as a start.\nDistributed development is a topic covered in the International Conference \non Global Software Engineering, and looking at the proceedings of this confer-\nence will give you access to the latest research in this area (www.icgse.org).\nRelease It! [Nygard 07] has a good discussion of monitorability (which he \ncalls transparency) as well as potential problems that are manifested after ex-\ntended operation of a system. The book also includes various patterns for dealing \nwith some of the problems.\nTo gain an appreciation for the importance of software safety, we suggest \nreading some of the disaster stories that arise when software fails. A vener-\nable source is the ACM Risks Forum newsgroup, known as comp.risks in the \nUSENET community, available at www.risks.org. This list has been moderated \nby Peter Neumann since 1985 and is still going strong.\nNancy Leveson is an undisputed thought leader in the area of software and \nsafety. If you’re working in safety-critical systems, you should become familiar \nwith her work. You can start small with a paper like [Leveson 04], which dis-\ncusses a number of software-related factors that have contributed to spacecraft \naccidents. Or you can start at the top with [Leveson 11], a book that treats safety \nin the context of today’s complex, sociotechnical, software-intensive systems. \nThe Federal Aviation Administration is the U.S. government agency charged \nwith oversight of the U.S. airspace system, and the agency is extremely concerned \nabout safety. Their 2000 System Safety Handbook is a good practical overview of \nthe topic [FAA 00].\nIEEE STD-1228-1994 (“Software Safety Plans”) defines best practices for \nconducting software safety hazard analyses, to help ensure that requirements and \nattributes are specified for safety-critical software [IEEE 94]. The aeronautical \nstandard DO-178B (due to be replaced by DO-178C as this book goes to publica-\ntion) covers software safety requirements for aerospace applications.\nA discussion of safety tactics can be found in the work of Wu and Kelly \n[Wu 06]. \nIn particular, interlocks are an important tactic for safety. They enforce some \nsafe sequence of events, or ensure that a safe condition exists before an action is \ntaken. Your microwave oven shuts off when you open the door because of a hard-\nware interlock. Interlocks can be implemented in software also. For an interesting \ncase study of this, see [Wozniak 07]. \n",
      "page_number": 212
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 221-233)",
      "start_page": 221,
      "end_page": 233,
      "detection_method": "topic_boundary",
      "content": "12.7  Discussion Questions\n201\n12.7  Discussion Questions\n1.\t\nThe Kingdom of Bhutan measures the happiness of its population, and \ngovernment policy is formulated to increase Bhutan’s GNH (gross national \nhappiness). Go read about how the GNH is measured (try www.grossna-\ntionalhappiness.com) and then sketch a general scenario for the quality \nattribute of happiness that will let you express concrete happiness require-\nments for a software system.\n2.\t\nChoose a quality attribute not described in Chapters 5–11. For that quality \nattribute, assemble a set of specific scenarios that describe what you mean \nby it. Use that set of scenarios to construct a general scenario for it.\n3.\t\nFor the QA you chose for discussion question 2, assemble a set of design \napproaches (patterns and tactics) that help you achieve it.\n4.\t\nFor the QA you chose for discussion question 2, develop a design checklist \nfor that quality attribute using the seven categories of guiding quality de-\nsign decisions outlined in Chapter 4.\n5.\t\nWhat might cause you to add a tactic or pattern to the sets of quality attri-\nbutes already described in Chapters 5–11 (or any other quality attribute, for \nthat matter)? \n6.\t\nAccording to slate.com and other sources, a teenage girl in Germany “went \ninto hiding after she forgot to set her Facebook birthday invitation to private \nand accidentally invited the entire Internet. After 15,000 people confirmed \nthey were coming, the girl’s parents canceled the party, notified police, and \nhired private security to guard their home.” Fifteen hundred people showed \nup anyway; several minor injuries ensued. Is Facebook “unsafe”? Discuss.\n7.\t\nAuthor James Gleick (“A Bug and a Crash,” www.around.com/ariane.html) \nwrites that “It took the European Space Agency 10 years and $7 billion to \nproduce Ariane 5, a giant rocket capable of hurling a pair of three-ton sat-\nellites into orbit with each launch. . . . All it took to explode that rocket less \nthan a minute into its maiden voyage . . . was a small computer program \ntrying to stuff a 64-bit number into a 16-bit space. One bug, one crash. Of \nall the careless lines of code recorded in the annals of computer science, \nthis one may stand as the most devastatingly efficient.” Write a safety sce-\nnario that addresses the Ariane 5 disaster and discuss tactics that might have \nprevented it.\n8.\t\nDiscuss how you think development distributability tends to “trade off” \nagainst the quality attributes of performance, availability, modifiability, and \ninteroperability.\n\n\n202 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nExtra Credit: Close your eyes and, without peeking, spell “distributability.” \nBonus points for successfully saying “development distributability” three \ntimes as fast as you can.\n9.\t What is the relationship between mobility and security?\n10.\t Relate monitorability to observability and controllability, the two parts of \ntestability. Are they the same? If you want to make your system more of \none, can you just optimize for the other?\n\n\n203\n13\nArchitectural Tactics \nand Patterns\nI have not failed. I’ve just found \n10,000 ways that won’t work. \n—Thomas Edison\nThere are many ways to do design badly, and just a few ways to do it well. Be-\ncause success in architectural design is complex and challenging, designers have \nbeen looking for ways to capture and reuse hard-won architectural knowledge. \nArchitectural patterns and tactics are ways of capturing proven good design \nstructures, so that they can be reused. \nArchitectural patterns have seen increased interest and attention, from both \nsoftware practitioners and theorists, over the past 15 years or more. An architec-\ntural pattern \n■\n■is a package of design decisions that is found repeatedly in practice,\n■\n■has known properties that permit reuse, and \n■\n■describes a class of architectures.\nBecause patterns are (by definition) found in practice, one does not invent \nthem; one discovers them. Cataloging patterns is akin to the job of a Linnaean \nbotanist or zoologist: “discovering” patterns and describing their shared charac-\nteristics. And like the botanist, zoologist, or ecologist, the pattern cataloger strives \nto understand how the characteristics lead to different behaviors and different re-\nsponses to environmental conditions. For this reason there will never be a com-\nplete list of patterns: patterns spontaneously emerge in reaction to environmental \nconditions, and as long as those conditions change, new patterns will emerge. \nArchitectural design seldom starts from first principles. Experienced architects \ntypically think of creating an architecture as a process of selecting, tailoring, and \ncombining patterns. The software architect must decide how to instantiate a pat-\ntern—how to make it fit with the specific context and the constraints of the problem.\n\n\n204 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nIn Chapters 5–11 we have seen a variety of architectural tactics. These are \nsimpler than patterns. Tactics typically use just a single structure or computa-\ntional mechanism, and they are meant to address a single architectural force. For \nthis reason they give more precise control to an architect when making design \ndecisions than patterns, which typically combine multiple design decisions into \na package. Tactics are the “building blocks” of design, from which architectural \npatterns are created. Tactics are atoms and patterns are molecules. Most patterns \nconsist of (are constructed from) several different tactics. For this reason we say \nthat patterns package tactics. \nIn this chapter we will take a very brief tour through the patterns universe, \ntouching on some of the most important and most commonly used patterns for ar-\nchitecture, and we will then look at the relationships between patterns and tactics: \nshowing how a pattern is constructed from tactics, and showing how tactics can \nbe used to tailor patterns when the pattern that you find in a book or on a website \ndoesn’t quite address your design needs.\n13.1  Architectural Patterns\nAn architectural pattern establishes a relationship between:\n■\n■A context. A recurring, common situation in the world that gives rise to a \nproblem.\n■\n■A problem. The problem, appropriately generalized, that arises in the given \ncontext. The pattern description outlines the problem and its variants, and \ndescribes any complementary or opposing forces. The description of the \nproblem often includes quality attributes that must be met.\n■\n■A solution. A successful architectural resolution to the problem, appro-\npriately abstracted. The solution describes the architectural structures \nthat solve the problem, including how to balance the many forces at \nwork. The solution will describe the responsibilities of and static rela-\ntionships among elements (using a module structure), or it will describe \nthe runtime behavior of and interaction between elements (laying out a \ncomponent-and-connector or allocation structure). The solution for a pat-\ntern is determined and described by:\n■\n■A set of element types (for example, data repositories, processes, and \nobjects)\n■\n■A set of interaction mechanisms or connectors (for example, method \ncalls, events, or message bus)\n■\n■A topological layout of the components \n■\n■A set of semantic constraints covering topology, element behavior, and \ninteraction mechanisms\n\n\n13.2  Overview of the Patterns Catalog\n205\nThe solution description should also make clear what quality attributes are \nprovided by the static and runtime configurations of elements.\nThis {context, problem, solution} form constitutes a template for document-\ning a pattern. \nComplex systems exhibit multiple patterns at once. A web-based system \nmight employ a three-tier client-server architectural pattern, but within this pat-\ntern it might also use replication (mirroring), proxies, caches, firewalls, MVC, \nand so forth, each of which may employ more patterns and tactics. And all of \nthese parts of the client-server pattern likely employ layering to internally struc-\nture their software modules.\n13.2  Overview of the Patterns Catalog\nIn this section we list an assortment of useful and widely used patterns. This cata-\nlog is not meant to be exhaustive—in fact no such catalog is possible. Rather it is \nmeant to be representative. We show patterns of runtime elements (such as broker \nor client-server) and of design-time elements (such as layers). For each pattern \nwe list the context, problem, and solution. As part of the solution, we briefly de-\nscribe the elements, relations, and constraints of each pattern. \nApplying a pattern is not an all-or-nothing proposition. Pattern definitions \ngiven in catalogs are strict, but in practice architects may choose to violate them \nin small ways when there is a good design tradeoff to be had (sacrificing a little \nof whatever the violation cost, but gaining something that the deviation gained). \nFor example, the layered pattern expressly forbids software in lower layers from \nusing software in upper layers, but there may be cases (such as to gain some per-\nformance) when an architecture might allow a few specific exceptions. \nPatterns can be categorized by the dominant type of elements that they \nshow: module patterns show modules, component-and-connector (C&C) patterns \nshow components and connectors, and allocation patterns show a combination \nof software elements (modules, components, connectors) and nonsoftware ele-\nments. Most published patterns are C&C patterns, but there are module patterns \nand allocation patterns as well. We’ll begin with the granddaddy of module pat-\nterns, the layered pattern.\nModule Patterns\nLayered Pattern\nContext: All complex systems experience the need to develop and evolve por-\ntions of the system independently. For this reason the developers of the system \nneed a clear and well-documented separation of concerns, so that modules of the \nsystem may be independently developed and maintained. \n\n\n206 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nProblem: The software needs to be segmented in such a way that the modules \ncan be developed and evolved separately with little interaction among the parts, \nsupporting portability, modifiability, and reuse.\nSolution: To achieve this separation of concerns, the layered pattern divides the \nsoftware into units called layers. Each layer is a grouping of modules that offers a \ncohesive set of services. There are constraints on the allowed-to-use relationship \namong the layers: the relations must be unidirectional. Layers completely parti-\ntion a set of software, and each partition is exposed through a public interface. \nThe layers are created to interact according to a strict ordering relation. If (A,B) \nis in this relation, we say that the implementation of layer A is allowed to use any \nof the public facilities provided by layer B. In some cases, modules in one layer \nmight be required to directly use modules in a nonadjacent lower layer; normally \nonly next-lower-layer uses are allowed. This case of software in a higher layer \nusing modules in a nonadjacent lower layer is called layer bridging. If many in-\nstances of layer bridging occur, the system may not meet its portability and modi-\nfiability goals that strict layering helps to achieve. Upward usages are not allowed \nin this pattern. \nOf course, none of this comes for free. Someone must design and build the \nlayers, which can often add up-front cost and complexity to a system. Also, if the \nlayering is not designed correctly, it may actually get in the way, by not provid-\ning the lower-level abstractions that programmers at the higher levels need. And \nlayering always adds a performance penalty to a system. If a call is made to a \nfunction in the top-most layer, this may have to traverse many lower layers before \nbeing executed by the hardware. Each of these layers adds some overhead of their \nown, at minimum in the form of context switching.\nTable 13.1 summarizes the solution of the layered pattern.\nLayers are almost always drawn as a stack of boxes. The allowed-to-use \nrelation is denoted by geometric adjacency and is read from the top down, as in \nFigure 13.1. \nA\nB\nC\nKey:\nLayer\nA layer is allowed to use \nthe next lower layer.\nFigure 13.1  Stack-of-boxes notation for layered designs\n\n\n13.2  Overview of the Patterns Catalog\n207\nTable 13.1  Layered Pattern Solution\nOverview\nThe layered pattern defines layers (groupings of modules that offer \na cohesive set of services) and a unidirectional allowed-to-use \nrelation among the layers. The pattern is usually shown graphically \nby stacking boxes representing layers on top of each other. \nElements\nLayer, a kind of module. The description of a layer should define \nwhat modules the layer contains and a characterization of the \ncohesive set of services that the layer provides.\nRelations\nAllowed to use, which is a specialization of a more generic \ndepends-on relation. The design should define what the layer usage \nrules are (e.g., “a layer is allowed to use any lower layer” or “a layer \nis allowed to use only the layer immediately below it”) and any \nallowable exceptions. \nConstraints\n■\n■\nEvery piece of software is allocated to exactly one layer.\n■\n■\nThere are at least two layers (but usually there are three or \nmore).\n■\n■\nThe allowed-to-use relations should not be circular (i.e., a lower \nlayer cannot use a layer above).\nWeaknesses\n■\n■\nThe addition of layers adds up-front cost and complexity to a \nsystem.\n■\n■\nLayers contribute a performance penalty.\nSome Finer Points of Layers\nA layered architecture is one of the few places where connections among \ncomponents can be shown by adjacency, and where “above” and “below” \nmatter. If you turn Figure 13.1 upside-down so that C is on top, this would \nrepresent a completely different design. Diagrams that use arrows among \nthe boxes to denote relations retain their semantic meaning no matter the \norientation. \nThe layered pattern is one of the most commonly used patterns in all of \nsoftware engineering, but I’m often surprised by how many people still get \nit wrong.\nFirst, it is impossible to look at a stack of boxes and tell whether layer \nbridging is allowed or not. That is, can a layer use any lower layer, or just \nthe next lower one? It is the easiest thing in the world to resolve this; all the \narchitect has to do is include the answer in the key to the diagram’s nota-\ntion (something we recommend for all diagrams). For example, consider the \nlayered pattern presented in Figure 13.2 on the next page.\nBut I’m still surprised at how few architects actually bother to do this. \nAnd if they don’t, their layer diagrams are ambiguous.\nSecond, any old set of boxes stacked on top of each other does not \nconstitute a layered architecture. For instance, look at the design shown \nin Figure 13.3, which uses arrows instead of adjacency to indicate the \n\n\n208 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nrelationships among the boxes. Here, everything is allowed to use every-\nthing. This is decidedly not a layered architecture. The reason is that if \nLayer A is replaced by a different version, Layer C (which uses it in this fig-\nure) might well have to change. We don’t want our virtual machine layer to \nchange every time our application layer changes. But I’m still surprised at \nhow many people call a stack of boxes lined up with each other “layers” (or \nthink that layers are the same as tiers in a multi-tier architecture).\nKey:\nApplications\nServices\nData Bank\nEnvironmental Models\nEnvironment Sensing\nJVM\nOS and Hardware\nSecurity\nlayer\nSoftware in a layer is allowed to use software \nin the same layer, or any layer immediately \nbelow or to the right.\nFigure 13.2  A simple layer diagram, with a simple key answering the uses \nquestion\nLayer\nAllowed to use\nA\nB\nC\nKey:\nFigure 13.3  A wolf in layer’s clothing\n\n\n13.2  Overview of the Patterns Catalog\n209\nThird, many architectures that purport to be layered look something \nlike Figure 13.4. This diagram probably means that modules in A, B, or C \ncan use modules in D, but without a key to tell us for sure, it could mean \nanything. “Sidecars” like this often contain common utilities (sometimes \nimported), such as error handlers, communication protocols, or database \naccess mechanisms. This kind of diagram makes sense only in the case \nwhere no layer bridging is allowed in the main stack. Otherwise, D could \nsimply be made the bottom-most layer in the main stack, and the “sidecar” \ngeometry would be unnecessary. But I’m still surprised at how often I see \nthis layout go unexplained.\nSometimes layers are divided into segments denoting a finer-grained \ndecomposition of the modules. Sometimes this occurs when a preexisting \nset of units, such as imported modules, share the same allowed-to-use \nrelation. When this happens, you have to specify what usage rules are in \neffect among the segments. Many usage rules are possible, but they must \nbe made explicit. In Figure 13.5, the top and the bottom layers are\nA\nB\nC\nD\nFigure 13.4  Layers with a “sidecar” \nKey:\nLayer\nUI\nBusiness Logic\nData Access\nLocal Data\nAccess\nRemote Data\nAccess\nWeb UI\nRich\nClient\nCommand\nLine\nLayer\nsegment\nAllowed to use\nFigure 13.5  Layered design with segmented layers \n\n\n210 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nsegmented. Segments of the top layer are not allowed to use each other, \nbut segments of the bottom layer are. If you draw the same diagram with-\nout the arrows, it will be harder to differentiate the different usage rules \nwithin segmented layers. Layered diagrams are often a source of hidden \nambiguity because the diagram does not make explicit the allowed-to-use \nrelations. \nFinally, the most important point about layering is that a layer isn’t \nallowed to use any layer above it. A module “uses” another module when it \ndepends on the answer it gets back. But a layer is allowed to make upward \ncalls, as long as it isn’t expecting an answer from them. This is how the \ncommon error-handling scheme of callbacks works. A program in layer A \ncalls a program in a lower layer B, and the parameters include a pointer to \nan error-handling program in A that the lower layer should call in case of \nerror. The software in B makes the call to the program in A, but cares not in \nthe least what it does. By not depending in any way on the contents of A, B \nis insulated from changes in A. \n—PCC\nOther Module Patterns\nDesigners in a particular domain often publish “standard” module decomposi-\ntions for systems in that domain. These standard decompositions, if put in the \n“context, problem, solution” form, constitute module decomposition patterns.\nSimilarly in the object-oriented realm, “standard” or published class/object \ndesign solutions for a class of system constitute object-oriented patterns.\nComponent-and-Connector Patterns\nBroker Pattern\nContext: Many systems are constructed from a collection of services distributed \nacross multiple servers. Implementing these systems is complex because you \nneed to worry about how the systems will interoperate—how they will connect to \neach other and how they will exchange information—as well as the availability of \nthe component services.\nProblem: How do we structure distributed software so that service users do not \nneed to know the nature and location of service providers, making it easy to dy-\nnamically change the bindings between users and providers?\nSolution: The broker pattern separates users of services (clients) from providers \nof services (servers) by inserting an intermediary, called a broker. When a client \nneeds a service, it queries a broker via a service interface. The broker then for-\nwards the client’s service request to a server, which processes the request. The ser-\nvice result is communicated from the server back to the broker, which then returns \n\n\n13.2  Overview of the Patterns Catalog\n211\nthe result (and any exceptions) back to the requesting client. In this way the client \nremains completely ignorant of the identity, location, and characteristics of the \nserver. Because of this separation, if a server becomes unavailable, a replacement \ncan be dynamically chosen by the broker. If a server is replaced with a different \n(compatible) service, again, the broker is the only component that needs to know \nof this change, and so the client is unaffected. Proxies are commonly introduced as \nintermediaries in addition to the broker to help with details of the interaction with \nthe broker, such as marshaling and unmarshaling messages. \nThe down sides of brokers are that they add complexity (brokers and \npossibly proxies must be designed and implemented, along with messaging \nprotocols) and add a level of indirection between a client and a server, which will \nadd latency to their communication. Debugging brokers can be difficult because \nthey are involved in highly dynamic environments where the conditions leading \nto a failure may be difficult to replicate. The broker would be an obvious point of \nattack, from a security perspective, and so it needs to be hardened appropriately. \nAlso a broker, if it is not designed carefully, can be a single point of failure for \na large and complex system. And brokers can potentially be bottlenecks for \ncommunication.\nTable 13.2 summarizes the solution of the broker pattern.\nTable 13.2  Broker Pattern Solution\nOverview\nThe broker pattern defines a runtime component, called a broker, that \nmediates the communication between a number of clients and servers. \nElements\nClient, a requester of services\nServer, a provider of services\nBroker, an intermediary that locates an appropriate server to fulfill a \nclient’s request, forwards the request to the server, and returns the \nresults to the client\nClient-side proxy, an intermediary that manages the actual \ncommunication with the broker, including marshaling, sending, and \nunmarshaling of messages\nServer-side proxy, an intermediary that manages the actual \ncommunication with the broker, including marshaling, sending, and \nunmarshaling of messages\nRelations\nThe attachment relation associates clients (and, optionally, client-side \nproxies) and servers (and, optionally, server-side proxies) with brokers.\nConstraints\nThe client can only attach to a broker (potentially via a client-side \nproxy). The server can only attach to a broker (potentially via a server-\nside proxy).\nWeaknesses Brokers add a layer of indirection, and hence latency, between clients \nand servers, and that layer may be a communication bottleneck.\nThe broker can be a single point of failure.\nA broker adds up-front complexity.\nA broker may be a target for security attacks.\nA broker may be difficult to test.\n\n\n212 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThe broker is, of course, the critical component in this pattern. The pattern \nprovides all of the modifiability benefits of the use-an-intermediary tactic \n(described in Chapter 7), an availability benefit (because the broker pattern \nmakes it easy to replace a failed server with another), and a performance benefit \n(because the broker pattern makes it easy to assign work to the least-busy server). \nHowever, the pattern also carries with it some liabilities. For example, the use of \na broker precludes performance optimizations that you might make if you knew \nthe precise location and characteristics of the server. Also the use of this pattern \nadds the overhead of the intermediary and thus latency.\nThe original version of the broker pattern, as documented by Gamma, Helm, \nJohnson, and Vlissides [Gamma 94], is given in Figure 13.6.\nThe first widely used implementation of the broker pattern was in the \nCommon Object Request Broker Architecture (CORBA). Other common uses \nof this pattern are found in Enterprise Java Beans (EJB) and Microsoft’s .NET \nplatform—essentially any modern platform for distributed service providers and \nconsumers implements some form of a broker. The service-oriented architecture \n(SOA) approach depends crucially on brokers, most commonly in the form of an \nenterprise service bus. \nModel-View-Controller Pattern\nContext: User interface software is typically the most frequently modified portion \nof an interactive application. For this reason it is important to keep modifications \n+pack_data()\n+unpack_data()\n+send_request()\n+return()\nClient-S ide Proxy\n+initialize()\n+enter_main_loop()\n+run_service()\n+use_Broker_API()\nServer\n+call_server()\n+start_task()\n+use_Broker_API()\nClient\n+pack_data()\n+unpack_data()\n+call_service()\n+send_response()\nServer-Side Proxy\n+locateServer()\n+locateClient()\n+registerServer()\n+unregisterServer()\nBroker\n+pack_data()\n+unpack_data()\n+forward_message()\n+transmit_message()\nBridge\n-transfers\n*\n1\n*\n-call\n1\n-uses\n*\n1\n0..1\n-call\n1\n-transfers\n*\n1\n*\n-call\n1\n-uses\n*\n1\nFigure 13.6  The broker pattern \n\n\n13.2  Overview of the Patterns Catalog\n213\nto the user interface software separate from the rest of the system. Users often \nwish to look at data from different perspectives, such as a bar graph or a pie chart. \nThese representations should both reflect the current state of the data. \nProblem: How can user interface functionality be kept separate from application \nfunctionality and yet still be responsive to user input, or to changes in the under-\nlying application’s data? And how can multiple views of the user interface be cre-\nated, maintained, and coordinated when the underlying application data changes?\nSolution: The model-view-controller (MVC) pattern separates application func-\ntionality into three kinds of components: \n■\n■A model, which contains the application’s data\n■\n■A view, which displays some portion of the underlying data and interacts \nwith the user\n■\n■A controller, which mediates between the model and the view and manages \nthe notifications of state changes\nMVC is not appropriate for every situation. The design and implementation \nof three distinct kinds of components, along with their various forms of \ninteraction, may be costly, and this cost may not make sense for relatively \nsimple user interfaces. Also, the match between the abstractions of MVC and \ncommercial user interface toolkits is not perfect. The view and the controller split \napart input and output, but these functions are often combined into individual \nwidgets. This may result in a conceptual mismatch between the architecture and \nthe user interface toolkit.\nTable 13.3 summarizes the solution of the MVC pattern.\nTable 13.3  Model-View-Controller Pattern Solution\nOverview\nThe MVC pattern breaks system functionality into three components: a \nmodel, a view, and a controller that mediates between the model and \nthe view.\nElements\nThe model is a representation of the application data or state, and it \ncontains (or provides an interface to) application logic.\nThe view is a user interface component that either produces a \nrepresentation of the model for the user or allows for some form of \nuser input, or both.\nThe controller manages the interaction between the model and the \nview, translating user actions into changes to the model or changes to \nthe view.\nRelations\nThe notifies relation connects instances of model, view, and controller, \nnotifying elements of relevant state changes. \nConstraints\nThere must be at least one instance each of model, view, and \ncontroller.\nThe model component should not interact directly with the controller.\nWeaknesses\nThe complexity may not be worth it for simple user interfaces.\nThe model, view, and controller abstractions may not be good fits for \nsome user interface toolkits.\n",
      "page_number": 221
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 234-246)",
      "start_page": 234,
      "end_page": 246,
      "detection_method": "topic_boundary",
      "content": "214 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThere may, in fact, be many views and many controllers associated with \na model. For example, a set of business data may be represented as columns of \nnumbers in a spreadsheet, as a scatter plot, or as a pie chart. Each of these is a \nseparate view, and this view can be dynamically updated as the model changes \n(for example, showing live transactions in a transaction processing system). A \nmodel may be updated by different controllers; for example, a map could be \nzoomed and panned via mouse movements, trackball movements, keyboard \nclicks, or voice commands; each of these different forms of input needs to be \nmanaged by a controller. \nThe MVC components are connected to each other via some flavor of \nnotification, such as events or callbacks. These notifications contain state updates. \nA change in the model needs to be communicated to the views so that they may \nbe updated. An external event, such as a user input, needs to be communicated to \nthe controller, which may in turn update the view and/or the model. Notifications \nmay be either push or pull.\nBecause these components are loosely coupled, it is easy to develop and \ntest them in parallel, and changes to one have minimal impact on the others. The \nrelationships between the components of MVC are shown in Figure 13.7.\n• Encapsulates application state\n• Responds to state queries\n• Exposes application functionality\n• Notifies views of changes\nModel\n• Renders the models\n• Requests updates from models\n• Sends user gestures to controller\n• Allows controller to select view\nView\n• Defines application behavior\n• Maps user actions to model updates\n• Selects view for response\n• One for each functionality\nController\nState\nQuery\nState\nChange\nUser Gestures\nView Selection\nChange\nNotification\nKey:\nEvents\nMethod \nInvocations\nFigure 13.7  The model-view-controller pattern\n\n\n13.2  Overview of the Patterns Catalog\n215\nThe MVC pattern is widely used in user interface libraries such as Java’s \nSwing classes, Microsoft’s ASP.NET framework, Adobe’s Flex software \ndevelopment kit, Nokia’s Qt framework, and many others. As such, it is common \nfor a single application to contain many instances of MVC (often one per user \ninterface object).\nPipe-and-Filter Pattern\nContext: Many systems are required to transform streams of discrete data items, \nfrom input to output. Many types of transformations occur repeatedly in practice, \nand so it is desirable to create these as independent, reusable parts. \nProblem: Such systems need to be divided into reusable, loosely coupled com-\nponents with simple, generic interaction mechanisms. In this way they can be \nflexibly combined with each other. The components, being generic and loosely \ncoupled, are easily reused. The components, being independent, can execute in \nparallel.\nSolution: The pattern of interaction in the pipe-and-filter pattern is characterized \nby successive transformations of streams of data. Data arrives at a filter’s input \nport(s), is transformed, and then is passed via its output port(s) through a pipe to \nthe next filter. A single filter can consume data from, or produce data to, one or \nmore ports. \nThere are several weaknesses associated with the pipe-and-filter pattern. For \ninstance, this pattern is typically not a good choice for an interactive system, as \nit disallows cycles (which are important for user feedback). Also, having large \nnumbers of independent filters can add substantial amounts of computational \noverhead, because each filter runs as its own thread or process. Also, pipe-and-\nfilter systems may not be appropriate for long-running computations, without the \naddition of some form of checkpoint/restore functionality, as the failure of any \nfilter (or pipe) can cause the entire pipeline to fail.\nThe solution of the pipe-and-filter pattern is summarized in Table 13.4. \nPipes buffer data during communication. Because of this property, filters can \nexecute asynchronously and concurrently. Moreover, a filter typically does not \nknow the identity of its upstream or downstream filters. For this reason, pipeline \npipe-and-filter systems have the property that the overall computation can be \ntreated as the functional composition of the computations of the filters, making it \neasier for the architect to reason about end-to-end behavior.\nData transformation systems are typically structured as pipes and filters, \nwith each filter responsible for one part of the overall transformation of the input \ndata. The independent processing at each step supports reuse, parallelization, and \nsimplified reasoning about overall behavior. Often such systems constitute the \nfront end of signal-processing applications. These systems receive sensor data at \na set of initial filters; each of these filters compresses the data and performs initial \nprocessing (such as smoothing). Downstream filters reduce the data further and \n\n\n216 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\ndo synthesis across data derived from different sensors. The final filter typically \npasses its data to an application, for example providing input to modeling or \nvisualization tools. \nOther systems that use pipe-and-filter include those built using UNIX pipes, \nthe request processing architecture of the Apache web server, the map-reduce \npattern (presented later in this chapter), Yahoo! Pipes for processing RSS feeds, \nmany workflow engines, and many scientific computation systems that have to \nprocess and analyze large streams of captured data. Figure 13.8 shows a UML \ndiagram of a pipe-and-filter system.\nTable 13.4  Pipe-and-Filter Pattern Solution\nOverview\nData is transformed from a system’s external inputs to its external \noutputs through a series of transformations performed by its filters \nconnected by pipes.\nElements\nFilter, which is a component that transforms data read on its input \nport(s) to data written on its output port(s). Filters can execute \nconcurrently with each other. Filters can incrementally transform \ndata; that is, they can start producing output as soon as they start \nprocessing input. Important characteristics include processing rates, \ninput/output data formats, and the transformation executed by the \nfilter.\nPipe, which is a connector that conveys data from a filter’s output \nport(s) to another filter’s input port(s). A pipe has a single source \nfor its input and a single target for its output. A pipe preserves the \nsequence of data items, and it does not alter the data passing \nthrough. Important characteristics include buffer size, protocol of \ninteraction, transmission speed, and format of the data that passes \nthrough a pipe.\nRelations\nThe attachment relation associates the output of filters with the input \nof pipes and vice versa. \nConstraints\nPipes connect filter output ports to filter input ports. \nConnected filters must agree on the type of data being passed along \nthe connecting pipe.\nSpecializations of the pattern may restrict the association of \ncomponents to an acyclic graph or a linear sequence, sometimes \ncalled a pipeline.\nOther specializations may prescribe that components have certain \nnamed ports, such as the stdin, stdout, and stderr ports of UNIX \nfilters.\nWeaknesses\nThe pipe-and-filter pattern is typically not a good choice for an \ninteractive system.\nHaving large numbers of independent filters can add substantial \namounts of computational overhead.\nPipe-and-filter systems may not be appropriate for long-running \ncomputations.\n\n\n13.2  Overview of the Patterns Catalog\n217\ncapacity = 40\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 30 sec and retry\ncapacity = 50\nend-of-data = ”EOT” String\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 20 sec and retry\ncapacity = 10\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 60 sec and retry\ncapacity = 40\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 30 sec and retry\n«pipe»\n«pipe»\n«pipe»\n«pipe»\nout\nin\nout\nin\nout\nout\nin\nin\n«filter»\n:XmlToObject\n«filter»\n:Process\nPayment\n«filter»\n:FormatRejected\nRecords\n«filter»\n:Calculate\nDirectDeposit\n«filter»\n:Format\nDirectDeposit\nFigure 13.8  A UML diagram of a pipe-and-filter-based system\nClient-Server Pattern\nContext: There are shared resources and services that large numbers of distrib-\nuted clients wish to access, and for which we wish to control access or quality of \nservice.\nProblem: By managing a set of shared resources and services, we can promote \nmodifiability and reuse, by factoring out common services and having to modify \nthese in a single location, or a small number of locations. We want to improve \nscalability and availability by centralizing the control of these resources and ser-\nvices, while distributing the resources themselves across multiple physical servers. \nSolution: Clients interact by requesting services of servers, which provide a set \nof services. Some components may act as both clients and servers. There may be \none central server or multiple distributed ones. \nThe client-server pattern solution is summarized in Table 13.5; the \ncomponent types are clients and servers; the principal connector type for the \nclient-server pattern is a data connector driven by a request/reply protocol used \nfor invoking services. \nSome of the disadvantages of the client-server pattern are that the server \ncan be a performance bottleneck and it can be a single point of failure. Also, \ndecisions about where to locate functionality (in the client or in the server) are \noften complex and costly to change after a system has been built.\n\n\n218 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.5  Client-Server Pattern Solution\nOverview\nClients initiate interactions with servers, invoking services as \nneeded from those servers and waiting for the results of those \nrequests.\nElements\nClient, a component that invokes services of a server \ncomponent. Clients have ports that describe the services they \nrequire. \nServer, a component that provides services to clients. Servers \nhave ports that describe the services they provide. Important \ncharacteristics include information about the nature of the \nserver ports (such as how many clients can connect) and \nperformance characteristics (e.g., maximum rates of service \ninvocation). \nRequest/reply connector, a data connector employing a \nrequest/reply protocol, used by a client to invoke services on a \nserver. Important characteristics include whether the calls are \nlocal or remote, and whether data is encrypted.\nRelations\nThe attachment relation associates clients with servers.\nConstraints\nClients are connected to servers through request/reply \nconnectors.\nServer components can be clients to other servers. \nSpecializations may impose restrictions:\n■\n■\nNumbers of attachments to a given port\n■\n■\nAllowed relations among servers\nComponents may be arranged in tiers, which are logical \ngroupings of related functionality or functionality that will share \na host computing environment (covered more later in this \nchapter).\nWeaknesses\nServer can be a performance bottleneck.\nServer can be a single point of failure.\nDecisions about where to locate functionality (in the client or \nin the server) are often complex and costly to change after a \nsystem has been built.\nSome common examples of systems that use the client-server pattern are these: \n■\n■Information systems running on local networks where the clients are GUI-\nlaunched applications and the server is a database management system\n■\n■Web-based applications where the clients are web browsers and the servers \nare components running on an e-commerce site \nThe computational flow of pure client-server systems is asymmetric: \nclients initiate interactions by invoking services of servers. Thus, the client must \nknow the identity of a service to invoke it, and clients initiate all interactions. \nIn contrast, servers do not know the identity of clients in advance of a service \nrequest and must respond to the initiated client requests. \nIn early forms of client-server, service invocation is synchronous: the \nrequester of a service waits, or is blocked, until a requested service completes its \n\n\n13.2  Overview of the Patterns Catalog\n219\nactions, possibly providing a return result. However, variants of the client-server \npattern may employ more-sophisticated connector protocols. For example:\n■\n■Web browsers don’t block until the data request is served up.\n■\n■In some client-server patterns, servers are permitted to initiate certain \nactions on their clients. This might be done by allowing a client to register \nnotification procedures, or callbacks, that the server calls at specific times. \n■\n■In other systems service calls over a request/reply connector are bracketed \nby a “session” that delineates the start and end of a set of a client-server \ninteraction.\nThe client-server pattern separates client applications from the services they \nuse. This pattern simplifies systems by factoring out common services, which are \nreusable. Because servers can be accessed by any number of clients, it is easy \nto add new clients to a system. Similarly, servers may be replicated to support \nscalability or availability. \nThe World Wide Web is the best-known example of a system that is based on \nthe client-server pattern, allowing clients (web browsers) to access information \nfrom servers across the Internet using HyperText Transfer Protocol (HTTP). \nHTTP is a request/reply protocol. HTTP is stateless; the connection between the \nclient and the server is terminated after each response from the server.\nFigure 13.9 uses an informal notation to describe the client-server view of \nan automatic teller machine (ATM) banking system.\nserver\nServer\nTCP socket connector with\nclient and server ports\nFTX server\ndaemon\nATM OS/2\nclient process\nWindows\napplication\nclient\nclient\nclient\nClient\nclient\nclient\nserver server\nserver\nserver\nKey:\nBank\ntransaction\nauthorizer\nATM\nmonitoring\nserver\nATM\nreconfiguration\nserver\nATM main\nprocess\nReconfigure\nand update\nprocess\nMonitoring\nstation\nprogram\nFigure 13.9  The client-server architecture of an ATM banking system \n\n\n220 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nPeer-to-Peer Pattern\nContext: Distributed computational entities—each of which is considered \nequally important in terms of initiating an interaction and each of which provides \nits own resources—need to cooperate and collaborate to provide a service to a \ndistributed community of users.\nProblem: How can a set of “equal” distributed computational entities be con-\nnected to each other via a common protocol so that they can organize and share \ntheir services with high availability and scalability?\nSolution: In the peer-to-peer (P2P) pattern, components directly interact as \npeers. All peers are “equal” and no peer or group of peers can be critical for \nthe health of the system. Peer-to-peer communication is typically a request/\nreply interaction without the asymmetry found in the client-server pattern. \nThat is, any component can, in principle, interact with any other component by \nrequesting its services. The interaction may be initiated by either party—that \nis, in client-server terms, each peer component is both a client and a server. \nSometimes the interaction is just to forward data without the need for a reply. \nEach peer provides and consumes similar services and uses the same protocol. \nConnectors in peer-to-peer systems involve bidirectional interactions, reflecting \nthe two-way communication that may exist between two or more peer-to-peer \ncomponents. \nPeers first connect to the peer-to-peer network on which they discover other \npeers they can interact with, and then initiate actions to achieve their computation \nby cooperating with other peers by requesting services. Often a peer’s search for \nanother peer is propagated from one peer to its connected peers for a limited \nnumber of hops. A peer-to-peer architecture may have specialized peer nodes \n(called supernodes) that have indexing or routing capabilities and allow a regular \npeer’s search to reach a larger number of peers. \nPeers can be added and removed from the peer-to-peer network with no sig-\nnificant impact, resulting in great scalability for the whole system. This provides \nflexibility for deploying the system across a highly distributed platform.\nTypically multiple peers have overlapping capabilities, such as providing \naccess to the same data or providing equivalent services. Thus, a peer acting as \nclient can collaborate with multiple peers acting as servers to complete a certain \ntask. If one of these multiple peers becomes unavailable, the others can still pro-\nvide the services to complete the task. The result is improved overall availability. \nThere are also performance advantages: The load on any given peer component \nacting as a server is reduced, and the responsibilities that might have required \nmore server capacity and infrastructure to support it are distributed. This can de-\ncrease the need for other communication for updating data and for central server \nstorage, but at the expense of storing the data locally.\n\n\n13.2  Overview of the Patterns Catalog\n221\nThe drawbacks of the peer-to-peer pattern are strongly related to its \nstrengths. Because peer-to-peer systems are decentralized, managing security, \ndata consistency, data and service availability, backup, and recovery are all more \ncomplex. In many cases it is difficult to provide guarantees with peer-to-peer \nsystems because the peers come and go; instead, the architect can, at best, offer \nprobabilities that quality goals will be met, and these probabilities typically in-\ncrease with the size of the population of peers. \nTable 13.6 on the next page summarizes the peer-to-peer pattern solution. \nPeer-to-peer computing is often used in distributed computing applications \nsuch as file sharing, instant messaging, desktop grid computing, routing, and \nwireless ad hoc networking. Examples of peer-to-peer systems include file-shar-\ning networks such as BitTorrent and eDonkey, and instant messaging and VoIP \napplications such as Skype. Figure 13.10 shows an example of an instantiation of \nthe peer-to-peer pattern.\nA\nB\nmoldy\n69.95.63.49\namidala\n70.116.152.15\nanakin\n207.192.20.13\nlambda\n50.64.16.14\noutrider\n74.12.41.111\nnaboo\n157.66.24.26\nKey:\nLeaf peer\nUltrapeer\nGnutella port\nHTTP file transfer\nfrom A to B\nRequest/reply using Gnutella\nprotocol over TCP or UDP\nFigure 13.10  A peer-to-peer view of a Gnutella network using an informal C&C \nnotation. For brevity, only a few peers are identified. Each of the identified leaf \npeers uploads and downloads files directly from other peers.\n\n\n222 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.6  Peer-to-Peer Pattern Solution\nOverview\nComputation is achieved by cooperating peers that request service \nfrom and provide services to one another across a network.\nElements\nPeer, which is an independent component running on a network \nnode. Special peer components can provide routing, indexing, and \npeer search capability.\nRequest/reply connector, which is used to connect to the peer \nnetwork, search for other peers, and invoke services from other \npeers. In some cases, the need for a reply is done away with.\nRelations\nThe  relation associates peers with their connectors. Attachments \nmay change at runtime.\nConstraints\nRestrictions may be placed on the following:\n■\n■\nThe number of allowable attachments to any given peer\n■\n■\nThe number of hops used for searching for a peer\n■\n■\nWhich peers know about which other peers\nSome P2P networks are organized with star topologies, in which \npeers only connect to supernodes.\nWeaknesses\nManaging security, data consistency, data/service availability, \nbackup, and recovery are all more complex.\nSmall peer-to-peer systems may not be able to consistently achieve \nquality goals such as performance and availability.\nService-Oriented Architecture Pattern\nContext: A number of services are offered (and described) by service provid-\ners and consumed by service consumers. Service consumers need to be able \nto understand and use these services without any detailed knowledge of their \nimplementation.\nProblem: How can we support interoperability of distributed components run-\nning on different platforms and written in different implementation languages, \nprovided by different organizations, and distributed across the Internet? How can \nwe locate services and combine (and dynamically recombine) them into meaning-\nful coalitions while achieving reasonable performance, security, and availability?\nSolution: The service-oriented architecture (SOA) pattern describes a collection \nof distributed components that provide and/or consume services. In an SOA, ser-\nvice provider components and service consumer components can use different \nimplementation languages and platforms. Services are largely standalone: service \nproviders and service consumers are usually deployed independently, and often \nbelong to different systems or even different organizations. Components have in-\nterfaces that describe the services they request from other components and the \nservices they provide. A service’s quality attributes can be specified and guar-\nanteed with a service-level agreement (SLA). In some cases, these are legally \nbinding. Components achieve their computation by requesting services from one \nanother.\n\n\n13.2  Overview of the Patterns Catalog\n223\nThe elements in this pattern include service providers and service consum-\ners, which in practice can take different forms, from JavaScript running on a \nweb browser to CICS transactions running on a mainframe. In addition to the \nservice provider and service consumer components, an SOA application may \nuse specialized components that act as intermediaries and provide infrastruc-\nture services:\n■\n■Service invocation can be mediated by an enterprise service bus (ESB). An \nESB routes messages between service consumers and service providers. In \naddition, an ESB can convert messages from one protocol or technology to \nanother, perform various data transformations (e.g., format, content, split-\nting, merging), perform security checks, and manage transactions. Using an \nESB promotes interoperability, security, and modifiability. Of course, com-\nmunicating through an ESB adds overhead thereby lowering performance, \nand introduces an additional point of failure. When an ESB is not in place, \nservice providers and consumers communicate with each other in a point-\nto-point fashion.\n■\n■To improve the independence of service providers, a service registry can be \nused in SOA architectures. The registry is a component that allows services \nto be registered at runtime. This enables runtime discovery of services, \nwhich increases system modifiability by hiding the location and identity of \nthe service provider. A registry can even permit multiple live versions of the \nsame service.\n■\n■An orchestration server (or orchestration engine) orchestrates the interac-\ntion among various service consumers and providers in an SOA system. It \nexecutes scripts upon the occurrence of a specific event (e.g., a purchase \norder request arrived). Applications with well-defined business processes or \nworkflows that involve interactions with distributed components or systems \ngain in modifiability, interoperability, and reliability by using an orches-\ntration server. Many commercially available orchestration servers support \nvarious workflow or business process language standards.\nThe basic types of connectors used in SOA are these: \n■\n■SOAP. The standard protocol for communication in the web services tech-\nnology. Service consumers and providers interact by exchanging request/\nreply XML messages typically on top of HTTP.\n■\n■Representational State Transfer (REST). A service consumer sends non-\nblocking HTTP requests. These requests rely on the four basic HTTP com-\nmands (POST, GET, PUT, DELETE) to tell the service provider to create, \nretrieve, update, or delete a resource.\n■\n■Asynchronous messaging, a “fire-and-forget” information exchange. \nParticipants do not have to wait for an acknowledgment of receipt, because \nthe infrastructure is assumed to have delivered the message successfully. \nThe messaging connector can be point-to-point or publish-subscribe.\n\n\n224 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nIn practice, SOA environments may involve a mix of the three connectors \njust listed, along with legacy protocols and other communication alternatives \n(e.g., SMTP). Commercial products such as IBM’s WebSphere MQ, Microsoft’s \nMSMQ, or Apache’s ActiveMQ are infrastructure components that provide asyn-\nchronous messaging. SOAP and REST are described in more detail in Chapter 6.\nAs you can see, the SOA pattern can be quite complex to design and im-\nplement (due to dynamic binding and the concomitant use of metadata). Other \npotential problems with this pattern include the performance overhead of the \nmiddleware that is interposed between services and clients and the lack of perfor-\nmance guarantees (because services are shared and, in general, not under control \nof the requester). These weaknesses are all shared with the broker pattern, which \nis not surprising because the SOA pattern shares many of the design concepts and \ngoals of broker. In addition, because you do not, in general, control the evolution \nof the services that you use, you may have to endure high and unplanned-for \nmaintenance costs. \nTable 13.7 summarizes the SOA pattern.\nThe main benefit and the major driver of SOA is interoperability. Because \nservice providers and service consumers may run on different platforms, ser-\nvice-oriented architectures often integrate a variety of systems, including legacy \nsystems. SOA also offers the necessary elements to interact with external ser-\nvices available over the Internet. Special SOA components such as the registry or \nthe ESB also allow dynamic reconfiguration, which is useful when there’s a need \nto replace or add versions of components with no system interruption. \nFigure 13.11 shows the SOA view of a system called Adventure Builder. \nAdventure Builder allows a customer on the web to assemble a vacation by \nchoosing an activity and lodging at and transportation to a destination. The Ad-\nventure Builder system interacts with external service providers to construct the \nvacation, and with bank services to process payment. The central OPC (Order \nProcessing Center) component coordinates the interaction with internal and ex-\nternal service consumers and providers. Note that the external providers can be \nlegacy mainframe systems, Java systems, .NET systems, and so on. The nature of \nthese external components is transparent because SOAP provides the necessary \ninteroperability. \n\n\n13.2  Overview of the Patterns Catalog\n225\nTable 13.7  Service-Oriented Architecture Pattern Solution\nOverview\nComputation is achieved by a set of cooperating components \nthat provide and/or consume services over a network. The \ncomputation is often described using a workflow language.\nElements\nComponents:\n■\n■\nService providers, which provide one or more services \nthrough published interfaces. Concerns are often tied to \nthe chosen implementation technology, and include perfor-\nmance, authorization constraints, availability, and cost. In \nsome cases these properties are specified in a service-level \nagreement.\n■\n■\nService consumers, which invoke services directly or through \nan intermediary.\n■\n■\nService providers may also be service consumers.\n■\n■\nESB, which is an intermediary element that can route and \ntransform messages between service providers and consum-\ners.\n■\n■\nRegistry of services, which may be used by providers to \nregister their services and by consumers to discover services \nat runtime.\n■\n■\nOrchestration server, which coordinates the interactions \nbetween service consumers and providers based on \nlanguages for business processes and workflows.\nConnectors:\n■\n■\nSOAP connector, which uses the SOAP protocol for \nsynchronous communication between web services, typically \nover HTTP. \n■\n■\nREST connector, which relies on the basic request/reply \noperations of the HTTP protocol.\n■\n■\nAsynchronous messaging connector, which uses a \nmessaging system to offer point-to-point or publish-subscribe \nasynchronous message exchanges.\nRelations\nAttachment of the different kinds of components available to the \nrespective connectors\nConstraints\nService consumers are connected to service providers, but \nintermediary components (e.g., ESB, registry, orchestration \nserver) may be used. \nWeaknesses\nSOA-based systems are typically complex to build.\nYou don’t control the evolution of independent services.\nThere is a performance overhead associated with the \nmiddleware, and services may be performance bottlenecks, and \ntypically do not provide performance guarantees.\n\n\n226 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nKey:\nAdventure Builder\njdbc\njdbc\nTBD\nOpcOrder\nTrackingService\nOpcPurchase\nOrderService\nWeb\nService\nBroker\nWeb\nbrowser\nConsumer\nWeb site\nOPC\nBank\nAdventure\nCatalog\nDB\nUser’s\ne-mail\nclient\nAirline\nProvider\nLodging\nProvider\nActivity\nProvider\nAdventure\nOPC DB\nService\nRegistry\nActivityPO\n Service\nLodgingPO\nService\nAirlinePO\nService\nCreditCard\nService\nClient-side\napplication\nJava EE\napplication\nWeb services\nendpoint\nData\nrepository\nHTTP/HTTPS\nSOAP call\nData access\nSMTP\nScope of the\napplication (not \na component)\nExternal Web\nservice provider\nFigure 13.11  Diagram of the SOA view for the Adventure Builder system. OPC \nstands for “Order Processing Center.”\nPublish-Subscribe Pattern\nContext: There are a number of independent producers and consumers of data \nthat must interact. The precise number and nature of the data producers and con-\nsumers are not predetermined or fixed, nor is the data that they share. \n",
      "page_number": 234
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 247-256)",
      "start_page": 247,
      "end_page": 256,
      "detection_method": "topic_boundary",
      "content": "13.2  Overview of the Patterns Catalog\n227\nProblem: How can we create integration mechanisms that support the ability to \ntransmit messages among the producers and consumers in such a way that they \nare unaware of each other’s identity, or potentially even their existence? \nSolution: In the publish-subscribe pattern, summarized in Table 13.8, compo-\nnents interact via announced messages, or events. Components may subscribe \nto a set of events. It is the job of the publish-subscribe runtime infrastructure to \nmake sure that each published event is delivered to all subscribers of that event. \nThus, the main form of connector in these patterns is an event bus. Publisher \ncomponents place events on the bus by announcing them; the connector then de-\nlivers those events to the subscriber components that have registered an interest in \nthose events. Any component may be both a publisher and a subscriber.\nPublish-subscribe adds a layer of indirection between senders and receivers. \nThis has a negative effect on latency and potentially scalability, depending on \nhow it is implemented. One would typically not want to use publish-subscribe in \na system that had hard real-time deadlines to meet, as it introduces uncertainty in \nmessage delivery times.\nAlso, the publish-subscribe pattern suffers in that it provides less control \nover ordering of messages, and delivery of messages is not guaranteed (because \nthe sender cannot know if a receiver is listening). This can make the publish-sub-\nscribe pattern inappropriate for complex interactions where shared state is critical.\nTable 13.8  Publish-Subscribe Pattern Solution\nOverview\nComponents publish and subscribe to events. When an event is \nannounced by a component, the connector infrastructure dispatches \nthe event to all registered subscribers.\nElements\nAny C&C component with at least one publish or subscribe port. \nConcerns include which events are published and subscribed to, and \nthe granularity of events.\nThe publish-subscribe connector, which will have announce and listen \nroles for components that wish to publish and subscribe to events.\nRelations\nThe attachment relation associates components with the publish-\nsubscribe connector by prescribing which components announce \nevents and which components are registered to receive events.\nConstraints\nAll components are connected to an event distributor that may be \nviewed as either a bus—connector—or a component. Publish ports \nare attached to announce roles and subscribe ports are attached to \nlisten roles. Constraints may restrict which components can listen to \nwhich events, whether a component can listen to its own events, and \nhow many publish-subscribe connectors can exist within a system.\nA component may be both a publisher and a subscriber, by having \nports of both types.\nWeaknesses\nTypically increases latency and has a negative effect on scalability and \npredictability of message delivery time.\nLess control over ordering of messages, and delivery of messages is \nnot guaranteed.\n\n\n228 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThere are some specific refinements of this pattern that are in common use. \nWe will describe several of these later in this section.\nThe computational model for the publish-subscribe pattern is best thought of \nas a system of independent processes or objects, which react to events generated \nby their environment, and which in turn cause reactions in other components as \na side effect of their event announcements. An example of the publish-subscribe \npattern, implemented on top of the Eclipse platform, is shown in Figure 13.12.\nTypical examples of systems that employ the publish-subscribe pattern are \nthe following:\n■\n■Graphical user interfaces, in which a user’s low-level input actions are \ntreated as events that are routed to appropriate input handlers\n■\n■MVC-based applications, in which view components are notified when the \nstate of a model object changes\n■\n■Enterprise resource planning (ERP) systems, which integrate many compo-\nnents, each of which is only interested in a subset of system events\n■\n■Extensible programming environments, in which tools are coordinated \nthrough events\n■\n■Mailing lists, where a set of subscribers can register interest in specific \ntopics \nKey:\nEclipse UI event manager\nRegister \naction\nhandlers\nUI\nevent\nhandle \nUI event\nCRUD\nfact data\nassert/modify/\nretract fact\nSEI.ArchE.UI\nplug-in config\nviews and\neditors\nFact\ndata in\nmemory\nArchE\ncore\nlistener\naction\nhandler\nArchE\ncore\nfaçade\nJess\nnew or\nsetField()\nnotify data\nchange\nregister views as \nobserver of facts\nregister to fact \ndata changes\nnotify fact\ndata change\nAction\nhandler\nobject\nUI screen\nobject\nJava\nobject\nExternal\nprogram\nXML file\nEvent manager\n(part of Eclipse\nplatform)\nRegister to\nlisten for event\nEvent send/\nreceive\nJava method\ncall\nFigure 13.12  A typical publish-subscribe pattern realization\n\n\n13.2  Overview of the Patterns Catalog\n229\n■\n■Social networks, where “friends” are notified when changes occur to a \nperson’s website\nThe publish-subscribe pattern is used to send events and messages to an un-\nknown set of recipients. Because the set of event recipients is unknown to the \nevent producer, the correctness of the producer cannot, in general, depend on \nthose recipients. Thus, new recipients can be added without modification to the \nproducers. \nHaving components be ignorant of each other’s identity results in easy mod-\nification of the system (adding or removing producers and consumers of data) but \nat the cost of runtime performance, because the publish-subscribe infrastructure \nis a kind of indirection, which adds latency. In addition, if the publish-subscribe \nconnector fails completely, this is a single point of failure for the entire system.\nThe publish-subscribe pattern can take several forms: \n■\n■List-based publish-subscribe is a realization of the pattern where every \npublisher maintains a subscription list—a list of subscribers that have \nregistered an interest in receiving the event. This version of the pattern is \nless decoupled than others, as we shall see below, and hence it does not \nprovide as much modifiability, but it can be quite efficient in terms of \nruntime overhead. Also, if the components are distributed, there is no single \npoint of failure.\n■\n■Broadcast-based publish-subscribe differs from list-based publish-\nsubscribe in that publishers have less (or no) knowledge of the subscribers. \nPublishers simply publish events, which are then broadcast. Subscribers \n(or in a distributed system, services that act on behalf of the subscribers) \nexamine each event as it arrives and determine whether the published event \nis of interest. This version has the potential to be very inefficient if there \nare lots of messages and most messages are not of interest to a particular \nsubscriber.\n■\n■Content-based publish-subscribe is distinguished from the previous two \nvariants, which are broadly categorized as “topic-based.” Topics are \npredefined events, or messages, and a component subscribes to all events \nwithin the topic. Content, on the other hand, is much more general. Each \nevent is associated with a set of attributes and is delivered to a subscriber \nonly if those attributes match subscriber-defined patterns.\nIn practice the publish-subscribe pattern is typically realized by some form \nof message-oriented middleware, where the middleware is realized as a broker, \nmanaging the connections and channels of information between producers and \nconsumers. This middleware is often responsible for the transformation of mes-\nsages (or message protocols), in addition to routing and sometimes storing the \nmessages. Thus the publish-subscribe pattern inherits the strengths and weak-\nnesses of the broker pattern.\n\n\n230 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nShared-Data Pattern\nContext: Various computational components need to share and manipulate large \namounts of data. This data does not belong solely to any one of those components.\nProblem: How can systems store and manipulate persistent data that is accessed \nby multiple independent components? \nSolution: In the shared-data pattern, interaction is dominated by the exchange of \npersistent data between multiple data accessors and at least one shared-data store. \nExchange may be initiated by the accessors or the data store. The connector type is \ndata reading and writing. The general computational model associated with shared-\ndata systems is that data accessors perform operations that require data from the data \nstore and write results to one or more data stores. That data can be viewed and acted \non by other data accessors. In a pure shared-data system, data accessors interact only \nthrough one or more shared-data stores. However, in practice shared-data systems \nalso allow direct interactions between data accessors. The data-store components of \na shared-data system provide shared access to data, support data persistence, man-\nage concurrent access to data through transaction management, provide fault toler-\nance, support access control, and handle the distribution and caching of data values.\nSpecializations of the shared-data pattern differ with respect to the nature \nof the stored data—existing approaches include relational, object structures, lay-\nered, and hierarchical structures. \nAlthough the sharing of data is a critical task for most large, complex sys-\ntems, there are a number of potential problems associated with this pattern. For \none, the shared-data store may be a performance bottleneck. For this reason, \nperformance optimization has been a common theme in database research. The \nshared-data store is also potentially a single point of failure. Also, the producers \nand consumers of the shared data may be tightly coupled, through their knowl-\nedge of the structure of the shared data.\nThe shared-data pattern solution is summarized in Table 13.9.\nThe shared-data pattern is useful whenever various data items are persistent and \nhave multiple accessors. Use of this pattern has the effect of decoupling the producer \nof the data from the consumers of the data; hence, this pattern supports modifiabil-\nity, as the producers do not have direct knowledge of the consumers. Consolidating \nthe data in one or more locations and accessing it in a common fashion facilitates \nperformance tuning. Analyses associated with this pattern usually center on qualities \nsuch as data consistency, performance, security, privacy, availability, scalability, and \ncompatibility with, for example, existing repositories and their data. \nWhen a system has more than one data store, a key architecture concern is the \nmapping of data and computation to the data. Use of multiple stores may occur be-\ncause the data is naturally, or historically, partitioned into separable stores. In other \ncases data may be replicated over several stores to improve performance or availabil-\nity through redundancy. Such choices can strongly affect the qualities noted above. \nFigure 13.13 shows the diagram of a shared-data view of an enterprise access \nmanagement system. There are three types of accessor components: Windows \n\n\n13.2  Overview of the Patterns Catalog\n231\napplications, web applications, and headless programs (programs or scripts that \nrun in background and don’t provide any user interface).\nTable 13.9  Shared-Data Pattern Solution\nOverview\nCommunication between data accessors is mediated by a shared-\ndata store. Control may be initiated by the data accessors or the \ndata store. Data is made persistent by the data store.\nElements\nShared-data store. Concerns include types of data stored, data \nperformance-oriented properties, data distribution, and number of \naccessors permitted. \nData accessor component. \nData reading and writing connector. An important choice here is \nwhether the connector is transactional or not, as well as the read/\nwrite language, protocols, and semantics.\nRelations\nAttachment relation determines which data accessors are \nconnected to which data stores.\nConstraints\nData accessors interact with the data store(s). \nWeaknesses\nThe shared-data store may be a performance bottleneck.\nThe shared-data store may be a single point of failure.\nProducers and consumers of data may be tightly coupled.\nKey:\nPassword\nsynchronizer\nWindows\nAD\nMicrosoft\nExchange\nServer\nAuthentication\nApplication\nWeb\nsign-in\nWeb\napplication\nPassword\nreset\nSelf\nregistration\nHR database\nAccount\nprovisioning\ncentralized security realm\nRights\nenablement\nEntitlement\nmanagement\nDelegated\nadministration\nRequest\ntracking\nAudit and\nmonitoring\nWindows GUI\napplication\nHeadless\nprogram\nWeb\napplication\nData\nrepository\nData\nread\nData\nwrite\nData \nread & write\nFigure 13.13  The shared-data diagram of an enterprise access management \nsystem\n\n\n232 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nAllocation Patterns\nMap-Reduce Pattern\nContext: Businesses have a pressing need to quickly analyze enormous volumes \nof data they generate or access, at petabyte scale. Examples include logs of inter-\nactions in a social network site, massive document or data repositories, and pairs \nof <source, target> web links for a search engine. Programs for the analysis of \nthis data should be easy to write, run efficiently, and be resilient with respect to \nhardware failure.\nProblem: For many applications with ultra-large data sets, sorting the data and \nthen analyzing the grouped data is sufficient. The problem the map-reduce pat-\ntern solves is to efficiently perform a distributed and parallel sort of a large data \nset and provide a simple means for the programmer to specify the analysis to be \ndone. \nSolution: The map-reduce pattern requires three parts: First, a specialized infra-\nstructure takes care of allocating software to the hardware nodes in a massively \nparallel computing environment and handles sorting the data as needed. A node \nmay be a standalone processor or a core in a multi-core chip. Second and third are \ntwo programmer-coded functions called, predictably enough, map and reduce. \nThe map function takes as input a key (key1) and a data set. The purpose of \nthe map function is to filter and sort the data set. All of the heavy analysis takes \nplace in the reduce function. The input key in the map function is used to filter \nthe data. Whether a data record is to be involved in further processing is deter-\nmined by the map function. A second key (key2) is also important in the map \nfunction. This is the key that is used for sorting. The output of the map function \nconsists of a <key2, value> pair, where the key2 is the sorting value and the value \nis derived from the input record.\nSorting is performed by a combination of the map and the infrastructure. \nEach record output by map is hashed by key2 into a disk partition. The infra-\nstructure maintains an index file for key2 on the disk partition. This allows for the \nvalues on the disk partition to be retrieved in key2 order.\nThe performance of the map phase of map-reduce is enhanced by having \nmultiple map instances, each processing a different portion of the disk file being \nprocessed. Figure 13.14 shows how the map portion of map-reduce processes \ndata. An input file is divided into portions, and a number of map instances are \ncreated to process each portion. The map function processes its portion into a \nnumber of partitions, based on programmer-specified logic.\nThe reduce function is provided with all the sets of <key2, value> pairs emit-\nted by all the map instances in sorted order. Reduce does some programmer-spec-\nified analysis and then emits the results of that analysis. The output set is almost \nalways much smaller than the input sets, hence the name “reduce.” The term \n“load” is sometimes used to describe the final set of data emitted. Figure 13.14 \nalso shows one instance (of many possible instances) of the reduce processing, \n\n\n13.2  Overview of the Patterns Catalog\n233\ncalled Reduce Instance 2. Reduce Instance 2 is receiving data from all of the \nPartition 2s produced by the various map instances. It is possible that there are \nseveral iterations of reduce for large files, but this is not shown in Figure 13.14.\nA classic teaching problem for map-reduce is counting word occurrences \nin a document. This example can be carried out with a single map function. The \ndocument is the data set. The map function will find every word in the document \nand output a <word, 1> pair for each. For example, if the document begins with \nthe words “Having a whole book . . . ,” then the first results of map will be\n<Having, 1>\n<a, 1>\n<whole, 1>\n<book, 1>\nIn practice, the “a” would be one of the words filtered by map. \nPseudocode for map might look like this:\nmap(String key, String value):\n// key: document name\n// value: document contents\nfor each word w in value:\nEmit (w, “1”);\nPortion i of\ninput file\nPortion j of\ninput file\nReduce\ninstance 2\nOutput \nfrom\ninstance 2\nMap instance j\nPartition 1 Partition 2 Partition 3\nPartition 1 Partition 2 Partition 3\nComponent\nDisk file\nOutput\nKey:\nMerge\nMap instance i\nFigure 13.14  A component-and-connector view of map-reduce showing how the \ndata processed by map is partitioned and subsequently processed by reduce\n\n\n234 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThe reduce function will take that list in sorted order, add up the 1s for each \nword to get a count, and output the result. \nThe corresponding reduce function would look like this:\nreduce(List <key, value>):\n// key: a word\n// value: an integer \nint result = 0;\nsort input\nfor each input value:\nfor each input pair with same word\nresult ++ ;\nEmit (word, result)\nresult = 0\nLarger data sets lead to a much more interesting solution. Suppose we want \nto continuously analyze Twitter posts over the last hour to see what topics are \ncurrently “trending.” This is analogous to counting word occurrences in millions \nof documents. In that case, each document (tweet) can be assigned to its own in-\nstance of the map function. (If you don’t have millions of processors handy, you \ncan break the tweet collection into groups that match the number of processors \nin your processor farm, and process the collection in waves, one group after the \nother.) Or we can use a dictionary to give us a list of words, and each map func-\ntion can be assigned its own word to look for across all tweets. \nThere can also be multiple instances of reduce. These are usually arranged \nso that the reduction happens in stages, with each stage processing a smaller list \n(with a smaller number of reduce instances) than the previous stage. The final \nstage is handled by a single reduce function that produces the final output. \nOf course, the map-reduce pattern is not appropriate in all instances. Some \nconsiderations that would argue against adopting this pattern are these: \n■\n■If you do not have large data sets, then the overhead of map-reduce is not \njustified.\n■\n■If you cannot divide your data set into similar sized subsets, the advantages \nof parallelism are lost.\n■\n■If you have operations that require multiple reduces, this will be complex to \norchestrate.\nCommercial implementations of map-reduce provide infrastructure that \ntakes care of assignment of function instances to hardware, recovery and reas-\nsignment in case of hardware failure (a common occurrence in massively parallel \ncomputing environments), and utilities like sorting of the massive lists that are \nproduced along the way. \nTable 13.10 summarizes the solution of the map-reduce pattern.\nMap-reduce is a cornerstone of the software of some of the most familiar \nnames on the web, including Google, Facebook, eBay, and Yahoo!\n\n\n13.2  Overview of the Patterns Catalog\n235\nTable 13.10  Map-Reduce Pattern Solution\nOverview\nThe map-reduce pattern provides a framework for analyzing a \nlarge distributed set of data that will execute in parallel, on a set \nof processors. This parallelization allows for low latency and high \navailability. The map performs the extract and transform portions \nof the analysis and the reduce performs the loading of the results. \n(Extract-transform-load is sometimes used to describe the functions of \nthe map and reduce.)\nElements\nMap is a function with multiple instances deployed across multiple \nprocessors that performs the extract and transformation portions of \nthe analysis.\nReduce is a function that may be deployed as a single instance or as \nmultiple instances across processors to perform the load portion of \nextract-transform-load.\nThe infrastructure is the framework responsible for deploying map and \nreduce instances, shepherding the data between them, and detecting \nand recovering from failure.\nRelations\nDeploy on is the relation between an instance of a map or reduce \nfunction and the processor onto which it is installed.\nInstantiate, monitor, and control is the relation between the \ninfrastructure and the instances of map and reduce. \nConstraints\nThe data to be analyzed must exist as a set of files.\nThe map functions are stateless and do not communicate with each \nother.\nThe only communication between the map instances and the reduce \ninstances is the data emitted from the map instances as <key, value> \npairs.\nWeaknesses\nIf you do not have large data sets, the overhead of map-reduce is not \njustified.\nIf you cannot divide your data set into similar sized subsets, the \nadvantages of parallelism are lost.\nOperations that require multiple reduces are complex to orchestrate.\nMulti-tier Pattern\nThe multi-tier pattern is a C&C pattern or an allocation pattern, depending on the \ncriteria used to define the tiers. Tiers can be created to group components of similar \nfunctionality, in which case it is a C&C pattern. However, in many, if not most, \ncases tiers are defined with an eye toward the computing environment on which \nthe software will run: A client tier in an enterprise system will not be running on \nthe computer that hosts the database. That makes it an allocation pattern, mapping \nsoftware elements—perhaps produced by applying C&C patterns—to computing \nelements. Because of that reason, we have chosen to list it as an allocation pattern.\nContext: In a distributed deployment, there is often a need to distribute a sys-\ntem’s infrastructure into distinct subsets. This may be for operational or business \nreasons (for example, different parts of the infrastructure may belong to different \norganizations).\n\n\n236 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nProblem: How can we split the system into a number of computationally inde-\npendent execution structures—groups of software and hardware—connected by \nsome communications media? This is done to provide specific server environ-\nments optimized for operational requirements and resource usage. \nSolution: The execution structures of many systems are organized as a set of \nlogical groupings of components. Each grouping is termed a tier. The grouping \nof components into tiers may be based on a variety of criteria, such as the type of \ncomponent, sharing the same execution environment, or having the same runtime \npurpose.\nThe use of tiers may be applied to any collection (or pattern) of runtime \ncomponents, although in practice it is most often used in the context of cli-\nent-server patterns. Tiers induce topological constraints that restrict which com-\nponents may communicate with other components. Specifically, connectors may \nexist only between components in the same tier or residing in adjacent tiers. The \nmulti-tier pattern found in many Java EE and Microsoft .NET applications is an \nexample of organization in tiers derived from the client-server pattern. \nAdditionally, tiers may constrain the kinds of communication that can take \nplace across adjacent tiers. For example, some tiered patterns require call-return \ncommunication in one direction but event-based notification in the other.\nThe main weakness with the multi-tier architecture is its cost and complex-\nity. For simple systems, the benefits of the multi-tier architecture may not justify \nits up-front and ongoing costs, in terms of hardware, software, and design and \nimplementation complexity.\nTiers are not components, but rather logical groupings of components. Also, \ndon’t confuse tiers with layers! Layering is a pattern of modules (a unit of imple-\nmentation), while tiers applies only to runtime entities. \nTable 13.11 summarizes the solution part of the multi-tier pattern.\nTiers make it easier to ensure security, and to optimize performance and \navailability in specialized ways. They also enhance the modifiability of the sys-\ntem, as the computationally independent subgroups need to agree on protocols \nfor interaction, thus reducing their coupling.\nFigure 13.15 uses an informal notation to describe the multi-tier architecture \nof the Consumer Website Java EE application. This application is part of the Ad-\nventure Builder system. Many component-and-connector types are specific to the \nsupporting platform, which is Java EE in this case.\n",
      "page_number": 247
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 257-267)",
      "start_page": 257,
      "end_page": 267,
      "detection_method": "topic_boundary",
      "content": "13.2  Overview of the Patterns Catalog\n237\nTable 13.11  Multi-tier Pattern Solution\nOverview\nThe execution structures of many systems are organized as a \nset of logical groupings of components. Each grouping is termed \na tier. The grouping of components into tiers may be based on a \nvariety of criteria, such as the type of component, sharing the same \nexecution environment, or having the same runtime purpose.\nElements\nTier, which is a logical grouping of software components. \nTiers may be formed on the basis of common computing platforms, \nin which case those platforms are also elements of the pattern.\nRelations\nIs part of, to group components into tiers.\nCommunicates with, to show how tiers and the components they \ncontain interact with each other. \nAllocated to, in the case that tiers map to computing platforms. \nConstraints\nA software component belongs to exactly one tier.\nWeaknesses\nSubstantial up-front cost and complexity.\nKey\nWeb\nbrowser\nSignOnFilter\n*.do\n*.screen\nMain\nServlet\nTemplate\nServlet\nScreen\nJSP\nindex.jsp\nSign On\nNotifier\nmappings.xml\nscreen\ndefinitions.xml\nsign-on-\nconfig.xml\nOrder\nFacade\nEJB tier\nBack end\nWeb tier\nClient tier\nCatalog\nFacade\nOPC\nAdventure\nCatalog\nDB\nUser\nMgmt\nFacade\nOpcOrder\nTrackingService\nOpcPurchase\nOrderService\nClient-side\napplication\nJava\nEE\nfilter\nStateless\nsession\nbean\nJava EE\napplication\nContext\nlistener\nData\nstore\nFile\nServlet\nContainer\nWeb services\nendpoint\nSOAP\ncall\nFile\nI/O\nJava\ncall\nHTTP/\nHTTPS\nJDBC\nFigure 13.15  A multi-tier view of the Consumer Website Java EE application, \nwhich is part of the Adventure Builder system\n\n\n238 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nOther Allocation Patterns.  There are several published deployment styles. \nMicrosoft publishes a “Tiered Distribution” pattern, which prescribes a particular \nallocation of components in a multi-tier architecture to the hardware they will run \non. Similarly, IBM’s WebSphere handbooks describe a number of what they call \n“topologies” along with the quality attribute criteria for choosing among them. \nThere are 11 topologies (specialized deployment patterns) described for Web-\nSphere version 6, including the “single machine topology (stand-alone server),” \n“reverse proxy topology,” “vertical scaling topology,” “horizontal scaling topol-\nogy,” and “horizontal scaling with IP sprayer topology.”\nThere are also published work assignment patterns. These take the form of \noften-used team structures. For example, patterns for globally distributed Agile \nprojects include these:\n■\n■Platform. In software product line development, one site is tasked with \ndeveloping reusable core assets of the product line, and other sites develop \napplications that use the core assets.\n■\n■Competence center. Work is allocated to sites depending on the technical \nor domain expertise located at a site. For example, user interface design is \ndone at a site where usability engineering experts are located.\n■\n■Open source. Many independent contributors develop the software product \nin accordance with a technical integration strategy. Centralized control is \nminimal, except when an independent contributor integrates his code into \nthe product line.\n13.3 Relationships between Tactics and Patterns \nPatterns and tactics together constitute the software architect’s primary tools of \nthe trade. How do they relate to each other?\nPatterns Comprise Tactics\nAs we said in the introduction to this chapter, tactics are the “building blocks” \nof design from which architectural patterns are created. Tactics are atoms and \npatterns are molecules. Most patterns consist of (are constructed from) several \ndifferent tactics, and although these tactics might all serve a common purpose—\nsuch as promoting modifiability, for example—they are often chosen to promote \ndifferent quality attributes. For example, a tactic might be chosen that makes an \navailability pattern more secure, or that mitigates the performance impact of a \nmodifiability pattern.\nConsider the example of the layered pattern, the most common pattern in all \nof software architecture (virtually all nontrivial systems employ layering). The \n\n\n13.3 Relationships between Tactics and Patterns \n239\nlayered pattern can be seen as the amalgam of several tactics—increase semantic \ncoherence, abstract common services, encapsulate, restrict communication paths, \nand use an intermediary. For example:\n■\n■Increase semantic coherence. The goal of ensuring that a layer’s respon-\nsibilities all work together without excessive reliance on other layers \nis achieved by choosing responsibilities that have semantic coherence. \nDoing so binds responsibilities that are likely to be affected by a change. \nFor example, responsibilities that deal with hardware should be allocated \nto a hardware layer and not to an application layer; a hardware respon-\nsibility typically does not have semantic coherence with the application \nresponsibilities.\n■\n■Restrict dependencies. Layers define an ordering and only allow a layer to \nuse the services of its adjacent lower layer. The possible communication \npaths are reduced to the number of layers minus one. This limitation has a \ngreat influence on the dependencies between the layers and makes it much \neasier to limit the side effects of replacing a layer. \nWithout any one of its tactics, the pattern might be ineffective. For example, \nif the restrict dependencies tactic is not employed, then any function in any layer \ncan call any other function in any other layer, destroying the low coupling that \nmakes the layering pattern effective. If the increase semantic coherence tactic \nis not employed, then functionality could be randomly sprinkled throughout the \nlayers, destroying the separation of concerns, and hence ease of modification, \nwhich is the prime motivation for employing layers in the first place.\nTable 13.12 shows a number of the architectural patterns described in the \nbook Pattern-Oriented Software Architecture Volume 1: A System of Patterns, by \nBuschmann et al., and shows which modifiability tactics they employ.\nUsing Tactics to Augment Patterns\nA pattern is described as a solution to a class of problems in a general context. \nWhen a pattern is chosen and applied, the context of its application becomes very \nspecific. A documented pattern is therefore underspecified with respect to apply-\ning it in a specific situation.\nTo make a pattern work in a given architectural context, we need to examine \nit from two perspectives:\n■\n■The inherent quality attribute tradeoffs that the pattern makes. Patterns exist \nto achieve certain quality attributes, and we need to compare the ones they \npromote (and the ones they diminish) with our needs.\n■\n■Other quality attributes that the pattern isn’t directly concerned with, but \nwhich it nevertheless affects, and which are important in our application.\n\n\n240 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.12  Architecture Patterns and Corresponding Tactics ([Bachmann 07])\nPattern\nModifiability\nIncrease \nCohesion\nReduce Coupling\nDefer Binding \nTime\nIncrease Semantic \nCoherence\nAbstract Common \nServices\nEncapsulate\nUse a Wrapper\nRestrict Comm. \nPaths\nUse an \nIntermediary\nRaise the \nAbstraction Level\nUse Runtime \nRegistration\nUse Startup-Time \nBinding\nUse Runtime \nBinding\nLayered\nX\nX \nX \nX\nX \nX \nPipes and Filters\nX\nX\nX\nX\nX\nBlackboard\nX\nX\nX\nX\nX\nX\nX\nBroker\nX\nX\nX\nX\nX\nX\nX\nModel View \nController\nX\nX\nX\nX\nPresentation \nAbstraction Control\nX\nX\nX\nX\nMicrokernel\nX\nX\nX\nX\nX\nReflection\nX\nX\nTo illustrate these concerns in particular, and how to use tactics to augment \npatterns in general, we’ll use the broker pattern as a starting point. \nThe broker pattern is widely used in distributed systems and dates back at \nleast to its critical role in CORBA-based systems. Broker is a crucial component \nof any large-scale, dynamic, service-oriented architecture. \nUsing this pattern, a client requesting some information from a server does \nnot need to know the location or APIs of the server. The client simply contacts \nthe broker (typically through a client-side proxy); this is illustrated in the UML \nsequence diagram in Figure 13.16.\nWeaknesses of the Broker Pattern.  In Section 13.2 we enumerated sev-\neral weaknesses of the broker pattern. Here we will examine these weaknesses \nin more detail. The broker pattern has several weaknesses with respect to certain \nquality attributes. For example: \n■\n■Availability. The broker, if implemented as suggested in Figure 13.6, is a \nsingle point of failure. The liveness of servers, the broker, and perhaps even \nthe clients need to be monitored, and repair mechanisms must be provided.\n\n\n13.3 Relationships between Tactics and Patterns \n241\n:Client\n:ClientProxy\n:ServerProxy\n:Server\nprocess boundary\nregisterServer()\nmarshallRequest()\nunmarshallRequest()\nclientID\nserverID\nOK\nresultA\nresultA\nsendRequest()\nmarshallResponse()\nunmarshallResponse()\nsendResponse()\nperformFunctionA()\nperformFunctionA()\nlocateServer()\nlocateClient()\nprocess boundary\n:Broker\nFigure 13.16  A sequence diagram showing a typical client-server interaction \nmediated by a broker\n■\n■Performance. The levels of indirection between the client (requesting \nthe information or service) and the server (providing the information or \nservice) add overhead, and hence add latency. Also, the broker is a potential \nperformance bottleneck if direct communication between the client and \nserver is not desired (for example, for security reasons).\n■\n■Testability. Brokers are employed in complex multi-process and multi-\nprocessor systems. Such systems are typically highly dynamic. Requests \nand responses are typically asynchronous. All of this makes testing and \ndebugging such systems extremely difficult. But the description of the \nbroker pattern provides no testing functionality, such as testing interfaces, \nstate or activity capture and playback capabilities, and so forth.\n\n\n242 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\n■\n■Security. Because the broker pattern is primarily used when the system \nspans process and processor boundaries—such as on web-based systems—\nsecurity is a legitimate concern. However, the broker pattern as presented \ndoes not offer any means to authenticate or authorize clients or servers, and \nprovides no means of protecting the communication between clients and \nservers.\nOf these quality attributes, the broker pattern is mainly associated with \npoor performance (the well-documented price for the loose coupling it brings to \nsystems). It is largely unconcerned with the other quality attributes in this list; \nthey aren’t mentioned in most published descriptions. But as the other bullets \nshow, they can be unacceptable “collateral damage” that come with the broker’s \nbenefits.\nImproving the Broker Pattern with Tactics.  How can we use tactics to \nplug the gaps between the “out of the box” broker pattern and a version of it that \nwill let us meet the requirements of a demanding distributed system? Here are \nsome options:\n■\n■The increase available resources performance tactic would lead to multiple \nbrokers, to help with performance and availability. \n■\n■The maintain multiple copies tactic would allow each of these brokers to \nshare state, to ensure that they respond identically to client requests.\n■\n■Load balancing (an application of the scheduling resources tactic) would \nensure that one broker is not overloaded while another one sits idle.\n■\n■Heartbeat, exception detection, or ping/echo would give the replicated \nbrokers a way of notifying clients and notifying each other when one of \nthem is out of service, as a means of detecting faults. \nOf course, each of these tactics brings a tradeoff. Each complicates the de-\nsign, which will now take longer to implement, be more costly to acquire, and \nbe more costly to maintain. Load balancing introduces indirection that will add \nlatency to each transaction, thus giving back some of the performance it was in-\ntended to increase. And the load balancer is a single point of failure, so it too \nmust be replicated, further increasing the design cost and complexity.\n13.4  Using Tactics Together \nTactics, as described in Chapters 5–11, are design primitives aimed at managing \na single quality attribute response. Of course, this is almost never true in prac-\ntice; every tactic has its main effect—to manage modifiability or performance \nor safety, and so on—and it has its side effects, its tradeoffs. On the face of it, \nthe situation for an architect sounds hopeless. Whatever you do to improve one \n\n\n13.4  Using Tactics Together \n243\nquality attribute endangers another. We are able to use tactics profitably because \nwe can gauge the direct and side effects of a tactic, and when the tradeoff is ac-\nceptable, we employ the tactic. In doing so we gain some benefit in our quality \nattribute of interest while giving up something else (with respect to a different \nquality attribute and, we hope, of a much smaller magnitude).\nThis section will walk through an example that shows how applying tactics \nto a pattern can produce negative effects in one area, but how adding other tactics \ncan bring relief and put you back in an acceptable design space. The point is to \nshow the interplay between tactics that you can use to your advantage. Just as \nsome combinations of liquids are noxious whereas others yield lovely things like \nstrawberry lemonade, tactics can either make things worse or put you in a happy \ndesign space. Here, then, is a walkthrough of tactic mixology.\nConsider a system that needs to detect faults in its components. A common \ntactic for detecting faults is ping/echo. Let us assume that the architect has de-\ncided to employ ping/echo as a way to detect failed components in the system. \nEvery tactic has one or more side effects, and ping/echo is no different. Common \nconsiderations associated with ping/echo are these:\n■\n■Security. How to prevent a ping flood attack?\n■\n■Performance. How to ensure that the performance overhead of ping/echo is \nsmall?\n■\n■Modifiability. How to add ping/echo to the existing architecture?\nWe can represent the architect’s reasoning and decisions thus far as shown \nin Figure 13.17.\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nFigure 13.17  Partial availability decisions\n\n\n244 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nSuppose the architect determines that the performance tradeoff (the overhead \nof adding ping/echo to the system) is the most severe. A tactic to address the \nperformance side effect is increase available resources. Considerations associated \nwith increase available resources are these:\n■\n■Cost. Increased resources cost more.\n■\n■Performance. How to utilize the increased resources efficiently?\nThis set of design decisions can now be represented as shown in Figure 13.18.\nNow the architect chooses to deal with the resource utilization consequence \nof employing increase available resources. These resources must be used efficiently \nor else they are simply adding cost and complexity to the system. A tactic that can \naddress the efficient use of resources is the employment of a scheduling policy. Con-\nsiderations associated with the scheduling policy tactic are these:\n■\n■Modifiability. How to add the scheduling policy to the existing architecture?\n■\n■Modifiability. How to change the scheduling policy in the future?\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease Available\nResources\nFigure 13.18  More availability decisions\n\n\n13.4  Using Tactics Together \n245\nThe set of design decisions that includes the scheduling policy tactic can \nnow be represented as in Figure 13.19.\nNext the architect chooses to deal with the modifiability consequence of \nemploying a scheduling policy tactic. A tactic to address the addition of the \nscheduler to the system is to use an intermediary, which will insulate the choice \nof scheduling policy from the rest of the system. One consideration associated \nwith use an intermediary is this:\n■\n■Modifiability. How to ensure that all communication passes through the \nintermediary?\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease Available\nResources\nAdd to \nsystem\nModify\npolicy\nScheduling\nPolicy\nFigure 13.19  Still more availability decisions\n\n\n246 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nWe can now represent the tactics-based set of architectural design decisions \nmade thus far as in Figure 13.20.\nA tactic to address the concern that all communication passes through the \nintermediary is restrict dependencies. One consideration associated with the \nrestrict dependencies tactic is this:\n■\n■Performance. How to ensure that the performance overhead of the \nintermediary is not excessive?\nThis design problem has now become recursive! At this point (or in fact, \nat any point in the tree of design decisions that we have described) the architect \nmight determine that the performance overhead of the intermediary is small \nenough that no further design decisions need to be made.\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease available\nResources\nAdd to \nsystem\nModify\npolicy\nScheduling\nPolicy\nEnsure usage\nUse an intermediary\nFigure 13.20  As far as we go with availability decisions\n\n\n13.5  Summary\n247\nApplying successive tactics is like moving through a game space, and it’s a \nlittle like chess: Good players are able to see the consequences of the move they’re \nconsidering, and the very good players are able to look several moves ahead. In \nChapter 17 we’ll see the activity of design treated as an exercise of “generate and \ntest”: propose a design and test it to see if it’s satisfactory. Applying tactics to \nan existing design solution, such as a pattern, is one technique for generating a \ndesign for subsequent testing.\n13.5  Summary\nAn architectural pattern\n■\n■is a package of design decisions that is found repeatedly in practice,\n■\n■has known properties that permit reuse, and \n■\n■describes a class of architectures. \nBecause patterns are (by definition) found repeatedly in practice, one does \nnot invent them; one discovers them. \nTactics are simpler than patterns. Tactics typically use just a single structure \nor computational mechanism, and they are meant to address a single architectural \nforce. For this reason they give more precise control to an architect when \nmaking design decisions than patterns, which typically combine multiple design \ndecisions into a package. Tactics are the “building blocks” of design from which \narchitectural patterns are created. Tactics are atoms and patterns are molecules. \nAn architectural pattern establishes a relationship between:\n■\n■A context. A recurring, common situation in the world that gives rise to a \nproblem.\n■\n■A problem. The problem, appropriately generalized, that arises in the given \ncontext. \n■\n■A solution. A successful architectural resolution to the problem, \nappropriately abstracted. \nComplex systems exhibit multiple patterns at once. \nPatterns can be categorized by the dominant type of elements that they show: \nmodule patterns show modules, component-and-connector patterns show compo-\nnents and connectors, and allocation patterns show a combination of software \nelements (modules, components, connectors) and nonsoftware elements. Most \npublished patterns are C&C patterns, but there are module patterns and allocation \npatterns as well. This chapter showed examples of each type.\nA pattern is described as a solution to a class of problems in a general con-\ntext. When a pattern is chosen and applied, the context of its application becomes \nvery specific. A documented pattern is therefore underspecified with respect to \n",
      "page_number": 257
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 268-275)",
      "start_page": 268,
      "end_page": 275,
      "detection_method": "topic_boundary",
      "content": "248 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\napplying it in a specific situation. We can make a pattern more specific to our \nproblem by augmenting it with tactics. Applying successive tactics is like mov-\ning through a game space, and is a little like chess: the consequences of the next \nmove are important, and looking several moves ahead is helpful.\n13.6  For Further Reading\nThere are many existing repositories of patterns and books written about patterns. \nThe original and most well-known work on object-oriented design patterns is by \nthe “Gang of Four” [Gamma 94].\nThe Gang of Four’s discussion of patterns included patterns at many levels \nof abstraction. In this chapter we have focused entirely on architectural patterns. \nThe patterns that we have presented here are intended as representative examples. \nThis chapter’s inventory of patterns is in no way meant to be exhaustive. For \nexample, while we describe the SOA pattern, entire repositories of SOA patterns \n(refinements of the basic SOA pattern) have been created. A good place to start is \nwww.soapatterns.org.\nSome good references for pattern-oriented architecture are [Buschmann 96], \n[Hanmer 07], [Schmidt 00], and [Kircher 03].\nA good place to learn more about the map-reduce pattern is Google’s foun-\ndational paper on it [Dean 04].\nMap-reduce is the tip of the spear of the so-called “NoSQL” movement, \nwhich seeks to displace the relational database from its venerable and taken-for-\ngranted status in large data-processing systems. The movement has some of the \nrevolutionary flavor of the Agile movement, except that NoSQL advocates are \nclaiming a better (for them) technology, as opposed to a better process. You can \neasily find NoSQL podcasts, user forums, conferences, and blogs; it’s also dis-\ncussed in Chapter 26.\n[Bachmann 07] discusses the use of tactics in the layered pattern and is the \nsource for some of our discussion of that.\nThe passage in this chapter about augmenting ping/echo with other tactics \nto achieve the desired combination of quality attributes is based on the work of \nKiran Kumar and TV Prabhakar [Kumar 10a] and [Kumar 10b]. \n[Urdangarin 08] is the source of the work assignment patterns described in \nSection 13.2.\nThe Adventure Builder system shown in Figures 13.11 and 13.15 comes \nfrom [AdvBuilder 10].\n\n\n13.7  Discussion Questions\n249\n13.7  Discussion Questions\n1.\t\nWhat’s the difference between an architectural pattern, such as those de-\nscribed in this chapter and in the Pattern-Oriented Software Architecture \nseries of books, and design patterns, such as those collected by the Gang of \nFour in 1994 and many other people subsequently? Given a pattern, how \nwould you decide whether it was an architectural pattern, a design pattern, \na code pattern, or something else?\n2.\t\nSOA systems feature dynamic service registration and discovery. Which \nquality attributes does this capability enhance and which does it threaten? \nIf you had to make a recommendation to your boss about whether your \ncompany’s SOA system should use external services it discovers at runtime, \nwhat would you say?\n3.\t\nWrite a complete pattern description for the “competence center” work as-\nsignment pattern mentioned in Section 13.2.\n4.\t\nFor a data set that is a set of web pages, sketch a map function and a reduce \nfunction that together provide a basic search engine capability. \n5.\t\nDescribe how the layered pattern makes use of these tactics: abstract com-\nmon services, encapsulate, and use an intermediary.\n\n\nThis page intentionally left blank \n\n\n251\n14\nQuality Attribute \nModeling and Analysis\nDo not believe in anything simply because you have \nheard it . . . Do not believe in anything merely on \nthe authority of your teachers and elders. Do not \nbelieve in traditions because they have been handed \ndown for many generations. But after observation \nand analysis, when you find that anything agrees \nwith reason and is conducive to the good and benefit \nof one and all, then accept it and live up to it.\n—Prince Gautama Siddhartha\nIn Chapter  2 we listed thirteen reasons why architecture is important, worth \nstudying, and worth practicing. Reason 6 is that the analysis of an architecture \nenables early prediction of a system’s qualities. This is an extraordinarily pow-\nerful reason! Without it, we would be reduced to building systems by choosing \nvarious structures, implementing the system, measuring the system for its quality \nattribute responses, and all along the way hoping for the best. Architecture lets \nus do better than that, much better. We can analyze an architecture to see how \nthe system or systems we build from it will perform with respect to their quality \nattribute goals, even before a single line of code has been written. This chapter \nwill explore how.\nThe methods available depend, to a large extent, on the quality attribute to \nbe analyzed. Some quality attributes, especially performance and availability, \nhave well-understood and strongly validated analytic modeling techniques. Other \nquality attributes, for example security, can be analyzed through checklists. Still \nothers can be analyzed through back-of-the-envelope calculations and thought \nexperiments. \nOur topics in this chapter range from the specific, such as creating models \nand analyzing checklists, to the general, such as how to generate and carry out the \nthought experiments to perform early (and necessarily crude) analysis. Models \n\n\n252 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nand checklists are focused on particular quality attributes but can aid in the anal-\nysis of any system with respect to those attributes. Thought experiments, on the \nother hand, can consider multiple quality attributes simultaneously but are only \napplicable to the specific system under consideration.\n14.1  \u0007Modeling Architectures to Enable \nQuality Attribute Analysis\nSome quality attributes, most notably performance and availability, have well-un-\nderstood, time-tested analytic models that can be used to assist in an analysis. \nBy analytic model, we mean one that supports quantitative analysis. Let us first \nconsider performance.\nAnalyzing Performance\nIn Chapter 12 we discussed the fact that models have parameters, which are val-\nues you can set to predict values about the entity being modeled (and in Chap-\nter 12 we showed how to use the parameters to help us derive tactics for the \nquality attribute associated with the model). As an example we showed a queuing \nmodel for performance as Figure 12.2, repeated here as Figure 14.1. The parame-\nters of this model are the following: \n■\n■The arrival rate of events\n■\n■The chosen queuing discipline\n■\n■The chosen scheduling algorithm\n■\n■The service time for events\n■\n■The network topology \n■\n■The network bandwidth\n■\n■The routing algorithm chosen\nIn this section, we discuss how such a model can be used to understand the \nlatency characteristics of an architectural design. \nTo apply this model in an analytical fashion, we also need to have previ-\nously made some architecture design decisions. We will use model-view-control-\nler as our example here. MVC, as presented in Section 13.2, says nothing about \nits deployment. That is, there is no specification of how the model, the view, and \nthe controller are assigned to processes and processors; that’s not part of the pat-\ntern’s concern. These and other design decisions have to be made to transform \na pattern into an architecture. Until that happens, one cannot say anything with \nauthority about how an MVC-based implementation will perform. For this exam-\nple we will assume that there is one instance each of the model, the view, and the \ncontroller, and that each instance is allocated to a separate processor. Figure 14.2 \nshows MVC following this allocation scheme.\n\n\n14.1  Modeling Architectures to Enable Quality Attribute Analysis\n253\nResults\nRouting of \nmessages\nArrivals\nQueue\nServer\nScheduling \nalgorithm\nFigure 14.1  A queuing model of performance\nInternet\nintranet\n<<deploy>>\n<<deploy>>\n<<deploy>>\nDatabase \nhost\n<<component>>\nModel\nUser’s \nmachine\n<<component>>\nView\nApp\nserver\n<<component>>\nController\nKey: UML 2.0\nFigure 14.2  An allocation view, in UML, of a model-view-controller architecture\nGiven that quality attribute models such as the performance model shown \nin Figure 14.1 already exist, the problem becomes how to map these allocation \nand coordination decisions onto Figure 14.1. Doing this yields Figure 14.3. \nThere are requests coming from users outside the system—labeled as 1 in Fig-\nure 14.3—arriving at the view. The view processes the requests and sends some \n\n\n254 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\ntransformation of the requests on to the controller—labeled as 2. Some actions of \nthe controller are returned to the view—labeled as 3. The controller sends other \nactions on to the model—labeled 4. The model performs its activities and sends \ninformation back to the view—labeled 5.\nTo analyze the model in Figure 14.3, a number of items need to be known \nor estimated: \n■\n■The frequency of arrivals from outside the system\n■\n■The queuing discipline used at the view queue\n■\n■The time to process a message within the view\n■\n■The number and size of messages that the view sends to the controller\n■\n■The bandwidth of the network that connects the view and the controller\n■\n■The queuing discipline used by the controller\n■\n■The time to process a message within the controller\n■\n■The number and size of messages that the controller sends back to the view\n■\n■The bandwidth of the network used for messages from the controller to the \nview\n■\n■The number and size of messages that the controller sends to the model\n■\n■The queuing discipline used by the model\n■\n■The time to process a message within the model\n■\n■The number and size of messages the model sends to the view\n■\n■The bandwidth of the network connecting the model and the view\nUsers\ngenerate \nrequests\n1\n2\n3\n4\n5\nController\nModel\nView\nFigure 14.3  A queuing model of performance for MVC\n\n\n14.1  Modeling Architectures to Enable Quality Attribute Analysis\n255\nGiven all of these assumptions, the latency for the system can be estimated. \nSometimes well-known formulas from queuing theory apply. For situations where \nthere are no closed-form solutions, estimates can often be obtained through sim-\nulation. Simulations can be used to make more-realistic assumptions such as the \ndistribution of the event arrivals. The estimates are only as good as the assump-\ntions, but they can serve to provide rough values that can be used either in design \nor in evaluation; as better information is obtained, the estimates will improve.\nA reasonably large number of parameters must be known or estimated to \nconstruct the queuing model shown in Figure 14.3. The model must then be \nsolved or simulated to derive the expected latency. This is the cost side of the \ncost/benefit of performing a queuing analysis. The benefit side is that as a result \nof the analysis, there is an estimate for latency, and “what if” questions can be \neasily answered. The question for you to decide is whether having an estimate of \nthe latency and the ability to answer “what if” questions is worth the cost of per-\nforming the analysis. One way to answer this question is to consider the impor-\ntance of having an estimate for the latency prior to constructing either the system \nor a prototype that simulates an architecture under an assumed load. If having a \nsmall latency is a crucial requirement upon which the success of the system re-\nlies, then producing an estimate is appropriate. \nPerformance is a well-studied quality attribute with roots that extend beyond \nthe computer industry. For example, the queuing model given in Figure 14.1 \ndates from the 1930s. Queuing theory has been applied to factory floors, to bank-\ning queues, and to many other domains. Models for real-time performance, such \nas rate monotonic analysis, also exist and have sophisticated analysis techniques. \nAnalyzing Availability\nAnother quality attribute with a well-understood analytic framework is availability. \nModeling an architecture for availability—or to put it more carefully, mod-\neling an architecture to determine the availability of a system based on that archi-\ntecture—is a matter of determining the failure rate and the recovery time. As you \nmay recall from Chapter 5, availability can be expressed as\nMTBF\n(MTBF + MTTR)\nThis models what is known as steady-state availability, and it is used to \nindicate the uptime of a system (or component of a system) over a sufficiently \nlong duration. In the equation, MTBF is the mean time between failure, which is \nderived based on the expected value of the implementation’s failure probability \ndensity function (PDF), and MTTR refers to the mean time to repair. \nJust as for performance, to model an architecture for availability, we need \nan architecture to analyze. So, suppose we want to increase the availability of a \nsystem that uses the broker pattern, by applying redundancy tactics. Figure 14.4 \n",
      "page_number": 268
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 276-283)",
      "start_page": 276,
      "end_page": 283,
      "detection_method": "topic_boundary",
      "content": "256 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nillustrates three well-known redundancy tactics from Chapter  5: active redun-\ndancy, passive redundancy, and cold spare. Our goal is to analyze each redun-\ndancy option for its availability, to help us choose one.\nAs you recall, each of these tactics introduces a backup copy of a compo-\nnent that will take over in case the primary component suffers a failure. In our \ncase, a broker replica is employed as the redundant spare. The difference among \nthem is how up to date with current events each backup keeps itself:\n■\n■In the case of active redundancy, the active and redundant brokers both \nreceive identical copies of the messages received from the client and server \nproxies. The internal broker state is synchronously maintained between the \nactive and redundant spare in order to facilitate rapid failover upon detec-\ntion of a fault in the active broker. \n■\n■For the passive redundancy implementation, only the active broker receives \nand processes messages from the client and server proxies. When using this \ntactic, checkpoints of internal broker state are periodically transmitted from \nthe active broker process to the redundant spare, using the checkpoint-based \nrollback tactic. \n■\n■Finally, when using the cold spare tactic, only the active broker receives \nand processes messages from the client and server proxies, because the \nredundant spare is in a dormant or even powered-off state. Recovery strate-\ngies using this tactic involve powering up, booting, and loading the broker \nimplementation on the spare. In this scenario, the internal broker state is \nrebuilt organically, rather than via synchronous operation or checkpointing, \nas described for the other two redundancy tactics.\nSuppose further that we will detect failure with the heartbeat tactic, where \neach broker (active and spare) periodically transmits a heartbeat message to a \nseparate process responsible for fault detection, correlation, reporting, and recov-\nery. This fault manager process is responsible for coordinating the transition of \nthe active broker role from the failed broker process to the redundant spare. \nYou can now use the steady state model of availability to assign values for \nMTBF and MTTR for each of the three redundancy tactics we are considering. \nDoing so will be an exercise left to the reader (as you’ll see when you reach the \ndiscussion questions for this chapter). Because the three tactics differ primarily in \nhow long it takes to bring the backup copy up to speed, MTTR will be where the \ndifference among the tactics shows up.\nMore sophisticated models of availability exist, based on probability. In \nthese models, we can express a probability of failure during a period of time. \nGiven a particular MTBF and a time duration T, the probability of failure R is \ngiven by\nR(T ) = e(    )\n–T\nMTBF\n\n\n14.1  Modeling Architectures to Enable Quality Attribute Analysis\n257\nBrokerACTIVE\nBrokerSPARE\n(Cold) Spare\nClient-Server\nProxy Traffic\nBrokerACTIVE\nBrokerSPARE\nPassive \nRedundancy\nClient-Server\nProxy Traffic\nPeriodic\nCheckpoint Data\nKey:\nmessage\nprocess\nBrokerACTIVE\nBrokerSPARE\nActive\nRedundancy\nClient-Server\nProxy Traffic\nState\nSynchronization\nFigure 14.4  Redundancy tactics, as applied to a broker pattern\nYou will recall from Statistics 101 that:\n■\n■When two events A and B are independent, the probability that A or B will \noccur is the sum of the probability of each event: P(A or B) = P(A) \n+ P(B).\n■\n■When two events A and B are independent, the probability of both occur-\nring is P(A and B) = P(A) • P(B).\n■\n■When two events A and B are dependent, the probability of both occurring \nis P(A and B) = P(A) • P(B|A), where the last term means “the \nprobability of B occurring, given that A occurs.”\n\n\n258 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nWe can apply simple probability arithmetic to an architecture pattern for \navailability to determine the probability of failure of the pattern given the proba-\nbility of failure of the individual components (and an understanding of their de-\npendency relations). For example, in an architecture pattern employing the pas-\nsive redundancy tactic, let’s assume that the failure of a component (which at any \nmoment might be acting as either the primary or backup copy) is independent of \na failure of its counterpart, and that the probability of failure of either is the same. \nThen the probability that both will fail is F = (1 – a) **2, where a is the \navailability of an individual component (assuming that failures are independent). \nStill other models take into account different levels of failure severity and \ndegraded operating states of the system. Although the derivation of these for-\nmulas is outside the scope of this chapter, you end up with formulas that look \nlike the following for the three redundancy tactics we’ve been discussing, where \nthe values C2 through C5 are references to the MTBF column of Table 14.1, D2 \nthrough D4 refer to the Active column, E2 through E3 refer to the Passive col-\numn, and F2 through F3 refer to the Spare column.\n■\n■Active redundancy:\n■\n■Availability(MTTR): 1 –((SUM(C2:C5) + D3) × D2)/((C2 × (C2 + C4 + \nD3) + ((C2 + C4 + D2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))\n■\n■P(Degraded) = ((C3 + C5) × D2)/((C2 × (C2 + C4 + D3) + ((C2 + C4 + \nD2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))\n■\n■Passive redundancy:\n■\n■Availability(MTTR_passive) = 1 – ((SUM(C2:C5) + E3) × E2)/((C2 × \n(C2 + C4 + E3) + ((C2 + C4 + E2) × (C3 + C5)) + ((C2 + C4) × (C2 + \nC4 + E3))))\n■\n■P(Degraded) = ((C3 + C5) × E2)/((C2 × (C2 + C4 + E3) + ((C2 + C4 + \nE2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + E3))))\n■\n■Spare:\n■\n■Availability(MTTR) = 1 – ((SUM(C2:C5) + F3) × F2)/((C2 × (C2 + C4 + \nF3) + ((C2 + C4 + F2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3))))\n■\n■P(Degraded) = ((C3 + C5) × F2)/((C2 × (C2 + C4 + F3) + ((C2 + C4 + \nF2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3)))) \nPlugging in these values for the parameters to the equations listed above \nresults in a table like Table 14.1, which can be easily calculated using any spread-\nsheet tool. Such a calculation can help in the selection of tactics.\n\n\n14.1  Modeling Architectures to Enable Quality Attribute Analysis\n259\n TABLE 14.1  Calculated Availability for an Availability-Enhanced Broker \nImplementation\nFunction\nFailure \nSeverity\nMTBF \n(Hours)\nMTTR (Seconds)\nActive \nRedundancy\n(Hot Spare)\nPassive \nRedundancy\n(Warm Spare)\nSpare\n(Cold Spare)\nHardware\n1\n250,000\n1\n5\n900\n2\n50,000\n30\n30\n30\nSoftware\n1\n50,000\n1\n5\n900\n2\n10,000\n30\n30\n30\nAvailability\n0.9999998\n0.999990\n0.9994\nThe Analytic Model Space\nAs we discussed in the preceding sections, there are a growing number of analytic \nmodels for some aspects of various quality attributes. One of the quests of software \nengineering is to have a sufficient number of analytic models for a sufficiently large \nnumber of quality attributes to enable prediction of the behavior of a designed sys-\ntem based on these analytic models. Table 14.2 shows our current status with respect \nto this quest for the seven quality attributes discussed in Chapters 5–11.\nTABLE 14.2  A Summary of the Analytic Model Space\nQuality \nAttribute\nIntellectual Basis\nMaturity/Gaps\nAvailability\nMarkov models; \nstatistical models\nModerate maturity; mature in the \nhardware reliability domain, less mature \nin the software domain. Requires models \nthat speak to state recovery and for which \nfailure percentages can be attributed to \nsoftware.\nInteroperability\nConceptual framework\nLow maturity; models require substantial \nhuman interpretation and input.\nModifiability\nCoupling and cohesion \nmetrics; cost models\nSubstantial research in academia; still \nrequires more empirical support in real-\nworld environments.\nPerformance\nQueuing theory; real-\ntime scheduling theory\nHigh maturity; requires considerable \neducation and training to use properly.\nSecurity\nNo architectural models\n \nTestability\nComponent interaction \nmetrics\nLow maturity; little empirical validation.\nUsability\nNo architectural models\n \n\n\n260 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nAs the table shows, the field still has a great deal of work to do to achieve \nthe quest for well-validated analytic models to predict behavior, but there is a \ngreat deal of activity in this area (see the “For Further Reading” section for ad-\nditional papers). The remainder of this chapter deals with techniques that can be \nused in addition to analytic models.\n14.2  Quality Attribute Checklists\nFor some quality attributes, checklists exist to enable the architect to test com-\npliance or to guide the architect when making design decisions. Quality attribute \nchecklists can come from industry consortia, from government organizations, \nor from private organizations. In large organizations they may be developed in \nhouse. \n These checklists can be specific to one or more quality attributes; checklists \nfor safety, security, and usability are common. Or they may be focused on a par-\nticular domain; there are security checklists for the financial industry, industrial \nprocess control, and the electric energy sector. They may even focus on some \nspecific aspect of a single quality attribute: cancel for usability, as an example. \nFor the purposes of certification or regulation, the checklists can be used by \nauditors as well as by the architect. For example, two of the items on the checklist \nof the Payment Card Industry (PCI) are to only persist credit card numbers in an \nencrypted form and to never persist the security code from the back of the credit \ncard. An auditor can ask to examine stored credit card data to determine whether \nit has been encrypted. The auditor can also examine the schema for data being \nstored to see whether the security code has been included.\nThis example reveals that design and analysis are often two sides of the \nsame coin. By considering the kinds of analysis to which a system will be sub-\njected (in this case, an audit), the architect will be led into making important \nearly architectural decisions (making the decisions the auditors will want to find).\nSecurity checklists usually have heavy process components. For example, a \nsecurity checklist might say that there should be an explicit security policy within \nan organization, and a cognizant security officer to ensure compliance with the \npolicy. They also have technical components that the architect needs to examine \nto determine the implications on the architecture of the system being designed or \nevaluated. For example, the following is an item from a security checklist gener-\nated by a group chartered by an organization of electric producers and distribu-\ntors. It pertains to embedded systems delivering electricity to your home:\nA designated system or systems shall daily or on request obtain current \nversion numbers, installation date, configuration settings, patch level \non all elements of a [portion of the electric distribution] system, \n\n\n14.1  Modeling Architectures to Enable Quality Attribute Analysis\n261\nIn Search of a Grand Unified Theory for Quality Attributes\nHow do we create analytic models for those quality attribute aspects for \nwhich none currently exist? I do not know the answer to this question, but \nif we had a basis set for quality attributes, we would be in a better position \nto create and validate quality attribute models. By basis set I mean a set \nof orthogonal concepts that allow one to define the existing set of quality \nattributes. Currently there is much overlap among quality attributes; a \nbasis set would enable discussion of tradeoffs in terms of a common set \nof fundamental and possibly quantifiable concepts. Once we have a basis \nset, we could develop analytic models for each of the elements of the set, \nand then an analytic model for a particular quality attribute becomes a \ncomposition of the models of the portions of the basis set that make up \nthat quality attribute. \nWhat are some of the elements of this basis set? Here are some of my \ncandidates:\n■\n■\nTime. Time is the basis for performance, some aspects of availability, \nand some aspects of usability. Time will surely be one of the fundamen-\ntal concepts for defining quality attributes. \n■\n■\nDependencies among structural elements. Modifiability, security, avail-\nability, and performance depend in some form or another on the strength \nof connections among various structural elements. Coupling is a form \nof dependency. Attacks depend on being able to move from one com-\npromised element to a currently uncompromised element through some \ndependency. Fault propagation depends on dependencies. And one of \nthe key elements of performance analysis is the dependency of one \ncomputation on another. Enumeration of the fundamental forms of de-\npendency and their properties will enable better understanding of many \nquality attributes and their interaction. \n■\n■\nAccess. How does a system promote or deny access through various \nmechanisms? Usability is concerned with allowing smooth access for \nhumans; security is concerned with allowing smooth access for some set \nof requests but denying access to another set of requests. Interoperabili-\nty is concerned with establishing connections and accessing information. \nRace conditions, which undermine availability, come about through un-\nmediated access to critical computations.\nThese are some of my candidates. I am sure there are others. The \ngeneral problem is to define a set of candidates for the basis set and then \nshow how current definitions of various quality attributes can be recast in \nterms of the elements of the basis set. I am convinced that this is a problem \nthat needs to be solved prior to making substantial progress in the quest for \na rich enough set of analytic models to enable prediction of system behav-\nior across the quality attributes important for a system.\n—LB\n\n\n262 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\ncompare these with inventory and configuration databases, and log all \ndiscrepancies. \nThis kind of rule is intended to detect malware masquerading as legitimate \ncomponents of a system. The architect will look at this item and conclude the \nfollowing:\n■\n■The embedded portions of the system should be able to report their version \nnumber, installation date, configuration settings, and patch levels. One tech-\nnique for doing this is to use “reflection” for each component in the system. \nReflection now becomes one of the important patterns used in this system.\n■\n■Each software update or patch should maintain this information. One tech-\nnique for doing this is to have automated update and patch mechanisms. \nThe architecture could also realize this functionality through reflection.\n■\n■A system must be designated to query the embedded components and per-\nsist the information. This means\n■\n■There must be overall inventory and configuration databases.\n■\n■Logs of discrepancies between current values and overall inventory must \nbe generated and sent to appropriate recipients.\n■\n■There must be network connections to the embedded components. This \naffects the network topology.\nThe creation of quality attribute checklists is usually a time-consuming ac-\ntivity, undertaken by multiple individuals and typically refined and evolved over \ntime. Domain specialists, quality attribute specialists, and architects should all \ncontribute to the development and validation of these checklists.\nThe architect should treat the items on an applicable checklist as require-\nments, in that they need to be understood and prioritized. Under particular cir-\ncumstances, an item in a checklist may not be met, but the architect should have a \ncompelling case as to why it is not.\n14.3  \u0007Thought Experiments and  \nBack-of-the-Envelope Analysis\nA thought experiment is a fancy name for the kinds of discussions that develop-\ners and architects have on a daily basis in their offices, in their meetings, over \nlunch, over whiteboards, in hallways, and around the coffee machine. One of the \nparticipants might draw two circles and an arrow on the whiteboard and make an \nassertion about the quality attribute behavior of these two circles and the arrow in \na particular context; a discussion ensues. The discussion can last for a long time, \nespecially if the two circles are augmented with a third and one more arrow, or if \nsome of the assumptions underlying a circle or an arrow are still in flux. In this \nsection, we describe this process somewhat more formally. \n\n\n14.3  Thought Experiments and Back-of-the-Envelope Analysis \n263\nThe level of formality one would use in performing a thought experiment \nis, as with most techniques discussed in this book, a question of context. If two \npeople with a shared understanding of the system are performing the thought ex-\nperiment for their own private purposes, then circles and lines on a whiteboard \nare adequate, and the discussion proceeds in a kind of shorthand. If a third person \nis to review the results and the third person does not share the common under-\nstanding, then sufficient details must be captured to enable the third person to un-\nderstand the argument—perhaps a quick legend and a set of properties need to be \nadded to the diagram. If the results are to be included in documentation as design \nrationale, then even more detail must be captured, as discussed in Chapter 18. \nFrequently such thought experiments are accompanied by rough analyses—back-\nof-the-envelope analyses—based on the best data available, based on past expe-\nriences, or even based on the guesses of the architects, without too much concern \nfor precision.\nThe purpose of thought experiments and back-of-the-envelope analysis is \nto find problems or confirmation of the nonexistence of problems in the quality \nattribute requirements as applied to sunny-day use cases. That is, for each use \ncase, consider the quality attribute requirements that pertain to that use case and \nanalyze the use case with the quality attribute requirements in mind. Models and \nchecklists focus on one quality attribute. To consider other quality attributes, one \nmust model or have a checklist for the second quality attribute and understand \nhow those models interact. A thought experiment may consider several of the \nquality attribute requirements simultaneously; typically it will focus on just the \nmost important ones.\nThe process of creating a thought experiment usually begins with listing the \nsteps associated with carrying out the use case under consideration; perhaps a se-\nquence diagram is employed. At each step of the sequence diagram, the (mental) \nquestion is asked: What can go wrong with this step with respect to any of the \nquality attribute requirements? For example, if the step involves user input, then \nthe possibility of erroneous input must be considered. Also the user may not have \nbeen properly authenticated and, even if authenticated, may not be authorized to \nprovide that particular input. If the step involves interaction with another system, \nthen the possibility that the input format will change after some time must be \nconsidered. The network passing the input to a processor may fail; the processor \nperforming the step may fail; or the computation to provide the step may fail, \ntake too long, or be dependent on another computation that may have had prob-\nlems. In addition, the architect must ask about the frequency of the input, and the \nanticipated distribution of requests (e.g., Are service requests regular and predict-\nable or irregular and “bursty”?), other processes that might be competing for the \nsame resources, and so forth. These questions go on and on.\nFor each possible problem with respect to a quality attribute requirement, \nthe follow-on questions consist of things like these: \n■\n■Are there mechanisms to detect that problem? \n",
      "page_number": 276
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 284-291)",
      "start_page": 284,
      "end_page": 291,
      "detection_method": "topic_boundary",
      "content": "264 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\n■\n■Are there mechanisms to prevent or avoid that problem? \n■\n■Are there mechanisms to repair or recover from that problem if it occurs? \n■\n■Is this a problem we are willing to live with? \nThe problems hypothesized are scrutinized in terms of a cost/benefit analy-\nsis. That is, what is the cost of preventing this problem compared to the benefits \nthat accrue if the problem does not occur?\nAs you might have gathered, if the architects are being thorough and if the \nproblems are significant (that is, they present a large risk for the system), then \nthese discussions can continue for a long time. The discussions are a normal por-\ntion of design and analysis and will naturally occur, even if only in the mind of a \nsingle designer. On the other hand, the time spent performing a particular thought \nexperiment should be bounded. This sounds obvious, but every grey-haired archi-\ntect can tell you war stories about being stuck in endless meetings, trapped in the \npurgatory of “analysis paralysis.” \nAnalysis paralysis can be avoided with several techniques:\n■\n■“Time boxing”: setting a deadline on the length of a discussion.\n■\n■Estimating the cost if the problem occurs and not spending more than that \ncost in the analysis. In other words, do not spend an inordinate amount of \ntime in discussing minor or unlikely potential problems. \nPrioritizing the requirements will help both with the cost estimation and \nwith the time estimation.\n14.4  Experiments, Simulations, and Prototypes\nIn many environments it is virtually impossible to do a purely top-down architec-\ntural design; there are too many considerations to weigh at once and it is too hard \nto predict all of the relevant technological barriers. Requirements may change in \ndramatic ways, or a key assumption may not be met: We have seen cases where a \nvendor-provided API did not work as specified, or where an API exposing a criti-\ncal function was simply missing. \nFinding the sweet spot within the enormous architectural design space of \ncomplex systems is not feasible by reflection and mathematical analysis alone; \nthe models either aren’t precise enough to deal with all of the relevant details or \nare so complicated that they are impractical to analyze with tractable mathemat-\nical techniques. \nThe purpose of experiments, simulations, and prototypes is to provide al-\nternative ways of analyzing the architecture. These techniques are invaluable in \n\n\n14.5  Analysis at Different Stages of the Life Cycle\n265\nresolving tradeoffs, by helping to turn unknown architectural parameters into \nconstants or ranges. For example, consider just a few of the questions that might \noccur when creating a web-conferencing system—a distributed client-server in-\nfrastructure with real-time constraints:\n■\n■Would moving to a distributed database from local flat files negatively im-\npact feedback time (latency) for users?\n■\n■How many participants could be hosted by a single conferencing server?\n■\n■What is the correct ratio between database servers and conferencing \nservers?\nThese sorts of questions are difficult to answer analytically. The answers to \nthese questions rely on the behavior and interaction of third-party components \nsuch as commercial databases, and on performance characteristics of software for \nwhich no standard analytical models exist. The approach used for the web-con-\nferencing architecture was to build an extensive testing infrastructure that sup-\nported simulations, experiments, and prototypes, and use it to compare the per-\nformance of each incremental modification to the code base. This allowed the \narchitect to determine the effect of each form of improvement before committing \nto including it in the final system. The infrastructure includes the following:\n■\n■A client simulator that makes it appear as though tens of thousands of cli-\nents are simultaneously interacting with a conferencing server.\n■\n■Instrumentation to measure load on the conferencing server and database \nserver with differing numbers of clients.\nThe lesson from this experience is that experimentation can often be a criti-\ncal precursor to making significant architectural decisions. Experimentation must \nbe built into the development process: building experimental infrastructure can \nbe time-consuming, possibly requiring the development of custom tools. Carry-\ning out the experiments and analyzing their results can require significant time. \nThese costs must be recognized in project schedules. \n14.5  Analysis at Different Stages of the Life Cycle\nDepending on your project’s state of development, different forms of analysis are \npossible. Each form of analysis comes with its own costs. And there are different \nlevels of confidence associated with each analysis technique. These are summa-\nrized in Table 14.3.\n\n\n266 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nTable 14.3  Forms of Analysis, Their Life-Cycle Stage, Cost, and Confidence in \nTheir Outputs\nLife-Cycle Stage\nForm of Analysis\nCost\nConfidence\nRequirements\nExperience-based analogy\nLow\nLow–High\nRequirements\nBack-of-the-envelope\nLow\nLow–Medium\nArchitecture\nThought experiment\nLow\nLow–Medium\nArchitecture\nChecklist\nLow\nMedium\nArchitecture\nAnalytic model\nLow–Medium\nMedium\nArchitecture\nSimulation\nMedium\nMedium\nArchitecture\nPrototype\nMedium\nMedium–High\nImplementation \nExperiment\nMedium–High\nMedium–High\nFielded System\nInstrumentation\nMedium–High\nHigh\nThe table shows that analysis performed later in the life cycle yields results that \nmerit high confidence. However, this confidence comes at a price. First, the cost of \nperforming the analysis also tends to be higher. But the cost of changing the system \nto fix a problem uncovered by analysis skyrockets later in the life cycle.\nChoosing an appropriate form of analysis requires a consideration of all of \nthe factors listed in Table 14.3: What life-cycle stage are you currently in? How \nimportant is the achievement of the quality attribute in question and how worried \nare you about being able to achieve the goals for this attribute? And finally, how \nmuch budget and schedule can you afford to allocate to this form of risk miti-\ngation? Each of these considerations will lead you to choose one or more of the \nanalysis techniques described in this chapter.\n14.6  Summary\nAnalysis of an architecture enables early prediction of a system’s qualities. We can \nanalyze an architecture to see how the system or systems we build from it will per-\nform with respect to their quality attribute goals. Some quality attributes, most nota-\nbly performance and availability, have well-understood, time-tested analytic models \nthat can be used to assist in quantitative analysis. Other quality attributes have less \nsophisticated models that can nevertheless help with predictive analysis. \nFor some quality attributes, checklists exist to enable the architect to test \ncompliance or to guide the architect when making design decisions. Quality at-\ntribute checklists can come from industry consortia, from government organiza-\ntions, or from private organizations. In large organizations they may be devel-\noped in house. The architect should treat the items on an applicable checklist as \nrequirements, in that they need to be understood and prioritized. \n\n\n14.7  For Further Reading\n267\nThought experiments and back-of-the-envelope analysis can often quickly \nhelp find problems or confirm the nonexistence of problems with respect to qual-\nity attribute requirements. A thought experiment may consider several of the \nquality attribute requirements simultaneously; typically it will focus on just the \nmost important ones. Experiments, simulations, and prototypes allow the explo-\nration of tradeoffs, by helping to turn unknown architectural parameters into con-\nstants or ranges whose values may be measured rather than estimated. \nDepending on your project’s state of development, different forms of analy-\nsis are possible. Each form of analysis comes with its own costs and its own level \nof confidence associated with each analysis technique.\n14.7  For Further Reading\nThere have been many papers and books published describing how to build and \nanalyze architectural models for quality attributes. Here are just a few examples.\nAvailability\nMany availability models have been proposed that operate at the architecture \nlevel of analysis. Just a few of these are [Gokhale 05] and [Yacoub 02].\nA discussion and comparison of different black-box and white-box models \nfor determining software reliability can be found in [Chandran 10].\nA book relating availability to disaster recovery and business recovery is \n[Schmidt 10].\nInteroperability\nAn overview of interoperability activities can be found in [Brownsword 04].\nModifiability\nModifiability is typically measured through complexity metrics. The classic work \non this topic is [Chidamber 94].\nMore recently, analyses based on design structure matrices have begun to \nappear [MacCormack 06].\nPerformance\nTwo of the classic works on software performance evaluation are [Smith 01] and \n[Klein 93].\n\n\n268 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nA broad survey of architecture-centric performance evaluation approaches \ncan be found in [Koziolek 10].\nSecurity\nChecklists for security have been generated by a variety of groups for different \ndomains. See for example:\n■\n■Credit cards, generated by the Payment Card Industry: www.pcisecurity-\nstandards.org/security_standards/\n■\n■Information security, generated by the National Institute of Standards and \nTechnology (NIST): [NIST 09].\n■\n■Electric grid, generated by Advanced Security Acceleration Project for the \nSmart Grid: www.smartgridipedia.org/index.php/ASAP-SG\n■\n■Common Criteria. An international standard (ISO/IEC 15408) for computer \nsecurity certification: www.commoncriteriaportal.org\nTestability\nWork in measuring testability from an architectural perspective includes measur-\ning testability as the measured complexity of a class dependency graph derived \nfrom UML class diagrams, and identifying class diagrams that can lead to code \nthat is difficult to test [Baudry 05]; and measuring controllability and observabil-\nity as a function of data flow [Le Traon 97]. \nUsability\nA checklist for usability can be found at www.stcsig.org/usability/topics/articles/\nhe-checklist.html\nSafety\nA checklist for safety is called the Safety Integrity Level: en.wikipedia.org/wiki/\nSafety_Integrity_Level\nApplications of Modeling and Analysis\nFor a detailed discussion of a case where quality attribute modeling and analysis \nplayed a large role in determining the architecture as it evolved through a number \nof releases, see [Graham 07].\n\n\n14.8  Discussion Questions\n269\n14.8  Discussion Questions\n1.\t\nBuild a spreadsheet for the steady-state availability equation MTBF / \n(MTBF + MTTR). Plug in different but reasonable values for MTBF and \nMTTR for each of the active redundancy, passive redundancy, and cold \nspare tactics. Try values for MTBF that are very large compared to MTTR, \nand also try values for MTBF that are much closer in size to MTTR. \nWhat do these tell you about which tactics you might want to choose for \navailability?\n2.\t\nEnumerate as many responsibilities as you can that need to be carried out \nfor providing a “cancel” operation in a user interface. Hint: There are at \nleast 21 of them, as indicated in a publication by (strong hint!) one of the \nauthors of this book whose last name (unbelievably strong hint!) begins \nwith “B.”\n3.\t\nThe M/M/1 (look it up!) queuing model has been employed in computing \nsystems for decades. Where in your favorite computing system would this \nmodel be appropriate to use to predict latency?\n4.\t\nSuppose an architect produced Figure 14.5 while you were sitting watching \nhim. Using thought experiments, how can you determine the performance \nand availability of this system? What assumptions are you making and what \nconclusions can you draw? How definite are your conclusions?\nFigure 14.5  Capture of a whiteboard sketch from an architect\n\n\nThis page intentionally left blank \n\n\n271\nPart  T H R EE\nArchitecture in \nthe Life Cycle\nPart I of this book introduced architecture and the various contextual lenses \nthrough which it could be viewed. To recap from Chapter 3, those contexts in-\nclude the following:\n■\n■Technical. What technical role does the software architecture play in the sys-\ntem or systems of which it’s a part? Part of the answer to this is what Part II \nof our book is about and the rest is included in Part IV. Part II describes how \ndecisions are made, and Part IV describes the environment that determines \nwhether the results of the decisions satisfy the needs of the organization.\n■\n■Project life cycle. How does a software architecture relate to the other \nphases of a software development life cycle? The answer to this is what \nPart III of our book is about.\n■\n■Business. How does the presence of a software architecture affect an orga-\nnization’s business environment? The answer to this is what Part IV of our \nbook is about.\n■\n■Professional. What is the role of a software architect in an organization or \na development project? The answer to this is threaded throughout the entire \nbook, but especially in Chapter 24, where we treat the duties, skills, and \nknowledge of software architects.\nPart II concentrated on the technical context of software architecture. In our \nphilosophy, this is tantamount to understanding quality attributes. If you have a \ndeep understanding of how architecture affects quality attributes, then you have \nmastered most of what you need to know about making design decisions.\n",
      "page_number": 284
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 292-299)",
      "start_page": 292,
      "end_page": 299,
      "detection_method": "topic_boundary",
      "content": "272\nHere in Part III we turn our attention to how to constructively apply that \nknowledge within the context of a particular software development project. Here \nis where software architecture meets software engineering: How do architecture \nconcerns affect the gathering of requirements, the carrying out of design deci-\nsions, the validation and capturing of the design, and the transformation of design \ninto implementation? In Part III, we’ll find out.\nA Word about Methods\nBecause this is a book about software architecture in practice, we’ve tried to spell \nout specific methods in enough detail so that you can emulate them. You’ll see \nPALM, a method for eliciting business goals that an architecture should accom-\nmodate. You’ll see Views and Beyond, an approach for documenting architecture \nin a set of views that serve stakeholders and their concerns. You’ll see ATAM, a \nmethod for evaluating an architecture against stakeholders’ ideas of what quality \nattributes it should provide. You’ll see CBAM, a method for assessing which evo-\nlutionary path of an architecture will best serve stakeholders’ needs. \nAll of these methods rely in some way or another on tapping stakehold-\ners’ knowledge about what an architecture under development should provide. As \npresented in their respective chapters, each of these methods includes a similar \nprocess of identifying the relevant stakeholders, putting them in a room together, \npresenting a briefing about the method that the stakeholders have been assembled \nto participate in, and then launching into the method. \nSo why is it necessary to put all of the stakeholders in the same room? The \nshort answer is that it isn’t. There are (at least) three major engagement models for \nconducting an architecture-focused method. Why three? Because we have identified \ntwo important factors, each of which has two values, that describe four potential en-\ngagement models for gathering information from stakeholders. These two factors are\n1.\t\nLocation (co-located or distributed)\n2.\t\nSynchronicity (synchronous or asynchronous) \nOne option (co-located and asynchronous) makes no sense, and so we are \nleft with three viable engagement models. The advantages and disadvantages \nwe’ve observed of each engagement model follow. \nWhy has the big-meeting format (co-located, synchronous) tended to pre-\nvail? There are several reasons:\n\n\n273\n■\n■It compresses the time required for the method. Time on site for remote \nparticipants is minimized, although as we will see, travel time is not con-\nsidered in this argument. All of the stakeholders are available with minimal \nexternal distractions.\n■\n■It emphasizes the importance of the method. Any meeting important \nenough to bring multiple people together for an extended time must be \njudged by management to be important. \n■\n■It benefits from the helpful group mentality that emerges when people are \nin the same room working toward a common goal. The group mentality \nfosters buy-in to the architecture and buy-in to the reasons it exists. Putting \nstakeholders in the same room lets them open communication paths with \nthe architect and with each other, paths that will often remain open long \nModel\nAdvantages\nDisadvantages\nAll stakeholders in \nthe same room for \nthe duration of the \nexercise (co-located \nand synchronous). \nAll stakeholders participate \nequally.\nGroup mentality produces \nbuy-in for architecture and \nthe results of the exercise.\nEnduring communication \npaths are opened among \nstakeholders.\nThis option takes the \nshortest calendar time.\nScheduling can be \nproblematic.\nSome stakeholders might not \nbe forthcoming in a crowd.\nStakeholders might incur \nsubstantial travel costs to \nattend.\nSome stakeholders \nparticipate in exercise \nremotely (distributed \nand synchronous). \nSaves travel costs for remote \nparticipants; this option \nmight permit participation by \nstakeholders who otherwise \nwould not be able to \ncontribute.\nTechnology is a limiting \nfactor; remote participants \nalmost always are second-\nclass citizens in terms of \ntheir participation and after-\nexercise “connection” to other \nparticipants.\nFacilitators \ninterviewing \nstakeholders \nindividually or in small \ngroups (distributed \nand asynchronous).\nAllows for in-depth \ninteraction between \nfacilitators and stakeholders.\nEliminates group factors that \nmight inhibit a stakeholder \nfrom speaking in public.\nIf stakeholders are widely \ndistributed, increased travel \ncosts incurred by facilitator(s).\nReduced group buy-in.\nReduced group mentality.\nReduced after-exercise \ncommunication among \nstakeholders.\nExercise stretched out over a \nlonger period of calendar time.\n\n\n274\nafter the meeting has run its course. We always enjoy seeing business cards \nexchanged with handshakes when stakeholders meet each other for the first \ntime. Putting the architect in a room full of stakeholders for a couple of \ndays is a very healthy thing for any project. \nBut there are, as ever, tradeoffs. The big-meeting format can be costly and \ndifficult to fit into an already crowded project schedule. Often the hardest aspect \nof executing any of our methods is finding two contiguous days when all the \nimportant stakeholders are available. Also, the travel costs associated with a big \nmeeting can be substantial in a distributed organization. And some stakeholders \nmight not be as forthcoming as we would like if they are in a room surrounded \nby strong-willed peers or higher-ups (although our methods use facilitation tech-\nniques to try to correct for this). \nSo which model is best? You already know the answer: It depends. You can \nsee the tradeoffs among the different approaches. Pick the one that does the best \njob for your organization and its particularities. \nConclusion\nAs you read Part III and learn about architecture methods, remember that the \nform of the method we present is the one in which the most practical experience \nresides. But:\n1.\t\nYou can always adjust the engagement model to be something other than \neverybody-in-the-same-room if that will work better for you. \n2.\t\nWhereas the steps of a method are nominally carried out in sequential order \naccording to a set agenda, sometimes there must be dynamic modifications \nto the schedule to accommodate personnel availability or architectural \ninformation. Every situation is unique, and there may be times when you \nneed to return briefly to an earlier step, jump forward to a later step, or \niterate among steps, as the need dictates. \nP.S.: We do provide one example of a shortened version of one of our methods \n—the ATAM. We call this Lightweight Architecture Evaluation, and it is de-\nscribed in Chapter 21.\n\n\n275\n15\nArchitecture in Agile \nProjects\nIt is not the strongest of the species that survives, \nnor the most intelligent that survives. It is the \none that is the most adaptable to change.\n—Charles Darwin\nSince their first appearance over a decade ago, the various flavors of Agile meth-\nods and processes have received increasing attention and adoption by the world-\nwide software community. New software processes do not just emerge out of thin \nair; they evolve in response to a palpable need. In this case, the software develop-\nment world was responding to a need for projects to be more responsive to their \nstakeholders, to be quicker to develop functionality that users care about, to show \nmore and earlier progress in a project’s life cycle, and to be less burdened by doc-\numenting aspects of a project that would inevitably change. Is any of this inimical \nto the use of architecture? We emphatically say “no.” In fact, the question for a \nsoftware project is not “Should I do Agile or architecture?”, but rather questions \nsuch as “How much architecture should I do up front versus how much should \nI defer until the project’s requirements have solidified somewhat?”, “When and \nhow should I refactor?”, and “How much of the architecture should I formally \ndocument, and when?” We believe that there are good answers to all of these \nquestions, and that Agile and architecture are not just well suited to live together \nbut in fact critical companions for many software projects.\nThe Agile software movement began to receive considerable public atten-\ntion approximately a decade ago, with the release of the “Agile Manifesto.” Its \nroots extend at least a decade earlier than that, in practices such as Extreme Pro-\ngramming and Scrum. The Agile Manifesto, originally signed by 17 developers, \nwas however a brilliant public relations move; it is brief, pithy, and sensible:\n\n\n276 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nManifesto for Agile Software Development\nWe are uncovering better ways of developing software by doing it and \nhelping others do it. Through this work we have come to value:\nIndividuals and interactions\nover\nprocesses and tools\nWorking software \nover\ncomprehensive documentation\nCustomer collaboration \nover\ncontract negotiation\nResponding to change\nover\nfollowing a plan\nThat is, while there is value in the items on the right, we value the items \non the left more. [agilemanifesto.org]\nThe authors of the Manifesto go on to describe the twelve principles that underlie \ntheir reasoning:\n1.\t Our highest priority is to satisfy the customer through early and continuous \ndelivery of valuable software.\n2.\t Welcome changing requirements, even late in development. Agile processes \nharness change for the customer’s competitive advantage.\n3.\t Deliver working software frequently, from a couple of weeks to a couple of \nmonths, with a preference to the shorter timescale.\n4.\t Business people and developers must work together daily throughout the \nproject.\n5.\t Build projects around motivated individuals. Give them the environment \nand support they need, and trust them to get the job done.\n6.\t The most efficient and effective method of conveying information to and \nwithin a development team is face-to-face conversation.\n7.\t Working software is the primary measure of progress.\n8.\t Agile processes promote sustainable development. The sponsors, develop-\ners, and users should be able to maintain a constant pace indefinitely.\n9.\t Continuous attention to technical excellence and good design enhances \nagility.\n10.\t Simplicity—the art of maximizing the amount of work not done—is \nessential.\n11.\t The best architectures, requirements, and designs emerge from self-organiz-\ning teams.\n12.\t At regular intervals, the team reflects on how to become more effective, \nthen tunes and adjusts its behavior accordingly.\nThere has been considerable elaboration of the Agile Manifesto, and Agile \nprocesses, since its first release, but the basic principles have remained solid. The \nAgile movement (and its predecessors) have gained considerable attention and \nhave enjoyed widespread adoption over the past two decades. These processes \n\n\n15.1  How Much Architecture?\n277\nwere initially employed on small- to medium-sized projects with short time \nframes and enjoyed considerable success. They were not often used for larger \nprojects, particularly those with distributed development. This is not surprising, \ngiven the twelve principles. \nIn particular principles 4 and 6 imply the need for co-location or, if co-lo-\ncation is not possible, then at least a high level of communication among the \ndistributed teams. Indeed, one of the core practices of Agile projects is frequent \n(often daily) face-to-face meetings. Principle 11 says that, for best results, teams \nshould be self-organizing. But self-organization is a social process that is much \nmore cumbersome if those teams are not physically co-located. In this case we \nbelieve that the creators of the twelve Agile principles got it wrong. The best \nteams may be self-organizing, but the best architectures still require much more \nthan this—technical skill, deep experience, and deep knowledge.\nPrinciple 1 argues for “early and continuous delivery of valuable software” \nand principle 7 claims that “Working software is the primary measure of prog-\nress.” One might argue that a focus on early and continuous release of software, \nwhere “working” is measured in terms of customer-facing features, leaves little \ntime for addressing the kinds of cross-cutting concerns and infrastructure critical \nto a high-quality large-scale system. \nIt has been claimed by some that there is an inherent tension between being \nagile and doing a conscientious job of architecting. But is there truly a tension? \nAnd if so, how do you go about characterizing it and reasoning about it? In short, \nhow much architecture is the “right” amount of architecture?\nOur brief answer, in this chapter, is that there is no tension. This issue is not \n“Agile versus Architecture” but rather “how best to blend Agile and Architecture.”\nOne more point, before we dive into the details: The Agile Manifesto is it-\nself a compromise: a pronouncement created by a committee. The fact that ar-\nchitecture doesn’t clearly live anywhere within it is most likely because they had \nno consensus opinion on this topic and not because there is any inherent conflict.\n15.1  How Much Architecture?\nWe often think of the early software development methods that emerged in the \n1970s—such as the Waterfall method—as being plan-driven and inflexible. But \nthis inflexibility is not for nothing. Having a strong up-front plan provides for \nconsiderable predictability (as long as the requirements don’t change too much) \nand makes it easier to coordinate large numbers of teams. Can you imagine a \nlarge construction or aerospace project without heavy up-front planning? Agile \nmethods and practitioners, on the other hand, often scorn planning, preferring in-\nstead teamwork, frequent face-to-face communication, flexibility, and adaptation. \nThis enhances invention and creativity. \n\n\n278 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nGarden Shed or Skyscraper?\nA few years ago I built a small shed in my back yard, for holding gardening \ntools, the lawn mower, the fertilizer cart, and so forth. I had a plan in my \nhead, a small team of physically co-located “developers,” and excellent \naccess to the customer (me) for making any last-minute decisions and for \nincorporating any late-breaking feature requests. What was my architec-\nture? For sure, nothing was written down; I had an image in my head. I \nwent to the local big-box hardware store/lumberyard and bought a bunch \nof building materials, primarily wood. I already owned a fine collection of \nhammers, saws, and drills. The boys and I started hammering and sawing \nand drilling. In short order I had a garden shed which has served its \npurpose, with the occasional repair, for quite a few years. My process was \nagile: I was able to accommodate the knowledge, skills, and characteris-\ntics of my developers; we were a self-organizing team; and I was able to \neasily accommodate feature requests that emerged late in the process.\nWould I recommend this process for the construction of a 20-story office \nbuilding, or even a building-code-compliant single-family house? Of course \nnot. All of these are built using the much-maligned BDUF (Big Design Up \nFront) process. \nMy ad hoc process for building the shed was ultimately agile, but it had \nlittle analysis or forethought. It did, however, have just enough forethought \nand planning. Doing BDUF—hiring an architect, a structural engineer, and \na surveyor, and doing a detailed analysis of soil conditions, potential snow \nloads, and options for future modifications—would have been folly; really \nexpensive folly! \nSo too with software. As with everything that we recommend in this \nbook, the amount of up-front planning and analysis should be justified by \nthe potential risks. In the end, everything in architecture is about cost/bene-\nfit tradeoffs.\n—RK\nLet us consider a specific case, to illustrate the tradeoff between up-front \nplanning and agility: the Agile technique of employing user stories. User stories \nare a cornerstone of the Agile approach. Each user story describes a set of features \nvisible to the user. Implementing user stories is a way of demonstrating progress \nto the customer. This can easily lead to an architecture in which every feature is \nindependently designed and implemented. In such an environment, concerns that \ncut across more than one feature become hard to capture. For example, suppose \nthere is a utility function that supports multiple features. To identify this utility \nfunction, coordination is required among the teams that develop the different fea-\ntures, and it also requires a role in which a broad overview across all of the fea-\ntures is maintained. If the development team is geographically distributed and the \n\n\n15.1  How Much Architecture?\n279\nsystem being developed is a large one, then emphasis on delivering features early \nwill cause massive coordination problems. In an architecture-centric project, a \nlayered architecture is a way to solve this problem, with features on upper layers \nusing shared functionality of the lower layers, but that requires up-front planning \nand design and feature analysis. \nSuccessful projects clearly need a successful blend of the two approaches. For \nthe vast majority of nontrivial projects, this is not and never should be an either/or \nchoice. Too much up-front planning and commitment can stifle creativity and the \nability to adapt to changing requirements. Too much agility can be chaos. No one \nwould want to fly in an aircraft where the flight control software had not been rig-\norously planned and thoroughly analyzed. Similarly, no one would want to spend \n18 months planning an e-commerce website for their latest cell-phone model, or \nvideo game, or lipstick (all of which are guaranteed to be badly out of fashion in \n18 months). What we all want is the sweet spot—what George Fairbanks calls “just \nenough architecture.” This is not just a matter of doing the right amount of architec-\nture work, but also doing it at the right time. Agile projects tend to want to evolve the \narchitecture, as needed, in real time, whereas large software projects have tradition-\nally favored considerable up-front analysis and planning. \nAn Analytic Perspective on Up-front Work vs. Agility\nBoehm and Turner, analyzing historical data from 161 industrial projects, \nexamine the effects of up-front architecture and risk resolution effort. This \ncorresponds to the COnstructive COst MOdel II (COCOMO II) scale factor \ncalled “RESL.” There are two activities that can add time to the basic \nproject schedule:\n■\n■\nUp-front design work on the architecture and up-front risk identification, \nplanning, and resolution work\n■\n■\nRework due to fixing defects and addressing modification requests. \nIntuitively, these two trade off against each other: The more we invest in \nplanning, the less (we hope) rework is needed. \nSo Boehm and Turner synthesized a model that allowed them to plot \nthese two values against each other. The axes of their graph (Figure 15.1) \nshow percent of time added for RESL and percent of time added to the \nschedule. The amount of architecture and risk resolution effort is plotted \nas the dashed line, moving up and to the right from near the origin, and \nranges from 5 to 50 percent of project effort. This effort is plotted against \nthree hypothetical projects, measured in thousands of source lines of code \n(KSLOC):\n■\n■\nOne project of 10 KSLOC \n■\n■\nOne project of 100 KSLOC\n■\n■\nOne project of 10,000 KSLOC\n",
      "page_number": 292
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 300-307)",
      "start_page": 300,
      "end_page": 307,
      "detection_method": "topic_boundary",
      "content": "280 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n0\n10\n20\n30\n40\n50\n60\nPercent of time added for architecture and risk resolution\nPercent of Project Schedule Devoted to \nInitial Architecture and Risk Resolution\nAdded Schedule Devoted to Rework\n(COCOMO II RESL Factor)\nTotal Percent Added to Schedule\nSweet Spot drivers:\nRapid Change: Leftward\nHigh Assurance: Rightward\nSweet Spot\nPercent of time added to Overall Schedule   \n10 KSLOC\n100 KSLOC\n10,000 KSLOC\nFigure 15.1  Architecture effort vs. rework\n\n\n15.2  Agility and Architecture Methods\n281\nThere is one line representing each of these three projects, starting \nnear the Y axis and descending, at different rates, to the X axis at the 50 \nmark. This shows that adding time for up-front work reduces later rework. \nNo surprise: that is exactly the point of doing more up-front work. However, \nwhen you sum each of those downward-trending lines (for the 10, 100, and \n1,000 KSLOC projects) with the upward sloping line for the up-front (initial \narchitecture and risk resolution) work, you get the second set of three lines, \nwhich start at the Y axis and meet the upward sloping line at the 50 mark \non the X axis. \nThese lines show that there is a sweet spot for each project. For the 10 \nKSLOC project, the sweet spot is at the far left. This says that devoting \nmuch, if any, time to up-front work is a waste for a small project (assuming \nthat the inherent domain complexity is the same for all three sets of lines). \nFor the 100 KSLOC project, the sweet spot is at around 20 percent of the \nproject schedule. And for the 1,000 KSLOC project, the sweet spot is at \naround 40 percent of the project schedule. These results are fairly intui-\ntive. A project with a million lines of code is enormously complex, and it is \ndifficult to imagine how Agile principles alone can cope with this complexity \nif there is no architecture to guide and organize the effort. \nThe graph shows that no one answer is appropriate for all situations, \nso you need methods to guide you to decide how much up-front work is \nright for you. Boehm and Turner’s work is a start, but expected lines of \ncode is not the only determinant for appropriateness of up-front planning. \nThe domain, the reliability or safety required, and the experience of your \ndevelopment team all play a role.\nThe whole point of choosing how much time to budget for architecture is to \nreduce risk. Risk may be financial, political, operational, or reputational. Some \nrisks might involve human life or the chance of legal action. Chapter 22 covers \nrisk management and budgets for planning in the context of architecture.\n15.2  Agility and Architecture Methods\nThroughout this book we emphasize methods for architecture design, analysis, \nand documentation. We unabashedly like methods! And so does the Agile com-\nmunity: dozens of books have been written on Scrum, Extreme Programming, \nCrystal Clear, and other Agile methods. But how should we think of architec-\nture-centric techniques and methods in an Agile context? How well do they fit \nwith the twelve Agile principles, for example?\nWe believe that they fit very well. The methods we present are based on the \nessential elements needed to perform the activity. If you believe that architecture \n\n\n282 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nneeds to be designed, analyzed, and documented, then the techniques we present \nare essential regardless of the project in which they are embedded. The methods \nwe present are essentially driven by the motivation to reduce risk, and by consid-\nerations of costs and benefits. \nAmong all of our methods—for extracting architecturally significant re-\nquirements, for architecture design, for architecture evaluation, for architecture \ndocumentation—that you’ll see in subsequent chapters, one might expect the \ngreatest Agile friction from evaluation and documentation. And so the rest of this \nsection will examine those two practices in an Agile context.\nArchitecture Documentation and YAGNI\nOur approach to architecture documentation is called Views and Beyond, and it \nwill be discussed in Chapter 18. Views and Beyond and Agile agree emphatically \non the following point: If information isn’t needed, don’t spend the resources \nto document it. All documentation should have an intended use and audience in \nmind, and be produced in a way that serves both. \nOne of our fundamental principles of technical documentation is “Write \nfor the reader.” That means understanding who will read the documentation \nand how they will use it. If there is no audience, there is no need to produce the \ndocumentation. This principle is so important in Agile methods that it has been \ngiven its own name: YAGNI. YAGNI means “you ain’t gonna need it,” and it \nrefers to the idea that you should only implement or document something when \nyou actually have the need for it. Do not spend time attempting to anticipate all \npossible needs. \nThe Views and Beyond approach uses the architectural view as the “unit” of \ndocumentation to be produced. Selecting the views to document is an example of \napplying this principle. The Views and Beyond approach prescribes producing a \nview if and only if it addresses substantial concerns of an important stakeholder \ncommunity. And because documentation is not a monolithic activity that holds up \nall other progress until it is complete, the view selection method prescribes pro-\nducing the documentation in prioritized stages to satisfy the needs of the stake-\nholders who need it now.\nWe document the portions of the architecture that we need to teach to \nnewcomers, that embody significant potential risks if not properly managed, \nand that we need to change frequently. We document what we need to convey \nto readers so they can do their job. Although “classic” Agile emphasizes doc-\numenting the minimum amount needed to let the current team of developers \nmake progress, our approach emphasizes that the reader might be a maintainer \nassigned to make a technology upgrade years after the original development \nteam has disbanded.\n\n\n15.3  A Brief Example of Agile Architecting\n283\nArchitecture Evaluation \nCould an architecture evaluation work as part of an Agile process? Absolutely. \nIn fact, doing so is perfectly Agile-consistent, because meeting stakeholders’ im-\nportant concerns is a cornerstone of Agile philosophy. \nOur approach to architecture evaluation is exemplified by the Architecture \nTradeoff Analysis Method (ATAM) of Chapter 21. It does not endeavor to an-\nalyze all, or even most, of an architecture. Rather, the focus is determined by \na set of quality attribute scenarios that represent the most important (but by no \nmeans all) of the concerns of the stakeholders. “Most important” is judged by \nthe amount of value the scenario brings to the architecture’s stakeholders, or the \namount of risk present in achieving the scenario. Once these scenarios have been \nelicited, validated, and prioritized, they give us an evaluation agenda based on \nwhat is important to the success of the system, and what poses the greatest risk \nfor the system’s success. Then we only delve into those areas that pose high risk \nfor the achievement of the system’s main functions and qualities.\nAnd as we will see in Chapter 21, it is easy to tailor a lightweight architec-\nture evaluation, for quicker and less-costly analysis and feedback whenever in the \nproject it is called for. \n15.3  A Brief Example of Agile Architecting\nOur claim is that architecture and agility are quite compatible. Now we will look \nat a brief case study of just that. This project, which one of the authors worked \non, involved the creation and evolution of a web-conferencing system. Through-\nout this project we practiced “agile architecting” and, we believe, hit the sweet \nspot between up-front planning where possible, and agility where needed.\nWeb-conferencing systems are complex and demanding systems. They must \nprovide real-time responsiveness, competitive features, ease of installation and \nuse, lightweight footprint, and much more. For example:\n■\n■They must work on a wide variety of hardware and software platforms, the \ndetails of which are not under the control of the architect.\n■\n■They must be reliable and provide low-latency response times, particularly \nfor real-time functionality such as voice over IP (VoIP) and screen sharing.\n■\n■They must provide high security, but do so over an unknown network topol-\nogy and an unknown set of firewalls and firewall policies.\n■\n■They must be easily modified and easily integrated into a wide variety of \nenvironments and applications.\n■\n■They must be highly usable and easily installed and learned by users with \nwidely varying IT skills. \n\n\n284 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nMany of the above-mentioned goals trade off against each other. Typically \nsecurity (in the form of encryption) comes at the expense of real-time perfor-\nmance (latency). Modifiability comes at the expense of time-to-market. Avail-\nability and performance typically come at the expense of modifiability and cost.\nEven if it is possible to collect, analyze, and prioritize all relevant data, func-\ntional requirements, and quality attribute requirements, the stringent time-to-mar-\nket constraints that prevail in a competitive climate such as web-conferencing \nwould have prevented us from doing this. Trying to support all possible uses is \nintractable, and the users themselves were poorly equipped for envisioning all \npossible potential uses of the system. So just asking the users what they wanted, \nin the fashion of a traditional requirements elicitation, was not likely to work.\nThis results in a classic “agility versus commitment” problem. On the one \nhand the architect wants to provide new capabilities quickly, and to respond to \ncustomer needs rapidly. On the other hand, long-term survival of the system and \nthe company means that it must be designed for extensibility, modifiability, and \nportability. This can best be achieved by having a simple conceptual model for \nthe architecture, based on a small number of regularly applied patterns and tac-\ntics. It was not obvious how we would “evolve” our way to such an architecture. \nSo, how is it possible to find the “sweet spot” between these opposing forces?\nThe WebArrow web-conferencing system faced precisely this dilemma. It \nwas impossible for the architect and lead designers to do purely top-down ar-\nchitectural design; there were too many considerations to weigh at once, and it \nwas too hard to predict all of the relevant technological challenges. For example, \nthey had cases where they discovered that a vendor-provided API did not work \nas specified—imagine that!—or that an API exposing a critical function was sim-\nply missing. In such cases, these problems rippled through the architecture, and \nworkarounds needed to be fashioned . . . fast! \nTo address the complexity of this domain, the WebArrow architect and de-\nvelopers found that they needed to think and work in two different modes at the \nsame time:\n■\n■Top-down—designing and analyzing architectural structures to meet the \ndemanding quality attribute requirements and tradeoffs \n■\n■Bottom-up—analyzing a wide array of implementation-specific and \nenvironment-specific constraints and fashioning solutions to them \nTo compensate for the difficulty in analyzing architectural tradeoffs with \nany precision, the team adopted an agile architecture discipline combined with a \nrigorous program of experiments aimed at answering specific tradeoff questions. \nThese experiments are what are called “spikes” in Agile terminology. And these \nexperiments proved to be the key in resolving tradeoffs, by helping to turn un-\nknown architectural parameters into constants or ranges. Here’s how it worked:\n\n\n15.3  A Brief Example of Agile Architecting\n285\n1.\t\nFirst, the WebArrow team quickly created and crudely analyzed an initial \nsoftware and system architecture concept, and then they implemented and \nfleshed it out incrementally, starting with the most critical functionality that \ncould be shown to a customer.\n2.\t\nThey adapted the architecture and refactored the design and code whenever \nnew requirements popped up or a better understanding of the problem do-\nmain emerged. \n3.\t\nContinuous experimentation, empirical evaluation, and architecture analysis \nwere used to help determine architectural decisions as the product evolved.\nFor example, incremental improvement in the scalability and fault-tolerance \nof WebArrow was guided by significant experimentation. The sorts of questions \nthat our experiments (spikes) were designed to answer were these:\n■\n■Would moving to a distributed database from local flat files negatively im-\npact feedback time (latency) for users?\n■\n■What (if any) scalability improvement would result from using mod_perl \nversus standard Perl? How difficult would the development and quality as-\nsurance effort be to convert to mod_perl?\n■\n■How many participants could be hosted by a single meeting server?\n■\n■What was the correct ratio between database servers and meeting servers?\nQuestions like these are difficult to answer analytically. The answers rely \non the behavior and interactions of third-party components, and on performance \ncharacteristics of software for which no standard analytic models exist. The Web­\nArrow team’s approach was to build an extensive testing infrastructure (including \nboth simulation and instrumentation), and to use this infrastructure to compare \nthe performance of each modification to the base system. This allowed the team \nto determine the effect of each proposed improvement before committing it to the \nfinal system. \nThe lesson here is that making architecture processes agile does not require \na radical re-invention of either Agile practices or architecture methods. The Web­\nArrow team’s emphasis on experimentation proved the key factor; it was our \nway of achieving an agile form of architecture conception, implementation, and \nevaluation. \nThis approach meant that the WebArrow architecture development approach \nwas in line with many of the twelve principles, including:\n■\n■Principle 1, providing early and continuous delivery of working software\n■\n■Principle 2, welcoming changing requirements\n■\n■Principle 3, delivering working software frequently\n■\n■Principle 8, promoting sustainable development at a constant pace \n■\n■Principle 9, giving continuous attention to technical excellence and good \ndesign\n\n\n286 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\n15.4  Guidelines for the Agile Architect\nBarry Boehm and colleagues have developed the Incremental Commitment \nModel—a hybrid process model framework that attempts to find the balance \nbetween agility and commitment. This model is based upon the following six \nprinciples:\n1.\t\nCommitment and accountability of success-critical stakeholders\n2.\t\nStakeholder “satisficing” (meeting an acceptability threshold) based on suc-\ncess-based negotiations and tradeoffs\n3.\t\nIncremental and evolutionary growth of system definition and stakeholder \ncommitment\n4.\t\nIterative system development and definition\n5.\t\nInterleaved system definition and development allowing early fielding of \ncore capabilities, continual adaptation to change, and timely growth of \ncomplex systems without waiting for every requirement and subsystem to \nbe defined\n6.\t\nRisk management—risk-driven anchor point milestones, which are key to \nsynchronizing and stabilizing all of this concurrent activity\nGrady Booch has also provided a set of guidelines for an agile architecture \n(which in turn imply some duties for the agile architect). Booch claims that all \ngood software-intensive architectures are agile. What does he mean by this? He \nmeans that a successful architecture is resilient and loosely coupled. It is com-\nposed of a core set of well-reasoned design decisions but still contains some \n“wiggle room” that allows modifications to be made and refactorings to be done, \nwithout ruining the original structure. \nBooch also notes that an effective agile process will allow the architecture to \ngrow incrementally as the system is developed and matures. The key to success \nis to have decomposability, separation of concerns, and near-independence of the \nparts. (Sound familiar? These are all modifiability tactics.)\nFinally, Booch notes that to be agile, the architecture should be visible and \nself-evident in the code; this means making the design patterns, cross-cutting \nconcerns, and other important decisions obvious, well communicated, and de-\nfended. This may, in turn, require documentation. But whatever architectural de-\ncisions are made, the architect must make an effort to “socialize” the architecture.\nWard Cunningham has coined the term “technical debt.” Technical debt is \nan analogy to the normal debt that we acquire as consumers: we purchase some-\nthing now and (hope to) pay for it later. In software the equivalent of “purchas-\ning something now” is quick-and-dirty implementation. Such implementation \nfrequently leaves technical debt that incurs penalties in the future, in terms of \nincreased maintenance costs. When technical debt becomes unacceptably high, \nprojects need to pay down some of this debt, in the form of refactoring, which is \na key part of every agile architecting process.\n\n\n15.5  Summary\n287\nWhat is our advice? \n1.\t\nIf you are building a large and complex system with relatively stable and \nwell-understood requirements, it is probably optimal to do a large amount \nof architecture work up front (see Figure 15.1 for some sample values for \n“large”).\n2.\t\nOn big projects with vague or unstable requirements, start by quickly de-\nsigning a complete candidate architecture even if it is just a “PowerPoint \narchitecture,” even if it leaves out many details, and even if you design it \nin just a couple of days. Alistair Cockburn has introduced a similar idea \nin his Crystal Clear method, called a “walking skeleton,” which is enough \narchitecture to be able to demonstrate end-to-end functionality, linking \ntogether the major system functions. Be prepared to change and elaborate \nthis architecture as circumstances dictate, as you perform your spikes and \nexperiments, and as functional and quality attribute requirements emerge \nand solidify. This early architecture will help guide development, help with \nearly problem understanding and analysis, help in requirements elicitation, \nhelp teams coordinate, and help in the creation of coding templates and oth-\ner project standards.\n3.\t\nOn smaller projects with uncertain requirements, at least try to get agree-\nment on the central patterns to be employed, but don’t spend too much time \non construction, documentation, or analysis up front. In Chapter 21 we will \nshow how analysis can be done in a relatively lightweight and “just-in-\ntime” fashion. \n15.5  Summary\nThe Agile software movement is emblemized by the Agile Manifesto and a set \nof principles that assign high value to close-knit teams and continuous and fre-\nquent delivery of working software. Agile processes were initially employed on \nsmall- to medium-sized projects with short time frames and enjoyed considerable \nsuccess. They were not often used for larger projects, particularly those with dis-\ntributed development. \nAlthough there might appear to be an inherent tension between being ag-\nile and architecture practices of the sort prescribed in this book, the underlying \nphilosophies are not at odds and can be married to great effect. Successful proj-\nects need a successful blend of the two approaches. Too much up-front planning \nand commitment can be stifling and unresponsive to customers’ needs, whereas \ntoo much agility can simply result in chaos. Agile architects tend to take a middle \nground, proposing an initial architecture and running with that, until its technical \ndebt becomes too great, at which point they need to refactor.\n",
      "page_number": 300
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 308-315)",
      "start_page": 308,
      "end_page": 315,
      "detection_method": "topic_boundary",
      "content": "288 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nBoehm and Turner, analyzing historical data from 161 industrial projects, \nexamined the effects of up-front architecture and risk resolution effort. They \nfound that projects tend to have a “sweet spot” where some up-front architecture \nplanning pays off and is not wasteful. \nAmong this book’s architecture methods, documentation and evaluation \nmight seem to be where the most friction with Agile philosophies might lie. \nHowever, our approaches to these activities are risk-based and embodied in meth-\nods that help you focus effort where it will most pay off.\nThe WebArrow example showed how adding experimentation to the proj-\nect’s processes enabled it to obtain benefits from both architecture and classic \nAgile practices, and be responsive to ever-changing requirements and domain \nunderstanding.\n15.6  For Further Reading\nAgile comes in many flavors. Here are some of the better-known ones:\n■\n■Extreme Programming [Beck 04]\n■\n■Scrum [Schwaber 04]\n■\n■Feature-Driven Development [Palmer 02]\n■\n■Crystal Clear [Cockburn 04] \nThe journal IEEE Software devoted an entire special issue in 2010 to the \ntopic of agility and architecture. The editor’s introduction [Abrahamsson 10] dis-\ncusses many of the issues that we have raised here.\nGeorge Fairbanks in his book Just Enough Architecture [Fairbanks 10] pro-\nvides techniques that are very compatible with Agile methods.\nBarry Boehm and Richard Turner [Boehm 04] offer a data- and analy-\nsis-driven perspective on the risks and tradeoffs involved in the continuum of \nchoices regarding agility and what they called “discipline.” The choice of “agil-\nity versus discipline” in the title of the book has angered and alienated many \npractitioners of Agile methods, most of which are quite disciplined. While this \nbook does not focus specifically on architecture, it does touch on the subject in \nmany ways. This work was expanded upon in 2010, when Boehm, Lane, Kool-\nmanojwong, and Turner described the Incremental Commitment Model and its \nrelationship to agility and architecture [Boehm 10]. All of Boehm and colleagues’ \nwork is informed by an active attention to risk. The seminal article on software \nrisk management [Boehm 91] was written by Barry Boehm, more than 20 years \nago, and it is still relevant and compelling reading today.\nCarriere, Kazman, and Ozkaya [Carriere 10] provide a way to reason about \nwhen and where in an architecture you should do refactoring—to reduce techni-\ncal debt—based on an analysis of the propagation cost of anticipated changes.\n\n\n15.7  Discussion Questions\n289\nThe article by Graham, Kazman, and Walmsley [Graham 07] provides sub-\nstantially more detail on the WebArrow case study of agile architecting, includ-\ning a number of architectural diagrams and additional description of the experi-\nmentation performed. \nWard Cunningham first coined the term “technical debt” in 1992 [Cunning-\nham 92]. Brown et al. [Brown 10], building in part on Cunningham’s work, offer \nan economics-driven perspective on how to enable agility through architecture. \nRobert Nord, Jim Tomayko, and Rob Wojcik [Nord 04] have analyzed the \nrelationship between several of the Software Engineering Institute’s architecture \nmethods and Extreme Programming. Grady Booch has blogged extensively on the \nrelationship between architecture and Agile in his blog, for example [Booch 11].\nFelix Bachmann [Bachmann 11] has provided a concrete example of a light-\nweight version of the ATAM that fits well with Agile projects and principles.\n15.7  Discussion Questions\n1.\t\nHow would you employ the Agile practices of pair programming, frequent \nteam interaction, and dedicated customer involvement in a distributed de-\nvelopment environment?\n2.\t\nSuppose, as a supporter of architecture practices, you were asked to write \nan Architecture Manifesto that was modeled on the Agile Manifesto. What \nwould it look like?\n3.\t\nAgile projects must be budgeted and scheduled like any other. How would \nyou do that? Does an architecture help or hinder this process?\n4.\t\nWhat do you think are the essential skills for an architect operating in an \nAgile context? How do you suppose they differ for an architect working in \na non-Agile project?\n5.\t\nThe Agile Manifesto professes to value individuals and interactions over \nprocesses and tools. Rationalize this statement in terms of the role of tools \nin the modern software development process: compilers, integrated devel-\nopment environments, debuggers, configuration managers, automatic test \ntools, and build and configuration tools.\n6.\t\nCritique the Agile Manifesto in the context of a 200-developer, 5-mil-\nlion-line project with an expected lifetime of 20 years.\n\n\nThis page intentionally left blank \n\n\n291\n16\nArchitecture and \nRequirements\nThe two most important requirements for major \nsuccess are: first, being in the right place at the \nright time, and second, doing something about it.\n—Ray Kroc\nArchitectures exist to build systems that satisfy requirements. That’s obvious. \nWhat may be less obvious is that, to an architect, not all requirements are created \nequal. Some have a much more profound effect on the architecture than others. \nAn architecturally significant requirement (ASR) is a requirement that will have \na profound effect on the architecture—that is, the architecture might well be dra-\nmatically different in the absence of such a requirement. \nYou cannot hope to design a successful architecture if you do not know the \nASRs. ASRs often, but not always, take the form of quality attribute require-\nments—the performance, security, modifiability, availability, usability, and so \nforth, that the architecture must provide to the system. In Chapters 5–13 we in-\ntroduced patterns and tactics to achieve quality attributes. Each time you select \na pattern or tactic to use in your architecture, you are changing the architecture \nas a result of the need to meet quality attribute requirements. The more difficult \nand important the QA requirement, the more likely it is to significantly affect the \narchitecture, and hence to be an ASR. \nArchitects have to identify ASRs, usually after doing a significant bit of \nwork to uncover candidate ASRs. Competent architects know this, and as we ob-\nserve experienced architects going about their duties, we notice that the first thing \nthey do is start talking to the important stakeholders. They’re gathering the in-\nformation they need to produce the architecture that will respond to the project’s \nneeds—whether or not this information has already been identified. \nThis chapter provides some systematic means for identifying the ASRs and \nother factors that will shape the architecture.\n\n\n292 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.1  Gathering ASRs from Requirements Documents\nAn obvious location to look for candidate ASRs is in the requirements documents \nor in user stories. After all, we are looking for requirements, and requirements \nshould be in requirements documents. Unfortunately, this is not usually the case, \nalthough as we will see, there is information in the requirements documents that \ncan be of use. \nDon’t Get Your Hopes Up\nMany projects don’t create or maintain the kind of requirements document that \nprofessors in software engineering classes or authors of traditional software en-\ngineering books love to prescribe. Whether requirements are specified using the \n“MoSCoW” style (must, should, could, won’t), or as a collection of “user sto-\nries,” neither of these is much help in nailing down quality attributes.\nFurthermore, no architect just sits and waits until the requirements are “fin-\nished” before starting work. The architect must begin while the requirements are \nstill in flux. Consequently, the QA requirements are quite likely to be up in the \nair when the architect starts work. Even where they exist and are stable, require-\nments documents often fail an architect in two ways.\nFirst, most of what is in a requirements specification does not affect the \narchitecture. As we’ve seen over and over, architectures are mostly driven or \n“shaped” by quality attribute requirements. These determine and constrain the \nmost important architectural decisions. And yet the vast bulk of most require-\nments specifications is focused on the required features and functionality of a \nsystem, which shape the architecture the least. The best software engineering \npractices do prescribe capturing quality attribute requirements. For example, the \nSoftware Engineering Body of Knowledge (SWEBOK) says that quality attribute \nrequirements are like any other requirements. They must be captured if they are \nimportant, and they should be specified unambiguously and be testable. \nIn practice, though, we rarely see adequate capture of quality attribute re-\nquirements. How many times have you seen a requirement of the form “The \nsystem shall be modular” or “The system shall exhibit high usability” or “The \nsystem shall meet users’ performance expectations”? These are not requirements, \nbut in the best case they are invitations for the architect to begin a conversation \nabout what the requirements in these areas really are. \nSecond, much of what is useful to an architect is not in even the best re-\nquirements document. Many concerns that drive an architecture do not manifest \nthemselves at all as observables in the system being specified, and so are not \nthe subject of requirements specifications. ASRs often derive from business goals \nin the development organization itself; we’ll explore this in Section 16.3. De-\nvelopmental qualities are also out of scope; you will rarely see a requirements \n\n\n16.1  Gathering ASRs from Requirements Documents\n293\ndocument that describes teaming assumptions, for example. In an acquisition \ncontext, the requirements document represents the interests of the acquirer, not \nthat of the developer. But as we saw in Chapter 3, stakeholders, the technical en-\nvironment, and the organization itself all play a role in influencing architectures. \nSniffing Out ASRs from a Requirements Document\nAlthough requirements documents won’t tell an architect the whole story, they \nare an important source of ASRs. Of course, ASRs aren’t going to be conve-\nniently labeled as such; the architect is going to have to perform a bit of excava-\ntion and archaeology to ferret them out.\nChapter 4 categorizes the design decisions that architects have to make. Ta-\nble 16.1 summarizes each category of architectural design decision, and it gives \na list of requirements to look for that might affect that kind of decision. If a re-\nquirement affects the making of a critical architectural design decision, it is by \ndefinition an ASR.\nTable 16.1  Early Design Decisions and Requirements That Can Affect Them\nDesign Decision Category\nLook for Requirements Addressing . . .\nAllocation of Responsibilities\nPlanned evolution of responsibilities, user roles, \nsystem modes, major processing steps, commercial \npackages\nCoordination Model\nProperties of the coordination (timeliness, currency, \ncompleteness, correctness, and consistency)\nNames of external elements, protocols, sensors \nor actuators (devices), middleware, network \nconfigurations (including their security properties)\nEvolution requirements on the list above \nData Model\nProcessing steps, information flows, major domain \nentities, access rights, persistence, evolution \nrequirements\nManagement of Resources\nTime, concurrency, memory footprint, scheduling, \nmultiple users, multiple activities, devices, energy \nusage, soft resources (buffers, queues, etc.) \nScalability requirements on the list above\nMapping among Architectural \nElements\nPlans for teaming, processors, families of \nprocessors, evolution of processors, network \nconfigurations\nBinding Time Decisions\nExtension of or flexibility of functionality, regional \ndistinctions, language distinctions, portability, \ncalibrations, configurations\nChoice of Technology\nNamed technologies, changes to technologies \n(planned and unplanned)\n\n\n294 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.2  Gathering ASRs by Interviewing Stakeholders\nSay your project isn’t producing a comprehensive requirements document. Or it \nis, but it’s not going to have the QAs nailed down by the time you need to start \nyour design work. What do you do?\nArchitects are often called upon to help set the quality attribute requirements \nfor a system. Projects that recognize this and encourage it are much more likely \nto be successful than those that don’t. Relish the opportunity. Stakeholders often \nhave no idea what QAs they want in a system, and no amount of nagging is going \nto suddenly instill the necessary insight. If you insist on quantitative QA require-\nments, you’re likely to get numbers that are arbitrary, and there’s a good chance \nthat you’ll find at least some of those requirements will be very difficult to satisfy. \nArchitects often have very good ideas about what QAs are exhibited by sim-\nilar systems, and what QAs are reasonable (and reasonably straightforward) to \nprovide. Architects can usually provide quick feedback as to which quality attri-\nbutes are going to be straightforward to achieve and which are going to be prob-\nlematic or even prohibitive. And architects are the only people in the room who \ncan say, “I can actually deliver an architecture that will do better than what you \nhad in mind—would that be useful to you?”\nInterviewing the relevant stakeholders is the surest way to learn what they \nknow and need. Once again, it behooves a project to capture this critical informa-\ntion in a systematic, clear, and repeatable way. Gathering this information from \nstakeholders can be achieved by many methods. One such method is the Quality \nAttribute Workshop (QAW), described in the sidebar.\nThe results of stakeholder interviews should include a list of architectural \ndrivers and a set of QA scenarios that the stakeholders (as a group) prioritized. \nThis information can be used to do the following:\n■\n■Refine system and software requirements\n■\n■Understand and clarify the system’s architectural drivers\n■\n■Provide rationale for why the architect subsequently made certain design \ndecisions\n■\n■Guide the development of prototypes and simulations\n■\n■Influence the order in which the architecture is developed\nThe Quality Attribute Workshop\nThe QAW is a facilitated, stakeholder-focused method to generate, prior-\nitize, and refine quality attribute scenarios before the software architec-\nture is completed. The QAW is focused on system-level concerns and \n\n\n16.2  Gathering ASRs by Interviewing Stakeholders\n295\nspecifically the role that software will play in the system. The QAW is \nkeenly dependent on the participation of system stakeholders.1\nThe QAW involves the following steps:\nStep 1: QAW Presentation and Introductions. QAW facilitators describe \nthe motivation for the QAW and explain each step of the method. Everyone \nintroduces themselves, briefly stating their background, their role in the \norganization, and their relationship to the system being built.\nStep 2: Business/Mission Presentation. The stakeholder representing the \nbusiness concerns behind the system (typically a manager or management \nrepresentative) spends about one hour presenting the system’s business \ncontext, broad functional requirements, constraints, and known quality \nattribute requirements. The quality attributes that will be refined in later \nsteps will be derived largely from the business/mission needs presented in \nthis step.\nStep 3: Architectural Plan Presentation. Although a detailed system \nor software architecture might not exist, it is possible that broad system \ndescriptions, context drawings, or other artifacts have been created that \ndescribe some of the system’s technical details. At this point in the work-\nshop, the architect will present the system architectural plans as they stand. \nThis lets stakeholders know the current architectural thinking, to the extent \nthat it exists.\nStep 4: Identification of Architectural Drivers. The facilitators will share \ntheir list of key architectural drivers that they assembled during steps 2 and \n3, and ask the stakeholders for clarifications, additions, deletions, and cor-\nrections. The idea is to reach a consensus on a distilled list of architectural \ndrivers that includes overall requirements, business drivers, constraints, \nand quality attributes. \nStep 5: Scenario Brainstorming. Each stakeholder expresses a scenario \nrepresenting his or her concerns with respect to the system. Facilitators \nensure that each scenario has an explicit stimulus and response. The \nfacilitators ensure that at least one representative scenario exists for each \narchitectural driver listed in step 4.\nStep 6: Scenario Consolidation. After the scenario brainstorming, similar \nscenarios are consolidated where reasonable. Facilitators ask stakeholders \nto identify those scenarios that are very similar in content. Scenarios that \nare similar are merged, as long as the people who proposed them agree \nand feel that their scenarios will not be diluted in the process. Consolidation \nhelps to prevent votes from being spread across several scenarios that \nare expressing the same concern. Consolidating almost-alike scenarios \nassures that the underlying concern will get all of the votes it is due. \nStep 7: Scenario Prioritization. Prioritization of the scenarios is ac-\ncomplished by allocating each stakeholder a number of votes equal to 30 \npercent of the total number of scenarios generated after consolidation. \nStakeholders can allocate any number of their votes to any scenario or \n1.  This material was adapted from [Barbacci 03].\n",
      "page_number": 308
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 316-324)",
      "start_page": 316,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": "296 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\ncombination of scenarios. The votes are counted, and the scenarios are \nprioritized accordingly.\nStep 8: Scenario Refinement. After the prioritization, the top scenar-\nios are refined and elaborated. Facilitators help the stakeholders put the \nscenarios in the six-part scenario form of source-stimulus-artifact-environ-\nment-response-response measure that we described in Chapter 4. As the \nscenarios are refined, issues surrounding their satisfaction will emerge. \nThese are also recorded. Step 8 lasts as long as time and resources allow.\n16.3  \u0007Gathering ASRs by Understanding \nthe Business Goals\nBusiness goals are the raison d’être for building a system. No organization builds \na system without a reason; rather, the organization’s leaders want to further the \nmission and ambitions of their organization and themselves. Common business \ngoals include making a profit, of course, but most organizations have many more \nconcerns than simply profit, and in other organizations (e.g., nonprofits, charities, \ngovernments), profit is the farthest thing from anyone’s mind. \nBusiness goals are of interest to architects because they often are the precursor \nor progenitor of requirements that may or may not be captured in a requirements \nspecification but whose achievement (or lack) signals a successful (or less than suc-\ncessful) architectural design. Business goals frequently lead directly to ASRs.\nThere are three possible relationships between business goals and an \narchitecture:\n1.\t\nBusiness goals often lead to quality attribute requirements. Or to put it \nanother way, every quality attribute requirement—such as user-visible \nresponse time or platform flexibility or ironclad security or any of a \ndozen other needs—should originate from some higher purpose that can \nbe described in terms of added value. If we ask, for example, “Why do \nyou want this system to have a really fast response time?”, we might hear \nthat this will differentiate the product from its competition and let the \ndeveloping organization capture market share; or that this will make the \nsoldier a more effective warfighter, which is the mission of the acquiring \norganization; or other reasons having to do with the satisfaction of some \nbusiness goal. \n2.\t\nBusiness goals may directly affect the architecture without precipitating \na quality attribute requirement at all. In Chapter 3 we told the story of \nthe architect who designed a system without a database until the manager \ninformed him that the database team needed work. The architecture was \nimportantly affected without any relevant quality attribute requirement.\n\n\n16.3  Gathering ASRs by Understanding the Business Goals\n297\nBusiness Goals\nQuality Attributes\nArchitecture\nNonarchitectural Solutions\nFigure 16.1  Some business goals may lead to quality attribute requirements \n(which lead to architectures), or lead directly to architectural decisions, or lead to \nnonarchitectural solutions.\n3.\t\nNo influence at all. Not all business goals lead to quality attributes. For \nexample, a business goal to “reduce cost” may be realized by lowering the \nfacility’s thermostats in the winter or reducing employees’ salaries or pensions. \nFigure 16.1 illustrates the major points just described. In the figure, the ar-\nrows mean “leads to.” The solid arrows are the ones highlighting relationships of \nmost interest to architects.\nArchitects often become aware of an organization’s business and business \ngoals via osmosis—working, listening, talking, and soaking up the goals that are \nat work in an organization. Osmosis is not without its benefits, but more system-\natic ways are possible. We describe one such way in the sidebar “A Method for \nCapturing Business Goals.”\nA Categorization of Business Goals\nBusiness goals are worth capturing explicitly. This is because they often imply \nASRs that would otherwise go undetected until it is too late or too expensive to \naddress them. Capturing business goals is well served by having a set of candi-\ndate business goals handy to use as conversation starters. If you know that many \nbusinesses want to gain market share, for instance, you can use that to engage \nthe right stakeholders in your organization to ask, “What are our ambitions about \nmarket share for this product, and how could the architecture contribute to meet-\ning them?”\nOur research in business goals has led us to adopt the categories shown in \nTable 16.2. These categories can be used as an aid to brainstorming and elici-\ntation. By employing the list of categories, and asking the stakeholders about \npossible business goals in each category, some assurance of coverage is gained. \n\n\n298 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nTable 16.2  A List of Standard Business Goal Categories\n1.\nContributing to the growth and continuity of the organization\n2.\nMeeting financial objectives\n3.\nMeeting personal objectives\n4.\nMeeting responsibility to employees\n5.\nMeeting responsibility to society\n6.\nMeeting responsibility to state\n7.\nMeeting responsibility to shareholders\n8.\nManaging market position\n9.\nImproving business processes\n10.\nManaging the quality and reputation of products \n11.\nManaging change in environmental factors\nThese categories are not completely orthogonal. Some business goals may \nfit into more than one category, and that’s all right. In an elicitation method, the \ncategories should prompt questions about the existence of organizational busi-\nness goals that fall into that category. If the categories overlap, then this might \ncause us to ask redundant questions. This is not harmful and could well be help-\nful. The utility of these categories is to help identify all business goals, not to \nprovide a taxonomy.\n1.\t Contributing to the growth and continuity of the organization. How does \nthe system being developed contribute to the growth and continuity of \nthe organization? In one experience using this business goal category, \nthe system being developed was the sole reason for the existence of the \norganization. If the system was not successful, the organization would \ncease to exist. Other topics that might come up in this category deal with \nmarket share, product lines, and international sales.\n2.\t Meeting financial objectives. This category includes revenue generated or \nsaved by the system. The system may be for sale, either in standalone form \nor by providing a service, in which case it generates revenue. The system \nmay be for use in an internal process, in which case it should make those \nprocesses more effective or more efficient. Also in this category is the cost \nof development, deployment, and operation of the system. But this category \ncan also include financial objectives of individuals: a manager hoping for a \nraise, for example, or a shareholder expecting a dividend.\n3.\t Meeting personal objectives. Individuals have various goals associated with \nthe construction of a system. They may range from “I want to enhance my \nreputation by the success of this system” to “I want to learn new technol-\nogies” to “I want to gain experience with a different portion of the devel-\nopment process than in the past.” In any case, it is possible that technical \ndecisions are influenced by personal objectives.\n\n\n16.3  Gathering ASRs by Understanding the Business Goals\n299\n4.\t Meeting responsibility to employees. In this category, the employees in \nquestion are usually those employees involved in development or those \ninvolved in operation. Responsibility to employees involved in develop-\nment might include ensuring that certain types of employees have a role \nin the development of this system, or it might include providing employ-\nees the opportunities to learn new skills. Responsibility to employees \ninvolved in operating the system might include safety, workload, or skill \nconsiderations.\n5.\t Meeting responsibility to society. Some organizations see themselves as \nbeing in business to serve society. For these organizations, the system under \ndevelopment is helping them meet those responsibilities. But all organiza-\ntions must discharge a responsibility to society by obeying relevant laws \nand regulations. Other topics that might come up under this category are \nresource usage, “green computing,” ethics, safety, open source issues, secu-\nrity, and privacy.\n6.\t Meeting responsibility to state. Government systems, almost by definition, \nare intended to meet responsibility to a state or country. Other topics that \nmight come up in this category deal with export controls, regulatory confor-\nmance, or supporting government initiatives.\n7.\t Meeting responsibility to shareholders. There is overlap between this cate-\ngory and the financial objectives category, but additional topics that might \ncome up here are liability protection and certain types of regulatory confor-\nmance such as, in the United States, adherence to the Sarbanes-Oxley Act.\n8.\t Managing market position. Topics that might come up in this category are \nthe strategy used to increase or hold market share, various types of intellec-\ntual property protection, or the time to market.\n9.\t Improving business processes. Although this category partially overlaps \nwith meeting financial objectives, reasons other than cost reduction exist for \nimproving business processes. It may be that improved business processes \nenable new markets, new products, or better customer support.\n10.\t Managing the quality and reputation of products. Topics that might come \nup in this category include branding, recalls, types of potential users, quali-\nty of existing products, and testing support and strategies.\n11.\t Managing change in environmental factors. As we said in Chapter 3, the \nbusiness context for a system might change. This item is intended to en-\ncourage the stakeholders to consider what might change in the business \ngoals for a system.\nExpressing Business Goals \nHow will you write down a business goal once you’ve learned it? Just as for \nquality attributes, a scenario makes a convenient, uniform, and clarifying way \nto express business goals. It helps ensure that all business goals are expressed \n\n\n300 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nclearly, in a consistent fashion, and contain sufficient information to enable their \nshared understanding by relevant stakeholders. Just as a quality attribute scenario \nadds precision and meaning to an otherwise vague need for, say, “modifiability,” \na business goal scenario will add precision and meaning to a desire to “meet fi-\nnancial objectives.”\nOur business goal scenario template has seven parts. They all relate to the \nsystem under development, the identity of which is implicit. The parts are these:\n1.\t\nGoal-source. These are the people or written artifacts providing the goal.\n2.\t\nGoal-subject. These are the stakeholders who own the goal and wish it to \nbe true. Each stakeholder might be an individual or (in the case of a goal \nthat has no one owner and has been assimilated into an organization) the or-\nganization itself. If the business goal is, for example, “Maximize dividends \nfor the shareholders,” who is it that cares about that? It is probably not the \nprogrammers or the system’s end users (unless they happen to own stock). \nGoal-subjects can and do belong to different organizations. The developing \norganization, the customer organizations, subcontractors, vendors and sup-\npliers, standards bodies, regulatory agencies, and organizations responsible \nfor systems with which ours must interact are all potential goal-subjects.\n3.\t\nGoal-object. These are the entities to which the goal applies. “Object” \nis used in the sense of the object of a verb in a sentence. All goals have \ngoal-objects: we want something to be true about something (or someone) \nthat (or whom) we care about. For example, for goals we would character-\nize as furthering one’s self-interest, the goal-object can be “myself or my \nfamily.” For some goals the goal-object is clearly the development orga-\nnization, but for some goals the goal-object can be more refined, such as \nthe rank-and-file employees of the organization or the shareholders of the \norganization. Table 16.3 is a representative cross-section of goal-objects. \nGoal-objects in the table start small, where the goal-object is a single indi-\nvidual, and incrementally grow until the goal-object is society at large.\n4.\t\nEnvironment. This is the context for this goal. For example, there are social, \nlegal, competitive, customer, and technological environments. Sometimes \nthe political environment is key; this is as a kind of social factor. Upcoming \ntechnology may be a major factor.\n5.\t\nGoal. This is any business goal articulated by the goal-source.\n6.\t\nGoal-measure. This is a testable measurement to determine how one would \nknow if the goal has been achieved. The goal-measure should usually \ninclude a time component, stating the time by which the goal should be \nachieved.\n\n\n16.3  Gathering ASRs by Understanding the Business Goals\n301\n7.\t\nPedigree and value. The pedigree of the goal tells us the degree of \nconfidence the person who stated the goal has in it, and the goal’s volatility \nand value. The value of a goal can be expressed by how much its owner \nis willing to spend to achieve it or its relative importance compared to \nother goals. Relative importance may be given by a ranking from 1 (most \nimportant) to n (least important), or by assigning each goal a value on \na fixed scale such as 1 to 10 or high-medium-low. We combine value \nand pedigree into one part although it certainly is possible to treat them \nseparately. The important concern is that both are captured.\nElements 2–6 can be combined into a sentence that reads: \nFor the system being developed, <goal-subject> desires that <goal-\nobject> achieve <goal> in the context of <environment> and will be \nsatisfied if <goal-measure>. \nThe sentence can be augmented by the goal’s source (element 1) and the \ngoal’s pedigree and value (element 7). Some sample business goal scenarios in-\nclude the following:\n■\n■For MySys, the project manager has the goal that his family’s stock in the \ncompany will rise by 5 percent (as a result of the success of MySys).\n■\n■For MySys, the developing organization’s CEO has the goal that MySys \nwill make it 50 percent less likely that his nation will be attacked.\n■\n■For MySys, the portfolio manager has the goal that MySys will make the \nportfolio 30 percent more profitable.\n■\n■For MySys, the project manager has the goal that customer satisfaction will \nrise by 10 percent (as a result of the increased quality of MySys).\nIn many contexts, the goals of different stakeholders may conflict. By iden-\ntifying the stakeholder who owns the goal, the sources of conflicting goals can be \nidentified.\nA General Scenario for Business Goals\nA general scenario (see Chapter 4) is a template for constructing specific or “con-\ncrete” scenarios. It uses the generic structure of a scenario to supply a list of \npossible values for each non-boilerplate part of a scenario. See Table 16.4 for a \ngeneral scenario for business goals. \nFor each of these scenarios you might want to additionally capture its source \n(e.g., Did this come directly from the goal-subject, a document, a third party, a \nlegal requirement?), its volatility, and its importance.\n\n\nTable 16.3  Business Goals and Their Goal-Objects\nGoal-Object\nBusiness Goals That Often Have This Goal-Object\nRemarks\nIndividual\nPersonal wealth, power, honor/face/reputation, game and gambling spirit, \nmaintain or improve reputation (personal), family interests\nThe individual who has these goals has them for \nhim/herself or his/her family.\nSystem\nManage flexibility, distributed development, portability, open systems/standards, \ntestability, product lines, integrability, interoperability, ease of installation and \nease of repair, flexibility/configurability, performance, reliability/availability, ease \nof use, security, safety, scalability/extendibility, functionality, system constraints, \ninternationalization, reduce time to market\nThese can be goals for a system being developed or \nacquired. The list applies to systems in general, but \nthe quantification of any one item likely applies to a \nsingle system being developed or acquired.\nPortfolio\nReduce cost of development, cost leadership, differentiation, reduce cost of \nretirement, smooth transition to follow-on systems, replace legacy systems, \nreplace labor with automation, diversify operational sequence, eliminate \nintermediate stages, automate tracking of business events, collect/communicate/\nretrieve operational knowledge, improve decision making, coordinate across \ndistance, align task and process, manage on basis of process measurements, \noperate effectively within the competitive environment, the technological \nenvironment, or the customer environment\nCreate something new, provide the best quality products and services possible, \nbe the leading innovator in the industry\nThese goals live on the cusp between an individual \nsystem and the entire organization. They apply either \nto a single system or to an organization’s entire \nportfolio that the organization is building or acquiring \nto achieve its goals. \nOrganization’s \nEmployees\nProvide high rewards and benefits to employees, create a pleasant and friendly \nworkplace, have satisfied employees, fulfill responsibility toward employees, \nmaintain jobs of workforce on legacy systems\nBefore we get to the organization as a whole, there \nare some goals aimed at specific subsets of the \norganization.\nOrganization’s \nShareholders\nMaximize dividends for the shareholders\nOrganization\nGrowth of the business, continuity of the business, maximize profits over the \nshort run, maximize profits over the long run, survival of the organization, \nmaximize the company’s net assets and reserves, be a market leader, maximize \nthe market share, expand or retain market share, enter new markets, maximize \nthe company’s rate of growth, keep tax payments to a minimum, increase sales \ngrowth, maintain or improve reputation, achieve business goals through financial \nobjectives, run a stable organization\nThese are goals for the organization as a whole. The \norganization can be a development or acquisition \norganization, although most were undoubtedly \ncreated with the former in mind.\nNation\nPatriotism, national pride, national security, national welfare\nBefore we get to society at large, this goal-object is \nspecifically limited to the goal owner’s own country.\nSociety\nRun an ethical organization, responsibility toward society, be a socially \nresponsible company, be of service to the community, operate effectively within \nsocial environment, operate effectively within legal environment\nSome interpret “society” as “my society,” which puts \nthis category closer to the nation goal-object, but we \nare taking a broader view.\n\n\nTable 16.4  General Scenario Generation Table for Business Goals\n2. Goal-subject\n. . . has the goal that . . .\n3. Goal-object\n. . . achieves . . .\n5. Goal\n. . . in the context of . . .\n4. Environment\n. . . and will be satisfied if . . .\n6. Goal-measure (examples, \nbased on goal categories)\n7. Value\nAny stakeholder \nor stakeholder \ngroup identified \nas having a \nlegitimate \ninterest in the \nsystem\nIndividual\nSystem\nPortfolio\nOrganization’s \nemployees\nOrganization’s \nshareholders\nOrganization\nNation\nSociety\nContributing to the \ngrowth and continuity \nof the organization\nMeeting financial \nobjectives\nMeeting personal \nobjectives\nMeeting responsibility \nto employees\nMeeting responsibility \nto society\nMeeting responsibility \nto state\nMeeting responsibility \nto shareholders\nManaging market \nposition\nImproving business \nprocesses\nManaging quality and \nreputation of products \nManaging change in \nenvironmental factors\nSocial (includes \npolitical)\nLegal\nCompetitive\nCustomer\nTechnological\nTime that business remains \nviable\nFinancial performance vs. \nobjectives\nPromotion or raise achieved in \nperiod\nEmployee satisfaction; turnover \nrate\nContribution to trade deficit/\nsurplus\nStock price, dividends\nMarket share\nTime to carry out a business \nprocess\nQuality measures of products\nTechnology-related problems\nTime window for achievement\n1–n\n1–10\nH-M-L\nResources \nwilling to \nexpend\n\n\n304 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nCapturing Business Goals\nBusiness goals are worth capturing because they can hold the key to discovering \nASRs that emerge in no other context. One method for eliciting and documenting \nbusiness goals is the Pedigreed Attribute eLicitation Method, or PALM. The word \n“pedigree” means that the business goal has a clear derivation or background. \nPALM uses the standard list of business goals and the business goal scenario \nformat we described earlier. \nPALM can be used to sniff out missing requirements early in the life cycle. \nFor example, having stakeholders subscribe to the business goal of improving the \nquality and reputation of their products may very well lead to (for example) se-\ncurity, availability, and performance requirements that otherwise might not have \nbeen considered.\nPALM can also be used to discover and carry along additional informa-\ntion about existing requirements. For example, a business goal might be to pro-\nduce a product that outcompetes a rival’s market entry. This might precipitate \na performance requirement for, say, half-second turnaround when the rival fea-\ntures one-second turnaround. But if the competitor releases a new product with \nhalf-second turnaround, then what does our requirement become? A conventional \nrequirements document will continue to carry the half-second requirement, but \nthe goal-savvy architect will know that the real requirement is to beat the compet-\nitor, which may mean even faster performance is needed.\nFinally, PALM can be used to examine particularly difficult quality attribute \nrequirements to see if they can be relaxed. We know of more than one system \nwhere a quality attribute requirement proved quite expensive to provide, and only \nafter great effort, money, and time were expended trying to meet it was it re-\nvealed that the requirement had no actual basis other than being someone’s best \nguess or fond wish at the time.\n16.4  Capturing ASRs in a Utility Tree\nAs we have seen, ASRs can be extracted from a requirements document, captured \nfrom stakeholders in a workshop such as a QAW, or derived from business goals. \nIt is helpful to record them in one place so that the list can be reviewed, refer-\nenced, used to justify design decisions, and revisited over time or in the case of \nmajor system changes.\nTo recap, an ASR must have the following characteristics:\n■\n■A profound impact on the architecture. Including this requirement will very \nlikely result in a different architecture than if it were not included.\n■\n■A high business or mission value. If the architecture is going to satisfy this \nrequirement—potentially at the expense of not satisfying others—it must be \nof high value to important stakeholders.\n",
      "page_number": 316
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 325-340)",
      "start_page": 325,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "16.4  Capturing ASRs in a Utility Tree\n305\nUsing a single list can also help evaluate each potential ASR against these \ncriteria, and to make sure that no architectural drivers, stakeholder classes, or \nbusiness goals are lacking ASRs that express their needs.\nA Method for Capturing Business Goals\nPALM is a seven-step method, nominally carried out over a day and a half \nin a workshop attended by architects and stakeholders who can speak to \nthe business goals of the organizations involved. The steps are these:\n1.\t PALM overview presentation. Overview of PALM, the problem it \nsolves, its steps, and its expected outcomes. \n2.\t Business drivers presentation. Briefing of business drivers \nby project management. What are the goals of the customer \norganization for this system? What are the goals of the \ndevelopment organization? This is normally a lengthy discussion \nthat allows participants to ask questions about the business goals \nas presented by project management. \n3.\t Architecture drivers presentation. Briefing by the architect on the \ndriving business and quality attribute requirements: the ASRs. \n4.\t Business goals elicitation. Using the standard business goal \ncategories to guide discussion, we capture the set of important \nbusiness goals for this system. Business goals are elaborated and \nexpressed as scenarios. We consolidate almost-alike business \ngoals to eliminate duplication. Participants then prioritize the \nresulting set to identify the most important goals. \n5.\t Identification of potential quality attributes from business goals. \nFor each important business goal scenario, participants describe \na quality attribute that (if architected into the system) would help \nachieve it. If the QA is not already a requirement, this is recorded \nas a finding.\n6.\t Assignment of pedigree to existing quality attribute drivers. \nFor each architectural driver named in step 3, we identify which \nbusiness goals it is there to support. If none, that’s recorded \nas a finding. Otherwise, we establish its pedigree by asking \nfor the source of the quantitative part. For example: Why is \nthere a 40-millisecond performance requirement? Why not 60 \nmilliseconds? Or 80 milliseconds?\n7.\t Exercise conclusion. Review of results, next steps, and participant \nfeedback.\n\n\n306 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nArchitects can use a construct called a utility tree for all of these purposes. A \nutility tree begins with the word “utility” as the root node. Utility is an expression \nof the overall “goodness” of the system. We then elaborate this root node by listing \nthe major quality attributes that the system is required to exhibit. (We said in Chap-\nter 4 that quality attribute names by themselves were not very useful. Never fear: \nwe are using them only as placeholders for subsequent elaboration and refinement!) \nUnder each quality attribute, record a specific refinement of that QA. For \nexample, performance might be decomposed into “data latency” and “transac-\ntion throughput.” Or it might be decomposed into “user wait time” and “time \nto refresh web page.” The refinements that you choose should be the ones that \nare relevant to your system. Under each refinement, record the appropriate ASRs \n(usually expressed as QA scenarios).\nSome ASRs might express more than one quality attribute and so might ap-\npear in more than one place in the tree. That is not necessarily a problem, but it \ncould be an indication that the ASR tries to cover too much diverse territory. Such \nASRs may be split into constituents that each attach to smaller concerns.\nOnce the ASRs are recorded and placed in the tree, you can now evaluate \nthem against the two criteria we listed above: the business value of the candidate \nASR and the architectural impact of including it. You can use any scale you like, \nbut we find that a simple “H” (high), “M” (medium), and “L” (low) suffice for \neach criterion.\nFor business value, High designates a must-have requirement, Medium is for a \nrequirement that is important but would not lead to project failure were it omitted. \nLow describes a nice requirement to have but not something worth much effort. \nFor architectural impact, High means that meeting this ASR will profoundly \naffect the architecture. Medium means that meeting this ASR will somewhat af-\nfect the architecture. Low means that meeting this candidate ASR will have little \neffect on the architecture. \nTable 16.5 shows a portion of a sample utility tree drawn from a health care ap-\nplication called Nightingale. Each ASR is labeled with a pair of “H,” “M,” and “L” \nvalues indicating (a) the ASR’s business value and (b) its effect on the architecture.\nOnce you have a utility tree filled out, you can use it to make important \nchecks. For instance:\n■\n■A QA or QA refinement without any ASR is not necessarily an error or \nomission that needs to be rectified, but it is an indication that attention should \nbe paid to finding out for sure if there are unrecorded ASRs in that area.\n■\n■ASRs that rate a (H,H) rating are obviously the ones that deserve the most \nattention from you; these are the most significant of the significant require-\nments. A very large number of these might be a cause for concern about \nwhether the system is achievable. \n■\n■Stakeholders can review the utility tree to make sure their concerns are ad-\ndressed. (An alternative to the organization we have described here is to use \nstakeholder roles rather than quality attributes as the organizing rule under \n“Utility.”)\n\n\n16.4  Capturing ASRs in a Utility Tree\n307\nTABLE 16.5  Tabular Form of the Utility Tree for the Nightingale ATAM Exercise \nQuality \nAttribute\nAttribute \nRefinement \nASR\nPerformance \nTransaction \nresponse time\nA user updates a patient’s account in response to a \nchange-of-address notification while the system is under \npeak load, and the transaction completes in less than \n0.75 second. (H,M)\nA user updates a patient’s account in response to a \nchange-of-address notification while the system is under \ndouble the peak load, and the transaction completes in \nless than 4 seconds. (L,M)\nThroughput \nAt peak load, the system is able to complete 150 \nnormalized transactions per second. (M,M)\nUsability \nProficiency \ntraining\nA new hire with two or more years’ experience in the \nbusiness becomes proficient in Nightingale’s core \nfunctions in less than 1 week. (M,L)\nA user in a particular context asks for help, and the \nsystem provides help for that context, within 3 seconds. \n(H,M)\nNormal \noperations\nA hospital payment officer initiates a payment plan for a \npatient while interacting with that patient and completes \nthe process without the system introducing delays. (M,M)\nConfigurability \nUser-defined \nchanges\nA hospital increases the fee for a particular service. The \nconfiguration team makes the change in 1 working day; \nno source code needs to change. (H,L)\nMaintainability\nRoutine \nchanges\nA maintainer encounters search- and response-time \ndeficiencies, fixes the bug, and distributes the bug fix with \nno more than 3 person-days of effort. (H,M)\nA reporting requirement requires a change to the report-\ngenerating metadata. Change is made in 4 person-hours \nof effort. (M,L)\nUpgrades to \ncommercial \ncomponents\nThe database vendor releases a new version that must \nbe installed in less than 3 person-weeks. (H,M)\nExtensibility \nAdding new \nproduct\nA product that tracks blood bank donors is created within \n2 person-months. (M,M)\nSecurity \nConfidentiality A physical therapist is allowed to see that part of a \npatient’s record dealing with orthopedic treatment but not \nother parts nor any financial information. (H,M)\nIntegrity\nThe system resists unauthorized intrusion and reports the \nintrusion attempt to authorities within 90 seconds. (H,M)\nAvailability\nNo downtime\nThe database vendor releases new software, which is \nhot-swapped into place, with no downtime. (H,L)\nThe system supports 24/7 web-based account access by \npatients. (L,L)\n\n\n308 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.5  Tying the Methods Together\nHow should you employ requirements documents, stakeholder interviews, Qual-\nity Attribute Workshops, PALM, and utility trees in concert with each other?\nAs for most complex questions, the answer to this one is “It depends.” If you \nhave a requirements process that gathers, identifies, and prioritizes ASRs, then \nuse that and consider yourself lucky.\nIf you feel your requirements fall short of this ideal state, then you can bring \nto bear one or more of the other approaches. For example, if nobody has captured \nthe business goals behind the system you’re building, then a PALM exercise \nwould be a good way to ensure that those goals are represented in the system’s \nASRs. \nIf you feel that important stakeholders have been overlooked in the require-\nments-gathering process, then it will probably behoove you to capture their con-\ncerns through interviews. A Quality Attribute Workshop is a structured method to \ndo that and capture their input.\nBuilding a utility tree is a good way to capture ASRs along with their priori-\ntization—something that many requirements processes overlook.\nFinally, you can blend all the methods together: PALM makes an excellent \n“subroutine call” from a Quality Attribute Workshop for the step that asks about \nbusiness goals, and a quality attribute utility tree makes an excellent repository \nfor the scenarios that are the workshop’s output. \nIt is unlikely, however, that your project will have the time and resources to \nsupport this do-it-all approach. Better to pick the approach that fills in the biggest \ngap in your existing requirements: stakeholder representation, business goal man-\nifestation, or ASR prioritization.\n16.6  Summary\nArchitectures are driven by architecturally significant requirements: requirements \nthat will have profound effects on the architecture. Architecturally significant \nrequirements may be captured from requirements documents, by interviewing \nstakeholders, or by conducting a Quality Attribute Workshop.\nIn gathering these requirements, we should be mindful of the business goals \nof the organization. Business goals can be expressed in a common, structured \nform and represented as scenarios. Business goals may be elicited and docu-\nmented using a structured facilitation method called PALM.\nA useful representation of quality attribute requirements is in a utility tree. \nThe utility tree helps to capture these requirements in a structured form, starting \nfrom coarse, abstract notions of quality attributes and gradually refining them to \n\n\n16.8  Discussion Questions\n309\nthe point where they are captured as scenarios. These scenarios are then priori-\ntized, and this prioritized set defines your “marching orders” as an architect.\n16.7  For Further Reading\nPALM can be used to capture the business goals that conform to a business goal \nviewpoint; that is, you can use PALM to populate a business goal view of your \nsystem, using the terminology of ISO Standard 42010. We discuss this in The \nBusiness Goals Viewpoint [Clements 10c]. Complete details of PALM can be \nfound in CMU/SEI-2010-TN-018, Relating Business Goals to Architecturally \nSignificant Requirements for Software Systems [Clements 10b].\nThe Open Group Architecture Framework, available at www.opengroup.org/\ntogaf/, provides a very complete template for documenting a business scenario \nthat contains a wealth of useful information. Although we believe architects can \nmake use of a lighter-weight means to capture a business goal, it’s worth a look.\nThe definitive reference source for the Quality Attribute Workshop is [Bar-\nbacci 03].\nThe term architecturally significant requirement was created by the Soft-\nware Architecture Review and Assessment (SARA) group [Obbink 02].\nWhen dealing with systems of systems (SoS), the interaction and handoff \nbetween the systems can be a source of problems. The Mission Thread Workshop \nand Business Thread Workshop focus on a single thread of activity within the \noverall SoS context and identify potential problems having to do with the inter-\naction of the disparate systems. Descriptions of these workshops can be found at \n[Klein 10] and [Gagliardi 09].\n16.8  Discussion Questions\n1.\t\nInterview representative stakeholders for your business’s or university’s ex-\npense recovery system. Capture the business goals that are motivating the sys-\ntem. Use the seven-part business goal scenario outline given in Section 16.3.\n2.\t\nDraw a relation between the business goals you uncovered for the previous \nquestion and ASRs.\n3.\t\nConsider an automated teller machine (ATM) system. Attempt to apply the \n11 categories of business goals to that system and infer what goals might \nhave been held by various stakeholders involved in its development.\n\n\n310 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n4.\t\nCreate a utility tree for the ATM system above. (Interview some of your \nfriends and colleagues if you like, to have them contribute quality attribute \nconsiderations and scenarios.) Consider a minimum of four different quali-\nty attributes. Ensure that the scenarios that you create at the leaf nodes have \nexplicit responses and response measures.\n5.\t\nRestructure the utility tree given in Section 16.4 using stakeholder roles as \nthe organizing principle. What are the benefits and drawbacks of the two \nrepresentations?\n6.\t\nFind a software requirements specification that you consider to be of high \nquality. Using colored pens (real ones if the document is printed, virtual \nones if the document is online), color red all the material that you find com-\npletely irrelevant to a software architecture for that system. Color yellow all \nof the material that you think might be relevant, but not without further dis-\ncussion and elaboration. Color green all of the material that you are certain \nis architecturally significant. When you’re done, every part of the document \nthat’s not white space should be red, yellow, or green. Approximately what \npercentage of each color did your document end up being? Do the results \nsurprise you?\n\n\n311\n17\nDesigning an \nArchitecture\nIn most people’s vocabularies, design means veneer. \nIt’s interior decorating. It’s the fabric of the curtains or \nthe sofa. But to me, nothing could be further from the \nmeaning of design. Design is the fundamental soul of \na human-made creation that ends up expressing itself \nin successive outer layers of the product or service. \n—Steve Jobs\nWe have discussed the building blocks for designing a software architecture, \nwhich principally are locating architecturally significant requirements; capturing \nquality attribute requirements; and choosing, generating, tailoring, and analyzing \ndesign decisions for achieving those requirements. All that’s missing is a way to \npull the pieces together. The purpose of this chapter is to provide that way. \nWe begin by describing our strategy for designing an architecture and then \npresent a packaging of these ideas into a method: the Attribute-Driven Design \nmethod. \n17.1  Design Strategy\nWe present three ideas that are key to architecture design methods: decomposi-\ntion, designing to architecturally significant requirements, and generate and test.\nDecomposition \nArchitecture determines the quality attributes of a system. Hopefully, we have \nconvinced you of that by now. The quality attributes are properties of the system \n\n\n312 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nas a whole. Latency, for example, is the time between the arrival of an event and \nthe output of the processing of that event. Availability refers to the system provid-\ning services, and so forth.\nGiven the fact that quality attributes refer to the system as a whole, if we \nwish to design to achieve quality attribute requirements, we must begin with the \nsystem as a whole. As the design is decomposed, the quality attribute require-\nments can also be decomposed and assigned to the elements of the decomposition.\nA decomposition strategy does not mean that we are assuming the design is \na green-field design or that there are no constraints on the design to use partic-\nular preexisting components either externally developed or legacy. Just as when \nyou choose a route from one point to another, you may choose to stop at various \ndestinations along the route, constraints on the design can be accommodated by \na decomposition strategy. You as the designer must keep in mind the constraints \ngiven to you and arrange the decomposition so that it will accommodate those \nconstraints. In some contexts, the system may end up being constructed mostly \nfrom preexisting components; in others, the preexisting components may be a \nsmaller portion of the overall system. In either case, the goal of the design ac-\ntivity is to generate a design that accommodates the constraints and achieves the \nquality and business goals for the system. \nWe have already talked about module decomposition, but there are other \nkinds of decompositions that one regularly finds in an architecture, such as the \ndecomposition of a component in a components-and-connectors (C&C) pattern \ninto its subcomponents. For example, a user interface implemented using the \nmodel-view-controller (MVC) pattern would be decomposed into a number of \ncomponents for the model, one or more views, and one or more controllers.\nDesigning to Architecturally Significant Requirements\nIn Chapter 16, we discussed architecturally significant requirements (ASRs) and \ngave a technique for collecting and structuring them. These are the requirements \nthat drive the architectural design; that is why they are significant. Driving the \ndesign means that these requirements have a profound effect on the architecture. \nIn other words, you must design to satisfy these requirements. This raises two \nquestions: What happens to the other requirements? and Do I design for one ASR \nat a time or all at once? \n1.\t\nWhat about the non-ASR requirements? The choice of ASRs implies a \nprioritization of the requirements. Once you have produced a design that \nsatisfies the ASRs, you know that you are in good shape. However, in the \nreal world, there are other requirements that, while not ASRs, you would \nlike to be able to satisfy. You have three options with respect to meeting \nthese other requirements: (a) You can still meet the other requirements. \n(b) You can meet the other requirements with a slight adjustment of the \nexisting design, and this slight adjustment does not keep the higher priority \n\n\n17.1  Design Strategy\n313\nrequirements from being met. (c) You cannot meet the other requirements \nunder the current design. In case (a) or (b), there is nothing more to be \ndone. You are happy. In case (c), you now have three options: (i) If you are \nclose to meeting the requirement, you can see if the requirement can be \nrelaxed. (ii) You can reprioritize the requirements and revisit the design. \n(iii) You can report that you cannot meet the requirement. All of these \nlatter three options involve adjusting either the requirement or its priority. \nDoing so may have a business impact, and it should be reported up the \nmanagement chain.\n2.\t\nDesign for all of the ASRs or one at a time? The answer to this question is a \nmatter of experience. When you learn chess, you begin by learning that the \nhorsey goes up two and over one. After you have been playing for a while, \nyou internalize the moves of the knight and you can begin to look further \nahead. The best players may look ahead a dozen or more moves. This situ-\nation applies to when you are designing to satisfy ASRs. Left to their own \ndevices, novice architects will likely focus on one ASR at a time. But you \ncan do better than that. Eventually, through experience and education, you \nwill develop an intuition for designing, and you will employ patterns to aid \nyou in designing for multiple ASRs.\nGenerate and Test\nOne way of viewing design is as a process of “generate and test.” This gener-\nate-and-test approach views a particular design as a hypothesis: namely, the de-\nsign satisfies the requirements. Testing is the process of determining whether the \ndesign hypothesis is correct. If it is not, then another design hypothesis must be \ngenerated. Figure 17.1 shows this iteration.\nGenerate \nInitial \nHypothesis \nTest \nHypothesis \nGenerate \nHypothesis \nFigure 17.1  The generate-and-test process of architecture design\n\n\n314 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nFor this process to be effective, the generation of the next design hypothesis \nmust build on the results of the tests. That is, the things wrong with the current \ndesign hypothesis are fixed in the next design hypothesis, and the things that are \nright are kept. If there is no coupling between the testing and the generation of \nthe next design hypothesis, then this process becomes “guess and test” and that \nis not effective.\nGenerate and test as a design strategy leads to the following questions:\n1.\t\nWhere does the initial hypothesis come from?\n2.\t\nWhat are the tests that are applied?\n3.\t\nHow is the next hypothesis generated?\n4.\t\nWhen are you done?\nWe have already seen many of the elements of the answers to these ques-\ntions. But now we can think about them and organize them more systematically.\nCreating the Initial Hypothesis\nDesign solutions are created using “collateral” that is available to the project. \nCollateral can include existing systems, frameworks available to the project, \nknown architecture patterns, design checklists, or a domain decomposition. \n■\n■Existing systems. Very few systems are completely unprecedented, even \nwithin a single organization. Organizations are in a particular business, their \nbusiness leads to specialization, and specialization leads to the development \nof variations on a theme. It is likely that systems already exist that are \nsimilar to the system being constructed in your company. \nExisting systems are likely to provide the most powerful collateral, \nbecause the business context and requirements for the existing system are \nlikely to be similar to the business context and requirements for the new \nsystem, and many of the problems that occur have already been solved in \nthe existing design.\nA common special case is when the existing system you’re drawing \non for knowledge is the same one that you’re building. This occurs when \nyou’re evolving a system, not building one from scratch. The existing \ndesign serves as the initial design hypothesis. The “test” part of this pro-\ncess will reveal the parts that don’t work under the current (presumably \nchanged) set of requirements and will therefore pinpoint the parts of the \nsystem’s design that need to change.\nAnother special case is when you have to combine existing legacy sys-\ntems into a single system. In this case, the collection of legacy systems can \nbe mined to determine the initial design hypothesis.\n■\n■Frameworks. A framework is a partial design (accompanied by code) that \nprovides services that are common in particular domains. Frameworks exist \nin a great many domains, ranging from web applications to middleware sys-\ntems to decision support systems. The design of the framework (especially \n\n\n17.1  Design Strategy\n315\nthe architectural assumptions it makes) provides the initial design hypoth-\nesis. For example, a design framework might constrain all communication \nto be via a broker, or via a publish-subscribe bus, or via callbacks. In each \ncase this design framework has constrained your initial design hypothesis.\n■\n■Patterns and tactics. As we discussed in Chapter 13, a pattern is a known \nsolution to a common problem in a given context. Cataloged architectural \npatterns, possibly augmented with tactics, should be considered as candi-\ndates for the design hypothesis you’re building. \n■\n■Domain decomposition. Another option for the initial design hypothesis \ncomes from performing a domain decomposition. For example, most ob-\nject-oriented analysis and design processes begin this way, identifying ac-\ntors and entities in the domain. This decomposition will divide the respon-\nsibilities to make certain modifications easier, but by itself it does not speak \nto many other quality attribute requirements.\n■\n■Design checklists. The design checklists that we presented in Chapters 5–11 \ncan guide an architect to making quality-attribute-targeted design choices. \nThe point of using a checklist is to ensure completeness: Have I thought \nabout all of the issues that might arise with respect to the many quality \nattribute concerns that I have? The checklist will provide guidance and \nconfidence to an architect. \nChoosing the Tests\nThree sources provide the tests to be applied to the hypothesis:\n1.\t\nThe analysis techniques described in Chapter 14.\n2.\t\nThe design checklists for the quality attributes that we presented in Chap-\nters 5–11 can also be used to test the design decisions already made, from \nthe sources listed above. For the important quality attribute requirements, \nuse the design checklists to assess whether the decisions you’ve made so \nfar are sound and complete. For example, if testability is important for your \nsystem, the checklist says to ensure that the coordination model supports \ncapturing the activity that led to a fault. \n3.\t\nThe architecturally significant requirements. If the hypothesis does not \nprovide a solution for the ASRs, then it must be improved.\nGenerating the Next Hypothesis\nAfter applying the tests, you might be done—everything looks good. On the \nother hand, you might still have some concerns; specifically, you might have a list \nof quality attribute problems associated with your analysis of the current hypoth-\nesis. This is the problem that tactics are intended to solve: to improve a design \nwith respect to a particular quality attribute. Use the sets of tactics described in \neach of Chapters 5–11 to help you to choose the ones that will improve your de-\nsign so that you can satisfy these outstanding quality attribute requirements.\n\n\n316 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nTerminating the Process\nYou are done with the generate-and-test process when you either have a design \nthat satisfies the ASRs or when you exhaust your budget for producing the de-\nsign. In Chapter 22, we discuss how much time should be budgeted for producing \nthe architecture. \nIf you do not produce such a design within budget, then you have two op-\ntions depending on the set of ASRs that are satisfied. Your first option is to pro-\nceed to implementation with the best hypothesis you were able to produce, with \nthe realization that some ASRs may not be met and may need to be relaxed or \neliminated. This is the most common case. Your second option is to argue for \nmore budget for design and analysis, potentially revisiting some of the major \nearly design decisions and resuming generate and test from that point. If all else \nfails, you could suggest that the project be terminated. If all of the ASRs are crit-\nical and you were not able to produce an acceptable or nearly acceptable design, \nthen the system you produce from the design will not be satisfactory and there is \nno sense in producing it.\n17.2  The Attribute-Driven Design Method\nThe Attribute-Driven Design (ADD) method is a packaging of the strategies that \nwe have just discussed. ADD is an iterative method that, at each iteration, helps \nthe architect to do the following:\n■\n■Choose a part of the system to design.\n■\n■Marshal all the architecturally significant requirements for that part.\n■\n■Create and test a design for that part.\nThe output of ADD is not an architecture complete in every detail, but an \narchitecture in which the main design approaches have been selected and vetted. \nIt produces a “workable” architecture early and quickly, one that can be given to \nother project teams so they can begin their work while the architect or architec-\nture team continues to elaborate and refine.\nInputs to ADD\nBefore beginning a design process, the requirements—functional, quality, and \nconstraints—should be known. In reality, waiting for all of the requirements to \nbe known means the project will never be finished, because requirements are con-\ntinually arriving to a project as a result of increased knowledge on the part of the \nstakeholders and changes in the environment (technical, social, legal, financial, \nor political) over time. ADD can begin when a set of architecturally significant \nrequirements is known. \n\n\n17.2  The Attribute-Driven Design Method\n317\nThis increases the importance of having the correct set of ASRs. If the set \nof ASRs changes after design has begun, then the design may well need to be \nreworked (a truth under any design method, not just ADD). To the extent that you \nhave any influence over the requirements-gathering process, it would behoove \nyou to lobby for collection of ASRs first. Although these can’t all be known a \npriori, as we saw in Chapter 16, quality attribute requirements are a good start.\nIn addition to the ASRs, input to ADD should include a context description. \nThe context description gives you two vital pieces of information as a designer:\n1.\t\nWhat are the boundaries of the system being designed? What is inside the \nsystem and what is outside the system must be known in order to constrain \nthe problem and establish the scope of the architecture you are designing. \nThe system’s scope is unknown or unclear surprisingly often, and it will \nhelp the architecture to nail down the scope as soon as you can. \n2.\t\nWhat are the external systems, devices, users, and environmental conditions \nwith which the system being designed must interact? By “environmental con-\nditions” here we are referring to the system’s runtime environment. The sys-\ntem’s environmental conditions are an enumeration of factors such as where \nthe input comes from, where the output goes, what forms they take, what \nquality attributes they have, and what forces may affect the operation of the \nsystem. It is possible that not all of the external systems are known at design \ntime. In this case, the system must have some discovery mechanisms, but the \ncontext description should enumerate the assumptions that can be made about \nthe external systems even if their specifics are not yet known. An example of \naccommodating environment conditions can be seen in a system that must \nbe sent into space. In addition to handling its inputs, outputs, and quality \nattributes, such a system must accommodate failures caused by stray gamma \nrays, certainly a force affecting the operation of the system. \nOutput of ADD\nThe output of ADD is a set of sketches of architectural views. The views together \nwill identify a collection of architectural elements and their relationships or in-\nteractions. One of the views produced will be a module decomposition view, and \nin that view each element will have an enumeration of its responsibilities listed. \nOther views will be produced according to the design solutions chosen \nalong the way. For example, if at one point in executing the method, you choose \nthe service-oriented architecture (SOA) pattern for part of the system, then you \nwill capture this in an SOA view (whose scope is that part of the system to which \nyou applied the pattern). \nThe interactions of the elements are described in terms of the information \nbeing passed between the elements. For example, we might specify protocol \nnames, synchronous, asynchronous, level of encryption, and so forth.\n\n\n318 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nThe reason we refer to “sketches” above is that ADD does not take the \ndesign so far as to include full-blown interface specifications, or even so far as \nchoosing the names and parameter types of interface programs (methods). That \ncan come later. ADD does identify the information that passes through the inter-\nfaces and important characteristics of the information. If any aspects of an inter-\nface have quality attribute implications, those are captured as annotations. \nWhen the method reaches the end, you will have a full-fledged architecture \nthat is roughly documented as a set of views. You can then polish this collection, \nperhaps merging some of the views as appropriate, to the extent required by your \nproject. In an Agile project, this set of rough sketches may be all you need for \nquite a while, or for the life of the project.\n17.3  The Steps of ADD\nADD is a five-step method:\n1.\t\nChoose an element of the system to design.\n2.\t\nIdentify the ASRs for the chosen element.\n3.\t\nGenerate a design solution for the chosen element.\n4.\t\nInventory remaining requirements and select the input for the next iteration.\n5.\t\nRepeat steps 1–4 until all the ASRs have been satisfied.\nStep 1: Choose an Element of the System to Design\nADD works by beginning with a part of the system that has not yet been de-\nsigned, and designing it. In this section, we’ll discuss how to make that choice.\nFor green-field designs, the “element” to begin with is simply the entire \nsystem. The first trip through the ADD steps will yield a broad, shallow design \nthat will produce a set of newly identified architectural elements and their inter-\nactions. These elements will almost certainly require more design decisions to \nflesh out what they do and how they satisfy the ASRs allocated to them; during \nthe next iteration of ADD, those elements become candidates for the “choose an \nelement” step.\nSo, nominally, the first iteration of ADD will create a collection of elements \nthat together constitute the entire system. The second iteration will take one of \nthese elements—what we call the “chosen element”—and design it, resulting in \nstill finer-grained elements. The third iteration will take another element—either \none of the children of the whole system or one of the children that was created \nfrom the design of one of the children of the whole system—and so forth. For ex-\nample, if you choose an SOA pattern in the first iteration, you might choose child \nelements such as service clients, service providers, and the SOA infrastructure \n\n\n17.3  The Steps of ADD\n319\ncomponents. In the next iteration through the loop, you would refine one of these \nchild elements, perhaps the infrastructure components. In the next iteration you \nnow have a choice: refine another child of the SOA pattern, such as a service pro-\nvider, or refine one of the child elements of the infrastructure components. Figure \n17.2 shows these choices as a decomposition tree, annotated with the ADD itera-\ntion that applies to each node. (The example components are loosely based on the \nAdventure Builder system, introduced in Chapter 13.) Figure 17.2 is a decompo-\nsition view of our hypothetical system after two iterations of ADD.\nThere are cases when the first iteration of ADD is different. Perhaps you are \nnot creating a system but evolving an existing one. Perhaps you are required to \nuse a piece of software that your company already owns, and therefore must fit it \ninto the design. There are many reasons why some of the design might already be \ndone for you, and the first time through the steps of ADD you won’t pick “whole \nsystem” as the starting point. Nevertheless, step 1 still holds: All it requires is that \nat least one of the elements you know about needs further design.\nThere are two main refinement strategies to pursue with ADD: breadth first \nand depth first. Breadth first means that all of the second-level elements are de-\nsigned before any of the third-level elements, and so forth. Depth first means that \none downward chain is completed before beginning a second downward chain. \nThe order that you should work through ADD is influenced by the business and \ntechnical contexts within which the project is operating. Some of the important \nfactors include the following:\nIteration #2: \nSOA infrastructure \ncomponents refined\nOr an SOA infrastructure component?\nFigure 17.2  Iteration 1 applied the SOA pattern. Iteration 2 refined the \ninfrastructure components. Where will iteration 3 take you?\n\n\n320 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\n■\n■Personnel availability may dictate a refinement strategy. If an important \ngroup or team has a window of availability that will close soon and will \nwork on a particular part of the system, then it behooves the architect to \ndesign that part of the system to the point where it can be handed off for \nimplementation—depth first. But if the team is not currently available but \nwill be available at some definite time in the future, then you can defer their \npart of the design until later. \n■\n■Risk mitigation may dictate a refinement strategy. The idea is to design the \nrisky parts of the system to enough depth so that problems can be identified \nand solved early. For example, if an unfamiliar technology is being intro-\nduced on the project, prototypes using that technology will likely be devel-\noped to gain understanding of its implications. These prototypes are most \nuseful if they reflect the design of the actual system. A depth-first strategy \ncan provide a context for technology prototyping. Using this context you \ncan build the prototype in a fashion that allows for its eventual integration \ninto the architecture. On the other hand, if the risk is in how elements at the \nsame level of the design interact with each other to meet critical quality at-\ntributes, then a breadth-first strategy is in order.\n■\n■Deferral of some functionality or quality attribute concerns may dictate a \nmixed approach. For example, suppose the system being constructed has \na medium-priority availability requirement. In this case you might adopt \na strategy of employing redundancy for availability but defer detailed \nconsideration of this redundancy strategy to allow for the rapid generation \nof the high-priority functionality in an intermediate release. You might \ntherefore apply a breadth-first approach for everything but availability, \nand then in subsequent design iterations you revisit some of the elements \nto enable the addition of the responsibilities to support availability. In \nreality this approach will require some backtracking, where you revisit \nearlier decisions and refine them or modify them to accommodate this new \nrequirement.\nAll else being equal, a breadth-first refinement strategy is preferred because \nit allows you to apportion the most work to the most teams soonest. Breadth first \nallows for consideration of the interaction among the elements at the same level.\nStep 2: Identify the ASRs for This Element\nIn Chapter 16 we described a number of methods for discovering the ASRs for \na system. One of those methods involved building a utility tree. To support the \ndesign process, the utility tree has an advantage over the other methods: it guides \nthe stakeholders in prioritizing the QA requirements. The two factors used to pri-\noritize the ASRs in a utility tree are business value and architectural impact. The \nbusiness value of an ASR typically will not change throughout the design process \nand does not need to be reconsidered. \n",
      "page_number": 325
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 341-348)",
      "start_page": 341,
      "end_page": 348,
      "detection_method": "topic_boundary",
      "content": "17.3  The Steps of ADD\n321\nIf the chosen element for design in step 1 is the whole system, then a utility \ntree can be a good source for the ASRs. Otherwise, construct a utility tree spe-\ncifically focused on this chosen element, using the quality attribute requirements \nthat apply to this element (you’ll see how to assign those in step 4). Those that \nare labeled (High, High) are the ASRs for this element. As an architect you will \nalso need to pay attention to the (High, Medium) and (Medium, High) utility tree \nleaves as well. These will almost certainly also be ASRs for this element.\nStep 3: Generate a Design Solution for the Chosen Element\nThis step is the heart of the ADD. It is the application of the generate-and-test \nstrategy. Upon entry to this step, we have a chosen element for design and a list \nof ASRs that apply to it. For each ASR, we develop a solution by choosing a can-\ndidate design approach. \nYour initial candidate design will likely be inspired by a pattern, possibly \naugmented by one or more tactics. You may then refine this candidate design by \nconsidering the design checklists that we gave for the quality attributes in Chap-\nters 5–11. For ASRs that correspond to quality attributes, you can invoke those \nchecklists to help you instantiate or refine the major design approach (such as a \npattern) that you’ve chosen. For example, the layered pattern is helpful for build-\ning systems in which modifiability is important, but the pattern does not tell you \nhow many layers you should have or what each one’s responsibility should be. \nBut the checklist for the “allocation of responsibilities” design decision category \nfor modifiability in Chapter 7 will help you ask the right questions to make that \ndetermination.\nAlthough this step is performed for each ASR in turn, the sources of de-\nsign candidates outlined above—patterns, tactics, and checklists—will usually do \nmuch better than that. That is, you’re likely to find design candidates that address \nseveral of your ASRs at once. This is because to the extent that the system you’re \nbuilding is similar to others you know about, or to the extent that the problem you \nare solving is similar to the problems solved by patterns, it is likely that the solu-\ntions you choose will be solving a whole collection of ASRs simultaneously. If \nyou can bring a solution to bear that solves more than one of your ASRs at once, \nso much the better.\nThe design decisions made in this step now become constraints on all future \nsteps of the method.\nStep 4: Verify and Refine Requirements and \nGenerate Input for the Next Iteration\nIt’s possible that the design solution you came up with in the prior step won’t \nsatisfy all the ASRs. Step 4 of ADD is a test step that is applied to your design for \nthe element you chose to elaborate in step 1 of this iteration. One of the possible \n\n\n322 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\noutcomes of step 4 is “backtrack,” meaning that an important requirement was \nnot satisfied and cannot be satisfied by further elaborating this design. In this \ncase, the design needs to be reconsidered.\nThe ASRs you have not yet satisfied could be related to the following:\n1.\t\nA quality attribute requirement allocated to the parent element\n2.\t\nA functional responsibility of the parent element\n3.\t\nOne or more constraints on the parent element\nTable 17.1 summarizes the types of problems and the actions we recom-\nmend for each.\nIn most real-world systems, requirements outstrip available time and re-\nsources. Consequently you will find yourself unable to meet some of the QA \nrequirements, functional requirements, and constraints. These kinds of decisions \nare outside the scope of the ADD method, but they are clearly important drivers \nof the design process, and as an architect you will be continually negotiating de-\ncisions of this form.\nStep 4 is about taking stock and seeing what requirements are left that still \nhave not been satisfied by our design so far. At this point you should sequence \nthrough the quality attribute requirements, responsibilities, and constraints for the \nelement just designed. For each one there are four possibilities:\nTable 17.1  Recommended Actions for Problems with the Current Hypothesis\nType of ASR Not Met\nAction Recommended\n1. Quality attribute requirement\nConsider applying (more) tactics to improve the \ndesign with respect to the quality attribute. For \neach candidate tactic, ask: \n■\n■\nWill this tactic improve the quality attribute \nbehavior of the current design sufficiently?\n■\n■\nShould this tactic be used in conjunction with \nanother tactic?\n■\n■\nWhat are the tradeoff considerations when \napplying this tactic?\n2. Functional responsibility\nAdd responsibilities either to existing modules or to \nnewly created modules:\n■\n■\nAssign the responsibility to a module containing \nsimilar responsibilities.\n■\n■\nBreak a module into portions when it is too \ncomplex.\n■\n■\nAssign the responsibility to a module containing \nresponsibilities with similar quality attribute \ncharacteristics—for example, similar timing \nbehavior, similar security requirements, or \nsimilar availability requirements. \n3. Constraint\nModify the design or try to relax the constraint:\n■\n■\nModify the design to accommodate the \nconstraint. \n■\n■\nRelax the constraint.\n\n\n17.3  The Steps of ADD\n323\n1.\t\nThe quality attribute requirement, functional requirement, or constraint \nhas been satisfied. In this case, the design with respect to that requirement \nis complete; the next time around, when you further refine the design, \nthis requirement will not be considered. For example, if a constraint is to \nuse a particular middleware and the system is decomposed into elements \nthat all use this middleware, the constraint has been satisfied and can be \nremoved from consideration. An example of a quality attribute requirement \nbeing satisfied is a requirement to make it easy to modify elements and \ntheir interactions. If a publish-subscribe pattern can be shown to have been \nemployed throughout the system, then this QA requirement can be said to \nbe satisfied. \n2.\t\nThe quality attribute requirement, functional requirement, or constraint \nis delegated to one of the children. For example, if a constraint is to use a \nparticular middleware and the decomposition has a child element that acts \nas the infrastructure, then delegating that constraint to that child will retain \nthe constraint and have it be reconsidered when the infrastructure element \nis chosen for subsequent design. Similarly, with the example we gave ear-\nlier about providing extensibility, if there is as yet no identifiable plug-in \nmanager, then this requirement is delegated to the child where the plug-in \nmanager is likely to appear.\n3.\t\nThe quality attribute requirement, functional requirement, or constraint is \ndistributed among the children. For example, a constraint might be to use \n.NET. In this case, .NET Remoting might become a constraint on one child \nand ASP.NET on another. Or a quality attribute requirement that constrains \nend-to-end latency of a certain operation to 2 seconds could be distributed \namong the element’s three children so that the latency requirement for one \nelement is 0.8 seconds, the latency for a second element is 0.9 seconds, and \nthe latency for a third is 0.3 seconds. When those elements are subsequent-\nly chosen for further design, those times will serve as constraints on them \nindividually.\n4.\t\nThe quality attribute requirement, functional requirement, or constraint \ncannot be satisfied with the current design. In this case there are the same \ntwo options we discussed previously: you can either backtrack—revisit the \ndesign to see if the constraint or quality attribute requirement can be sat-\nisfied some other way—or push back on the requirement. This will almost \ncertainly involve the stakeholders who care about that requirement, and you \nshould have convincing arguments as to why the dropping of the require-\nment is necessary.\nReport to the project manager that the constraint cannot be satisfied \nwithout jeopardizing other requirements. You must be prepared to justify \nsuch an assertion. Essentially, this is asking, “What’s more important—the \nconstraint or these other requirements?”\n\n\n324 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nStep 5: Repeat Steps 1–4 Until Done\nAfter the prior steps, each element has a set of responsibilities, a set of quality \nattribute requirements, and a set of constraints assigned to it. If it’s clear that all \nof the requirements are satisfied, then this unequivocally ends the ADD process.\nIn projects in which there is a high degree of trust between you and the im-\nplementation teams, the ADD process can be terminated when only a sketch of \nthe architecture is available. This could be as soon as two levels of breadth-first \ndesign, depending on the size of the system. In this case, you trust the implemen-\ntation team to be able to flesh out the architecture design in a manner consistent \nwith the overall design approaches you have laid out. The test for this is if you \nbelieve that you could begin implementation with the level of detail available and \ntrust the implementation team to that extent. If you have less trust in the imple-\nmentation team, then an additional level (or levels) of design may be necessary. \n(And, of course, you will need to subsequently ensure that the implementation is \nfaithfully followed by the team.)\nOn the other hand, if there is a contractual arrangement between your or-\nganization and the implementation organization, then the specification of the \nportion of the system that the implementers are providing must be legally en-\nforceable. This means that the ADD process must continue until that level of \nspecificity has been achieved. \nFinally, another condition for terminating ADD is when the project’s design \nbudget has been exhausted. This happens more often than you might think. \nChoosing when to terminate ADD and when to start releasing the architec-\nture that you’ve sketched out are not the same decision. You can, and in many \ncases should, start releasing early architectural views based on the needs of the \nproject (such as scheduled design reviews or customer presentations) and your \nconfidence in the design so far. The unpalatable alternative is to make everyone \nwait until the architecture design is finished. You-can’t-have-it-until-it’s-done is \nparticularly unpalatable in Agile projects, as we discussed in Chapter 15. \nYou should release the documentation with a caveat as to how likely you \nthink it is to change. But even early broad-and-shallow architectural descriptions \ncan be enormously helpful to implementers and other project staff. A first- or sec-\nond-level module decomposition view, for instance, lets experts start scouring the \nmarketplace for commercial products that provide the responsibilities of the iden-\ntified modules. Managers can start making budgets and schedules for implemen-\ntation that are based on the architecture and not just the requirements. Support \nstaff can start building the infrastructure and file systems to hold project artifacts \n(these are often structured to mirror the module decomposition view). And early \nrelease invites early feedback.\n\n\n17.5  For Further Reading\n325\n17.4  Summary\nThe Attribute-Driven Design method is an application of the generate-and-test \nphilosophy. It keeps the number of requirements that must be satisfied to a hu-\nmanly achievable quantity. ADD is an iterative method that, at each iteration, \nhelps the architect to do the following:\n■\n■Choose an element of the system to design.\n■\n■Marshal all the architecturally significant requirements for the chosen \nelement.\n■\n■Create and test a design for that chosen element.\nThe output of ADD is not an architecture complete in every detail, but an ar-\nchitecture in which the main design approaches have been selected and validated. \nIt produces a “workable” architecture early and quickly, one that can be given to \nother project teams so they can begin their work while the architect or architec-\nture team continues to elaborate and refine.\nADD is a five-step method:\n1.\t\nChoose the element of the system to design. For green-field designs, the \n“part” to begin with is simply the entire system. For designs that are already \npartially completed (either by external constraints or by previous iterations \nthrough ADD), the part is an element that is not yet designed. Choosing the \nnext element can proceed in a breadth-first, depth-first, or mixed manner.\n2.\t\nIdentify the ASRs for the chosen element.\n3.\t\nGenerate a design solution for the chosen element, using design collateral \nsuch as existing systems, frameworks, patterns and tactics, and the design \nchecklists from Chapters 5–11.\n4.\t\nVerify and refine requirements and generate input for the next iteration. \nEither the design in step 3 will satisfy all of the chosen element’s ASRs or \nit won’t. If it doesn’t, then either they can be allocated to elements that will \nbe elaborated in future iterations of ADD, or the existing design is inad-\nequate and we must backtrack. Furthermore, non-ASR requirements will \neither be satisfied, allocated to children, or indicated as not achievable.\n5.\t\nRepeat steps 1–4 until all the ASRs have been satisfied, or until the archi-\ntecture has been elaborated sufficiently for the implementers to use it. \n17.5  For Further Reading\nYou can view design as the process of making decisions; this is another philoso-\nphy of design. This view of design leads to an emphasis on design rationale and \ntools to capture design rationale. The view of design as the process of making \n\n\n326 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\ndecisions dates to the 1940s [Mettler 91], but it has been recently applied to ar-\nchitecture design most prominently by Philippe Kruchten [Kruchten 04], and \nHans van Vliet and Jan Bosch [van Vliet 05].\nThe Software Engineering Institute has produced a number of reports de-\nscribing the ADD method and its application in a variety of contexts. These in-\nclude [Wojcik 06], [Kazman 04], and [Wood 07].\nGeorge Fairbanks has written an engaging book that describes a risk-driven \nprocess of architecture design, entitled Just Enough Software Architecture: A \nRisk-Driven Approach [Fairbanks 10].\nTony Lattanze has created an Architecture-Centric Design Method (ACDM), \ndescribed in his book Architecting Software Intensive Systems: A Practitioners \nGuide [Lattanze 08].\nIan Gorton’s Essential Architecture, Second Edition, emphasizes the middle-\nware aspects of a design [Gorton 10].\nWoods and Rozanski have written Software Systems Architecture, Second \nEdition, which interprets the design process through the prism of different views \n[Woods 11].\nA number of authors have compared five different industrial architecture de-\nsign methods. You can find this comparison at [Hofmeister 07].\nRaghvinder Sangwan and his colleagues describe the design of a building \nmanagement system that was originally designed using object-oriented tech-\nniques and then was redesigned using ADD [Sangwan 08].\n17.6  Discussion Questions\n1.\t\nADD does not help with the detailed design of interfaces for the \narchitectural elements it identifies. Details of an interface include what each \nmethod does, whether you need to call a single all-encompassing method \nto perform the work of the element or many methods of finer-grained \nfunction, what exceptions are raised on the interface, and more. What are \nsome examples where the specific design of an interface might bring more \nor less performance, security, or availability to a system? (By the way, if \nthere are quality attribute implications to an interface, you can capture those \nas annotations on the element.)\n2.\t\nWhat sets a constraint apart from other (even high-priority) requirements is \nthat it is not negotiable. Should this consideration guide the design process? \nFor example, would it be wise to design to satisfy all of the constraints be-\nfore worrying about other ASRs?\n3.\t\nIn discussion question 4 of Chapter 16 you were asked to create a utility \ntree for an ATM. Now choose the two most important ASRs from that util-\nity tree and create a design fragment using the ADD method employing and \ninstantiating a pattern.\n\n\n327\n18\nDocumenting Software \nArchitectures\nIf it is not written down, it does not exist.\n—Philippe Kruchten\nEven the best architecture, the most perfectly suited for the job, will be essen-\ntially useless if the people who need to use it do not know what it is; cannot un-\nderstand it well enough to use, build, or modify it; or (worst of all) misunderstand \nit and apply it incorrectly. And all of the effort, analysis, hard work, and insight-\nful design on the part of the architecture team will have been wasted. They might \nas well have gone on vacation for all the good their architecture will do.\nCreating an architecture isn’t enough. It has to be communicated in a way \nto let its stakeholders use it properly to do their jobs. If you go to the trouble of \ncreating a strong architecture, one that you expect to stand the test of time, then \nyou must go to the trouble of describing it in enough detail, without ambiguity, \nand organizing it so that others can quickly find and update needed information. \nDocumentation speaks for the architect. It speaks for the architect today, \nwhen the architect should be doing other things besides answering a hundred \nquestions about the architecture. And it speaks for the architect tomorrow, when \nhe or she has left the project and now someone else is in charge of its evolution \nand maintenance.\nThe sad truth is that architectural documentation today, if it is done at all, \nis often treated as an afterthought, something people do because they have to. \nMaybe a contract requires it. Maybe a customer demands it. Maybe a compa-\nny’s standard process calls for it. In fact, these may all be legitimate reasons. \nBut none of them are compelling enough to produce high-quality documentation. \nWhy should the architect spend valuable time and energy just so a manager can \ncheck off a deliverable?\n\n\n328  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe best architects produce good documentation not because it’s “required” \nbut because they see that it is essential to the matter at hand—producing a \nhigh-quality product, predictably and with as little rework as possible. They see \ntheir immediate stakeholders as the people most intimately involved in this un-\ndertaking: developers, deployers, testers, and analysts. \nBut architects also see documentation as delivering value to themselves. \nDocumentation serves as the receptacle to hold the results of major design deci-\nsions as they are confirmed. A well-thought-out documentation scheme can make \nthe process of design go much more smoothly and systematically. Documenta-\ntion helps the architect(s) reason about the architecture design and communicate \nit while the architecting is in progress, whether in a six-month design phase or a \nsix-day Agile sprint.\n18.1  \u0007Uses and Audiences for \nArchitecture Documentation\nArchitecture documentation must serve varied purposes. It should be sufficiently \ntransparent and accessible to be quickly understood by new employees. It should \nbe sufficiently concrete to serve as a blueprint for construction. It should have \nenough information to serve as a basis for analysis. \nArchitecture documentation is both prescriptive and descriptive. For some \naudiences, it prescribes what should be true, placing constraints on decisions yet \nto be made. For other audiences, it describes what is true, recounting decisions \nalready made about a system’s design.\nThe best architecture documentation for, say, performance analysis may well \nbe different from the best architecture documentation we would wish to hand to \nan implementer. And both of these will be different from what we put in a new \nhire’s “welcome aboard” package or a briefing we put together for an executive. \nWhen planning and reviewing documentation, you need to ensure support for all \nthe relevant needs.\nWe can see that many different kinds of people are going to have a vested \ninterest in an architecture document. They hope and expect that the architecture \ndocument will help them do their respective jobs. Understanding their uses of \narchitecture documentation is essential, as those uses determine the important \ninformation to capture. \nFundamentally, architecture documentation has three uses:\n1.\t\nArchitecture documentation serves as a means of education. The education-\nal use consists of introducing people to the system. The people may be new \nmembers of the team, external analysts, or even a new architect. In many \ncases, the “new” person is the customer to whom you’re showing your \n",
      "page_number": 341
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 349-359)",
      "start_page": 349,
      "end_page": 359,
      "detection_method": "topic_boundary",
      "content": "18.2  Notations for Architecture Documentation\n329\nsolution for the first time, a presentation you hope will result in funding or \ngo-ahead approval.\n2.\t\nArchitecture documentation serves as a primary vehicle for communication \namong stakeholders. An architecture’s precise use as a communication ve-\nhicle depends on which stakeholders are doing the communicating. \nPerhaps one of the most avid consumers of architecture documentation \nis none other than the architect in the project’s future. The future architect \nmay be the same person or may be a replacement, but in either case he or \nshe is guaranteed to have an enormous stake in the documentation. New \narchitects are interested in learning how their predecessors tackled the dif-\nficult issues of the system and why particular decisions were made. Even if \nthe future architect is the same person, he or she will use the documentation \nas a repository of thought, a storehouse of design decisions too numerous \nand hopelessly intertwined to ever be reproducible from memory alone. See \nthe sidebar “Schmucks and Jerks.”\n3.\t\nArchitecture documentation serves as the basis for system analysis and con-\nstruction. Architecture tells implementers what to implement. Each module \nhas interfaces that must be provided and uses interfaces from other mod-\nules. Not only does this provide instructions about the provided and used \ninterfaces, but it also determines with what other teams the development \nteam for the module must communicate.\nDuring development, an architecture can be very complex, with many is-\nsues left to resolve. Documentation can serve as a receptacle for registering \nand communicating these issues that might otherwise be overlooked.\nFor those interested in the ability of the design to meet the system’s \nquality objectives, the architecture documentation serves as the fodder for \nevaluation. It must contain the information necessary to evaluate a vari-\nety of attributes, such as security, performance, usability, availability, and \nmodifiability.\nFor system builders who use automatic code-generation tools, the doc-\numentation may incorporate the models used for generation. These models \nprovide guidance to those who wish to understand the behavior of the mod-\nule in more detail than is normally documented but in less detail than exam-\nining the code would provide.\n18.2  Notations for Architecture Documentation\nNotations for documenting views differ considerably in their degree of formality. \nRoughly speaking, there are three main categories of notation:\n\n\n330  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n■\n■Informal notations. Views are depicted (often graphically) using gener-\nal-purpose diagramming and editing tools and visual conventions chosen \nfor the system at hand. The semantics of the description are characterized in \nnatural language, and they cannot be formally analyzed. In our experience, \nthe most common tool for informal notations is PowerPoint.\n■\n■Semiformal notations. Views are expressed in a standardized notation that \nprescribes graphical elements and rules of construction, but it does not \nprovide a complete semantic treatment of the meaning of those elements. \nRudimentary analysis can be applied to determine if a description satisfies \nsyntactic properties. UML is a semiformal notation in this sense.\n■\n■Formal notations. Views are described in a notation that has a precise (usu-\nally mathematically based) semantics. Formal analysis of both syntax and \nsemantics is possible. There are a variety of formal notations for software \narchitecture available. Generally referred to as architecture description lan-\nguages (ADLs), they typically provide both a graphical vocabulary and an \nunderlying semantics for architecture representation. In some cases these \nnotations are specialized to particular architectural views. In others they allow \nmany views, or even provide the ability to formally define new views. The \nusefulness of ADLs lies in their ability to support automation through associ-\nated tools: automation to provide useful analysis of the architecture or assist \nin code generation. In practice, the use of such notations is rare.\nSchmucks and Jerks\nOne day I was sitting in a meeting with a well-known compiler guru. He \nwas recounting some of his favorite war stories from his long career. One of \nthese stories particularly stuck with me. He was talking about the time that \nhe was chasing down a very nasty and subtle bug in the code of a compiler \nthat he was maintaining. After a long and exasperating search, he finally \nlocated and eventually fixed the bug. But the search itself had gotten him \nso worked up, and he was so infuriated at the irresponsible thought and \nprogramming that led to the bug, that he decided to do a bit more detective \nwork and figure out who was the jerk responsible for that bug. \nBy going backward through the revision history, he found the culprit. It \nwas him. He was the jerk. It turns out that he was the one who—eight years \nearlier—had written that offending piece of code. The trouble was, he had \nno recollection of writing the code and no recollection of the rationale for \nwriting it the way he had done. Perhaps there was a good reason to do so \nat the time, but if so it was lost now. \nThat is why we document. The documentation helps the poor schmuck \nwho has to maintain your code in the future, and that schmuck might very \nwell be you!\n—RK\n\n\n18.3  Views\n331\nDetermining which form of notation to use involves making several \ntradeoffs. Typically, more formal notations take more time and effort to create \nand understand, but they repay this effort in reduced ambiguity and more oppor-\ntunities for analysis. Conversely, more informal notations are easier to create, but \nthey provide fewer guarantees. \nRegardless of the level of formality, always remember that different no-\ntations are better (or worse) for expressing different kinds of information. For-\nmality aside, no UML class diagram will help you reason about schedulability, \nnor will a sequence chart tell you very much about the system’s likelihood of \nbeing delivered on time. You should choose your notations and representation \nlanguages always keeping in mind the important issues you need to capture and \nreason about.\n18.3  Views\nPerhaps the most important concept associated with software architecture docu-\nmentation is that of the view. A software architecture is a complex entity that can-\nnot be described in a simple one-dimensional fashion. A view is a representation \nof a set of system elements and relations among them—not all system elements, \nbut those of a particular type. For example, a layered view of a system would \nshow elements of type “layer”—that is, it would show the system’s decompo-\nsition into layers—and the relations among those layers. A pure layered view \nwould not, however, show the system’s services, or clients and servers, or data \nmodel, or any other type of element.\nThus, views let us divide the multidimensional entity that is a software ar-\nchitecture into a number of (we hope) interesting and manageable representations \nof the system. The concept of views gives us our most fundamental principle of \narchitecture documentation: \nDocumenting an architecture is a matter of documenting the relevant \nviews and then adding documentation that applies to more than one view.\nThis maxim gives our approach to documentation its name: Views and \nBeyond.\nWhat are the relevant views? This depends entirely on your goals. As we \nsaw previously, architecture documentation can serve many purposes: a mission \nstatement for implementers, a basis for analysis, the specification for automatic \ncode generation, the starting point for system understanding and asset recovery, \nor the blueprint for project planning. \nDifferent views also expose different quality attributes to different degrees. \nTherefore, the quality attributes that are of most concern to you and the other \nstakeholders in the system’s development will affect the choice of what views to \n\n\n332  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\ndocument. For instance, a layered view will let you reason about your system’s \nportability, a deployment view will let you reason about your system’s perfor-\nmance and reliability, and so forth.\nDifferent views support different goals and uses. This is why we do not ad-\nvocate a particular view or collection of views. The views you should document \ndepend on the uses you expect to make of the documentation. Different views \nwill highlight different system elements and relations. How many different views \nto represent is the result of a cost/benefit decision. Each view has a cost and a \nbenefit, and you should ensure that the benefits of maintaining a particular view \noutweigh its costs.\nViews may be driven by the need to document a particular pattern in your \ndesign. Some patterns are composed of modules, others of components and con-\nnectors, and still others have deployment considerations. Module views, compo-\nnent-and-connector (C&C) views, and allocation views are the appropriate mech-\nanisms for representing these considerations.\nModule Views \nA module is an implementation unit that provides a coherent set of responsibili-\nties. A module might take the form of a class, a collection of classes, a layer, an \naspect, or any decomposition of the implementation unit. Example module views \nare decomposition, uses, and layers. Every module has a collection of properties \nassigned to it. These properties are intended to express the important information \nassociated with the module, as well as constraints on the module. Sample proper-\nties are responsibilities, visibility information, and revision history. The relations \nthat modules have to one another include is part of, depends on, and is a.\nThe way in which a system’s software is decomposed into manageable \nunits remains one of the important forms of system structure. At a minimum, \nthis determines how a system’s source code is decomposed into units, what kinds \nof assumptions each unit can make about services provided by other units, and \nhow those units are aggregated into larger ensembles. It also includes global data \nstructures that impact and are impacted by multiple units. Module structures of-\nten determine how changes to one part of a system might affect other parts and \nhence the ability of a system to support modifiability, portability, and reuse.\nIt is unlikely that the documentation of any software architecture can be \ncomplete without at least one module view.\nTable 18.1 summarizes the elements, relations, constraints, and purpose of \nthe module views in general. Later we provide this information specific to each \nof a number of often used module views. \nProperties of modules that help to guide implementation or are input to anal-\nysis should be recorded as part of the supporting documentation for a module \nview. The list of properties may vary but is likely to include the following:\n\n\n18.3  Views\n333\nTable 18.1  Summary of the Module Views\nElements\nModules, which are implementation units of software that \nprovide a coherent set of responsibilities.\nRelations\n■\n■\nIs part of, which defines a part/whole relationship between \nthe submodule—the part—and the aggregate module—the \nwhole.\n■\n■\nDepends on, which defines a dependency relationship be-\ntween two modules. Specific module views elaborate what \ndependency is meant.\n■\n■\nIs a, which defines a generalization/specialization relation-\nship between a more specific module—the child—and a \nmore general module—the parent.\nConstraints\nDifferent module views may impose specific topological \nconstraints, such as limitations on the visibility between \nmodules.\nUsage\n■\n■\nBlueprint for construction of the code\n■\n■\nChange-impact analysis\n■\n■\nPlanning incremental development\n■\n■\nRequirements traceability analysis\n■\n■\nCommunicating the functionality of a system and the struc-\nture of its code base\n■\n■\nSupporting the definition of work assignments, implementa-\ntion schedules, and budget information\n■\n■\nShowing the structure of information that the system needs \nto manage\n■\n■Name. A module’s name is, of course, the primary means to refer to it. A \nmodule’s name often suggests something about its role in the system. In ad-\ndition, a module’s name may reflect its position in a decomposition hierar-\nchy; the name A.B.C, for example, refers to a module C that is a submodule \nof a module B, itself a submodule of A.\n■\n■Responsibilities. The responsibility property for a module is a way to iden-\ntify its role in the overall system and establishes an identity for it beyond \nthe name. Whereas a module’s name may suggest its role, a statement of \nresponsibility establishes it with much more certainty. Responsibilities \nshould be described in sufficient detail to make clear to the reader what \neach module does.\n■\n■Visibility of interface(s). When a module has submodules, some interfaces \nof the submodules are public and some may be private; that is, the inter-\nfaces are used only by the submodules within the enclosing parent module. \nThese private interfaces are not visible outside that context. \n■\n■Implementation information. Modules are units of implementation. It is \ntherefore useful to record information related to their implementation from \nthe point of view of managing their development and building the system \nthat contains them. This might include the following:\n\n\n334  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n■\n■Mapping to source code units. This identifies the files that constitute the \nimplementation of a module. For example, a module Account, if imple-\nmented in Java, might have several files that constitute its implementa-\ntion: IAccount.java (an interface), AccountImpl.java (the implementation \nof Account functionality), AccountBean.java (a class to hold the state of \nan account in memory), AccountOrmMapping.xml (a file that defines the \nmapping between AccountBean and a database table—object-relational \nmapping), and perhaps even a unit test AccountTest.java.\n■\n■Test information. The module’s test plan, test cases, test scaffolding, and \ntest data are important to document. This information may simply be a \npointer to the location of these artifacts.\n■\n■Management information. A manager may need information about the \nmodule’s predicted schedule and budget. This information may simply be \na pointer to the location of these artifacts.\n■\n■Implementation constraints. In many cases, the architect will have an \nimplementation strategy in mind for a module or may know of constraints \nthat the implementation must follow.\n■\n■Revision history. Knowing the history of a module including authors and \nparticular changes may help when you perform maintenance activities.\nBecause modules partition the system, it should be possible to determine how \nthe functional requirements of a system are supported by module responsibilities. \nModule views that show dependencies among modules or layers (which are groups \nof modules that have a specific pattern of allowed usage) provide a good basis for \nchange-impact analysis. Modules are typically modified as a result of problem re-\nports or change requests. Impact analysis requires a certain degree of design com-\npleteness and integrity of the module description. In particular, dependency informa-\ntion has to be available and correct to be able to create useful results. \nA module view can be used to explain the system’s functionality to some-\none not familiar with it. The various levels of granularity of the module decom-\nposition provide a top-down presentation of the system’s responsibilities and \ntherefore can guide the learning process. For a system whose implementation is \nalready in place, module views, if kept up to date, are helpful, as they explain \nthe structure of the code base to a new developer on the team. Thus, up-to-date \nmodule views can simplify and regularize system maintenance. \nOn the other hand, it is difficult to use the module views to make inferences \nabout runtime behavior, because these views are just a static partition of the func-\ntions of the software. Thus, a module view is not typically used for analysis of \nperformance, reliability, and many other runtime qualities. For those, we rely on \ncomponent-and-connector and allocation views.\nModule views are commonly mapped to component-and-connector views. \nThe implementation units shown in module views have a mapping to components \nthat execute at runtime. Sometimes, the mapping is quite straightforward, even \none-to-one for small, simple applications. More often, a single module will be \n\n\n18.3  Views\n335\nreplicated as part of many runtime components, and a given component could \nmap to several modules. Module views also provide the software elements that \nare mapped to the diverse nonsoftware elements of the system environment in the \nvarious allocation views.\nComponent-and-Connector Views \nComponent-and-connector views show elements that have some runtime pres-\nence, such as processes, objects, clients, servers, and data stores. These elements \nare termed components. Additionally, component-and-connector views include as \nelements the pathways of interaction, such as communication links and proto-\ncols, information flows, and access to shared storage. Such interactions are rep-\nresented as connectors in C&C views. Sample C&C views are service-oriented \narchitecture (SOA), client-server, or communicating process views.\nComponents have interfaces called ports. A port defines a point of potential \ninteraction of a component with its environment. A port usually has an explicit \ntype, which defines the kind of behavior that can take place at that point of in-\nteraction. A component may have many ports of the same type, each forming a \ndifferent input or output channel at runtime. In this respect ports differ from inter-\nfaces of modules, whose interfaces are never replicated. You can annotate a port \nwith a number or range of numbers to indicate replication; for example, “1..4” \nmight mean that an interface could be replicated up to four times. A component’s \nports should be explicitly documented, by showing them in the diagram and de-\nfining them in the diagram’s supporting documentation.\nA component in a C&C view may represent a complex subsystem, which \nitself can be described as a C&C subarchitecture. This subarchitecture can be \ndepicted graphically in situ when the substructure is not too complex, by showing \nit as nested inside the component that it refines. Often, however, it is documented \nseparately. A component’s subarchitecture may employ a different pattern than \nthe one in which the component appears. \nConnectors are the other kind of element in a C&C view. Simple examples \nof connectors are service invocation; asynchronous message queues; event multi-\ncast supporting publish-subscribe interactions; and pipes that represent asynchro-\nnous, order-preserving data streams. Connectors often represent much more com-\nplex forms of interaction, such as a transaction-oriented communication channel \nbetween a database server and a client, or an enterprise service bus that mediates \ninteractions between collections of service users and providers.\nConnectors have roles, which are its interfaces, defining the ways in which \nthe connector may be used by components to carry out interaction. For exam-\nple, a client-server connector might have invokes-services and provides-services \nroles. A pipe might have writer and reader roles. Like component ports, connec-\ntor roles differ from module interfaces in that they can be replicated, indicating \n\n\n336  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nhow many components can be involved in its interaction. A publish-subscribe \nconnector might have many instances of the publisher and subscriber roles. \nLike components, complex connectors may in turn be decomposed into col-\nlections of components and connectors that describe the architectural substruc-\nture of those connectors. Connectors need not be binary. That is, they need not \nhave exactly two roles. For example, a publish-subscribe connector might have \nan arbitrary number of publisher and subscriber roles. Even if the connector is ul-\ntimately implemented using binary connectors, such as a procedure call, it can be \nuseful to adopt n-ary connector representations in a C&C view. Connectors em-\nbody a protocol of interaction. When two or more components interact, they must \nobey conventions about order of interactions, locus of control, and handling of \nerror conditions and timeouts. The protocol of interaction should be documented.\nThe primary relation within a C&C view is attachment. Attachments indicate \nwhich connectors are attached to which components, thereby defining a system as a \ngraph of components and connectors. Specifically, an attachment is denoted by as-\nsociating (attaching) a component’s port to a connector’s role. A valid attachment is \none in which the ports and roles are compatible with each other, under the semantic \nconstraints defined by the view. Compatibility often is defined in terms of informa-\ntion type and protocol. For example, in a call-return architecture, you should check \nto make sure that all “calls” ports are attached to some call-return connector. At a \ndeeper semantic level, you should check to make sure that a port’s protocol is con-\nsistent with the behavior expected by the role to which it is attached.\nAn element (component or connector) of a C&C view will have various \nassociated properties. Every element should have a name and type. Additional \nproperties depend on the type of component or connector. Define values for the \nproperties that support the intended analyses for the particular C&C view. For \nexample, if the view will be used for performance analysis, latencies, queue ca-\npacities, and thread priorities may be necessary. The following are examples of \nsome typical properties and their uses:\n■\n■Reliability. What is the likelihood of failure for a given component or connec-\ntor? This property might be used to help determine overall system availability.\n■\n■Performance. What kinds of response time will the component provide un-\nder what loads? What kind of bandwidth, latency, jitter, transaction volume, \nor throughput can be expected for a given connector? This property can \nbe used with others to determine system-wide properties such as response \ntimes, throughput, and buffering needs.\n■\n■Resource requirements. What are the processing and storage needs of a \ncomponent or a connector? This property can be used to determine whether \na proposed hardware configuration will be adequate.\n■\n■Functionality. What functions does an element perform? This property can \nbe used to reason about overall computation performed by a system.\n■\n■Security. Does a component or a connector enforce or provide security fea-\ntures, such as encryption, audit trails, or authentication? This property can \nbe used to determine system security vulnerabilities.\n\n\n18.3  Views\n337\n■\n■Concurrency. Does this component execute as a separate process or thread? \nThis property can help to analyze or simulate the performance of concur-\nrent components and identify possible deadlocks.\n■\n■Modifiability. Does the messaging structure support a structure to cater for \nevolving data exchanges? Can the components be adapted to process those \nnew messages? This property can be defined to extend the functionality of a \ncomponent.\n■\n■Tier. For a tiered topology, what tier does the component reside in? This \nproperty helps to define the build and deployment procedures, as well as \nplatform requirements for each tier.\nC&C views are commonly used to show to developers and other stakehold-\ners how the system works—one can “animate” or trace through a C&C view, \nshowing an end-to-end thread of activity. C&C views are also used to reason \nabout runtime system quality attributes, such as performance and availability. In \nparticular, a well-documented view allows architects to predict overall system \nproperties such as latency or reliability, given estimates or measurements of prop-\nerties of the individual elements and their interactions. \nTable 18.2 summarizes the elements, relations, and properties that can ap-\npear in C&C views. This table is followed by a more detailed discussion of these \nconcepts, together with guidelines concerning their documentation. \nTable 18.2  Summary of Component-and-Connector Views\nElements\n■\n■\nComponents. Principal processing units and data stores. A compo-\nnent has a set of ports through which it interacts with other compo-\nnents (via connectors).\n■\n■\nConnectors. Pathways of interaction between components. Connec-\ntors have a set of roles (interfaces) that indicate how components \nmay use a connector in interactions.\nRelations\n■\n■\nAttachments. Component ports are associated with connector roles \nto yield a graph of components and connectors.\n■\n■\nInterface delegation. In some situations component ports are associ-\nated with one or more ports in an “internal” subarchitecture. The case \nis similar for the roles of a connector. \nConstraints\n■\n■\nComponents can only be attached to connectors, not directly to other \ncomponents.\n■\n■\nConnectors can only be attached to components, not directly to other \nconnectors.\n■\n■\nAttachments can only be made between compatible ports and roles.\n■\n■\nInterface delegation can only be defined between two compatible \nports (or two compatible roles).\n■\n■\nConnectors cannot appear in isolation; a connector must be attached \nto a component.\nUsage\n■\n■\nShow how the system works.\n■\n■\nGuide development by specifying structure and behavior of runtime \nelements.\n■\n■\nHelp reason about runtime system quality attributes, such as perfor-\nmance and availability. \n\n\n338 \nPart Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nNotations for C&C Views\nAs always, box-and-line drawings are available to represent C&C views. Al-\nthough informal notations are limited in the semantics that can be conveyed, \nfollowing some simple guidelines can lend rigor and depth to the descriptions. \nThe primary guideline is simple: assign each component type and each connector \ntype a separate visual form (symbol), and list each of the types in a key.\nUML components are a good semantic match to C&C components because \nthey permit intuitive documentation of important information like interfaces, \nproperties, and behavioral descriptions. UML components also distinguish be-\ntween component types and component instances, which is useful when defining \nview-specific component types.\nUML ports are a good semantic match to C&C ports. A UML port can be \ndecorated with a multiplicity, as shown in the left portion of Figure 18.1, though \nthis is typically only done on component types. The number of ports on compo-\nnent instances, as shown in the right portion of Figure 18.1, is typically bound to \na specific number. Components that dynamically create and manage a set of ports \nshould retain a multiplicity descriptor on instance descriptions.\nWhile C&C connectors are as semantically rich as C&C components, the \nsame is not true of UML connectors. UML connectors cannot have substructure, \nattributes, or behavioral descriptions. This makes choosing how to represent \nC&C connectors more difficult, as UML connectors are not always rich enough.\nYou should represent a “simple” C&C connector using a UML connec-\ntor—a line. Many commonly used C&C connectors have well-known, applica-\ntion-independent semantics and implementations, such as function calls or data \nread operations. If the only information you need to supply is the type of the \nconnector, then a UML connector is adequate. Call-return connectors can be rep-\nresented by a UML assembly connector, which links a component’s required in-\nterface (socket) to the other component’s provided interface (lollipop). You can \nuse a stereotype to denote the type of connector. If all connectors in a primary \npresentation are of the same type, you can note this once in a comment rather \nthan explicitly on each connector to reduce visual clutter. Attachment is shown \nby connecting the endpoints of the connector to the ports of components. Con-\nnector roles cannot be explicitly represented with a UML connector because the \nUML connector element does not allow the inclusion of interfaces (unlike the \nUML port, which does allow interfaces). The best approximation is to label the \nconnector ends and use these labels to identify role descriptions that must be doc-\numented elsewhere.\nYou should represent a “rich” C&C connector using a UML component, or \nby annotating a line UML connector with a tag or other auxiliary documentation \nthat explains the meaning of the complex connector. \n\n\n18.3  Views\n339\n: Account\nDatabase\n«Repository»\nAccount Database\nServer [1..5]\nServer\nServer\nAdmin\nAdmin\nKey: UML\nFigure 18.1  A UML representation of the ports on a C&C component type (left) \nand component instance (right). The Account Database component type has \ntwo types of ports, Server and Admin (noted by the boxes on the component’s \nborder). The Server port is defined with a multiplicity, meaning that multiple \ninstances of the port are permitted on any corresponding component instance. \nAllocation Views \nAllocation views describe the mapping of software units to elements of an envi-\nronment in which the software is developed or in which it executes. The environ-\nment might be the hardware, the operating environment in which the software is \nexecuted, the file systems supporting development or deployment, or the devel-\nopment organization(s). \nTable 18.3 summarizes the characteristics of allocation views. Allocation \nviews consist of software elements and environmental elements. Examples of en-\nvironmental elements are a processor, a disk farm, a file or folder, or a group of \ndevelopers. The software elements come from a module or C&C view.\nTable 18.3  Summary of the Characteristics of Allocation Views\nElements\n■\n■\nSoftware element. A software element has properties that are \nrequired of the environment. \n■\n■\nEnvironmental element. An environmental element has properties \nthat are provided to the software.\nRelations\nAllocated to. A software element is mapped (allocated to) an \nenvironmental element. Properties are dependent on the particular \nview.\nConstraints\nVaries by view\nUsage\n■\n■\nFor reasoning about performance, availability, security, and safety. \n■\n■\nFor reasoning about distributed development and allocation of \nwork to teams. \n■\n■\nFor reasoning about concurrent access to software versions. \n■\n■\nFor reasoning about the form and mechanisms of system \ninstallation.\n",
      "page_number": 349
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 360-370)",
      "start_page": 360,
      "end_page": 370,
      "detection_method": "topic_boundary",
      "content": "340  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe relation in an allocation view is allocated to. We usually talk about alloca-\ntion views in terms of a mapping from software elements to environmental elements, \nalthough the reverse mapping can also be relevant and interesting. A single software \nelement can be allocated to multiple environmental elements, and multiple software \nelements can be allocated to a single environmental element. If these allocations \nchange over time, either during development or execution of the system, then the \narchitecture is said to be dynamic with respect to that allocation. For example, pro-\ncesses might migrate from one processor or virtual machine to another. Similarly \nmodules might migrate from one development team to another.\nSoftware elements and environmental elements have properties in allocation \nviews. The usual goal of an allocation view is to compare the properties required \nby the software element with the properties provided by the environmental ele-\nments to determine whether the allocation will be successful or not. For example, \nto ensure a component’s required response time, it has to execute on (be allocated \nto) a processor that provides sufficiently fast processing power. For another ex-\nample, a computing platform might not allow a task to use more than 10 kilo-\nbytes of virtual memory. An execution model of the software element in question \ncan be used to determine the required virtual memory usage. Similarly, if you are \nmigrating a module from one team to another, you might want to ensure that the \nnew team has the appropriate skills and background knowledge.\nAllocation views can depict static or dynamic views. A static view depicts a \nfixed allocation of resources in an environment. A dynamic view depicts the condi-\ntions and the triggers for which allocation of resources changes according to load-\ning. Some systems recruit and utilize new resources as their load increases. An ex-\nample is a load-balancing system in which new processes or threads are created on \nanother machine. In this view, the conditions under which the allocation changes, \nthe allocation of runtime software, and the dynamic allocation mechanism need to \nbe documented. (Recall from Chapter 1 that one of the allocation structures is the \nwork assignment structure, which allocates modules to teams for development. That \nrelationship, too, can be allocated dynamically, depending on “load”—in this case, \nthe load on development teams.)\nQuality Views \nModule, C&C, and allocation views are all structural views: They primarily show \nthe structures that the architect has engineered into the architecture to satisfy \nfunctional and quality attribute requirements.\nThese views are excellent for guiding and constraining downstream develop-\ners, whose primary job it is to implement those structures. However, in systems in \nwhich certain quality attributes (or, for that matter, some other kind of stakeholder \nconcerns) are particularly important and pervasive, structural views may not be the \nbest way to present the architectural solution to those needs. The reason is that the \nsolution may be spread across multiple structures that are inconvenient to combine \n(for example, because the element types shown in each are different).\n\n\n18.4  Choosing the Views\n341\nAnother kind of view, which we call a quality view, can be tailored for spe-\ncific stakeholders or to address specific concerns. These quality views are formed \nby extracting the relevant pieces of structural views and packaging them together. \nHere are five examples:\n■\n■A security view can show all of the architectural measures taken to provide \nsecurity. It would show the components that have some security role or \nresponsibility, how those components communicate, any data repositories \nfor security information, and repositories that are of security interest. The \nview’s context information would show other security measures (such as \nphysical security) in the system’s environment. The behavior part of a se-\ncurity view would show the operation of security protocols and where and \nhow humans interact with the security elements. It would also capture how \nthe system would respond to specific threats and vulnerabilities. \n■\n■A communications view might be especially helpful for systems that are \nglobally dispersed and heterogeneous. This view would show all of the \ncomponent-to-component channels, the various network channels, quali-\nty-of-service parameter values, and areas of concurrency. This view can be \nused to analyze certain kinds of performance and reliability (such as dead-\nlock or race condition detection). The behavior part of this view could show \n(for example) how network bandwidth is dynamically allocated.\n■\n■An exception or error-handling view could help illuminate and draw atten-\ntion to error reporting and resolution mechanisms. Such a view would show \nhow components detect, report, and resolve faults or errors. It would help \nidentify the sources of errors and appropriate corrective actions for each. \nRoot-cause analysis in those cases could be facilitated by such a view.\n■\n■A reliability view would be one in which reliability mechanisms such as \nreplication and switchover are modeled. It would also depict timing issues \nand transaction integrity. \n■\n■A performance view would include those aspects of the architecture useful \nfor inferring the system’s performance. Such a view might show network \ntraffic models, maximum latencies for operations, and so forth.\nThese and other quality views reflect the documentation philosophy of ISO/\nIEC/IEEE standard 42010:2011, which prescribes creating views driven by stake-\nholder concerns about the architecture. \n18.4  Choosing the Views\nDocumenting decisions during the design process (something we strongly rec-\nommend) produces views, which are the heart of an architecture document. It \nis most likely that these views are rough sketches more than finished products \n\n\n342  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nready for public release; this will give you the freedom to back up and rethink \ndesign decisions that turn out to be problematic without having wasted time on \nbroad dissemination and cosmetic polish. They are documented purely as your \nown memory aid.\nBy the time you’re ready to release an architecture document, you’re likely \nto have a fairly well-worked-out collection of architecture views. At some point \nyou’ll need to decide which to take to completion, with how much detail, and \nwhich to include in a given release. You’ll also need to decide which views can \nbe usefully combined with others, so as to reduce the total number of views in the \ndocument and reveal important relations among the views.\nYou can determine which views are required, when to create them, and how \nmuch detail to include if you know the following:\n■\n■What people, and with what skills, are available\n■\n■Which standards you have to comply with\n■\n■What budget is on hand\n■\n■What the schedule is\n■\n■What the information needs of the important stakeholders are\n■\n■What the driving quality attribute requirements are\n■\n■What the size of the system is\nAt a minimum, expect to have at least one module view, at least one C&C \nview, and for larger systems, at least one allocation view in your architecture doc-\nument. Beyond that basic rule of thumb, however, there is a three-step method for \nchoosing the views: \n■\n■Step 1: Build a stakeholder/view table. Enumerate the stakeholders for \nyour project’s software architecture documentation down the rows. Be as \ncomprehensive as you can. For the columns, enumerate the views that ap-\nply to your system. (Use the structures discussed in Chapter 1, the views \ndiscussed in this chapter, and the views that your design work in ADD has \nsuggested as a starting list of candidates.) Some views (such as decom-\nposition, uses, and work assignment) apply to every system, while others \n(various C&C views, the layered view) only apply to some systems. For the \ncolumns, make sure to include the views or view sketches you already have \nas a result of your design work so far. \nOnce you have the rows and columns defined, fill in each cell to describe \nhow much information the stakeholder requires from the view: none, overview \nonly, moderate detail, or high detail. The candidate view list going into step 2 \nnow consists of those views for which some stakeholder has a vested interest.\n■\n■Step 2: Combine views. The candidate view list from step 1 is likely to \nyield an impractically large number of views. This step will winnow the list \nto manageable size. Look for marginal views in the table: those that require \nonly an overview, or that serve very few stakeholders. Combine each mar-\nginal view with another view that has a stronger constituency. \n\n\n18.5  Combining Views \n343\n■\n■Step 3: Prioritize and stage. After step 2 you should have the minimum \nset of views needed to serve your stakeholder community. At this point you \nneed to decide what to do first. What you do first depends on your project, \nbut here are some things to consider:\n■\n■The decomposition view (one of the module views) is a particularly \nhelpful view to release early. High-level (that is, broad and shallow) \ndecompositions are often easy to design, and with this information the \nproject manager can start to staff development teams, put training in \nplace, determine which parts to outsource, and start producing budgets \nand schedules.\n■\n■Be aware that you don’t have to satisfy all the information needs of all the \nstakeholders to the fullest extent. Providing 80 percent of the information \ngoes a long way, and this might be good enough so that the stakeholders \ncan do their job. Check with the stakeholder to see if a subset of informa-\ntion would be sufficient. They typically prefer a product that is delivered \non time and within budget over getting the perfect documentation.\n■\n■You don’t have to complete one view before starting another. People can \nmake progress with overview-level information, so a breadth-first ap-\nproach is often the best.\n18.5  Combining Views \nThe basic principle of documenting an architecture as a set of separate views \nbrings a divide-and-conquer advantage to the task of documentation, but if the \nviews were irrevocably different, with no association with one another, nobody \nwould be able to understand the system as a whole.\nBecause all views in an architecture are part of that same architecture and \nexist to achieve a common purpose, many of them have strong associations with \neach other. Managing how architectural structures are associated is an important \npart of the architect’s job, independent of whether any documentation of those \nstructures exists. \nSometimes the most convenient way to show a strong association between \ntwo views is to collapse them into a single combined view, as dictated by step 2 \nof the three-step method just presented to choose the views. A combined view \nis a view that contains elements and relations that come from two or more other \nviews. Combined views can be very useful as long as you do not try to overload \nthem with too many mappings.\nThe easiest way to merge views is to create an overlay that combines the in-\nformation that would otherwise have been in two separate views. This works well \nif the coupling between the two views is tight; that is, there are strong associations \n\n\n344  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nbetween elements in one view and elements in the other view. If that is the case, \nthe structure described by the combined view will be easier to understand than \nthe two views seen separately. For an example, see the overlay of decomposition \nand uses sketches shown in Figure 18.2. In an overlay, the elements and the rela-\ntions keep the types as defined in their constituent views.\nThe views below often combine naturally:\n■\n■Various C&C views. Because C&C views all show runtime relations among \ncomponents and connectors of various types, they tend to combine well. \nDifferent (separate) C&C views tend to show different parts of the system, \nor tend to show decomposition refinements of components in other views. \nThe result is often a set of views that can be combined easily.\n«subsystem»\nCCS\nutils\n«subsystem»\nitc\nservlet\n«subsystem»\nadlsc\ncontroller\nbusiness\nfacades\nclient\nentity\ntest\ntaglibs\nobjects\n«subsystem»\ntdc\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\nServer-Side Application Modules\ncommon\nsecurity\nportal\nwebservice\nNotation: UML\nFigure 18.2  A decomposition view overlaid with “uses” information, to create a \ndecomposition/uses overlay. \n\n\n18.6  Building the Documentation Package\n345\n■\n■Deployment view with either SOA or communicating-processes views. An \nSOA view shows services, and a communicating-processes view shows pro-\ncesses. In both cases, these are components that are deployed onto proces-\nsors. Thus there is a strong association between the elements in these views.\n■\n■Decomposition view and any of work assignment, implementation, uses, or \nlayered views. The decomposed modules form the units of work, develop-\nment, and uses. In addition, these modules populate layers.\n18.6  Building the Documentation Package\nRemember the principle of architecture documentation, with which we started \nthis chapter. This principle tells us that our task is to document the relevant views \nand to document the information that applies to more than one view.\nDocumenting a View\nFigure 18.3 shows a template for documenting a view.\nSection 1. Primary Presentation\nSection 2. Element Catalog\n \nSection 2.A. Elements and Their Properties\n \nSection 2.B. Relations and Their Properties\n \nSection 2.C. Element Interfaces\n \nSection 2.D. Element Behavior\nSection 3. Context Diagram\nSection 4. Variability Guide\nSection 5. Rationale\nTemplate for a View\nFigure 18.3  View template\n\n\n346  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nNo matter what the view, the documentation for a view can be placed into a \nstandard organization consisting of these parts:\n■\n■Section 1: The Primary Presentation. The primary presentation shows \nthe elements and relations of the view. The primary presentation should \ncontain the information you wish to convey about the system—in the vo-\ncabulary of that view. It should certainly include the primary elements and \nrelations but under some circumstances might not include all of them. For \nexample, you may wish to show the elements and relations that come into \nplay during normal operation but relegate error handling or exception pro-\ncessing to the supporting documentation. \nThe primary presentation is most often graphical. It might be a diagram \nyou’ve drawn in an informal notation using a simple drawing tool, or it \nmight be a diagram in a semiformal or formal notation imported from a \ndesign or modeling tool that you’re using. If your primary presentation is \ngraphical, make sure to include a key that explains the notation. Lack of \na key is the most common mistake that we see in documentation in practice.\nOccasionally the primary presentation will be textual, such as a table or \na list. If that text is presented according to certain stylistic rules, these rules \nshould be stated or incorporated by reference, as the analog to the graphi-\ncal notation key. Regardless of whether the primary presentation is textual \ninstead of graphical, its role is to present a terse summary of the most im-\nportant information in the view. \n■\n■Section 2: The Element Catalog. The element catalog details at least those \nelements depicted in the primary presentation. For instance, if a diagram \nshows elements A, B, and C, then the element catalog needs to explain what \nA, B, and C are. In addition, if elements or relations relevant to this view \nwere omitted from the primary presentation, they should be introduced and \nexplained in the catalog. Specific parts of the catalog include the following:\n■\n■Elements and their properties. This section names each element in the \nview and lists the properties of that element. Each view introduced in \nChapter 1 listed a set of suggested properties associated with that view. \nFor example, elements in a decomposition view might have the property \nof “responsibility”—an explanation of each module’s role in the sys-\ntem—and elements in a communicating-processes view might have tim-\ning parameters, among other things, as properties. Whether the properties \nare generic to the view chosen or the architect has introduced new ones, \nthis is where they are documented and given values.\n■\n■Relations and their properties. Each view has specific relation types that \nit depicts among the elements in that view. Mostly, these relations are \nshown in the primary presentation. However, if the primary presentation \ndoes not show all the relations or if there are exceptions to what is depict-\ned in the primary presentation, this is the place to record that information.\n■\n■Element interfaces. This section documents element interfaces.\n\n\n18.6  Building the Documentation Package\n347\n■\n■Element behavior. This section documents element behavior that is not \nobvious from the primary presentation.\n■\n■Section 3: Context Diagram. A context diagram shows how the system or \nportion of the system depicted in this view relates to its environment. The \npurpose of a context diagram is to depict the scope of a view. Here “con-\ntext” means an environment with which the part of the system interacts. \nEntities in the environment may be humans, other computer systems, or \nphysical objects, such as sensors or controlled devices. \n■\n■Section 4: Variability Guide. A variability guide shows how to exercise \nany variation points that are a part of the architecture shown in this view.\n■\n■Section 5: Rationale. Rationale explains why the design reflected in the view \ncame to be. The goal of this section is to explain why the design is as it is and \nto provide a convincing argument that it is sound. The choice of a pattern in \nthis view should be justified here by describing the architectural problem that \nthe chosen pattern solves and the rationale for choosing it over another.\nDocumenting Information Beyond Views \nAs shown in Figure 18.4, documentation beyond views can be divided into two \nparts:\n1.\t\nOverview of the architecture documentation. This tells how the documen-\ntation is laid out and organized so that a stakeholder of the architecture can \nfind the information he or she needs efficiently and reliably.\n2.\t\nInformation about the architecture. Here, the information that remains to be \ncaptured beyond the views themselves is a short system overview to ground \nany reader as to the purpose of the system and the way the views are related \nto one another, an overview of and rationale behind system-wide design \napproaches, a list of elements and where they appear, and a glossary and an \nacronym list for the entire architecture. \nFigure 18.4 summarizes our template for documentation beyond views. \nDocumentation beyond views consists of the following sections:\n■\n■Document control information. List the issuing organization, the current \nversion number, date of issue and status, a change history, and the procedure \nfor submitting change requests to the document. Usually this is captured in \nthe front matter. Change control tools can provide much of this information.\n■\n■Section 1: Documentation Roadmap. The documentation roadmap tells \nthe reader what information is in the documentation and where to find it. A \ndocumentation map consists of four sections:\n■\n■Scope and summary. Explain the purpose of the document and briefly \nsummarize what is covered and (if you think it will help) what is not cov-\nered. Explain the relation to other documents (such as downstream design \ndocuments or upstream system engineering documents).\n\n\n348  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nSection 1. Documentation Roadmap\nSection 2. How a View Is Documented\nSection 3. System Overview\nSection 4. Mapping Between Views\nSection 5. Rationale\nSection 6. Directory — index, glossary, \n \nacronym list\nTemplate for Documentation\nBeyond Views\nArchitecture\ndocumentation\ninformation\nArchitecture\ninformation\nFigure 18.4  Summary of documentation beyond views\n■\n■How the documentation is organized. For each section in the documenta-\ntion, give a short synopsis of the information that can be found there. An \nalternative to this is to use an annotated table of contents. This is a table \nthat doesn’t just list section titles and page numbers, but also gives a syn-\nopsis with each entry. It provides one-stop shopping for a reader attempt-\ning to look up a particular kind of information.\n■\n■View overview. The major part of the map describes the views that the \narchitect has included in the package. For each view, the map gives the \nfollowing information:\n■\n■The name of the view and what pattern it instantiates, if any.\n■\n■A description of the view’s element types, relation types, and property \ntypes. This lets a reader begin to understand the kind of information \nthat is presented in the view.\n■\n■A description of language, modeling techniques, or analytical methods \nused in constructing the view.\n■\n■How stakeholders can use the documentation. The map follows with a \nsection describing which stakeholders and concerns are addressed by \neach view; this is conveniently captured as a table. This section shows \nhow various stakeholders might use the documentation to help address \ntheir concerns. Include short scenarios, such as “A maintainer wishes to \nknow the units of software that are likely to be changed by a proposed \nmodification. The maintainer consults the decomposition view to under-\nstand the responsibilities of each module in order to identify the modules \nlikely to change. The maintainer then consults the uses view1 to see what \n1.   The uses view is a module view. It shows the uses structure discussed in Chapter 1.\n\n\n18.6  Building the Documentation Package\n349\nmodules use the affected modules (and thus might also have to change).” \nTo be compliant with ISO/IEC 42010-2007, you must consider the con-\ncerns of at least users, acquirers, developers, and maintainers. \n■\n■Section 2: How a View Is Documented. This is where you explain the \nstandard organization you’re using to document views—either the one de-\nscribed in this chapter or one of your own. It tells your readers how to find \ninformation in a view. If your organization has standardized on a template \nfor a view, as it should, then you can simply refer to that standard. If you \nare lacking such a template, then text such as that given above describ-\ning our view template should appear in this section of your architecture \ndocumentation.\n■\n■Section 3: System Overview. This is a short prose description of the sys-\ntem’s function, its users, and any important background or constraints. This \nsection provides your readers with a consistent mental model of the system \nand its purpose. This might be just a pointer to a concept-of-operations \ndocument.\n■\n■Section 4: Mapping Between Views. Because all the views of an archi-\ntecture describe the same system, it stands to reason that any two views \nwill have much in common. Helping a reader understand the associations \nbetween views will help that reader gain a powerful insight into how the \narchitecture works as a unified conceptual whole. \nThe associations between elements across views in an architecture are, \nin general, many-to-many. For instance, each module may map to multiple \nruntime elements, and each runtime element may map to multiple modules. \nView-to-view associations can be conveniently captured as tables. List \nthe elements of the first view in some convenient lookup order. The table \nitself should be annotated or introduced with an explanation of the associa-\ntion that it depicts; that is, what the correspondence is between the elements \nacross the two views. Examples include “is implemented by” for mapping \nfrom a component-and-connector view to a module view, “implements” for \nmapping from a module view to a component-and-connector view, “in-\ncluded in” for mapping from a decomposition view to a layered view, and \nmany others.\n■\n■Section 5: Rationale. This section documents the architectural decisions \nthat apply to more than one view. Prime candidates include documentation \nof background or organizational constraints or major requirements that led \nto decisions of system-wide import. The decisions about which fundamen-\ntal architecture patterns to use are often described here.\n■\n■Section 6: Directory. The directory is a set of reference material that helps \nreaders find more information quickly. It includes an index of terms, a glos-\nsary, and an acronym list.\n\n\n350  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nOnline Documentation, Hypertext, and Wikis \nA document can be structured as linked web pages. Compared with documents \nwritten with a text-editing tool, web-oriented documents typically consist of \nshort pages (created to fit on one screen) with a deeper structure. One page usu-\nally provides some overview information and has links to more detailed informa-\ntion. When done well, a web-based document is easier to use for people who just \nneed overview information. On the other hand, it can become more difficult for \npeople who need detail. Finding information can be more difficult in multi-page, \nweb-based documents than in a single-file, text-based document, unless a search \nengine is available.\nUsing readily available tools, it’s possible to create a shared document that \nmany stakeholders can contribute to. The hosting organization needs to decide \nwhat permissions it wants to give to various stakeholders; the tool used has to \nsupport the permissions policy. In the case of architecture documentation, we \nwould want all stakeholders to comment on and add clarifying information to the \narchitecture, but we would only want architects to be able to change the architec-\nture or at least provide architects with a “final approval” mechanism. A special \nkind of shared document that is ideal for this purpose is a wiki.\nFollow a Release Strategy\nYour project’s development plan should specify the process for keeping the im-\nportant documentation, including architecture documentation, current. The archi-\ntect should plan to issue releases of the documentation to support major project \nmilestones, which usually means far enough ahead of the milestone to give devel-\nopers time to put the architecture to work. For example, the end of each iteration \nor sprint or incremental release could be associated with providing revised docu-\nmentation to the development team.\nDocumenting Patterns\nArchitects can, and typically do, use patterns as a starting point for their design, \nas we have discussed in Chapter 13. These patterns might be published in exist-\ning catalogs or in an organization’s proprietary repository of standard designs, \nor created specifically for the problem at hand by the architect. In each of these \ncases, they provide a generic (that is, incomplete) solution approach that the ar-\nchitect will have to refine and instantiate. \nFirst, record the fact that the given pattern is being used. Then say why this \nsolution approach was chosen—why it is a good fit to the problem at hand. If the \nchosen approach comes from a pattern, this will consist essentially of showing \nthat the problem at hand fits the problem and context of the pattern.\nUsing a pattern means making successive design decisions that eventually \nresult in an architecture. These design decisions manifest themselves as newly \n",
      "page_number": 360
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 371-378)",
      "start_page": 371,
      "end_page": 378,
      "detection_method": "topic_boundary",
      "content": "18.7  Documenting Behavior\n351\ninstantiated elements and relations among them. The architect can document a \nsnapshot of the architecture at each stage. How many stages there are depends on \nmany things, not the least of which is the ability of readers to follow the design \nprocess in case they have to revisit it in the future.\n18.7  Documenting Behavior\nDocumenting an architecture requires behavior documentation that complements \nstructural views by describing how architecture elements interact with each other. \nReasoning about characteristics such as a system’s potential to deadlock, a sys-\ntem’s ability to complete a task in the desired amount of time, or maximum mem-\nory consumption requires that the architecture description contain information \nabout both the characteristics of individual elements as well as patterns of inter-\naction among them—that is, how they behave with each other. In this section, we \nprovide guidance as to what types of things you will want to document in order \nto reap these benefits. In our architecture view template, behavior has its own \nsection in the element catalog.\nThere are two kinds of notations available for documenting behavior. The \nfirst kind of notation is called trace-oriented languages; the second is called com-\nprehensive languages.\nTraces are sequences of activities or interactions that describe the system’s \nresponse to a specific stimulus when the system is in a specific state. A trace \ndescribes a sequence of activities or interactions between structural elements of \nthe system. Although it is conceivable to describe all possible traces to generate \nthe equivalent of a comprehensive behavioral model, it is not the intention of \ntrace-oriented documentation to do so. Below we describe four notations for doc-\numenting traces: use cases, sequence diagrams, communication diagrams, and \nactivity diagrams. Although other notations are available (such as message se-\nquence charts, timing diagrams, and the Business Process Execution Language), \nwe have chosen these four as a representative sample of trace-oriented languages.\n■\n■Use cases describe how actors can use a system to accomplish their goals. \nUse cases are frequently used to capture the functional requirements for a \nsystem. UML provides a graphical notation for use case diagrams but does \nnot say how the text of a use case should be written. The UML use case dia-\ngram can be used effectively as an overview of the actors and the behavior \nof a system. The use case description is textual and should contain the use \ncase name and brief description, the actor or actors who initiate the use case \n(primary actors), other actors who participate in the use case (secondary \nactors), flow of events, alternative flows, and nonsuccess cases. \n■\n■A UML sequence diagram shows a sequence of interactions among in-\nstances of elements pulled from the structural documentation. It shows only \n\n\n352  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nthe instances participating in the scenario being documented. A sequence \ndiagram has two dimensions: vertical, representing time, and horizontal, \nrepresenting the various instances. The interactions are arranged in time \nsequence from top to bottom. Figure 18.5 is an example of a sequence dia-\ngram that illustrates the basic UML notation. \nObjects (i.e., element instances) have a lifeline, drawn as a vertical \ndashed line along the time axis. The sequence is usually started by an ac-\ntor on the far left. The instances interact by sending messages, which are \nshown as horizontal arrows. A message can be a method or function call, an \nevent sent through a queue, or something else. The message usually maps \nto a resource (operation) in the interface of the receiver instance. A filled \narrowhead on a solid line represents a synchronous message, whereas the \nopen arrowhead represents an asynchronous message. The dashed arrow is \na return message. The execution occurrence bars along the lifeline indicate \nthat the instance is processing or blocked waiting for a return. \nKey (UML)\n:Login\nPage\n:Login\nController\n:UserDao\n:Logger\n:User\nlogin\nlogin(…)\ncheckPwd(…)\nnew\n:User\nSession\nActor\nObject\nLifeline\nExecution\noccurrence\nSynchronous\nmessage\nAsynchronous\nmessage\nReturn\nmessage\nregister User Login(…)\nFigure 18.5  A simple example of a UML sequence diagram\n\n\n18.7  Documenting Behavior\n353\n■\n■A UML communication diagram shows a graph of interacting elements \nand annotates each interaction with a number denoting order. Similarly to \nsequence diagrams, instances shown in a communication diagram are ele-\nments described in the accompanying structural documentation. Commu-\nnication diagrams are useful when the task is to verify that an architecture \ncan fulfill the functional requirements. The diagrams are not useful if the \nunderstanding of concurrent actions is important, as when conducting a \nperformance analysis.\n■\n■UML activity diagrams are similar to flow charts. They show a business \nprocess as a sequence of steps (called actions) and include notation to ex-\npress conditional branching and concurrency, as well as to show sending \nand receiving events. Arrows between actions indicate the flow of control. \nOptionally, activity diagrams can indicate the architecture element or actor \nperforming the actions. Activity diagrams can express concurrency. A fork \nnode (depicted as a thick bar orthogonal to the flow arrows) splits the flow \ninto two or more concurrent flows of actions. The concurrent flows may lat-\ner be synchronized into a single flow through a join node (also depicted as \nan orthogonal bar). The join node waits for all incoming flows to complete \nbefore proceeding. Different from sequence and communication diagrams, \nactivity diagrams don’t show the actual operations being performed on spe-\ncific objects. Activity diagrams are useful to broadly describe the steps in a \nspecific workflow. Conditional branching (diamond symbol) allows a single \ndiagram to represent multiple traces, although it’s not usually the intent of \nan activity diagram to show all possible traces or the complete behavior for \nthe system or part of it. \nIn contrast to trace notations, comprehensive models show the complete be-\nhavior of structural elements. Given this type of documentation, it is possible \nto infer all possible paths from initial state to final state. The state machine for-\nmalism represents the behavior of architecture elements because each state is an \nabstraction of all possible histories that could lead to that state. State machine \nlanguages allow you to complement a structural description of the elements of \nthe system with constraints on interactions and timed reactions to both internal \nand environmental stimuli.\nUML state machine diagram notation is based on the statechart graphical \nformalism developed by David Harel for modeling reactive systems; it allows \nyou to trace the behavior of your system, given specific inputs. A UML state ma-\nchine diagram shows states represented as boxes and transitions between states \nrepresented as arrows. The state machine diagrams help to model elements of the \narchitecture and help to illustrate their runtime interactions. Figure 18.6 is a sim-\nple example showing the states of a vehicle cruise control system. \n\n\n354  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nKey: UML\npress “–”\nto coast\npress “+” \nto accelerate\n press “set” or \n“resume” buttons\npress “cruise \non/off” button\npress “cruise \non/off” button\npress “cruise on/off” button\ntap brake pedal\npush \nthrottle\npedal\noff\non,\ndisengaged\non,\nengaged\nFigure 18.6  UML state machine diagram for the cruise control system of a \nmotor vehicle\nEach transition in a state machine diagram is labeled with the event caus-\ning the transition. For example, in Figure 18.6, the transitions correspond to the \nbuttons the driver can press or driving actions that affect the cruise control sys-\ntem. Optionally, the transition can specify a guard condition, which is enclosed in \nbrackets. When the event corresponding to the transition occurs, the guard condi-\ntion is evaluated and the transition is only enabled if the guard is true at that time. \nTransitions can also have consequences, called actions or effects, indicated by a \nslash. When an action is noted, it indicates that the behavior following the slash \nwill be performed when the transition occurs. The states may also specify entry \nand exit actions.\nOther notations exist for describing comprehensive behavior. For exam-\nple, Architecture Analysis and Design Language (AADL) can be used to reason \nabout runtime behavior. Specification and Description Language (SDL) is used \nin telephony. \n18.8  Architecture Documentation and Quality Attributes \nIf architecture is largely about the achievement of quality attributes and if one of \nthe main uses of architecture documentation is to serve as a basis for analysis (to \nmake sure the architecture will achieve its required quality attributes), where do \nquality attributes show up in the documentation? Short of a full-fledged quality \nview (see page 340), there are five major ways:\n1.\t\nAny major design approach (such as an architecture pattern) will have \nquality attribute properties associated with it. Client-server is good for \nscalability, layering is good for portability, an information-hiding-based \ndecomposition is good for modifiability, services are good for interopera-\nbility, and so forth. Explaining the choice of approach is likely to include \na discussion about the satisfaction of quality attribute requirements and \n\n\n18.9  Documenting Architectures That Change Faster Than You Can Document Them  355\ntradeoffs incurred. Look for the place in the documentation where such an \nexplanation occurs. In our approach, we call that rationale.\n2.\t\nIndividual architectural elements that provide a service often have qual-\nity attribute bounds assigned to them. Consumers of the services need to \nknow how fast, secure, or reliable those services are. These quality attri-\nbute bounds are defined in the interface documentation for the elements, \nsometimes in the form of a service-level agreement. Or they may simply be \nrecorded as properties that the elements exhibit.\n3.\t\nQuality attributes often impart a “language” of things that you would look \nfor. Security involves security levels, authenticated users, audit trails, \nfirewalls, and the like. Performance brings to mind buffer capacities, dead-\nlines, periods, event rates and distributions, clocks and timers, and so on. \nAvailability conjures up mean time between failure, failover mechanisms, \nprimary and secondary functionality, critical and noncritical processes, and \nredundant elements. Someone fluent in the “language” of a quality attribute \ncan search for the kinds of architectural elements (and properties of those \nelements) that were put in place precisely to satisfy that quality attribute \nrequirement.\n4.\t\nArchitecture documentation often contains a mapping to requirements that \nshows how requirements (including quality attribute requirements) are sat-\nisfied. If your requirements document establishes a requirement for avail-\nability, for instance, then you should be able to look it up by name or refer-\nence in your architecture document to see the places where that requirement \nis satisfied.\n5.\t\nEvery quality attribute requirement will have a constituency of stakeholders \nwho want to know that it is going to be satisfied. For these stakeholders, the \narchitect should provide a special place in the documentation’s introduction \nthat either provides what the stakeholder is looking for, or tells the stake-\nholder where in the document to find it. It would say something like this: \n“If you are a performance analyst, you should pay attention to the processes \nand threads and their properties (defined [here]), and their deployment on \nthe underlying hardware platform (defined [here]).” In our documentation \napproach, we put this here’s-what-you’re-looking-for information in a sec-\ntion called the documentation roadmap.\n18.9  \u0007Documenting Architectures That Change \nFaster Than You Can Document Them\nWhen your web browser encounters a file type it’s never seen before, odds are \nthat it will go to the Internet, search for and download the appropriate plug-in to \nhandle the file, install it, and reconfigure itself to use it. Without even needing to \n\n\n356  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nshut down, let alone go through the code-integrate-test development cycle, the \nbrowser is able to change its own architecture by adding a new component. \nService-oriented systems that utilize dynamic service discovery and binding \nalso exhibit these properties. More challenging systems that are highly dynamic, \nself-organizing, and reflective (meaning self-aware) already exist. In these cases, \nthe identities of the components interacting with each other cannot be pinned \ndown, let alone their interactions, in any static architecture document.\nAnother kind of architectural dynamism, equally challenging from a docu-\nmentation perspective, is found in systems that are rebuilt and redeployed with \ngreat rapidity. Some development shops, such as those responsible for commer-\ncial websites, build and “go live” with their system many times every day. \nWhether an architecture changes at runtime, or as a result of a high-frequency \nrelease-and-deploy cycle, the changes occur much faster than the documentation \ncycle. In either case, nobody is going to hold up things until a new architecture \ndocument is produced, reviewed, and released. \nBut knowing the architecture of these systems is every bit as important, and \narguably more so, than for systems in the world of more traditional life cycles. \nHere’s what you can do if you’re an architect in a highly dynamic environment:\n■\n■Document what is true about all versions of your system. Your web brows-\ner doesn’t go out and grab just any piece of software when it needs a new \nplug-in; a plug-in must have specific properties and a specific interface. \nAnd it doesn’t just plug in anywhere, but in a predetermined location in \nthe architecture. Record those invariants as you would for any architecture. \nThis may make your documented architecture more a description of con-\nstraints or guidelines that any compliant version of the system must follow. \nThat’s fine.\n■\n■Document the ways the architecture is allowed to change. In the previous \nexamples, this will usually mean adding new components and replacing \ncomponents with new implementations. In the Views and Beyond approach, \nthe place to do this is called the variability guide (captured in Section 4 of \nour view template). \n18.10  \u0007Documenting Architecture in an \nAgile Development Project\n“Agile” refers to an approach to software development that emphasizes rapid and \nflexible development and de-emphasizes project and process infrastructure for their \nown sake. In Chapter 15 we discuss the relationships between architecture and Ag-\nile. Here we focus just on how to document architecture in an Agile environment.\n\n\n18.10  Documenting Architecture in an Agile Development Project\n357\nThe Views and Beyond and Agile philosophies agree strongly on a central \npoint: If information isn’t needed, don’t document it. All documentation should \nhave an intended use and audience in mind, and be produced in a way that serves \nboth. One of the fundamental principles of technical documentation is “Write \nfor the reader.” That means understanding who will read the documentation and \nhow they will use it. If there is no audience, there is no need to produce the \ndocumentation.\nArchitecture view selection is an example of applying this principle. The \nViews and Beyond approach prescribes producing a view if and only if it ad-\ndresses the concerns of an explicitly identified stakeholder community.\nAnother central idea to remember is that documentation is not a monolithic \nactivity that holds up all other progress until it is complete. The view selection \nmethod given earlier prescribes producing the documentation in prioritized stages \nto satisfy the needs of the stakeholders who need it now.\nWhen producing Views and Beyond-based architecture documentation us-\ning Agile principles, keep the following in mind:\n■\n■Adopt a template or standard organization to capture your design decisions.\n■\n■Plan to document a view if (but only if) it has a strongly identified stake-\nholder constituency.\n■\n■Fill in the sections of the template for a view, and for information beyond \nviews, when (and in whatever order) the information becomes available. \nBut only do this if writing down this information will make it easier (or \ncheaper or make success more likely) for someone downstream doing their \njob.\n■\n■Don’t worry about creating an architectural design document and then a \nfiner-grained design document. Produce just enough design information to \nallow you to move on to code. Capture the design information in a format \nthat is simple to use and simple to change—a wiki, perhaps. \n■\n■Don’t feel obliged to fill up all sections of the template, and certainly not \nall at once. We still suggest you define and use rich templates because they \nmay be useful in some situations. But you can always write “N/A” for the \nsections for which you don’t need to record the information (perhaps be-\ncause you will convey it orally). \n■\n■Agile teams sometimes make models in brief discussions by the white-\nboard. When documenting a view, the primary presentation may consist of \na digital picture of the whiteboard. Further information about the elements \n(element catalog), rationale discussion (architecture background), variabil-\nity mechanisms being used (variability guide), and all else can be com-\nmunicated verbally to the team—at least for now. Later on, if you find out \nthat it’s useful to record a piece of information about an element, a context \ndiagram, rationale for a certain design decision, or something else, the tem-\nplate will have the right place ready to receive it. \n\n\n358  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe Software You’re Delivering Isn’t the Only Software That Matters\nAbout ninety-nine percent of the treatment of architecture in this book (and \nothers) is concerned with the software elements that make up the opera-\ntional system that is delivered to its customer. Component-and-connector \nviews show the units of runtime behavior of that system. Module views \nshow the units of implementation that have to be built in order to create \nthat system.\nA colleague of mine is a project manager for a Fortune 500 software \ncompany. On the day I wrote this sidebar, she found out that the develop-\nment platform her project relied on had been infected with a virulent new \nvirus, and the company’s IT department was removing it from service, \nalong with all the backup images, until the virus could be completely re-\nmoved. That was going to take about five days. After that, all of her project’s \nsoftware and tooling would have to be reinstalled and brought back up to \nlatest-version status. Her project was in user final acceptance test, racing \nagainst a delivery deadline, and the IT department’s decision doomed \nher project to join the countless others in our industry that are delivered \nlate. The snarling email she sent to the IT department for (a) allowing the \nplatform to become infected and (b) not providing a backup platform (real \nor virtual) in a timely fashion would melt your screen.\nThe treatment of software architecture we describe in this book is \nperfectly capable of representing and usefully incorporating software other \nthan the software that your customer is paying you to deliver. Allocation \nviews, recall, are about mapping that software to structures in the envi-\nronment. “Uses” views show which software elements rely on the correct \npresence of other software in order to work. Context diagrams are all about \nshowing relations between your system and important elements of its envi-\nronment. It would be the easiest thing in the world to use these constructs \nto represent support software including, in my friend’s case, the develop-\nment platform.\nAn avionics project I worked on years ago included in our decomposition \nview a module called the System Generation Module. This consisted of all \nof the software we needed to construct a loadable image of the product \nwe were building. Not a single byte of code from the System Generation \nModule made it onto the aircraft, but it was as important as any other. \nEven if you don’t build any of your support software but use off-the-shelf \ndevelopment tools from your favorite vendor, someone in your organiza-\ntion is responsible for the care and feeding of that software: its acquisition, \ninstallation, configuration, and upgrade. That constitutes a nontrivial work \nassignment, which suggests that support software also belongs in the work \nassignment view (a kind of allocation view). And of course you always build \nsome of it yourself—test scripts, build scripts, and so forth—so it’s even \nmore deserving of a place in your architecture.\n",
      "page_number": 371
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 379-387)",
      "start_page": 379,
      "end_page": 387,
      "detection_method": "topic_boundary",
      "content": "18.11  Summary\n359\nPromoting support and development software to first-class architec-\ntural status makes us ask the right questions about it, especially the most \nimportant one: What quality attributes do we require of it? Will it provide \nus with the right security if (for example) we want to exclude our subcon-\ntracting partners from access to some of our IP during development? Will it \nhave the availability to be up and running at 2 a.m. Sunday morning when \nour project goes into its inevitable final delivery crunch? And if it crashes, \nwill the IT folks have someone standing by to bring it back up? Will it be \nmodifiable or configurable enough to support the way your project intends \nto use it? \nThink about what other software and environmental resources your proj-\nect depends on, and consider using the architectural tools, models, views, \nand concepts at your disposal to help you do what architecture always \nhelps you do: Ask the right questions at the right time to expose risks and \nbegin to mitigate them. These concepts include quality attribute scenarios, \n“uses” views, and deployment and work assignment views that include \nsupport software.\n—PCC\n18.11  Summary\nWriting architectural documentation is much like other types of writing. You \nmust understand the uses to which the writing is to be put and the audience for \nthe writing. Architectural documentation serves as a means for communication \namong various stakeholders, not only up the management chain and down to the \ndevelopers but also across to peers. \nAn architecture is a complicated artifact, best expressed by focusing on par-\nticular perspectives depending on the message to be communicated. These per-\nspectives are called views, and you must choose the views to document, must \nchoose the notation to document these views, and must choose a set of views that \nis both minimal and adequate. This may involve combining various views that \nhave a large overlap. You must document not only the structure of the architecture \nbut also the behavior.\nOnce you have decided on the views, you must decide how to package the \ndocumentation. The packaging will depend on the media used for expressing the \ndocumentation. Print has different characteristics for understanding and group-\ning than various online media. Different online media will also have different \ncharacteristics.\nThe context of the project will also affect the documentation. Some of the \ncontextual factors are the important quality attributes of the system, the rate of \nchange of the system, and the project management strategy.\n\n\n360  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n18.12  For Further Reading\nDocumenting Software Architectures (second edition) [Clements 10a] is a com-\nprehensive treatment of the Views and Beyond approach. It describes a multitude \nof different views and notations for them. It also describes how to package the \ndocumentation into a coherent whole.\nISO/IEC/IEEE 42010:2011 (“eye-so-forty-two-ten” for short) is the ISO \n(and IEEE) standard [ISO 11] Systems and software engineering—Architecture \ndescription. The first edition of that standard, IEEE Std. 1471-2000, was devel-\noped by an IEEE working group drawing on experience from industry, academia, \nand other standards bodies between 1995 and 2000. ISO/IEC/IEEE 42010 is cen-\ntered on two key ideas: a conceptual framework for architecture description and a \nstatement of what information must be found in any ISO/IEC/IEEE 42010-com-\npliant architecture description, using multiple viewpoints driven by stakeholders’ \nconcerns.\nUnder ISO/IEC/IEEE 42010, as in the Views and Beyond approach, views \nhave a central role in documenting software architecture. The architecture de-\nscription of a system includes one or more views.\nIf you want to use the Views and Beyond approach to produce an ISO/IEC/\nIEEE 42010-compliant architecture document, you certainly can. The main addi-\ntional obligation is to choose and document a set of viewpoints, identifying the \nstakeholders, their concerns, and the elements catalog for each view, and (to a \nlesser degree) address ISO/IEC/IEEE 42010’s other required information content. \nAADL is an SAE standard. The SAE is an organization for engineering pro-\nfessionals in the aerospace, automotive, and commercial vehicle industries. The \nwebsite for the AADL standard is at www.aadl.info.\nSDL is a notation used in the telecom industry. It is targeted at describing \nthe behavior of reactive and distributed systems in general and telecom systems \nin particular. A real-time version of SDL can be found at www.sdl-rt.org/stan-\ndard/V2.2/pdf/SDL-RT.pdf.\nUML 2.0 added several features specifically to allow architecture to be mod-\neled, such as ports. It is managed by the Object Management Group and can be \nfound at www.omg.org/spec/UML/.\n18.13  Discussion Questions\n1.\t\nGo to the website of your favorite open source system. On the site, look \nfor the architectural documentation for that system. What is there? What \nis missing? How would this affect your ability to contribute code to this \nproject?\n\n\n18.13  Discussion Questions\n361\n2.\t\nBanks are justifiably cautious about security. Sketch the documentation you \nwould need for an automatic teller machine (ATM) in order to reason about \nits security architecture.\n3.\t\nSuppose your company has just purchased another company and that you \nhave been given the task of merging a system in your company with a simi-\nlar system in the other company. What views of the other system’s architec-\nture would you like to see and why? Would you ask for the same views of \nboth systems?\n4.\t\nWhen would you choose to document behavior using trace models or using \ncomprehensive models? What value do you get and what effort is required \nfor each of them?\n5.\t\nHow much of a project’s budget would you devote to software architecture \ndocumentation? Why? How would you measure the cost and the benefit?\n6.\t\nAntony Tang, an architect and one of the reviewers of this book, says that \nhe has used a development view—a kind of quality view—that describes \nhow the software should be developed in relation to the use of tools and \ndevelopment workflows, the use of standard library routines such as for ex-\nception handling, some coding conventions and standards, and some testing \nand deployment conventions. Sketch a definition of a development view.\n\n\nThis page intentionally left blank \n\n\n363\n19\nArchitecture, \nImplementation, \nand Testing\nYou don’t make progress by standing on the \nsidelines, whimpering and complaining. You \nmake progress by implementing ideas.\n—Shirley Hufstedler\nAlthough this is a book about software architecture—you’ve noticed that by now, \nno doubt—we need to remind ourselves from time to time that architecture is \nnot a goal unto itself, but only the means to an end. Building systems from the \narchitecture is the end game, systems that have the qualities necessary to meet the \nconcerns of their stakeholders.\nThis chapter covers two critical areas in system-building—implementation \nand testing—from the point of view of architecture. What is the relationship of \narchitecture to implementation (and vice versa)? What is the relationship of ar-\nchitecture to testing (and vice versa)?\n19.1  Architecture and Implementation\nArchitecture is intended to serve as the blueprint for implementation. The sidebar \n“Potayto, Potahto . . .” makes the point that architectures and implementations \nrely on different sets of vocabulary, which results in development tools usually \nserving one community or the other fairly well, but not both. Frequently the \nimplementers are so engrossed in their immediate task at hand that they make \n\n\n364 \nPart Three\t\n19—Architecture, Implementation, and Testing\nimplementation choices that degrade the modular structure of the architecture, \nfor example.\nThis leads to one of the most frustrating situations for architects. It is very \neasy for code and its intended architecture to drift apart; this is sometimes called \n“architecture erosion.” This section talks about four techniques to help keep the \ncode and the architecture consistent.\nEmbedding the Design in the Code\nA key task for implementers is to faithfully execute the prescriptions of the ar-\nchitecture. George Fairbanks, in Just Enough Architecture, prescribes using an \n“architecturally-evident coding style.” Throughout the code, implementers can \ndocument the architectural concept or guidance that they’re reifying. That is, they \ncan “embed” the architecture in their implementations. They can also try to local-\nize the implementation of each architectural element, as opposed to scattering it \nacross different implementation entities.\nThis practice is made easier if implementers (consistently across a project) \nadopt a set of conventions for how architectural concepts “show up” in code. For \nexample, identifying the layer to which a code unit belongs will make it more \nlikely that implementers and maintainers will respect (and hence not violate) the \nlayering.\nFrameworks\n“Framework” is a terribly overused term, but here we mean a reusable set of \nlibraries or classes for a software system. “Library” and “class” are implementa-\ntion-like terms, but frameworks have broad architectural implications—they are a \nplace where architecture and implementation meet. The classes (in an object-ori-\nented framework) are appropriate to the application domain of the system that is \nbeing constructed. Frameworks can range from small and straightforward (such \nas ones that provide a set of standard and commonly used data types to a system) \nto large and sophisticated. For example, the AUTomotive Open System ARchi-\ntecture (AUTOSAR) is a framework for automotive software, jointly developed \nby automobile manufacturers, suppliers, and tool developers.\nFrameworks that are large and sophisticated often encode architectural in-\nteraction mechanisms, by encoding how the classes (and the objects derived from \nthem) communicate and synchronize with each other. For example, AUTOSAR is \nan architecture and not (just) an architecture framework.\nA framework amounts to a substantial (in some cases, enormous) piece \nof reusable software, and it brings with it all of the advantages of reuse: saving \ntime and cost, avoiding a costly design task, encoding domain knowledge, and \ndecreasing the chance of errors from individual implementers coding the same \nthing differently and erroneously. On the other hand, frameworks are difficult to \n\n\n19.1  Architecture and Implementation\n365\ndesign and get correct. Adopting a framework means investing in a selection pro-\ncess as well as training, and the framework may not provide all the functionality \nthat you require. The learning curve for a framework is often extremely steep. \nA framework that provides a complete set of functionality for implementing an \napplication in a particular domain is called a “platform.” \nCode Templates\nA template provides a structure within which some architecture-specific func-\ntionality is achieved, in a consistent fashion system-wide. Many code generators, \nsuch as user interface builders, produce a template into which a developer inserts \ncode, although templates can also be provided by the development environment.\nSuppose that an architecture for a high-availability system prescribes that \nevery component that implements a critical responsibility must use a failover \ntechnique that switches control to a backup copy of itself in case a fault is de-\ntected in its operation.\nThe architecture could, and no doubt would, describe the failover protocol. \nIt might go something like this:\nIn the event that a failure is detected in a critical-application component, a \nswitchover occurs as follows:\n1.\t\nA secondary copy, executing in parallel in background on a different pro-\ncessor, is promoted to the new primary.\n2.\t\nThe new primary reconstitutes with the application’s clients by sending \nthem a message that means, essentially: The operational unit that was \nserving you has had a failure. Were you waiting for anything from us at the \ntime? It then proceeds to service any requests received in response.\n3.\t\nA new secondary is started to serve as a backup for the new primary.\n4.\t\nThe newly started secondary announces itself to the new primary, which \nstarts sending it messages as appropriate to keep it up to date while it is \nexecuting in background.\nIf failure is detected within a secondary, a new one is started on some other \nprocessor. It coordinates with its primary and starts receiving state data.\nEven though the primary and secondary copies are never doing the same \nthing at the same time (the primary is performing its duty and sending state up-\ndates to its backups, and the secondaries are waiting to leap into action and ac-\ncepting state updates), both components come from identical copies of the same \nsource code. \nTo accomplish this, the coders of each critical component would be ex-\npected to implement that protocol. However, a cleverer way is to give the coder \na code template that contains the tricky failover part as boilerplate and contains \nfill-in-the-blank sections where coders can fill in the implementation for the func-\ntionality that is unique to each application. This template could be embedded in \n\n\n366 \nPart Three\t\n19—Architecture, Implementation, and Testing\nthe development environment so that when the developer specifies that the mod-\nule being developed is to support a failover protocol, the template appears as the \ninitial code for the module.\nAn example of such a template, taken from an air traffic control system, \nis illustrated in Figure 19.1. The structure is a continuous loop that services in-\ncoming events. If the event is one that causes the application to take a normal \n(non-fault-tolerance-related) action, it carries out the appropriate action, followed \nby an update of its backup counterparts’ data so that the counterpart can take \nover if necessary. Most applications spend most of their time processing normal \nevents. Other events that may be received involve the transfer (transmission and \nreception) of state and data updates. Finally, there is a set of events that involves \nboth the announcement that this unit has become the primary and requests from \nclients for services that the former (now failed) primary did not complete.\nUsing a template has architectural implications: it makes it simple to add \nnew applications to the system with a minimum of concern for the actual work-\nings of the fault-tolerant mechanisms designed into the approach. Coders and \nmaintainers of applications do not need to know about message-handling mecha-\nnisms except abstractly, and they do not need to ensure that their applications are \nfault tolerant—that has been handled architecturally.\nCode templates have implications for reliability: once the template is de-\nbugged, then entire classes of coding errors across the entire system disappear. \nBut in the context of this discussion, templates represent a true common ground \nwhere the architecture and the implementation come together in a consistent and \nuseful fashion.\nKeeping Code and Architecture Consistent\nCode can drift away from architecture in a depressingly large number of ways. \nFirst, there may be no constraints imposed on the coders to follow the archi-\ntecture. This makes no apparent sense, for why would we bother to invest in an \narchitecture if we aren’t going to use it to constrain the code? However, this hap-\npens more often than you might think. Second, some projects use the published \narchitecture to start out, but when problems are encountered (either technical or \nschedule-related), the architecture is abandoned and coders scramble to field the \nsystem as best they can. Third (and perhaps most common), after the system has \nbeen fielded, changes to it are accomplished with code changes only, but these \nchanges affect the architecture. However, the published architecture is not up-\ndated to guide the changes, nor updated afterward to keep up with them. \nOne simple method to remedy the lack of updating the architecture is to \nnot treat the published architecture as an all-or-nothing affair—it’s either all cor-\nrect or all useless. Parts of the architecture may become out of date, but it will \nhelp enormously if those parts are marked as “no longer applicable” or “to be \nrevised.” Conscientiously marking sections as out of date keeps the architecture \n\n\n19.1  Architecture and Implementation\n367\ndocumentation a living document and (paradoxically) sends a stronger message \nabout the remainder: it is still correct and can still be trusted.\nterminate:= false\ninitialize application/application protocols\nask for current state (image request)\nLoop\nGet_event\nCase Event_Type is\n-- “normal” (non-fault-tolerant-related) requests to \n-- perform actions; only happens if this unit is the\n-- current primary address space\nwhen X => Process X\nSend state data updates to other address spaces\nwhen Y => Process Y\nSend state data updates to other address spaces\n...\nwhen Terminate_Directive => clean up resources; terminate  \n          := true\nwhen State_Data_Update => apply to state data\n-- will only happen if this unit is a secondary address\n-- space, receiving the update from the primary after it\n-- has completed a “normal” action sending, receiving \n-- state data\nwhen Image_Request => send current state data to new  \n          address space\nwhen State_Data_Image => Initialize state data\nwhen Switch_Directive => notify service packages of  \n          change in rank\n-- these are requests that come in after a PAS/SAS\n-- switchover; they report services that they had\n-- requested from the old (failed) PAS which this unit \n-- (now the PAS) must complete. A, B, etc. are the names \n-- of the clients.\nwhen Recon_from_A => reconstitute A\nwhen Recon_from_B => reconstitute B\n...\nwhen others => log error\nend case\nexit when terminate\nend loop\nFigure 19.1  A code template for a failover protocol.  “Process X” and \n“Process Y” are placeholders for application-specific code.\n",
      "page_number": 379
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 388-396)",
      "start_page": 388,
      "end_page": 396,
      "detection_method": "topic_boundary",
      "content": "368 \nPart Three\t\n19—Architecture, Implementation, and Testing\nIn addition, strong management and process discipline will help prevent ero-\nsion. One way is to mandate that changes to the system, no matter when they oc-\ncur, are vetted through the architecture first. The alternatives for achieving code \nalignment with the architecture include the following:\n■\n■Sync at life-cycle milestone. Developers change the code until the end of \nsome phase, such as a release or end of an iteration. At that point, when the \nschedule pressure is less, the architecture is updated.\n■\n■Sync at crisis. This undesirable approach happens when a project has found \nitself in a technical quagmire and needs architectural guidance to get itself \ngoing again. \n■\n■Sync at check-in. Rules for the architecture are codified and used to vet any \ncheck-in. When a change to the code “breaks” the architecture rules, key \nproject stakeholders are informed and then either the code or the architec-\nture rules must be modified. This process is typically automated by tools.\nThese alternatives can work only if the implementation follows the archi-\ntecture mostly, departing from it only here and there and in small ways. That is, \nit works when syncing the architecture involves an update and not a wholesale \noverhaul or do-over.\nPotayto, Potahto, Tomayto, Tomahto— \nLet’s Call the Whole Thing Off!\nOne of the most vexing realities about architecture-based software de-\nvelopment is the gulf between architectural and implementation ontolo-\ngies, the set of concepts and terms inherent in an area. Ask an architect \nwhat concepts they work with all day, and you’re likely to hear things like \nmodules, components, connectors, stakeholders, evaluation, analysis, \ndocumentation, views, modeling, quality attributes, business goals, and \ntechnology roadmaps.\nAsk an implementer the same question, and you likely won’t hear any of \nthose words. Instead you’ll hear about objects, methods, algorithms, data \nstructures, variables, debugging, statements, code comments, compilers, \ngenerics, operator overloading, pointers, and build scripts.\nThis is a gap in language that reflects a gap in concepts. This gap is, in \nturn, reflected in the languages of the tools that each community uses. UML \nstarted out as a way to model object-oriented designs that could be quickly \nconverted to code—that is, UML is conceptually “close” to code. Today it is \na de facto architecture description language, and likely the most popular \none. But it has no built-in concept for the most ubiquitous of architectural \nconcepts, the layer. If you want to represent layers in UML, you have to adopt \nsome convention to do it. Packages stereotyped as <<layer>>, associated \nwith stereotyped <<allowed to use>> dependencies do the trick. But it is a \ntrick, a workaround for a language deficiency. UML has “connectors,” two of \n\n\n19.1  Architecture and Implementation\n369\nthem in fact. But they are a far cry from what architects think of as connec-\ntors. Architectural connectors can and do have rich functionality. For instance, \nan enterprise service bus (ESB) in a service-oriented architecture handles \nrouting, data and format transformation, technology adaptation, and a host of \nother work. It is most natural to depict the ESB as a connector tying together \nservices that interact with each other through it. But UML connectors are \nimpoverished things, little more than bookkeeping mechanisms that have no \nfunctionality whatsoever. The delegation connector in UML exists merely to \nassociate the ports of a parent component with ports of its nested children, \nto send inputs from the outside into a child’s input port, and outputs from a \nchild to the output port of the parent. And the assembly connector simply ties \ntogether one component’s “requires” interface with another’s “provides” inter-\nface. These are no more than bits of string to tie two components together. To \nrepresent a true architectural connector in UML, you have to adopt a conven-\ntion—another workaround—such as using simple associations tagged with \nexplanatory annotations, or abandon the architectural concept completely \nand capture the functionality in another component. \nPart of the concept gap between architecture and implementation is inevi-\ntable. Architectures, after all, are abstractions of systems and their implemen-\ntations. Back in Chapter 2, we said that was one of the valuable properties \nof architecture: you could build many different systems from one. And that’s \nwhat an abstraction is: a one-to-many mapping. One abstraction, many \ninstances; one architecture, many implementations. That architecture is an \nabstraction of implementation is almost its whole point: architecture lets us \nachieve intellectual control over a system without having to capture, let alone \nmaster, all of the countless and myriad truths about its implementation.\nAnd here comes the gap again: All of those truths about its implementa-\ntion are what coders produce for a living, without which the system remains \nbut an idea. Architects, on the other hand, dismiss all of that reality by \nannouncing that they are not interested in implementation “details.” \nCan’t we all get along?\nWe could. There is nothing inherently impossible about a language that \nembraces architectural as well as coding concepts, and several people have \nproposed some. But UML is beastly difficult to change, and programming \nlanguage purveyors all seem to focus their attention down on the underlying \nmachine and not up to the architecture that is directing the implementation. \nUntil this gap is resolved, until architects and coders (and their tools) \nspeak the same conceptual language, we are likely to continue to deal with \nthe most vexing result of this most vexing reality: writing code (or introducing \na code change) that ignores the architecture is the easiest thing in the world. \nThe good news is that even though architecture and implementation \nspeak different languages, they aren’t languages from different planets. \nConcepts in one ontology usually correspond pretty well to concepts in an-\nother. Frameworks are an area where the languages enjoy a fair amount of \noverlap. So are interfaces. These constructs live on the cusp of the two do-\nmains, and provide hope that we might one day speak the same language.\n—PCC\n\n\n370 \nPart Three\t\n19—Architecture, Implementation, and Testing\n19.2  Architecture and Testing\nWhat is the relationship between architecture and testing? One possible answer is \n“None,” or “Not much.” Testing can be seen as the process of making sure that a \nsoftware system meets its requirements, that it brings the necessary functionality \n(endowed with the necessary quality attributes) to its user community. Testing, \nseen this way, is simply connected to requirements, and hardly connected to ar-\nchitecture at all. As long as the system works as expected, who cares what the ar-\nchitecture is? Yes, the architecture played the leading role in getting the system to \nwork as expected, thank you very much, but once it has played that role it should \nmake a graceful exit off the stage. Testers work with requirements: Thanks, archi-\ntecture, but we’ll take it from here. \nNot surprisingly, we don’t like that answer. This is an impoverished view of \ntesting, and in fact an unrealistic one as well. As we’ll see, architecture cannot \nhelp but play an important role in testing. Beyond that, though, we’ll see that \narchitecture can help make testing less costly and more effective when embraced \nin testing activities. We’ll also see what architects can do to help testers, and what \ntesters can do to take advantage of the architecture.\nLevels of Testing and How Architecture Plays a Role in Each\nThere are “levels” of testing, which range from testing small, individual pieces in \nisolation to an entire system. \n■\n■Unit testing refers to tests run on specific pieces of software. Unit testing is \nusually a part of the job of implementing those pieces. In fact, unit tests are \ntypically written by developers themselves. When the tests are written be-\nfore developing the unit, this practice is known as test-driven development. \nCertifying that a unit has passed its unit tests is a precondition for delivery \nof that unit to integration activities. Unit tests test the software in a standalone \nfashion, often relying on “stubs” to play the role of other units with which the \ntested unit interacts, as those other units may not yet be available. Unit tests \nwon’t usually catch errors dealing with the interaction between elements—\nthat comes later—but unit tests provide confidence that each of the system’s \nbuilding blocks is exhibiting as much correctness as is possible on its own.\nA unit corresponds to an architectural element in one of the architec-\nture’s module views. In object-oriented software, a unit might correspond to \na class. In a layered system, a unit might correspond to a layer, or a part of \na layer. Most often a unit corresponds to an element at the leaf of a module \ndecomposition tree.\nArchitecture plays a strong role in unit testing. First, it defines the units: \nthey are architectural elements in one or more of the module views. Second, \nit defines the responsibilities and requirements assigned to each unit. \n\n\n19.2  Architecture and Testing\n371\nModifiability requirements can also be tested at unit test time. How long \nit will take to make specified changes can be tested, although this is seldom \ndone in practice. If specified changes take too long for the developers to \nmake, imagine how long they will take when a new and separate mainte-\nnance group is in charge without the intimate knowledge of the modules.\nAlthough unit testing goes beyond architecture (tests are based on \nnonarchitectural information such as the unit’s internal data structures, \nalgorithms, and control flows), they cannot begin their work without the \narchitecture.\n■\n■Integration testing tests what happens when separate software units start to \nwork together. Integration testing concentrates on finding problems related \nto the interfaces between elements in a design. Integration testing is inti-\nmately connected to the specific increments or subsets that are planned in a \nsystem’s development. \nThe case where only one increment is planned, meaning that integration \nof the entire system will occur in a single step, is called “big bang integra-\ntion” and has largely been discredited in favor of integrating many incre-\nmentally larger subsets. Incremental integration makes locating errors much \neasier, because any new error that shows up in an integrated subset is likely \nto live in whatever new parts were added this time around. \nAt the end of integration testing, the project has confidence that the \npieces of software work together correctly and provide at least some correct \nsystem-wide functionality (depending on how big a subset of the system is \nbeing integrated). Special cases of integration testing are these:\n■\n■System testing, which is a test of all elements of the system, including \nsoftware and hardware in their intended environment\n■\n■Integration testing that involves third-party software\nOnce again, architecture cannot help but play a strong role in integration \ntesting. First, the increments that will be subject to integration testing must \nbe planned, and this plan will be based on the architecture. The uses view is \nparticularly helpful for this, as it shows what elements must be present for a \nparticular piece of functionality to be fielded. That is, if the project requires \nthat (for example) in the next increment of a social networking system users \nwill be able to manage photographs they’ve allowed other users to post in \ntheir own member spaces, the architect can report that this new functionality \nis part of the user_permissions module, which will use a new part of the \nphoto_sharing module, which in turn will use a new structure in the mas-\nter user_links database, and so forth. Project management will know, then, \nthat all of the software must be ready for integration at the same time.\nSecond, the interfaces between elements are part of the architecture, and \nthose interfaces determine the integration tests that are created and run. \nIntegration testing is where runtime quality attribute requirements can \nbe tested. Performance and reliability testing can be accomplished. A \n\n\n372 \nPart Three\t\n19—Architecture, Implementation, and Testing\nsophisticated test harness is useful for performing these types of tests. How \nlong does an end-to-end synchronization of a local database with a global \ndatabase take? What happens if faults are injected into the system? What \nhappens when a process fails? All of these conditions can be tested at inte-\ngration time.\nIntegration testing is also the time to test what happens when the system \nruns for an extended period. You could monitor resource usage during the \ntesting and look for resources that are consumed but not freed. Does your \npool of free database connections decrease over time? Then maybe data-\nbase connections should be managed more aggressively. Does the thread \npool show signs of degradation over time? Ditto.\n■\n■Acceptance testing is a kind of system testing that is performed by users, \noften in the setting in which the system will run. Two special cases of ac-\nceptance testing are alpha and beta testing. In both of these, users are given \nfree rein to use the system however they like, as opposed to testing that \noccurs under a preplanned regimen of a specific suite of tests. Alpha testing \nusually occurs in-house, whereas beta testing makes the system available to \na select set of end users under a “User beware” proviso. Systems in beta test \nare generally quite reliable—after all, the developing organization is highly \nmotivated to make a good first impression on the user community—but us-\ners are given fair warning that the system might not be bug-free or (if “bug-\nfree” is too lofty a goal) at least not up to its planned quality level.\nArchitecture plays less of a role in acceptance testing than at the other \nlevels, but still an important one. Acceptance testing involves stressing the \nsystem’s quality attribute behavior by running it at extremely heavy loads, \nsubjecting it to security attacks, depriving it of resources at critical times, \nand so forth. A crude analogy is that if you want to bring down a house, you \ncan whale away at random walls with a sledgehammer, but your task will \nbe accomplished much more efficiently if you consult the architecture first \nto find which of the walls is holding up the roof. (The point of testing is, \nafter all, to “bring down the house.”)\nOverlaying all of these types of testing is regression testing, which is testing \nthat occurs after a change has been made to the system. The name comes from \nthe desire to uncover old bugs that might resurface after a change, a sign that the \nsoftware has “regressed” to a less mature state. Regression testing can occur at \nany of the previously mentioned levels, and often consists of rerunning the bank \nof tests and checking for the occurrence of old (or for that matter, new) faults.\nBlack-Box and White-Box Testing\nTesting (at any level) can be “black box” or “white box.” Black-box testing \ntreats the software as an opaque “black box,” not using any knowledge about the \n\n\n19.2  Architecture and Testing\n373\ninternal design, structure, or implementation. The tester’s only source of informa-\ntion about the software is its requirements.\nArchitecture plays a role in black-box testing, because it is often the archi-\ntecture document where the requirements for a piece of the system are described. \nAn element of the architecture is unlikely to correspond one-to-one with a re-\nquirement nicely captured in a requirements document. Rather, when the archi-\ntect creates an architectural element, he or she usually assigns it an amalgamation \nof requirements, or partial requirements, to carry out. In addition, the interface to \nan element also constitutes a set of “requirements” for it—the element must hap-\npily accept the specified parameters and produce the specified effect as a result. \nTesters performing black-box testing on an architectural element (such as a major \nsubsystem) are unlikely to be able to do their jobs using only requirements pub-\nlished in a requirements document. They need the architecture as well, because \nthe architecture will help the tester understand what portions of the requirements \nrelate to the specified subsystem.\nWhite-box testing makes full use of the internal structures, algorithms, and \ncontrol and data flows of a unit of software. Tests that exercise all control paths \nof a unit of software are a primary example of white-box testing. White-box test-\ning is most often associated with unit testing, but it has a role at higher levels as \nwell. In integration testing, for example, white-box testing can be used to con-\nstruct tests that attempt to overload the connection between two components by \nexploiting knowledge about how a component (for example) manages multiple \nsimultaneous interactions. \nGray-box testing lies, as you would expect, between black and white. Tes-\nters get to avail themselves of some, but not all, of the internal structure of a \nsystem. For example, they can test the interactions between components but not \nemploy tests based on knowledge of a component’s internal data structures.\nThere are advantages and disadvantages with each kind of testing. Black-\nbox testing is not biased by a design or implementation, and it concentrates on \nmaking sure that requirements are met. But it can be inefficient by (for example) \nrunning many unit tests that a simple code inspection would reveal to be unnec-\nessary. White-box testing often keys in on critical errors more quickly, but it can \nsuffer from a loss of perspective by concentrating tests to make the implemen-\ntation break, but not concentrating on the software delivering full functionality \nunder all points in its input space.\nRisk-based Testing\nRisk-based testing concentrates effort on areas where risk is perceived to be the \nhighest, perhaps because of immature technologies, requirements uncertainty, de-\nveloper experience gaps, and so forth. Architecture can inform risk-based testing \nby contributing categories of risks to be considered. Architects can identify areas \nwhere architectural decisions (if wrong) would have a widespread impact, where \n\n\n374 \nPart Three\t\n19—Architecture, Implementation, and Testing\narchitectural requirements are uncertain, quality attributes are demanding on the \narchitecture, technology selections risky, or third-party software sources unre-\nliable. Architecturally significant requirements are natural candidates for risk-\nbased test cases. If the architecturally significant requirements are not met, then \nthe system is unacceptable, by definition.\nTest Activities\nTesting, depending on the project, can consume from 30 to 90 percent of a devel-\nopment’s schedule and budget. Any activity that gobbles resources as voraciously \nas that doesn’t just happen, of course, but needs to be planned and carried out \npurposefully and as efficiently as possible. Here are some of the activities associ-\nated with testing:\n■\n■Test planning. Test activities have to be planned so that appropriate resourc-\nes can be allocated. “Resources” includes time in the project schedule, \nlabor to run the tests, and technology with which the testing will be carried \nout. Technology might include test tools, automatic regression testers, test \nscript builders, test beds, test equipment or hardware such as network sniff-\ners, and so forth.\n■\n■Test development. This is an activity in which the test procedures are writ-\nten, test cases are chosen, test datasets are created, and test suites are script-\ned. The tests can be developed either before or after development. Develop-\ning the tests prior to development and then developing a module to satisfy \nthe test is a characteristic of test-first development. \n■\n■Test execution. Here, testers apply the tests to the software and capture and \nrecord errors. \n■\n■Test reporting and defect analysis. Testers report the results of specific tests \nto developers, and they report overall metrics about the test results to the \nproject’s technical management. The analysis might include a judgment \nabout whether the software is ready for release. Defect analysis is done by \nthe development team usually along with the customer, to adjudicate dispo-\nsition of each discovered fault: fix it now, fix it later, don’t worry about it, \nand so on. \n■\n■Test harness creation. One of the architect’s common responsibilities is to \ncreate, along with the architecture, a set of test harnesses through which \nelements of the architecture may be conveniently tested. Such test harness-\nes typically permit setting up the environment for the elements to be tested, \nalong with controlling their state and the data flowing into and out of the \nelements.\nOnce again, architecture plays a role and informs each of these activities; \nthe architect can contribute useful information and suggestions for each. For \ntest planning, the architecture provides the list of software units and incremental \n\n\n19.2  Architecture and Testing\n375\nsubsets. The architect can also provide insight as to the complexity or, if the soft-\nware does not yet exist, the expected complexity of each of the software units. \nThe architect can also suggest useful test technologies that will be compatible \nwith the architecture; for example, Java’s ability to support assertions in the code \ncan dramatically increase software testability, and the architect can provide ar-\nguments for or against adopting that technology. For test development, the ar-\nchitecture can make it easy to swap datasets in and out of the system. Finally, \ntest reporting and defect analysis are usually reported in architectural terms: this \nelement passed all of its tests, but that element still has critical errors showing. \nThis layer passed the delivery test, but that layer didn’t. And so forth.\nThe Architect’s Role\nHere are some of the things an architect can do to facilitate quality testing. First \nand foremost, the architect can design the system so that it is highly testable. \nThat is, the system should be designed with the quality attribute of testability in \nmind. Applying the cardinal rule of architecture (“Know your stakeholders!”), \nthe architect can work with the test team (and, to the extent they have a stake in \ntesting, other stakeholders) to establish what is needed. Together, they can come \nup with a definition of the testability requirements using scenarios, as described \nin Chapter 10. Testability requirements are most likely to be a concern of the de-\nveloping organization and not so much of the customer or users, so don’t expect \nto see many testing requirements in a requirements document. Using those test-\nability requirements, the testability tactics in Chapter 10 can be brought to bear to \nprovide the testability needed. \nIn addition to designing for testability, the architect can also do these other \nthings to help the test effort:\n■\n■Insure that testers have access to the source code, design documents, and \nthe change records.\n■\n■Give testers the ability to control and reset the entire dataset that a program \nstores in a persistent database. Reverting the database to a known state is \nessential for reproducing bugs or running regression tests. Similarly, load-\ning a test bed into the database is helpful. Even products that don’t use da-\ntabases can benefit from routines to automatically preload a set of test data. \nOne way to achieve this is to design a “persistence layer” so that the whole \nprogram is database independent. In this way, the entire database can be \nswapped out for testing, even using an in-memory database if desired.\n■\n■Give testers the ability to install multiple versions of a software product on \na single machine. This helps testers compare versions, isolating when a bug \nwas introduced. In distributed applications, this aids testing deployment \nconfigurations and product scalability. This capability could require con-\nfigurable communication ports and provisions for avoiding collisions over \nresources such as the registry. \n\n\n376 \nPart Three\t\n19—Architecture, Implementation, and Testing\nAs a practical matter, the architect cannot afford to ignore the testing pro-\ncess because if, after delivery, something goes seriously wrong, the architect will \nbe one of the first people brought in to diagnose the problem. In one case we \nheard about, this involved flying to the remote mountains of Peru to diagnose a \nproblem with mining equipment.\n19.3  Summary\nArchitecture plays a key role in both implementation and testing. In the imple-\nmentation phase, letting future readers of the code know what architectural con-\nstructs are being used, using frameworks, and using code templates all make life \neasier both at implementation time and during maintenance.\nDuring testing the architecture determines what is being tested at which \nstage of development. Development quality attributes can be tested during unit \ntest and runtime quality attributes can be tested during integration testing.\nTesting, as with other activities in architecture-based development, is a cost/\nbenefit activity. Do not spend as much time testing for faults whose consequences \nare small and spend the most time testing for faults whose consequences are se-\nrious. Do not neglect testing for faults that show up after the system has been \nexecuting for an extended period.\n19.4  For Further Reading\nGeorge Fairbanks gives an excellent treatment of architecture and implementa-\ntion in Chapter 10 of his book Just Enough Software Architecture, which is enti-\ntled “The Code Model” [Fairbanks 10]. \nMary Shaw long ago recognized the conceptual gap between architecture \nand implementation and wrote about it eloquently in her article “Procedure Calls \nAre the Assembly Language of Software Interconnections: Connectors Deserve \nFirst-Class Status” [Shaw 94]. In it she pointed out the disparity between rich \nconnectors available in architecture and the impoverished subroutine call that is \nthe mainstay of nearly every programming language.\nDetails about the AUTOSAR framework can be found at www.autosar.org.\nArchitecture-based testing is an active field of research. [Bertolino 96b], \n[Muccini 07], [Muccini 03], [Eickelman 96], [Pettichord 02], and [Binder 94] \nspecifically address designing systems so that they are more testable. In fact, the \nthree bullets concerning the architect’s role in Section 19.2 are drawn from Petti-\nchord’s work.\n",
      "page_number": 388
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "detection_method": "topic_boundary",
      "content": "19.5  Discussion Questions\n377\nVoas [Voas 95] defines testability, identifies its contributing factors, and de-\nscribes how to measure it.\nBertolino extends Voas’s work and ties testability to dependability [Berto-\nlino 96].\nFinally, Baudry et al. have written an interesting paper that examines the \ntestability of well-known design patterns [Baudry 03].\n19.5  Discussion Questions\n1.\t\nIn a distributed system each computer will have its own clock. It is difficult \nto perfectly synchronize those clocks. How will this complicate making \nperformance measures of distributed systems? How would you go about \ntesting that the performance of a particular system activity is adequate?\n2.\t\nPlan and implement a modification to a module. Ask your colleagues to do \nthe same modification independently. Now compare your results to those of \nyour colleagues. What is the mean and the standard deviation for the time it \ntakes to make that modification?\n3.\t\nList some of the reasons why an architecture and a code base inevitably \ndrift apart. What processes and tools might address this gap? What are their \ncosts and benefits?\n4.\t\nMost user interface frameworks work by capturing events from the user \nand by establishing callbacks or hooks to application-specific functionality. \nWhat limitations do these architectural assumptions impose on the rest of \nthe system?\n5.\t\nConsider building a test harness for a large system. What quality attributes \nshould this harness exhibit? Create scenarios to concretize each of the qual-\nity attributes.\n6.\t\nTesting requires the presence of a test oracle, which determines the success \n(or failure) of a test. For scalability reasons, the oracle must be automatic. \nHow can you ensure that your oracle is correct? How do you ensure that \nits performance will scale appropriately? What process would you use to \nrecord and fix faults in the testing infrastructure?\n7.\t\nIn embedded systems faults often occur “in the field” and it is difficult to \ncapture and replicate the state of the system that led to its failure. What ar-\nchitectural mechanisms might you use to solve this problem?\n8.\t\nIn integration testing it is a bad idea to integrate everything all at once (big \nbang integration). How would you use architecture to help you plan integra-\ntion increments?\n\n\nThis page intentionally left blank \n\n\n379\n20\nArchitecture \nReconstruction and \nConformance\nIt was six men of Indostan / To learning much inclined, \nWho went to see the Elephant / (Though all of them were blind), \nThat each by observation / Might satisfy his mind.\nThe First approach’d the Elephant, / And happening to fall \nAgainst his broad and sturdy side, / At once began to bawl: \n“God bless me! but the Elephant / Is very like a wall!”\nThe Second, feeling of the tusk, / Cried, — “Ho! what have we here \nSo very round and smooth and sharp? / To me ’tis mighty clear \nThis wonder of an Elephant / Is very like a spear!”\nThe Third approached the animal, / And happening to take \nThe squirming trunk within his hands, / Thus boldly up and spake: \n“I see,” quoth he, “the Elephant / Is very like a snake!”\nThe Fourth reached out his eager hand, / And felt about the knee. \n“What most this wondrous beast is like / Is mighty plain,” quoth he, \n“’Tis clear enough the Elephant / Is very like a tree!”\nThe Fifth, who chanced to touch the ear, / Said: “E’en the blindest man \nCan tell what this resembles most; / Deny the fact who can, \nThis marvel of an Elephant / Is very like a fan!”\nThe Sixth no sooner had begun / About the beast to grope, \nThen, seizing on the swinging tail / That fell within his scope, \n“I see,” quoth he, “the Elephant / Is very like a rope!”\nAnd so these men of Indostan / Disputed loud and long, \nEach in his own opinion / Exceeding stiff and strong, \nThough each was partly in the right, / And all were in the wrong!\n—“The Blind Men and the Elephant,” by John Godfrey Saxe\n\n\n380 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nThroughout this book we have treated architecture as something largely under \nyour control and shown how to make architectural decisions to achieve the goals \nand requirements in place for a system under development. But there is another \nside to the picture. Suppose you have been given responsibility for a system that \nalready exists, but you do not know its architecture. Perhaps the architecture \nwas never recorded by the original developers, now long gone. Perhaps it was \nrecorded but the documentation has been lost. Or perhaps it was recorded but \nthe documentation is no longer synchronized with the system after a series of \nchanges. How do you maintain such a system? How do you manage its evolution \nto maintain the quality attributes that its architecture (whatever it may be) has \nprovided for us?\nThis chapter surveys techniques that allow an analyst to build, maintain, and \nunderstand a representation of an existing architecture. This is a process of re-\nverse engineering, typically called architecture reconstruction. Architecture re-\nconstruction is used, by the architect, for two main purposes:\n■\n■To document an architecture where the documentation never existed or \nwhere it has become hopelessly out of date \n■\n■ To ensure conformance between the as-built architecture and the as-de-\nsigned architecture. \nIn architecture reconstruction, the “as-built” architecture of an implemented \nsystem is reverse-engineered from existing system artifacts. \nWhen a system is initially developed, its architectural elements are mapped \nto specific implementation elements: functions, classes, files, objects, and so \nforth. This is forward engineering. When we reconstruct those architectural el-\nements, we need to apply the inverses of the original mappings. But how do we \ngo about determining these mappings? One way is to use automated and semiau-\ntomated extraction tools; the second way is to probe the original design intent of \nthe architect. Typically we use a combination of both techniques in reconstruct-\ning an architecture.\nIn practice, architecture reconstruction is a tool-intensive activity. Tools ex-\ntract information about the system, typically by scouring the source code, but \nthey may also analyze other artifacts as well, such as build scripts or traces from \nrunning systems. But architectures are abstractions—they can not be seen in the \nlow-level implementation details, the programming constructs, of most systems. \nSo we need tools that aid in building and aggregating the abstractions that we \nneed, as architects, on top of the ground facts that we develop, as developers. If \nour tools are usable and accurate, the end result is an architectural representation \nthat aids the architect in reasoning about the system. Of course, if the original \narchitecture and its implementation are “spaghetti,” the reconstruction will faith-\nfully expose this lack of organization. \nArchitecture reconstruction tools are not, however, a panacea. In some cases, \nit may not be possible to generate a useful architectural representation. Further-\nmore, not all aspects of architecture are easy to automatically extract. Consider \n\n\n20.1  Architecture Reconstruction Process \n381\nthis: there is no programming language construct in any major programming lan-\nguage for “layer” or “connector” or other architectural elements; we can’t simply \npick these out of a source code file. Similarly, architectural patterns, if used, are \ntypically not explicitly documented in code. \nArchitecture reconstruction is an interpretive, interactive, and iterative pro-\ncess involving many activities; it is not automatic. It requires the skills and atten-\ntion of both the reverse-engineering expert and, in the best case, the architect (or \nsomeone who has substantial knowledge of the architecture). And whether the \nreconstruction is successful or not, there is a price to pay: the tools come with a \nlearning curve that requires time to climb.\n20.1  Architecture Reconstruction Process \nArchitecture reconstruction requires the skillful application of tools, often with a \nsteep learning curve. No single tool does the entire job. For one reason, there is \noften diversity in the number of implementation languages and dialects in which \na software system is implemented—a mature MRI scanner or a legacy banking \napplication may easily comprise more than ten different programming and script-\ning languages. No tool speaks every language. \nInstead we are inevitably led to a “tool set” approach to support architecture \nreconstruction activities. And so the first step in the reconstruction process is to \nset up the workbench. \nAn architecture reconstruction workbench should be open (making it easy to \nintegrate new tools as required) and provide an integration framework whereby \nnew tools that are added to the tool set do not impact the existing tools or data \nunnecessarily. \nWhether or not an explicit workbench is used, the software architecture re-\nconstruction process comprises the following phases (each elaborated in a subse-\nquent section):\n1.\t\nRaw view extraction. In the raw view extraction phase, raw information \nabout the architecture is obtained from various sources, primarily source \ncode, execution traces, and build scripts. Each of these sets of raw informa-\ntion is called a view.1\n2.\t\nDatabase construction. The database construction phase involves convert-\ning the raw extracted information into a standard form (because the various \nextraction tools may each produce their own form of output). This standard-\nized form of the extracted views is then used to populate a reconstruction \n1.  This use of the term “view” is consistent with our definition in Chapter 18: “a representation of a \nset of system elements and relations among them.”\n\n\n382 \nPart Three\t\n20—Architecture Reconstruction and Conformance\ndatabase. When the reconstruction process is complete, the database will be \nused to generate authoritative architecture documentation.\n3.\t\nView fusion and manipulation. The view fusion phase combines the var-\nious views of the information stored in the database. Individual views \nmay not contain complete or fully accurate information. View fusion can \nimprove the overall accuracy. For example, a static view extracted from \nsource code might miss dynamically bound information such as calling \nrelationships. This could then be combined with a dynamic view from an \nexecution trace, which will capture all dynamically bound calling infor-\nmation, but which may not provide complete coverage. The combination \nof these views will provide higher quality information than either could \nprovide alone. Furthermore, view creation and fusion is typically associ-\nated with some expert interpretation and manipulation. For example, an \nexpert might decide that a group of elements should be aggregated togeth-\ner to form a layer.\n4.\t\nArchitecture analysis. View fusion will result in a set of hypotheses about \nthe architecture. These hypotheses take the form of architectural elements \n(such as layers) and the constraints and relationships among them. These \nhypotheses need to be tested to see if they are correct, and that is the func-\ntion of the analysis step. Some of these hypotheses might be disproven, \nrequiring additional view extraction, fusion, and manipulation.\nThe four phases of architecture reconstruction are iterative. Figure 20.1 \ndepicts the major tasks of architecture reconstruction and their relationships \nand outputs. Solid lines represent data flow and dashed lines represent human \ninteraction.\nAll of these activities are greatly facilitated by engaging people who are fa-\nmiliar with the system. They can provide insights about what to look for—that is, \nwhat views are amenable to extraction—and provide a guided approach to view \nfusion and analysis. They can also point out or explain exceptions to the design \nrules (which will show up as violations of the hypotheses during the analysis \nphase). If the experts are long gone, reconstruction is still possible, but it may \nwell require more backtracking from incorrect initial guesses.\n20.2  Raw View Extraction\nRaw view extraction involves analyzing a system’s existing design and implementa-\ntion artifacts to construct one or more models of it. The result is a set of information \nthat is used in the view fusion activity to construct more-refined views of the system \nthat directly support the goals of the reconstruction, goals such as these: \n\n\n20.2  Raw View Extraction\n383\nKey:\nView Extraction\nLexical\nParsing\nInstrumentation\n. . .\nDatabase\nConstruction\nArchitecture\nAnalysis\nProcess step\nData flow\nInforms\nView Fusion\nand Manipulation\nDatabase\nArchitecture\nDocumentation\nFigure 20.1  Architecture reconstruction process\n■\n■Extracting and representing a target set of architectural views, to support \nthe overall architecture documentation effort.\n■\n■Answering specific questions about the architecture. For example, “What \ncomponents are potentially affected if I choose to rewrite component X?” \nor “How can I refactor my layering to remove cyclic dependencies?”\nThe raw view extraction process is a blend of the ideal (what information \ndo you want to discover about the architecture that will most help you meet the \ngoals of your reconstruction effort?) and the practical (what information can your \navailable tools actually extract and present?). \nFrom the source artifacts (code, header files, build files, and so on) and other \nartifacts (e.g., execution traces), you can identify and capture the elements of in-\nterest within the system (e.g., files, functions, variables) and their relationships to \nobtain several base system views. Table 20.1 shows a typical list of the elements \nand several relationships among them that might be extracted.\n\n\n384 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nTable 20.1  Examples of Extracted Elements and Relations\nSource \nElement\nRelation\nTarget \nElement\nDescription\nFile\nincludes\nFile\nC preprocessor #include of one \nfile by another\nFile\ncontains\nFunction\nDefinition of a function in a file\nFile\ndefines _ var\nVariable\nDefinition of a variable in a file\nDirectory\ncontains\nDirectory\nDirectory contains a subdirectory\nDirectory\ncontains\nFile\nDirectory contains a file\nFunction\ncalls\nFunction\nStatic function call\nFunction\naccess _ read\nVariable\nRead access on a variable\nFunction\naccess _ write\nVariable\nWrite access on a variable\nEach of the relationships between the elements gives different information \nabout the system:\n■\n■The calls relationship between functions helps us build a call graph. \n■\n■The includes relationship between the files gives us a set of dependen-\ncies between system files. \n■\n■The access_read and access_write relationships between func-\ntions and variables show us how data is used. Certain functions may write \na set of data and others may read it. This information is used to determine \nhow data is passed between various parts of the system. We can determine \nwhether or not a global data store is used or whether most information is \npassed through function calls.\n■\n■Certain elements or subsystems may be stored in particular directories, and \ncapturing relations such as dir_contains_file and dir_contains_\ndir is useful when trying to identify elements later.\n■\n■If the system to be reconstructed is object oriented, classes and methods \nare added to the list of elements to be extracted, and relationships such as \nclass_is_subclass_of_class and class_contains_method \nare extracted and used.\nInformation obtained can be categorized as either static or dynamic. Static \ninformation is obtained by observing only the system artifacts, while dynamic in-\nformation is obtained by observing how the system runs. The goal is to fuse both \nto create more accurate system views. \nIf the architecture of the system changes at runtime, that runtime config-\nuration should be captured and used when carrying out the reconstruction. For \n",
      "page_number": 397
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 405-414)",
      "start_page": 405,
      "end_page": 414,
      "detection_method": "topic_boundary",
      "content": "20.2  Raw View Extraction\n385\nexample, in some systems a configuration file is read in by the system at startup, \nor a newly started system examines its operating environment, and certain ele-\nments are executed or connections are made as a result.\nAnother reason to capture dynamic information is that some architecturally \nrelevant information may not exist in the source artifacts because of late binding. \nExamples of late binding include the following:\n■\n■Polymorphism\n■\n■Function pointers\n■\n■Runtime parameterization\n■\n■Plug-ins\n■\n■Service interactions mediated by brokers\nFurther, the precise topology of a system may not be determined until \nruntime. For example, in peer-to-peer systems, service-oriented architectures, \nand cloud computing, the topology of the system is established dynamically, \ndepending on the availability, loading, and even dynamic pricing of system re-\nsources. The topology of such systems cannot be directly recovered from their \nsource artifacts and hence cannot be reverse-engineered using static extraction \ntools.\nTherefore, it may be necessary to use tools that can generate dynamic in-\nformation about the system (e.g., profiling tools, instrumentation that generates \nruntime traces, or aspects in an aspect-oriented programming language that can \nmonitor dynamic activity). Of course, this requires that such tools be available \non the platforms on which the system executes. Also, it may be difficult to col-\nlect the results from code instrumentation. For example, embedded systems often \nhave no direct way to output such information.\nTable 20.2 summarizes some of the common categories of tools that might \nbe used to populate the views loaded into the reconstruction database.\nTools to analyze design models, build files, and executables can also be used \nto extract further information as required. For instance, build files include in-\nformation on module or file dependencies that exist within the system, and this \ninformation may not be reflected in the source code, or anywhere else. \nAn additional activity that is often required prior to loading a raw view into \nthe database is to prune irrelevant information. For example, in a C code base \nthere may be several main() routines, but only one of those (and its resulting \ncall graph) will be of concern for analysis. The others may be for test harnesses \nand other utility functions. Similarly if you are building or using libraries that \nare operating-system specific, you may only be interested in a specific OS (e.g., \nLinux) and thus want to discard the libraries for other platforms.\n\n\n386 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nTable 20.2  Tool Categories for Populating Reconstructed Architecture Views\nTool\nStatic or \nDynamic\nDescription\nParsers\nStatic\nParsers analyze the code and generate internal \nrepresentations from it (for the purpose of \ngenerating machine code). It is possible to save \nthis internal representation to obtain a view.\nAbstract Syntax \nTree (AST) \nAnalyzers\nAST analyzers do a similar job to parsers, but \nthey build an explicit tree representation of \nthe parsed information. We can build analysis \ntools that traverse the AST and output selected \npieces of architecturally relevant information in \nan appropriate format.\nLexical Analyzers\nLexical analyzers examine source artifacts \npurely as strings of lexical elements or tokens. \nThe user of a lexical analyzer can specify \na set of code patterns to be matched and \noutput. Similarly, a collection of ad hoc tools \nsuch as grep and Perl can carry out pattern \nmatching and searching within the code to \noutput some required information. All of these \ntools—code-generating parsers, AST-based \nanalyzers, lexical analyzers, and ad hoc pattern \nmatchers—are used to output static information.\nProfilers\nDynamic\nProfiling and code coverage analysis tools can \nbe used to output information about the code as \nit is being executed, and usually do not involve \nadding new code to the system. \nCode \nInstrumentation \nTools\nCode instrumentation, which has wide \napplicability in the field of testing, involves \nadding code to the system to output specific \ninformation while the system is executing. \nAspects, in an aspect-oriented programming \nlanguage, can serve the same purpose \nand have the advantage of keeping the \ninstrumentation code separate from the code \nbeing monitored.\n20.3  Database Construction\nSome of the information extracted from the raw view extraction phase, while \nnecessary for the process of reconstruction, may be too specific to aid in archi-\ntectural understanding. Consider Figure 20.2. In this figure we show a set of facts \nextracted from a code base consisting of classes and methods, and inclusion and \ncalling relations. Each element is plotted on a grid and each relation is drawn as a \n\n\n20.3  Database Construction\n387\nline between the elements. This view, while accurate, provides no insight into the \noverarching abstractions or coarse-grained structures present in the architecture.\nThus we need to manipulate such raw views, to collapse information (for \nexample, hiding methods inside class definitions), and to show abstractions (for \nexample, showing all of the connections between business objects and user inter-\nface objects, or identifying distinct layers).\nIt is helpful to use a database to store the extracted information because the \namount of information being stored is large, and the manipulations of the data \nare tedious and error-prone if done manually. Some reverse-engineering tools, \nsuch as Lattix, SonarJ, and Structure101, fully encapsulate the database, and so \nthe user of the tool need not be concerned with its operation. However, those who \nare using a suite of tools together—a workbench—will need to choose a database \nand decide on internal representations of the views.\nFigure 20.2  A raw extracted view: white noise\n\n\n388 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n20.4  View Fusion\nOnce the raw facts have been extracted and stored in a database, the reconstructor \ncan now perform view fusion. In this phase, the extracted views are manipulated \nto create fused views. Fused views combine information from one or more ex-\ntracted views, each of which may contain specialized information. For example, \na static call view might be fused with a dynamic call view. One might want to \ncombine these two views because a static call view will show all explicit calls \n(where method A calls method B) but will miss calls that are made via late bind-\ning mechanisms. A dynamically extracted call graph will never miss a call that is \nmade during an execution, but it suffers the “testing” problem: it will only report \nresults from those paths through the system that are traversed during its execu-\ntion. So a little-used part of the system—perhaps for initialization or error recov-\nery—might not show up in the dynamic view. Therefore we fuse these two views \nto produce a more complete and more accurate graph of system relationships.\nThe process of creating a fused view is the process of creating a hypothesis \nabout the architecture and a visualization of it to aid in analysis. These hypothe-\nses result in new aggregations that show various abstractions or clusterings of the \nelements (which may be source artifacts or previously identified abstractions). \nBy interpreting these fused views and analyzing them, it is possible to produce \nhypothesized architectural views of the system. These views can be interpreted, \nfurther refined, or rejected. There are no universal completion criteria for this \nprocess; it is complete when the architectural representation is sufficient to sup-\nport the analysis needs of its stakeholders.\nFor example, Figure 20.3 shows the early results of interacting with the tool \nSonarJ. SonarJ first extracts facts from a set of source code files (in this case, \nwritten in Java) and lets you define a set of layers and vertical slices through \nthose layers in a system. SonarJ will then instantiate the user-specified definitions \nof layers and slices and populate them with the extracted software elements. \nFigure 20.3  Hypothesized layers and vertical slices\n\n\n20.5  Architecture Analysis: Finding Violations\n389\nIn the figure there are five layers: Controller, Data, Domain, DSI, and Ser-\nvice. And there are six vertical slices defined that span these layers: Common, \nContact, Customer, Distribution, Request, and User. At this point, however, there \nare no relationships between the layers or vertical slides shown—this is merely \nan enumeration of the important system abstractions. \n20.5  Architecture Analysis: Finding Violations\nConsider the following situation: You have designed an architecture but you have \nsuspicions that the developers are not faithfully implementing what you devel-\noped. They may do this out of ignorance, or because they have differing agendas \nfor the system, or simply because they were rushing to meet a deadline and ig-\nnored any concern not on their critical path. Whatever the root cause, this diver-\ngence of the architecture and the implementation spells problems for you, the \narchitect. So how do you test and ensure conformance to the design?\nThere are two major possibilities for maintaining conformance between \ncode and architecture: \n■\n■Conformance by construction. Ensuring consistency by construction—that \nis, automatically generating a substantial part of the system based on an \narchitectural specification—is highly desirable because tools can guarantee \nconformance. Unfortunately, this approach has limited applicability. It can \nonly be applied in situations where engineers can employ specific architec-\nture-based development tools, languages, and implementation strategies. \nFor systems that are composed of existing parts or that require a style of \narchitecture or implementation outside those supported by generation tools, \nthis approach does not apply. And this is the vast majority of systems.\n■\n■Conformance by analysis. This technique aims to ensure conformance by \nanalyzing (reverse-engineering) system information to flag nonconform-\ning elements, so that they can be fixed: brought into conformance. When \nan implementation is sufficiently constrained so that modularization and \ncoding patterns can be identified with architectural elements, this tech-\nnique can work well. Unfortunately, however, the technique is limited in \nits applicability. There is an inherent mismatch between static, code-based \nstructures such as classes and packages (which are what programmers see) \nand the runtime structures, such as processes, threads, clients, servers, and \ndatabases, that are the essence of most architectural descriptions. Further \ncomplicating this analysis, the actual runtime structures may not be known \nor established until the program executes: clients and servers may come and \ngo dynamically, components not under direct control of the implementers \nmay be dynamically loaded, and so forth.\n\n\n390 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nWe will focus on the second option: conformance by analysis.\nIn the previous step, view fusion gave us a set of hypotheses about the ar-\nchitecture. These hypotheses take the form of architectural elements (sometimes \naggregated, such as layers) and the constraints and relationships among them. \nThese hypotheses need to be tested to see if they are correct—to see if they con-\nform with the architect’s intentions. That is the function of the analysis step.\nFigure 20.4 shows the results of adding relationships and constraints to the \narchitecture initially created in Figure 20.3. These relationship and constraints \nare information added by the architect, to reflect the design intent. In this ex-\nample, the architect has indicated the relationships between the layers of Figure \n20.3. These relationships are indicated by the directed lines drawn between the \nlayers (and vertical slices). Using these relationships and constraints, a tool such \nas SonarJ is able to automatically detect and report violations of the layering in \nthe software.\nWe can now see that the Data layer (row 2 in Figure 20.4) can access, and \nhence depends on, the DSI layer. We can further see that it may not access, and \nhas no dependencies on, Domain, Service, or Controller (rows 1, 3, and 5 in the \nfigure). \nIn addition we can see that the JUnit component in the “External” compo-\nnent is defined to be inaccessible. This is an example of an architectural con-\nstraint that is meant to pervade the entire system: no portion of the application \nshould depend upon JUnit, because this should only be used by test code. \nFigure 20.4  Layers, vertical slices, relationships, and constraints\n\n\n20.5  Architecture Analysis: Finding Violations\n391\nFigure 20.5 shows an example of an architecture violation of the previous \nrestriction. This violation is found by SonarJ by searching through its database, \napplying the user-defined patterns, and finding violations of those patterns. In this \nfigure you can see an arc between the Service layer and JUnit. This arc is high-\nlighted to indicate that this is an illegal dependency and an architectural viola-\ntion. (This figure also shows some additional dependencies, to external modules.)\nArchitecture reconstruction is a means of testing the conformance to such \nconstraints. The preceding example showed how these constraints might be de-\ntected and enforced using static code analysis. But static analysis is primarily \nuseful for understanding module structures. What if one needed to understand \nruntime information, as represented by C&C structures?\nIn the example given in Figure 20.6, an architecture violation was discov-\nered via dynamic analysis, using the research DiscoTect system. In this case an \nanalysis of the runtime architecture of the Duke’s Bank application—a simple \nEnterprise JavaBeans (EJB) banking application created by Sun Microsystems \nas a demonstration of EJB functionality—was performed. The code was “instru-\nmented” using AspectJ; instrumentation aspects were woven into the compiled \nbytecode of the EJB application. These aspects emitted events when methods en-\ntered or exited and when objects were constructed.\nFigure 20.5  Highlighting an architecture violation\n\n\n392 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nFigure 20.6  An architecture violation discovered by dynamic analysis\nFigure 20.6 shows that a “database write” connector was discovered in the \ndynamic analysis of the architecture. Sun’s EJB specification and its documented \narchitecture of Duke’s Bank forbid such connections. All database access is sup-\nposed to be managed by entity beans, and only by entity beans. Such architec-\ntural violations are difficult to find in the source code—often just a single line \nof code is involved—and yet can substantially affect the quality attributes of the \nresulting system. \n20.6  Guidelines\nThe following are a set of guidelines for the reconstruction process:\n■\n■Have a goal and a set of objectives or questions in mind before undertaking \nan architecture reconstruction project. In the absence of these, a lot of effort \ncould be spent on extracting information and generating architecture views \nthat may not be helpful or serve any useful purpose.\n\n\n20.7  Summary\n393\n■\n■Obtain some representation, however coarse, of the system before begin-\nning the detailed reconstruction process. This representation serves several \npurposes, including the following:\n■\n■It identifies what information needs to be extracted from the system.\n■\n■It guides the reconstructor in determining what to look for in the architec-\nture and what views to generate.\nIdentifying layers is a good place to start.\n■\n■In many cases, the existing documentation for a system may not accurate-\nly reflect the system as it is implemented. Therefore it may be necessary \nto disregard the existing documentation and use it only to generate the \nhigh-level views of the system, because it should give an indication of the \nhigh-level concepts.\n■\n■Tools can support the reconstruction effort and shorten the reconstruction \nprocess, but they cannot do an entire reconstruction effort automatically. \nThe work involved in the effort requires the involvement of people (archi-\ntects, maintainers, and developers) who are familiar with the system. It \nis important to get these people involved in the effort at an early stage as \nit helps the reconstructor get a better understanding of the system being \nreconstructed.\n20.7  Summary\nArchitecture reconstruction and architecture conformance are crucial tools in the \narchitect’s toolbox to ensure that a system is built the way it was designed, and \nthat it evolves in a way that is consistent with its creators’ intentions. All nontriv-\nial long-lived systems evolve: the code and the architecture both evolve. This is a \ngood thing. But if the code evolves in an ad hoc manner, the result will be the big \nball of mud, and the system’s quality attributes will inevitably suffer. The only \ndefense against this erosion is consistent attention to architecture quality, which \nimplies the need to maintain architecture conformance.\nThe results of architectural reconstruction can be used in several ways:\n■\n■If no documentation exists or if it is seriously out of date, the recovered \narchitectural representation can be used as a basis for documenting the ar-\nchitecture, as discussed in Chapter 18. \n■\n■It can be used to recover the as-built architecture, or to check conformance \nagainst an “as-designed” architecture. Conformance checking assures us \nthat our developers and maintainers have followed the architectural edicts \nset forth for them and are not eroding the architecture by breaking down ab-\nstractions, bridging layers, compromising information hiding, and so forth. \n\n\n394 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n■\n■The reconstruction can be used as the basis for analyzing the architec-\nture or as a starting point for reengineering the system to a new desired \narchitecture. \n■\n■Finally, the representation can be used to identify elements for reuse or to \nestablish an architecture-based software product line (see Chapter 25). \nThe software architecture reconstruction process comprises the following \nphases:\n1.\t\nRaw view extraction. In the raw view extraction phase, raw information \nabout the architecture is obtained from various sources, primarily source \ncode, execution traces, and build scripts. Each of these sets of raw informa-\ntion is called a view.\n2.\t\nDatabase construction. The database construction phase involves convert-\ning the extracted information into a standard form (because the various \nextraction tools may each produce their own form of output) and populating \na reconstruction database with this information.\n3.\t\nView fusion. The view fusion phase combines views of the information \nstored in the database.\n4.\t\nArchitecture analysis. View fusion has given us a set of hypotheses about \nthe architecture. These hypotheses take the form of architectural elements \n(sometimes aggregated, such as layers) and the constraints and relationships \namong them. These hypotheses need to be tested to see if they are correct, \nand that is the function of the analysis step.\n20.8  For Further Reading\nThe Software Engineering Institute (SEI) has developed two reconstruction \nworkbenches: Dali and Armin. Dali was our first attempt at creating a workbench \nfor architecture recovery and conformance [Kazman 99]. Armin, a complete re-\nwrite and rethink of Dali, is described in [O’Brien 03].\nBoth Armin and Dali were primarily focused on module structures of an \narchitecture. A later tool, called DiscoTect, was aimed at discovering C&C struc-\ntures. This is described in [Schmerl 06].\nMany other architecture reverse-engineering tools have been created. A few \nof the notable ones created in academia are [van Deursen 04], [Murphy 01], and \n[Storey 97].\nIn addition there are a number of commercial architecture extraction and \nreconstruction tools that have been slowly gaining market acceptance in the past \ndecade. Among these are the following:\n■\n■SonarJ (www.hello2morrow.com)\n■\n■Lattix (www.lattix.com)\n",
      "page_number": 405
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 415-426)",
      "start_page": 415,
      "end_page": 426,
      "detection_method": "topic_boundary",
      "content": "20.9  Discussion Questions \n395\n■\n■Understand (www.scitools.com)\nCai et al. [Cai 2011] compellingly demonstrate the need for architecture \nconformance testing in an experimental study that they conducted, wherein they \nfound that software engineering students, given UML designs for a variety of rel-\natively simple systems, violate those designs over 70 percent of the time.\nFinally, the set of guidelines presented in this chapter for how to go about \nreconstructing an architecture was excerpted from [Kazman 02].\n20.9  Discussion Questions \n1.\t\nSuppose that for a given system you wanted to extract the architectural \nstructures (as discussed in Chapter 1) listed in the table rows below. For \neach row, fill in each column to appraise each strategy listed in the columns. \n“VH” (very high) means the strategy would be very effective at extracting \nthis structure; “VL” means it would be very ineffective; “H,” M,” and “L” \nhave the obvious in-between values.\nArchitectural Structures\nInterviewing experts on  \nthe system\nReconstruction Strategies\nAnalyzing  \nstructure of  \nsource code files\nStatic  \nanalysis of \nsource code\nDynamic  \nanalysis of system’s \nexecution\nModule structures\nDecomposition\nUses\nLayers\nClass\nData model\nC&C \nstructures\nService (for SOA \nsystems)\nConcurrency\nAllocation structures\nDeployment\nImplementation\nWork assignment\n\n\n396 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n2.\t\nRecall that in layered systems, the relationship among layers is allowed to \nuse. Also recall that it is possible for one piece of software to use another \npiece without actually calling it—for example, by depending on it leaving \nsome shared resource in a usable state. Does this interpretation change your \nanswer above for the “Uses” and “Layers” structures?\n3.\t\nWhat inferences can you make about a system’s module structures from \nexamining a set of behavioral traces gathered dynamically?\n4.\t\nSuppose you believe that the architecture for a system follows a broker \npattern. What information would you want to extract from the source code \nto confirm or refute this hypothesis? What behavioral or interaction pattern \nwould you expect to observe at runtime?\n5.\t\nSuppose you hypothesize that a system makes use of particular tactics to \nachieve a particular quality attribute. Fill in the columns of the table below \nto show how you would go about verifying your hypothesis. (Begin by fill-\ning in column 1 with a particular tactic for the named quality attribute.)\nTactics for…\nReconstruction Strategies\nInterviewing \nexperts on \nthe system\nAnalyzing \nstructure of \nsource code \nfiles\nStatic \nanalysis of \nsource code\nDynamic \nanalysis of \nsystem’s \nexecution\nAvailability\nInteroperability\nModifiability\nPerformance\nSecurity\nTestability\nUsability\n6.\t\nSuppose you want to confirm that developers and maintainers had remained \nfaithful to an architecture over the lifetime of the system. Describe the re-\nconstruction and/or auditing processes you would undertake.\n\n\n397\n21\nArchitecture Evaluation\nFear cannot be banished, but it can be calm and without \npanic; it can be mitigated by reason and evaluation.\n—Vannevar Bush\nWe discussed analysis techniques in Chapter 14. Analysis lies at the heart of ar-\nchitecture evaluation, which is the process of determining if an architecture is fit \nfor the purpose for which it is intended. Architecture is such an important con-\ntributor to the success of a system and software engineering project that it makes \nsense to pause and make sure that the architecture you’ve designed will be able \nto provide all that’s expected of it. That’s the role of evaluation. Fortunately there \nare mature methods to evaluate architectures that use many of the concepts and \ntechniques you’ve already learned in previous chapters of this book.\n21.1  Evaluation Factors\nEvaluation usually takes one of three forms:\n■\n■Evaluation by the designer within the design process\n■\n■Evaluation by peers within the design process \n■\n■Analysis by outsiders once the architecture has been designed\nEvaluation by the Designer\nEvery time the designer makes a key design decision or completes a design \nmilestone, the chosen and competing alternatives should be evaluated using the \nanalysis techniques of Chapter 14. Evaluation by the designer is the “test” part \nof the “generate-and-test” approach to architecture design that we discussed in \nChapter 17.\n\n\n398 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nHow much analysis? This depends on the importance of the decision. Ob-\nviously, decisions made to achieve one of the driving architectural requirements \nshould be subject to more analysis than others, because these are the ones that \nwill shape critical portions of the architecture. But in all cases, performing anal-\nysis is a matter of cost and benefit. Do not spend more time on a decision than it \nis worth, but also do not spend less time on an important decision than it needs. \nSome specific considerations include these:\n■\n■The importance of the decision. The more important the decision, the more \ncare should be taken in making it and making sure it’s right.\n■\n■The number of potential alternatives. The more alternatives, the more time \ncould be spent in evaluating them. Try to eliminate alternatives quickly so \nthat the number of viable potential alternatives is small.\n■\n■Good enough as opposed to perfect. Many times, two possible alternatives \ndo not differ dramatically in their consequences. In such a case, it is more \nimportant to make a choice and move on with the design process than it is \nto be absolutely certain that the best choice is being made. Again, do not \nspend more time on a decision than it is worth.\nPeer Review\nArchitectural designs can be peer reviewed just as code can be peer reviewed. \nA peer review can be carried out at any point of the design process where a can-\ndidate architecture, or at least a coherent reviewable part of one, exists. There \nshould be a fixed amount of time allocated for the peer review, at least several \nhours and possibly half a day. A peer review has several steps:\n1.\t\nThe reviewers determine a number of quality attribute scenarios to \ndrive the review. Most of the time these scenarios will be architecturally \nsignificant requirements, but they need not be. These scenarios can be \ndeveloped by the review team or by additional stakeholders.\n2.\t\nThe architect presents the portion of the architecture to be evaluated. (At \nthis point, comprehensive documentation for it may not exist.) The review-\ners individually ensure that they understand the architecture. Questions at \nthis point are specifically for understanding. There is no debate about the \ndecisions that were made. These come in the next step.\n3.\t\nFor each scenario, the designer walks through the architecture and explains \nhow the scenario is satisfied. (If the architecture is already documented, \nthen the reviews can use it to assess for themselves how it satisfies the \nscenario.) The reviewers ask questions to determine two different types of \ninformation. First, they want to determine that the scenario is, in fact, sat-\nisfied. Second, they want to determine whether any of the other scenarios \nbeing considered will not be satisfied because of the decisions made in the \nportion of the architecture being reviewed.\n\n\n21.1  Evaluation Factors\n399\n4.\t\nPotential problems are captured. The list of potential problems forms the \nbasis for the follow-up of the review. If the potential problem is a real prob-\nlem, then it either must be fixed or a decision must be explicitly made by \nthe designers and the project manager that they are willing to accept the \nproblem and its probability of occurrence.\nIf the designers are using the ADD process described in Chapter 17, then a \npeer review can be done at the end of step 3 of each ADD iteration.\nAnalysis by Outsiders\nOutside evaluators can cast an objective eye on an architecture. “Outside” is rel-\native; this may mean outside the development project, outside the business unit \nwhere the project resides but within the same company; or outside the company \naltogether. To the degree that evaluators are “outside,” they are less likely to be \nafraid to bring up sensitive problems, or problems that aren’t apparent because of \norganizational culture or because “we’ve always done it that way.” \nOften, outsiders are chosen because they possess specialized knowledge \nor experience, such as knowledge about a quality attribute that’s important to \nthe system being examined, or long experience in successfully evaluating \narchitectures.\nAlso, whether justified or not, managers tend to be more inclined to listen to \nproblems uncovered by an outside team hired at considerable cost. (This can be \nunderstandably frustrating to project staff who may have been complaining about \nthe same problems to no avail for months.)\nIn principle, an outside team may evaluate a completed architecture, an in-\ncomplete architecture, or a portion of an architecture. In practice, because en-\ngaging them is complicated and often expensive, they tend to be used to evaluate \ncomplete architectures. \nContextual Factors\nFor peer reviews or outside analysis, there are a number of contextual factors \nthat must be considered when structuring an evaluation. These include the arti-\nfacts available, whether the results are public or private, the number and skill of \nevaluators, the number and identity of the participating stakeholders, and how the \nbusiness goals are understood by the evaluators.\n■\n■What artifacts are available? To perform an architectural evaluation, there \nmust be an artifact that describes the architecture. This must be located \nand made available. Some evaluations may take place after the system is \noperational. In this case, recovery tools as described in Chapter 20 may be \nused both to assist in discovering the architecture and to test that the as-\nbuilt system conforms to the as-designed system. \n\n\n400 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n■\n■Who sees the results? Some evaluations are performed with the full knowl-\nedge and participation of all of the stakeholders. Others are performed more \nprivately. The private evaluations may be done for a variety of reasons, \nranging from corporate culture to (in one case we know about) an execu-\ntive wanting to determine which of a collection of competitive systems he \nshould back in an internal dispute about the systems.\n■\n■Who performs the evaluation? Evaluations can be carried out by an individ-\nual or a team. In either case, the evaluator(s) should be highly skilled in the \ndomain and the various quality attributes for which the system is to be eval-\nuated. And for carrying out evaluation methods with extensive stakeholder \ninvolvement, excellent organizational and facilitation skills are a must.\n■\n■Which stakeholders will participate? The evaluation process should provide \na method to elicit the goals and concerns that the important stakeholders \nhave regarding the system. Identifying the individuals who are needed and \nassuring their participation in the evaluation is critical.\n■\n■What are the business goals? The evaluation should answer whether \nthe system will satisfy the business goals. If the business goals are not \nexplicitly captured and prioritized prior to the evaluation, then there should \nbe a portion of the evaluation dedicated to doing so.\n21.2  The Architecture Tradeoff Analysis Method\nThe Architecture Tradeoff Analysis Method (ATAM) has been used for over a de-\ncade to evaluate software architectures in domains ranging from automotive to fi-\nnancial to defense. The ATAM is designed so that evaluators need not be familiar \nwith the architecture or its business goals, the system need not yet be constructed, \nand there may be a large number of stakeholders. \nParticipants in the ATAM \nThe ATAM requires the participation and mutual cooperation of three groups:\n■\n■The evaluation team. This group is external to the project whose \narchitecture is being evaluated. It usually consists of three to five people. \nEach member of the team is assigned a number of specific roles to play \nduring the evaluation. (See Table 21.1 for a description of these roles, along \nwith a set of desirable characteristics for each. A single person may adopt \nseveral roles in an ATAM.) The evaluation team may be a standing unit in \nwhich architecture evaluations are regularly performed, or its members may \nbe chosen from a pool of architecturally savvy individuals for the occasion. \n\n\n21.2  The Architecture Tradeoff Analysis Method\n401\nThey may work for the same organization as the development team whose \narchitecture is on the table, or they may be outside consultants. In any case, \nthey need to be recognized as competent, unbiased outsiders with no hidden \nagendas or axes to grind. \n■\n■Project decision makers. These people are empowered to speak for the \ndevelopment project or have the authority to mandate changes to it. They \nusually include the project manager, and if there is an identifiable customer \nwho is footing the bill for the development, he or she may be present (or \nrepresented) as well. The architect is always included—a cardinal rule of \narchitecture evaluation is that the architect must willingly participate.\n■\n■Architecture stakeholders. Stakeholders have a vested interest in the \narchitecture performing as advertised. They are the ones whose ability to do \ntheir job hinges on the architecture promoting modifiability, security, high \nreliability, or the like. Stakeholders include developers, testers, integrators, \nmaintainers, performance engineers, users, builders of systems interacting \nwith the one under consideration, and others listed in Chapter 3. Their job \nduring an evaluation is to articulate the specific quality attribute goals that \nthe architecture should meet in order for the system to be considered a \nsuccess. A rule of thumb—and that is all it is—is that you should expect to \nenlist 12 to 15 stakeholders for the evaluation of a large enterprise-critical \narchitecture. Unlike the evaluation team and the project decision makers, \nstakeholders do not participate in the entire exercise.\nTable 21.1  ATAM Evaluation Team Roles\nRole\nResponsibilities\nTeam Leader\nSets up the evaluation; coordinates with client, making sure client’s \nneeds are met; establishes evaluation contract; forms evaluation \nteam; sees that final report is produced and delivered (although the \nwriting may be delegated)\nEvaluation \nLeader\nRuns evaluation; facilitates elicitation of scenarios; administers \nscenario selection/prioritization process; facilitates evaluation of sce-\nnarios against architecture; facilitates on-site analysis\nScenario \nScribe\nWrites scenarios on flipchart or whiteboard during scenario elicitation; \ncaptures agreed-on wording of each scenario, halting discussion until \nexact wording is captured\nProceedings \nScribe\nCaptures proceedings in electronic form on laptop or workstation: \nraw scenarios, issue(s) that motivate each scenario (often lost in the \nwording of the scenario itself), and resolution of each scenario when \napplied to architecture(s); also generates a printed list of adopted \nscenarios for handout to all participants\nQuestioner\nRaises issues of architectural interest, usually related to the quality \nattributes in which he or she has expertise\n\n\n402 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nOutputs of the ATAM\nAs in any testing process, a large benefit derives from preparing for the test. In \npreparation for an ATAM exercise, the project’s decision makers must prepare the \nfollowing:\n1.\t\nA concise presentation of the architecture. One of the requirements of the \nATAM is that the architecture be presented in one hour, which leads to an \narchitectural presentation that is both concise and, usually, understandable.\n2.\t\nArticulation of the business goals. Frequently, the business goals presented in \nthe ATAM are being seen by some of the assembled participants for the first \ntime, and these are captured in the outputs. This description of the business \ngoals survives the evaluation and becomes part of the project’s legacy. \nThe ATAM uses prioritized quality attribute scenarios as the basis for \nevaluating the architecture, and if those scenarios do not already exist (perhaps \nas a result of a prior requirements capture exercise or ADD activity), they are \ngenerated by the participants as part of the ATAM exercise. Many times, ATAM \nparticipants have told us that one of the most valuable outputs of ATAM is this \nnext output:\n3.\t\nPrioritized quality attribute requirements expressed as quality attribute sce-\nnarios. These quality attribute scenarios take the form described in Chap-\nter 4. These also survive past the evaluation and can be used to guide the \narchitecture’s evolution.\nThe primary output of the ATAM is a set of issues of concern about the \narchitecture. We call these risks:\n4.\t\nA set of risks and nonrisks. A risk is defined in the ATAM as an architec-\ntural decision that may lead to undesirable consequences in light of stated \nquality attribute requirements. Similarly, a nonrisk is an architectural deci-\nsion that, upon analysis, is deemed safe. The identified risks form the basis \nfor an architectural risk mitigation plan.\n5.\t\nA set of risk themes. When the analysis is complete, the evaluation team \nexamines the full set of discovered risks to look for overarching themes that \nidentify systemic weaknesses in the architecture or even in the architecture \nprocess and team. If left untreated, these risk themes will threaten the \nproject’s business goals. \nFinally, along the way, other information about the architecture is discovered \nand captured:\n6.\t\nMapping of architectural decisions to quality requirements. Architectural \ndecisions can be interpreted in terms of the qualities that they support or \nhinder. For each quality attribute scenario examined during an ATAM, those \n\n\n21.2  The Architecture Tradeoff Analysis Method\n403\narchitectural decisions that help to achieve it are determined and captured. \nThis can serve as a statement of rationale for those decisions.\n7.\t\nA set of identified sensitivity and tradeoff points. These are architectural \ndecisions that have a marked effect on one or more quality attributes. \nThe outputs of the ATAM are used to build a final written report that recaps \nthe method, summarizes the proceedings, captures the scenarios and their \nanalysis, and catalogs the findings.\nThere are intangible results of an ATAM-based evaluation. These include a \npalpable sense of community on the part of the stakeholders, open communica-\ntion channels between the architect and the stakeholders, and a better overall un-\nderstanding on the part of all participants of the architecture and its strengths and \nweaknesses. While these results are hard to measure, they are no less important \nthan the others and often are the longest-lasting.\nPhases of the ATAM\nActivities in an ATAM-based evaluation are spread out over four phases: \n■\n■In phase 0, “Partnership and Preparation,” the evaluation team leadership \nand the key project decision makers informally meet to work out the details \nof the exercise. The project representatives brief the evaluators about the \nproject so that the team can be supplemented by people who possess the \nappropriate expertise. Together, the two groups agree on logistics, such as \nthe time and place of meetings, who brings the flipcharts, and who supplies \nthe donuts and coffee. They also agree on a preliminary list of stakeholders \n(by name, not just role), and they negotiate on when the final report is to \nbe delivered and to whom. They deal with formalities such as a statement \nof work or nondisclosure agreements. The evaluation team examines the \narchitecture documentation to gain an understanding of the architecture and \nthe major design approaches that it comprises. Finally, the evaluation team \nleader explains what information the manager and architect will be expect-\ned to show during phase 1, and helps them construct their presentations if \nnecessary. \n■\n■Phase 1 and phase 2 are the evaluation phases, where everyone gets down \nto the business of analysis. By now the evaluation team will have studied \nthe architecture documentation and will have a good idea of what the \nsystem is about, the overall architectural approaches taken, and the quality \nattributes that are of paramount importance. During phase 1, the evaluation \nteam meets with the project decision makers (for one to two days) to \nbegin information gathering and analysis. For phase 2, the architecture’s \nstakeholders join the proceedings and analysis continues, typically for \ntwo days. Unlike the other phases, phase 1 and phase 2 comprise a set of \nspecific steps; these are detailed in the next section.\n\n\n404 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nTable 21.2  ATAM Phases and Their Characteristics\nPhase\nActivity\nParticipants\nTypical Duration\n0\nPartnership and \npreparation\nEvaluation team lead-\nership and key project \ndecision makers\nProceeds informally as \nrequired, perhaps over \na few weeks\n1\nEvaluation\nEvaluation team and  \nproject decision makers\n1–2 days followed by a \nhiatus of 1–3 weeks\n2\nEvaluation  \n(continued)\nEvaluation team, project \ndecision makers, and \nstakeholders\n2 days\n3\nFollow-up\nEvaluation team and \nevaluation client\n1 week\nSource: Adapted from [Clements 01b].\n■\n■Phase 3 is follow-up, in which the evaluation team produces and delivers \na written final report. It is first circulated to key stakeholders to make sure \nthat it contains no errors of understanding, and after this review is complete \nit is delivered to the person who commissioned the evaluation.\nTable 21.2 shows the four phases of the ATAM, who participates in each \none, and an approximate timetable. \nSteps of the Evaluation Phases\nThe ATAM analysis phases (phase 1 and phase 2) consist of nine steps. Steps 1 \nthrough 6 are carried out in phase 1 with the evaluation team and the project’s \ndecision makers: typically, the architecture team, project manager, and project \nsponsor. In phase 2, with all stakeholders present, steps 1 through 6 are \nsummarized and steps 7 through 9 are carried out.\nTable 21.3 shows a typical agenda for the first day of phase 1, which covers \nsteps 1 through 5. Step 6 in phase 1 is carried out the next day.\nStep 1: Present the ATAM.  The first step calls for the evaluation leader to \npresent the ATAM to the assembled project representatives. This time is used to \nexplain the process that everyone will be following, to answer questions, and to \nset the context and expectations for the remainder of the activities. Using a stan-\ndard presentation, the leader describes the ATAM steps in brief and the outputs of \nthe evaluation.\nStep 2: Present the Business Drivers.  Everyone involved in the eval-\nuation—the project representatives as well as the evaluation team members—\nneeds to understand the context for the system and the primary business drivers \n\n\n21.2  The Architecture Tradeoff Analysis Method\n405\nmotivating its development. In this step, a project decision maker (ideally the \nproject manager or the system’s customer) presents a system overview from a \nbusiness perspective. The presentation should describe the following:\n■\n■The system’s most important functions\n■\n■Any relevant technical, managerial, economic, or political constraints \n■\n■The business goals and context as they relate to the project\n■\n■The major stakeholders\n■\n■The architectural drivers (that is, the architecturally significant \nrequirements)\nStep 3: Present the Architecture.  Here, the lead architect (or architecture \nteam) makes a presentation describing the architecture at an appropriate level \nof detail. The “appropriate level” depends on several factors: how much of the \narchitecture has been designed and documented; how much time is available; and \nthe nature of the behavioral and quality requirements. \nIn this presentation the architect covers technical constraints such as \noperating system, hardware, or middleware prescribed for use, and other systems \nwith which the system must interact. Most important, the architect describes the \narchitectural approaches (or patterns, or tactics, if the architect is fluent in that \nvocabulary) used to meet the requirements.\nTo make the most of limited time, the architect’s presentation should have a \nhigh signal-to-noise ratio. That is, it should convey the essence of the architecture \nand not stray into ancillary areas or delve too deeply into the details of just a few \naspects. Thus, it is extremely helpful to brief the architect beforehand (in phase \n0) about the information the evaluation team requires. A template such as the one \nin the sidebar can help the architect prepare the presentation. Depending on the \narchitect, a dress rehearsal can be included as part of the phase 0 activities.\nTable 21.3  Agenda for Day 1 of the ATAM\nTime\nActivity\n0830 – 1000\nIntroductions; Step 1: Present the ATAM\n1000 – 1100 \nStep 2: Present Business Drivers\n1100 – 1130\nBreak\n1130 – 1230\nStep 3: Present Architecture\n1230 – 1330\nLunch\n1330 – 1430\nStep 4: Identify Architectural Approaches\n1430 – 1530\nStep 5: Generate Utility Tree\n1530 – 1600 \nBreak\n1600 – 1700\nStep 5: Generate Utility Tree (continued)\n\n\n406 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nArchitecture Presentation (Approximately 20 slides; 60 Minutes)\nDriving architectural requirements, the measurable quantities you \nassociate with these requirements, and any existing standards/models/\napproaches for meeting these (2–3 slides)\nImportant architectural information (4–8 slides):\n■\n■\nContext diagram—the system within the context in which it will exist. \nHumans or other systems with which the system will interact.\n■\n■\nModule or layer view—the modules (which may be subsystems or \nlayers) that describe the system’s decomposition of functionality, along \nwith the objects, procedures, functions that populate these, and the \nrelations among them (e.g., procedure call, method invocation, callback, \ncontainment).\n■\n■\nComponent-and-connector view—processes, threads along with the \nsynchronization, data flow, and events that connect them.\n■\n■\nDeployment view—CPUs, storage, external devices/sensors along with \nthe networks and communication devices that connect them. Also shown \nare the processes that execute on the various processors.\nArchitectural approaches, patterns, or tactics employed, including what \nquality attributes they address and a description of how the approaches \naddress those attributes (3–6 slides):\n■\n■\nUse of commercial off-the-shelf (COTS) products and how they are cho-\nsen/integrated (1–2 slides).\n■\n■\nTrace of 1 to 3 of the most important use case scenarios. If possible, \ninclude the runtime resources consumed for each scenario (1–3 slides).\n■\n■\nTrace of 1 to 3 of the most important change scenarios. If possible, \ndescribe the change impact (estimated size/difficulty of the change) in \nterms of the changed modules or interfaces (1–3 slides).\n■\n■\nArchitectural issues/risks with respect to meeting the driving \narchitectural requirements (2–3 slides).\n■\n■\nGlossary (1 slide).\nSource: Adapted from [Clements 01b].\nAs may be seen in the presentation template, we expect architectural views, \nas described in Chapters 1 and 18, to be the primary vehicle for the architect \nto convey the architecture. Context diagrams, component-and-connector views, \nmodule decomposition or layered views, and the deployment view are useful in \nalmost every evaluation, and the architect should be prepared to show them. Other \nviews can be presented if they contain information relevant to the architecture \nat hand, especially information relevant to achieving important quality attribute \ngoals. \n",
      "page_number": 415
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 427-435)",
      "start_page": 427,
      "end_page": 435,
      "detection_method": "topic_boundary",
      "content": "21.2  The Architecture Tradeoff Analysis Method\n407\nAs a rule of thumb, the architect should present the views that he or she \nfound most important during the creation of the architecture and the views that \nhelp to reason about the most important quality attribute concerns of the system.\nDuring the presentation, the evaluation team asks for clarification based on \ntheir phase 0 examination of the architecture documentation and their knowledge \nof the business drivers from the previous step. They also listen for and write down \nany architectural tactics or patterns they see employed.\nStep 4: Identify Architectural Approaches.  The ATAM focuses on \nanalyzing an architecture by understanding its architectural approaches. As we \nsaw in Chapter 13, architectural patterns and tactics are useful for (among other \nreasons) the known ways in which each one affects particular quality attributes. \nA layered pattern tends to bring portability and maintainability to a system, \npossibly at the expense of performance. A publish-subscribe pattern is scalable \nin the number of producers and consumers of data. The active redundancy tactic \npromotes high availability. And so forth.\nBy now, the evaluation team will have a good idea of what patterns and \ntactics the architect used in designing the system. They will have studied the \narchitecture documentation, and they will have heard the architect’s presentation \nin step 3. During that step, the architect is asked to explicitly name the patterns \nand tactics used, but the team should also be adept at spotting ones not mentioned.\nIn this short step, the evaluation team simply catalogs the patterns and \ntactics that have been identified. The list is publicly captured by the scribe for all \nto see and will serve as the basis for later analysis.\nStep 5: Generate Quality Attribute Utility Tree.  In this step, the quality \nattribute goals are articulated in detail via a quality attribute utility tree. Utility \ntrees, which were described in Chapter  16, serve to make the requirements \nconcrete by defining precisely the relevant quality attribute requirements that the \narchitects were working to provide. \nThe important quality attribute goals for the architecture under consideration \nwere named in step 2, when the business drivers were presented, but not to \nany degree of specificity that would permit analysis. Broad goals such as \n“modifiability” or “high throughput” or “ability to be ported to a number of \nplatforms” establish important context and direction, and provide a backdrop \nagainst which subsequent information is presented. However, they are not \nspecific enough to let us tell if the architecture suffices. Modifiable in what way? \nThroughput that is how high? Ported to what platforms and in how much time? \nIn this step, the evaluation team works with the project decision makers to \nidentify, prioritize, and refine the system’s most important quality attribute goals. \nThese are expressed as scenarios, as described in Chapter 4, which populate the \nleaves of the utility tree.\n\n\n408 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nStep 6: Analyze Architectural Approaches.  Here the evaluation team \nexamines the highest-ranked scenarios (as identified in the utility tree) one \nat a time; the architect is asked to explain how the architecture supports each \none. Evaluation team members—especially the questioners—probe for the \narchitectural approaches that the architect used to carry out the scenario. Along \nthe way, the evaluation team documents the relevant architectural decisions and \nidentifies and catalogs their risks, nonrisks, sensitivity points, and tradeoffs. For \nwell-known approaches, the evaluation team asks how the architect overcame \nknown weaknesses in the approach or how the architect gained assurance that \nthe approach sufficed. The goal is for the evaluation team to be convinced that \nthe instantiation of the approach is appropriate for meeting the attribute-specific \nrequirements for which it is intended.\nScenario walkthrough leads to a discussion of possible risks, nonrisks, \nsensitivity points, or tradeoff points. For example:\n■\n■The frequency of heartbeats affects the time in which the system can detect \na failed component. Some assignments will result in unacceptable values of \nthis response—these are risks. \n■\n■The number of simultaneous database clients will affect the number of \ntransactions that a database can process per second. Thus, the assignment \nof clients to the server is a sensitivity point with respect to the response as \nmeasured in transactions per second. \n■\n■The frequency of heartbeats determines the time for detection of a fault. \nHigher frequency leads to improved availability but will also consume \nmore processing time and communication bandwidth (potentially leading to \nreduced performance). This is a tradeoff. \nThese, in turn, may catalyze a deeper analysis, depending on how the architect \nresponds. For example, if the architect cannot characterize the number of clients \nand cannot say how load balancing will be achieved by allocating processes \nto hardware, there is little point in a sophisticated performance analysis. If such \nquestions can be answered, the evaluation team can perform at least a rudimentary, \nor back-of-the-envelope, analysis to determine if these architectural decisions are \nproblematic vis-à-vis the quality attribute requirements they are meant to address. \nThe analysis is not meant to be comprehensive. The key is to elicit sufficient \narchitectural information to establish some link between the architectural \ndecisions that have been made and the quality attribute requirements that need to \nbe satisfied. \nFigure 21.1 shows a template for capturing the analysis of an architectural \napproach for a scenario. As shown, based on the results of this step, the evaluation \nteam can identify and record a set of sensitivity points and tradeoffs, risks, and \nnonrisks. \nAt the end of step 6, the evaluation team should have a clear picture of the \nmost important aspects of the entire architecture, the rationale for key design \ndecisions, and a list of risks, nonrisks, sensitivity points, and tradeoff points. \nAt this point, phase 1 is concluded.\n\n\n21.2  The Architecture Tradeoff Analysis Method\n409\nScenario #: A12\nScenario: Detect and recover from HW failure\nof main switch.\nAttribute(s)\nAvailability\nEnvironment\nNormal operations\nStimulus\nOne of the CPUs fails\nResponse\n0.999999 availability of switch\nArchitectural decisions\nSensitivity\nTradeoff\nRisk\nNonrisk\nBackup CPU(s)\nS2\nR8\nNo backup data channel\nS3\nT3\nR9\nWatchdog\nS4\nN12\nHeartbeat\nS5\nN13\nFailover routing\nS6\nN14\nReasoning\nEnsures no common mode failure by using different hardware\nand operating system (see Risk 8)\nWorst-case rollover is accomplished in 4 seconds as computing\nstate takes that long at worst\nGuaranteed to detect failure within 2 seconds based on rates of\nheartbeat and watchdog\nWatchdog is simple and has proved reliable\nAvailability requirement might be at risk due to lack of backup\ndata channel ... (see Risk 9)\nArchitecture\ndiagram\nBackup\ny\nSwitch\nCPU\nheartbeat\n(1 sec.)\n(OS1)\nCPU with\nWatchdog\n(OS2)\nPrimar\nima\nimary\nCPU\n(OS1)\nFigure 21.1  Example of architecture approach analysis (adapted from \n[Clements 01b])\nHiatus and Start of Phase 2.  The evaluation team summarizes what it has \nlearned and interacts informally (usually by phone) with the architect during a \nhiatus of a week or two. More scenarios might be analyzed during this period, if \ndesired, or questions of clarification can be resolved. \n\n\n410 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nPhase 2 is attended by an expanded list of participants with additional \nstakeholders attending. To use an analogy from programming: Phase 1 is akin to \nwhen you test your own program, using your own criteria. Phase 2 is when you \ngive your program to an independent quality assurance group, who will likely \nsubject your program to a wider variety of tests and environments.\nIn phase 2, step 1 is repeated so that the stakeholders understand the method \nand the roles they are to play. Then the evaluation leader recaps the results of \nsteps 2 through 6, and shares the current list of risks, nonrisks, sensitivity points, \nand tradeoffs. Now the stakeholders are up to speed with the evaluation results so \nfar, and the remaining three steps can be carried out. \nStep 7: Brainstorm and Prioritize Scenarios.  In this step, the evaluation \nteam asks the stakeholders to brainstorm scenarios that are operationally \nmeaningful with respect to the stakeholders’ individual roles. A maintainer will \nlikely propose a modifiability scenario, while a user will probably come up with \na scenario that expresses useful functionality or ease of operation, and a quality \nassurance person will propose a scenario about testing the system or being able to \nreplicate the state of the system leading up to a fault. \nWhile utility tree generation (step 5) is used primarily to understand how \nthe architect perceived and handled quality attribute architectural drivers, the \npurpose of scenario brainstorming is to take the pulse of the larger stakeholder \ncommunity: to understand what system success means for them. Scenario \nbrainstorming works well in larger groups, creating an atmosphere in which the \nideas and thoughts of one person stimulate others’ ideas. \nOnce the scenarios have been collected, they must be prioritized, for the \nsame reasons that the scenarios in the utility tree needed to be prioritized: the \nevaluation team needs to know where to devote its limited analytical time. First, \nstakeholders are asked to merge scenarios they feel represent the same behavior \nor quality concern. Then they vote for those they feel are most important. Each \nstakeholder is allocated a number of votes equal to 30 percent of the number of \nscenarios,1 rounded up. So, if there were 40 scenarios collected, each stakeholder \nwould be given 12 votes. These votes can be allocated in any way that the \nstakeholder sees fit: all 12 votes for 1 scenario, 1 vote for each of 12 distinct \nscenarios, or anything in between. \nThe list of prioritized scenarios is compared with those from the utility tree \nexercise. If they agree, it indicates good alignment between what the architect \nhad in mind and what the stakeholders actually wanted. If additional driving \nscenarios are discovered—and they usually are—this may itself be a risk, if the \ndiscrepancy is large. This would indicate that there was some disagreement in the \nsystem’s important goals between the stakeholders and the architect. \n1.  This is a common facilitated brainstorming technique.\n\n\n21.2  The Architecture Tradeoff Analysis Method\n411\nStep 8: Analyze Architectural Approaches.  After the scenarios have \nbeen collected and prioritized in step 7, the evaluation team guides the architect \nin the process of carrying out the highest ranked scenarios. The architect explains \nhow relevant architectural decisions contribute to realizing each one. Ideally this \nactivity will be dominated by the architect’s explanation of scenarios in terms of \npreviously discussed architectural approaches. \nIn this step the evaluation team performs the same activities as in step 6, \nusing the highest-ranked, newly generated scenarios. \nTypically, this step might cover the top five to ten scenarios, as time permits.\nStep 9: Present Results.  In step 9, the evaluation team groups risks into \nrisk themes, based on some common underlying concern or systemic deficiency. \nFor example, a group of risks about inadequate or out-of-date documentation \nmight be grouped into a risk theme stating that documentation is given insufficient \nconsideration. A group of risks about the system’s inability to function in the face \nof various hardware and/or software failures might lead to a risk theme about \ninsufficient attention to backup capability or providing high availability. \nFor each risk theme, the evaluation team identifies which of the business \ndrivers listed in step 2 are affected. Identifying risk themes and then relating them \nto specific drivers brings the evaluation full circle by relating the final results \nto the initial presentation, thus providing a satisfying closure to the exercise. \nAs important, it elevates the risks that were uncovered to the attention of \nmanagement. What might otherwise have seemed to a manager like an esoteric \ntechnical issue is now identified unambiguously as a threat to something the \nmanager is on record as caring about. \nThe collected information from the evaluation is summarized and presented \nto stakeholders. This takes the form of a verbal presentation with slides. The \nevaluation leader recapitulates the steps of the ATAM and all the information \ncollected in the steps of the method, including the business context, driving \nrequirements, constraints, and architecture. Then the following outputs are \npresented:\n■\n■The architectural approaches documented\n■\n■The set of scenarios and their prioritization from the brainstorming\n■\n■The utility tree \n■\n■The risks discovered\n■\n■The nonrisks documented\n■\n■The sensitivity points and tradeoff points found\n■\n■Risk themes and the business drivers threatened by each one\n\n\n412 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n“. . . but it was OK.”\nYears of experience have taught us that no architecture evaluation exer-\ncise ever goes completely by the book. And yet for all the ways that an \nexercise might go terribly wrong, for all the details that can be overlooked, \nfor all the fragile egos that can be bruised, and for all the high stakes that \nare on the table, we have never had an architecture evaluation exercise \nspiral out of control. Every single one has been a success, as measured \nby the feedback we gather from clients.\nWhile they all turned out successfully, there were a few memorable \ncliffhangers. \nMore than once, we began an architecture evaluation only to discover \nthat the development organization had no architecture to be evaluated. \nSometimes there was a stack of class diagrams or vague text descriptions \nmasquerading as an architecture. Once we were promised that the archi-\ntecture would be ready by the time the exercise began, but in spite of good \nintentions, it wasn’t. (We weren’t always so prudent about pre-exercise \npreparation and qualification. Our current diligence was a result of experi-\nences like these.) But it was OK. In cases like these, the evaluation’s main \nresults included the articulated set of quality attributes, a “whiteboard” ar-\nchitecture sketched during the exercise, plus a set of documentation obliga-\ntions on the architect. In all cases, the client felt that the detailed scenarios, \nthe analysis we were able to perform on the elicited architecture, plus the \nrecognition of what needed to be done, more than justified the exercise.\nA couple of times we began an evaluation only to lose the architect in \nthe middle of the exercise. In one case, the architect resigned between \npreparation and execution of the evaluation. This was an organization in \nturmoil and the architect simply got a better offer in a calmer environment \nelsewhere. Normally we don’t proceed without the architect, but it was OK. \nIn this case the architect’s apprentice stepped in. A little additional prework \nto prepare him, and we were all set. The evaluation went off as planned, \nand the preparation that the apprentice did for the exercise helped mightily \nto prepare him to step into the architect’s shoes.\nOnce we discovered halfway through an ATAM exercise that the archi-\ntecture we had prepared to evaluate was being jettisoned in favor of a new \none that nobody had bothered to mention. During step 6 of phase 1, the ar-\nchitect responded to a problem raised by a scenario by casually mentioning \nthat “the new architecture” would not suffer from that deficiency. Everyone \nin the room, stakeholders and evaluators alike, looked at each other in the \npuzzled silence that followed. “What new architecture?” I asked blankly, and \nout it came. The developing organization (a contractor for the U.S. military, \nwhich had commissioned the evaluation), had prepared a new architecture \nfor the system, to handle the more stringent requirements they knew were \ncoming in the future. We called a timeout, conferred with the architect and \nthe client, and decided to continue the exercise using the new architecture \nas the subject instead of the old. We backed up to step 3 (the architecture \npresentation), but everything else on the table—business drivers, utility \n\n\n21.2  The Architecture Tradeoff Analysis Method\n413\ntree, scenarios—still were completely valid. The evaluation proceeded as \nbefore, and at the conclusion of the exercise our military client was ex-\ntremely pleased at the knowledge gained.\nIn perhaps the most bizarre evaluation in our experience, we lost the \narchitect midway through phase 2. The client for this exercise was the \nproject manager in an organization undergoing a massive restructuring. \nThe manager was a pleasant gentleman with a quick sense of humor, but \nthere was an undercurrent about him that said he was not to be crossed. \nThe architect was being reassigned to a different part of the organization \nin the near future; this was tantamount to being fired from the project, and \nthe manager said he wanted to establish the quality of the architecture \nbefore his architect’s awkward departure. (We didn’t find any of this out \nuntil after the evaluation.) When we set up the ATAM exercise, the manager \nsuggested that the junior designers attend. “They might learn something,” \nhe said. We agreed. As the exercise began, our schedule (which was very \ntight to begin with) kept being disrupted. The manager wanted us to meet \nwith his company’s executives. Then he wanted us to have a long lunch \nwith someone who could, he said, give us more architectural insights. The \nexecutives, it turned out, were busy just now, and so could we come back \nand meet with them a bit later? By now, phase 2 was thrown off schedule \nby so much that the architect, to our horror, had to leave to fly back to his \nhome in a distant city. He was none too happy that his architecture was \ngoing to be evaluated without him. The junior designers, he said, would \nnever be able to answer our questions. Before his departure, our team \nhuddled. The exercise seemed to be teetering on the brink of disaster. We \nhad an unhappy departing architect, a blown schedule, and questionable \nexpertise available. We decided to split our evaluation team. One half of the \nteam would continue with phase 2 using the junior designers as our infor-\nmation resource. The second half of the team would continue with phase 2 \nby telephone the next day with the architect. Somehow we would make the \nbest of a bad situation. \nSurprisingly, the project manager seemed completely unperturbed by \nthe turn of events. “It will work out, I’m sure,” he said pleasantly, and then \nretreated to confer with various vice presidents about the reorganization. \nI led the team interviewing the junior designers. We had never gotten \na completely satisfactory architecture presentation from the architect. \nDiscrepancies in the documentation were met with a breezy “Oh, well, \nthat’s not how it really works.” So I decided to start over with ATAM step 3. \nWe asked the half dozen or so designers what their view of the architecture \nwas. “Could you draw it?” I asked them. They looked at each other ner-\nvously, but one said, “I think I can draw part of it.” He took to the whiteboard \nand drew a very reasonable component-and-connector view. Someone else \nvolunteered to draw a process view. A third person drew the architecture for \nan important offline part of the system. Others jumped in to assist.\nAs we looked around the room, everyone was busy transcribing the \nwhiteboard pictures. None of the pictures corresponded to anything we \nhad seen in the documentation so far. “Are these diagrams documented \n\n\n414 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nanywhere?” I asked. One of the designers looked up from his busy scrib-\nbling for a moment to grin. “They are now,” he said.\nAs we proceeded to step 8, analyzing the architecture using the \nscenarios previously captured, the designers did an astonishingly good \njob of working together to answer our questions. Nobody knew everything, \nbut everybody knew something. Together in a half day, they produced a \nclear and consistent picture of the whole architecture that was much more \ncoherent and understandable than anything the architect had been willing \nto produce in two whole days of pre-exercise discussion. And by the end \nof phase 2, the design team was transformed. This erstwhile group of \ninformation-starved individuals with limited compartmentalized knowledge \nbecame a true architecture team. The members drew out and recognized \neach others’ expertise. This expertise was revealed and validated in front \nof everyone—and most important, in front of their project manager, who \nhad slipped back into the room to observe. There was a look of supreme \nsatisfaction on his face. It began to dawn on me that—you guessed it—it \nwas OK. \nIt turned out that this project manager knew how to manipulate events \nand people in ways that would have impressed Machiavelli. The architect’s \ndeparture was not because of the reorganization, but merely coincident \nwith it. The project manager had orchestrated it. The architect had, the \nmanager felt, become too autocratic and dictatorial, and the manager \nwanted the junior design staff to be given the opportunity to mature and \ncontribute. The architect’s mid-exercise departure was exactly what the \nproject manager had wanted. And the design team’s emergence under \nfire had been the primary purpose of the evaluation exercise all along. \nAlthough we found several important issues related to the architecture, the \nproject manager knew about every one of them before we ever arrived. In \nfact, he made sure we uncovered some of them by a few discreet remarks \nduring breaks or after a day’s session.\nWas this exercise a success? The client could not have been more \npleased. His instincts about the architecture’s strengths and weaknesses \nwere confirmed. We were instrumental in helping his design team, which \nwould guide the system through the stormy seas of the company’s \nreorganization, come together as an effective and cohesive unit at exactly \nthe right time. And the client was so pleased with our final report that he \nmade sure the company’s board of directors saw it.\nThese cliffhangers certainly stand out in our memory. There was no \narchitecture documented. But it was OK. It wasn’t the right architecture. \nBut it was OK. There was no architect. But it was OK. The client really only \nwanted to effect a team reorganization. In every instance we reacted as \nreasonably as we could, and each time it was OK.\nWhy? Why, time after time, does it turn out OK? I think there are three \nreasons.\nFirst, the people who have commissioned the architecture evaluation \nreally want it to succeed. The architect, developers, and stakeholders \n\n\n21.3  Lightweight Architecture Evaluation\n415\nassembled at the client’s behest also want it to succeed. As a group, they \nhelp to keep the exercise marching toward the goal of architectural insight. \nSecond, we are always honest. If we feel that the exercise is derailing, \nwe call a timeout and confer among ourselves, and usually confer with \nthe client. While a small amount of bravado can come in handy during \nan exercise, we never, ever try to bluff our way through an evaluation. \nParticipants can detect that instinctively, and the evaluation team must \nnever lose the respect of the other participants. Third, the methods are \nconstructed to establish and maintain a steady consensus throughout the \nexercise. There are no surprises at the end. The participants lay down \nthe ground rules for what constitutes a suitable architecture, and they \ncontribute to the risks uncovered at every step of the way.\nSo: Do the best job you can. Be honest. Trust the methods. Trust in the \ngoodwill and good intentions of the people you have assembled. And it will \nbe OK. (Adapted from [Clements 01b])\n—PCC\n21.3  Lightweight Architecture Evaluation\nAlthough we attempt to use time in an ATAM exercise as efficiently as possible, \nit remains a substantial undertaking. It requires some 20 to 30 person-days of \neffort from an evaluation team, plus even more for the architect and stakeholders. \nInvesting this amount of time only makes sense on a large and costly project, \nwhere the risks of making a major mistake in the architecture are unacceptable. \nFor this reason, we have developed a Lightweight Architecture Evaluation \nmethod, based on the ATAM, for smaller, less risky projects. A Lightweight \nArchitecture Evaluation exercise may take place in a single day, or even \na half-day meeting. It may be carried out entirely by members internal to the \norganization. Of course this lower level of scrutiny and objectivity may not \nprobe the architecture as deeply, but this is a cost/benefit tradeoff that is entirely \nappropriate for many projects.\nBecause the participants are all internal to the organization and fewer in \nnumber than for the ATAM, giving everyone their say and achieving a shared \nunderstanding takes much less time. Hence the steps and phases of a Lightweight \nArchitecture Evaluation can be carried out more quickly. A suggested schedule \nfor phases 1 and 2 is shown in Table 21.4.\n",
      "page_number": 427
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 436-446)",
      "start_page": 436,
      "end_page": 446,
      "detection_method": "topic_boundary",
      "content": "416 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nTable 21.4  A Typical Agenda for Lightweight Architecture Evaluation\nStep\nTime \nAllotted\nNotes\n1: Present the ATAM\n0 hrs\nThe participants are familiar with the process. \nThis step may be omitted.\n2: Present Business \nDrivers\n0.25 hrs\nThe participants are expected to understand \nthe system and its business goals and their \npriorities. Fifteen minutes is allocated for a brief \nreview to ensure that these are fresh in every-\none’s mind and that there are no surprises.\n3: Present Architecture\n0.5 hrs\nAgain, all participants are expected to be famil-\niar with the system and so a brief overview of \nthe architecture, using at least module and C&C \nviews, is presented and 1 to 2 scenarios are \ntraced through these views.\n4: Identify Architectural \nApproaches\n0.25 hrs\nThe architecture approaches for specific quality \nattribute concerns are identified by the architect. \nThis may be done as a portion of step 3.\n5: Generate Quality \nAttribute Utility Tree\nVariable\n0.5 hrs – \n1.5 hrs\nScenarios might exist: part of previous evals, \npart of design, part of requirements elicitation. \nIf you’ve got ’em, use ’em and make them into a \ntree. Half hour. Otherwise, it will take longer. \nA utility tree should already exist; the team re-\nviews the existing tree and updates it, if needed, \nwith new scenarios, new response goals, or new \nscenario priorities and risk assessments.\n6: Analyze Architectural \nApproaches\n2–3 hrs\nThis step—mapping the highly ranked scenari-\nos onto the architecture—consumes the bulk of \nthe time and can be expanded or contracted as \nneeded.\n7: Brainstorm and \nPrioritize Scenarios\n0 hrs\nThis step can be omitted as the assembled (in-\nternal) stakeholders are expected to contribute \nscenarios expressing their concerns in step 5.\n8: Analyze Architectural \nApproaches\n0 hrs\nThis step is also omitted, since all analysis is \ndone in step 6.\n9: Present Results\n0.5 hrs\nAt the end of an evaluation, the team reviews \nthe existing and newly discovered risks, non-\nrisks, sensitivities, and tradeoffs and discusses \nwhether any new risk themes have arisen. \nTOTAL\n4–6 hrs\nThere is no final report, but (as in the regular ATAM) a scribe is responsible \nfor capturing results, which can then be distributed and serve as the basis for risk \nremediation.\nAn entire Lightweight Architecture Evaluation can be prosecuted in less than \na day—perhaps an afternoon. The results will depend on how well the assembled \nteam understands the goals of the method, the techniques of the method, and the \nsystem itself. The evaluation team, being internal, is typically not objective, and \nthis may compromise the value of its results—one tends to hear fewer new ideas \n\n\n21.5  For Further Reading\n417\nand fewer dissenting opinions. But this version of evaluation is inexpensive, easy \nto convene, and relatively low ceremony, so it can be quickly deployed whenever \na project wants an architecture quality assurance sanity check.\n21.4  Summary\nIf a system is important enough for you to explicitly design its architecture, then \nthat architecture should be evaluated. \nThe number of evaluations and the extent of each evaluation may vary from \nproject to project. A designer should perform an evaluation during the process of \nmaking an important decision. Lightweight evaluations can be performed several \ntimes during a project as a peer review exercise. \nThe ATAM is a comprehensive method for evaluating software architectures. \nIt works by having project decision makers and stakeholders articulate a precise \nlist of quality attribute requirements (in the form of scenarios) and by illuminating \nthe architectural decisions relevant to carrying out each high-priority scenario. \nThe decisions can then be understood in terms of risks or nonrisks to find any \ntrouble spots in the architecture.\nLightweight Architecture Evaluation, based on the ATAM, provides an \ninexpensive, low-ceremony architecture evaluation that can be carried out in an \nafternoon. \n21.5  For Further Reading\nFor a more comprehensive treatment of the ATAM, see [Clements 01b].\nMultiple case studies of applying the ATAM are available. They can be found \nby going to www.sei.cmu.edu/library and searching for “ATAM case study.”\nTo understand the historical roots of the ATAM, and to see a second (simpler) \narchitecture evaluation method, you can read about the software architecture \nanalysis method (SAAM) in [Kazman 94]. \nSeveral lighter weight architecture evaluation methods have been developed. \nThey can be found in [Bouwers 10], [Kanwal 10], and [Bachmann 11].\nMaranzano et al. have published a paper dealing with a long tradition of \narchitecture evaluation at AT&T and its successor companies [Maranzano 05]. \n\n\n418 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n21.6  Discussion Questions \n1.\t\nThink of a software system that you’re working on. Prepare a 30-minute \npresentation on the business drivers for this system.\n2.\t\nIf you were going to evaluate the architecture for this system, who would \nyou want to participate? What would be the stakeholder roles and who \ncould you get to represent those roles?\n3.\t\nUse the utility tree that you wrote for the ATM in Chapter 16 and the \ndesign that you sketched for the ATM in Chapter 17 to perform the scenario \nanalysis step of the ATAM. Capture any risks and nonrisks that you \ndiscover. Better yet, perform the analysis on the design carried out by a \ncolleague.\n4.\t\nIt is not uncommon for an organization to evaluate two competing \narchitectures. How would you modify the ATAM to produce a quantitative \noutput that facilitates this comparison?\n5.\t\nSuppose you’ve been asked to evaluate the architecture for a system in \nconfidence. The architect isn’t available. You aren’t allowed to discuss the \nevaluation with any of the system’s stakeholders. How would you proceed?\n6.\t\nUnder what circumstances would you want to employ a full-strength ATAM \nand under what circumstances would you want to employ a Lightweight \nArchitecture Evaluation?\n\n\n419\n22\nManagement and \nGovernance\nHow does a project get to be a year behind \nschedule? One day at a time.\n—Fred Brooks\nIn this chapter we deal with those aspects of project management and governance \nthat are important for an architect to know. The project manager is the person \nwith whom you, as the architect, must work most closely, from an organizational \nperspective, and consequently it is important for you to have an understanding of \nthe project manager’s problems and the techniques available to solve those prob-\nlems. We will deal with project management from the perspectives of planning, \norganizing, implementing, and measuring. We will also discuss various gover-\nnance issues associated with architecture.\nIn this chapter, we advocate a middleweight approach to architecture. It has the \nfollowing aspects:\n■\n■Design the software architecture\n■\n■Use the architecture to develop realistic schedules\n■\n■Use incremental development to get to market quickly\nArchitecture is most useful in medium- to large-scale projects—projects that typ-\nically have multiple teams, too much complexity for any individual to fully com-\nprehend, substantial investment, and multiyear duration. For such projects the \nteams need to coordinate, quality attribute problems are not easily corrected, and \nmanagement demands both short time to market and adequate oversight. Light-\nweight management methods do not provide for a framework to guide team coor-\ndination and frequently require extensive restructuring to repair quality attribute \nproblems. Heavyweight management is usually associated with heavy oversight \nand a great emphasis on contractual commitments. In some contexts, this is un-\navoidable, but it has an inherent overhead that slows down development.\n\n\n420 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.1  Planning\nThe planning for a project proceeds over time. There is an initial plan that is nec-\nessarily top-down to convince upper management to build this system and give \nthem some idea of the cost and schedule. This top-down schedule is inherently \ngoing to be incorrect, possibly by large amounts. It does, however, enable the \nproject manager to educate upper managers as to different elements necessary in \nsoftware development. According to Dan Paulish, based on his experience at Sie-\nmens Corporation, some rules of thumb that can be used to estimate the top-down \nschedule for medium-sized (~150 KSLOC) projects are these:\n■\n■Number of components to be estimated: ~150\n■\n■Paper design time per component: ~4 hours\n■\n■Time between engineering releases: ~8 weeks \n■\n■Overall project development allocation:\n■\n■40 percent design: 5 percent architectural, 35 percent detailed \n■\n■20 percent coding\n■\n■40 percent testing\nOnce the system has been given a go-ahead and a budget, the architecture \nteam is formed and produces an initial architecture design. The budget item de-\nserves some further mention. One case is that the budget is for the whole project \nand includes the schedule as well. We will call this case top-down planning. The \nsecond case is that the budget is just for the architecture design phase. In this case, \nthe overall project budget emerges from the architecture design phase. This pro-\nvides a gate that the team has to pass through and gives the holders of the purse \nstrings a chance to consider whether the value of the project is worth the cost. \nWe now describe a merged process that includes both the top-down budget \nand schedule as well as a bottom-up budget and schedule that emerges from the \narchitecture design phase.\nThe architecture team produces the initial architecture design and the re-\nlease plans for the system: what features will be released and when the releases \nwill occur. Once an initial architecture design has been produced, then leads \nfor the various pieces of the project can be assigned and they can build their \nteams. The definition of the various pieces of the project and their assignment \nto teams is sometimes called the work breakdown structure. At this point, cost \nand schedule estimates from the team leads and an overall project schedule can \nbe produced. This bottom-up schedule is usually much more accurate than the \ntop-down schedule, but there may also be significant differences due to differing \nassumptions. These differences between the top-down and bottom-up schedules \nneed to be discussed and sometimes negotiated. Finally, a software development \nplan can be written that shows the initial (internal) release as the architectural \nskeleton with feature-oriented incremental development after that. The features \nto be in each release are developed in consultation with marketing.\n\n\n22.1  Planning\n421\nSoftware \nDevelopment Plan\nTop-Down\nSchedule\nBottom-Up\nSchedule\nFirst-Level\nDecomposition\nReconciliation\nFigure 22.1  Overview of planning process \nFigure 22.1 shows a process that includes both a top-down schedule and a \nbottom-up schedule.\nOnce the software development plan has been written, the teams can deter-\nmine times and groups for integration and can define the coordination needs for \nthe various projects. As we will see in the subsection on global development in \nSection 22.2, the coordination needs for distributed teams can be significant.\n\n\n422 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.2  Organizing\nSome of the elements of organizing a project are team organization, division of \nresponsibilities between project manager and software architect, and planning for \nglobal or distributed development.\nSoftware Development Team Organization\nOnce the architecture design is in place, it can be used to define the project or-\nganization. Each member of the team that designed the software architecture be-\ncomes the lead for a team whose responsibility is to implement a portion of the \narchitecture. Thus, responsibility for fleshing out and implementing the design is \ndistributed to those who had a role in its definition.\nIn addition, many support functions such as writing user documentation, \nsystem testing, training, integration, quality assurance, and configuration man-\nagement are done in a “matrix” form. That is, individuals who are “matrixed” \nreport to one person—a functional manager—for their tasking and professional \nadvancement and to another individual (or to several individuals)—project man-\nagers—for their project responsibilities. Matrix organizations have the advantage \nof being able to allocate and balance resources as needed rather than assign them \npermanently to a project that may have sporadic needs for individuals with par-\nticular skills. They have the disadvantage that the people in them tend to work on \nseveral projects simultaneously. This can cause problems such as divided loyal-\nties or competition among projects for resources.\nTypical roles within a software development team are the following:\n■\n■Team leader—manages tasks within the team.\n■\n■Developer—designs and implements subsystem code.\n■\n■Configuration manager—performs regular builds and integration tests. This \nrole can frequently be shared among multiple software development teams.\n■\n■System test manager—system test and acceptance testing.\n■\n■Product manager—represents marketing; defines feature sets and how \nsystem being developed integrates with other systems in a product suite.\nDivision of Responsibilities between Project Manager and \nSoftware Architect\nOne of the important relations within a team is between the software architect \nand the project manager. You can view the project manager as responsible for the \nexternal-facing aspects of the project and the software architect as responsible \nfor the internal aspects of the project. This division will only work if the external \nview accurately reflects the internal situation and the internal activities accurately \n\n\n22.2  Organizing\n423\nreflect the expectations of the external stakeholders. That is, the project manager \nshould know, and reflect to management, the progress and the risks within the \nproject, and the software architect should know, and reflect to developers, stake-\nholder concerns. The relationship between the project manager and the software \narchitect will have a large impact on the success of a project. They need to have a \ngood working relation and be mindful of the roles they are filling and the bound-\naries of those roles.\nWe use the knowledge areas for project management taken from the Proj-\nect Management Body of Knowledge (PMBOK) to show the duties of these two \nroles in a variety of categories. Table 22.1 (on the next page) gives the knowledge \narea in the language of the PMBOK, defines the knowledge area in English, what \nthat means in software terms, and how the project manager and the software ar-\nchitect collaborate to satisfy that category.\nObserve in this table that the project manager is responsible for the business \nside of the project—providing resources, creating and managing the budget and \nschedule, negotiating with marketing, ensuring quality—and the software archi-\ntect is responsible for the technical side of the project—achieving quality, deter-\nmining measures to be used, reviewing requirements for feasibility, generating \ndevelop-time requirements, and leading the development team. \nGlobal Development\nMost substantial projects today are developed by distributed teams. In many or-\nganizations these teams are globally distributed. Some reasons for this trend are \nthe following:\n■\n■Cost. Labor costs vary depending on location, and there is a perception that \nmoving some development to a low-cost venue will decrease the overall \ncost of the project. Experience has shown that, at least for software devel-\nopment, savings may only be reaped in the long term. Until the developers \nin the low-cost venue have a sufficient level of domain expertise and until \nthe management practices are adapted to compensate for the difficulties of \ndistributed development, a large amount of rework must be done, thereby \ncutting into and perhaps overwhelming any savings from wages. \n■\n■Skill sets and labor availability. Organizations may not be able to hire \ndevelopers at a single location: relocation costs are high, the size of the de-\nveloper pool may be small, or the skill sets needed are specialized and un-\navailable in a single location. Developing a system in a distributed fashion \nallows for the work to move to where the workers are rather than forcing \nthe workers to move to the work location. \n■\n■Local knowledge of markets. Developers who are developing variants of a \nsystem to be sold in their market have more knowledge about the types of \nfeatures that are assumed and the types of cultural issues that may arise.\n\n\n424 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nTable 22.1  Division of Responsibilities between Project Manager and Architect\nPMBOK Knowledge \nArea\nDescription\nTask\nProject Manager\nSoftware Architect\nProject Integration \nManagement\nEnsuring that the \nvarious elements of \nthe project are properly \ncoordinated\nDeveloping, oversee-\ning, and updating the \nproject plan. Manag-\ning change control \nprocess.\nOrganize project, man-\nage resources, bud-\ngets and schedules. \nDefine metrics and \nmetric collection strat-\negy. Oversee change \ncontrol process.\nCreate, design, and organize \nteam around design. Manage de-\npendencies. Implement the cap-\nture of the metrics. Orchestrate \nrequests for changes. Ensure \nthat appropriate IT infrastructure \nexists.\nProject Scope \nManagement\nEnsuring that the proj-\nect includes all of the \nwork required and only \nthe work required\nRequirements \nNegotiate project \nscope with marketing \nand software architect.\nElicit, negotiate, and review run-\ntime requirements and generate \ndevelopment requirements. \nEstimate cost, schedule, and risk \nof meeting requirements.\nProject Time  \nManagement\nEnsuring that the \nproject completes in a \ntimely fashion\nWork breakdown \nstructure and comple-\ntion tracking. Project \nnetwork diagram with \ndates.\nOversee progress \nagainst schedule. Help \ndefine work breakdown \nstructure. Schedule \ncoarse activities to \nmeet deadlines.\nHelp define work breakdown \nstructure. Define tracking mea-\nsures. Recommend assignment \nof resources to software develop-\nment teams.\nProject Cost  \nManagement\nEnsuring that the proj-\nect is completed within \nthe required budget\nResource planning, \ncost estimation, cost \nbudgeting\nCalculate cost to \ncompletion at various \nstages. Make deci-\nsions regarding build/\nbuy and allocation of \nresources.\nGather costs from individual \nteams. Make recommendations \nregarding build/buy and resource \nallocations.\nProject Quality \nManagement\nEnsuring that the \nproject will satisfy the \nneeds for which it was \nundertaken\nQuality and metrics\nDefine productivity, \nsize, and project-level \nquality measures.\nDesign for quality and track \nsystem against design. Define \ncode-level quality metrics. \n\n\n22.2  Organizing\n425\nPMBOK Knowledge \nArea\nDescription\nTask\nProject Manager\nSoftware Architect\nProject Human \nResource  \nManagement\nEnsuring that the \nproject makes the most \neffective use of the \npeople involved with the \nproject\nManaging people and \ntheir careers\nMap skill sets of peo-\nple against required \nskill sets. Ensure that \nappropriate training is \nprovided. Monitor and \nmentor career paths of \nindividuals. Authorize \nrecruitment.\nDefine required technical skill \nsets. Mentor developers about ca-\nreer paths. Recommend training. \nInterview candidates.\nProject  \nCommunications \nManagement\nEnsuring timely and \nappropriate generation, \ncollection, dissemi-\nnation, storage, and \ndisposition of project \ninformation\nCommunicating\nManage communi-\ncation between team \nand external entities. \nReport to upper man-\nagement.\nEnsure communication and \ncoordination among developers. \nSolicit feedback as to progress, \nproblems, and risks.\nProject Risk  \nManagement\nIdentifying, analyzing, \nand responding to \nproject risk\nRisk management\nPrioritize risks. Report \nrisks to management. \nTake steps to mitigate \nrisks.\nIdentify and quantify risks. Adjust \narchitecture and processes to \nmitigate risk.\nProject  \nProcurement  \nManagement\nAcquiring goods and \nservices from outside \norganization\nTechnology\nProcure necessary \nresources. Introduce \nnew technology. \nDetermine technology require-\nments. Recommend technology, \ntraining, and tools.\n\n\n426 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nA consequence of performing global or distributed development is that ar-\nchitecture and allocation of responsibilities to teams is more important than in \nco-located development, where all of the developers are in a single office, or at \nleast in close proximity. For example, assume Module A uses an interface from \nModule B. In time, as circumstances change, this interface may need to be mod-\nified. This means the team responsible for Module B must coordinate with the \nteam responsible for Module A, as indicated in Figure 22.2.\nMethods for coordination include the following:\n■\n■Informal contacts. Informal contacts, such as meeting at the coffee room or \nin the hallway, are only possible if the teams are co-located. \n■\n■Documentation. Documentation, if it is well written, well organized, and \nproperly disseminated, can be used as a means to coordinate the teams, \nwhether co-located or at a distance. \n■\n■Meetings. Teams can hold meetings, either scheduled or ad hoc and either \nface to face or remote, to help bring the team together and raise awareness \nof issues.\n■\n■Electronic communication. Various forms of electronic communication can \nbe used as a coordination mechanism, such as email, news groups, blogs, \nand wikis.\nThe choice of method depends on many factors, including infrastructure \navailable, corporate culture, language skills, time zones involved, and the number \nof teams dependent on a particular module. Until an organization has established \na working method for coordinating among distributed teams, misunderstandings \namong the teams are going to cause delays and, in some cases, serious defects in \na project.\nCoordina\u0013on\t\r  \nModule\t\r  A\t\r  \n\t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \nTeam\t\r  A\t\r  \nDependency\t\r  \nModule\t\r  B\t\r  \n\t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \nTeam\t\r  B\t\r  \nFigure 22.2  If there is a dependency between Module A and Module B, then \nthe teams must coordinate to develop and modify the interface.\n",
      "page_number": 436
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 447-454)",
      "start_page": 447,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "22.3  Implementing\n427\n22.3  Implementing\nDuring the implementation phase of a project, the project manager and architect \nhave a series of decisions to make. In this section we discuss those involving \ntradeoffs, incremental development, and managing risk.\nTradeoffs\nFrom the project manager’s perspective, tradeoffs are between quality, schedule, \nfunctionality, and cost. These are the aspects of the project that are important to \nthe external stakeholders, and the external stakeholders are the project manager’s \nconstituency. Which of these aspects is most important depends on the project \ncontext, and one of the project manager’s major responsibilities is to make this \ndetermination.\nOver time, there is always new functionality that someone wants to have \nadded to the project. Frequently these requests come from the marketing depart-\nment. It is important that the consequences of these new requirements, in terms \nof cost and schedule, be communicated to all concerned stakeholders. This is an \narea where the project manager and the architect must cooperate. What appear \nto be small requirements changes from an outsider’s perspective can, at times, \nrequire major modifications to the architecture and consequently delay a project \nsignificantly.\nThe project manager’s first response to creeping functionality is to resist \nit. Acting as a gatekeeper for the project and shielding it from distractions is a \nportion of the job description. One technique that is frequently used to manage \nchange is a change control board. Bureaucracy can, at times, be your friend. \nChange control boards are committees set up for the purpose of managing change \nwithin a project. The original architecture team members are good candidates to \nsit on the change control board. Before changing an interface, for example, the \nimpact on those modules that depend on the interface needs to be considered. \nAny change to the architecture will incur costs, and it is the architect’s re-\nsponsibility to be the gatekeeper for such changes. A change in the architecture \nimplies changes in code, changes in the architecture documentation, and perhaps \nchanges in build-time tools that enforce architectural conformance. \nDocumentation is especially important in distributed development. Co-lo-\ncated teams have a variety of informal coordination possibilities, such as going \nto the next office or meeting in the coffee room or the hall. Remote teams do not \nhave these informal mechanisms and so must rely on more formal mechanisms \nsuch as documentation; team members must have the initiative to talk to each \nother when doubts arise. One company mounted a webcam on each developer’s \ndesktop to facilitate personal communication.\n\n\n428 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nIncremental Development\nRecall that the software development plan lays out the overall schedule. Every six \nto eight weeks a new release should be available and the specifics of the next re-\nlease are decided. Forty percent of a typical project’s effort is devoted to testing. \nThis means that testing should begin as soon as possible. Testing for a release can \nbegin once the forward development has begun on the next release. The schedule \nalso has to accommodate repairing the faults uncovered by the testing. This leads \nto a release being in one of three states:\n1.\t\nPlanning. This occurs toward the end of the prior development release. \nEnough of the prior release must be completed to understand what will be \nunfinished in that release and must be carried forward to the next one. At \nthis stage, the software development plan is updated.\n2.\t\nDevelopment. The planned release is coded. We will discuss below how \nthe project manager and architect track progress on the release. Daily \nbuilds and automated testing can give some insight into problems during \ndevelopment.\n3.\t\nTest and repair. The release is tested through exercise of the test plan. In \nChapter 19 we described how the architecture can inform the test plan \nand even obviate the need for certain types of testing. The problems found \nduring test are repaired or are carried forward to the next release.\nTracking Progress\nThe project manager can track progress through personal contact with developers \n(this tends to not scale up well), formal status meetings, metrics, and risk man-\nagement. Metrics will be discussed in the next section. Personal contact involves \nchecking with key personnel individually to determine progress. These are one-\non-one meetings, either scheduled or unscheduled.\nMeetings, in general, are either status or working meetings. The two types \nof meetings should not be mixed. In a status meeting, various teams report on \nprogress. This allows for communication among the teams. Issues raised at status \nmeetings should be resolved outside of these meetings—either by individuals or \nby separately scheduled working meetings. When an issue is raised at a status \nmeeting, a person should be assigned to be responsible for the resolution of that \nissue.\nMeetings are expensive. Holding effective meetings is an important skill for \na manager, whether the project manager or the architect. Meetings should have \nwritten agendas that are circulated before the meetings begin, attendees should be \nexpected to have done some prework for the meeting (such as read-ahead), and \nonly essential individuals should attend.\nOne of the outputs of status meetings is a set of risks. A risk is a potential \nproblem together with the consequences if it occurs. The likelihood of the risk \n\n\n22.4  Measuring\n429\noccurring should also be recorded. Risks are also raised at reviews. We have dis-\ncussed architecture evaluation, and it is important from a project management \nperspective that reviews are included in the schedule. These can be code reviews, \narchitecture reviews, or requirements reviews. Risks are also raised by develop-\ners. They are the ones who have the best perspective on potential problems at the \nimplementation level. Architecture evaluation is also important because it is a \nsource of discovering risks.\nThe project manager must prioritize the risks, frequently with the assistance \nof the architect, and, for the most serious risks, develop a mitigation strategy. \nMitigating risks is also a cost, and so implementing the strategy to reduce a risk \ndepends on its priority, likelihood of occurrence, and cost if it does occur.\n22.4  Measuring\nMetrics are an important tool for project managers. They enable the manager to \nhave an objective basis both for their own decision making and for reporting to \nupper management on the progress of the project.\nMetrics can be global—pertaining to the whole project—or they may de-\npend on a particular phase of the project. Another important class of metrics is \n“cost to complete.” We discuss each of these below.\nGlobal Metrics\nGlobal metrics aid the project manager in obtaining an overall sense of the proj-\nect and tracking its progress over time. All global metrics should be updated from \ntime to time as the project proceeds. \nFirst, the project manager needs a measure of project size. The three most \ncommon measures of project size are lines of code, function points, and size of \nthe test suite. None of these is completely satisfactory as a predictor of cost or \neffort, but these are the most commonly used size metrics in practice. \nSchedule deviation is another global metric. Schedule deviation is measured \nby taking the difference between the planned work time and the actual work time. \nOnce time has passed, it can never be recovered. If a project falls behind in its \nschedule, a tradeoff must be made between the aspects we mentioned before: \nschedule, quality, functionality, and cost. As the project proceeds, schedule de-\nviation indicates a failure of estimation or an unforeseen occurrence. By drilling \ndown in this metric and discovering which teams are slipping, the project man-\nager can decide to reallocate resources, if necessary.\nDeveloper productivity is another metric that the project manager can \ntrack. The project manager should look for anomalies in the productivity of the \n\n\n430 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\ndevelopers. An anomaly indicates a potential problem that should be investigated: \nperhaps a developer is inadequately trained for a task, or perhaps the task was im-\nproperly estimated. Earned value management is one technique for measuring the \nproductivity of developers.\nFinally, defects should be tracked. Again, anomalies in the number of defects \ndiscovered over time indicate a potential problem that should be investigated. Not \nonly defects but technical debt should be tracked as well (see Chapter 15).\nAll of these measures have both a historical basis and a project basis. The \nhistorical basis is used to make the initial estimates and then the project basis is \nused for ongoing management activities. \nPhase Metrics and Costs to Complete\nOpen issues should be kept for each phase. For example, until a design is com-\nplete, there are always open issues. These should be tracked and additional re-\nsources allocated if they are not resolved in a timely fashion. Risks represent \nopen issues that should be tracked, as does the project backlog.\nUnmitigated risks from reviews are treated in a similar fashion. We have al-\nready discussed risk management, but one item a project manager can report to upper \nmanagement is the number of high-priority risks and the status of their mitigation.\nCosts to complete is a bottom-up measure that derives from the bottom-up \nschedule. Once the various pieces of the architecture have been assigned to teams, \nthen the teams take ownership of their schedule and the cost to complete their pieces.\n22.5  Governance\nUp to this point, we have maintained a  focus in this chapter. We have focused \non the project manager as the embodiment of the project and have not discussed \nthe external forces that act on the project manager. The topic of governance deals \ndirectly with these other forces. The Open Group defines architecture governance \nas “the practice and orientation by which enterprise architectures and other archi-\ntectures are managed and controlled.” Implicit in this definition is the idea that \nthe project—which is the focus of this book—exists in an organizational context. \nThis context will mediate the interactions of the system being constructed with \nthe other systems in the organization. \nThe Open Group goes on to identify four items as responsibilities of a gov-\nernance board:\n■\n■Implementing a system of controls over the creation and monitoring of all \narchitectural components and activities, to ensure the effective introduction, \nimplementation, and evolution of architectures within the organization\n\n\n22.5  Governance\n431\n■\n■Implementing a system to ensure compliance with internal and external \nstandards and regulatory obligations\n■\n■Establishing processes that support effective management of the above pro-\ncesses within agreed parameters\n■\n■Developing practices that ensure accountability to a clearly identified stake-\nholder community, both inside and outside the organization\nNote the emphasis on processes and practices. Maintaining an effective gov-\nernance process without excessive overhead is a line that is difficult to maintain \nfor an organization.\nThe problem comes about because each system that exists in an enterprise has \nits own stakeholders and its own internal governance processes. Creating a system \nthat utilizes a collection of other systems raises the issue of who is in control.\nConsider the following example. Company A has a collection of products \nthat cover different portions of a manufacturing facility. One collection of sys-\ntems manages the manufacturing process, another manages the processes by \nwhich various portions of the end product are integrated, and a third collection \nmanages the enterprise. Each of these collections of systems has its own set of \ncustomers.\nNow suppose that the board of directors wishes to market the collection as \nan end-to-end solution for a manufacturing facility. It further turns out that the \nsystems that manage the manufacturing process have a 6-month release cycle be-\ncause the technology is changing quickly in this area. The systems that manage \nthe integration process have a 9-month release cycle because they are based on \na widely used commercial product with a 9-month release cycle. The enterprise \nsystems have a 12-month release cycle because they are based on an organiza-\ntion’s fiscal year and reflect tax and regulatory changes that are likely to occur \non fiscal year boundaries. Table 22.2 shows these schedules beginning at date 0.\nWhat should the release schedule be for the combined end-to-end solution? \nRecall that each of these sets of products has reasons for their release schedule \nand each has its own set of customers who will not be receptive to changes in the \nrelease schedule. This is typical of the sort of problem that a governance commit-\ntee deals with.\nTable 22.2  Release Schedules of Different Types of Products\nManufacturing Process \nControl\nIntegration Process \nControl\nEnterprise Management\nVersion 1—date 0\nVersion 1—date 0\nVersion 1—date 0\nVersion 2—date 6 months\nVersion 2—date 9 months\nVersion 2—date 12 months\nVersion 3—date 12 months\nVersion 3—date 18 months\nVersion 3—date 24 months\n\n\n432 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.6  Summary\nA project must be planned, organized, implemented, tracked, and governed. \nThe plan for a project is initially developed as a top-down schedule with \nan acknowledgement that it is only an estimate. Once the decomposition of the \nsystem has been done, a bottom-up schedule can be developed. The two must be \nreconciled, and this becomes the basis for the software development plan.\nTeams are created based on the software development plan. The software \narchitect and the project manager must coordinate to oversee the implementation. \nGlobal development creates a need for an explicit coordination strategy that is \nbased on more formal methods than needed for co-located development.\nThe implementation itself causes tradeoffs between schedule, function, and \ncost. Releases are done in an incremental fashion and progress is tracked by both \nformal metrics and informal communication.\nLarger systems require formal governance mechanisms. The issue of who \nhas control over a particular portion of the system may prevent some business \ngoals from being realized.\n22.7  For Further Reading\nDan Paulish has written an excellent book on managing in an architecture-centric \nenvironment— [Paulish 02]—and much of the material in this chapter is adapted \nfrom his book.\nThe Agile community has independently arrived at a medium-weight man-\nagement process that advocates up-front architecture. See [Coplein 10] for an \nexample of the Agile community’s description of architecture.\nThe responsibilities of a governance board can be found in The Open Group \nArchitecture Framework (TOGAF), produced by the Open Group. www.open-\ngroup.org/architecture/togaf8-doc/arch/chap26.html\nBasic concepts of project management are covered in [IEEE 11]. \nUsing concepts of lean manufacturing, Kanban is a method for scheduling \nthe production of a system [Ladas 09].\nEarned value management is discussed in en.wikipedia.org/wiki/Earned_ \nvalue_management\n\n\n22.8  Discussion Questions\n433\n22.8  Discussion Questions\n1.\t\nWhat are the reasons that top-down and bottom-up schedule estimates dif-\nfer and how would you resolve this conflict in practice?\n2.\t\nGeneric project management practices advocate creating a work breakdown \nstructure as the first artifact produced by a project. What is wrong with this \npractice from an architectural perspective?\n3.\t\nIf you were managing a globally distributed team, what architectural docu-\nmentation artifacts would you want to create first?\n4.\t\nIf you were managing a globally distributed team, what aspects of project \nmanagement would have to change to account for cultural differences?\n5.\t\nEnumerate three different global metrics and discuss the advantages and \ndisadvantages of each?\n6.\t\nHow could you use architectural evaluation as a portion of a governance \nplan?\n7.\t\nIn Chapter 1 we described a work assignment structure for software archi-\ntecture, which can be documented as a work assignment view. (Because it \nmaps software elements—modules—to a nonsoftware environmental struc-\nture—the organizational units that will be responsible for implementing the \nmodules—it is a kind of allocation view.) Discuss how documenting a work \nassignment view for your architecture provides a vehicle for software archi-\ntects and managers to work together to staff a project. Where is the dividing \nline between the part of the work assignment view that the architect should \nprovide and the part that the manager should provide?\n\n\nThis page intentionally left blank \n",
      "page_number": 447
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 455-464)",
      "start_page": 455,
      "end_page": 464,
      "detection_method": "topic_boundary",
      "content": "435\nPart  FO UR\nArchitecture and \nBusiness\nPerhaps the most important job of an architect is to be a fulcrum where business \nand technical decisions meet and interact. The architect is responsible for many \naspects of the business, and must be continually translating business needs and \ngoals into technical realizations, and translating technical opportunities and lim-\nitations into business consequences. \nIn this section of the book, we explore some of the most important business \nconsequences of architecture. This includes treating software architecture deci-\nsions as business investments, dealing with the organizational aspects of architec-\nture (such as organizational learning and knowledge management), and looking \nat architecture as the key enabler of software product lines.\nIn Chapter 23 we discuss the economic implications of architectural de-\ncisions and provide a method—called the CBAM, or Cost Benefit Analysis \nMethod—for making rational, business-driven architectural choices. The CBAM \nbuilds upon other architecture methods and principles that you have already seen \nin this book—scenarios, quality attributes, active stakeholder involvement—but \nadds a new twist to the evaluation of architectural improvements: an explicit con-\nsideration of the utility that an architectural improvement brings. \nIn Chapter 24 we consider the fact that architectures are created by actual \nhuman beings, called architects working in actual organizations. This chapter \nconsiders the questions of how to foster individual competence—that is, how \n\n\n436\nto create competent architects—and how to create architecturally competent \norganizations. \nFinally, in Chapter 25 we look at the concept of software product lines. Not \nsurprisingly, we find that software architectures are the most important compo-\nnent of software-intensive product lines.\n\n\n437\n23\nEconomic Analysis of \nArchitectures\nArthur Dent: “I think we have different value systems.” \nFord Prefect: “Well mine’s better.” \n—Douglas Adams, Mostly Harmless\nThus far, we have been primarily investigating the relationships between archi-\ntectural decisions and the quality attributes that the architecture’s stakeholders \nhave deemed important: If I make this architectural decision, what effect will it \nhave on the quality attributes? If I have to achieve that quality attribute require-\nment, what architectural decisions will do the trick?\nAs important as this effort is, this perspective is missing a crucial consider-\nation: What are the economic implications of an architectural decision?\nUsually an economic discussion is focused on costs, primarily the costs of \nbuilding the system in the first place. Other costs, often but not always down-\nplayed, include the long-term costs incurred through cycles of maintenance and \nupgrade. However, as we argue in this chapter, as important as costs are the bene-\nfits that an architectural decision may bring to an organization.\nGiven that the resources for building and maintaining a system are finite, \nthere must be a rational process that helps us choose among architectural options, \nduring both an initial design phase and subsequent upgrade periods. These op-\ntions will have different costs, will consume differing amounts of resources, will \nimplement different features (each of which brings some benefit to the organiza-\ntion), and will have some inherent risk or uncertainty. To capture these aspects, \nwe need economic models of software that take into account costs, benefits, risks, \nand schedule implications.\n\n\n438 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\n23.1  Decision-Making Context\nAs we saw in Chapter 16, business goals play a key role in requirements for ar-\nchitectures. Because major architectural decisions have technical and economic \nimplications, the business goals behind a software system should be used to \ndirectly guide those decisions. The most immediate economic implication of a \nbusiness goal decision on an architecture is how it affects the cost of implement-\ning the system. The quality attributes achieved by the architecture decisions have \nadditional economic implications because of the benefits (which we call utility) \nthat can be derived from those decisions; for example, making the system faster \nor more secure or easier to maintain and update. It is this interplay between the \ncosts and the benefits of architectural decisions that guides (and torments) the \narchitect. Figure 23.1 show this interplay.\nFor example, using redundant hardware to achieve a desired level of avail-\nability has a cost; checkpointing to a disk file has a different cost. Furthermore, \nboth of these architectural decisions will result in (presumably different) measur-\nable levels of availability that will have some value to the organization develop-\ning the system. Perhaps the organization believes that its stakeholders will pay \nmore for a highly available system (a telephone switch or medical monitoring \nsoftware, for example) or that it will be sued if the system fails (for example, the \nsoftware that controls antilock brakes in an automobile).\nKnowing the costs and benefits associated with particular decisions enables \nreasoned selection from among competing alternatives. The economic analysis \ndoes not make decisions for the stakeholders, just as a financial advisor does not \ntell you how to invest your money. It simply aids in the elicitation and documen-\ntation of value for cost (VFC): a function of the costs, benefits, and uncertainty of \na “portfolio” of architectural investments. It gives the stakeholders a framework \nwithin which they can apply a rational decision-making process that suits their \nneeds and their risk aversion.\nBusiness\nArchitecture\nStrategies\nCost\nBenefit\n•\n•\n•\nPerformance\nSecurity\nModifiability\nUsability\nGoals\nFigure 23.1  Business goals, architectural decisions, costs, and benefits\n\n\n23.2  The Basis for the Economic Analyses\n439\nEconomic analysis isn’t something to apply to every architectural decision, \nbut rather to the most basic ones that put an overarching architectural strategy in \nplace. It can help you assess the viability of that strategy. It can also be the key to \nobjective selection among competing strategies, each of which might have advo-\ncates pushing their own self-interests.\n23.2  The Basis for the Economic Analyses\nWe now describe the key ideas that form the basis for the economic analyses. The \npractical realization of these ideas can be packaged in a variety of ways, as we \ndescribe in Section 23.3. Our goal here is to develop the theory underpinning a \nmeasure of VFC for various architectural strategies in light of scenarios chosen \nby the stakeholders.\nWe begin by considering a collection of scenarios generated as a portion of \nrequirements elicitation, an architectural evaluation, or specifically for the eco-\nnomic analysis. We examine how these scenarios differ in the values of their pro-\njected responses and we then assign utility to those values. The utility is based on \nthe importance of each scenario being considered with respect to its anticipated \nresponse value.\nArmed with our scenarios, we next consider the architectural strategies that \nlead to the various projected responses. Each strategy has a cost, and each im-\npacts multiple quality attributes. That is, an architectural strategy could be imple-\nmented to achieve some projected response, but while achieving that response, \nit also affects some other quality attributes. The utility of these “side effects” \nmust be taken into account when considering a strategy’s overall utility. It is this \noverall utility that we combine with the project cost of an architectural strategy to \ncalculate a final VFC measure.\nUtility-Response Curves\nOur economic analysis uses quality attribute scenarios (from Chapter 4) as the \nway to concretely express and represent specific quality attributes. We vary the \nvalues of the responses, and ask what the utility is of each response. This leads to \nthe concept of a utility-response curve.\nEach scenario’s stimulus-response pair provides some utility (value) to the \nstakeholders, and the utility of different possible values for the response can be \ncompared. This concept of utility has roots that go back to the eighteenth cen-\ntury, and it is a technique to make comparable very different concepts. To help \nus make major architectural decisions, we might wish to compare the value of \nhigh performance against the value of high modifiability against the value of high \nusability, and so forth. The concept of utility lets us do that.\n\n\n440 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nAlthough sometimes it takes a little prodding to get them to do it, stakehold-\ners can express their needs using concrete response measures, such as “99.999 \npercent available.” But that leaves open the question of how much they would \nvalue slightly less demanding quality attributes, such as “99.99 percent avail-\nable.” Would that be almost as good? If so, then the lower cost of achieving that \nlower value might make that the preferred option, especially if achieving the \nhigher value was going to play havoc with another quality attribute like perfor-\nmance. Capturing the utility of alternative responses of a scenario better enables \nthe architect to make tradeoffs involving that quality attribute.\nWe can portray each relationship between a set of utility measures and a cor-\nresponding set of response measures as a graph—a utility-response curve. Some \nexamples of utility-response curves are shown in Figure 23.2. In each, points la-\nbeled a, b, or c represent different response values. The utility-response curve \nthus shows utility as a function of the response value. \nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nc\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 23.2  Some sample utility-response curves\n\n\n23.2  The Basis for the Economic Analyses\n441\nThe utility-response curve depicts how the utility derived from a particular \nresponse varies as the response varies. As seen in Figure 23.2, the utility could \nvary nonlinearly, linearly, or even as a step function. For example, graph (c) por-\ntrays a steep rise in utility over a narrow change in a quality attribute response \nlevel. In graph (a), a modest change in the response level results in only a very \nsmall change in utility to the user. \nIn Section 23.3 we illustrate some ways to engage stakeholders to get them \nto construct utility curves.\nWeighting the Scenarios \nDifferent scenarios will have different importance to the stakeholders; in order \nto make a choice of architectural strategies that is best suited to the stakeholders’ \ndesires, we must weight the scenarios. It does no good to spend a great deal of ef-\nfort optimizing a particular scenario in which the stakeholders actually have very \nlittle interest. Section 23.3 presents a technique for applying weights to scenarios.\nSide Effects \nEvery architectural strategy affects not only the quality attributes it was selected \nto achieve, but also other quality attributes as well. As you know by now, these \nside effects on other quality attributes are often negative. If those effects are too \nnegative, we must make sure there is a scenario for the side effect attribute and \ndetermine its utility-response curve so that we can add its utility to the deci-\nsion-making mix. We calculate the benefit of applying an architectural strategy \nby summing its benefits to all relevant quality attributes; for some quality attri-\nbutes the benefit of a strategy might be negative.\nDetermining Benefit and Normalization\nThe overall benefit of an architectural strategy across quality attribute scenarios \nis the sum of the utility associated with each one, weighted by the importance of \nthe scenario. For each architectural strategy i, its benefit Bi over j scenarios (each \nwith weight Wj) is\nBi = ∑j (bi,j × Wj)\nReferring to Figure 23.2, each bi,j is calculated as the change in utility (over \nwhatever architectural strategy is currently in place, or is in competition with the \none being considered) brought about by the architectural strategy with respect to \nthis scenario: \nbi,j = Uexpected – Ucurrent\n\n\n442 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nThat is, the utility of the expected value of the architectural strategy minus \nthe utility of the current system relative to this scenario. \nCalculating Value for Cost\nThe VFC for each architectural strategy is the ratio of the total benefit, Bi, to the \ncost, Ci, of implementing it:\nVFC = Bi / Ci\nThe cost Ci is estimated using a model appropriate for the system and the \nenvironment being developed, such as a cost model that estimates implementa-\ntion cost by measuring an architecture’s interaction complexity. You can use this \nVFC score to rank-order the architectural strategies under consideration.\nConsider curves (a) and (b) in Figure 23.2. Curve (a) flattens out as the qual-\nity attribute response improves. In this case, it is likely that a point is reached past \nwhich VFC decreases as the quality attribute response improves; spending more \nmoney will not yield a significant increase in utility. On the other hand, curve (b) \nshows that a small improvement in quality attribute response can yield a very sig-\nnificant increase in utility. In that situation, an architectural strategy whose VFC \nis low might rank significantly higher with a modest improvement in its quality \nattribute response.\n23.3  Putting Theory into Practice: The CBAM\nWith the concepts in place we can now describe techniques for putting \nthem into practice, in the form of a method we call the Cost Benefit Analysis \nMethod (CBAM). As we describe the method, remember that, like all of our \nstakeholder-based methods, it could take any of the forms for stakeholder interac-\ntion that we discussed in the introduction to Part III.\nPracticalities of Utility Curve Determination\nTo build the utility-response curve, we first determine the quality attribute levels \nfor the best-case and worst-case situations. The best-case quality attribute level is \nthat above which the stakeholders foresee no further utility. For example, a sys-\ntem response to the user of 0.1 second is perceived as instantaneous, so improv-\ning it further so that it responds in 0.03 second has no additional utility. Simi-\nlarly, the worst-case quality attribute level is a minimum threshold above which a \n\n\n23.3  Putting Theory into Practice: The CBAM\n443\nsystem must perform; otherwise it is of no use to the stakeholders. These levels—\nbest-case and worst-case—are assigned utility values of 100 and 0, respectively.\nWe then determine the current and desired utility levels for the scenario. The re-\nspective utility values (between 0 and 100) for various alternative strategies are \nelicited from the stakeholders, using the best-case and worst-case values as refer-\nence points. For example, our current design provides utility about half as good \nas we would like, but an alternative strategy being considered would give us 90 \npercent of the maximum utility. Hence, the current utility level is set to 50 and the \ndesired utility level is set to 90.\nIn this manner the utility curves are generated for all of the scenarios.\nShow Business or Accounting?\nAs software architects, what kind of business are we in? One of Irving \nBerlin’s most famous songs is entitled “There’s No Business Like Show \nBusiness.” David Letterman, riffing off this song title, once quipped, \n“There’s no business like show business, but there are several businesses \nlike accounting.”\nHow should we think of ourselves, as architects? Consider two more \nquotations from famous business leaders:\nI never get the accountants in before I start up a business. It’s done on gut \nfeeling.   —Richard Branson\nIt has been my experience that competency in mathematics, both in numeri-\ncal manipulations and in understanding its conceptual foundations, enhances \na person’s ability to handle the more ambiguous and qualitative relationships \nthat dominate our day-to-day financial decision-making.   —Alan Greenspan\nArchitectures are at the fulcrum of a set of business, social, and techni-\ncal decisions. A poor decision in any dimension can be disastrous for an \norganization. A decision in any one dimension is influenced by the other \ndimensions. So in our roles as architects, which are we, Alan Greenspan or \nRichard Branson?\nMy claim is that, as an industry, we in software are more like Richard \nBranson. We make decisions that have enormous economic consequences \non a gut feeling, without ever examining their financial consequences in \na disciplined way. This might be OK if you are an intuitive genius, but it \ndoesn’t work well for most of us. Engineering is about making good deci-\nsions in a rational, predictable way. For this we need methods.\n—RK\n\n\n444 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nPracticalities of Weighting Determination\nOne method of weighting the scenarios is to prioritize them and use their prior-\nity ranking as the weight. So for N scenarios, the highest priority one is given a \nweight of 1, the next highest is given a weight of (N–1)/N, and so on. This turns \nthe problem of weighting the scenarios into one of assigning priorities.\nThe stakeholders can determine the priorities through a variety of voting \nschemes. One simple method is to have each stakeholder prioritize the scenarios \n(from 1 to N) and the total priority of the scenario is the sum of the priorities it \nreceives from all of the stakeholders. This voting can be public or secret.\nOther schemes are possible. Regardless of the scheme used, it must make \nsense to the stakeholders and it must suit their culture. For example, in some cor-\nporate environments, everything is done by consensus. In others there is a strict \nhierarchy, and in still others decisions are made in a democratic fashion. In the \nend it is up to the stakeholders to make sure that the scenario weights agree with \ntheir intuition.\nPracticalities of Cost Determination\nOne of the shortcomings of the field of software architecture is that there are very \nfew cost models for various architectural strategies. There are many software cost \nmodels, but they are based on overall system characteristics such as size or func-\ntion points. These are inadequate to answer the question of how much does it \ncost to, for example, use a publish-subscribe pattern in a particular portion of the \narchitecture. There are cost models that are based on complexity of modules (by \nfunction point analysis according to the requirements assigned to each module) \nand the complexity of module interaction, but these are not widely used in prac-\ntice. More widely used in practice are corporate cost models based on previous \nexperience with the same or similar architectures, or the experience and intuition \nof senior architects.\nLacking cost models whose accuracy can be assured, architects often turn \nto estimation techniques. To proceed, remember that an absolute number for cost \nisn’t necessary to rank candidate architecture strategies. You can often say some-\nthing like “Suppose strategy A costs $x. It looks like strategy B will cost $2x, and \nstrategy C will cost $0.5x.” That’s enormously helpful. A second approach is to \nuse very coarse estimates. Or if you lack confidence for that degree of certainty, \nyou can say something like “Strategy A will cost a lot, strategy B shouldn’t cost \nvery much, and strategy C is probably somewhere in the middle.”\nCBAM\nNow we describe the method we use for economic analysis: the Cost Benefit \nAnalysis Method. CBAM has for the most part been applied when an organization \n",
      "page_number": 455
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 465-472)",
      "start_page": 465,
      "end_page": 472,
      "detection_method": "topic_boundary",
      "content": "23.3  Putting Theory into Practice: The CBAM\n445\nwas considering a major upgrade to an existing system and they wanted to un-\nderstand the utility and value for cost of making the upgrade, or they wanted to \nchoose between competing architectural strategies for the upgrade. CBAM is also \napplicable for new systems as well, especially for helping to choose among com-\npeting strategies. Its key concepts (quality attribute response curves, cost, and \nutility) do not depend on the setting.\nSteps.  A process flow diagram for the CBAM is given in Figure 23.3. The first \nfour steps are annotated with the relative number of scenarios they consider. That \nnumber steadily decreases, ensuring that the method concentrates the stakehold-\ners’ time on the scenarios believed to be of the greatest potential in terms of VFC.\nThis description of CBAM assumes that a collection of quality attribute sce-\nnarios already exists. This collection might have come from a previous elicitation \nexercise such as an ATAM exercise (see Chapter 21) or quality attribute utility \ntree construction (see Chapter 16).\nThe stakeholders in a CBAM exercise include people who can authorita-\ntively speak to the utility of various quality attribute responses, and probably \ninclude the same people who were the source of the quality attribute scenarios \nbeing used as input. The steps are as follows:\n1.\t\nCollate scenarios. Give the stakeholders the chance to contribute new sce-\nnarios. Ask the stakeholders to prioritize the scenarios based on satisfying \nthe business goals of the system. This can be an informal prioritization \nusing a simple scheme such as “high, medium, low” to rank the scenarios. \nChoose the top one-third for further study.\n2.\t\nRefine scenarios. Refine the scenarios chosen in step 1, focusing on their \nstimulus-response measures. Elicit the worst-case, current, desired, and \nbest-case quality attribute response level for each scenario. For example, \na refined performance scenario might tell us that worst-case performance \nfor our system’s response to user input is 12 seconds, the best case is 0.1 \nseconds, and our desired response is 0.5 seconds. Our current architecture \nprovides a response of 1.5 seconds:\nScenario\nWorst Case\nCurrent\nDesired\nBest Case\nScenario #17: \nResponse to \nuser input\n12 seconds\n1.5 seconds\n0.5 seconds\n0.1 seconds\n…\n3. \nPrioritize scenarios. Prioritize the refined scenarios, based on stakeholder \nvotes. You give 100 votes to each stakeholder and have them distribute the \nvotes among the scenarios, where their voting is based on the desired response \nvalue for each scenario. Total the votes and choose the top 50 percent of \nthe scenarios for further analysis. Assign a weight of 1.0 to the highest-rated \nscenario; assign the other scenarios a weight relative to the highest rated. This \n\n\n446 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nbecomes the weighting used in the calculation of a strategy’s overall benefit. \nMake a list of the quality attributes that concern the stakeholders.\n4. \nAssign utility. Determine the utility for each quality attribute response level \n(worst-case, current, desired, best-case) for the scenarios from step 3. You can \nconveniently capture these utility curves in a table (one row for each scenario, \none column for each of the four quality attribute response levels). Continuing \nour example from step 2, this step would assign utility values from 1 to 100 \nfor each of the latency values elicited for this scenario in step 2:\nScenario\nWorst Case\nCurrent\nDesired\nBest Case\nScenario #17: \nResponse to \nuser input\n12 seconds\n1.5 seconds\n0.5 seconds\n0.1 seconds\nUtility 5\nUtility 50\nUtility 80\nUtility 85\n5.\t\nMap architectural strategies to scenarios and determine their expected \nquality attribute response levels. For each architectural strategy under con-\nsideration, determine the expected quality attribute response levels that will \nresult for each scenario.\n6.\t\nDetermine the utility of the expected quality attribute response levels by \ninterpolation. Using the elicited utility values (that form a utility curve), \ndetermine the utility of the expected quality attribute response level for the \narchitectural strategy. Do this for each relevant quality attribute enumerated \nin step 3. For example, if we are considering a new architectural strategy \nthat would result in a response time of 0.7 seconds, we would assign this \na utility proportionately between 50 (which it exceeds) and 80 (which it \ndoesn’t exceed).\nThe formula for interpolation between two data points (xa, ya) and (xb, yb) \nis given by:\ny = ya + (yb – ya) (x – xa)\n(xb – xa)\nFor us, the x values are the quality attribute response levels and the y \nvalues are the utility values. So, employing this formula, the utility value of \na 0.7-second response time is 74.\n7.\t\nCalculate the total benefit obtained from an architectural strategy. Subtract \nthe utility value of the “current” level from the expected level and normal-\nize it using the votes elicited in step 3. Sum the benefit due to a particular \narchitectural strategy across all scenarios and across all relevant quality \nattributes.\n8. \nChoose architectural strategies based on VFC subject to cost and schedule \nconstraints. Determine the cost and schedule implications of each archi-\ntectural strategy. Calculate the VFC value for each as a ratio of benefit to \ncost. Rank-order the architectural strategies according to the VFC value and \nchoose the top ones until the budget or schedule is exhausted.\n\n\n23.3  Putting Theory into Practice: The CBAM\n447\n9.\t\nConfirm results with intuition. For the chosen architectural strategies, con-\nsider whether these seem to align with the organization’s business goals. If \nnot, consider issues that may have been overlooked while doing this analy-\nsis. If there are significant issues, perform another iteration of these steps.\nStep 5: Map architectural strategies to \nscenarios and determine quality attribute \nresponse levels\nStep 1: Collate scenarios:\nPrioritize to choose top one-third\nN\nscenarios\nStep 2: Refine scenarios: Determine quality\nattribute response levels for best, worst,\ncurrent, and desired cases of the scenario\nN/3\nscenarios\nStep 3: Prioritize scenarios:\nEliminate half of the scenarios\nN/3\nscenarios\nStep 4: Assign utility for the current and the\ndesired levels for each scenario\nN/6\nscenarios\nStep 6: Determine the expected utility value\nof architectural strategy using interpolation\nStep 7: Calculate total benefit obtained\nfrom an architectural strategy\nStep 8: Choose architectural strategies\nbased on ROI subject to cost constraints\nStep 9: Confirm results with intuition\nFigure 23.3  Process flow diagram for the CBAM\n\n\n448 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nComputing Benefit for Architectural Variation Points\nThis chapter is about calculating the cost and benefit of competing archi-\ntectural options. While there are plenty of metrics and methods to mea-\nsure cost, usually as some function of complexity, benefit is more slippery. \nCBAM uses an assemblage of stakeholders to work out jointly what utility \neach architectural option will bring with it. At the end of the day, CBAM’s \nmeasures of utility are subjective, intuitive, and imprecise. That’s all right; \nstakeholders seldom are able to express benefit any better than that, and \nso CBAM takes what stakeholders know and formulates it into a justified \nchoice. That is, CBAM elicits inputs that are imprecise, because nothing \nbetter is available, and does the best that can be done with them.\nAs a counterpoint to CBAM, there is one area of architecture in which \nthe architectural options are of a specific variety: product-line architectures. \nIn Chapter 25, you’ll be introduced to software architectures that serve \nfor an entire family of systems. The architect introduces variation points \ninto the architecture, which are places where it can be quickly tailored \nin preplanned ways so that it can serve each of a variety of different but \nrelated products. In the product-line context, the major architectural option \nis whether to build a particular variation point in the architecture. Doing so \nisn’t free; otherwise, every product-line architecture would have an infin-\nitude of variation points. So the question becomes: When will adding a \nvariation point pay off?\nCBAM would work for this case as well; you could ask the product line’s \nstakeholders what the utility of a new variation point would be, vis-à-vis other \noptions such as including a different variation point instead, or none at all. \nBut in this case, there is a quantitative formula to measure the benefit. John \nMcGregor of Clemson University has long been interested in the econom-\nics of software product lines, and along with others (including me) invented \nSIMPLE, a cost-modeling language for software product lines. SIMPLE is \ngreat at estimating the cost of product-line options, but not so great at estimat-\ning its benefits. Recently, McGregor took a big step toward remedying that.\nHere is his formula for modeling the marginal value of building in an \nadditional variation point to the architecture:\nvi(t,T) = max(0, –E\nci(τ)e–r(τ–t)\nτ=t\nT\n\n+Þi,T E[ \nk\n \nτ=T\nT*\n\nXi,k(τ)e –r(τ–t))])\nmax(0, \nGot that? No? Right, me neither. But we can understand it if we build it \nup from pieces.\nThe equation says (as all value equations do) that value is benefit minus \ncost, and so those are the two terms.\nThe first term measures the expected cost of building variation point i \nover a time period from now until time T (some far-off horizon of interest \nsuch as fielding your fifth system or getting your next round of venture \n\n\n23.3  Putting Theory into Practice: The CBAM\n449\ncapital funding). The function ci(τ) measures the cost of building variation \npoint i at time τ; it’s evaluated using conventional cost-estimating tech-\nniques for software. The e factor is a standard economic term that accounts \nfor the net present value of money; r is the assumed current interest rate. \nThis is summed up over all time between now and T, and made negative to \nreflect that cost is negative value. So here’s the first term decomposed:\n E\nci(τ)e–r(τ–t)\nτ=t\nT\n\nThe second term evaluates the benefit. The function Xi,k(τ) is the key; it \nmeasures the value of the variation point in the kth product of the product \nline. This is equal to the marginal value of the variation point to the kth \nproduct, minus the cost of using (exercising) the variation point in that kth \nproduct. That first part is the hardest part to come by, but your marketing \ndepartment should be able to help by expressing the marginal value in \nterms of the additional products that the variation point would enable you to \nbuild and how much revenue each would bring in. (That’s what marketing \ndepartments are paid to figure out.)\nvalue of variation point i in product k\nat time \nXi,k(τ)e –r(τ–t)\nVMPi,k(τ) – MCi,k(τ)\nτ=\nmarginal value of the i th variation\npoint in the kth product at time τ\nmarginal cost of tailoring variation\npoint i for use in product k\n. . . adjusted by a factor to account \nfor net present value of money\nTo the function X we apply the same factor as before to account for the \nnet present value of money, sum it up over all time periods between now \nand T, and make it nonnegative:\nSYMBOLS FOR TIME \nτ = time variable \nt = time now \nT = target date   \nT* = modeling limit (t=forever) \ni = index over variation points \nr = assumed interest rate \nCost spent to build variation point i at time τ \nExpected cost summed over \nall relevant time intervals \n. . . adjusted by a factor to account \nfor net present value of money\n\n\n450 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nThen we sum that up over all products k in the product line and multiply \nit by the probability that the variation point will in fact be ready by the time \nit’s needed:\nAdd the two terms together and there you have it. It’s easy to put this \nin a spreadsheet that calculates the result given the relevant inputs, and it \nprovides a quantitative measurement to help guide selection of architectural \noptions—in this case, introduction of variation points to support a product line.\n—PCC\n23.4  Case Study: The NASA ECS Project\nWe will now apply the CBAM to a real-world system as an example of the \nmethod in action.\nThe Earth Observing System is a constellation of NASA satellites that gathers \ndata for the U.S. Global Change Research Program and other scientific communities \nworldwide. The Earth Observing System Data Information System (EOSDIS) Core \nSYMBOLS FOR TIME \nτ = time variable \nt = time now \nT = target date   \nT* = modeling limit (t=forever) \ni = index over variation points \nr = assumed interest rate\nk = index over products \nValue cannot \nbe negative\nsummed over all time\nvalue of variation point i in product k\nat time \nXi,k(τ)e\n)\n–r(τ–t)\nmax (0, \nτ=T\nT*\nVMPi,k(τ) – MCi,k(τ)\nτ=\nmarginal value of the i th variation\npoint in the kth product at time τ\nmarginal cost of tailoring variation\npoint i for use in product k\n. . . adjusted by a factor to account \nfor net present value of money\nprobability that variation point i \nwill be ready for use by time T\nvalue of variation point i in product k over \nall time . . . \n      . . . and over all products\n  Þi,T E[ \nk\n \nτ=T\nT*\n\nXi,k(τ)e –r(τ–t))])\nmax(0, \n\n\n23.4  Case Study: The NASA ECS Project\n451\nSystem (ECS) collects data from various satellite downlink stations for further pro-\ncessing. ECS’s mission is to process the data into higher-form information and make \nit available to scientists in searchable form. The goal is to provide both a common \nway to store (and hence process) data and a public mechanism to introduce new data \nformats and processing algorithms, thus making the information widely available.\nThe ECS processes an input stream of hundreds of gigabytes of raw environ-\nment-related data per day. The computation of 250 standard “products” results in \nthousands of gigabytes of information that is archived at eight data centers in the \nUnited States. The system has important performance and availability require-\nments. The long-term nature of the project also makes modifiability important.\nThe ECS project manager had a limited annual budget to maintain and en-\nhance his current system. From a prior analysis—in this case an ATAM exercise—a \nlarge set of desirable changes to the system was elicited from the system stakehold-\ners, resulting in a large set of architectural strategies. The problem was to choose \na (much) smaller subset for implementation, as only 10 to 20 percent of what was \nbeing proposed could actually be funded. The manager used the CBAM to make a \nrational decision based on the economic criterion of return on investment.\nIn the execution of the CBAM described next, we concentrated on analyzing \nthe Data Access Working Group (DAWG) portion of the ECS.\nStep 1: Collate Scenarios\nA subset of the raw scenarios put forward by the DAWG team were as shown in \nTable 23.1. Note that they are not yet well formed and that some of them do not \nhave defined responses. These issues are resolved in step 2, when the number of \nscenarios is reduced.1\nTable 23.1  Collected Scenarios in Priority Order\nScenario Scenario Description\n  1\nReduce data distribution failures that result in hung distribution requests \nrequiring manual intervention.\n  2\nReduce data distribution failures that result in lost distribution requests.\n  3\nReduce the number of orders that fail on the order submission process.\n  4\nReduce order failures that result in hung orders that require manual intervention.\n  5\nReduce order failures that result in lost orders.\n  6\nThere is no good method of tracking ECSGuest failed/canceled orders without \nmuch manual intervention (e.g., spreadsheets).\n  7\nUsers need more information on why their orders for data failed.\n  8\nBecause of limitations, there is a need to artificially limit the size and number of \norders.\n  9\nSmall orders result in too many notifications to users.\n10\nThe system should process a 50-GB user request in one day, and a 1-TB user \nrequest in one week.\n1.  In the presentation of the DAWG case study, we only show the reduced set of scenarios.\n\n\n452 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nStep 2: Refine Scenarios\nThe scenarios were refined, paying particular attention to precisely specifying \ntheir stimulus-response measures. The worst-case, current-case, desired-case, \nand best-case response goals for each scenario were elicited and recorded, as \nshown in Table 23.2.\nTable 23.2  Response Goals for Refined Scenarios\nScenario\nResponse Goals\nWorst\nCurrent\nDesired\nBest\n  1\n10% hung\n5% hung\n1% hung\n0% hung\n  2\n> 5% lost\n< 1% lost\n0% lost\n0% lost\n  3\n10% fail\n5% fail\n1% fail\n0% fail\n  4\n10% hung\n5% hung\n1% hung\n0% hung\n  5\n10% lost\n< 1% lost\n0% lost\n0% lost\n  6\n50% need help\n25% need help\n0% need help\n0% need help\n  7\n10% get \ninformation\n50% get \ninformation\n100% get \ninformation\n100% get \ninformation\n  8\n50% limited\n30% limited\n0% limited\n0% limited\n  9\n1/granule\n1/granule\n1/100 granules\n1/1,000 granules\n10\n< 50% meet goal\n60% meet goal\n80% meet goal\n> 90% meet goal\nStep 3: Prioritize Scenarios\nIn voting on the refined representation of the scenarios, the close-knit team devi-\nated slightly from the method. Rather than vote individually, they chose to dis-\ncuss each scenario and arrived at a determination of its weight via consensus. The \nvotes allocated to the entire set of scenarios were constrained to 100, as shown in \nTable 23.3. Although the stakeholders were not required to make the votes mul-\ntiples of 5, they felt that this was a reasonable resolution and that more precision \nwas neither needed nor justified.\nStep 4: Assign Utility\nIn this step the utility for each scenario was determined by the stakeholders, again \nby consensus. A utility score of 0 represented no utility; a score of 100 represented \nthe most utility possible. The results of this process are given in Table 23.4.\n",
      "page_number": 465
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 473-485)",
      "start_page": 473,
      "end_page": 485,
      "detection_method": "topic_boundary",
      "content": "23.4  Case Study: The NASA ECS Project\n453\nTable 23.3  Refined Scenarios with Votes\nScenario\nResponse Goals\nVotes Worst\nCurrent\nDesired\nBest\n  1\n10\n10% hung\n5% hung\n1% hung\n0% hung\n  2\n15\n> 5% lost\n< 1% lost\n0% lost\n0% lost\n  3\n15\n10% fail\n5% fail\n1% fail\n0% fail\n  4\n10\n10% hung\n5% hung\n1% hung\n0% hung\n  5\n15\n10% lost\n< 1% lost\n0% lost\n0% lost\n  6\n10\n50% need help 25% need help 0% need help\n0% need help\n  7\n5\n10% get \ninformation\n50% get \ninformation\n100% get \ninformation\n100% get \ninformation\n  8\n5\n50% limited\n30% limited\n0% limited\n0% limited\n  9\n10\n1/granule\n1/granule\n1/100 granules\n1/1,000 granules\n10\n5\n< 50% meet \ngoal\n60% meet goal 80% meet goal\n> 90% meet goal\nTable 23.4  Scenarios with Votes and Utility Scores\nScenario\nUtility Scores\nVotes\nWorst\nCurrent\nDesired\nBest\n  1\n10\n10\n80\n  95\n100\n  2\n15\n  0\n70\n100\n100\n  3\n15\n25\n70\n100\n100\n  4\n10\n10\n80\n  95\n100\n  5\n15\n  0\n70\n100\n100\n  6\n10\n  0\n80\n100\n100\n  7\n  5\n10\n70\n100\n100\n  8\n  5\n  0\n20\n100\n100\n  9\n10\n50\n50\n80\n  90\n10\n5\n50\n50\n80\n 90\nStep 5: Develop Architectural Strategies for Scenarios and \nDetermine Their Expected Quality Attribute Response Levels\nBased on the requirements implied by the preceding scenarios, a set of 10 archi-\ntectural strategies was developed by the ECS architects. Recall that an architec-\ntural strategy may affect more than one scenario. To account for these complex \nrelationships, the expected quality attribute response level that each strategy is \npredicted to achieve had to be determined with respect to each relevant scenario.\nThe set of architectural strategies, along with the determination of the scenar-\nios they address, is shown in Table 23.5. For each architectural strategy/scenario \npair, the response levels expected to be achieved with respect to that scenario are \nshown (along with the current response, for comparison purposes).\n\n\nTable 23.5  Architectural Strategies and Scenarios Addressed\nStrategy Name\nDescription\nScenarios \nAffected\nCurrent  \nResponse\nExpected \nResponse\n  1\nOrder persistence on \nsubmission\nStore an order as soon as it arrives in the system.\n3\n5% fail\n2% Fail\n5\n<1% lost\n0% lost\n6\n25% need help\n0% need help\n  2\nOrder chunking\nAllow operators to partition large orders into multiple small orders.\n8\n30% limited\n15% limited\n  3\nOrder bundling\nCombine multiple small orders into one large order.\n9\n1 per granule\n1 per 100\n10\n60% meet goal\n55% meet goal\n  4\nOrder segmentation\nAllow an operator to skip items that cannot be retrieved due to data \nquality or availability issues.\n4\n5% hung\n2% hung\n  5\nOrder reassignment\nAllow an operator to reassign the media type for items in an order.\n1\n5% hung\n2% hung\n  6\nOrder retry\nAllow an operator to retry an order or items in an order that may \nhave failed due to temporary system or data problems.\n4\n5% hung\n3% hung\n  7\nForced order \ncompletion\nAllow an operator to override an item’s unavailability due to data \nquality constraints.\n1\n5% hung\n3% hung\n  8\nFailed order \nnotification\nEnsure that users are notified only when part of their order has truly \nfailed and provide detailed status of each item; user notification \noccurs only if operator okays notification; the operator may edit \nnotification.\n6\n25% need help\n20% need help\n7\n50% get \ninformation\n90% get \ninformation\n  9\nGranule-level order \ntracking\nAn operator and user can determine the status for each item in their \norder.\n6\n25% need help\n10% need help\n7\n50% get \ninformation\n95% get \ninformation\n10\nLinks to user \ninformation\nAn operator can quickly locate a user’s contact information. \nServer will access SDSRV information to determine any data \nrestrictions that might apply and will route orders/order segments to \nappropriate distribution capabilities, including DDIST, PDS, external \nsubsetters and data processing tools, etc.\n7\n50% get \ninformation\n60% get \ninformation\n\n\n23.4  Case Study: The NASA ECS Project\n455\nStep 6: Determine the Utility of the “Expected” Quality \nAttribute Response Levels by Interpolation\nOnce the expected response level of every architectural strategy has been char-\nacterized with respect to a set of scenarios, their utility can be calculated by con-\nsulting the utility scores for each scenario’s current and desired responses for all \nof the affected attributes. Using these scores, we may calculate, via interpolation, \nthe utility of the expected quality attribute response levels for the architectural \nstrategy/scenario pair applied to the DAWG of ECS.\nTable 23.6  Architectural Strategies and Their Expected Utility\nStrategy \nName\nScenarios \nAffected\nCurrent \nUtility\nExpected \nUtility\n  1\nOrder persistence on submission\n3\n5\n6\n70\n70\n80\n  90\n100\n100\n  2\nOrder chunking\n8\n20\n  60\n  3\nOrder bundling\n9\n10\n50\n70\n  80\n  65\n  4\nOrder segmentation\n4\n80\n  90\n  5\nOrder reassignment\n1\n80\n  92\n  6\nOrder retry\n4\n80\n  85\n  7\nForced order completion\n1\n80\n  87\n  8\nFailed order notification\n6\n7\n80\n70\n  85\n  90\n  9\nGranule-level order tracking\n6\n7\n80\n70\n  90\n  95\n10\nLinks to user information\n7\n70\n  75\nStep 7: Calculate the Total Benefit Obtained \nfrom an Architectural Strategy\nBased on the information collected, as represented in Table 23.6, the total benefit \nof each architectural strategy can now be calculated, following the equation from \nSection 23.2, repeated here:\nBi = ∑j (bi,j × Wj)\nThis equation calculates total benefit as the sum of the benefit that accrues to \neach scenario, normalized by the scenario’s relative weight. Using this formula, \nthe total benefit scores for each architectural strategy are now calculated, and the \nresults are given in Table 23.7.\n\n\n456 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nTable 23.7  Total Benefit of Architectural Strategies\nStrategy \nScenario \nAffected\nScenario \nWeight\nRaw \nArchitectural \nStrategy \nBenefit\nNormalized \nArchitectural \nStrategy \nBenefit\nTotal Architectural \nStrategy Benefit\n  1\n  3\n15\n20\n300\n  1\n  5 \n15\n30\n450\n  1\n  6 \n10\n20\n200\n950\n  2\n 8\n 5\n40\n200\n200\n  3\n  9 \n10\n30\n300\n  3\n10\n  5\n–5\n–25\n275\n  4\n  4 \n10\n10\n100\n100\n  5\n  1 \n10\n12\n120\n120\n  6\n  4 \n10\n  5\n  50\n  50\n  7\n  1\n10\n  7\n  70\n  70\n  8\n  6\n10\n  5\n  50\n  8\n  7\n  5\n20\n100\n150\n  9\n  6\n10\n10\n100\n  9\n  7\n  5\n25\n125\n225\n10\n 7\n 5\n 5\n 25\n 25\nStep 8: Choose Architectural Strategies Based \non VFC Subject to Cost Constraints\nTo complete the analysis, the team estimated cost for each architectural strategy. \nThe estimates were based on experience with the system, and a return on invest-\nment for each architectural strategy was calculated. Using the VFC, we were able \nto rank each strategy. This is shown in Table 23.8. Not surprisingly, the ranks \nroughly follow the ordering in which the strategies were proposed: strategy 1 has \nthe highest rank; strategy 3 the second highest. Strategy 9 has the lowest rank; \nstrategy 8, the second lowest. This simply validates stakeholders’ intuition about \nwhich architectural strategies were going to be of the greatest benefit. For the \nECS these were the ones proposed first.\nResults of the CBAM Exercise\nThe most obvious results of the CBAM are shown in Table 23.8: an ordering of \narchitectural strategies based on their predicted VFC. However, just as for the \nATAM method, the benefits of the CBAM extend beyond the qualitative out-\ncomes. There are social and cultural benefits as well.\n\n\n23.5  Summary\n457\nTable 23.8  VFC of Architectural Strategies\nStrategy\nCost\nTotal Strategy Benefit\nStrategy VFC\nStrategy Rank\n  1\n1200\n950\n0.79\n  1\n  2\n  400\n200\n0.5\n  3\n  3\n  400\n275\n0.69\n  2\n  4\n  200\n100\n0.5\n  3\n  5\n  400\n120\n0.3\n  7\n  6\n  200\n  50\n0.25\n  8\n  7\n  200\n  70\n0.35\n  6\n  8\n  300\n150\n0.5\n  3\n  9\n1000\n225\n0.22\n10\n10\n  100\n  25\n0.25\n  8\nJust as important as the ranking of architectural strategies in CBAM is the \ndiscussion that accompanies the information-collecting and decision-making pro-\ncesses. The CBAM process provides a great deal of structure to what is always \nlargely unstructured discussions, where requirements and architectural strategies \nare freely mixed and where stimuli and response goals are not clearly articulated. \nThe CBAM process forces the stakeholders to make their scenarios clear in ad-\nvance, to assign utility levels of specific response goals, and to prioritize these \nscenarios based on the resulting determination of utility. Finally, this process re-\nsults in clarification of both scenarios and requirements, which by itself is a sig-\nnificant benefit.\n23.5  Summary\nArchitecture-based economic analysis is grounded on understanding the utili-\nty-response curve of various scenarios and casting them into a form that makes \nthem comparable. Once they are in this common form—based on the common \ncoin of utility—the VFC for each architecture improvement, with respect to each \nrelevant scenario, can be calculated and compared.\nApplying the theory in practice has a number of practical difficulties, but in \nspite of those difficulties, we believe that the application of economic techniques \nis inherently better than the ad hoc decision-making approaches that projects \n(even quite sophisticated ones) employ today. Our experience with the CBAM \ntells us that giving people the appropriate tools to frame and structure their dis-\ncussions and decision making is an enormous benefit to the disciplined develop-\nment of a complex software system.\n\n\n458 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\n23.6  For Further Reading\nThe origins of the CBAM can be found in two papers: [Moore 03] and Kazman \n[01].\nA more general background in economic approaches to software engineer-\ning may be found in the now-classic book by Barry Boehm [Boehm 81].\nAnd a more recent, and somewhat broader, perspective on the field can be \nfound in [Biffl 10].\nThe product-line analysis we used in the sidebar on the value of variation \npoints came from a paper in the 2011 International Software Product Line Con-\nference by John McGregor and his colleagues [McGregor 11].\n23.7  Discussion Questions\n1.\t\nThis chapter is about choosing an architectural strategy using rational, eco-\nnomic criteria. See how many other ways you can think of to make a choice \nlike this. Hint: Your candidates need not be “rational.”\n2.\t\nHave two or more different people generate the utility curve for a quality \nattribute scenario for an ATM. What are the difficulties in generating the \ncurve? What are the differences between the two curves? How would you \nreconcile the differences?\n3.\t\nDiscuss the advantages and disadvantages of the method for generating \nscenario priorities used in the CBAM. Can you think of a different way to \nprioritize the scenarios? What are the pluses and minuses of your method?\n4.\t\nUsing the results of your design exercise for the ATM from Chapter 17 as a \nstarting point, develop an architectural strategy for achieving a quality attri-\nbute scenario that your design does not cover.\n5.\t\nGenerate the utility curves for two different systems in the same domain. \nWhat are the differences? Do you believe that there are standard curves de-\npending on the domain? Defend your answer.\n\n\n459\n24\nArchitecture \nCompetence\nThe ideal architect should be a man of letters, a \nskillful draftsman, a mathematician, familiar with \nhistorical studies, a diligent student of philosophy, \nacquainted with music, not ignorant of medicine, \nlearned in the responses of jurisconsults, familiar \nwith astronomy and astronomical calculations.\n—Vitruvius, De Architectura (25 B.C.)\nThe lyf so short, the craft so long to lerne.\n—Geoffrey Chaucer\nIf software architecture is worth “doing,” surely it’s worth doing well. Most of \nthe literature about architecture concentrates on the technical aspects. This is \nnot surprising; it is a deeply technical discipline. There is little information that \nspeaks to the fact that architectures are created by architects working in organiza-\ntions, full of actual human beings. Dealing with these humans is decidedly non-\ntechnical. What can be done to help architects, especially architects-in-training, \nbe better at this important dimension of their job? And what can be done to help \narchitecture organizations do a better job in fostering their architects to produce \ntheir best work?\nAn organization’s ability to routinely produce high-quality architectures that \nare aligned with its business goals well cannot be understood simply through ex-\namination of past architectures and measurement of their deficiencies. The or-\nganizational and human causes of those deficiencies also need to be understood. \nThis chapter is about the competence of individual architects and the orga-\nnizations that wish to produce high-quality architects. We define the architecture \ncompetence of an organization as follows:\nThe architecture competence of an organization is the ability of \nthat organization to grow, use, and sustain the skills and knowledge \n\n\n460 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nnecessary to effectively carry out architecture-centric practices at the \nindividual, team, and organizational levels to produce architectures \nwith acceptable cost that lead to systems aligned with the organization’s \nbusiness goals.\nBecause the architecture competence of an organization depends, in part, on \nthe competence of architects, we begin by asking what it is that architects are ex-\npected to do, know, and be skilled at. Then we’ll look at what organizations can and \nshould do to help their architects produce better architectures. Individual and orga-\nnizational competencies are intertwined. Studying only one or the other won’t do. \n24.1  \u0007Competence of Individuals: Duties, \nSkills, and Knowledge of Architects\nArchitects perform many activities beyond directly producing an architecture. \nThese activities, which we call duties, form the backbone of an individual’s ar-\nchitecture competence. We surveyed a broad body of information aimed at archi-\ntects (such as websites, courses, books, and position descriptions for architects). \nWe also surveyed practicing architects. These surveys tell us that duties are but \none aspect of individual competence. Writers about architects also speak of skills \nand knowledge. For example, the ability to communicate ideas clearly and to ne-\ngotiate effectively are skills often ascribed to competent architects. In addition, \narchitects need to have up-to-date knowledge about patterns, database platforms, \nweb services standards, quality attributes, and a host of other topics. \nDuties, skills, and knowledge1 form a triad upon which architecture compe-\ntence for individuals rests. The relationship among these three is shown in Figure \n24.1—namely, skills and knowledge support the ability to perform the required du-\nties. Infinitely talented architects are of no use if they cannot (for whatever reason) \nperform the duties required of the position; we would not say they were competent. \nTo give examples of these concepts:\n■\n■“Design the architecture” is a duty. \n■\n■“Ability to think abstractly” is a skill. \n■\n■“Patterns and tactics” is a part of the body of knowledge. \nThis example purposely illustrates that skills and knowledge are important \n(only) for supporting the ability to carry out duties effectively. As another exam-\nple, “documenting the architecture” is a duty, “ability to write clearly” is a skill, \nand “ISO Standard 42010” is part of the related body of knowledge. Of course, a \nskill or knowledge area can support more than one duty. \n1.   Some writers speak of the importance of experience. We count experience as a form of \nknowledge. \n\n\n24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n461\nDuties\nKnowledge\nSkills\nSupport\nFigure 24.1  Skills and knowledge support the execution of duties.\nKnowing the duties, skills, and knowledge of architects (or, more precisely, \nthe duties, skills, and knowledge that are needed of architects in a particular or-\nganizational setting) can help establish measurement and improvement strategies \nfor individual architects. If you want to improve your individual architectural \ncompetence, you should do the following:\n1.\t\nGain experience carrying out the duties. Apprenticeship is a productive \npath to achieving experience. Education alone is not enough, because edu-\ncation without on-the-job application merely enhances knowledge.\n2.\t\nImprove your nontechnical skills. This dimension of improvement involves \ntaking professional development courses, for example, in leadership or time \nmanagement. Some people will never become truly great leaders or com-\nmunicators, but we can all improve on these skills.\n3.\t\nMaster the body of knowledge. One of the most important things a compe-\ntent architect must do is master the body of knowledge and remain up to \ndate on it. To emphasize the importance of remaining up to date, consider \nthe advances in knowledge required for architects that have emerged in \njust the last few years. For example, the cloud and edge computing that we \ndiscuss in Chapters 26 and 27 were not important topics several years ago. \nTaking courses, becoming certified, reading books and journals, visiting \nwebsites and portals, reading blogs, attending architecture-oriented confer-\nences, joining professional societies, and meeting with other architects are \nall useful ways to improve knowledge. \nDuties\nThis section summarizes a wide variety of an architect’s duties. Not every architect \nin every organization will perform every one of these duties on every project. But \na competent architect should not be surprised to find himself or herself engaged in \n\n\n462 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nany of the activities listed here. We divide the duties into technical duties (Table \n24.1) and nontechnical duties (Table 24.2). One immediate observation you should \nmake is how many nontechnical duties there are. An obvious implication, for those \nof you who wish to be architects, is that you must pay adequate attention to the \nnontechnical aspects of your education and your professional activities.\nTable 24.1  The Technical Duties of a Software Architect\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nArchitecting\nCreating an \narchitecture\nDesign or select an architecture. Create software \narchitecture design plan. Build product line or product \narchitecture. Make design decisions. Expand detail \nand refine design to converge on final design. Identify \nthe patterns and tactics and articulate the principles \nand key mechanisms of the architecture. Partition the \nsystem. Define how the components fit together and \ninteract. \nEvaluating and \nanalyzing an \narchitecture\nEvaluate an architecture (for your current system \nor for other systems) to determine satisfaction of \nuse cases and quality attribute scenarios. Create \nprototypes. Participate in design reviews. Review \nconstruction-level designs. Review the designs \nof the components designed by junior engineers. \nReview designs for compliance with the architecture. \nCompare software architecture evaluation \ntechniques. Apply value-based architecting \ntechniques to evaluate architectural decisions. Model \nalternatives. Perform tradeoff analysis.\nDocumenting an \narchitecture\nPrepare architectural documents and presentations \nuseful to stakeholders. Document software \ninterfaces. Produce documentation standards. \nDocument variability and dynamic behavior. \nWorking with \nand transforming \nexisting \nsystem(s)\nMaintain and evolve existing system and its \narchitecture. Redesign existing architecture(s) for \nmigration to new technology and platforms. \nPerforming other \narchitecting \nduties \nSell the vision, keep the vision alive. Participate in \nproduct design meetings. Give technical advice \non architecture, design, and development. Provide \narchitectural guidelines for software design activities. \nLead architecture improvement activities. Participate \nin software process definition and improvement. \nDefine philosophy and principles for global \narchitecture. Oversee or manage the architecture \ndefinition process. Provide architecture oversight of \nsoftware development projects.\n\n\n24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n463\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nDuties \nconcerned \nwith life-cycle \nactivities \nother than \narchitecting\nManaging the \nrequirements\nAnalyze functional and quality attribute software \nrequirements. Understand business and customer \nneeds and ensure that the requirements meet \nthese needs. Capture customer, organizational, and \nbusiness requirements on the architecture. Create \nsoftware specifications from business requirements. \nArticulate and refine architectural requirements. \nListen to and understand the scope of the project. \nUnderstand the client’s key design needs and \nexpectations. \nImplementing \nthe product\nProduce code. Conduct code reviews. Develop \nreusable software components. Analyze, select, \nand integrate software components. Set and ensure \nadherence to coding guidelines. Recommend \ndevelopment methodologies and coding standards. \nMonitor, mentor, and review the work of outside \nconsultants and vendors.\nTesting the \nproduct\nEstablish architecture-based testing procedures. \nSupport system testers. Support field testing. Support \nbug fixing and maintenance.\nEvaluating future \ntechnologies\nEvaluate and recommend enterprise’s software \nsolutions. Manage the introduction of new software \nsolutions. Analyze current IT environment and \nrecommend solutions for deficiencies. Work with \nvendors to represent organization’s requirements \nand influence future products. Develop and present \ntechnical white papers. \nSelecting tools \nand technology \nPerform technical feasibility studies of new \ntechnologies and architectures. Evaluate commercial \ntools and software components from an architectural \nperspective. Develop internal technical standards and \ncontribute to the development of external technical \nstandards. \nSkills\nGiven the wide range of duties enumerated in the previous section, what skills \n(beyond mastery of the technical body of knowledge) does an architect need to \npossess? Much has been written about the architect’s special role of leadership in \na project; the ideal architect is an effective communicator, manager, team builder, \nvisionary, and mentor. Some certificate or certification programs emphasize non-\ntechnical skills. Common to these certification programs are nontechnical assess-\nment areas of leadership, organization dynamics, and communication. \nTable 24.3 enumerates the set of nontechnical skills most useful to an \narchitect. \n\n\n464 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nTable 24.2  The Nontechnical Duties of a Software Architect\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nManagement\nManaging the \nproject\nHelp with budgeting and planning. Follow budgetary \nconstraints. Manage resources. Perform sizing and \nestimation. Perform migration planning and risk \nassessment. Take care of or oversee configuration \ncontrol. Create development schedules. Measure \nresults using quantitative metrics and improve both \npersonal results and teams’ productivity. Identify and \nschedule architectural releases. \nManaging the \npeople\nBuild “trusted advisor” relationships. Coordinate. \nMotivate. Advocate. Delegate. Act as a supervisor.\nSupporting the \nmanagement\nProvide feedback on appropriateness and difficulty of \nproject. Advise the project manager on the tradeoffs \nbetween software design choices and requirements \nchoices. Provide input to software project manager in \nthe software project planning and estimation process. \nServe as a “bridge” between the technical team and \nthe project manager. \nOrganization and \nbusiness-related \nduties\nSupporting the \norganization \nGrow an architecture evaluation capability in the \norganization. Review and contribute to research and \ndevelopment efforts. Participate in the hiring process \nfor the team. Help with product marketing. Institute \nand oversee cost-effective software architecture \ndesign reviews. Help develop intellectual property. \nSupporting the \nbusiness\nTranslate business strategy into technical vision and \nstrategy. Influence the business strategy. Understand \nand evaluate business processes. Understand \nand communicate the business value of software \narchitecture. Help the organization meet its business \ngoals. Understand customer and market trends. \nIdentify, understand, and resolve business issues. \nAlign architecture with the business goals and \nobjectives.\nLeadership and \nteam building\nProviding \ntechnical \nleadership\nMentor other architects. Produce technology trend \nanalysis or roadmaps. \nBuilding a team Set team context (vision). Build the architecture \nteam and align them with the vision. Mentor junior \narchitects. Educate the team on the use of the \narchitecture. Maintain morale, both within and outside \nthe architecture group. Foster the professional \ndevelopment of team members. Coach teams of \nsoftware design engineers for planning, tracking, and \ncompletion of work within the agreed plan. Mentor \nand coach staff in the use of software technologies. \nWork both as a leader and as an individual \ncontributor.\n\n\n24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n465\nTable 24.3  The Nontechnical Skills of a Software Architect\nGeneral Skill \nArea\nSpecific Skill \nArea\nExample  \nSkills\nCommunication \nskills\nOutward\nAbility to make oral and written communications \nand presentations. Ability to present and explain \ntechnical information to diverse audiences. Ability \nto transfer knowledge. Ability to persuade. Ability to \nsee from and sell to multiple viewpoints.\nInward\nAbility to listen, interview, consult, and negotiate. \nAbility to understand and express complex topics.\nInterpersonal \nskills\nWithin team\nAbility to be a team player. Ability to work \neffectively with superiors, colleagues, and \ncustomers. Ability to maintain constructive \nworking relationships. Ability to work in a diverse \nteam environment. Ability to inspire creative \ncollaboration. Ability to build consensus. \nWith other \npeople\nAbility to demonstrate interpersonal skills. Ability to \nbe diplomatic and respect others. Ability to mentor \nothers. Ability to handle and resolve conflict. \nWork skills\nLeadership\nAbility to make decisions. Ability to take initiative \nand be innovative. Ability to demonstrate \nindependent judgment, be influential, and \ncommand respect. \nWorkload \nmanagement\nAbility to work well under pressure, plan, manage \ntime, estimate. Ability to support a wide range \nof issues and work on multiple complex projects \nconcurrently. Ability to effectively prioritize and \nexecute tasks in a high-pressure environment. \nSkills to excel \nin corporate \nenvironment\nAbility to think strategically. Ability to work under \ngeneral supervision and under given constraints. \nAbility to organize workflow. Ability to sense where \nthe power is and how it flows in an organization. \nAbility to do what it takes to get the job done. Ability \nto be entrepreneurial, be assertive without being \naggressive, and receive constructive criticism.\nSkills for \nhandling \ninformation\nAbility to be detail oriented while maintaining \noverall vision and focus. Ability to see the big \npicture. Ability to deal with abstraction. \nSkills for \nhandling the \nunexpected\nAbility to tolerate ambiguity. Ability to take and \nmanage risks. Ability to solve problems. \nAbility to be adaptable, flexible, open minded, and \nresilient. Ability to do the juggling necessary to \ndeploy successful software projects.\n",
      "page_number": 473
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 486-494)",
      "start_page": 486,
      "end_page": 494,
      "detection_method": "topic_boundary",
      "content": "466 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nKnowledge\nA competent architect has an intimate familiarity with the architectural body of \nknowledge. The software architect should \n■\n■Be comfortable with all branches of software engineering from require-\nments definition to implementation, development, verification and valida-\ntion, and deployment\n■\n■Be familiar with supporting disciplines such as configuration management \nand project management \n■\n■Understand current design and implementation tools and technologies \nKnowledge and experience in one or more application domains is also \nnecessary.\nTable 24.4 is the set of knowledge areas for an architect.\nTable 24.4  The Knowledge Areas of a Software Architect\nGeneral \nKnowledge  \nArea\nSpecific  \nKnowledge  \nArea\nSpecific  \nKnowledge  \nExamples\nComputer \nscience \nknowledge\nKnowledge of \narchitecture \nconcepts\nKnowledge of architecture frameworks, \narchitectural patterns, tactics, viewpoints, \nstandard architectures, relationship \nto system and enterprise architecture, \narchitecture description languages, emerging \ntechnologies, architecture evaluation models \nand methods, and quality attributes.\nKnowledge \nof software \nengineering \nKnowledge of systems engineering. \nKnowledge of software development life \ncycle, software process management, \nand improvement techniques. Knowledge \nof requirements analysis, mathematics, \ndevelopment methods and modeling \ntechniques, elicitation techniques. Knowledge \nof component-based software development, \nreuse methods and techniques, software \nproduct-line techniques, documentation, \ntesting and debugging tools. \nDesign knowledge Knowledge of different tools and design \ntechniques. Knowledge of how to design \ncomplex multi-product systems. Knowledge \nof object-oriented analysis and design, UML \ndiagrams, and UML analysis modeling.\nProgramming \nknowledge\nKnowledge of programming languages and \nprogramming language models. Knowledge \nof specialized programming techniques for \nsecurity, real time, etc. \n\n\n24.2  Competence of a Software Architecture Organization\n467\nGeneral \nKnowledge  \nArea\nSpecific  \nKnowledge  \nArea\nSpecific  \nKnowledge  \nExamples\nKnowledge of \ntechnologies \nand platforms\nKnowledge \nof specific \ntechnologies and \nplatforms\nKnowledge of hardware/software interfaces, \nweb-based applications, and Internet \ntechnologies. Knowledge of specific software/\noperating systems, such as RDBMS concepts, \ncloud platforms, and SOA implementations.\nGeneral \nknowledge of \ntechnologies and \nplatforms\nKnowledge of IT industry future directions and \nthe ways in which infrastructure impacts an \napplication. \nKnowledge \nabout the \norganization’s \ncontext and \nmanagement\nDomain \nknowledge\nKnowledge of the most relevant domain(s) \nand domain-specific technologies. \nIndustry \nknowledge\nKnowledge of the industry’s best practices and \nindustry standards. Knowledge of how to work \nin onshore/offshore team environment. \nEnterprise \nknowledge\nKnowledge of the company’s business \npractices, and your competition’s products, \nstrategies, and processes. Knowledge of \nbusiness and technical strategy, and business \nreengineering principles and processes. \nKnowledge of strategic planning, financial \nmodels, and budgeting.\nLeadership and \nmanagement \ntechniques \nKnowledge of coaching, mentoring, and \ntraining software developers. Knowledge of \nproject management. Knowledge of project \nengineering.\n24.2  \u0007Competence of a Software \nArchitecture Organization\nOrganizations by their practices and structure can help or hinder architects in per-\nforming their duties. For example, if an organization has a career path for archi-\ntects, that will motivate employees to become architects. If an organization has a \nstanding architecture review board, then the project architect will know how and \nwith whom to schedule a review. Lack of these items will mean that an architect \nhas to fight battles with the organization or determine how to carry out a review \nwithout internal guidance. It makes sense, therefore, to ask whether a particular \norganization is architecturally competent and to develop instruments whose goal \nis measuring the architectural competence of an organization. The architectural \ncompetence of organizations is the topic of this section.\n\n\n468 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nActivities Carried Out by a Competent Organization\nOrganizations have duties, skills, and knowledge for architecture as well. For ex-\nample, adequately funding the architecture effort is an organizational duty, as is \neffectively using the available architecture workforce (by appropriate teaming, \nand so on). These are organizational duties because they are outside the control \nof individual architects. An organization-level skill might be effective knowl-\nedge management or human resource management as applied to architects. An \nexample of organizational knowledge is the composition of an architecture-based \nlife-cycle model that software projects may employ. \nHere are some things—duties—that an organization can perform to help im-\nprove the success of its architecture efforts:\n■\n■Hire talented architects.\n■\n■Establish a career track for architects.\n■\n■Make the position of architect highly regarded through visibility, reward, \nand prestige.\n■\n■Establish a clear statement of responsibilities and authority for architects. \n■\n■Establish a mentoring program for architects.\n■\n■Establish an architecture training and education program. \n■\n■Establish an architect certification program.\n■\n■Have architects receive external architect certifications.\n■\n■Measure architects’ performance.\n■\n■Establish a forum for architects to communicate and share information and \nexperience.\n■\n■Establish a repository of reusable architectures and architecture-based \nartifacts.\n■\n■Develop reusable reference architectures.\n■\n■Establish organization-wide architecture practices.\n■\n■Establish an architecture review board.\n■\n■Measure the quality of architectures produced.\n■\n■Provide a centralized resource to analyze and help with architecture tools.\n■\n■Hold an organization-wide architecture conference.\n■\n■Have architects join professional organizations.\n■\n■Bring in outside expert consultants on architecture.\n■\n■Include architecture milestones in project plans.\n■\n■Have architects provide input into product definition.\n■\n■Have architects advise on the development team structure.\n■\n■Give architects influence throughout the entire project life cycle. \n■\n■Reward or penalize architects based on project success or failure.\n\n\n24.2  Competence of a Software Architecture Organization\n469\nAssessment Goals\nThe activities enumerated above can be assessed. What are the potential goals \nfrom an assessment of an organization’s architecture competence? There are at \nleast four sets of reasons for assessing organizational architectural competence: \n1.\t\nThere are goals relevant to any business that wishes to improve their archi-\ntectural competence. Businesses regularly assess their own performance in \na variety of means—technical, fiscal, operational (for example, consider the \nwidespread use of multi-criteria techniques such as the Balanced Scorecard \nor Business/IT Alignment in industry)—for a variety of reasons. These \ninclude determining whether they are meeting industry norms and gauging \ntheir progress over time in meeting organizational goals.\n2.\t\nThere are goals relevant to an acquisition organization. For example, an \norganization can use an assessment of architecture competence to assess \na contractor in much the same way that contractors are scrutinized with \nrespect to their CMMI level. Or an organization might use an assessment \nof architecture competence to aid in deciding among competing bids from \ncontractors. All other things being equal, an acquiring organization would \nprefer a contractor with a higher level of architectural competence because \nthis typically means fewer downstream problems and rework. An acquisi-\ntion organization might assess the contractors directly, or hire a third party \nto do the assessment.\n3.\t\nThere are goals relevant to service organizations: such organizations might \nbe motivated to maintain, measure, and advertise their architectural compe-\ntence as a means of attracting and retaining customers. In such a case they \nwould typically rely on outside organizations to assess their level of compe-\ntence in an objective fashion.\n4.\t\nFinally, there are goals that are relevant to product builders: these organi-\nzations would be motivated to assess, monitor (and, over time, increase) \ntheir level of architectural competence as it would (1) aid in advertising the \nquality of their products and (2) aid in their internal productivity and pre-\ndictability. In fact, in these ways their motivations are aligned with those of \nservice organizations. \nAssessing an Organization’s Competence\nIn addition to duties, skills, and knowledge, there are other models of individual \nand organizational competence that are helpful in building an instrument for as-\nsessing an organization’s competence. They are the following:\n■\n■The Human Performance Technology model, which measures the value of \nan individual or department’s output and the cost of producing that output. \n\n\n470 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nIt holds that competent people produce the most value for every organiza-\ntional dollar spent. \n■\n■The Organizational Coordination model, which measures how teams in \nmultiple sites developing a single product or related set of products cooper-\nate to produce a functioning product. An organization that is architecturally \ncompetent will have more effective and efficient coordination mechanisms \nthan an organization that is not architecturally competent. \n■\n■The Organizational Learning model, which measures how well an organi-\nzation’s learning processes transform experience into knowledge, moderat-\ned by context. \nWe have created a framework for organizational architecture competence \nthat forms the foundation for a competence assessment procedure. A small team \nof trained assessors can use the framework to conduct interviews (with architects, \ndevelopers, and technical and organizational managers), examine current prac-\ntices, read documents and evidentiary artifacts (such as organizational standards), \nand investigate architecture-based successes and failures in the recent past. They \ncan use their findings to identify systemic trouble spots and recommend improve-\nment strategies.\nTable 24.5 shows the framework. For convenience, it is divided into prac-\ntice areas that relate to software and system engineering, technical management \n(which is by and large the management of single projects or small numbers of re-\nlated projects), and organizational management (which is management at a scope \nmore broad than that of projects). \nThe framework is populated by questions inspired by the four models of \ncompetence that we previously described. Each question has a set of answers \nthat we might expect to see in a competent organization. For example, a ques-\ntion associated with the practice area “Hire talented architects” deals with how a \ncandidate architect’s experience and capabilities are assessed. Expected answers \nmight include having the architect take a test, requiring that candidates possess \nan architecture certification, or examining previous architectures designed by the \ncandidate. (Our expected answers grow as we visit more and more organizations. \nIt’s a pleasant surprise when we find an organization carrying out a practice area \nin a clever way that we hadn’t thought of.)\nQuestions Based on the Duties, Skills, and Knowledge Model.  Our \nassessment framework contains dozens of questions related to duties, skills, and \nknowledge. The questions are posed in terms of the organization: The questions \nask how the organization ensures that the architectural duties are carried out in \na competent manner, and how the organization measures and nurtures its archi-\ntects’ skills and knowledge. Here is a small set of example questions based on \nthe Duties, Skills, and Knowledge model (chosen from among the dozens that \npopulate our assessment framework) that we use in an architecture competence \nassessment exercise with an organization.\n\n\n24.2  Competence of a Software Architecture Organization\n471\nTable 24.5  Framework for Organizational Architecture Competence\nSoftware \nEngineering  \nPractice Areas\nQuality Attribute Elicitation Practices\nTools and Technology Selection\nModeling and Prototyping Practices\nArchitecture Design Practices\nArchitecture Description Practices\nArchitecture Evaluation Practices\nSystem Implementation Practices\n■\n■\nSoftware design practices (design conforms to \narchitecture)\n■\n■\nSoftware coding practices (code conforms to design and \narchitecture)\nSoftware Verification Practices\n■\n■\nProving properties of the software\n■\n■\nSoftware testing\nArchitecture Reconstruction Practices\nTechnical \nManagement \nPractice Areas\nBusiness or Mission Goals Practices\n■\n■\nSetting goals\n■\n■\nMeasuring achievement of organization’s goals\n■\n■\nPerformance-based compensation\nProduct or System Definition Practices\n■\n■\nSetting functional requirements\nAllocating Resources\n■\n■\nSetting architect’s workload and schedule\n■\n■\nFunding stakeholder involvement\nProject Management Practices \n■\n■\nProject plan structure aligned with architecture structure\n■\n■\nAdequate time planned for architecture evaluation\nProcess Discipline Practices\n■\n■\nEstablish organization-wide architecture practices\n■\n■\nProcess monitoring and improvement practices\n■\n■\nReuse practices\nCollaboration with Manager Practices\n■\n■\nArchitects advise managers\n■\n■\nArchitects support managers\ncontinues\n\n\n472 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nTable 24.5  Framework for Organizational Architecture Competence, continued\nOrganizational \nManagement \nPractice Areas\nHire Talented Architects\nEstablish a Career Track for Architects\n■\n■\nLeadership roles for architects\n■\n■\nSuccession planning\nProfessional Development Practices\n■\n■\nOngoing training\n■\n■\nCreating and sustaining an internal community of \narchitects\n■\n■\nSupporting participation in external communities\nOrganizational Planning Practices \nTechnology Planning and Forecasting Practices\nDuty: Creating an Architecture\nQuestion: How do you create an architecture?\n■\n■How do you ensure that the architecture is aligned with the business goals?\n■\n■What is the input into the architecture creation process? What inputs are \nprovided to the architect? \n■\n■How does the architect validate the information provided? What does the \narchitect do in case the input is insufficient or inadequate?\nDuty: Architecture Evaluation and Analysis\nQuestion: How do you evaluate and analyze an architecture?\n■\n■Are evaluations part of the normal software development life cycle or are \nthey done when problems are encountered? \n■\n■Is the evaluation incremental or “big bang”? How is the timing determined?\n■\n■Does the evaluation include an explicit activity relating architecture to busi-\nness goals?\n■\n■What are the inputs to the evaluation? How are they validated?\n■\n■What are the outputs from an evaluation? How are the outputs of the evalu-\nation utilized? Are the outputs differentiated according to impact or impor-\ntance? How are the outputs validated? Who is communicated what outputs?\nKnowledge: Architecture Concepts\nQuestion: How does your organization ensure that its architects have adequate \narchitectural knowledge?\n■\n■How are architects trained in general knowledge of architecture?\n■\n■How do architects learn about architectural frameworks, patterns, tactics, \nstandards, documentation notations, and architecture description languages?\n\n\n24.2  Competence of a Software Architecture Organization\n473\n■\n■How do architects learn about new or emerging architectural technologies \n(e.g., multi-core processors)? \n■\n■How do architects learn about analysis and evaluation techniques and \nmethods?\n■\n■How do architects learn quality attribute-specific knowledge, such as tech-\nniques for analyzing and managing availability, performance, modifiability, \nand security?\n■\n■How are architects tested to ensure that their level of knowledge is ade-\nquate, and remains adequate, for the tasks that they face?\nQuestions Based on the Organizational Coordination Model.  Ques-\ntions based on the Organizational Coordination model focus on how the organi-\nzation establishes its teams and what support it provides for those teams to coor-\ndinate effectively. Here are a couple of example questions:\nQuestion: How is the architecture designed with distribution of work to \nteams in mind?\n■\n■How available or broadly shared is the architecture to various teams?\n■\n■How do you manage the evolution of architecture during development?\n■\n■Is the work assigned to the teams before or after the architecture is defined, \nand with due consideration of the architectural structure?\nQuestion: Are the aspects of the architecture that will require a lot of inter-\nteam coordination supported by the organization’s coordination/communication \ninfrastructure?\n■\n■Do you co-locate teams with high coordination? Or at least put them in the \nsame time zone? \n■\n■Must all coordination among teams go through the architecture team?\nQuestions Based on the Human Performance Technology Model.  \nThe Human Performance Technology questions deal with the value and cost of \nthe organization’s architectural activities. Here are examples of questions based \non the Human Performance Technology model:\nQuestion: Do you track how much the architecture effort costs, and how it \nimpacts overall project cost and schedule?\n■\n■How do you track the end of architecture activities?\n■\n■How do you track the impact of architecture activities?\nQuestion: Do you track the value or benefits of the architecture? \n■\n■How do you measure stakeholder satisfaction?\n■\n■How do you measure quality? \n\n\n474 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nQuestions Based on the Organizational Learning Model.  Finally, a \nset of example questions, based on the Organizational Learning model, which \ndeal with how the organization systematically internalizes knowledge to its \nadvantage:\nQuestion: How do you capture and share experiences, lessons learned, techno-\nlogical decisions, techniques and methods, and knowledge about available tooling?\n■\n■Do you use any knowledge management tools?\n■\n■Is capture and use of architectural knowledge embedded in your processes?\n■\n■Where is the information about “who knows what” captured and how is this \ninformation maintained?\n■\n■How complete and up to date is your architecture documentation? How \nwidely disseminated is it?\nPerforming an Assessment\nOur organizational competence assessment is carried out using a team of three \nto four assessors. The exercise is set up by establishing the scope of the review: \nAre we assessing the entire company? One of its divisions? Or perhaps a single \nimportant project? \nAfter we establish the scope, we identify the groups we wish to interview. Of \ncourse, we’ll want to interview the architecture team(s) within the scope. From there \nwe identify groups both upstream and downstream of the architects. Upstream are \ngroups that manage the architects or provide organization-wide architecture training. \nDownstream, we interview the “consumers” of the architectures, such as developers, \nintegrators, testers, and maintainers. We interview small groups, making sure that no \nmembers of an interview group have reporting relationships with each other. \nWe try hard to establish an informal atmosphere in the group interviews, to \navoid inhibiting the participants. The tone is conversational, not inquisitional. We \nbegin each interview by reminding the participants of the purpose of the exercise, \nand to assure them that nothing they say will be quoted to anyone outside the \ngroup in any way that could identify them.\nFor each group, we have planned which parts of the framework we wish to \ndiscuss with that group. We won’t ask testers, for example, questions intended for \nmanagers, and vice versa. We use the questions as a guide for the conversation, but \nnot a rigid script. Whenever we pose a question in the assessment instrument, there \nare a number of meta-questions that automatically accompany it. For example:\n■\n■What evidence could you show us to support your answer? Supporting ev-\nidence might include a software development plan that lays out the role of \narchitecture in a project, an organization’s architecture-based training cur-\nriculum, or many other kinds of documentation.\n■\n■How sustainable is your answer over time, over different systems, and \nacross different architects? For example, we might ask how an answer \nmight change if a different architect came on board the project.\n",
      "page_number": 486
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 495-511)",
      "start_page": 495,
      "end_page": 511,
      "detection_method": "topic_boundary",
      "content": "24.4  For Further Reading\n475\nThe outcome of an assessment is organized by the practice areas of the \nframework. For each practice, we assign one of three values that correspond to \n“you’re doing this well,” “you could be doing this better (and experiencing more \nbenefit),” and “this is an area of high risk.” Graphically, we show this as green \nlight, yellow light, red light. We have found that this simple metric provides orga-\nnizations with enough granularity to turn their attention to problem areas, which \nis the whole point of the assessment. We do not give an overall rating. Thus, we \nclosely mirror the “continuous representation” option of maturity models such as \nCMMI, in which the result is a vector rather than a scalar. \nWe present the findings in a written report and a slide presentation. In both, \nwe describe and justify each finding, based on what we were told in the inter-\nviews and/or read in provided documents. \n24.3  Summary\nThe vast majority of work on software and systems architecture (including our \nown) has focused on the technical aspects. But an architecture is much more \nthan a technical “blueprint” for a system. This has led us to try to understand, in \na more holistic way, what an architect and an architecture-centric organization \nmust do to succeed. To this end, we have developed a framework that aids us in \nassessing an organization for competence. \nWe use the framework to ask questions about an organization’s practices. \nWe can also ask about recent architecture successes and failures, and investigate \nthe causes of each. The output of this exercise is a formal report that assesses \ncompetence at organization, team, and individual levels. Along with this report \nwe make improvement recommendations based on assessment results; these, too, \nare tied to the underlying competence models. \nYou can do the same sort of evaluation on your own organization. The key \nto the process is in understanding the various models and in creating questions \nbased on these models that aid you in assessing how well you are doing in those \nareas that you care about. Given this knowledge, you can create your own im-\nprovement plan, as an individual architect or for an entire organization.\n24.4  For Further Reading\nThe four models that underlie the assessment framework presented here are de-\nscribed in more detail in the Technical Note “Models for Evaluating and Improv-\ning Architecture Competence” [Bass 08]. These models are the following:\n\n\n476 \nPart Four  Architecture and Business\t\n24—Architecture Competence\n■\n■Duties, Skills, and Knowledge (DSK) model of competence. This model is \npredicated on the belief that architects and architecture-producing organiza-\ntions are useful sources for understanding the tasks necessary to the job of ar-\nchitecting. To assemble a comprehensive set of duties, skills, and knowledge \nfor architects, we surveyed approximately 200 sources of information target-\ned to professional architects—books, websites, blogs, position descriptions, \nand more. The results of this survey can be found in [Clements 07].\n■\n■Human Performance model of competence. This model is based on the hu-\nman performance engineering work of Thomas Gilbert [Gilbert 07]. This \nmodel is predicated on the belief that competent individuals in any pro-\nfession are the ones who produce the most valuable results at a reasonable \ncost. Using this model will involve figuring out how to measure the value \nand cost of the outputs of architecture efforts, finding areas where that ratio \ncan be improved, and crafting improvement strategies based on environ-\nmental and behavioral factors.\n■\n■Organizational Coordination model of competence. The focus of this mod-\nel is on creating an interteam coordination model for teams developing a \nsingle product or a closely related set of products. The architecture for the \nproduct induces a requirement for teams to coordinate during the realiza-\ntion or refinement of architectural decisions. The organizational structure, \npractices, and tool environment of the teams allow for particular types of \ncoordination with a particular interteam communication bandwidth. The \ncoordination model of competence compares the requirements for coor-\ndination that the architecture induces with the bandwidth for coordination \nsupported by the organizational structure, practices, and tool environment \n[Cataldo 07].\n■\n■Organizational Learning model of competence. This model is based on \nthe concept that organizations, and not just individuals, can learn. Organi-\nzational learning is a change in the organization that occurs as a function \nof experience. This change can occur in the organization’s cognitions or \nknowledge (e.g., as presented by Fiol and Lyles [Fiol 85]), its routines or \npractices (e.g., as demonstrated by Levitt and March [Levitt 88]), or its \nperformance (e.g., as presented by Dutton and Thomas [Dutton 84]). Al-\nthough individuals are the medium through which organizational learning \ngenerally occurs, learning by individuals within the organization does not \nnecessarily imply that organizational learning has occurred. For learning to \nbe organizational, it has to have a supra-individual component [Levitt 88]. \nThere are three approaches to measure organizational learning: (1) measure \nknowledge directly through questionnaires, interviews, and verbal proto-\ncols; (2) treat changes in routines and practices as indicators of changes in \nknowledge; or (3) view changes in organizational performance indicators \nassociated with experience as reflecting changes in knowledge [Argote 07].\n\n\n24.5  Discussion Questions\n477\nThe Open Group offers a certification program for qualifying the skills, \nknowledge, and experience of IT, business, and enterprise architects, which is \nrelated to measuring and certifying an individual architect’s competence. Visit \nopengroup.org for details. The International Association of Software Architects \n(IASA) offers a similar certification; see iasahome.org. \nDana Bredemeyer and Ruth Malan have written many articles on the \nrole of the software architect (www.bredemeyer.com/who.htm), including \ntheir duties and skills [Bredemeyer 11] (www.bredemeyer.com/Architect/ \nArchitectSkillsLinks.htm). They have their own competence framework as well \nas a skills development program.\nThe U.K. Chapter of the International Council on Systems Engineering (IN-\nCOSE) maintains a “Core Competencies Framework” for systems engineers that \nincludes a “Basic Skills and Behaviours” section listing “the usual common at-\ntributes required by any professional engineer” [INCOSE 05]. The list includes \ncoaching, communication, negotiation and influencing, and “team working.” \nThe classic work on the Balanced Scorecard was created by Kaplan and \nNorton [Kaplan 92] and the classic work on Business/IT Alignment was origi-\nnally created by Luftman [Luftman 00], although this has been updated to explic-\nitly consider the role of architecture in alignment [Chen 10].\nBoehm, Valerdi, and Honour [Boehm 07] provide one of the few empirical \nstudies of systems engineering and how an investment in “better” engineering \npays off (or doesn’t) in the future. \n24.5  Discussion Questions\n1.\t\nIn which skills and knowledge discussed in this chapter do you think you \nmight be deficient? How would you reduce these deficiencies?\n2.\t\nWhich duties, skills, or knowledge do you think are the most important or \ncost-effective to improve in an individual architect? Justify your answer.\n3.\t\nHow would you measure the specific value of architecture in a project? \nHow would you distinguish the value added by architecture from the val-\nue added by other activities such as quality assurance or configuration \nmanagement?\n4.\t\nHow do you measure items such as “customer satisfaction” or “negotiation \nskills”? How would you validate such measurements?\n5.\t\nHow would you distinguish benefits caused by systematic organizational \nlearning from the benefits due to heroic efforts by individuals within the \norganization?\n\n\n478 \nPart Four  Architecture and Business\t\n24—Architecture Competence\n6.\t\nSection 24.2 listed a number of practices of an architecturally competent \norganization. Prioritize that list based on expected benefit over expected \ncost. \n7.\t\nSuppose you are in charge of hiring an architect for an important system \nin your company. How would you go about it? What would you ask the \ncandidates in an interview? Would you ask them to produce anything? If so, \nwhat? Would you have them take a test of some kind? If so, what? Who in \nyour company would you have interview them? Why?\n\n\n479\n25\nArchitecture and \nSoftware Product Lines\nComing together is a beginning. Keeping together \nis progress. Working together is success.\n—Henry Ford\nA software architecture represents a significant investment of time and effort, \nusually by senior talent. So it is natural to want to maximize the return on this \ninvestment by reusing an architecture across multiple systems. \nThere are many ways this happens in practice. The patterns we discussed in \nChapter 13 are a big step in this direction; using a pattern is reusing a package of \narchitectural decisions (albeit not a complete architecture). And strictly speaking, \nevery time you make a change to a system, you are reusing its architecture (or \nwhatever portion of its architecture you don’t have to change). \nThis chapter shows yet another way to reuse a software architecture (and \nmany other assets as well) across a family of related systems, and the benefits \nthat doing so can bring. Many software-producing organizations tend to produce \nsystems or products that resemble each other more than they differ. This is an \nopportunity for reusing the architecture across these similar products. These soft-\nware product lines simplify the creation of new members of a family of similar \nsystems.\nThis kind of reuse has been shown to bring substantial benefits that include \nreduced cost of construction, higher quality, and greatly reduced time to market. \nThis is the lure of the software product line approach to system building.\nThe Software Engineering Institute defines a software product line as “a set \nof software-intensive systems sharing a common, managed set of features that \nsatisfy the specific needs of a particular market segment or mission and that are \ndeveloped from a common set of core assets in a prescribed way.”\nThe vision is of a set of reusable assets (called core assets) based on a com-\nmon architecture and the software elements that populate that architecture. The \n\n\n480 \nPart Four\t\n25—Architecture and Software Product Lines\ncore assets also include designs and their documentation, user manuals, project \nmanagement artifacts such as budgets and schedules, software test plans and test \ncases, and more. \nThe product line approach works because the core assets were built specifi-\ncally to support multiple members of the same family of products. Hence, reusing \nthem is faster and less expensive than reinventing those software assets for each \nnew product or system in the organization’s portfolio. Core assets, including the \narchitecture, are usually designed with built-in variation points—places where \nthey can be quickly tailored in preplanned ways. \nOnce the core assets are in place, system building becomes a matter of\n■\n■Accessing the appropriate assets in the core asset base\n■\n■Exercising the variation points to configure them as required for the system \nbeing built \n■\n■Assembling that system \nIn the ideal case, this can be done automatically. Additional software developed \nfor an individual product, if needed at all, tends to account for a small fraction of \nthe total software. Integration and testing replace design and coding as the pre-\ndominant activities. \nProduct lines are nothing new in manufacturing. Many historians trace the con-\ncept to Eli Whitney’s use of interchangeable parts to build rifles in the early \n1800s, but earlier examples also exist. Today, there are hundreds of examples \nin manufacturing: think of the products of companies like General Motors, Toy-\nota, Boeing, Airbus, Dell, even McDonald’s, and the portfolio of similar products \nthat each one produces. Each company exploits commonality in different ways. \nBoeing, for example, developed the 757 and 767 in tandem, and the parts lists of \nthese two very different aircraft overlap by about 60 percent. \nThe improvements in cost, time to market, and productivity that come with a suc-\ncessful software product line can be breathtaking. Consider:\n■\n■Nokia credits the software product line approach with giving it flexibility to \nbring over a dozen phones to market each year, as opposed to the three or \nso it could manage before, all with an unprecedented variety of features.\n■\n■Cummins, Inc., was able to reduce the time it takes to produce the software \nfor a diesel engine from about a year to about a week.\n■\n■Hewlett-Packard builds products using one-quarter of the staff, in one-third \nof the time, and with one twenty-fifth the number of defects, compared with \nsoftware built before the advent of software product line engineering. \n■\n■Deutsche Bank estimates $4 million in savings per year realized from \nbuilding its global transaction and settlement software as a product line. \n■\n■Philips reports reduced faults during integration in its high-end television \nportfolio by adopting the product line approach. Product diversity used \nto be one of the top three concerns of their architects. Now it doesn’t \neven make the list of concerns at all; the product line approach has taken \n\n\n25—Architecture and Software Product Lines﻿\n481\nsoftware development off the critical path—the software no longer deter-\nmines the delivery date of the product. \n■\n■With a product line of satellite ground control systems it commissioned, the \nU.S. National Reconnaissance Office reported the first product requiring \n10 percent the expected number of developers and having one-tenth the ex-\npected number of defects.\n■\n■In Philips’s medical systems product line, the software product line ap-\nproach has cut both software defects and time to market by more than half.\nCreating a successful product line depends on a coordinated strategy involving \nsoftware engineering, technical management, and organization management. Be-\ncause this is a book on software architecture, we focus on the architectural as-\npects of software product lines, but all aspects must work together in order for an \norganization to successfully create a product line.\nThat Silver Lining Might Have a Cloud\nThe software product line paradigm is a powerful way to leverage an \ninvestment in architecture (and other core assets) into a family of related \nsystems and thus see order-of-magnitude improvements in time to mar-\nket, quality, and productivity. These results are possible and have been \ndemonstrated by companies large and small in many different domains. \nThe effects are real. Further, data from many sources and companies con-\nfirms with astonishing consistency that, to make the investment pay off, \nan organization needs to build only three products. This is the minimum \nnumber we would expect to have in a product line.\nBut other results are possible as well, and a spectacular crash-and-burn \nis not out of the question when trying to adopt this approach. Product line \npractice, like any technology, needs careful thought given to its adoption, \nand a company’s history, situation, and culture must be taken into account. \nFactors that can contribute to product line failure include these:\n■\n■\nLack of a champion in a position of sufficient control and visibility\n■\n■\nFailure of management to provide sustained and unwavering support\n■\n■\nReluctance of middle managers to relinquish autocratic control of projects\n■\n■\nFailure to clearly identify business goals for adopting the product line \napproach\n■\n■\nAbandoning the approach at the first sign of difficulty\n■\n■\nFailure to adequately train staff in the approach and failure to explain or \njustify the change adequately\n■\n■\nLack of discipline in managing the architecture’s variation points\n■\n■\nScoping the product line too broadly or too narrowly\n■\n■\nLack of product line tooling to help manage and exercise the variation \npoints\n\n\n482 \nPart Four\t\n25—Architecture and Software Product Lines\nFortunately, there are strategies for overcoming most of these factors. \nOne good strategy is to launch a small but visible pilot project to demon-\nstrate the quantitative benefits of software product lines. The pilot can \nbe staffed by those most willing to try something new while the skeptics \ngo about their business. It can work out process issues, clarify roles and \nresponsibilities, and in general work out the bugs before the approach is \ntransitioned to a wider setting. \n—PCC\n25.1  An Example of Product Line Variability\nThe following example will help us illustrate the concept of product line vari-\nability. In a product line of software to support U.S. bank loan offices, suppose \nwe have a software module that calculates what a customer owes in the current \nmonth. For 18 of the 21 products in our product line, this module is completely \nadequate. However, our company is about to enter the market in the state of Del-\naware, which has certain laws that affect what a customer can owe. For the three \nproducts we plan to sell in Delaware, we need a module that differs from the \n“standard” module. Analysis shows that the difference will affect about 250 lines \nof source code in our 8,000-line module. \nTo build one of the Delaware products, what do we do? An obvious op-\ntion is to copy the module, change the 250 or so lines, and use the new version \nin the three products. This practice is called “clone-and-own”—the new projects \n“clone” the module, change it, and then “own” the new version. Most companies, \nwhen faced with this situation, resort to clone-and-own. It’s expedient in that it \nprovides a quick start to a new product, but it comes with a substantial cost down \nthe road. \nThe problem with clone-and-own is that it doesn’t scale. Suppose each of \nour 21 products comprises roughly 100 modules. If each module is allowed to \ndiverge for each product, that’s potentially 2,100 modules that the maintenance \nstaff has to deal with, each one spiraling off on its own separate maintenance \ntrajectory based on the needs of the lone project each version is used in. Many \ncompanies’ growth in a market is limited—brought to a halt, in fact—by their in-\nability to staff the maintenance of so many separate versions of so many different \nassets composing the products in their portfolio. An organization fielding several \nversions of several products finds itself dealing with a staggeringly complex code \nbase. The strain begins to show when a systematic change needs to be made to all \nof the products—for example, to add a new feature, or migrate to a new platform, \nor make the user interface work in a different language. Because each version \nof each component used in each product has been allowed to evolve separately, \nnow suddenly making a systematic change becomes prohibitively expensive (and \n\n\n25.2  What Makes a Software Product Line Work?\n483\nonly gets worse each time a new product is added—the labor involved grows as \nthe square of the number of products). It only takes a few such portfolio-wide \nchanges before organizations feel that they’ve hit a wall of complexity and \nexpense.\nSo much for clone-and-own. What else can we do? Instead of allowing up to \n21 versions of each module, we would much rather find a way to take advantage \nof the fact that these nearly identical modules vary only in small, well-defined \nways. To take advantage of their similarities, we introduce a variation mechanism \ninto the module. (Variation mechanisms are often realized as tactics, such as the \n“defer binding” set of tactics described in Chapter 7.) This variation mechanism \nwill let us maintain a single module that can adapt to the range of variations in \nthe applications (in our example, the 21 banking products) that it has to support. \nIf we plan to market our products in states that, like Delaware, have their own \nlaws affecting what a customer owes, we may need to support additional varia-\ntions of the module. So our variation mechanism should be able to accommodate \nthose possibilities as well. \nThe payoff for this up-front planning is that an asset used in any of the prod-\nucts exists as a single version that (through the exercising of built-in variation \nmechanisms) works for all of the products in the product line. And now, mak-\ning a portfolio-wide change merely consists of changing the core assets that are \naffected. Because all future versions of all products use the same core assets, \nchanging the core asset base has the effect of changing all of the products in the \norganization’s portfolio.\n25.2  What Makes a Software Product Line Work?\nWhat makes product lines succeed is that the commonalities shared by the prod-\nucts can be exploited through reuse to achieve production economies. The poten-\ntial for reuse is broad and far-ranging, including the following: \n■\n■Requirements. Most of the requirements are common with those of earlier \nsystems and so can be reused. In fact, many organizations simply maintain \na single set of requirements that apply across the entire family as a core \nasset; the requirements for a particular system are then written as “delta” \ndocuments off the full set. In any case, most of the effort consumed by re-\nquirements analysis is saved from system to system.\n■\n■Architectural design. An architecture for a software system represents a \nlarge investment of time from the organization’s most talented engineers. \nAs we have seen, the quality goals for a system—performance, reliability, \nmodifiability, and so forth—are largely promoted or inhibited once the \narchitecture is in place. If the architecture is wrong, the system cannot be \n\n\n484 \nPart Four\t\n25—Architecture and Software Product Lines\nsaved. For a new product, however, this most important design step is al-\nready done and need not be repeated.\n■\n■Software elements. Software elements are applicable across individual \nproducts. Element reuse includes the (often difficult) initial design work. \nDesign successes are captured and reused; design dead ends are avoided, \nnot repeated. This includes design of each element’s interface, its docu-\nmentation, its test plans and procedures, and any models (such as perfor-\nmance models) used to predict or measure its behavior. One reusable set of \nelements is the system’s user interface, which represents an enormous and \nvital set of design decisions. And as a result of this interface reuse, products \nin a product line usually enjoy the same look and feel as each other, an ad-\nvantage in the marketplace.\n■\n■Modeling and analysis. Performance models, schedulability analysis, dis-\ntributed system issues (such as proving the absence of deadlock), allocation \nof processes to processors, fault tolerance schemes, and network load poli-\ncies all carry over from product to product. Companies that build real-time \ndistributed systems report that one of the major headaches associated with \nproduction has all but vanished. When they field a new product in their \nproduct line, they have high confidence that the timing problems have been \nworked out and that the bugs associated with distributed computing—\nsynchronization, network loading, and absence of deadlock—have been \neliminated.\n■\n■Testing. Test plans, test processes, test cases, test data, test harnesses, and \nthe communication paths required to report and fix problems are already in \nplace. \n■\n■Project planning artifacts. Budgeting and scheduling are more predictable \nbecause experience is a high-fidelity indicator of future performance. Work \nbreakdown structures need not be invented each time. Teams, team size, \nand team composition are all easily determined.\nAll of these represent valuable core assets, each of which can be imbued \nwith its own variation points that can be exercised to build a product. We’ll look \nat architectural variation points later in this chapter, but for now imagine that any \nartifact represented by text can consist of text blocks that are exposed or hidden \nfor a particular product. Thus, the artifact that is maintained in the core asset base \nrepresents a superset of any version that will be produced for a product.\nArtifact reuse in turn enables reuse of knowledge:\n■\n■Processes, methods, and tools. Configuration control procedures and fa-\ncilities, documentation plans and approval processes, tool environments, \nsystem generation and distribution procedures, coding standards, and many \nother day-to-day engineering support activities can all be carried over from \nproduct to product. The software development process is in place and has \nbeen used before.\n\n\n25.2  What Makes a Software Product Line Work?\n485\nGiving Software Reuse a New Lease on Life\nSoftware product lines rely on reuse, but reuse has a long but less than \nstellar history in software engineering, with the promise almost always \nexceeding the payoff. One reason for this failure is that until now reuse \nhas been predicated on the idea of “If you build it, they will come.” A reuse \nlibrary is stocked with snippets from previous projects, and developers are \nexpected to check it first before coding new elements. Almost everything \nconspires against this model. If the library is too sparse, the developer will \nnot find anything of use and will stop looking. If the library is too rich, it will \nbe hard to understand and search. If the elements are too small, it is eas-\nier to rewrite them than to find them and carry out whatever modifications \nthey might need. If the elements are too large, it is difficult to determine \nexactly what they do in detail, which in any case is not likely to be exactly \nright for the new application. In most reuse libraries, pedigree is hazy at \nbest. The developer cannot be sure exactly what the element does, how \nreliable it is, or under what conditions it was tested. And there is almost \nnever a match between the quality attributes needed for the new applica-\ntion and those provided by the elements in the library.\nIn any case, it is common that the elements were written for a different \narchitectural model than the one the developer of the new system is using. \nEven if you find something that does the right thing with the right quality \nattributes, it is doubtful that it will be the right kind of architectural element \n(if you need an object, you might find a process), that it will have the right \ninteraction protocol, that it will comply with the new application’s error-han-\ndling or failover policies, and so on.\nThis has led to so many reuse failures that many project managers have \ngiven up on the idea. “Bah!” they exclaim. “We tried reuse before, and it \ndoesn’t work!”\nSoftware product lines make reuse work by establishing a strict context for \nit. The architecture is defined; the functionality is set; the quality attributes are \nknown. Nothing is placed in the reuse library—or “core asset base” in product \nline terms—that was not built to be reused in that product line. Product lines \nwork by relying on strategic or planned, not opportunistic, reuse.\n—PCC\n■\n■People. Because of the commonality of applications, personnel can be \ntransferred among projects as required. Their expertise is applicable across \nthe entire line.\n■\n■Exemplar systems. Deployed products serve as high-quality demonstration \nprototypes or engineering models of performance, security, safety, and \nreliability.\n\n\n486 \nPart Four\t\n25—Architecture and Software Product Lines\n■\n■Defect elimination. Product lines enhance quality because each new system \ntakes advantage of the defect elimination in its forebears. Developer and cus-\ntomer confidence both rise with each new instantiation. The more complicat-\ned the system, the higher the payoff for solving vexing performance, distribu-\ntion, reliability, and other engineering issues once for the entire family.\nAll of this reuse helps products launch more quickly, with higher quality, \nlower cost, and more predictable budget and schedule. This is critical for getting \na product to market in a timely fashion. However, these benefits do not come \nfor free. A product line may require a substantial up-front investment of time \nand effort to set up and manage, as well as to keep the core assets responsive to \nchanging market needs.\n25.3  Product Line Scope\nOne of the most important inputs to an architect building an architecture for a \nsoftware product line is the product line’s scope. A product line’s scope is a state-\nment about what systems an organization is willing to build as part of its line \nand what systems it is not willing to build. Defining a product line’s scope is like \ndrawing a doughnut in the space of all possible systems, as shown in Figure 25.1. \nThe doughnut’s center represents the systems that the organization could eas-\nily build using its base of core assets; these are within its production capability. \nSystems outside the doughnut are out of scope because they are ones the product \nline’s core assets are not well equipped to handle; this would be like asking Toy-\nota to build, say, apple pies on one of its automotive assembly lines. \nFigure 25.1  The space of all possible systems is divided into areas within \nscope (white), areas outside of scope (speckled), and areas that require case-by-\ncase disposition (gray).\n\n\n25.3  Product Line Scope\n487\nSystems on the doughnut itself could be handled, but with some effort. \nThese often represent invitations from the marketplace asking the organization \nto extend its product line. To take advantage of such an opportunity, the orga-\nnization would have to broaden its production capability—that is, make its core \nasset base able to handle the new product. These opportunities require case-by-\ncase disposition as they arise, to see if the potential payoff (such as entry into a \nslightly different area of the market) would outweigh the cost to modify the core \nassets. This would be like asking Toyota to build a riding lawnmower.\nThe scope represents the organization’s best prediction about what products \nit will be asked to build in the foreseeable future. Input to the scoping process \ncomes from the organization’s strategic planners, marketing staff, domain ana-\nlysts who can catalog similar systems (both existing and on the drawing board), \nand technology experts.\nA product line scope is a critical factor in the success of the product line. \nScope too narrowly (the products only vary in a small number of features) and \nan insufficient number of products will be derived to justify the investment in \ndevelopment. Scope too broadly (the products vary in kind as well as in features) \nand the effort required to develop individual products from the core assets is too \ngreat to lead to significant savings. Scope can be refined as a portion of the initial \nestablishment of the product line or opportunistically depending on the product \nline adoption strategy (see the section on adoption strategies in Section 25.8).\nThe problem in defining the scope is not in finding commonality—a cre-\native architect can find points of commonality between any two systems—but \nin finding commonality that can be exploited to substantially reduce the cost of \nconstructing the systems that an organization intends to build. When consider-\ning scope, more than just the systems being built should be considered. Market \nsegmentation and types of customer interactions assumed will help determine \nthe scope of any particular product line. For example, Philips, the Dutch manu-\nfacturer of consumer electronics, has distinct product lines for home video elec-\ntronic systems and digital video communication. Video is the common thread, \nbut one is a mass market, where the customer is assumed to have very little video \nsophistication, and the other is a much smaller market consisting purely of video \nprofessionals. The products being developed reflect these assumptions about the \nsophistication of customers and the amount of care each customer will receive. \nThese differences were sufficient to keep Philips from attempting to develop a \nsingle product line for both markets.\nNarrowly scoped product lines offer opportunities to build specialized tools \nto support the specification of new products. For example, General Motors’ Pow-\nertrain division builds a software product line of automotive software. It makes \nan individual product from its product line core assets based on contracts stored \nin a database. Each element has well-defined interfaces and possible variation \npoints. A tool searches the database based on desired features and assembles the \nproduct. \n\n\n488 \nPart Four\t\n25—Architecture and Software Product Lines\nThe scope definition is vital to the product line architect because the scope \ndefines what is common across all members of the product line, and the specific \nways in which the products differ from each other. The fixed part of a product \nline architecture reflects what is constant, and the architecture’s variation points \naccommodate the variations among products. \n25.4  The Quality Attribute of Variability\nScoping decisions, which tell the product line architect what kinds of systems are \n“in” and what kinds of systems are “out” of the product line, lead to the introduc-\ntion of variability in the core assets. In fact, the quality attribute of variability is \nmost closely associated with product lines. Some may feature high-performance \nproducts, or high-security products, or high-availability products, but all prod-\nuct lines feature variability aimed at satisfying the commonalities and variations \nidentified by the product line’s scope.\nWe introduced variability in Chapter 12. There we said that variability is a \nspecial form of modifiability, pertaining to the ability of a core asset to adapt to \nusages in the different product contexts that are within the product line scope. \nThe goal of variability in a software product line is to make it easy to build and \nmaintain products in the product line over time. \nTable 25.1 gives the general scenario for variability. The source is some actor in \nthe product line organization who identifies a need for variation; this actor is proba-\nbly someone involved in setting the product line’s scope, such as a marketer.\nIdentifying variation is a constant, iterative process in the life of a software \nproduct line. Because of the many different ways a product can vary, particu-\nlar variants can be identified at virtually any time during the development pro-\ncess. Some variations are identified during product line requirement elicitations; \nothers, during architecture design; and still others, during implementation. Vari-\nations may also be identified during implementation of the second (and subse-\nquent) products as well.\nProduct line architectures feature variability as an important quality attri-\nbute. They achieve this by incorporation of variation mechanisms, which we will \ndiscuss in more detail shortly.\n25.5  The Role of a Product Line Architecture\nOf all of the assets in a core asset repository, the software architecture plays the \nmost central role. There is both a tactical and a strategic reason for this.\n\n\n25.5  The Role of a Product Line Architecture\n489\nTable 25.1  The General Scenario for Variability\nPortion of Scenario\nPossible Values\nSource \nActor requesting variability\nStimulus\nRequests to support variations in the following:\n■\n■\nHardware\n■\n■\nFeature sets\n■\n■\nTechnologies\n■\n■\nUser interfaces\n■\n■\nQuality attributes\n■\n■\n. . . and more\nfor the range of products affected, such as:\n■\n■\nAll\n■\n■\nA specified subset\n■\n■\nThose that include feature set x\n■\n■\nNew products\nEnvironment\nVariants are to be created at:\n■\n■\nRuntime\n■\n■\nBuild time\n■\n■\nDevelopment time\nArtifact\nAsset(s) affected, such as:\n■\n■\nRequirements \n■\n■\nArchitecture\n■\n■\nComponent x\n■\n■\nTest suite y\n■\n■\nProject plan z\n■\n■\n. . . and more\nResponse\nThe requested variants can be created.\nResponse measure\nA specified cost and/or time to create the core assets and \nto create the variants using these core assets\nThe tactical reason is the importance the architecture plays in building prod-\nucts in a product line. The essence of building a successful software product line \nis discriminating between what is expected to remain constant across all family \nmembers and what is expected to vary. Software architecture is ideal for handling \nthis variation, because all architectures are abstractions that admit multiple in-\nstances. By its very nature every architecture is a statement about what we expect \nto remain constant and what we admit may vary. For example, interfaces to com-\nponents are designed to remain stable, with anticipated changes hidden behind \nthose interfaces.\nIn a software product line, the architecture has to encompass both the \nvarying and the nonvarying aspects. A product line architecture must be de-\nsigned to accommodate a set of explicitly allowed variations. Thus, identifying \n\n\n490 \nPart Four\t\n25—Architecture and Software Product Lines\nthe allowable variations is part of the architect’s responsibility, as is providing \nbuilt-in mechanisms for achieving them. Those variations may be substantial. \nProducts in a software product line exist simultaneously and may vary in terms \nof their behavior, quality attributes, platform, network, physical configuration, \nmiddleware, scale factors, and so forth.\nThe strategic reason has to do with the capability it imparts to an organiza-\ntion outside the realm of an existing product line. As we saw in Chapters 2 and 3, \nan architecture can serve as a technical platform for launching new applications \nand even new business models, and it can serve as a springboard for an organiza-\ntion diving into a new business area. This seems to be especially true for product \nline architectures. There are many cases where an organization has taken advan-\ntage of its production capability—that is, its core asset base crowned by a product \nline architecture—by using that capability to enter new markets. For example, \nCummins took its product line of automotive diesel engines to enter and quickly \ndominate the neighboring market for industrial diesel engines. Industrial diesel \nengines power things like rock crushers and ski lifts, markets of low volume and \nhigh specialization. Systems in that market built uniquely for each application are \nexpensive and don’t yield a high return. But a product line that includes industrial \ndiesel engines in its scope, and whose production capability supports industrial \ndiesel engines, is a recipe for rapid market capture.\nA product line architect needs to consider three things that are unique to \nproduct line architectures:\n■\n■Identifying variation points. This is done by using the scope definition and \nproduct line requirements as input. The product line architect determines \nwhere in the architecture variation points should be made available to sup-\nport the rapid building of products.\n■\n■Supporting variation points. This is done by introducing variation mecha-\nnisms, which will be discussed in the next section.\n■\n■Evaluating the architecture for product line suitability, which will be dis-\ncussed later in this chapter.\n25.6  Variation Mechanisms\nIn a conventional architecture, the mechanism for achieving different instances \noften comes down to modifying the code. But in a software product line, modify-\ning code is undesirable, because this leads to a large number of separately main-\ntained implementations that quickly outstrip an organization’s ability to keep \nthem up to date and consistent. \nThree primary architectural variation mechanisms are these:\n\n\n25.6  Variation Mechanisms\n491\n■\n■Inclusion or omission of elements. This decision can be reflected in the \nbuild procedures for different products, or the implementation of an ele-\nment can be conditionally compiled based on some parameter indicating its \npresence or absence.\n■\n■Inclusion of a different number of replicated elements. For instance, \nhigh-capacity variants might be produced by adding more servers—the ac-\ntual number should be unspecified, as a point of variation, and may be done \ndynamically.\n■\n■Selection of different versions of elements that have the same interface but \ndifferent behavioral or quality attribute characteristics. Selection can occur \nat compile time, build time, or runtime. Selection mechanisms include stat-\nic libraries, which contain external functions linked after compilation time; \ndynamic link libraries, which have the flexibility of static libraries but defer \nthe decision until runtime based on context and execution conditions; and \nadd-ons (e.g., plug-ins, extensions, and themes), which add or modify ap-\nplication functionality at runtime. By changing the libraries, we can change \nthe implementation of functions whose names and signatures are known. \nSome variation mechanisms can be introduced that change aspects of a par-\nticular software element. Modifying the source code each time the element is \nused in a new product—that is, clone-and-own—falls into this category, although \nit is undesirable. More sophisticated techniques include the following:\n■\n■Extension points. These are identified places in the architecture where addi-\ntional behavior or functionality can be safely added.\n■\n■Reflection. This is the ability of a program to manipulate data on itself or its \nexecution environment or state. Reflective programs can adjust their behav-\nior based on their context.\n■\n■Overloading. This is a means of reusing a named functionality to operate \non different types. Overloading promotes code reuse, but at the cost of un-\nderstandability and code complexity.\nOther commonly used variation mechanisms include those in Table 25.2. \nChoosing the right variation mechanism affects numerous costs:\n■\n■The skill set required to implement, or learn and use, the specific variation \nmechanism, such as server or framework programming\n■\n■The one-time costs of building or acquiring the tools (such as compilers or \ngenerators) required to create the variation mechanism \n■\n■The recurring cost and time to exercise the variation mechanism \nThe choice of variation mechanism also affects downstream users and \ndevelopers:\n■\n■The targeted group of users that use the mechanism for product-specific \nadaptation, such as product developer, integrator, system administrator, and \nend user\n",
      "page_number": 495
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 512-520)",
      "start_page": 512,
      "end_page": 520,
      "detection_method": "topic_boundary",
      "content": "492 \nPart Four\t\n25—Architecture and Software Product Lines\nFinally, the choice of variation mechanism affects product quality:\n■\n■The impact of the variation mechanism on quality, such as possible perfor-\nmance penalties or memory consumption\n■\n■The impact on the mechanism’s maintainability\nThe architect should document the choice of variation mechanisms. In fact, \nthe documentation of variation mechanisms is the primary way in which the doc-\numentation for a product line architecture differs from that of a conventional ar-\nchitecture. In the documentation template we presented in Chapter 18, the section\nTable 25.2  Common Variation Mechanisms\nVariation \nMechanism\nProperties Relevant to \nBuilding the Core Assets\nProperties Relevant to Exercising  \nthe Variation Mechanism When \nBuilding Products\nInheritance; \nspecializing or \ngeneralizing a \nparticular class\nCost: Medium\nSkills: Object-oriented \nlanguages\nStakeholder: Product developers\nTools: Compiler\nCost: Medium\nComponent \nsubstitution\nCost: Medium\nSkills: Interface definitions\nStakeholder: Product developer, system \nadministrator\nTools: Compiler\nCost: Low\nAdd-ons, plug-\nins\nCost: High\nSkills: Framework \nprogramming\nStakeholder: End user\nTools: None\nCost: Low\nTemplates\nCost: Medium\nSkills: Abstractions\nStakeholder: Product developer, system \nadministrator\nTools: None\nCost: Medium\nParameters \n(including text \npreprocessors)\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: Product developer, system \nadministrator, end user\nTools: None\nCost: Low\nGenerator\nCost: High\nSkills: Generative \nprogramming\nStakeholder: System administrator, end \nuser\nTools: Generator\nCost: Low\nAspects\nCost: Medium\nSkills: Aspect-oriented \nprogramming\nStakeholder: Product developer\nTools: Aspect-oriented language \ncompiler\nCost: Medium\nRuntime \nconditionals\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: None\nTools: None\nCost: No development cost; some \nperformance cost\nConfigurator\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: Product developer\nTools: Configurator\nCost: Low to medium\n\n\n25.7  Evaluating a Product Line Architecture\n493\ncalled the variability guide is reserved for exactly this purpose. The variability \nguide should describe each variation mechanism, how and when to exercise it, \nand what allowed variations it supports. The architecture documentation should \nalso describe the architecture’s instantiation process—that is, how its variation \npoints are exercised. Also, if certain combinations of variations are disallowed, \nthen the documentation needs to explain valid and invalid variation choices.\n25.7  Evaluating a Product Line Architecture\nLike any other, the architecture for a software product line should be evaluated \nfor fitness of purpose. The architecture should be evaluated for its robustness and \ngenerality, to make sure it can serve as the basis for products in the product line’s \nenvisioned scope. It should also be evaluated to make sure it meets the specific \nbehavioral and quality requirements of the product at hand. We begin by focusing \non the what and how of the evaluation and then turn to when it should take place.\nWhat and How to Evaluate. The evaluation will have to focus on the vari-\nation points to make sure they are appropriate, that they offer sufficient flexibility \nto cover the product line’s intended scope, that they allow products to be built \nquickly, and that they do not impose unacceptable runtime performance costs. If \nyour evaluation is scenario based, expect to elicit scenarios that involve instanti-\nating the architecture to support different products in the family. Also, different \nproducts in the product line may have different quality attribute requirements, \nand the architecture will have to be evaluated for its ability to provide all required \ncombinations. Here again, try to elicit scenarios that capture the quality attributes \nrequired of family members.\nOften, some of the hardware and other performance-affecting factors for \na product line architecture are unknown to begin with. In this case, evaluation \ncan establish bounds on the performance that the architecture is able to achieve, \nassuming bounds on hardware and other variables. The evaluation can identify \npotential contention so that you can put in place the policies and strategies to \nresolve it.\nWhen to Evaluate. An evaluation should be performed on an instance or vari-\nation of the architecture that will be used to build one or more products in the prod-\nuct line. The extent to which this is a separate, dedicated evaluation depends on the \nextent to which the product’s requirements differ from the product line architecture \nenvelope. If it does not differ, the product architecture evaluation can be abbreviated, \nbecause many of the issues normally raised in a single product evaluation will have \nbeen dealt with in the product line evaluation. In fact, just as the product architecture \nis a variation of the product line architecture, the product architecture evaluation is \na variation of the product line architecture evaluation. Therefore, depending on the \nevaluation method used, the evaluation artifacts (scenarios, checklists, and so on) \n\n\n494 \nPart Four\t\n25—Architecture and Software Product Lines\nwill have reuse potential, and you should create them with that in mind. The results \nof evaluation of product architectures often provide useful feedback to the product \nline architects and fuel architectural improvements.\nWhen a new product is proposed that falls outside the scope of the original \nproduct line (for which the architecture was presumably evaluated), the product \nline architecture can be reevaluated to see if it will suffice for it. If it does, the \nproduct line’s scope can be expanded to include the new product, or to spawn a \nnew product line. If it does not, the evaluation can determine how the architecture \nwill have to be modified to accommodate the new product. The product line and \nproduct instance architectures can be evaluated not only to determine architec-\ntural risks but also to understand economic consequences (see Chapter 23), to \ndetermine which products will yield the most return.\n25.8  Key Software Product Line Issues\nIt takes considerable maturity in the developing organization to successfully field \na product line. Technology is not the only barrier to this; organization, process, \nand business issues are equally vital to master to fully reap the benefits of the \nsoftware product line approach.\nArchitecture definition is an important activity for any project, but as we saw \nin the previous section, it needs to emphasize variation points in a software product \nline. Configuration management is also an important activity for any project, but it is \nmore complex for a software product line because each product is the result of bind-\ning a large number of variations. The configuration management problem for prod-\nuct lines is to reproduce any version of any product delivered to any customer, where \n“product” means code and supporting artifacts ranging from requirement specs and \ntest cases to user manuals and installation guides. This involves knowing what ver-\nsion of each core asset was used in a product’s construction, how every asset was \ntailored, and what special-purpose code or documentation was added.\nExamining every facet of launching a product line and institutionalizing a \nproduct line culture is outside the scope of this book, but the next sections will \nexamine a few of the key areas that must be addressed. These are issues that an \norganization will have to face when considering whether to adopt a product line \napproach for software development and, if so, how to go about it.\nAdoption Strategies\nAn organization’s culture and context will dramatically affect how it goes about \nadopting a product line approach. Here are some of the important organizational \nand process factors that we have seen in practice.\n\n\n25.8  Key Software Product Line Issues\n495\nTop-Down vs. Bottom-Up.  Top-down adoption arises when a (typically \nhigh level) manager decrees that the organization will use the approach. The \nproblem is to get employees in the trenches to change the way they work. Bot-\ntom-up adoption happens when designers and developers working at the product \nlevel realize that they are needlessly duplicating each other’s work and begin to \nshare resources and develop generic core assets. The problem is finding a man-\nager willing to sponsor the work and spread the technique to other parts of the \norganization. Both approaches work; both are helped enormously by the presence \nof a strong champion—someone who has thoroughly internalized the product \nline vision and can share that compelling vision with others. (It works better if \nthe champion is in a position of some authority.)\nProactive vs. Reactive.  There are two primary models for how an organiza-\ntion may grow a product line:\n■\n■In a proactive product line, an organization defines the family using a com-\nprehensive definition of scope. They do this not with a crystal ball but by \ntaking advantage of their experience in the application area, their knowledge \nabout the market and technology trends, and their good business sense. \nThe proactive model allows the organization to make the most far-reaching \nstrategic decisions. Explicitly scoping the product line allows you to look \nat areas that are underrepresented by products already in the marketplace, \nmake small extensions to the product line, and move quickly to fill the gap. In \nshort, proactive product line scope allows an organization to take charge of its \nown fate. Sometimes an organization does not have the ability to forecast the \nneeds of the market with the certainty suggested by the proactive model. The \nproactive model also takes some time to define and implement, and in that \ntime the organization needs to continue to construct products.\n■\n■In a reactive product line, an organization builds the next member or mem-\nbers of the product family from earlier products. This is best used when \nthere is uncertainty of requirements. Perhaps the domain is a new one. \nPerhaps the market is in flux. Or perhaps the organization cannot afford to \nbuild a core asset base that will cover the entire scope all at once. In the \nreactive model, with each new product the architecture is extended as need-\ned and the core asset base is built up from what has turned out to be com-\nmon. The reactive model puts much less emphasis on up-front planning and \nstrategic direction setting. Rather, the organization lets itself be taken where \nthe market dictates. This is an example of agile architecting, as described in \nChapter 15.\nIncremental vs. Big Bang.  If you are proactively building a product line, \nyou still need to choose how to populate it: all at once or incrementally over time. \nPopulating the core asset base all at once is a strategy that has worked success-\nfully for some organizations. However, it tends to require all or nearly all of the \n\n\n496 \nPart Four\t\n25—Architecture and Software Product Lines\norganization’s resources be focused on that task, at the expense of new product \nproduction. A different approach is to populate the core asset base incrementally, \nas circumstances and resources permit. Each product that goes out the door is \nbuilt with whatever core assets are available at the time. That means that early \nproducts will include software not derived from core assets. But those products \nwill still be better off (that is, faster to market, of higher quality, and easier to \nmaintain) than products built entirely from unique code. And it’s entirely pos-\nsible that some of the software unique to those early products can be extracted, \nadapted, and generalized to become core assets themselves, thus helping populate \nthe core asset base in a reactive fashion.\nKnowing the various adoption models can help an organization choose the \none that is right for it. For example, the proactive model requires a heavier initial \ninvestment but less rework than the reactive model. The reactive model relies \nexclusively on rework with little initial investment. Which model should act as a \nguide for a particular organization depends on the business situation.\nCreating Products and Evolving a Product Line\nAn organization that has a product line will have an architecture and a collection \nof elements associated with it. From time to time, the organization will create a \nnew member of the product line that will have features both in common with and \ndifferent from those of other members.\nOne problem associated with a product line is managing its evolution. As \ntime passes, the line—or, more precisely, the set of core assets from which prod-\nucts are built—must evolve. That evolution will be driven by both external and \ninternal sources:\nExternal sources\n■\n■New versions of existing elements within the line will be released by their \nvendors, and future products will need to be constructed from them. \n■\n■New externally created elements may be added to the line. Thus, for ex-\nample, functions that were previously performed by internally developed \nelements may now be performed by elements acquired externally, or vice \nversa. Or future products will need to take advantage of new technology, as \nembodied in externally developed elements.\n■\n■New features may be added to the product line to keep it responsive to user \nneeds or competitive pressures.\nInternal sources\n■\n■Some entity within the organization must determine if new functions add-\ned to a product are within the product line’s scope. If so, they can simply \nbe built from the asset base. If not, a decision must be made: either the \nenhanced product spins off from the product line, following its own evolu-\ntionary path, or the asset base must be expanded to include it. Updating the \n\n\n25.9  Summary\n497\nline may be the wisest choice if the new functionality is likely to be used in \nfuture products, but this capability comes at the cost of the time necessary \nto update the core assets.\n■\n■An organization may wish to replace old products with ones built from the \nmost up-to-date version of the asset base. Keeping products compatible \nwith the product line takes time and effort. But not doing so may make fu-\nture upgrades more time consuming, because either the product will need to \nbe brought into compliance with the latest product line elements or it will \nnot be able to take advantage of improvements in the line.\nOrganizational Structure\nAn asset base on which products depend, but which has its own evolutionary \npath (perhaps driven by technology change), requires an organization to decide \nhow to manage both it and product development. There are two main organiza-\ntional strategies from which to choose, plus a number of minor variations. The \ntwo main structures reflect different answers to the question “Shall we have a \ndedicated group whose sole job is to build and maintain our core asset base?”\n1.\t\nWe’re all in this together. In this scheme, there is no separate core asset \ngroup. The product-building development teams coordinate closely, and \ndivide up the core asset responsibilities among themselves. That is, Product \nTeam 1 might be assigned responsibility for the development and mainte-\nnance of Core Assets 3, 6, 9, 12, and 15; Product Team 2 might take Core \nAssets 1, 4, and 8; and so forth. This works well enough for small organi-\nzations, but as size grows the communication channels become untenable. \nAlso, each team has to resist the temptation to build core assets that are \nespecially appropriate to its needs, but less so to other teams’ needs.\n2.\t\nSeparate core asset unit. In this scheme, a special unit is given responsibility \nfor the development and maintenance of the core asset base. Separate devel-\nopment teams in the organization’s business units build the products. In this \nscheme, the core asset unit (sometimes called a domain engineering unit) \nassumes the responsibility for the overall strategic direction of the product \nline. To the product teams, they appear almost like an external supplier. The \nproduct teams coordinate among themselves to set the core asset team’s de-\nvelopment and test priorities, based on product delivery obligations. \n25.9  Summary\nThis chapter presented an architecture-based development paradigm known \nas software product lines. The product line approach is steadily climbing in \n\n\n498 \nPart Four\t\n25—Architecture and Software Product Lines\npopularity as more organizations see true order-of-magnitude improvements in \ncost, schedule, and quality from using it.\nLike all technologies, however, this one holds some surprises for the un-\naware. Architecturally, the key is identifying and managing commonalities and \nvariations, but nontechnical issues must be addressed as well, including how \nthe organization adopts the model, structures itself, and maintains its external \ninterfaces.\n25.10  For Further Reading\n[Clements 01a] is a comprehensive treatment of software product lines. It in-\ncludes a number of case studies as well as a thorough discussion of product line \n“practice areas,” which are areas of expertise a product line organization should \nhave (or should develop) to help bring about product line success. \n[van der Linden 07] contains a rich set of product line case studies.\n[Anastasopoulos 00] presents a good list of variation mechanisms, as do [Ja-\ncobson 97] and [Svahnberg 00]. [Bachmann 05] provides a list of their own, as \nwell as a treatment of each in terms of cost (it was the source for Table 25.2). \nOrganizational models for software product lines are treated in [Bosch 00]. \nThere is an active software product line community of research and practice. \nThe Software Product Line Conference (SPLC) is the mainstream forum for new \nsoftware product line research and success stories. You can find it at www.splc.\nnet. SPLC maintains a “Software Product Line Hall of Fame,” which showcases \nsuccessful software product lines that can serve as engineering models (and in-\nspiration) to aspiring product line organizations. Each year, new members of the \nHall of Fame are nominated, and in most years a new candidate is inducted. You \ncan see the winners at www.splc.net/fame.html.\nThe SEI’s website contains a wealth of material about software product \nlines, including a collection of “getting started” material: www.sei.cmu.edu/\nproductlines.\n25.11  Discussion Questions\n1.\t\nVariability is achieved by adding variation mechanisms to a system. Vari-\nation mechanisms include inheritance, component substitution, plug-ins, \ntemplates, parameters (including text preprocessors), generators, aspects, \nruntime conditionals, and a configurator tool. Because variability can be \nseen as a kind of modifiability, see if you can map each of these variation \nmechanisms to one or more modifiability tactics given in Chapter 7.\n\n\n25.11  Discussion Questions\n499\n2.\t\nSuppose a company builds two similar systems using a large set of common \nassets, including an architecture. Which of the following would you say \nconstitutes a product line?\n■\n■Sharing only an architecture but no elements. \n■\n■Sharing only a single element. \n■\n■Sharing the same operating system and programming language runtime \nlibraries. \n■\n■Sharing the same team of developers.\nDefend your answer.\n3.\t\nPick a type of system you’re familiar with—for example, an automobile or \na smartphone. Think of three instances of that kind of system. Make a list \nof all of the things the three instances have in common. Now make a list of \nall of the things that distinguish the three instances from each other (that is, \ntheir variation points). If automobiles turn out to be too complex, start with \na simpler kind of “system,” such as an electric light.\n4.\t\nWrite some concrete scenarios to express the variability you identified in \nthe previous question.\n5.\t\nDo the list of variation mechanisms in this chapter constitute tactics for \nvariability? Discuss.\n6.\t\nIn many software product lines, products differ by the quality attributes \nthey exhibit. For instance, a company might sell a cheap, low-security ver-\nsion of its product alongside a more expensive, high-security version of the \nsame product. Which variation mechanisms might you choose to achieve \nthis kind of variability?\n\n\nThis page intentionally left blank \n",
      "page_number": 512
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 521-528)",
      "start_page": 521,
      "end_page": 528,
      "detection_method": "topic_boundary",
      "content": "501\nPart  FIVE\nThe Brave New World\nParts I through IV of this book have dealt with the technical, organizational, and \nbusiness perspectives on software architecture. In this part, we turn our atten-\ntion to emerging technologies. We have often been asked whether principles or \ntechnology is more important, and the answer, of course, is “Yes, they are both \nimportant.” Principles have a long lifetime; technology that affects architects \ntends to change every decade or so. In this part, we provide brief introductions \nto two technologies that we believe will last and have a significant impact on \narchitects—the cloud and the edge. We also discuss one of the continuing prob-\nlems of many architects: How do I get my organization to embrace architectural \nprinciples?\nThe cloud provides you with the option of outsourcing your data center. The \nvision is that computing resources are available to an application as electricity \nis available to a consumer. That is, one plugs in an appliance and electricity is \navailable. In data center terms, you hook your web browser up to an application \nand computation power is available. All of the capacity, management, and opera-\ntional issues of a data center are taken care of by a third party, and all you, as an \narchitect, need to do is to utilize the resources you need. This trend has been ac-\ncompanied by a vast expansion of the amount of data that organizations manage. \nGoogle, Yahoo!, Facebook, and the other web giants all must manage petabytes \nof data. In Chapter 26, we provide a brief introduction to the technologies associ-\nated with the cloud and with managing these vast amounts of data.\nCloud computing is associated with the world of social networks and open \nsource. The term “edge-periphery” is used to describe both the crowdsource and \nthe open source movements. The term refers simultaneously to crowdsourced \n\n\n502\nsystems such as Facebook and Wikipedia and open source systems such as the \nApache Web Server and Hadoop. In Chapter 27, we describe this phenomenon \nand explore some of the architectural implications of this aspect of the brave new \nworld.\nWe end by discussing “adoption.” It describes an approach to dealing with \nthe following problem: “OK, you guys have convinced me. Now I need to con-\nvince my organization of the importance of architectural principles. How do I do \nthat?”\n\n\n503\n26\nArchitecture in the \nCloud\nThere was a time when every household, town, farm \nor village had its own water well. Today, shared public \nutilities give us access to clean water by simply turning \non the tap; cloud computing works in a similar fashion.\n—Vivek Kundra\nIf you have read anything about the history of computing, you will have read \nabout time-sharing. This was the era, in the late 1960s and the 1970s, sandwiched \nbetween eras when individuals had sole, although limited, access to multimil-\nlion-dollar computers and when individuals had access to their own personal \ncomputers. Time-sharing involved multiple users (maybe as many as several \nhundred) simultaneously accessing a powerful mainframe computer through a \nterminal, potentially remote from the mainframe. The operating system on the \nmainframe made it appear as if each user had sole access to that computer except, \npossibly, for performance considerations. The driving force behind the develop-\nment of time-sharing was economic; it was infeasible to provide every user with \na multimillion-dollar computer, but efficiently sharing this expensive but power-\nful resource was the solution.\nIn some ways, cloud computing is a re-creation of that era. In fact, some \nof the basic techniques—such as virtualization—that are used in the cloud to-\nday date from that period. Any user of an application in the cloud does not need \nto know that the application and the data it uses are situated several time zones \naway, and that thousands of other users are sharing it. Of course, with the advent \nof the Internet, the availability of much more powerful computers today, and the \nrequirement for controlled sharing, designing the architecture for a cloud-based \napplication is much different from designing the architecture for a time-shar-\ning-based application. The driving forces, however, remain much the same. The \n\n\n504 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\neconomics of using the cloud as a deployment platform are so compelling that \nfew organizations today can afford to ignore this set of technologies.\nIn this chapter we introduce cloud concepts, and we discuss various ser-\nvice models and deployment options for the cloud, the economic justification \nfor the cloud, the base architectures and mechanisms that make the cloud work, \nand some sample technologies. We will conclude by discussing how an architect \nshould approach building a system in the cloud.\n26.1  Basic Cloud Definitions\nThe essential characteristics of cloud computing (based, in part, on definitions \nprovided by the U.S. National Institute of Standards and Technology, or NIST) \nare the following:\n1.\t\nOn-demand self-service. A resource consumer can unilaterally provision \ncomputing services, such as server time and network storage, as needed \nautomatically without requiring human interaction with each service’s pro-\nvider. This is sometimes called empowerment of end users of computing \nresources. Examples of resources include storage, processing, memory, net-\nwork bandwidth, and virtual machines.\n2.\t\nUbiquitous network access. Cloud services and resources are available over \nthe network and accessed through standard networking mechanisms that \npromote use by a heterogeneous collection of clients. For example, you can \neffectively run large applications on small platforms such as smart phones, \nlaptops, and tablets  by running the resource-intensive portion of those \napplications on the cloud. This capability is independent of location and \ndevice; all you need is a client and the Internet.\n3.\t\nResource pooling. The cloud provider’s computing resources are pooled. \nIn this way they can efficiently serve multiple consumers. The provider can \ndynamically assign physical and virtual resources to consumers, according \nto their instantaneous demands. \n4.\t\nLocation independence. The location independence provided by ubiquitous \nnetwork access is generally a good thing. It does, however, have one poten-\ntial drawback. The consumer generally has less control over, or knowledge \nof, the location of the provided resources than in a traditional implementa-\ntion. This can have drawbacks for data latency. The consumer may be able \nto ameliorate this drawback by specifying abstract location information \n(e.g., country, state, or data center). \n5.\t\nRapid elasticity. Due to resource pooling, it is easy for capabilities to be \nrapidly and elastically provisioned, in some cases automatically, to quickly \nscale out or in. To the consumer, the capabilities available for provisioning \noften appear to be virtually unlimited.\n\n\n26.2  Service Models and Deployment Options\n505\n6.\t\nMeasured service. Cloud systems automatically control and optimize re-\nsource use by leveraging a metering capability for the chosen service (e.g., \nstorage, processing, bandwidth, and user accounts). Resource usage can be \nmonitored, controlled, and reported so that consumers of the services are \nbilled only for what they use.\n7.\t\nMulti-tenancy. Multi-tenancy is the use of a single application that is re-\nsponsible for supporting distinct classes or users. Each class or user has its \nown set of data and access rights, and different users or classes of users are \nkept distinct by the application.\n26.2  Service Models and Deployment Options\nIn this section we discuss more terminology and basic concepts. First we discuss \nthe most important models for a consumer using the cloud.\nCloud Service Models\nSoftware as a Service (SaaS).  The consumer in this case is an end user. \nThe consumer uses applications that happen to be running on a cloud. The ap-\nplications can be as varied as email, calendars, video streaming, and real-time \ncollaboration. The consumer does not manage or control the underlying cloud \ninfrastructure, including network, servers, operating systems, storage, or even in-\ndividual application capabilities, with the possible exception of limited user-spe-\ncific application configuration settings.\nPlatform as a Service (PaaS).  The consumer in this case is a developer or \nsystem administrator. The platform provides a variety of services that the con-\nsumer may choose to use. These services can include various database options, \nload-balancing options, availability options, and development environments. The \nconsumer deploys applications onto the cloud infrastructure using programming \nlanguages and tools supported by the provider. The consumer does not manage or \ncontrol the underlying cloud infrastructure, including network, servers, operating \nsystems, or storage, but has control over the deployed applications and possibly \napplication hosting environment configurations. Some levels of quality attributes \n(e.g., uptime, response time, security, fault correction time) may be specified by \nservice-level agreements (SLAs).\nInfrastructure as a Service (IaaS).  The consumer in this case is a de-\nveloper or system administrator. The capability provided to the consumer is to \nprovision processing, storage, networks, and other fundamental computing re-\nsources where the consumer is able to deploy and run arbitrary software, which \n\n\n506 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\ncan include operating systems and applications. The consumer can, for example, \nchoose to create an instance of a virtual computer and provision it with some \nspecific version of Linux. The consumer does not manage or control the underly-\ning cloud infrastructure but has control over operating systems, storage, deployed \napplications, and possibly limited control of select networking components (e.g., \nhost firewalls). Again, SLAs are often used to specify key quality attributes.\nDeployment Models\nThe various deployment models for the cloud are differentiated by who owns and \noperates the cloud. It is possible that a cloud is owned by one party and operated \nby a different party, but we will ignore that distinction and assume that the owner \nof the cloud also operates the cloud.\nThere are two basic models and then two additional variants of these. The \ntwo basic models are private cloud and public cloud:\n■\n■Private cloud. The cloud infrastructure is owned solely by a single organi-\nzation and operated solely for applications owned by that organization. The \nprimary purpose of the organization is not the selling of cloud services.\n■\n■Public cloud. The cloud infrastructure is made available to the general pub-\nlic or a large industry group and is owned by an organization selling cloud \nservices.\nThe two variants are community cloud and hybrid cloud:\n■\n■Community cloud. The cloud infrastructure is shared by several organiza-\ntions and supports a specific community that has shared concerns (e.g., mis-\nsion, security requirements, policy, and compliance considerations). \n■\n■Hybrid cloud. The cloud infrastructure is a composition of two or more \nclouds (private, community, or public) that remain unique entities. The con-\nsumer will deploy applications onto some combination of the constituent \ncloud. An example is an organization that utilizes a private cloud except for \nperiods when spikes in load lead to servicing some requests from a public \ncloud. Such a technique is called “cloud bursting.” \n26.3  Economic Justification\nIn this section we discuss three economic distinctions between (cloud) data cen-\nters based on their size and the technology that they use: \n1.\t\nEconomies of scale\n\n\n26.3  Economic Justification\n507\n2.\t\nUtilization of equipment\n3.\t\nMulti-tenancy\nThe aggregated savings of the three items we discuss may be as large as 80 \npercent for a 100,000-server data center compared to a 10,000-server data center. \nEconomic considerations have made almost all startups deploy into the cloud. \nMany larger enterprises deploy a portion of their applications into the cloud, and \nalmost every enterprise with substantial computation needs at least considers the \ncloud as a deployment platform. \nEconomies of Scale\nLarge data centers are inherently less expensive to operate per unit measure, such \nas cost per gigabyte, than smaller data centers. Large data centers may have hun-\ndreds of thousands of servers. Smaller data centers have servers numbered in the \nthousands or maybe even the hundreds. The cost of maintaining a data center \ndepends on four factors: \n1.\t\nCost of power. The cost of electricity to operate a data center currently is \n15 to 20 percent of the total cost of operation. The per-server power usage \ntends to be significantly lower in large data centers than in smaller ones \nbecause of the ability to share items such as racks and switches. In addi-\ntion, large power users can negotiate significant discounts (as much as 50 \npercent) compared to the retail rates that operators of small data centers \nmust pay. Some areas of the United States provide power at significantly \nlower rates than the national average, and large data centers can be located \nin those areas. Finally, organizations such as Google are buying or building \ninnovative and cheaper power sources, such as on- and offshore wind farms \nand rooftop solar energy.\n2.\t\nInfrastructure labor costs. Large data centers can afford to automate many \nof the repetitive management tasks that are performed manually in smaller \ndata centers. In a traditional data center, an administrator can service ap-\nproximately 140 servers, whereas in a cloud data center, the same adminis-\ntrator can service thousands of servers.\n3.\t\nSecurity and reliability. Maintaining a given level of security, redundancy, \nand disaster recovery essentially requires a fixed level of investment. Larger \ndata centers can amortize that investment over their larger number of serv-\ners and, consequently, the cost per server will be lower. \n4.\t\nHardware costs. Operators of large data centers can get discounts on hard-\nware purchases of up to 30 percent over smaller buyers. \nThese economies of scale depend only on the size of the data center and \ndo not depend on the deployment model being used. Operators of public clouds \n\n\n508 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nhave priced their offerings so that many of the cost savings are passed on to their \nconsumers. \nUtilization of Equipment\nCommon practice in nonvirtualized data centers is to run one application per \nserver. This is caused by the dependency of many enterprise applications on par-\nticular operating systems or even particular versions of these operating systems. \nOne result of the restriction of one application per server is extremely low utili-\nzation of the servers. Figures of 10 to 15 percent utilization for servers are quoted \nby several different vendors. \nUse of virtualization technology, described in Section 26.4, allows for easy \nco-location of distinct applications and their associated operating systems on the \nsame server hardware. The effect of this co-location is to increase the utilization \nof servers. Furthermore, variations in workload can be managed to further in-\ncrease the utilization. We look at five different sources of variation and discuss \nhow they might affect the utilization of servers:\n1.\t\nRandom access. End users may access applications randomly. For example, \nthe checking of email is for some people continuous and for others time-\nboxed into a particular time period. The more users that can be supported \non a single server, the more likely that the randomness of their accesses will \nend up imposing a uniform load on the server.\n2.\t\nTime of day. Those services that are workplace related, unsurprisingly, tend \nto be more heavily used during the work day. Those that are consumer re-\nlated tend to be heavily used during evening hours. Co-locating different \nservices with different time-of-day usage patterns will increase the overall \nutilization of a server. Furthermore, time differences among geographically \ndistinct locations will also affect utilization patterns and can be considered \nwhen planning deployment schedules.\n3.\t\nTime of year. Some applications respond to dates as well as time of day. \nConsumer sites will see increases during the Christmas shopping season, \nand floral sites will see increases around Valentine’s Day and Mother’s Day. \nTax preparation software will see increases around the tax return submis-\nsion due date. Again, these variations in utilization are predictable and can \nbe considered when planning deployment schedules.\n4.\t\nResource usage patterns. Not all applications use resources in the same \nfashion. Search, for example, is heavier in its usage of CPU than email but \nlighter in its use of storage. Co-locating applications with complementary \nresource usage patterns will increase the overall utilization of resources. \n5.\t\nUncertainty. Organizations must maintain sufficient capacity to support \nspikes in usage. Such spikes can be caused by news events if your site is a \nnews provider, by marketing events if your site is consumer-facing, or even \n",
      "page_number": 521
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 529-538)",
      "start_page": 529,
      "end_page": 538,
      "detection_method": "topic_boundary",
      "content": "26.4  Base Mechanisms\n509\nsporting events because viewers of sporting events may turn to their com-\nputers during breaks in the action. Startups can face surges in demand if \ntheir product catches on more quickly than they can build capacity.\nThe first four sources of variation are supported by virtualization without \nreference to the cloud or the cloud deployment model. The last source of varia-\ntion (uncertainty) depends on having a deployment model that can accommodate \nspikes in demand. This is the rationale behind cloud bursting, or keeping applica-\ntions in a private data center and offloading spikes in demand to the public cloud. \nPresumably, a public cloud provider can deploy sufficient capacity to accommo-\ndate any single organization’s spikes in demand.\nMulti-tenancy\nMulti-tenancy applications such as Salesforce.com or Microsoft Office 365 are \narchitected explicitly to have a single application that supports distinct sets of \nusers. The economic benefit of multi-tenancy is based on the reduction in costs \nfor application update and management. Consider what is involved in updating \nan application for which each user has an individual copy on their own desk-\ntop. New versions must be tested by the IT department and then pushed to the \nindividual desktops. Different users may be updated at different times because \nof disconnected operation, user resistance to updates, or scheduling difficulties. \nIncidents result because the new version may have some incompatibilities with \nolder versions, the new version may have a different user interface, or users with \nold versions are unable to share information with users of the new version.\nWith a multi-tenant application, all of these problems are pushed from IT \nto the vendor, and some of them even disappear. Any update is available at the \nsame instant to all of the users, so there are no problems with sharing. Any user \ninterface changes are referred to the vendor’s hotline rather than the IT hotline, \nand the vendor is responsible for avoiding incompatibilities for older versions.\nThe problems of upgrading do not disappear, but they are amortized over all \nof the users of the application rather than being absorbed by the IT department \nof every organization that uses the application. This amortization over more users \nresults in a net reduction in the costs associated with installing an upgraded ver-\nsion of an application.\n26.4  Base Mechanisms\nIn this section we discuss the base mechanisms that clouds use to provide their \nlow-level services. In an IaaS instance, the cloud provides to the consumer a vir-\ntual machine loaded with a machine image. Virtualization is not a new concept; \n\n\n510 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nit has been around since the 1960s. But today virtualization is economically en-\nticing. Modern hardware is designed to support virtualization, and the overhead \nit adds has been measured to be just 1 percent per instance running on the bare \nhardware.\nWe will discuss the architecture of an IaaS platform in Section 26.5. In this \nsection, we describe the concepts behind a virtual machine: the hypervisor and \nhow it manages virtual machines, a storage system, and the network.\nHypervisor\nA hypervisor is the operating system used to create and manage virtual machines. \nBecause each virtual machine has its own operating system, a consumer applica-\ntion is actually managed by two layers of operating system: the hypervisor and \nthe virtual machine operating system. The hypervisor manages the virtual ma-\nchine operating system and the virtual machine operating system manages the \nconsumer application. The key services used by the hypervisor to support the \nvirtual machines it manages are a virtual page mapper and a scheduler. A hyper-\nvisor, of course, provides additional services and has a much richer structure than \nwe present here, but these key services are the two that we will discuss.\nPage Mapper\nWe begin by describing how virtual memory works on a bare (nonvirtualized) \nmachine. All modern servers utilize virtual memory. Virtual memory allows an \napplication to assume it has a large amount of memory in which to execute. The \nassumed memory is mapped into a much smaller physical memory through the \nuse of page tables. The consumer application is divided into pages that are either \nin physical memory or temporarily residing on a disk. The page table contains the \nmapping of logical address (consumer application address) to physical address \n(actual machine address) or disk location. Figure 26.1 shows the consumer ap-\nplication executing its next instruction. This causes the CPU to generate a target \naddress from which to fetch the next instruction or data item. The target address \nis used to address into a page table. The page table provides a physical address \nwithin the computer where the actual instruction or data item can be found if it is \ncurrently in main memory. If the physical address is not currently resident in the \nmain memory of the computer, an interrupt is generated that causes a page that \ncontains the target address to be loaded. This is the mechanism that allows a large \n(virtual) address space to be supported on much smaller physical memory.\nTurning the virtual memory mechanism into a virtualization mechanism in-\nvolves adding another level of indirection. Figure 26.2 shows a logical sequence \nthat maps from the consumer application to a physical machine address. Modern \nprocessors contain many optimizations to make this process more efficient. A \nconsumer application generates the next instruction with its target address. This \ntarget address is within the virtual machine in which the consumer application is \n\n\n26.4  Base Mechanisms\n511\nTarget address of \nnext instruction\nFetch next\ninstruction\nfrom\nphysical\naddress\nFetch next\ninstruction\nfrom\ninterrupt\nhandler\nPhysical \naddress\ninside \ncurrent\naddress \nspace\nPhysical \naddress\noutside \ncurrent\naddress \nspace\nSoftware\ncomponent\nKey:\nHardware\ncomponent\nCPU\nPage table\nused to\nconvert\ntarget \naddress to\nphysical \naddress\nData flow\nFigure 26.1  Virtual memory page table\nSoftware\ncomponent\nKey:\nHardware\ncomponent\nData flow\nHost Server\nTarget address of \nnext instruction\nPage \nTable\nVMn\nPage \nTable\nnext\ninstruction\nVM1\nHypervisor\nHost page table\npoints to VM\npage table\nCPU\nFigure 26.2  Adding a second level of indirection to determine which virtual \nmachine the address references\n\n\n512 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nexecuting. The virtual machine page table maps this target address to an address \nwithin the virtual machine based on the target address as before (or indicates that \nthe page is not currently in memory). The address within the virtual machine is \nconverted to a physical address by use of a page table within the hypervisor that \nmanages the current virtual machines.\nScheduler\nThe hypervisor scheduler operates like any operating system scheduler. When-\never the hypervisor gets control, it decides on the virtual machine to which it will \npass control. A simple round-robin scheduling algorithm assigns the processor to \neach virtual machine in turn, but many other possible scheduling algorithms ex-\nist. Choosing the correct scheduling algorithm requires you to make assumptions \nabout the demand characteristics of the different virtual machines hosted within \na single server. One area of research is the application of real-time scheduling \nalgorithms to hypervisors. Real-time schedulers would be appropriate for the use \nof virtualization within embedded systems, but not necessarily within the cloud.\nStorage\nA virtual machine has access to a storage system for persistent data. The storage \nsystem is managed across multiple physical servers and, potentially, across clus-\nters of servers. In this section we describe one such storage system: the Hadoop \nDistributed File System (HDFS).\nWe describe the redundancy mechanism used in HDFS as an example of the \ntypes of mechanisms used in cloud virtual file systems. HDFS is engineered for \nscalability, high performance, and high availability.\nA component-and-connector view of HDFS within a cluster is shown in \nFigure 26.3. There is one NameNode process for the whole cluster, multiple \nDataNodes, and potentially multiple client applications. To explain the function \nof HDFS, we trace through a use case. We describe the successful use case for \n“write.” HDFS also has facilities to handle failure, but we do not describe these. \nSee the “For Further Reading” section for a reference to the HDFS failure-han-\ndling mechanisms. \nFor the “write” use case, we will assume that the file has already been \nopened. HDFS does not use locking to allow for simultaneous writing by differ-\nent processes. Instead, it assumes a single writer that writes until the file is com-\nplete, after which multiple readers can read the file simultaneously. The applica-\ntion process has two portions: the application code and a client library specific to \nHDFS. The application code can write to the client using a standard (but over-\nloaded) Java I/O call. The client buffers the information until a block of 64 MB \nhas been collected. Two of the techniques used by HDFS for enhancing perfor-\nmance are the avoidance of locks and the use of 64-MB blocks as the only block \n\n\n26.4  Base Mechanisms\n513\nsize supported. No substructure of the blocks is supported by HDFS. The blocks \nare undifferentiated byte strings. Any substructure and typing of the information \nis managed solely by the application. This is one example of a phenomenon that \nwe will notice in portions of the cloud: moving application-specific functionality \nup the stack as opposed to moving it down the stack to the infrastructure.\nFor reliability purposes each block is replicated a parameterizable number \nof times, with a default of three. For each block to be written, the NameNode al-\nlocates DataNodes to write the replicas. The DataNodes are chosen based on two \ncriteria: (1) their location—replicas are spread across racks to protect against the \npossibility that a rack fails; and (2) the dynamic load on the DataNode. Lightly \nloaded DataNodes are given preference over heavily loaded DataNodes to reduce \nthe possibility of contention for the DataNodes among different files being simul-\ntaneously accessed.\nOnce the client has collected a buffer of 64 MB, it asks the NameNode for \nthe identities of the DataNodes that will contain the actual replicas. The NameNode \nmanages only metadata; it is not involved in the actual transfer or recording of \ndata. These DataNode identities are sent from the NameNode to the client, which \nthen treats them as a pipeline. At this point the client streams the block to the first \nDataNode in the pipeline. The first DataNode then streams the data to the second \nProcess\nKey:\nConnector\nApplication layer\nClient layer\nRPC\nStreaming Protocol\nRPC\nNameNode Process\nDataNode Processes\nFigure 26.3  A component-and-connector view of an HDFS deployment. Each \nprocess exists on a distinct computer.\n\n\n514 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nDataNode in the pipeline, and so forth until the pipeline (of three DataNodes, unless \nthe client has specified a different replication value) is completed. Each DataNode \nreports back to the client when it has successfully written the block, and also reports \nto the NameNode that it has successfully written the block.\nNetwork\nIn this section we describe the basic concepts behind Internet Protocol (IP) ad-\ndressing and how a message arrives at your computer. In Section 26.5 we discuss \nhow an IaaS system manages IP addresses.\nAn IP address is assigned to every “device” on a network whether this de-\nvice is a computer, a printer, or a virtual machine. The IP address is used both to \nidentify the device and provide instructions on how to find it with a message. An \nIPv4 address is a constrained 32-bit number that is, typically, represented as four \ngroups for human readability. For example, 192.0.2.235 is a valid IP address. \nThe familiar names that we use for URLs, such as “http://www.pearsonhighered.\ncom/”, go through a translation process, typically through a domain name server \n(DNS), that results in a numeric IP address. A message destined for that IP ad-\ndress goes through a routing process to arrive at the appropriate location. \nEvery IP message consists of a header plus a payload. The header contains \nthe source IP address and the destination IP address. IPv6 replaces the 32-bit \nnumber with a 128-bit number, but the header of an IP message still includes the \nsource and destination IP addresses.\nIt is possible to replace the header of an IP message for various reasons. One \nreason is that an organization uses a gateway to manage traffic between external \ncomputers and computers within the organization. An IP address is either “public,” \nmeaning that it is unique within the Internet, or “private,” meaning that multiple \ncopies of the IP address are used, with each copy owned by a different organization. \nPrivate IP addresses must be accessed through a gateway into the organization that \nowns it. For outgoing messages, the gateway records the address of the internal ma-\nchine and its target and replaces the source address in the TCP header with its own \npublic IP address. On receipt of a return message, the gateway would determine the \ninternal address for the message and overwrite the destination address in the header \nand then send the message onto the internal network. Network address translation \n(NAT) is the name of this process of translation.\n26.5  Sample Technologies\nBuilding on the base mechanisms, we now discuss some of the technologies that \nexist in the cloud. We begin by discussing the design of a generic IaaS platform, \n\n\n26.5  Sample Technologies\n515\nthen we move up the stack to a PaaS, and finally we discuss database technology \nin the cloud.\nInfrastructure as a Service\nFundamentally, an IaaS installation provides three services: virtualized computa-\ntion, virtualized networking, and a virtualized file system. In the previous section \non base mechanisms, we described how the operating system for an individual \nserver manages memory to isolate each virtual machine and how TCP/IP mes-\nsages could be manipulated. An IaaS provides a management structure around \nthese base concepts. That is, virtual machines must be allocated and deallocated, \nmessages must be routed to the correct instance, and persistence of storage must \nbe ensured. \nWe now discuss the architecture of a generic IaaS platform. Various provid-\ners will offer somewhat different services within different architectures. Open-\nStack is an open source movement to standardize IaaS services and interfaces, \nbut as of this writing, it is still immature.\nFigure 26.4 shows an allocation view of a generic cloud platform. Each \nserver shown provides a different function to the platform, as we discuss next.\nAn IaaS installation has a variety of clusters. Each cluster may have thousands \nof physical servers. Each cluster has a cluster manager responsible for that cluster’s \nVirtual resource manager\nPersistent object manager\nCluster Manager\nFile System Manager\nNode Manager\nNode Manager\nNode Manager\nCluster Manager\nFile System Manager\nNode Manager\nNode Manager\nNode Manager\nCluster\nCluster\nKey:\nInternet message\nInternal cloud message\nCluster\nServer\nSOAP- or REST-\nbased tools\nWeb Browser\nFigure 26.4  A generic cloud allocation view\n\n\n516 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nresources. The persistent object manager supports the manipulation of persistent \nobjects, and the virtual resource managers are in charge of the other virtualized re-\nsources. For requests for new resources, the virtual resource manager is in charge \nof determining which cluster manager will service the request. For requests sent to \nexisting resources, the virtual resource manager is responsible for seeing that the re-\nquests get forwarded to the correct server. The virtual resource manager, in this case, \nacts as a gateway, as described in Section 26.4. \nSome of the services that IaaS providers offer to support applications are \nthese:\n■\n■Automatic reallocation of IP addresses in the case of a failure of the under-\nlying virtual machine instance. This service is useful in case the instance \nhas a public IP address. Unless the provider offers this service, the client \nmust register the IP address of a replacement instance with a domain name \nserver to ensure that messages are sent to the correct location.\n■\n■Automatic scaling. One of the virtues of the cloud is that new instances \ncan be created or deleted relatively quickly in the event of a variation in \ndemand. Detecting the variation in demand, allocating (or deleting) an in-\nstance in the event of a variation, and ensuring that the remaining instances \nare allocated their fair share of messages is another service that could be \nprovided by the IaaS.\nThe persistent object manager is responsible for maintaining files that are in-\ntended to persist past the deletion of a virtual machine instance. It may maintain \nthese files across multiple clusters in a variety of different geographic locations.\nFailure of the underlying hardware is a common occurrence in a large data \ncenter, consequently the virtual resource manager has mechanisms to manage re-\nquests in the event of failure. These mechanisms are typically designed to main-\ntain the availability of the IaaS infrastructure and do not extend to the applica-\ntions deployed with the virtual machines. What this means in practice is that if \nyou make a request for a new resource, it will be honored. If you make a request \nto an existing virtual machine instance, the infrastructure will guarantee that, if \nyour virtual machine instance is active, your request is delivered. If, however, the \nhost on which your virtual machine instance has been allocated has failed, then \nyour virtual machine instance is no longer active and it is your responsibility as \nan application architect to install mechanisms to recognize a failure of your vir-\ntual machine instances and recover from them.\nThe file system manager manages the file system for each cluster. It is sim-\nilar to the Hadoop Distributed File System that we discussed in Section 26.4. It \nalso assumes that failure is a common occurrence and has mechanisms to repli-\ncate the blocks and to manage handoffs in the event of failures.\nThe cluster manager controls the execution of virtual machines running on \nthe nodes within its clusters and manages the virtual networking between virtual \nmachines and between virtual machines and external users.\n\n\n26.5  Sample Technologies\n517\nThe final piece of Figure 26.4 is the node manager; it (through the function-\nality of a hypervisor) controls virtual machine activities, including the execution, \ninspection, and termination of virtual machine instances.\nA client initially requests a virtual machine instance and the virtual resource \nmanager decides on which cluster the virtual machine instance should reside. It \npasses the instance request to the cluster manager, which in turn decides which \nnode should host the virtual machine instance.\nSubsequent requests are routed through the pieces of the generic infrastruc-\nture to the correct instance. The instance can create files using the file system \nmanager. These files will either be deleted when the virtual machine instance is \nfinished or will be persisted past the existence of the virtual machine instance. \nThe choice is the client’s as to how long storage is persisted. If the storage is \npersisted, it can be accessed independently of the creating instance through the \npersistence manager.\nPlatform as a Service\nA Platform as a Service provides a developer with an integrated stack within the \ncloud to develop and deploy applications. IaaS provides virtual machines, and it \nis the responsibility of the developer using IaaS to provision the virtual machines \nwith the software they desire. PaaS is preprovisioned with a collection of inte-\ngrated software. \nConsider a virtual machine provisioned with the LAMP (Linux, Apache, \nMySQL, PHP/Perl/Python) stack. The developer writes code in Python, for ex-\nample, and has available the services provided by the other elements of the stack. \nTake this example and add automatic scaling across virtual machines based on \ncustomer load, automatic failure detection and recovery, backup/restore, security, \noperating system patch installation, and built-in persistence mechanisms. This \nyields a simple example of a PaaS.\nThe vendors offering PaaS and the substance of their offerings are rapidly \nevolving. Google and Microsoft are two of the current vendors.\n1.\t\nThe Google App Engine provides the developer with a development en-\nvironment for Python or Java. Google manages deploying and executing \ndeveloped code. Google provides a database service that is automatically \nreplicated across data centers.\n2.\t\nMicrosoft Azure provides an operating system and development platform \nto access/develop applications on Microsoft data centers. Azure provides \na development environment for applications running on Windows using \n.NET. It also provides for the automatic scaling and replication of instances. \nFor example, if an application instance fails, then the Azure infrastructure \nwill detect the failure and deploy another instance automatically. Azure also \nhas a database facility that automatically keeps replicas of your databases.\n\n\n518 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nDatabases\nA number of different forces have converged in the past decade, resulting in the \ncreation of database systems that are substantially different from the relational \ndatabase management systems (RDBMSs) that were prevalent during the 1980s \nand ’90s. \n■\n■Massive amounts of data began to be collected from web systems. A search \nengine must index billions of pages. Facebook, today, has over 800 million \nusers. Much of this data is processed sequentially and, consequently, the so-\nphisticated indexing and query optimizations of RDBMSs are not necessary.\n■\n■Large databases are continually being created during various types of pro-\ncessing of web data. The creation and maintenance of databases using a \ntraditional RDBMS requires a sophisticated data administrator.\n■\n■A theoretical result (the so-called CAP theorem) shows that it is not pos-\nsible to simultaneously achieve consistency, availability, and partitioning. \nOne of these properties must be sacrificed. For many applications, the \nchoice is to sacrifice consistency and provide immediate availability and \n“eventual consistency.” What this means, in practice, is that occasionally a \nuser will access stale data, but updates will be subsequently available. The \nalternative approach, taken by RDBMSs, is to lock values and not allow \naccess until they become consistent.\n■\n■The relational model is not the best model for some applications. The \nrelational model assumes there is one data item for each row-value/col-\numn-name pair. One method for handling web searches, for example, is to \nstore different versions of a single web page indexed by the same row-val-\nue/column-name pair so that the different versions of the web page can be \nquickly accessed and differences easily determined. Using the relational \nmodel requires that the system perform joins to retrieve all of the attributes \nassociated with a particular row value. Joins are expensive from a perfor-\nmance perspective, and consequently, newly emerging database systems \ntend to not support joins and require storing data in a denormalized form.\nThese forces resulted in the creation of new types of databases with different \ndata models and different access mechanisms. These new types of databases go \nunder the name of NoSQL—although as Michael Stonebraker has pointed out, \nthe existence or nonexistence of SQL within the database system is irrelevant to \nthe rationale for their existence.\nWe discuss two open source NoSQL database systems: a key-value one \n(HBase) and a document-centric one (MongoDB) .\nHBase\nHBase is a key-value database system based on the BigTable database system \ndeveloped by Google. Google uses BigTable to store data for many of their ap-\nplications. The number of data items in a HBase database can be in the billions \nor trillions.\n",
      "page_number": 529
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 539-546)",
      "start_page": 539,
      "end_page": 546,
      "detection_method": "topic_boundary",
      "content": "26.5  Sample Technologies\n519\nHBase supports tables, although there is no schema used. One column is \ndesignated as the key. The other columns are treated as field names. A data value \nis indexed by a row value, a column name, and a time stamp. Each row value/\ncolumn name can contain multiple versions of the same data differentiated by \ntime stamps.\nOne use of HBase is for web crawling. In this application, the row value is \nthe URL for the web page. Each column name refers to an attribute of a web page \nthat will support the analysis of the web page. For example, “contents” might be \none column name. In the relational model, each row value/column name would \nretrieve the contents of the web page. Web pages change over time, however, \nand so in the relational model, there would need to be a separate column with \nthe time stamp, and the primary key for the table would be the URL/time stamp. \nIn HBase, the versions of the web page are stored together and retrieved by the \nURL value/“contents”. All of the versions of the web page are retrieved, and it is \nthe responsibility of the application to separate the versions of the web page and \ndetermine which one is desired based on the time stamp.\nMongoDB\nMongoDB uses a document-centric data model. You can think of it as storing ob-\njects rather than tables. An object contains all of the information associated with \na particular concept without regard to whether relations among data items are \nstored in multiple different objects. Two distinct objects may have no field names \nin common, some field names in common, or all of the field names in common.\nYou may store links rather than data items. Links support the concept of \njoining different objects without requiring the maintenance of indices and query \noptimization. It is the responsibility of the application to follow the link.\nDocuments are stored in binary JavaScript Object Notation (JSON) form. \nIndices can be created on fields and can be used for retrieval, but there is no con-\ncept of primary versus secondary keys. A field is either indexed or it is not. Be-\ncause the same field can occur in multiple different documents, a field is indexed \nwherever it occurs.\nWhat Is Left Out of NoSQL Databases\nOne motivation for NoSQL databases is performance when accessing millions or \nbillions of data items. To this end, several standard RDBMS facilities are omit-\nted in NoSQL databases. If an application wishes to have these features, it must \nimplement them itself. Mainly, the features are omitted for performance reasons.\n■\n■Schemas. NoSQL databases typically do not require schemas for their \ndata model and, consequently, there is no checking of field names for \nconsistency.\n■\n■Transactions. NoSQL typically does not support transactions. Transactions \nlock data items, which hinders performance. Applications use techniques \n\n\n520 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nsuch as time stamps to determine whether fields have been modified \nthrough simultaneous access.\n■\n■Consistency. NoSQL databases are “eventually consistent.” This means that \nafter some time has passed, different replicas of a data item will have the \nsame value, but in the interim, it is possible to run two successive queries \nthat access the same data item and retrieve two different values.\n■\n■Normalization. NoSQL databases do not support joins. Joins are a require-\nment if you are to normalize your database.\n26.6  Architecting in a Cloud Environment\nNow we take the point of view of an architect who is designing a system to exe-\ncute in the cloud. In some ways, the cloud is a platform, and architecting a system \nto execute in the cloud, especially using IaaS, is no different than architecting for \nany other distributed platform. That is, the architect needs to pay attention to us-\nability, modifiability, interoperability, and testability, just as he or she would for \nany other platform. The quality attributes that have some significant differences \nare security, performance, and availability.\nSecurity\nSecurity, as always, has both technical and nontechnical aspects. The nontechnical \naspects of security are items such as what trust is placed in the cloud provider, what \nphysical security does the cloud provider utilize, how are employees of the cloud \nprovider screened, and so forth. We will focus on the technical aspects of security.\nApplications in the cloud are accessed over the Internet using standard Inter-\nnet protocols. The security and privacy issues deriving from the use of the Inter-\nnet are substantial but no different from the security issues faced by applications \nnot hosted in the cloud. The one significant security element introduced by the \ncloud is multi-tenancy. Multi-tenancy means that your application is utilizing a \nvirtual machine on a physical computer that is hosting multiple virtual machines. \nIf one of the other tenants on your machine is malicious, what damage can they \ndo to you?\nThere are four possible forms of attack utilizing multi-tenancy:\n1.\t\nInadvertent information sharing. Each tenant is given a set of virtual re-\nsources. Each virtual resource is mapped to some physical resource. It is \npossible that information remaining on a physical resource from one tenant \nmay “leak” to another tenant.\n2.\t\nA virtual machine “escape.” A virtual machine is isolated from other vir-\ntual machines through the use of a distinct address space. It is possible, \n\n\n26.6  Architecting in a Cloud Environment\n521\nhowever, that an attacker can exploit software errors in the hypervisor to \naccess information they are not entitled to. Thus far, such attacks are ex-\ntremely rare. \n3.\t\nSide-channel attacks. It is possible for a malicious attacker to deduce infor-\nmation about keys and other sensitive information by monitoring the timing \nactivity of the cache. Again, so far, this is primarily an academic exercise.\n4.\t\nDenial-of-service attacks. Other tenants may use sufficient resources on the \nhost computer so that your application is not able to provide service.\nSome providers allow customers to reserve entire machines for their exclu-\nsive use. Although this defeats some of the economic benefits of using the cloud, \nit is a mechanism to prevent multi-tenancy attacks. An organization should con-\nsider possible attacks when deciding which applications to host in the cloud, just \nas they should when considering any hosting option.\nPerformance\nThe instantaneous computational capacity of any virtual machine will vary de-\npending on what else is executing on that machine. Any application will need to \nmonitor itself to determine what resources it is receiving versus what it will need.\nOne virtue of the cloud is that it provides an elastic host. Elasticity means \nthat additional resources can be acquired as needed. An additional virtual ma-\nchine, for example, will provide additional computational capacity. Some \ncloud providers will automatically allocate additional resources as needed, \nwhereas other providers view requesting additional resources as the customer’s \nresponsibility.\nRegardless of whether the provider automatically allocates additional re-\nsources, the application should be self-aware of both its current resource usage \nand its projected resource usage. The best the provider can do is to use general \nalgorithms to determine whether there is a need to allocate or free resources. An \napplication should have a better model of its own behavior and be better equipped \nto do its own allocation or freeing of resources. In the worst case, the application \ncan compare its predictions to those of the provider to gain insight into what will \nhappen. It takes time for the additional resources to be allocated and freed. The \nfreeing of resources may not be instantaneously reflected in the charging algo-\nrithm used by the provider, and that charging algorithm also needs to be consid-\nered when allocating or freeing resources.\nAvailability\nThe cloud is assumed to be always available. But everything can fail. A virtual \nmachine, for example, is hosted on a physical machine that can fail. The virtual \n\n\n522 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nnetwork is less likely to fail, but it too is fallible. It behooves the architect of a \nsystem to plan for failure.\nThe service-level agreement that Amazon provides for its EC2 cloud service \nprovides a 99.95 percent guarantee of service. There are two ways of looking at \nthat number: (1) That is a high number. You as an architect do not need to worry \nabout failure. (2) That number indicates that the service may be unavailable for \n.05 percent of the time. You as an architect need to plan for that .05 percent.\nNetflix is a company that streams videos to home television sets, and its \nreliability is an important business asset. Netflix also hosts much of its operation \non Amazon EC2. On April 21, 2011, Amazon EC2 suffered a four-day sporadic \noutage. Netflix customers, however, were unaware of any problem. \nSome of the things that Netflix did to promote availability that served them \nwell during that period were reported in their tech blog. We discussed their Sim-\nian Army in Chapter 10. Some of the other things they did were applications of \navailability tactics that we discussed in Chapter 5. \n■\n■Stateless services. Netflix services are designed such that any service in-\nstance can serve any request in a timely fashion, so if a server fails, requests \ncan be routed to another service instance. This is an application of the spare \ntactic, because the other service instance acts as a spare.\n■\n■Data stored across zones. Amazon provides what they call “availability \nzones,” which are distinct data centers. Netflix ensured that there were mul-\ntiple redundant hot copies of the data spread across zones. Failures were \nretried in another zone, or a hot standby was invoked. This is an example of \nthe active redundancy tactic.\n■\n■Graceful degradation. The general principles for dealing with failure are \napplications of the degradation or the removal from service tactic:\n■\n■Fail fast: Set aggressive timeouts such that failing components don’t \nmake the entire system crawl to a halt.\n■\n■Fallbacks: Each feature is designed to degrade or fall back to a lower \nquality representation.\n■\n■Feature removal: If a feature is noncritical, then if it is slow it may be re-\nmoved from any given page. \nThe CAP Theorem\nThe CAP theorem—created by Eric Brewer at UC Berkeley—emerged \nover a decade ago. Unlike most theories postulated by academics, \nthis one did not sink into obscurity but rather has grown in renown and \ninfluence since then. The theory states that there are three important \nproperties of a distributed system managing shared data. These are the \nfollowing:\n\n\n26.6  Architecting in a Cloud Environment\n523\n■\n■\nConsistency (C): the data will be consistent throughout the distributed \nsystem. \n■\n■\nAvailability (A): the data will be highly available.\n■\n■\nPartitioning (P): the system will tolerate network partitioning.\nAnd the theory further states that no system can achieve all of these \nproperties simultaneously; the best we can hope for is to satisfy two out of \nthree while sacrificing (to some extent) the third property. Brewer explains \nit thus:\nThe easiest way to understand CAP is to think of two nodes on opposite \nsides of a partition. Allowing at least one node to update state will cause the \nnodes to become inconsistent, thus forfeiting C. Likewise, if the choice is to \npreserve consistency, one side of the partition must act as if it is unavailable, \nthus forfeiting A. Only when nodes communicate is it possible to preserve both \nconsistency and availability, thereby forfeiting P. \nIn fact, there is really another important facet to the CAP theorem that \nhas come to dominate the engineering challenge: latency. It wasn’t part of \nthe original acronym (although CLAP is certainly catchy), but a concern for \nlatency now infuses much of the discussion of the tradeoffs in implement-\ning NoSQL databases. \nCreators of large-scale distributed NoSQL databases are constantly \nfaced with tradeoffs. These days no one believes that you simply choose \ntwo of the three properties of CAP; the decisions are far richer and more \nsubtle than that. For example, designers of these systems like to speak of \n“eventual consistency”—partitions are allowed to become inconsistent on a \nregular basis, but with bounds that are carefully engineered and monitored. \nThey might want to specify that no more than x percent of the data should \nbe stale at any given time, and it should not take more than y seconds to \nrestore consistency (on average, or in the worst case). Another common \ntradeoff seen in practice is that availability and latency are typically favored \nover consistency. That is, a Facebook user should get quick response from \nthe system, even if their newsfeed is slightly stale.\nAll of this adds complexity to the system. The designer has to choose \nbetween faster/less consistent and slower/more consistent (as well as a \nhost of other quality issues). And the mechanisms for achieving eventual \nconsistency—caching, replication, message retries, timeouts, and so \nforth—are themselves nontrivial. Consistency, partitioning, latency, and \navailability are four qualities that can be traded off with NoSQL databases. \nIn addition, other quality attributes—interoperability, security, and so forth—\nalso add complexity, and so the tradeoffs involved can get more and more \ncomplicated.\nAlas, this is, increasingly, the world that we live in. Systems with global \nreach and enormous bases of distributed data are not going away anytime \nsoon. So as architects we need to be prepared to deal with tradeoffs and \ncomplexities for the foreseeable future.\n—RK\n\n\n524 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\n26.7  Summary\nThe cloud has become a viable alternative for the hosting of data centers primar-\nily for economic reasons. It provides an elastic set of resources through the use of \nvirtual machines, virtual networks, and virtual file systems.\nThe cloud can be used to provide infrastructure, platforms, or services. Each \nof these has its own characteristics.\nNoSQL database systems arose in reaction to the overhead introduced by \nlarge relational database management systems. NoSQL database systems fre-\nquently use a data model based on key-value or documents and do not provide \nsupport for common database services such as transactions.\nArchitecting in the cloud means that the architect should pay attention to \nspecific aspects of quality attributes that are substantially different in cloud envi-\nronments, namely: performance, availability, and security.\n26.8  For Further Reading\nDealers of Lightning: Xerox PARC and the Dawn of the Computer Age [Hiltzik \n00] has a discussion of time-sharing and covers the technologies and the person-\nalities involved in the development of the modern personal workstation.\nThe economics of the cloud are described in [Harms 10].\nThe Computer Measurement Group (CMG) is a not-for-profit worldwide \norganization that provides measurement and forecasting of various quantitative \naspects of computer usage. Their measurements of the overhead due to virtualiza-\ntion can be found at www.cmg.org/measureit/issues/mit39/m_39_1.html.\nIf you want to learn more about TCP/IP and NAT, you can find a discussion \nat www.ipcortex.co.uk/wp/fw.rhtm.\nThe BigTable system is described in [Chang 06].\nNetflix maintains a tech blog that is almost entirely focused on cloud issues. \nIt can be found at techblog.netflix.com.\nThe home page for MongoDB is www.mongodb.org/display/DOCS/Home \nand for HBase is hbase.apache.org.\nMichael Stonebraker is a database expert who has written extensively com-\nparing NoSQL systems with RDBMSs. Some of his writings are [Stonebraker \n09], [Stonebraker 10a], [Stonebraker 11], and [Stonebraker 10b].\nEric Brewer has provided a nice overview of the issues surrounding the CAP \ntheorem for today’s cloud-based systems: [Brewer 12].\n\n\n26.9  Discussion Questions\n525\n26.9  Discussion Questions\n1.\t\n“Service-oriented or cloud-based systems cannot meet hard-real-time re-\nquirements because it’s impossible to guarantee how long a service will \ntake to complete.” Do you think this statement is true or false?  In either \ncase, identify the one or two categories of design decisions that are most \nresponsible for the correctness (or incorrectness) of the statement.\n2.\t\n“Using the cloud assumes your application is service oriented.” Do you \nthink this is true or false? Find some examples that would support that \nstatement and, if it is not universally true, find some that would falsify it.\n3.\t\nNetflix discussed their movement from Oracle to SimpleDB on their tech \nblog. They also discussed moving from SimpleDB to Cassandra. Describe \ntheir rationale for these two moves.\n4.\t\nNetflix also describes their Simian Army in their tech blog. Which elements \nof the Simian Army could be offered as a SaaS? What would the design of \nsuch a SaaS look like?\n5.\t\nDevelop the “Hello World” application on an IaaS and on a PaaS.\n\n\nThis page intentionally left blank \n",
      "page_number": 539
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 547-554)",
      "start_page": 547,
      "end_page": 554,
      "detection_method": "topic_boundary",
      "content": "527\n27\nArchitectures for \nthe Edge\nWith Hong-Mei Chen\nHuman nature is not a machine to be built after a \nmodel, and set to do exactly the work prescribed for \nit, but a tree, which requires to grow and develop \nitself on all sides, according to the tendency of \nthe inward forces which make it a living thing.\n—John Stuart Mill\nIn this chapter we discuss the place of architecture in edge-dominant systems and \ndiscuss how an architect should approach building such systems. An edge-dom-\ninant system is one that depends crucially on the inputs of users for its success.\nWhat would Wikipedia be without the encyclopedic entries contributed by \nusers? What would YouTube be without the contributed videos? What would \nFacebook and Twitter be without their user communities? YouTube serves up ap-\nproximately 1 billion videos a day. Twitter boasts that its users tweet 50 million \ntimes per day. Facebook reports that it serves up about 30 billion pieces of con-\ntent each month. Flickr recently announced that users had uploaded more than 6 \nbillion photos. Strong, almost maniacal, user participation has elevated each of \nthese sites from fairly routine repositories to forces that shape society. \nIn each case, the value of these systems comes almost entirely from its us-\ners—who happily contribute their opinions and knowledge, their artistic content, \ntheir software, and their innovations—and not from some centralized organiza-\ntion. This phenomenon is a cornerstone of the so-called “Web 2.0” movement. \nDarcy DiNucci, credited with coining the term, wrote in 1999, “The [new] Web \nwill be understood not as screenfuls of text and graphics but as a transport mech-\nanism, the ether through which interactivity happens.” The “old” web was about \n\n\n528 \nPart Five  Brave New World\t\n27—Architectures for the Edge\ngoing to web pages for static information; the “new” web is about participating in \nthe information creation (“crowdsourcing”) and even becoming part of its organi-\nzation (“folksonomy”). \nMany have written about the social, political, and economic consequences \nof this change, and some see it as nothing short of a revolution along the lines of \nthe industrial revolution. Yochai Benkler’s book The Wealth of Networks—a play \non the title of Adam Smith’s classic book The Wealth of Nations, which heralded \nthe start of the industrial revolution—argues that the “radical transformation” of \nhow we create our information environment is restructuring society, particularly \nour models of production and consumption. Benkler calls this new economic \nmodel commons-based peer production. And it’s big: crowdsourced websites that \nare built on this model have become some of the dominant forces on the web and \nin society in the past few years. Populist revolutions are catalyzed by Twitter and \nFacebook. As of the time this book went to press, five of the top ten websites by \ntraffic are peer produced: Facebook, YouTube, Blogger, Wikipedia, and Twitter. \nAnd the other five are portals or search engines that pore through the content cre-\nated by billions of worldwide users. Websites that actually sell something from a \ncentralized organization are rare in the world’s top sites; Amazon.com is the only \nexample, and it’s no accident that it’s the bookseller that derives much of its pop-\nularity from the value created by customers. \nAlong with this paradigm shift, much of the world’s software is now open \nsource. The two most popular web browsers in the world are open source (Mo-\nzilla Firefox and Google Chrome). Apache is the most popular web server, cur-\nrently powering almost two out of every three websites. Open source databases, \nIDEs, content management systems, and operating systems are all heavy hitters \nin their respective market spaces.\nWhy study this in a book about architecture? First, commons-based \npeer-produced systems are an excellent example of the architecture influence \ncycle. Second, the architecture of such systems has some important differences \nfrom the architectures that you would build for traditional systems. We start by \nexamining how the forces of commons-based peer production change the very \nnature of the system’s development life cycle. \n27.1  The Ecosystem of Edge-Dominant Systems\nAll successful edge-dominant systems, and the organizations that develop and \nuse these systems, share a common ecosystem structure, as shown in Figure 27.1. \nThis is called a “Metropolis” structure, by analogy with a city.\nThe Metropolis structure is not an architecture diagram: it is a representa-\ntion of three communities of stakeholders:\n\n\n27.1  The Ecosystem of Edge-Dominant Systems\n529\nCore \n    Edge \nMasses \nOpen  \nSource  \nSoftware \nOpen  \nContent \nSystems \n(Prosumers) \n(Developers) \n(End Users) \n(Customers) \nFigure 27.1  The Metropolis structure of an edge-dominant system \n■\n■Customers and end users, who consume the value produced by the \nMetropolis\n■\n■Developers, who write software and key content for the Metropolis\n■\n■Prosumers, who consume content but also produce it \nThe Metropolis structure also presents three realms of roles and \ninfrastructure: \n■\n■In the outermost ring reside the masses of end users of such systems. They \ncontribute requirements but not content. \n■\n■The middle ring contains developers and prosumers. These are the stake-\nholders at the edge whose actions and whose value creation the organiza-\ntion would like to facilitate. \n■\n■All of this is held together by the core. The core is software; it provides its \nservices through a set of APIs upon which the periphery can build. Linux’s \nkernel, Apache’s core, Wikipedia’s wiki, Facebook’s application platform, \nor the application platforms of the iPhone or Android.\nIn the Metropolis structure, the realms have different “permeability,” which \nthe figure indicates by the dashed and solid lines. \nIn open source systems (such as Linux, MySQL, Apache, Eclipse, and Fire-\nfox), it is possible to move from the role of an end user to a developer to a core \narchitect, by consistently contributing and moving up in the meritocracy. How-\never, in open content systems (such as Wikipedia, Twitter, YouTube, Slashdot, \n\n\n530 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nand Facebook), nobody will invite you to become a software developer for the \ncore, no matter how many cool movies or blog entries you contribute.\nThus, a key question for an organization wishing to foster edge-dominant \nsystems: How should we architect the core and what development principles \nshould we embrace for the periphery/edge?\n27.2  Changes to the Software Development Life Cycle\nAll our familiar software development life cycles—waterfall, Agile, iterative, or \nprototyping-based—are broken in an edge-dominant, crowdsourced world. These \nmodels all assume that requirements can be known; software is developed, tested, \nand released in planned increments; projects have dedicated finite resources; and \nmanagement can “manage” these resources. None of these conditions is true in \nthe Metropolis. Let us consider each aspect in turn:\n■\n■Requirements can be known. In edge-dominant systems, requirements \nemerge from its individuals, operating independently. Requirements are \nnever knowable in any global sense and they will inevitably conflict, just \nas the requirements of a city’s inhabitants often conflict (some want better \nhighways, some want more park land).\n■\n■Software is developed, tested, and released in planned increments. All ex-\nisting software development models assume that systems evolve in an or-\nderly way, through planned releases. An edge-dominant system, on the oth-\ner hand, is constantly changing. It doesn’t make sense, for example, to talk \nabout the “latest release” of Wikipedia. Resources are noncentralized and \nso such a system is never “stable.” One cannot conceive of its functionality \nin terms of “releases” any more than a city has a release; parts are being \ncreated, modified, and torn down at all times.\n■\n■Projects have dedicated finite resources. Edge-dominant projects are \n“staffed” by members who are not employed by the project. Such projects \nare subject to the whims of the members who are not required and cannot \nbe compelled to contribute anything. However, successful projects tend to \nattract large numbers of contributors. Unlike traditional projects, which \nhave finite resources, typically limited by budgetary constraints, there is no \nnatural limit to the resources available to an edge-dominant project. And \nthese large numbers tend to ameliorate the (unreliable) actions of any indi-\nvidual contributor.\n■\n■Management can “manage” these resources. In edge-dominant systems, \nthe developers are often volunteers. They participate in decentralized pro-\nduction processes with no identifiable managers. Linus Torvalds, the creator \nof the Linux operating system, has noted that he has no authority to order \nanyone to do anything; he can only attempt to lead and persuade, in the \n\n\n27.3  Implications for Architecture\n531\nhope that others will follow. Teams in this world are often diverse with dif-\nfering, sometimes irreconcilable, views. \nFor edge-dominant systems, the old rules and tools of software development \nwon’t work. Such projects are, to varying degrees, community driven and de-\ncentralized with little overall control, as is the case with the major social net-\nworking communities (e.g., Twitter, Google+, Facebook), and open content sys-\ntems (e.g., Wikipedia, YouTube), especially coupled with open source software \ndevelopment. \n27.3  Implications for Architecture\nThe Metropolis structure presented in the previous section, while not an architec-\nture, has important implications for architecture. The key architectural choice for \nan edge-dominant system is the distinction between core and edge. That is, the \narchitecture of successful edge-dominant systems is, without fail, bifurcated into \n■\n■a core (or kernel) infrastructure and \n■\n■a set of peripheral functions or services that are built on the core. \nThis constitutes an architectural pattern very reminiscent of layering. Linux, \nFirefox, and Apache—to name just a few—are based upon this architectural pat-\ntern. Linux applies this pattern twice: at the outermost level the core is the entire \nLinux kernel, and individual applications, libraries, resources, and auxiliaries act \nas extensions to the kernel’s functionality—the periphery. Digging into the Li-\nnux kernel, we can once again discern a core/periphery pattern. Inside the Linux \nkernel, modules are defined to enable parallel development of different subsys-\ntems. The different functions that one expects to find in an operating system ker-\nnel are all present, but they are designed to be separate modules. For instance, \nthere are modules for processor/cache control, memory management, resource \nmanagement, file system interfacing, networking stacks, device I/O, security, and \nso forth. All of these modules interact, but they are clearly identifiable, separate \nmodules within the kernel.\nWe have said many times in this book that architectures come from business \ngoals, as interpreted through the lens of architecturally significant requirements. \nBut what are the business goals for an edge-dominant system? We said earlier \nthat you cannot “know” the requirements for such a system, in any complete \nsense. Well, this was perhaps a bit hasty. \nRequirements for such systems are typically bifurcated into core require-\nments and periphery requirements:\n■\n■Core requirements deliver little or no end-user value and focus on quality \nattributes and tradeoffs—defining the system’s performance, modularity, \n\n\n532 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nsecurity, and so forth. These requirements are generally slow to change as \nthey define the major capabilities and qualities of the system. \n■\n■Periphery requirements, on the other hand, are unknowable because they \nare contributed by the peer network. These requirements deliver the majori-\nty of the function and end-user value and change relatively rapidly.\nGiven this structure, the majority of implementation (the periphery) is \ncrowdsourced to the world using their own tools, to their own standards, at their \nown pace. The implementers of the core, on the other hand, are generally close \nknit and highly motivated. \nThis has at least three implications for the core, which the architect will \nneed to address:\n■\n■The core needs to be highly modular, and it provides the foundation for the \nachievement of quality attributes. The core in a successful core/periphery \npattern is designed by a small, coherent team. In open source projects, these \npeople are referred to as the “committers.” In Linux, for example, a strong \nemphasis on modularity has been postulated to account for its enormous \ngrowth. This allows for successful contributions of independent enhance-\nments by scores of distributed and unknown-to-each-other programmers. \nThe peripheral services are enabled by and constrained by the kernel, but \nare otherwise unspecified. \n■\n■The core must be highly reliable. Most cores are heavily tested, which \nmeans that testability is important. Heavy testing for the core is tractable \nbecause the core is typically small—often orders of magnitude smaller than \nthe periphery—highly controlled, and relatively slow to change. If the core \ncannot be made small, then its components can be made to be as indepen-\ndent of each other as possible, which eases the testing burden.\n■\n■The core must be highly robust with respect to errors in its environment. \nThe reliability of the periphery software is entirely in the hands of the pe-\nriphery community and the masses (end users and customers). The masses \nare typically recruited as testers (Mozilla claims to have three million), \nalthough this testing is often no more than clicking a button that signals a \nuser’s agreement to have bugs and quality information reported back to the \nproject. Given that the core will undoubtedly be supporting flawed periph-\nery components, robustness of the core is a key requirement; quite simply, \nfailures in the periphery must not cause failures of the core. This means that \na system employing the core/periphery pattern should create monitoring \nmechanisms to determine the current state of the system, and control mech-\nanisms so that bugs in the periphery cannot undermine the core.\nThe core (often called a platform) is usually implemented as a set of ser-\nvices; complex platforms have hundreds. The Amazon EC2 cloud, for exam-\nple, has over 110 different APIs documented, and EC2 is only a portion of the \n\n\n27.4  Implications of the Metropolis Model\n533\nAmazon platform. To make these services available to peripheral developers, a \nnumber of conditions must hold: \n■\n■Documentation must be available for each API, it must be well written, \nit must be well organized, and it must be up to date. Because a peripheral \ndeveloper is frequently a volunteer, incomplete, out of date, or unclear doc-\numentation presents a barrier to entry. Even if there is a financial motivation \n(such as from developing an iPhone or Android application), the documen-\ntation still must not present a barrier.\n■\n■There must be a discovery service. Having hundreds of services means \nthat some of them are going to be redundant and others are going to be \nunavailable. A discovery service becomes a necessity to enable navigation \nand flexibility in such a world. A discovery service, in turn, implies a regis-\ntration service. Services must proactively register upon initialization and be \nremoved if they are no longer active.\n■\n■Error detection becomes extremely complicated. If you as a peripheral de-\nveloper encounter a bug in a service, it may be a bug in the service you are \ninvoking, it may be a bug in a service invoked by the service you are invok-\ning, or anywhere in the chain of services. Reporting a problem and getting \nit resolved may end up being extremely time-consuming. Quality assurance \nof services requires constant testing of their availability and correctness. \nThe Netflix Simian Army we discussed in Chapter 10 is an example of how \nquality assurance on a platform might be structured.\n■\n■All of the peer services might be potential denial-of-service attackers. \nThrottling, monitoring, and quotas must be employed to ensure that service \nrequesters receive adequate responses to their requests.\nBuilding a platform to be a core to support peripheral developers is a non-\ntrivial undertaking. Yet having such a core has paid dramatic dividends for com-\npanies comprising the Who’s Who of today’s web.\n27.4  Implications of the Metropolis Model\nThe Metropolis model, as we’ve seen, is paired with the core/periphery pattern \nfor architecture for edge-dominant systems. Adopting this duo brings with it \nchanges to the way that software is developed; in effect, it implies a software de-\nvelopment model, with its implications on tools, processes, activities, roles, and \nexpectations. Many such models have evolved over the years, each with its own \ncharacteristics, strengths, and weaknesses. Clearly, no one model is best for all \nprojects. For instance, Agile methods are best in projects with rapidly evolving \nrequirements and short time-to-market constraints, whereas a waterfall model is \n\n\n534 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nbest for large projects with well-understood and stable requirements and complex \norganizational structures. \nThe Metropolis model requires a new perspective on system development, \nresulting in several important changes to how we must create systems: \n1.\t\nIndifference to phases. The Metropolis model uses the metaphor of a bull’s \neye, as opposed to a waterfall, a spiral, a “V,” or other representations that \nprevious models have adopted. The contrast to these previous models is \nsalient: the “phases” of development disappear in the bull’s eye. Instead, we \nmust focus managerial attention on the explicit inclusion of customers (the \nperiphery and the masses) for system development. \n2.\t\nCrowd management. Policies for crowd management must be aligned with \nthe organization’s strategic goals and must be established early. Crowds \nare good for certain tasks, but not for all. This implies that business models \nmust be examined to consider policies and associated system development \ntasks for crowd engagement, performance management monitoring, com-\nmunity protection, and so on. As crowdsourcing is rooted in the “gift” cul-\nture, for-profit organizations must carefully align tasks with the volunteers’ \nvalues and intentions.\n3.\t\nCore versus periphery. The Metropolis model differentiates the core and \nperiphery communities, with different tools, processes, activities, roles, and \nexpectations for each. The core must be small and tightly controlled by a \ngroup who focus on modularity, core services, and core quality attributes; \nthis enables the unbridled and uncoordinated growth at the periphery.\n4.\t\nRequirements process. The requirements for a Metropolis system are pri-\nmarily asserted by the periphery, not elicited from the masses; they emerge \nfrom the collective experiences of the community of the periphery, typically \nthrough their emails, wikis, and discussion forums. So such forums must \nbe made available—typically provided by members of the core—and the \nperiphery should be encouraged to participate in discussions about the re-\nquirements, in effect, to create a community. This changes the fundamental \nnature of requirements engineering, which has traditionally focused on col-\nlecting requirements, making them complete and consistent, and removing \nredundancies wherever possible.\n5.\t\nFocus on architecture. The core architecture is the fabric that holds to-\ngether a Metropolis system. As such, it must be consciously designed to \naccommodate the specific characteristics of open content and open source \nsystems. For this reason, the architecture cannot “emerge,” as it often does \nin traditional life-cycle models, and in Agile models. It must be designed \nup front, built by a small, experienced, motivated team who focus on (1) \nmodularity, to enable the parallel activities of the periphery, and (2) the \ncore quality attributes (security, performance, availability, and so on). There \nshould be a lead architect, or a small team of leads, who can manage proj-\nect coordination and who have the final say in matters affecting the core. \n",
      "page_number": 547
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 555-562)",
      "start_page": 555,
      "end_page": 562,
      "detection_method": "topic_boundary",
      "content": "27.4  Implications of the Metropolis Model\n535\nLinus Torvalds, for example, still exerts “veto” rights on matters affecting \nLinux’s kernel. Virtually every open source project distinguishes between \nthe roles of contributor (who can contribute patches) and committer (who \nchooses which patches make it into any given release). \n6.\t\nDistributed testing. The core/periphery distinction also provides guidance \nfor testing activities. The core must be heavily tested and validated, because \nit is the fabric that holds the system together. Thus, when planning a Me-\ntropolis project, it is important to focus on validation of the core and to put \ntools, guidelines, and processes in place to facilitate this. For example, the \ncore should be kept small; the project should have frequent (perhaps night-\nly) builds and frequent releases; bug reporting should be built in to the sys-\ntem and require little effort on the part of the periphery. The project must \nexplicitly take advantage of the “many eyes” provided by the periphery. \n7.\t\nAutomated delivery. Delivery mechanisms must be created that work in \na distributed, asynchronous manner. These mechanisms must be flexible \nenough to accept incompleteness of the installed base as the norm. Thus, \nany delivery mechanism must be tolerant of older versions, multiple coex-\nisting versions, or even incomplete versions. A Metropolis system should \nalso, as far as possible, be tolerant of incompatibilities both within the \nsystem and between systems. For example, modern web browsers will still \nparse old versions of HTML or interact with old versions of web servers; \nbrowser add-ons and plug-ins coexist at different version levels and no \ncombination of them will “break” the browser. \n8.\t\nManagement of the periphery. One important aspect of the core/periphery \nmodel is that the core exercises very little control over the periphery. Yet \nthis does not mean that the periphery is totally unmanaged. If we examine \nthe extant platforms that are either crowdsourced or peripheral developer \nsourced, we see that there is always a governance policy set by a managing \norganization. The Internet and the World Wide Web have a collection of \ngoverning boards, large open source projects and Wikipedia are managed \nby foundations and meritocracies, and private companies such as Facebook \nor Apple have their own management structures. The governance policies \ncreated by the management organizations are enforced in either a proactive \nor reactive fashion. Some policies are enforced by a combination of both:\n■\n■Proactive enforcement. Proactive enforcement inhibits contributions by \nthe prosumers or the peripheral developers unless they meet certain cri-\nteria. Within the Internet, for example, IP addresses are assigned. One \ncannot make up one’s own IP address. Communication protocols and \nweb standards are defined by groups chartered by one of the Internet or \nweb governing organizations. Apple, as another example, screens appli-\ncations before they are eligible for inclusion in the App Store. And every \nplatform has a collection of APIs that also constrain and govern how a \nperipheral application interacts with it.\n\n\n536 \nPart Five  Brave New World\t\n27—Architectures for the Edge\n■\n■Reactive enforcement. Reactive enforcement dictates the response in case \nthere is a violation of the organization’s policy. Wikipedia has a collection \nof editors who are responsible for ensuring the quality of contributions \nafter they have been made. Facebook, YouTube, Flickr and most other \ncrowdsourced sites have procedures to report violations. And if a periph-\neral developer does not adhere to a protocol or a set of APIs, then their \nproduct is flawed in some fashion and the market will likely punish them.\nThe analogy of a city to explain some of the facets of the core/periphery \nmodel can be extended. Zoning is a policy that describes permissible land use \nfor a city or other governmental organization. It specifies, for example, that cer-\ntain pieces of land are for residential use and other pieces are for industrial use. \nZoning policies have both proactive and reactive enforcement. Figure 27.2 shows \nsome of the actors associated with a zoning board. The zoning board is the gov-\nernance organization; it produces a building code that prescribes legitimate uses \nand restrictions on various buildings. The building inspector is a reactive enforcer \nwho is responsible for verifying that the buildings conform to permissible stan-\ndards and usage. As with any analogy, the zoning board is not an exact descrip-\ntion of the core/periphery, but it does identify many of the elements that go into \ncontrolling contributions.\nBuilding \ninspector \nBuilding code \nMember \nserves on \nAppointing authority \nDeveloper \nCitizen or \ncitizen \ngroups \nproduces \nselects \nZoning Board\nconstrains \nRequests \nvariance  \nor zoning \nchange \nEnforces building code \nRequests \nvariance or \nzoning change \nGovernment \nagency\nprovides\nfunds \nApproves \nor rejects \nchange \nrequest \nExpert \nadvises \nFigure 27.2  Zoning board stakeholders\n\n\n27.5  Summary\n537\nLife-cycle models are never revolutionary; they arise in reaction to ambient \nconditions in the software development world. The Waterfall model was created \nto focus more attention on removing flaws early in a project’s life cycle, in reac-\ntion to the delays, bugs, and failures of projects of ever-increasing complexity. \nThe spiral model and, later, the Rational Unified Process were created because \nprojects needed to produce working versions of software more quickly and to \nmitigate risks earlier in the software development life cycle. Agile methods grew \nout of the desire for less bureaucracy, more responsiveness to customer needs, \nand shorter times to market.\nSimilarly, the Metropolis model is formally capturing a market response \nthat is already occurring: the rise of commons-based peer production and ser-\nvice-dominant logic. Prior life-cycle models are simply inadequate—mostly \nmute—on the concerns of edge-dominant systems: crowdsourcing, emergent re-\nquirements, and change as a constant. This model offers new ways to think about \nhow a new breed of systems can be developed; its principles help management \nshift to new project management styles and architecture models that take advan-\ntage of the “wisdom of crowds.” \nMetropolis model concepts are not appropriate for all forms of development. \nSmaller systems with limited scope will continue to benefit from the conceptual \nintegrity that accompanies a small, cohesive team. High-security and safety-criti-\ncal systems, and systems that are built around protected intellectual property, will \ncontinue to be built in traditional ways for the foreseeable future. But more and \nmore crowdsourcing, mashups, open source, and other forms of edge-dominant \ndevelopment are being harnessed for value cocreation, and the Metropolis model \ndoes speak to this.\n27.5  Summary\nAn edge-dominant system is one that depends crucially on the inputs of users for \nits success. Users participate in information creation (“crowdsourcing”) and even \nits organization (“folksonomy”). These systems, part of the “Web 2.0” move-\nment, are having profound social, political, and economic consequences.\nAll successful edge-dominant systems, and the organizations that develop \nand use these systems, share a common ecosystem structure known as the “Me-\ntropolis” structure. This structure shows how customers, end users, developers, \nand “prosumers” are related.\nEdge systems bring a new life-cycle model to the fore, in which require-\nments are not completely known, software developed in planned increments is re-\nplaced by software that is constantly changing, and projects are staffed by mem-\nbers outside the purview of the central developing organization.\n\n\n538 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nThe dominant pattern for edge systems is the core/periphery pattern. This \npattern divides the world into a closely controlled core and a loosely controlled \nperiphery. To work, the core needs to be highly modular, highly reliable, and \nhighly robust with respect to external faults. Cores are often designed as a set \nof services with well-documented APIs, discovery and registration, and sophisti-\ncated error detection and reporting.\n27.6  For Further Reading\nAn interesting interview of Linus Torvalds, showing his management style—\nwhat he calls “shepherding”—appeared in BusinessWeek magazine several years \nago [Hamm 04].\nYochai Benkler’s intriguing book The Wealth of Networks [Benkler 07] puts \nforth a powerful premise: that the networked information economy is transform-\ning society. It shows how the modern networked economy transforms methods of \nproduction and consumption, and creates new forms of value that do not depend \non market strategies.\nFor more information and background on the Metropolis model, you can \nread the original paper describing it [Kazman 09].\nMuch of the inspiration for the Metropolis model comes from the Ultra-\nLarge-Scale Systems report [Feiler 06].\nMacCormack and colleagues have written extensively on the architecture \nand properties of what they call “core/periphery” systems. See, for example, \n[MacCormack 10].\n27.7  Discussion Questions\n1.\t\nDraw the architecture influence cycle for Web 2.0 software systems in gen-\neral, and for one of its flagship examples (such as Twitter or Facebook).\n2.\t\nCreate a complete pattern description for the core/periphery pattern, mod-\neled on those in Chapter 13.\n3.\t\nHow might the role of architecture documentation be different for an \nedge-dominant system?\n4.\t\nWhich architectural views would you expect to be the most important to \ndocument for a system built under the Metropolis model?\n\n\n27.7  Discussion Questions\n539\n5.\t\nHow would you establish the architecturally significant requirements for \nthe core? Would you use a Quality Attribute Workshop, or would you use \nsomething less structured and more open? Why?\n6.\t\nMetropolis systems are frequently open source. Some organizations that \nmight want to contribute to or build on top of such a system may balk at \nreleasing all of their code to the public. What architectural means might \nyou employ to address this situation?\n7.\t\nChoose your favorite crowdsourced system. Write a testability scenario for \nthis system, and choose a set of testability tactics that you would use for it.\n8.\t\nConstructing and releasing an application on a platform such as the iPhone \nor the Android requires the developer to adhere to certain specifications and \nto pass through certain hoops. Redraw Figure 27.2 to reflect the Apple iP-\nhone ecosystem and the Android ecosystem.\n9.\t\nFind a study that discusses the motivation of Wikipedia contributors. Find \nanother study that discusses the motivation of open source developers. \nCompare the results of these two studies.\n\n\nThis page intentionally left blank \n\n\n541\n28\nEpilogue\nDon’t let it end like this. Tell them I said something.\n—Pancho Villa\nYou’ve arrived at the end of the journey. Nice work.\nWe hope you can find some valuable takeaways from this book. We suggest \nthe list should include the following:\n1.\t\nWhat architecture is, why it’s important, what influences it, and what it \ninfluences. \n2.\t\nThe role that architecture plays in a business, technical, project, and profes-\nsional context. \n3.\t\nThe critically important symbiosis between architecture and quality attri-\nbutes, how to specify quality attribute requirements, and the quick immer-\nsion you’ve had into the dozen or so of the most important QAs. \n4.\t\nHow to capture the architecturally significant requirements for an \narchitecture.\n5.\t\nHow to design an architecture, document it, guide an implementation with \nit, evaluate it to see if it’s a good one for your needs, reverse-engineer it, \nand manage a project around it. \n6.\t\nHow to evaluate an architecture’s cost and benefit, what it means to be \narchitecturally competent, and how to use architecture as the basis for an \nentire software product line. \n7.\t\nArchitectural concepts and patterns for systems on the current technological \nfrontier: edge applications and the cloud.\nFine. Now what?\nIt’s very tempting to end the book with a cheery imperative to go forth and \narchitect great systems. But the truth is, life isn’t that simple. \nThe authors of this book have, collectively, an embarrassingly large num-\nber of years of experience in teaching software architecture. We teach it through \nbooks like this one, in the classroom to students, and in industrial short courses \n\n\n542 \nPart Five  Brave New World\t\n28—Epilogue\nto practicing software architects of all stripes, from “aspiring” to “old hand.” \nAnd often, much too often, we know that after our students conscientiously learn \nwhat we have to offer, they walk into an organization that is not as architecturally \nsavvy as the students now are, and our students have no practical way to put what \nthey have learned into practice. \nMost of you will not be able to dictate an architecture-based philosophy to \nthe projects to which you’ll be assigned, if it’s not already present. You won’t \nbe able to say, “This Agile project needs a lead software architect!” and make it \nstick if the team leaders think that Agile methods won’t permit any overarching \ndesign. You won’t be able to say, “We’re going to include an explicit architecture \nevaluation in our project schedule” and have everyone comply. You won’t be able \nto say, “We’re going to use the Views and Beyond approach and templates for our \narchitecture documentation” and have your directive obeyed.\nLest you feel that all your time absorbing the material in this book was time \nspent for a lost cause, we want to close the book with some advice for carrying \nwhat you’ve learned into your professional setting: \n1.\t\nSpeak the right language. You know by now that architecture is the means \nto an end, and not an end in itself. The decision makers in your organization \ntypically care about the ends, not the means. They care about products, not \nthe architectures of those products. They care about ensuring that the prod-\nucts are competitive in the marketplace. And they care about executing the \norganization’s marketing and business strategy. They may not express their \nconcerns in architecturally significant terms, but rather in market terms that \nyou’ll have to translate. \n2.\t\nSpeak the right language, part 2. Project managers care about reduction \nof technical risk, reliable and realistic scheduling and budgeting, and plan-\nning the production of those products. To the extent that you can justify \na focus on architecture in those terms, you’ll more likely be successful in \ngaining the freedom to carry out some of the practices espoused in this \nbook. And you really can justify this focus: A sound architecture is an \nunparalleled risk reduction strategy, a reliable work estimator, and a good \npredictor of production methods. \n3.\t\nGet involved. One of the best ways to insert architectural concerns into a \nproject is to show its value to stakeholders who don’t often get a chance to \nsee it. Requirements engineers are a case in point. Most often, they meet \nwith customers and users, capture their concerns, write them up, and toss \nthe requirements over the fence (usually a very tall fence) to the designers. \nChallenge this segregation! Try to get involved in the requirements capture \nactivity. Invite yourself to meetings. Say that, as an architect, you want to \nunderstand the problem by hearing the concerns straight from the horse’s \nmouth. This will give you critical exposure to the very stakeholders you’ll \nneed to help with your design, evaluation, and documentation chores. Fur-\nthermore it will give you a chance to add value to the requirements capture \n",
      "page_number": 555
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 563-575)",
      "start_page": 563,
      "end_page": 575,
      "detection_method": "topic_boundary",
      "content": "Epilogue﻿\n543\nprocess. Because you may have a design approach in mind, you may be \nable to offer better quality attribute responses than the customer has in \nmind, and that might make our marketers very happy. Or you may be able \nto spot troublesome requirements early on, and help nudge the customer to \na perfectly adequate but more architecturally palatable QA response. Also, \nyou can take it upon yourself to contact your organization’s marketers. \nThey are often the ones who come up with product concepts. You would \ndo well to learn how they do that, and eventually you could help them by \npointing out useful variations on existing products that could use the same \narchitecture.\n4.\t\nIt’s the economy, stupid. Think in, and couch your arguments in, eco-\nnomic terms. If you think an architectural trade study, or an architecture \ndocument, or an architecture evaluation, or ensuring code compliance with \nthe architecture is a good idea for your project, make a business case for \nit. Pointy-haired bosses in comic strips notwithstanding, managers are re-\nally—trust us here—rational people. But their goals are broad and almost \nalways have to do with economics. You should be able to argue, using \nback-of-the-envelope arithmetic, that (for example) producing an updated \nversion of the architecture document is a worthwhile activity when the \nsystem undergoes a major change. You should be able to argue that activi-\nties undertaken with the new architecture documentation will be much less \nerror-prone (and therefore less expensive) than those same activities under-\ntaken without a guiding architecture. And the effort to keep the documenta-\ntion up to date is much less than the expected savings. You can plug some \nnumbers in a spreadsheet to make this argument. The numbers don’t have \nto be accurate, just reasonable, and they’ll still make your case.\n5.\t\nStart a guerrilla movement. Find like-minded people in your organiza-\ntion and nurture their interest in architecture. Start a brown-bag lunchtime \nreading and discussion group that covers books or book chapters or papers \nor even blogs about architecture. For example, your group could read the \nchapter in this book about architecture competence, and discuss what prac-\ntices you’d like to see your organization adopt, and what it would take to \nadopt them. Or the group could agree on a joint documentation template \nfor architecture, or come up with a set of quality attribute scenarios that \napply across your collective projects. Especially appealing is to come up \nwith a set of patterns that apply to the systems you’re building. Or bring a \nvexing design problem from your individual project, and let the group work \non a written solution to it. Or have the group offer its services as a roving \narchitecture evaluation team to other projects. Your group should meet reg-\nularly and often, and adopt a specific set of tangible goals. The importance \nof an enthusiastic and dedicated leader—you?—who is keen to mature the \norganization’s architecture practices cannot be overstated. Advertise your \n\n\n544 \nPart Five  Brave New World\t\n28—Epilogue\nmeetings, advertise your results, and keep asking more members to join \nyour group.\n6.\t\nRelish small victories. You don’t have to change the world overnight. Ev-\nery improvement you make will put you and your organization in a better \nplace than it would have been otherwise. \nGetting Architecture Reviews into an Organization through \nthe Back Door\nIf you search the web for “code review computer science,” you’ll turn up \nmillions of hits that describe code reviews and the steps that are taken to \nperform them. If you search for “design review computer science,” you’ll \nturn up little that is useful. \nOther disciplines routinely practice and teach design critiques. Search \nfor “design critique” and you will find many hits together with instructions. A \ndesign is a set of decisions of whatever type that attempts to solve a par-\nticular problem, whether an art problem, a user interface design problem, \nor a software problem. Solutions to important design problems should be \nsubject to peer review, just as code should be subject to peer review.\nThere is a wealth of data that points out that the earlier in the life cycle a \nproblem is discovered and fixed, the less the cost of finding and fixing the \nproblem. Design precedes code and so having appropriate design reviews \nseems both intuitively and empirically justified. In addition, the documents \naround the review, both the original design document and also the cri-\ntiques, are valuable learning tools for new developers. In many organiza-\ntions developers switch systems frequently, and so they are constantly \nlearning.\nThis view is not universally shared. A software engineer working in a \nmajor software house tells me that even though the organization aspires to \nwriting and reviewing design documents, it rarely happens. Senior develop-\ners tend to limit their review to a cursory glance. Code reviews, on the other \nhand, are taken quite seriously by the senior developers.\n My software engineer friend offers two possible explanations for this \nstate of affairs:\n1.\t The code review is the last opportunity to affect what is built: \n“review this or live with it.” This explanation assumes that senior \ndevelopers do not believe that the output of design reviews are \nactionable and thus wait to engage until later in the process. \n2.\t The code is more concrete than the design, and is therefore \neasier to assess. This explanation assumes that senior developers \nare incapable of understanding designs. \nI do not find either of these explanations compelling, but I am unable to \ncome up with a better one.\n\n\n﻿\n545\nWhat to do?\nWhat this software engineer did is to look for a surrogate process where \na design review could be surreptitiously performed. This individual noticed \nthat when the organization did code reviews, questions such as “Why did \nyou do that?” were frequently asked. The result of such questions was a \ndiscussion of rationale. So the individual would code up a solution to a \nproblem, submit it to a code review, and wait for the question that would \nlead to the rationale discussion.\nA design review is a review where design decisions are presented \ntogether with their rationale. Frequently, design alternatives are explored. \nWhether this is done under the name of code review or design review is not \nnearly as important as getting it done. \nOf course, my friend’s surreptitious approach has drawbacks. It is ineffi-\ncient to code a solution that may have to be thrown away. Also, embedding \ndesign reviews into code reviews means that the designs and reviews end \nup being embedded in the code review tool, making it difficult to search this \ntool for design and design rationale. But these inefficiencies are dwarfed by \nthe inefficiency of pursuing an incorrect solution to a particular problem.\n—LB\nThe practice and discipline of architecture for software systems has come of \nage. You can be proud of joining a profession that has always strived, and is still \nstriving, to be more disciplined, more reliable, more productive, and more effi-\ncient, to produce systems that improve the lives of their stakeholders.\nWith that thought, now it’s time for the cheery book-ending imperative: Go \nforth and architect great systems. Your predecessors have designed systems that \nhave changed the world. It’s your turn.\n\n\nThis page intentionally left blank \n\n\n547\nReferences\n[Abrahamsson 10] P. Abrahamsson, M.A. Babar, and P. Kruchten. “Agility and Ar-\nchitecture: Can They Coexist?” IEEE Software, Vol. 27, No. 2, (March-April \n2010), pp. 16-22.\n[AdvBuilder 10] Java Adventure Builder Reference Application. https://java.net/\nprojects/adventurebuilder/pages/home\n[Anastasopoulos 00] M. Anastasopoulos and C. Gacek. “Implementing Product Line \nVariabilities” (IESE-Report No. 089.00/E, V1.0). Kaiserslautern, Germany: \nFraunhofer Institut Experimentelles Software Engineering, 2000.\n[Anderson 08] Ross Anderson. Security Engineering: A Guide to Building  \nDependable Distributed Systems, Second Edition. Wiley, 2008.\n[Argote 07]. L. Argote and G. Todorova. International Review of Industrial and \nOrganizational Psychology. John Wiley & Sons, Ltd., 2007. \n[Avižienis 04] Algirdas Avižienis, Jean-Claude Laprie, Brian Randell, and Carl \nLandwehr. “Basic Concepts and Taxonomy of Dependable and Secure Comput-\ning,” IEEE Transactions on Dependable and Secure Computing, Vol. 1, No. 1 \n(January 2004), pp. 11-33.\n[Bachmann 05] F. Bachmann and P. Clements. “Variability in Software Product \nLines,” CMU/SEI-2005-TR-012, 2005.\n[Bachmann 07] Felix Bachmann, Len Bass, and Robert Nord. “Modifiability  \nTactics,” CMU/SEI-2007-TR-002, September 2007.\n[Bachmann 11] F. Bachmann. “Give the Stakeholders What They Want: Design Peer \nReviews the ATAM Style,” Crosstalk, November/December 2011, pp. 8-10, \nhttp://www.crosstalkonline.org/storage/issue- \narchives/2011/201111/201111-Bachmann.pdf\n[Barbacci 03] M. Barbacci, R. Ellison, A. Lattanze, J. Stafford, C. Weinstock, and \nW. Wood. “Quality Attribute Workshops (QAWs), Third Edition,” CMU/SEI-\n2003-TR-016, http://www.sei.cmu.edu/reports/03tr016.pdf\n[Bass 03] L. Bass and B.E. John. “Linking Usability to Software Architecture  \nPatterns through General Scenarios,” Journal of Systems and Software 66(3), \npp. 187-197.\n[Bass 08] Len Bass, Paul Clements, Rick Kazman, and Mark Klein. “Models for \nEvaluating and Improving Architecture Competence,” CMU/SEI-2008-TR-006, \nMarch 2008, http://www.sei.cmu.edu/library/abstracts/reports/08tr006.cfm\n\n\n548 \nReferences\t\n[Baudry 03] B. Baudry, Yves Le Traon, Gerson Sunyé, and Jean-Marc Jézéquel. \n“Measuring and Improving Design Patterns Testability,” Proceedings of the \nNinth International Software Metrics Symposium (METRICS ’03), 2003.\n[Baudry 05] B. Baudry and Y. Le Traon. “Measuring Design Testability of a UML \nClass Diagram,” Information & Software Technology 47(13)(October 2005), \npp. 859-879.\n[Beck 04] Kent Beck and Cynthia Andres. Extreme Programming Explained: Em-\nbrace Change, Second Edition. Addison-Wesley, 2004.\n[Beizer 90] B. Beizer. Software Testing Techniques, Second Edition. International \nThomson Computer Press, 1990.\n[Bellcore 98] Bell Communications Research. GR-1230-CORE, SONET Bidirec-\ntional Line-Switched Ring Equipment Generic Criteria. 1998.\n[Bellcore 99] Bell Communications Research. GR-1400-CORE, SONET Dual-Fed \nUnidirectional Path Switched Ring (UPSR) Equipment Generic Criteria. 1999.\n[Benkler 07] Y. Benkler. The Wealth of Networks: How Social Production Trans-\nforms Markets and Freedom. Yale University Press, 2007.\n[Bertolino 96a] Antonia Bertolino and Lorenzo Strigini. “On the Use of Testability \nMeasures for Dependability Assessment,” IEEE Transactions on Software \nEngineering, Vol. 22, No. 2 (February 1996), pp. 97-108.\n[Bertolino 96b] A. Bertolino and P. Inverardi. “Architecture-Based Software Test-\ning,” in Proceedings of the Second International Software Architecture \nWorkshop (ISAW-2), L. Vidal, A. Finkelstain, G. Spanoudakis, and A.L. Wolf, \neds. Joint Proceedings of the SIGSOFT ’96 Workshops, San Francisco, October \n1996, ACM Press.\n[Biffl 10] S. Biffl, A. Aurum, B. Boehm, H. Erdogmus, and P. Grunbacher, eds.  \nValue-Based Software Engineering. Springer, 2010.\n[Binder 94] R.V. Binder. “Design for Testability in Object-Oriented Systems,” \nCACM 37(9), pp. 87-101, 1994.\n[Boehm 78] B.W. Boehm, J.R. Brown, J.R. Kaspar, M.L. Lipow, and G. MacCleod. \nCharacteristics of Software Quality. American Elsevier, 1978. \n[Boehm 81] B. Boehm. Software Engineering Economics. Prentice-Hall, 1981.\n[Boehm 91] Barry Boehm. “Software Risk Management: Principles and Practices,” \nIEEE Software, Vol. 8, No. 1, pp. 32-41, January 1991.\n[Boehm 04] B. Boehm and R. Turner. Balancing Agility and Discipline: A Guide \nfor the Perplexed. Addison-Wesley, 2004.\n[Boehm 07] B. Boehm, R. Valerdi, and E. Honour. “The ROI of Systems Engineer-\ning: Some Quantitative Results for Software Intensive Systems,” Systems Engi-\nneering, Vol. 11, No. 3, pp. 221-234. \n[Boehm 10] B. Boehm, J. Lane, S. Koolmanojwong, and R. Turner. “Architected \nAgile Solutions for Software-Reliant Systems,” Technical Report USC-\nCSSE-2010-516, 2010.\n[Booch 11] Grady Booch. “An Architectural Oxymoron,” podcast available at http://\nwww.computer.org/portal/web/computingnow/onarchitecture. Retrieved January \n21, 2011.\n\n\nReferences\n549\n[Bosch 00] J. Bosch. “Organizing for Software Product Lines,” Proceedings of the \n3rd International Workshop on Software Architectures for Product Families \n(IWSAPF-3), pp. 117-134. Las Palmas de Gran Canaria, Spain, March 15-17, \n2000. Springer, 2000.\n[Bouwers 10] E. Bouwers and A. van Deursen. “A Lightweight Sanity Check for Im-\nplemented Architectures,” IEEE Software 27(4), July/August 2010, pp. 44-50.\n[Bredemeyer 11] D. Bredemeyer and R. Malan. “Architect Competencies: What You \nKnow, What You Do and What You Are,” http://www.bredemeyer.com/ \nArchitect/ArchitectSkillsLinks.htm\n[Brewer 12] E. Brewer. “CAP Twelve Years Later: How the ‘Rules’ Have Changed,” \nIEEE Computer, February 2012, pp. 23-29.\n[Brown 10] N. Brown, R. Nord, and I. Ozkaya. “Enabling Agility Through Architec-\nture,” Crosstalk, November/December 2010, pp. 12-17.\n[Brownsword 96] Lisa Brownsword and Paul Clements. “A Case Study in Successful \nProduct Line Development,” Technical Report CMU/SEI-96-TR-016, October \n1996.\n[Brownsword 04] Lisa Brownsword, David Carney, David Fisher, Grace Lewis, \nCraig Meterys, Edwin Morris, Patrick Place, James Smith, and Lutz Wrage. \n“Current Perspectives on Interoperability,” CMU/SEI-2004-TR-009, http://\nwww.sei.cmu.edu/reports/04tr009.pdf\n[Bruntink 06] Magiel Bruntink and Arie van Deursen. “An Empirical Study into \nClass Testability,” Journal of Systems and Software 79(9)(2006),  \npp. 1219-1232.\n[Buschmann 96] Frank Buschmann, Regine Meunier, Hans Rohnert, Peter Sommer-\nlad, and Michael Stal. Pattern-Oriented Software Architecture Volume 1: A \nSystem of Patterns. Wiley, 1996.\n[Cai 11] Yuanfang Cai, Daniel Iannuzzi, and Sunny Wong. “Leveraging Design \nStructure Matrices in Software Design Education,” Conference on Software \nEngineering Education and Training 2011, pp. 179-188.\n[Cappelli 12] Dawn M. Cappelli, Andrew P. Moore, and Randall F. Trzeciak. The \nCERT Guide to Insider Threats: How to Prevent, Detect, and Respond to \nInformation Technology Crimes (Theft, Sabotage, Fraud). Addison-Wesley, \n2012.\n[Carriere 10] J. Carriere, R. Kazman, and I. Ozkaya. “A Cost-Benefit Framework for \nMaking Architectural Decisions in a Business Context,” Proceedings of 32nd \nInternational Conference on Software Engineering (ICSE 32), Capetown, \nSouth Africa, May 2010.\n[Cataldo 07] M. Cataldo, M. Bass, J. Herbsleb, and L. Bass. “On Coordination \nMechanisms in Global Software Development,” Proceedings Second IEEE \nInternational Conference on Global Software Development, 2007.\n[Chandran 10] S. Chandran, A. Dimov, and S. Punnekkat. “Modeling Uncertainties \nin the Estimation of Software Reliability—A Pragmatic Approach,” Fourth \nIEEE International Conference on Secure Software Integration and  \nReliability Improvement, 2010.\n\n\n550 \nReferences\t\n[Chang 06] F. Chang, J. Dean, S. Ghemawat, W. Hsieh, et al. “Bigtable: A Distrib-\nuted Storage System for Structured Data,” Proceedings Operating Systems \nDesign and Implementation, 2006, http://research.google.com/archive/ \nbigtable.html\n[Chen 10] H.-M. Chen, R. Kazman, and O. Perry. “From Software Architecture \nAnalysis to Service Engineering: An Empirical Study of Enterprise SOA Imple-\nmentation,” IEEE Transactions on Services Computing 3(2)(April-June 2010), \npp. 145-160.\n[Chidamber 94] S. Chidamber and C. Kemerer. “A Metrics Suite for Object Oriented \nDesign,” IEEE Transactions on Software Engineering, Vol. 20, No. 6 (June \n1994).\n[Clements 01a] P. Clements and L. Northrop. Software Product Lines.  \nAddison-Wesley, 2001.\n[Clements 01b] P. Clements, R. Kazman, and M. Klein. Evaluating Software  \nArchitectures. Addison-Wesley, 2001.\n[Clements 07] P. Clements, R. Kazman, M. Klein, D. Devesh, S. Reddy, and P. \nVerma. “The Duties, Skills, and Knowledge of Software Architects,” Proceed-\nings of the Working IEEE/IFIP Conference on Software Architecture, 2007.\n[Clements 10a] Paul Clements, Felix Bachmann, Len Bass, David Garlan, James  \nIvers, Reed Little, Paulo Merson, Robert Nord, and Judith Stafford. Document-\ning Software Architectures: Views and Beyond, Second Edition.  \nAddison-Wesley, 2010.\n[Clements 10b] Paul Clements and Len Bass. “Relating Business Goals to Ar-\nchitecturally Significant Requirements for Software Systems,” CMU/SEI-\n2010-TN-018, May 2010.\n[Clements 10c] P. Clements and L. Bass. “The Business Goals Viewpoint,” IEEE \nSoftware 27(6)(November-December 2010), pp. 38-45.\n[Cockburn 04] Alistair Cockburn. Crystal Clear: A Human-Powered Methodology \nfor Small Teams. Addison-Wesley, 2004. \n[Conway 68] Melvin E. Conway. “How Do Committees Invent?” Datamation,  \nVol. 14, No. 4 (1968), pp. 28-31.\n[Coplein 10] J. Coplein and G. Bjornvig. Lean Architecture for Agile Software \nDevelopment. Wiley, 2010.\n[Cunningham 92] W. Cunningham. “The Wycash Portfolio Management System,” \nin Addendum to the Proceedings of Object-Oriented Programming Systems, \nLanguages, and Applications (OOPSLA), pp. 29-30, ACM Press, 1992.\n[CWE 12] The Common Weakness Enumeration. http://cwe.mitre.org/\n[Dean 04] Jeffrey Dean and Sanjay Ghemawat. “MapReduce: Simplified Data  \nProcessing on Large Clusters,” Proceedings Operating System Design and \nImplementation, 1994, http://research.google.com/archive/mapreduce.html\n[Dijkstra 68] E.W. Dijkstra. “The Structure of the ‘THE’-Multiprogramming \nSystem,” Communications of the ACM 11(5), pp. 341-346.\n[Dix 04] Alan Dix, Janet Finlay, Gregory Abowd, and Russell Beale. Human-\nComputer Interaction, Third Edition. Prentice Hall, 2004.\n\n\nReferences\n551\n[Douglass 99] Bruce Douglass. Real-Time Design Patterns: Robust Scalable \nArchitecture for Real-Time Systems. Addison-Wesley, 1999.\n[Dutton 84] J.M. Dutton and A. Thomas. “Treating Progress Functions as a \nManagerial Opportunity,” Academy of Management Review 9 (1984),  \npp. 235-247.\n[Eickelman 96] N. Eickelman and D. Richardson. “What Makes One Software \nArchitecture More Testable Than Another?” in Proceedings of the Second \nInternational Software Architecture Workshop (ISAW-2), L. Vidal, A. \nFinkelstein, G. Spanoudakis, and A.L. Wolf, eds., Joint Proceedings of the \nSIGSOFT ’96 Workshops, San Francisco, October 1996, ACM Press.\n[EOSAN 07] “WP 8.1.4—Define Methodology for Validation within OATA: \nArchitecture Tactics Assessment Process,” http://www.eurocontrol.int/valfor/\ngallery/content/public/OATA-P2-D8.1.4-01%20DMVO%20Architecture%20\nTactics%20Assessment%20Process.pdf\n[FAA 00] “System Safety Handbook,” http://www.faa.gov/library/manuals/aviation/\nrisk_management/ss_handbook/\n[Fairbanks 10] G. Fairbanks. Just Enough Software Architecture. Marshall & \nBrainerd, 2010.\n[Feiler 06] P. Feiler, R.P. Gabriel, J. Goodenough, R. Linger, T. Longstaff, R. \nKazman, M. Klein, L. Northrop, D. Schmidt, K. Sullivan, and K. Wallnau. \nUltra-Large-Scale Systems: The Software Challenge of the Future, http://\nwww.sei.cmu.edu/library/assets/ULS_Book20062.pdf\n[Fiol 85] C.M. Fiol and M.A. Lyles. “Organizational Learning,” Academy of \nManagement Review 10(4)(1985), p. 803.\n[Freeman 09] Steve Freeman and Nat Pryce. Growing Object-Oriented Software, \nGuided by Tests. Addison-Wesley, 2009. \n[Gacek 95] Cristina Gacek, Ahmed Abd-Allah, Bradford Clark, and Barry Boehm. \n“On the Definition of Software System Architecture,” USC/CSE-95-TR-500, \nApril 1995.\n[Gagliardi 09] M. Gagliardi, W. Wood, J. Klein, and J. Morley. “A Uniform \nApproach for System of Systems Architecture Evaluation,” Crosstalk, Vol. 22, \nNo. 3 (March/April 2009), pp. 12-15.\n[Gamma 94] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: \nElements of Reusable Object-Oriented Software. Addison-Wesley, 1994.\n[Garlan 93] D. Garlan and M. Shaw. “An Introduction to Software Architecture,” in \nAmbriola and Tortola, eds., Advances in Software Engineering & Knowledge \nEngineering, Vol. II. World Scientific Pub. Co., 1993, pp. 1-39.\n[Garlan 95] David Garlan, Robert Allen, and John Ockerbloom. “Architectural \nMismatch or Why it’s hard to build systems out of existing parts,” ICSE 1995. \n17th International Conference on Software Engineering, April 1995. \n[Gilbert 07] T. Gilbert. Human Competence: Engineering Worthy Performance. \nPfeiffer, Tribute Edition, 2007.\n[Gokhale 05] S. Gokhale, J. Crigler, W. Farr, and D. Wallace. “System Availability \nAnalysis Considering Hardware/Software Failure Severities,” Proceedings of \nthe 29th Annual IEEE/NASA Software Engineering Workshop (SEW ’05), \nGreenbelt, MD, April 2005, IEEE 2005.\n\n\n552 \nReferences\t\n[Gorton 10] Ian Gorton. Essential Software Architecture, Second Edition. \nSpringer, 2010.\n[Graham 07] T.C.N. Graham, R. Kazman, and C. Walmsley. “Agility and Experimen-\ntation: Practical Techniques for Resolving Architectural Tradeoffs,” Proceed-\nings of the 29th International Conference on Software Engineering (ICSE \n29), Minneapolis, MN, May 2007.\n[Gray 93] Jim Gray and Andreas Reuter. Distributed Transaction Processing: Con-\ncepts and Techniques. Morgan Kaufmann, 1993.\n[Grinter 99] Rebecca E. Grinter. “Systems Architecture: Product Designing and \nSocial Engineering,” in Proceedings of the International Joint Conference \non Work Activities Coordination and Collaboration (WACC ’99), Dimitrios \nGeorgakopoulos, Wolfgang Prinz, and Alexander L. Wolf, eds. ACM, 1999,  \npp. 11-18.\n[Hamm 04] “Linus Torvalds’ Benevolent Dictatorship,” BusinessWeek, Au-\ngust 18, 2004, http://www.businessweek.com/technology/content/aug2004/\ntc20040818_1593.htm\n[Hamming 80] R.W. Hamming. Coding and Information Theory. Prentice Hall, \n1980.\n[Hanmer 07] Robert Hanmer. Patterns for Fault Tolerant Software, Wiley, 2007.\n[Harms 10] R. Harms and M. Yamartino. “The Economics of the Cloud,” http:// \neconomics.uchicago.edu/pdf/Harms_110111.pdf\n[Hartman 10] Gregory Hartman. “Attentiveness: Reactivity at Scale,” CMU-\nISR-10-111, 2010.\n[Hiltzik 00] M. Hiltzik. Dealers of Lightning: Xerox PARC and the Dawn of the \nComputer Age. Harper Business, 2000.\n[Hoffman 00] Daniel M. Hoffman and David M. Weiss. Software Fundamentals: \nCollected Papers by David L. Parnas. Addison-Wesley, 2000.\n[Hofmeister 00] Christine Hofmeister, Robert Nord, and Dilip Soni. Applied Soft-\nware Architecture. Addison-Wesley, 2000.\n[Hofmeister 07] Christine Hofmeister, Philippe Kruchten, Robert L. Nord, Henk \nObbink, Alexander Ran, and Pierre America. “A General Model of Software \nArchitecture Design Derived from Five Industrial Approaches,” Journal of Sys-\ntems and Software, Vol. 80, No. 1 (January 2007), pp. 106-126.\n[Howard 04] Michael Howard. “Mitigate Security Risks by Minimizing the Code \nYou Expose to Untrusted Users,” MSDN Magazine, http://msdn.microsoft.com/\nen-us/magazine/cc163882.aspx\n[IEEE 94] “IEEE Standard for Software Safety Plans,” STD-1228-1994, http:// \nstandards.ieee.org/findstds/standard/1228-1994.html\n[IEEE 11] “IEEE Guide—Adoption of the Project Management Institute (PMI) \nStandard: A Guide to the Project Management Body of Knowledge (PMBOK \nGuide), Fourth Edition,” http://www.projectsmart.co.uk/pmbok.html\n[IETF 04] Internet Engineering Task Force. “RFC 3746, Forwarding and Control \nElement Separation (ForCES) Framework,” 2004. \n[IETF 05] Internet Engineering Task Force. “RFC 4090, Fast Reroute Extensions to \nRSVP-TE for LSP Tunnels,” 2005.\n\n\nReferences\n553\n[IETF 06a] Internet Engineering Task Force. “RFC 4443, Internet Control Message \nProtocol (ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,” \n2006.\n[IETF 06b] Internet Engineering Task Force. “RFC 4379, Detecting Multi-Protocol \nLabel Switched (MPLS) Data Plane Failures,” 2006. \n[INCOSE 05] International Council on Systems Engineering. “System Engineering \nCompetency Framework 2010-0205,” http://www.incose.org/ProductsPubs/\nproducts/competenciesframework.aspx\n[ISO 11] International Organization for Standardization. “ISO/IEC 25010: 2011 Sys-\ntems and software engineering—Systems and software Quality Requirements \nand Evaluation (SQuaRE)—System and software quality models.” \n[Jacobson 97] I. Jacobson, M. Griss, and P. Jonsson. Software Reuse: Architecture, \nProcess, and Organization for Business Success. Addison-Wesley, 1997.\n[Kanwal 10] F. Kanwal, K. Junaid, and M.A. Fahiem. “A Hybrid Software Archi-\ntecture Evaluation Method for FDD—An Agile Process Mode,” 2010 Interna-\ntional Conference on Computational Intelligence and Software Engineering \n(CiSE), December 2010, pp. 1-5.\n[Kaplan 92] R. Kaplan and D. Norton. “The Balanced Scorecard: Measures That \nDrive Performance,” Harvard Business Review, January/February 1992,  \npp. 71-79.\n[Karat 94] Claire Marie Karat. “A Business Case Approach to Usability Cost Justi-\nfication,” in Cost-Justifying Usability, R. Bias and D. Mayhew, eds. Academic \nPress, 1994.\n[Kazman 94] Rick Kazman, Len Bass, Mike Webb, and Gregory Abowd. “SAAM: \nA Method for Analyzing the Properties of Software Architectures,” in Proceed-\nings of the 16th International Conference on Software Engineering (ICSE \n’94). Los Alamitos, CA. IEEE Computer Society Press, pp. 81-90. \n[Kazman 99] R. Kazman and S.J. Carriere. “Playing Detective: Reconstructing Soft-\nware Architecture from Available Evidence,” Automated Software Engineering \n6(2)(April 1999), pp. 107-138.\n[Kazman 01] R. Kazman, J. Asundi, and M. Klein. “Quantifying the Costs and \nBenefits of Architectural Decisions,” Proceedings of the 23rd International \nConference on Software Engineering (ICSE 23), Toronto, Canada, May 2001, \npp. 297-306.\n[Kazman 02] R. Kazman, L. O’Brien, and C. Verhoef. “Architecture Reconstruc-\ntion Guidelines, Third Edition,” CMU/SEI Technical Report, CMU/SEI-\n2002-TR-034, 2002.\n[Kazman 04] R. Kazman, P. Kruchten, R. Nord, and J. Tomayko. “Integrating Soft-\nware-Architecture-Centric Methods into the Rational Unified Process,” Techni-\ncal Report CMU/SEI-2004-TR-011, July 2004, http://www.sei.cmu.edu/library/\nabstracts/reports/04tr011.cfm\n[Kazman 05] Rick Kazman and Len Bass. “Categorizing Business Goals for Soft-\nware Architectures,” CMU/SEI-2005-TR-021, December 2005.\n[Kazman 09] R. Kazman and H.-M. Chen. “The Metropolis Model: A New Logic for \nthe Development of Crowdsourced Systems,” Communications of the ACM, \nJuly 2009, pp. 76-84.\n\n\n554 \nReferences\t\n[Kircher 03] Michael Kircher and Prashant Jain. Pattern-Oriented Software Archi-\ntecture Volume 3: Patterns for Resource Management. Wiley, 2003.\n[Klein 10] J. Klein and M. Gagliardi. “A Workshop on Analysis and Evaluation of \nEnterprise Architectures,” CMU/SEI-2010-TN-023, http://www.sei.cmu.edu/\nreports/10tn023.pdf\n[Klein 93] M. Klein, T. Ralya, B. Pollak, R. Obenza, and M. Gonzalez Harbour. A \nPractitioner’s Handbook for Real-Time Systems Analysis. Kluwer Academic, \n1993.\n[Koziolet 10] H. Koziolek. “Performance Evaluation of Component-Based Software \nSystems: A Survey,” Performance Evaluation 67(8)(August 2010).\n[Kruchten 95] P.B. Kruchten. “The 4+1 View Model of Architecture,” IEEE Soft-\nware, Vol. 12, No. 6 (November 1995), pp. 42-50.\n[Kruchten 03] Philippe Kruchten. The Rational Unified Process: An Introduction, \nThird Edition. Addison-Wesley, 2003.\n[Kruchten 04] Philippe Kruchten. “An Ontology of Architectural Design Decisions,” \nin Jan Bosch, ed., Proceedings of the 2nd Workshop on Software Variability \nManagement, Groningen, NL, Dec. 3-4, 2004.\n[Kumar 10a] K. Kumar and TV Prabhakar. “Pattern-Oriented Knowledge Model for \nArchitecture Design,” in Pattern Languages of Programs Conference 2010, \nOctober 15-18, 2010, Reno/Tahoe, Nevada.\n[Kumar 10b] Kiran Kumar and TV Prabhakar. “Design Decision Topology Model \nfor Pattern Relationship Analysis,” Asian Conference on Pattern Languages of \nPrograms 2010, March 15-17, 2010, Tokyo, Japan.\n[Ladas 09] Corey Ladas. Scrumban: Essays on Kanban Systems for Lean Soft-\nware Development. Modus Cooperandi Press, 2009.\n[Lattanze 08] Tony Lattanze. Architecting Software Intensive Systems: A Practi-\ntioner’s Guide. Auerbach Publications, 2008.\n[Le Traon 97] Y. Le Traon and C. Robach. “Testability Measurements for Data Flow \nDesigns,” Proceedings of the 4th International Symposium on Software Met-\nrics (METRICS ’97), pp. 91-98. November 1997, Washington, D.C.\n[Leveson 04] Nancy G. Leveson. “The Role of Software in Spacecraft Accidents,” \nJournal of Spacecraft and Rockets 41(4)(July 2004), pp. 564-575.\n[Leveson 11] Nancy G. Leveson. Engineering a Safer World: Systems Thinking \nApplied to Safety. MIT Press, 2011.\n[Levitt 88] B. Levitt and J. March. “Organizational Learning,” Annual Review of \nSociology 14 (1988), pp. 319-340.\n[Liu 00] Jane Liu. Real-Time Systems. Prentice Hall, 2000.\n[Liu 09] Henry Liu. Software Performance and Scalability: A Quantitative  \nApproach. Wiley, 2009. \n[Luftman 00] J. Luftman. “Assessing Business Alignment Maturity,” Communica-\ntions of AIS, Vol. 4, No. 14, 2000.\n[Lyons 62] R. E. Lyons and W. Vanderkulk. “The Use of Triple-Modular Redun-\ndancy to Improve Computer Reliability,” IBM J. Res. Dev. 6(2)(April 1962), \npp. 200-209.\n\n\nReferences\n555\n[MacCormack 06] A. MacCormack, J. Rusnak, and C. Baldwin. “Exploring the \nStructure of Complex Software Designs: An Empirical Study of Open Source \nand Proprietary Code,” Management Science 52(7)(July 2006), pp. 1015-1030.\n[MacCormack 10] A. MacCormack, C. Baldwin, and J. Rusnak. “The Architecture \nof Complex Systems: Do Core-Periphery Structures Dominate?” MIT Sloan \nResearch Paper No. 4770-10, http://www.hbs.edu/research/pdf/10-059.pdf\n[Malan 00] Ruth Malan and Dana Bredemeyer. “Creating an Architectural Vision: \nCollecting Input,” http://www.bredemeyer.com/pdf_files/vision_input.pdf, July \n25, 2000.\n[Maranzano 05] Joseph F. Maranzano, Sandra A. Rozsypal, Gus H. Zimmerman, \nGuy W. Warnken, Patricia E. Wirth, and David M. Weiss. “Architecture Re-\nviews: Practice and Experience,” IEEE Software, March/April 2005, pp. 34-43.\n[Mavis 02] D.G. Mavis. “Soft Error Rate Mitigation Techniques for Modern Mi-\ncrocircuits,” 40th Annual Reliability Physics Symposium Proceedings, April \n2002, Dallas, TX. IEEE, 2002. \n[McCall 77] J.A. McCall, P.K. Richards, and G.F. Walters. Factors in Software \nQuality. Griffiths Air Force Base, N.Y. : Rome Air Development Center Air \nForce Systems Command.\n[McGregor 11] John D. McGregor, J. Yates Monteith, and Jie Zhang. “Quantifying \nValue in Software Product Line Design,” in Proceedings of the 15th Interna-\ntional Software Product Line Conference, Volume 2 (SPLC ’11), Ina Schae-\nfer, Isabel John, and Klaus Schmid, eds.\n[Mettler 91] R. Mettler. “Frederick C. Lindvall,” in Memorial Tributes: National \nAcademy of Engineering, Volume 4, pp. 213-216. National Academy of Engi-\nneering, 1991. \n[Moore 03] M. Moore, R. Kazman, M. Klein, and J. Asundi. “Quantifying the Value \nof Architecture Design Decisions: Lessons from the Field,” Proceedings of the \n25th International Conference on Software Engineering (ICSE 25), Portland, \nOR, May 2003, pp. 557-562.\n[Morelos-Zaragoza 06] R.H. Morelos-Zaragoza. The Art of Error Correcting Cod-\ning, Second Edition. Wiley, 2006.\n[Muccini 03] H. Muccini, A. Bertolino, and P. Inverardi. “Using Software Architec-\nture for Code Testing,” IEEE Transactions on Software Engineering 30(3), \npp. 160-171.\n[Muccini 07] H. Muccini. “What Makes Software Architecture-Based Testing Distin-\nguishable,” in Proc. Sixth Working IEEE/IFIP Conference on Software Archi-\ntecture, WICSA 2007, Mumbai, India, January 2007.\n[Murphy 01] G. Murphy, D. Notkin, and K. Sullivan. “Software Reflexion Models: \nBridging the Gap between Design and Implementation,” IEEE Transactions on \nSoftware Engineering, Vol. 27, pp. 364-380, 2001.\n[Nielsen 08] Jakob Nielsen. “Usability ROI Declining, But Still Strong,” http://www.\nuseit.com/alertbox/roi.html\n[NIST 02] National Institute of Standards and Technology. “Security Requirements \nFor Cryptographic Modules,” FIPS Pub. 140-2, http://csrc.nist.gov/publications/\nfips/fips140-2/fips1402.pdf\n",
      "page_number": 563
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 576-583)",
      "start_page": 576,
      "end_page": 583,
      "detection_method": "topic_boundary",
      "content": "556 \nReferences\t\n[NIST 04] National Institute of Standards and Technology. “Standards for Security \nCategorization of Federal Information Systems,” FIPS Pub. 199, http://csrc.nist.\ngov/publications/fips/fips199/FIPS-PUB-199-final.pdf\n[NIST 06] National Institute of Standards and Technology. “Minimum Security Re-\nquirements for Federal Information and Information Systems,” FIPS Pub. 200, \nhttp://csrc.nist.gov/publications/fips/fips200/FIPS-200-final-march.pdf\n[NIST 09] National Institute of Standards and Technology. “800-53 v3 Recom-\nmended Security Controls for Federal Information Systems and Organizations,” \nAugust 2009, http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-\nrev3-final.pdf\n[Nord 04] R. Nord, J. Tomayko, and R. Wojcik. “Integrating Software Archi-\ntecture-Centric Methods into Extreme Programming (XP),” CMU/SEI-\n2004-TN-036. Software Engineering Institute, Carnegie Mellon University, \n2004.\n[Nygard 07] Michael T. Nygard. Release It!: Design and Deploy \nProduction-Ready Software. Pragmatic Programmers, 2007.\n[Obbink 02] H. Obbink, P. Kruchten, W. Kozaczynski, H. Postema, A. Ran, L. Dom-\ninic, R. Kazman, R. Hilliard, W. Tracz, and E. Kahane. “Software Architecture \nReview and Assessment (SARA) Report, Version 1.0,” 2002, http://pkruchten.\nwordpress.com/architecture/SARAv1.pdf/\n[O’Brien 03] L. O’Brien and C. Stoermer. “Architecture Reconstruction Case Study,” \nCMU/SEI Technical Note, CMU/SEI-2003-TN-008, 2003.\n[ODUSD 08] Office of the Deputy Under Secretary of Defense for Acquisition and \nTechnology. “Systems Engineering Guide for Systems of Systems, Version 1.0,” \n2008, http://www.acq.osd.mil/se/docs/SE-Guide-for-SoS.pdf\n[Palmer 02] Stephen Palmer and John Felsing. A Practical Guide to \nFeature-Driven Development. Prentice Hall, 2002.\n[Parnas 72] D.L. Parnas. “On the Criteria to Be Used in Decomposing Systems into \nModules,” Communications of the ACM 15(12)(December 1972).\n[Parnas 74] D. Parnas. “On a ‘Buzzword’: Hierarchical Structure,” Proceedings \nIFIP Congress 74, pp. 336-339. North Holland Publishing Company, 1974.\n[Parnas 76] D.L. Parnas. “On the Design and Development of Program Families,” \nIEEE Transactions on Software Engineering, SE-2, 1 (March 1976), pp. 1-9.\n[Parnas 79] D. Parnas. “Designing Software for Ease of Extension and Contraction,” \nIEEE Transactions on Software Engineering, SE-5, 2 (1979), pp. 128-137.\n[Parnas 95] David Parnas and Jan Madey. “Functional Documents for Computer \nSystems,” chapter in Science of Computer Programming. Elsevier, 1995.\n[Paulish 02] Daniel J. Paulish. Architecture-Centric Software Project Manage-\nment: A Practical Guide. Addison-Wesley, 2002.\n[Pena 87] William Pena. Problem Seeking: An Architectural Programming \nPrimer. AIA Press, 1987.\n[Perry 92] Dewayne E. Perry and Alexander L. Wolf. “Foundations for the Study of \nSoftware Architecture,” SIGSOFT Softw. Eng. Notes 17(4)(October 1992),  \npp. 40-52.\n[Pettichord 02] B. Pettichord. “Design for Testability,” Pacific Northwest Software \nQuality Conference, Portland, Oregon, October 2002.\n\n\nReferences\n557\n[Powel Douglass 99] B. Powel Douglass. Doing Hard Time: Developing Real-Time \nSystems with UML, Objects, Frameworks, and Patterns. Addison-Wesley, \n1999.\n[Sangwan 08] Raghvinder Sangwan, Colin Neill, Matthew Bass, and Zakaria El \nHouda. “Integrating a Software Architecture-Centric Method into Object- \nOriented Analysis and Design,” Journal of Systems and Software, Vol. 81, No. \n5 (May 2008), pp. 727-746.\n[Schmerl 06] B. Schmerl, J. Aldrich, D. Garlan, R. Kazman, and H. Yan. “Discov-\nering Architectures from Running Systems,” IEEE Transactions on Software \nEngineering 32(7)(July 2006), pp. 454-466.\n[Schmidt 00] Douglas Schmidt, M. Stal, H. Rohnert, and F. Buschmann.  \nPattern-Oriented Software Architecture: Patterns for Concurrent and Net-\nworked Objects. Wiley, 2000.\n[Schmidt 10] Klaus Schmidt. High Availability and Disaster Recovery: Concepts, \nDesign, Implementation. Springer, 2010.\n[Schneier 96] B. Schneier. Applied Cryptography. Wiley, 1996.\n[Schneier 08] Bruce Schneier. Schneier on Security. Wiley, 2008.\n[Schwaber 04] Ken Schwaber. Agile Project Management with Scrum. Microsoft \nPress, 2004.\n[Scott 09] James Scott and Rick Kazman. “Realizing and Refining Architectural \nTactics: Availability,” Technical Report CMU/SEI-2009-TR-006, August 2009.\n[Seacord 05] Robert Seacord. Secure Coding in C and C++. Addison-Wesley, \n2005.\n[SEI 12] Software Engineering Institute. “A Framework for Software Product Line \nPractice, Version 5.0,” http://www.sei.cmu.edu/productlines/frame_report/ \nPL.essential.act.htm\n[Shaw 94] Mary Shaw. “Procedure Calls Are the Assembly Language of Software \nInterconnections: Connectors Deserve First-Class Status,” Carnegie Mellon \nUniversity Technical Report, 1994, http://repository.cmu.edu/cgi/viewcontent.\ncgi?article=1234&context=sei\n[Shaw 95] Mary Shaw. “Beyond Objects: A Software Design Paradigm Based on \nProcess Control,” ACM Software Engineering Notes, Vol. 20, No. 1 (January \n1995), pp. 27-38.\n[Smith 01] Connie U. Smith and Lloyd G. Williams. Performance Solutions: A \nPractical Guide to Creating Responsive, Scalable Software. Addison-Wesley, \n2001. \n[Soni 95] Dilip Soni, Robert L. Nord, and Christine Hofmeister. “Software Architec-\nture in Industrial Applications,” International Conference on Software Engi-\nneering 1995, April 1995, pp. 196-207.\n[Stonebraker 09] M. Stonebraker. “The ‘NoSQL’ Discussion \nHas Nothing to Do with SQL,” http://cacm.acm.org/blogs/\nblog-cacm/50678-the-nosql-discussion-has-nothing-to-do-with-sql/fulltext \n[Stonebraker 10a] M. Stonebraker. “SQL Databases v. NoSQL Databases,” Commu-\nnications of the ACM 53(4), p. 10.\n\n\n558 \nReferences\t\n[Stonebraker 10b] M. Stonebraker, D. Abadi, D.J. Dewitt, S. Madden, E. Paulson, A. \nPavlo, and A. Rasin. “MapReduce and Parallel DBMSs,” Communications of \nthe ACM 53, p. 6.\n[Stonebraker 11] M. Stonebraker. “Stonebraker on NoSQL and Enterprises,” Com-\nmunications of the ACM 54(8), p. 10.\n[Storey 97] M.-A. Storey, K. Wong, and H. Müller. “Rigi—A Visualization Envi-\nronment for Reverse Engineering (Research Demonstration Summary),” 19th \nInternational Conference on Software Engineering (ICSE 97), May 1997,  \npp. 606-607. IEEE Computer Society Press.\n[Svahnberg 00] M. Svahnberg and J. Bosch. “Issues Concerning Variability in Soft-\nware Product Lines,” in Proceedings of the Third International Workshop on \nSoftware Architectures for Product Families, Las Palmas de Gran Canaria, \nSpain, March 15-17, 2000, pp. 50-60. Springer, 2000.\n[Taylor 09] R. Taylor, N. Medvidovic, and E. Dashofy. Software Architecture: \nFoundations, Theory, and Practice. Wiley, 2009.\n[Telcordia 00] Telcordia. “GR-253-CORE, Synchronous Optical Network (SONET) \nTransport Systems: Common Generic Criteria.” 2000.\n[Urdangarin 08] R. Urdangarin, P. Fernandes, A. Avritzer, and D. Paulish. “Experi-\nences with Agile Practices in the Global Studio Project,” Proceedings of the \nIEEE International Conference on Global Software Engineering, 2008.\n[Utas 05] G. Utas. Robust Communications Software: Extreme Availability, Reli-\nability, and Scalability for Carrier-Grade Systems. Wiley, 2005. \n[van der Linden 07] F. van der Linden, K. Schmid, and E. Rommes. Software \nProduct Lines in Action. Springer, 2007.\n[van Deursen 04] A. van Deursen, C. Hofmeister, R. Koschke, L. Moonen, and C. \nRiva. “Symphony: View-Driven Software Architecture Reconstruction,” Pro-\nceedings of the 4th Working IEEE/IFIP Conference on Software Architec-\nture (WICSA 2004), June 2004, Oslo, Norway. IEEE Computer Society.\n[van Vliet 05] H. van Vliet. “The GRIFFIN project, A GRId For inFormatIoN about \narchitectural knowledge,” http://griffin.cs.vu.nl/, Vrije Universiteit, Amsterdam, \nApril 16, 2005.\n[Verizon 12] “Verizon 2012 Data Breach Investigations Re-\nport,” http://www.verizonbusiness.com/resources/reports/\nrp_data-breach-investigations-report-2012_en_xg.pdf\n[Vesely 81] W.E. Vesely, F.F. Goldberg, N.H. Roberts, and D.F. Haasl. “Fault Tree \nHandbook,” http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr0492/\nsr0492.pdf\n[Vesely 02] William Vesely, Michael Stamatelatos, Joanne Dugan, Joseph Fragola, \nJoseph Minarick III, and Jan Railsback. “Fault Tree Handbook with Aerospace \nApplications,” http://www.hq.nasa.gov/office/codeq/doctree/fthb.pdf\n[Viega 01] John Viega and Gary McGraw. Building Secure Software: How to Avoid \nSecurity Problems the Right Way. Addison-Wesley, 2001.\n[Voas 95] Jeffrey M. Voas and Keith W. Miller. “Software Testability: the New Verifi-\ncation,” IEEE Software 12(3)(May 1995), pp. 17-28. \n\n\nReferences\n559\n[Von Neumann 56] J. Von Neumann. “Probabilistic Logics and the Synthesis of Re-\nliable Organisms from Unreliable Components,” Automata Studies, C.E. Shan-\nnon and J. McCarthy, eds. Princeton University Press, 1956. \n[Wojcik 06] R. Wojcik, F. Bachmann, L. Bass, P. Clements, P. Merson, R. Nord, and \nW. Wood. “Attribute-Driven Design (ADD), Version 2.0,” Technical Report \nCMU/SEI-2006-TR-023, November 2006, http://www.sei.cmu.edu/library/ \nabstracts/reports/06tr023.cfm\n[Wood 07] W. Wood. “A Practical Example of Applying Attribute-Driven Design \n(ADD), Version 2.0,” Technical Report CMU/SEI-2007-TR-005, February 2007, \nhttp://www.sei.cmu.edu/library/abstracts/reports/07tr005.cfm\n[Woods 11] E. Woods and N. Rozanski. Software Systems Architecture: Working \nwith Stakeholders Using Viewpoints and Perspectives, Second Edition.  \nAddison-Wesley, 2011.\n[Wozniak 07] J. Wozniak, V. Baggiolini, D. Garcia Quintas, and J. Wenninger.  \n“Software Interlocks System,” Proceedings ICALEPCS07, http://ics-web4.sns.\nornl.gov/icalepcs07/WPPB03/WPPB03.PDF \n[Wu 06] W. Wu and T. Kelly. “Deriving Safety Requirements as Part of System Ar-\nchitecture Definition,” in Proceedings of 24th International System  \nSafety Conference, published by the System Safety Society, August 2006,  \nAlbuquerque, NM.\n[Yacoub 02] S. Yacoub and H. Ammar. “A Methodology for Architecture-Level  \nReliability Risk Analysis,” IEEE Transactions on Software Engineering, Vol. \n28, No. 6 (June 2002).\n[Yin 94] James Bieman and Hwei Yin. “Designing for Software Testability Using \nAutomated Oracles,” Proceedings International Test Conference, September \n1992, pp. 900-907.\n\n\nThis page intentionally left blank \n\n\n561\nAbout the Authors\nLen Bass is a Senior Principal Researcher at National ICT Australia Ltd. \n(NICTA). He joined NICTA in 2011 after 25 years at the Software Engineer-\ning Institute (SEI) at Carnegie Mellon University. He is the coauthor of two \naward-winning books in software architecture, including Documenting Software \nArchitectures: Views and Beyond, Second Edition (Addison-Wesley, 2011), as \nwell as several other books and numerous papers in computer science and soft-\nware engineering on a wide range of topics. Len has almost 50 years’ experience \nin software development and research in multiple domains, such as scientific \nanalysis systems, embedded systems, and information systems.\nPaul Clements is the Vice President of Customer Success at BigLever Software, \nInc., where he works to spread the adoption of systems and software product \nline engineering. Prior to this position, he was Senior Member of the Technical \nStaff at the SEI, where for 17 years he was leader or co-leader of projects in \nsoftware product line engineering and software architecture documentation and \nanalysis. Other books Paul has coauthored include Documenting Software Archi-\ntectures: Views and Beyond, Second Edition (Addison-Wesley, 2011), Evaluating \nSoftware Architectures: Methods and Case Studies (Addison-Wesley, 2002), and \nSoftware Product Lines: Practices and Patterns (Addison-Wesley, 2002). In ad-\ndition, he has also published dozens of papers in software engineering reflecting \nhis long-standing interest in the design and specification of challenging software \nsystems. Paul was a founding member of the IFIP WG2.10 Working Group on \nSoftware Architecture.\nRick Kazman is a Professor at the University of Hawaii and a Visiting Scientist \n(and former Senior Member of the Technical Staff) at the SEI. He is a coauthor \nof Evaluating Software Architectures: Methods and Case Studies (Addison-Wes-\nley, 2002) and author of more than 100 technical papers. Rick’s primary research \ninterests focus on software architecture, design and analysis, software visualiza-\ntion, and software engineering economics. Rick has created several highly influ-\nential methods and tools for architecture analysis, including the SAAM (Soft-\nware Architecture Analysis Method), the ATAM (Architecture Tradeoff Analysis \nMethod), the CBAM (Cost-Benefit Analysis Method), and the Dali architecture \nreverse-engineering tool.\n\n\nThis page intentionally left blank \n\n\n563\nIndex\nAADL (Architecture Analysis and Design \nLanguage), 354\nAbstract common services tactic, 124\nAbstract data sources for testability, 165\nAbstract Syntax Tree (AST) analyzers, 386\nAbstraction, architecture as, 5–6\nAcceptance testing, 372\nAccess\nbasis sets, 261\nnetwork, 504\naccess_read relationship, 384\naccess_write relationship, 384\nACID (atomic, consistent, isolated, and \ndurable) properties, 95\nAcknowledged system of systems, 106\nActive redundancy, 91, 256–259\nActiveMQ product, 224\nActivities\ncompetence, 468\ntest, 374–375\nActivity diagrams for traces, 353\nActors tactic, 152–153\nAdams, Douglas, 437\nADD method. See Attribute-Driven Design \n(ADD) method\nAdd-ons, 491–492\nADLs (architecture description languages), 330\nAdolphus, Gustavus, 42\nAdoption strategies, 494–496\nAdventure Builder system, 224, 226, 237\nAggregation for usability, 180\nAgile projects, 533\narchitecture example, 283–285\narchitecture methods, 281–283\narchitecture overview, 277–281\ndescription, 44–45\ndocumenting, 356–357\nguidelines, 286–287\nintroduction, 275–277\npatterns, 238\nrequirements, 56\nsummary, 287–288\nAIC (Architecture Influence Cycle)\ndescription, 58\nVasa ship, 43\nAir France flight 447, 192\nAir traffic control systems, 366–367\nAllen, Woody, 79\nAllocated to relation\nallocation views, 339–340\ndeployment structure, 14\nmulti-tier pattern, 237\nAllocation of responsibilities category\nASRs, 293\navailability, 96\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 73\nsecurity, 154\ntestability, 169\nusability, 181\nAllocation patterns\nmap-reduce, 232–235\nmiscellaneous, 238\nmulti-tier, 235–237\nAllocation structures, 5, 11, 14\nAllocation views, 339–340\nAllowed-to-use relationship, 206–207\nAlpha testing, 372\nAlternatives, evaluating, 398\nAmazon service-level agreements, 81, 522\nAnalysis\narchitecture, 47–48\nATAM, 408–409, 411\navailability, 255–259\nback-of-the-envelope, 262–264\nconformance by, 389–392\neconomic. See Economic analysis\noutsider, 399\nperformance, 252–255\nAnalysts, 54\nAnalytic model space, 259–260\nAnalytic perspective on up-front work vs. \nagility, 279–281\nAnalytic redundancy tactic, 90\nAND gate symbol, 84\nAnonymizing test data, 171\nAntimissile system, 104\nApache web server, 528, 531\nApproaches\nATAM, 407–409, 411\nCIA, 147–148\nLightweight Architecture Evaluation, 416\n",
      "page_number": 576
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 584-591)",
      "start_page": 584,
      "end_page": 591,
      "detection_method": "topic_boundary",
      "content": "564 \nIndex\t\nArchitects\nbackground and experience, 51–52\ncloud environments, 520–523\ncommunication with, 29\ncompetence, 459–467\ndescription and interests, 54\nduties, 460–464\nknowledge, 466–467\nresponsibilities, 422–423\nskills, 463, 465\ntest role, 375–376\nArchitectural structures\nallocation, 14\ncomponent-and-connector, 13–14\ndocumentation, 17–18\ninsight from, 11–12\nkinds, 10–11\nlimiting, 17\nmodule, 12–13\nrelating to each other, 14, 16–17\nselecting, 17\ntable of, 15\nviews, 9–10\nArchitecturally significant requirements \n(ASRs), 46–47, 291–292\nADD method, 320–321\nfrom business goals, 296–304\ndesigning to, 311–312\ninterviewing stakeholders, 294–296\nfrom requirements documents, 292–293\nutility trees for, 304–307\nArchitecture\nAgile projects. See Agile projects\nanalyzing, 47–48\navailability. See Availability\nbusiness context, 49–51\nchanges, 27–28\ncloud. See Cloud environments\ncompetence. See Competence\nconceptual integrity of, 189\ndesign. See Design and design strategy\ndocumenting. See Documentation\ndrivers in PALM, 305\neconomics. See Economic analysis\nevaluation. See Evaluation\nimplementation. See Implementation\ninfluences, 56–58\nin life cycle, 271–274\nmanagement. See Management and \ngovernance\nmodifiability. See Modifiability\npatterns. See Patterns\nperformance. See Performance\nproduct lines. See Software product lines\nproduct reuse, 483–484\nQAW drivers, 295\nQAW plan presentation, 295\nquality attributes. See Quality attributes\nreconstruction and conformance. See \nReconstruction and conformance\nrequirements. See Architecturally \nsignificant requirements (ASRs); \nRequirements\nsecurity. See Security\nstructures. See Architectural structures\ntactics. See Tactics\ntestability. See Testability\nusability. See Usability\nArchitecture Analysis and Design Language \n(AADL), 354\nArchitecture-centric projects, 279\nArchitecture description languages (ADLs), \n330\nArchitecture Influence Cycle (AIC)\ndescription, 58\nVasa ship, 43\nArchitecture Tradeoff Analysis Method \n(ATAM), 48, 283, 400\napproaches, 407–409, 411\nbusiness drivers, 404–405\nexample exercise, 411–414\noutputs, 402–403\nparticipants, 400–401\nphases, 403–404\npresentation, 403–406\nresults, 411\nscenarios, 408, 410\nsteps, 404–411\nAriane 5 explosion, 192\nAristotle, 185\nArrival pattern for events, 133\nArtifacts\navailability, 85–86\nin evaluation, 399\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nproduct reuse, 484\nquality attributes expressions, 69–70\nsecurity, 148, 150\ntestability, 162–163\nusability, 176\nvariability, 489\nASP.NET framework, 215\nAspects\nfor testability, 167\n\n\nIndex\n565\nvariation mechanism, 492\nASRs. See Architecturally significant \nrequirements (ASRs)\nAssembly connectors in UML, 369\nAssertions for system state, 166\nAssessment goals, 469\nAssessment of competence, 469–472, \n474–475\nAssign utility\nCBAM, 446\nNASA ECS project, 452\nAST (Abstract Syntax Tree) analyzers, 386\nAsymmetric flow in client-server pattern, 218\nAsynchronous messaging, 223, 225\nATAM. See Architecture Tradeoff Analysis \nMethod (ATAM)\nATM (automatic teller machine) banking \nsystem, 219\nAtomic, consistent, isolated, and durable \n(ACID) properties, 95\nAttachment relation\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector structures, 13\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nshared-data pattern, 231\nAttachments in component-and-connector \nviews, 336–337\nAttribute-Driven Design (ADD) method, 316\nASRs, 320–321\nelement choice, 318–319\nelement design solution, 321\ninputs, 316\noutput, 317–318\nrepeating steps, 324\nverify and refine requirements step, \n321–323\nAttributes. See Quality attributes\nAudiences for documentation, 328–329\nAuditor checklists, 260\nAudits, 153\nAuthenticate actors tactic, 152\nAuthentication in CIA approach, 148\nAuthorization in CIA approach, 148\nAuthorize actors tactic, 152\nAutomated delivery in Metropolis model, 535\nAutomatic reallocation of IP addresses, 516\nAutomatic scaling, 516\nAutomatic teller machine (ATM) banking \nsystem, 219\nAutomation for testability, 171–172\nAUTOSAR framework, 364\nAvailability\nanalytic model space, 259\nanalyzing, 255–259\nbroker pattern, 240\ncalculations, 259\nCAP theorem, 523\nCIA approach, 147\ncloud, 521\ndesign checklist, 96–98\ndetect faults tactic, 87–91\ngeneral scenario, 85–86\nintroduction, 79–81\nplanning for failure, 82–85\nprevent faults tactic, 94–95\nrecover-from-faults tactics, 91–94\nsummary, 98–99\ntactics overview, 87\nAvailability of resources tactic, 136\nAvailability quality attribute, 307\nAvailability zones, 522\nAvižienis, Algirdas, 79\nBack door reviews, 544–545\nBack-of-the-envelope analysis, 262–264\nBackground of architects, 51–52\nBank application, 391–392\nBase mechanisms in cloud, 509–514\nBasis sets for quality attributes, 261\nBDUF (Big Design Up Front) process, 278\nBehavior\ndocumenting, 351–354\nelement, 347\nin software architecture, 6–7\nBenefit in economic analysis, 441–442\nBenkler, Yochai, 528\nBeta testing, 372\nBig bang integration, 371\nBig bang models, 495–496\nBig Design Up Front (BDUF) process, 278\nBigTable database system, 518\nBinder, Robert, 167\nBinding\nlate, 385, 388\nmodifiability, 124–125\nuser interface, 178\nBinding time category\nASRs, 293\navailability, 98\ninteroperability, 115\nmodifiability, 122, 127\nperformance, 144\nquality design, 75–76\nsecurity, 156\n\n\n566 \nIndex\t\nBinding time category, continued\ntestability, 170\nusability, 182\nBitTorrent networks, 221\nBlack-box testing, 372–373\n“Blind Men and the Elephant” (Saxe), 379\nBlocked time in performance, 136\nBlogger website, 528\nBoehm, Barry, 279, 281, 286, 288\nBooch, Grady, 286\nBoolean logic diagrams, 83\nBottom-up adoption, 495\nBottom-up analysis mode, 284\nBottom-up schedules, 420–421\nBound execution times tactic, 138\nBound queue sizes tactic, 139\nBoundaries in ADD method, 317\nBox-and-line drawings\nas architectures, 6\ncomponent-and-connector views, 338\nBPEL (Business Process Execution \nLanguage), 108\nBrainstorming\nATAM, 410\nLightweight Architecture Evaluation, 416\nQAW, 295\nBranson, Richard, 443\nBreadth first ADD strategy, 319\nBrewer, Eric, 522\nBroadcast-based publish-subscribe pattern, \n229\nBroker pattern\navailability, 255–259\ndescription, 210–212\nweaknesses, 240–242\nBrooks, Fred, 47, 419\nBuley, Taylor, 147\nBureaucracy in implementation, 427\nBush, Vannevar, 397\nBusiness cases in project life-cycle context, \n46\nBusiness context\narchitecture influence on, 58\narchitectures and business goals, 49–50\nBusiness drivers\nATAM, 404–405\nLightweight Architecture Evaluation, 416\nPALM method, 305\nBusiness goals\nASRs from, 296–304\nassessment, 469\nATAM, 402\nbusiness context, 49–50\ncapturing, 304\ncategorization, 297–299\nevaluation process, 400\nexpressing, 299–301\ngeneral scenario, 301–303\nPALM method, 305\nviews for, 332\nBusiness managers, 54\nBusiness/mission presentation in QAW, 295\nBusiness Process Execution Language \n(BPEL), 108\nBusiness process improvements as business \ngoal, 299\nBusiness-related architect skills, 465\nC&C structures. See Component-and-connector \n(C&C) patterns and structures\nCaching tactic, 139\nCallbacks in Model-View-Controller pattern, \n214\nCalls relationship in view extraction, 384\nCancel command, 179\nCAP theorem, 518, 522–523\nCapture scenarios for quality attributes, \n196–197\nCapturing\nASRs in utility trees, 304–307\nbusiness goals, 304–307\nCatastrophic failures, 82\nCategorization of business goals, 297–299\nCBAM. See Cost Benefit Analysis Method \n(CBAM)\nChange\ndocumenting, 355–356\nmodifiability. See Modifiability\nreasoning and managing, 27–28\nChange control boards, 427\nChange default settings tactic, 153\nChaos Monkey, 160–161\nChaucer, Geoffrey, 459\nCheck-in, syncing at, 368\nChoice of technology category\nASRs, 293\navailability, 98\ninteroperability, 115\nmodifiability, 127\nperformance, 144\nsecurity, 156\ntestability, 170\nusability, 182\nCIA (confidentiality, integrity, and availabil-\nity) approach, 147–148\nCity analogy in Metropolis model, 536\n\n\nIndex\n567\nclass_contains_method relationship, 384\nclass_is_subclass_of_class relationship, 384\nClass structure, 13\nClasses in testability, 167\nClements, Paul, 66\nClient-server patterns, 19, 217–219\nClient-side proxies, 211\nClients\nbroker pattern, 211\nsimulators, 265\nClone-and-own practice, 482–483\nCloud environments\narchitecting in, 520–523\navailability, 521\nbase mechanisms, 509–514\ndatabase systems, 517–520\ndefinitions, 504–505\ndeployment models, 506\neconomic justification, 506–509\nequipment utilization, 508–509\nIaaS model, 515–517\nintroduction, 503–504\nmulti-tenancy applications, 509\nPaaS model, 517\nperformance, 521\nsecurity, 520–521\nservice models, 505–506\nsummary, 524\nCluster managers, 515\nCMG (Computer Measurement Group), 524\nCo-located teams\nAgile, 277\ncoordination, 427\nCockburn, Alistair, 287\nCOCOMO II (COnstructive COst MOdel II) \nscale factor, 279\nCode\narchitecture consistency, 366–368\ndesign in, 364\nKSLOC, 279–281\nmapping to, 334\nsecurity, 157\ntemplates, 365–367\nCohesion\nin modifiability, 121–123\nin testability, 167\nCold spares, 92, 256–259\nCollaborative system of systems, 106\nCollating scenarios\nCBAM, 445\nNASA ECS project, 451\nCOMBINATION gate symbol, 84\nCombining views, 343–345\nCommercial implementations of map-reduce \npatterns, 234\nCommon Object Request Broker Architecture \n(CORBA), 212\nCommunicates with relation, 237\nCommunication\nAgile software development, 277\narchitect skills, 465\narchitecture, 47\ndocumentation for, 329\nglobal development, 425\nstakeholder, 29–31\nCommunication diagrams for traces, 353\nCommunications views, 341\nCommunity clouds, 506\nCompatibility in component-and-connector \nviews, 336\nCompatibility quality attribute, 193\nCompetence\nactivities, 468\narchitects, 459–467\nassessment, 469–472, 474–475\nassessment goals, 469\nintroduction, 459–460\nmodels, 476\nquestions, 470, 472–474\nsoftware architecture organizations, \n467–475\nsummary, 475\nCompetence center patterns, 19, 238\nCompetence set tactic, 95\nComplexity\nbroker pattern, 211\nquality attributes, 71\nin testability, 167–168\nComponent-and-connector (C&C) patterns \nand structures, 5, 10–11\nbroker, 210–212\nclient-server, 217–219\nModel-View-Controller, 212–215\npeer-to-peer, 220–222\npipe-and-filter, 215–217\npublish-subscribe, 226–229\nservice-oriented architecture, 222–226\nshared-data, 230–231\ntypes, 13–14\nviews, 335–339, 344, 406\nComponents, 5\nindependently developed, 35–36\nreplacing for testability, 167\nsubstituting in variation mechanism, 492\nComprehensive models for behavior docu-\nmentation, 351, 353–354\n\n\n568 \nIndex\t\nComputer Measurement Group (CMG), 524\nComputer science knowledge of architects, \n466\nConcepts and terms, 368–369\nConceptual integrity of architecture, 189\nConcrete quality attribute scenarios, 69\nConcurrency\ncomponent-and-connector views, 13–14, \n337\nhandling, 132–133\nCondition monitoring tactic, 89\nConfidence in usability, 175\nConfidentiality, integrity, and availability \n(CIA) approach, 147–148\nConfigurability quality attribute, 307\nConfiguration manager roles, 422\nConfigurators, 492\nConformance, 380–381\nby analysis, 389–392\narchitectural, 48\nby construction, 389\nConformance checkers, 54\nConformity Monkey, 161\nConnectors\ncomponent-and-connector views, \n335–339\nmulti-tier pattern, 236\npeer-to-peer systems, 220\nREST, 225\nUML, 369\nConsistency\nCAP theorem, 523\ncode and architecture, 366–368\ndatabases, 520\nConsolidation in QAW, 295\nConstraints\nADD method, 322–323\nallocation views, 339\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector views, 337\nconformance, 390\ndefining, 32–33\nlayered pattern, 207\nmap-reduce patterns, 235\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 236–237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nrequirements, 64–65\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nConstruction, conformance by, 389\nCOnstructive COst MOdel II (COCOMO II) \nscale factor, 279\nContent-based publish-subscribe pattern, 229\nContention for resources tactic, 136\nContext diagrams\nATAM presentations, 406\nin documentation, 347\nContexts\narchitecture influence, 56–58\nbusiness, 49–51, 58\ndecision-making, 438–439\nprofessional, 51–52\nproject life-cycle, 44–48\nin relationships, 204–205\nstakeholders, 52–55\nsummary, 59\ntechnical, 40–43\nthought experiments, 263\ntypes, 39–40\nContextual factors in evaluation, 399–400\nContinuity as business goal, 298\nControl relation in map-reduce patterns, 235\nControl resource demand tactic, 137–138\nControl tactics for testability, 164–167\nControllers in Model-View-Controller \npattern, 213–214\nConway’s law, 38\nCoordination model category\nASRs, 293\navailability, 96\nglobal development, 426\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 73–74\nsecurity, 155\ntestability, 169\nusability, 181\nCORBA (Common Object Request Broker \nArchitecture), 212\nCore asset units, 497\nCore requirements, 531–532\nCore vs. periphery in Metropolis model, 534\nCorrelation logic for faults, 81\nCost Benefit Analysis Method (CBAM), 442\ncost determination, 444\nresults, 456–457\nsteps, 445–447\nutility curve determination, 442–443\nvariation points, 448–450\nweighting determination, 444\n\n\nIndex\n569\nCosts\nCBAM, 444\nof change, 118\nestimates, 34\nglobal development, 423–424\nindependently developed components \nfor, 36\npower, 507\nresources, 244\nthought experiments, 263\nvalue for, 442\nCosts to complete measure, 430\nCoupling\nin modifiability, 121–124\nin testability, 167\nCrashes and availability, 85\nCredit cards, 147, 157, 260, 268\nCrisis, syncing at, 368\nCriteria for ASRs, 306\nCrowd management in Metropolis model, \n534\nCrowdsourcing, 528\nCRUD operations, 109\nCruiseControl tool, 172\nCrystal Clear method, 44, 287\nCummins, Inc., 480, 490\nCunningham, Ward, 286\nCustomers\ncommunication with, 29\nedge-dominant systems, 529\nCustomization of user interface, 180\nDarwin, Charles, 275\nData Access Working Group (DAWG), 451\nData accessors in shared-data pattern, \n230–231\nData latency, utility trees for, 306\nData model category, 13\nASRs, 293\navailability, 96\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 74\nsecurity, 155\ntestability, 169\nusability, 182\nData reading and writing in shared-data \npattern, 230–231\nData replication, 139\nData sets\nmap-reduce pattern, 232–233\nfor testability, 170–171\nData stores in shared-data pattern, 230–231\nData transformation systems, 215\nDatabase administrators, 54\nDatabase systems\ncloud, 517–520\nin reconstruction, 386–387\nDataNodes, 512–514\nDAWG (Data Access Working Group), 451\nDeadline monotonic prioritization strategy, \n140\nDeadlines in processing, 134\nDebugging brokers, 211\nDecision makers on ATAM teams, 401\nDecision-making context, 438–439\nDecisions\nevaluating, 398\nmapping to quality requirements, 402–403\nquality design, 72–76\nDecomposition\ndescription, 311–312\nmodule, 5, 12, 16\nviews, 16, 343, 345\nDedicated finite resources, 530\nDefects\nanalysis, 374\neliminating, 486\ntracking, 430\nDefer binding\nmodifiability, 124–125\nuser interface, 178\nDegradation tactic, 93\nDelegation connectors, 369\nDemilitarized zones (DMZs), 152\nDenial-of-service attacks, 79, 521, 533\nDependencies\nbasis set elements, 261\non computations, 136\nintermediary tactic for, 123–124\nuser interface, 178\nDependent events in probability, 257\nDepends-on relation\nlayered pattern, 207\nmodules, 332–333\nDeploy on relation, 235\nDeployability attribute, 129, 187\nDeployers, 54\nDeployment models for cloud, 506\nDeployment structure, 14\nDeployment views\nATAM presentations, 406\ncombining, 345\npurpose, 332\nDepth first ADD strategy, 319\n\n\n570 \nIndex\t\nDesign and design strategy, 311\nADD. See Attribute-Driven Design \n(ADD) method\narchitecturally significant requirements, \n311–312\nin code, 364\ndecomposition, 311–312\nearly decisions, 31–32\ngenerate and test process, 313–316\ninitial hypotheses, 314–315\nnext hypotheses, 315\nquality attributes, 197\nsummary, 325\ntest choices, 315\nDesign checklists\navailability, 96–98\ndesign strategy hypotheses, 315\ninteroperability, 114–115\nmodifiability, 125–127\nperformance, 142–144\nquality attributes, 199\nsecurity, 154–156\nsummary, 183\ntestability, 169–170\nusability, 181–182\nDesigners\ndescription and interests, 54\nevaluation by, 397–398\nDetect attacks tactics, 151\nDetect faults tactic, 87–91\nDetect intrusion tactic, 151\nDetect message delay tactic, 151\nDetect service denial tactic, 151\nDeutsche Bank, 480\nDevelopers\nedge-dominant systems, 529\nroles, 422\nDevelopment\nbusiness context, 50–51\nglobal, 423–426\nincremental, 428\nproject life-cycle context, 44–45\ntests, 374\nDevelopment distributability attribute, 186\nDeviation\nfailure from, 80\nmeasuring, 429\nDevices in ADD method, 317\nDiNucci, Darcy, 527\ndir_contains_dir relationship, 384\ndir_contains_file relationship, 384\nDirected system of systems, 106\nDirectories in documentation, 349\nDiscoTect system, 391\nDiscover service tactic, 111\nDiscovery in interoperability, 105\nDiscovery services, 533\nDistributed computing, 221\nDistributed development, 427\nDistributed testing in Metropolis model, 535\nDMZs (demilitarized zones), 152\nDNS (domain name server), 514\nDoctor Monkey, 161\nDocumentation\nAgile development projects, 356–357\narchitect duties, 462\narchitectural structures, 17–18\narchitecture, 47, 347–349\nbehavior, 351–354\nchanging architectures, 355–356\ndistributed development, 427\nglobal development, 426\nintroduction, 327–328\nnotations, 329–331\nonline, 350\npackages, 345–351\npatterns, 350–351\nand quality attributes, 354–355\nservices, 533\nsummary, 359\nuses and audiences for, 328–329\nviews. See Views\nYAGNI, 282\nDocumentation maps, 347–349\nDocuments, control information, 347\nDomain decomposition, 315\nDomain knowledge of architects, 467\nDomain name server (DNS), 514\nDrivers\nATAM, 404–405\nLightweight Architecture Evaluation, 416\nPALM method, 305\nQAW, 295\nDSK (Duties, Skills, and Knowledge) model \nof competence, 476\nDuke’s Bank application, 391–392\nDuties\narchitects, 460–464\ncompetence, 472\nprofessional context, 51\nDuties, Skills, and Knowledge (DSK) model \nof competence, 476\nDynamic allocation views, 340\nDynamic analysis with fault trees, 83\nDynamic priority scheduling strategies, \n140–141\n\n\nIndex\n571\nDynamic structures, 5\nDynamic system information, 385–386\nEarliest-deadline-first scheduling strategy, 141\nEarly design decisions, 31–32\nEarth Observing System Data Information \nSystem (EOSDIS) Core System \n(ECS). See NASA ECS project\neBay, 234\nEC2 cloud service, 81, 160, 522, 532\nEclipse platform, 228\nEconomic analysis\nbasis, 439–442\nbenefit and normalization, 441–442\ncase study, 451–457\nCBAM. See Cost Benefit Analysis \nMethod (CBAM)\ncost value, 442\ndecision-making context, 438–439\nintroduction, 437\nscenario weighting, 441\nside effects, 441\nsummary, 457\nutility-response curves, 439–441\nEconomics\ncloud, 506–509\nissues, 543\nEconomies of scale in cloud, 507–508\nEcosystems, 528–530\nECS system. See NASA ECS project\nEdge-dominant systems, 528–530\nEdison, Thomas, 203\neDonkey networks, 221\nEducation, documentation as, 328–329\nEffective resource utilization, 187\nEffectiveness category for quality, 189\nEfficiency category for quality, 189–190\nEinstein, Albert, 175\nEJB (Enterprise Java Beans), 212\nElasticity, rapid, 504–505\nElasticity property, 187\nElectric grids, 106\nElectricity, 191, 570\nElectronic communication in global \ndevelopment, 426\nElements\nADD method, 318–319\nallocation views, 339–340\nbroker pattern, 211\ncatalogs, 346–347\nclient-server pattern, 218\ncomponent-and-connector views, 337\ndefined, 5\nlayered pattern, 207\nmap-reduce patterns, 235\nmapping, 75\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\nproduct reuse, 484\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nEmployees\nas goal-object, 302\nresponsibilities to, 299\nEnabling quality attributes, 26–27\nEncapsulation tactic, 123\nEncrypt data tactic, 152\nEnd users in edge-dominant systems, 529\nEnterprise architecture vs. system \narchitecture, 7–8\nEnterprise Java Beans (EJB), 212\nEnterprise resource planning (ERP) systems, \n228\nEnterprise service bus (ESB), 223, 225, 369\nEnvironment\nADD method, 317\nallocation views, 339–340\navailability, 85–86\nbusiness goals, 300\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntechnical context, 41–42\ntestability, 162–163\nusability, 176\nvariability, 489\nEnvironmental change as business goal, 299\nERP (enterprise resource planning) systems, \n228\nErrors, 80\ncore handling of, 532\ndetection by services, 533\nerror-handling views, 341\nin usability, 175\nESB (enterprise service bus), 223, 225, 369\nEscalating restart tactic, 94\nEstimates, cost and schedule, 34\nEvaluation\narchitect duties, 462–463\narchitecture, 47–48\n",
      "page_number": 584
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 592-600)",
      "start_page": 592,
      "end_page": 600,
      "detection_method": "topic_boundary",
      "content": "572 \nIndex\t\nEvaluation, continued\nATAM. See Architecture Tradeoff \nAnalysis Method (ATAM)\ncontextual factors, 399–400\nby designer, 397–398\nLightweight Architecture Evaluation, \n415–417\noutsider analysis, 399\npeer review, 398–399\nquestions, 472\nsoftware product lines, 493–494\nsummary, 417\nEvaluators, 54\nEvent bus in publish-subscribe pattern, 227\nEvents\nModel-View-Controller pattern, 214\nperformance, 131, 133\nprobability, 257\nEventual consistency model, 168, 523\nEvolutionary prototyping, 33–34\nEvolving software product lines, 496–497\nException detection tactic, 90\nException handling tactic, 92\nException prevention tactic, 95\nException views, 341\nExchanging information via interfaces, \n104–105\nEXCLUSIVE OR gate symbol, 84\nExecutable assertions for system state, 166\nExecution of tests, 374\nExemplar systems, 485\nExercise conclusion in PALM method, 305\nExisting systems in design, 314\nExpected quality attribute response levels, 453\nExperience of architects, 51–52\nExperiments in quality attribute modeling, \n264–265\nExpressing business goals, 299–301\nExtensibility quality attribute, 307\nExtensible programming environments, 228\nExtension points for variation, 491\nExternal sources for product lines, 496\nExternal system representatives, 55\nExternal systems in ADD method, 317\nExternalizing change, 125\nExtract-transform-load functions, 235\nExtraction, raw view, 382–386\nExtreme Programming development \nmethodology, 44\nFacebook, 527–528\nmap-reduce patterns, 234\nusers, 518\nFail fast principle, 522\nFailure Mode, Effects, and Criticality \nAnalysis (FMECA), 83–84\nFailures, 80\navailability. See Availability\nplanning for, 82–85\nprobabilities and effects, 84–85\nFairbanks, George, 279, 364\nFallbacks principle, 522\nFault tree analysis, 82–84\nFaults, 80\ncorrelation logic, 81\ndetection, 87–91\nprevention, 94–95\nrecovery from, 91–94\nFeature removal principle, 522\nFIFO (first-in/first-out) queues, 140\nFile system managers, 516\nFilters in pipe-and-filter pattern, 215–217\nFinancial objectives as business goal, 298\nFinding violations, 389–392\nFire-and-forget information exchange, 223\nFirefox, 531\nFirst-in/first-out (FIFO) queues, 140\nFirst principles from tactics, 72\nFixed-priority scheduling, 140\nFlex software development kit, 215\nFlexibility\ndefer binding tactic, 124\nindependently developed components \nfor, 36\nFlickr service, 527, 536\nFlight control software, 192–193\nFMECA (Failure Mode, Effects, and \nCriticality Analysis), 83–84\nFocus on architecture in Metropolis model, \n534–535\nFollow-up phase in ATAM, 403–404\nFolsonomy, 528\nFord, Henry, 479\nFormal documentation notations, 330\nFrameworks\ndesign strategy hypotheses, 314–315\nimplementation, 364–365\nFrankl, Viktor E., 63\nFreedom from risk category for quality, 189\nFunctional redundancy tactic, 90\nFunctional requirements, 64, 66\nFunctional responsibility in ADD method, \n322–323\nFunctional suitability quality attribute, 193\nFunctionality\ncomponent-and-connector views, 336\n\n\nIndex\n573\ndescription, 65\nFused views, 388–389\nGamma, E., 212\nGate symbols, 83–84\nGeneral Motors product line, 487\nGeneralization structure, 13\nGenerate and test process, 313–316\nGenerators of variation, 492\nGet method for system state, 165\nGlobal development, 423–426\nGlobal metrics, 429–430\nGnutella networks, 221\nGoal components in business goals, 300\nGoals. See Business goals\nGoldberg, Rube, 102\nGood architecture, 19–21\nGood enough vs. perfect, 398\nGoogle\ndatabase system, 518\nGoogle App Engine, 517\nGoogle Maps, 105–107\ngreenhouse gas from, 190–191\nmap-reduce patterns, 234\npower sources, 507\nGovernance, 430–431\nGovernment, responsibilities to, 299\nGraceful degradation, 522\nGraphical user interfaces in publish-subscribe \npattern, 228\nGray-box testing, 373\nGreen computing, 190–191\nGreenspan, Alan, 443\nGrowth and continuity as business goal, 298\nGuerrilla movements, 543–544\nHadoop Distributed File System (HDFS), 512\nHardware costs for cloud, 507\nHarel, David, 353\nHarnesses for tests, 374\nHazard analysis, 82\nHazardous failures, 82\nHBase database system, 518–519\nHDFS (Hadoop Distributed File System), \n512\nHeartbeat tactic, 89, 256, 408\nHelm, R., 212\nHewlett-Packard, 480\nHiatus stage in ATAM, 409\nHigh availability. See Availability\nHighway systems, 142\nHorizontal scalability, 187\nHot spare tactic, 91\nHTTP (HyperText Transfer Protocol),  \n219\nHudson tool, 172\nHufstedler, Shirley, 363\nHuman body structure, 9\nHuman Performance model of competence, \n476\nHuman Performance Technology model, \n469–473\nHuman resource management in global \ndevelopment, 425\nHybertsson, Henrik, 42–43\nHybrid clouds, 506\nHydroelectric power station catastrophe, 188, \n192\nHypertext for documentation, 350\nHyperText Transfer Protocol (HTTP), 219\nHypervisors, 510–512\nHypotheses\nconformance, 390\ndesign strategy, 314–315\nfused views, 388\nIaaS (Infrastructure as a Service) model, \n505–506, 515–517\nIdentify actors tactic, 152\nIgnore faulty behavior tactic, 93\nImplementation, 363–364, 427\narchitect duties, 463\ncode and architecture consistency, 366–368\ncode templates, 365–367\ndesign in code, 364\nframeworks, 364–365\nincremental development, 428\nmodules, 333–334\nstructure, 14\nsummary, 376\ntesting, 370–376\ntracking progress, 428–429\ntradeoffs, 427\nImplementors, 55\nIn-service software upgrade (ISSU), 92\nIncludes relationship, 384\nInclusion of elements for variation, 491\nIncrease cohesion tactic, 123\nIncrease competence set tactic, 95\nIncrease efficiency tactic, 142\nIncrease resource efficiency tactic, 138\nIncrease resources tactic, 138–139, 142\nIncrease semantic coherence tactic, 123, 239\nIncremental Commitment Model, 286\nIncremental development, 428\nIncremental integration, 371\n\n\n574 \nIndex\t\nIncremental models in adoption strategies, \n495–496\nIndependent events in probability, 257\nIndependently developed components, 35–36\nInflexibility of methods, 277\nInform actors tactic, 153\nInformal contacts in global development, 426\nInformal notations for documentation, 330\nInformation handling skills, 465\nInformation sharing in cloud, 520\nInfrastructure as a Service (IaaS) model, \n505–506, 515–517\nInfrastructure in map-reduce patterns, 235\nInfrastructure labor costs in cloud, 507\nInheritance variation mechanism, 492\nInherits from relation, 13\nINHIBIT gate symbol, 84\nInhibiting quality attributes, 26–27\nInitial hypotheses in design strategy, 314–315\nInputs in ADD method, 316, 321–323\nInstantiate relation, 235\nIntegration management in global \ndevelopment, 424\nIntegration testing, 371–372\nIntegrators, 55\nIntegrity\narchitecture, 189\nCIA approach, 147\nInterchangeable parts, 35–36, 480\nInterfaces\nexchanging information via, 104–105\nseparating, 178\nIntermediary tactic, 123\nIntermediate states in failures, 80\nInternal sources of product lines, 496–497\nInternet Protocol (IP) addresses\nautomatic reallocation, 516\noverview, 514\nInteroperability\nanalytic model space, 259\ndesign checklist, 114–115\ngeneral scenario, 107–110\nintroduction, 103–106\nservice-oriented architecture pattern, 224\nand standards, 112–113\nsummary, 115\ntactics, 110–113\nInterpersonal skills, 465\nInterpolation in CBAM, 446\nInterviewing stakeholders, 294–296\nIntroduce concurrency tactic, 139\nInvokes-services role, 335\nInvolvement, 542–543\nIowability, 195–196\nIP (Internet Protocol) addresses\nautomatic reallocation, 516\noverview, 514\nIs a relation, 332–333\nIs-a-submodule-of relation, 12\nIs an instance of relation, 13\nIs part of relation\nmodules, 332–333\nmulti-tier pattern, 237\nISO 25010 standard, 66, 193–195\nISSU (in-service software upgrade), 92\nIterative approach\ndescription, 44\nreconstruction, 382\nrequirements, 56\nJanitor Monkey, 161\nJavaScript Object Notation (JSON) form, 519\nJitter, 134\nJobs, Steve, 311\nJohnson, R., 212\nJSON (JavaScript Object Notation) form, 519\nJust Enough Architecture (Fairbanks), 279, \n364\nKeys in map-reduce pattern, 232\nKnowledge\narchitects, 460, 466–467\ncompetence, 472–473\nprofessional context, 51\nKroc, Ray, 291\nKruchten, Philippe, 327\nKSLOC (thousands of source lines of code), \n279–281\nKundra, Vivek, 503\nLabor availability in global development, 423\nLabor costs\ncloud, 507\nglobal development, 423\nLanguage, 542\nLarger data sets in map-reduce patterns, 234\nLate binding, 385, 388\nLatency\nCAP theorem, 523\nperformance, 133, 255\nqueuing models for, 198–199\nutility trees for, 306\nLatency Monkey, 161\nLattix tool, 387\nLawrence Livermore National Laboratory, 71\nLayer bridging, 206\n\n\nIndex\n575\nLayer structures, 13\nLayer views in ATAM presentations, 406\nLayered patterns, 19, 205–210\nLayered views, 331–332\nLeaders on ATAM teams, 401\nLeadership skills, 464–465\nLearning issues in usability, 175\nLeast-slack-first scheduling strategy, 141\nLePatner, Barry, 3\nLetterman, David, 443\nLevels\nfailure, 258\nrestart, 94\ntesting, 370–372\nLeveson, Nancy, 200\nLexical analyzers, 386\nLife cycle\narchitecture in, 271–274\nchanges, 530–531\nMetropolis model, 537\nproject. See Project life-cycle context\nquality attribute analysis, 265–266\nLife-cycle milestones, syncing at, 368\nLightweight Architecture Evaluation method, \n415–417\nLikelihood of change, 117\nLimit access tactic, 152\nLimit complexity tactic, 167\nLimit event response tactic, 137\nLimit exposure tactic, 152\nLimit structural complexity tactic, 167–168\nLinux, 531\nList-based publish-subscribe pattern, 229\nLoad balancers, 139\nLocal changes, 27–28\nLocal knowledge of markets in global devel-\nopment, 423\nLocalize state storage for testability, 165\nLocate tactic, 111\nLocation independence, 504\nLock computer tactic, 153\nLogical threads in concurrency, 13–14\nMacros for testability, 167\nMailing lists in publish-subscribe pattern, \n228\nMaintain multiple copies tactic, 142\nMaintain multiple copies of computations \ntactic, 139\nMaintain multiple copies of data tactic, 139\nMaintain system model tactic, 180\nMaintain task model tactic, 180\nMaintain user model tactic, 180\nMaintainability quality attribute, 195, 307\nMaintainers, 55\nMajor failures, 82\nManage event rate tactic, 142\nManage resources tactic, 137–139\nManage sampling rate tactic\nperformance, 137\nquality attributes, 72\nManagement and governance\narchitect skills, 464\ngovernance, 430–431\nimplementing, 427–429\nintroduction, 419\nmeasuring, 429–430\norganizing, 422–426\nplanning, 420–421\nsummary, 432\nManagement information in modules, 334\nManagers, communication with, 29\nManaging interfaces tactic, 111\nManifesto for Agile software development, \n276\nMap architectural strategies in CBAM, 446\nMap-reduce pattern, 232–235\nMapping\nto requirements, 355, 402–403\nto source code units, 334\nMapping among architectural elements \ncategory\nASRs, 293\navailability, 97\ninteroperability, 114\nmodifiability, 127\nperformance, 144\nquality design decisions, 75\nsecurity, 155\ntestability, 169\nusability, 182\nMaps, documentation, 347–349\nMarket position as business goal, 299\nMarketability category for quality, 190\nMarkov analysis, 83\nMatrixed team members, 422\nMcGregor, John, 448\nMean time between failures (MTBF), 80, \n255–259\nMean time to repair (MTTR), 80, 255–259\nMeasured services, 505\nMeasuring, 429–430\nMeetings\nglobal development, 426\nprogress tracking, 428\nMethods in product reuse, 484\n\n\n576 \nIndex\t\nMetrics, 429–430\nMetropolis structure\nedge-dominant systems, 528–530\nimplications, 533–537\nMicrosoft Azure, 517\nMicrosoft Office 365, 509\nMigrates-to relation, 14\nMill, John Stuart, 527\nMinimal cut sets, 83\nMinor failures, 82\nMissile defense system, 104\nMissile warning system, 192\nMixed initiative in usability, 177\nMobility attribute, 187\nModel driven development, 45\nModel-View-Controller (MVC) pattern\noverview, 212–215\nperformance analysis, 252–254\nuser interface, 178\nModels\nproduct reuse, 484\nquality attributes, 197–198\ntransferable and reusable, 35\nModifiability\nanalytic model space, 259\ncomponent-and-connector views, 337\ndesign checklist, 125–127\ngeneral scenario, 119–120\nintroduction, 117–119\nmanaging, 27\nping/echo, 243\nrestrict dependencies tactic, 246\nscheduling policy tactic, 244–245\nsummary, 128\ntactics, 121–125\nand time-to-market, 284\nunit testing, 371\nin usability, 179\nModularity of core, 532\nModules and module patterns, 10, 205–210\ncoupling, 121\ndecomposition structures, 5\ndescription, 4–5\ntypes, 12–13\nviews, 332–335, 406\nMongoDB database, 519\nMonitor relation in map-reduce patterns, 235\nMonitor tactic, 88–89\nMonitorability attribute, 188\nMoSCoW style, 292\nMSMQ product, 224\nMTBF (mean time between failures), 80, \n255–259\nMTTR (mean time to repair), 80, 255–259\nMulti-tenancy\ncloud, 509, 520\ndescription, 505\nMulti-tier patterns, 19, 235–237\nMultitasking, 132–133\nMusket production, 35–36\nMVC (Model-View-Controller) pattern\noverview, 212–215\nperformance analysis, 252–254\nuser interface, 178\nMythical Man-Month (Brooks), 47\nNameNode process, 512–513\nNames for modules, 333\nNASA ECS project, 451\narchitectural strategies, 452–456\nassign utility, 452\ncollate scenarios, 451\nexpected quality attribute response level, \n453\nprioritizing scenarios, 452\nrefining scenarios, 451–452\nNation as goal-object, 302\nNational Reconnaissance Office, 481\n.NET platform, 212\nNetflix\ncloud, 522\nSimian Army, 160–161\nNetwork administrators, 55\nNetworked services, 36\nNetworks, cloud, 514\nNightingale application, 306–307\nNo effect failures, 82\nNode managers, 516\nNokia, 480\nnon-ASR requirements, 312–313\nNon-stop forwarding (NSF) tactic, 94\nNondeterminism in testability, 168\nNonlocal changes, 27\nNonrepudiation in CIA approach, 148\nNonrisks in ATAM, 402\nNormalization\ndatabases, 520\neconomic analysis, 441–442\nNoSQL database systems, 518–520, 523\nNoSQL movement, 248\nNotations\ncomponent-and-connector views, 338–339\ndocumentation, 329–331\nNotifications\nfailures, 80\nModel-View-Controller pattern, 214\n\n\nIndex\n577\nNSF (non-stop forwarding) tactic, 94\nNumber of events not processed measure-\nment, 134\nObject-oriented systems\nin testability, 167\nuse cases, 46\nObjects in sequence diagrams, 352\nObservability of failures, 80\nObserve system state tactics, 164–167\nOff-the-shelf components, 36\nOmissions\navailability faults from, 85\nfor variation, 491\nOn-demand self-service, 504\n1+1 redundancy tactic, 91\nOnline documentation, 350\nOntologies, 368–369\nOPC (Order Processing Center) component, \n224, 226\nOpen content systems, 529\nOpen Group\ncertification program, 477\ngovernance responsibilities, 430–431\nOpen source software, 36, 238\nOperation Desert Storm, 104\nOR gate symbol, 84\nOrchestrate tactic, 111\nOrchestration servers, 223, 225\nOrder Processing Center (OPC) component, \n224, 226\nOrganization\nglobal development, 423–426\nproject manager and software architect \nresponsibilities, 422–423\nsoftware development teams, 422\nOrganizational Coordination model, 470, \n473, 476\nOrganizational Learning model, 470, 474, 476\nOrganizations\nactivities for success, 468\narchitect skills, 464\narchitecture influence on, 33\nas goal-object, 302\nsecurity processes, 157\nstructural strategies for products, 497\nOutages. See Availability\nOutputs\nADD method, 317–318\nATAM, 402–403\nOutsider analysis, 399\nOverlay views, 343\nOverloading for variation, 491\nOverview presentations in PALM method, \n305\nP2P (peer-to-peer) pattern, 220–222\nPaaS (Platform as a Service) model, 505, 517\nPage mappers, 510–512\nPALM (Pedigreed Attribute eLicitation \nMethod), 304–305\nParameter fence tactic, 90\nParameter typing tactic, 90\nParameters for variation mechanism, 492\nParser tool, 386\nPartitioning CAP theorem, 523\nPartnership and preparation phase in ATAM, \n403–404\nPassive redundancy, 91–92, 256–259\nPatterns, 18–19\nallocation, 232–237\ncomponent-and-connector. See \nComponent-and-connector (C&C) \npatterns and structures\ndocumenting, 350–351\nintroduction, 203–204\nmodule, 205–210\nrelationships, 204–205\nsummary, 247–248\nand tactics, 238–247, 315\nPaulish, Dan, 420\nPause/resume command, 179\nPayment Card Industry (PCI), 260\nPDF (probability density function), 255\nPDM (platform-definition model), 45\nPedigree and value component of business \ngoals, 301\nPedigreed Attribute eLicitation Method \n(PALM), 304–305\nPeer nodes, 220\nPeer review, 398–399\nPeer-to-peer (P2P) pattern, 220–222\nPenalties in Incremental Commitment Model, \n286\nPeople\nmanaging, 464\nin product reuse, 485\nPerfect vs. good enough, 398\nPerformance\nanalytic model space, 259\nanalyzing, 252–255\nbroker pattern, 241\ncloud, 521\ncomponent-and-connector views, 336\ncontrol resource demand tactics, 137–138\ndesign checklist, 142–144\n\n\n578 \nIndex\t\nPerformance, continued\ngeneral scenario, 132–134\nintroduction, 131–132\nmanage resources tactics, 138–139\nmap-reduce pattern, 232\nping/echo, 243\nand quality, 191\nquality attributes tactics, 72\nqueuing models for, 198–199\nresource effects, 244, 246\nsummary, 145\ntactics overview, 135–137\nviews, 341\nPerformance quality attribute, 307\nPerformance efficiency quality attribute, 193\nPeriodic events, 133\nPeriphery\nMetropolis model, 535\nrequirements, 532\nPersistent object managers, 515–516\nPersonal objectives as business goal, 298\nPersonnel availability in ADD method, 320\nPetrov, Stanislav Yevgrafovich, 192\nPhases\nATAM, 403–404\nmetrics, 430\nMetropolis model, 534\nPhilips product lines, 480–481, 487\nPhysical security, 191\nPIM (platform-independent model), 45\nPing/echo tactic, 87–88, 243\nPipe-and-filter pattern, 215–217\nPlanned increments, 530\nPlanning\nfor failure, 82–85\nincremental development, 428\noverview, 420–421\ntests, 374\nPlatform as a Service (PaaS) model, 505, 517\nPlatform-definition model (PDM), 45\nPlatform-independent model (PIM), 45\nPlatforms\narchitect knowledge about, 467\nframeworks in, 365\npatterns, 19, 238\nservices for, 532–533\nPlug-in architectures, 34\nPMBOK (Project Management Body of \nKnowledge), 423–425\nPointers, smart, 95\nPolicies, scheduling, 140\nPooling resources, 504\nPortability quality attributes, 67, 186, 195\nPortfolio as goal-object, 302\nPorts in component-and-connector views, \n335, 337–338\nPotential alternatives, 398\nPotential problems, peer review for, 399\nPotential quality attributes, 305\nPower station catastrophe, 188, 192\nPredicting system qualities, 28\nPredictive model tactic, 95\nPreemptible processes, 141\nPreparation-and-repair tactic, 91–93\nPreprocessor macros, 167\nPresentation\nATAM, 402–406\ndocumentation, 346\nLightweight Architecture Evaluation, 416\nPALM method, 305\nQAW, 295\nPrevent faults tactics, 94–95\nPrimary presentations in documentation, 346\nPrinciples\nAgile, 276–277\ncloud failures, 522\ndesign fragments from, 72\nIncremental Commitment Model, 286\nPrioritize events tactic, 137–138, 142\nPrioritizing\nATAM scenarios, 410\nCBAM scenarios, 445–446\nCBAM weighting, 444\nLightweight Architecture Evaluation \nscenarios, 416\nNASA ECS project scenarios, 452\nQAW, 295–296\nrisk, 429\nschedules, 140–141\nviews, 343\nPRIORITY AND gate symbol, 84\nPrivate clouds, 506\nPrivate IP addresses, 514\nProactive enforcement in Metropolis model, \n535\nProactive product line models, 495\nProbability density function (PDF), 255\nProbability for availability, 256–259\nProblem relationships in patterns, 204–205\nProceedings scribes, 401\nProcesses\ndevelopment, 44–45\nproduct reuse, 484\nrecommendations, 20\nsecurity, 157\nProcessing time in performance, 136\n\n\nIndex\n579\nProcurement management, 425\nProduct-line managers, 55\nProduct lines. See Software product lines\nProduct manager roles, 422\nProductivity metrics, 429–430\nProfessional context, 51–52, 58\nProfiler tools, 386\nProgramming knowledge of architects, 466\nProject context, 57\nProject life-cycle context\narchitecturally significant requirements, \n46–47\narchitecture analysis and evaluation, 47–48\narchitecture documentation and \ncommunication, 47\narchitecture selection, 47\nbusiness cases, 46\ndevelopment processes, 44–45\nimplementation conformance, 48\nProject Management Body of Knowledge \n(PMBOK), 423–425\nProject managers\ndescription and interests, 55\nresponsibilities, 422–423\nProject planning artifacts in product reuse, 484\nPropagation costs of change, 288\nProsumers in edge-dominant systems, 529\nProtection groups, 91\nPrototypes\nevolutionary, 33–34\nquality attribute modeling and analysis, \n264–265\nfor requirements, 47\nProvides-services role, 335\nProxy servers, 146, 211\nPublic clouds, 506\nPublic IP addresses, 514\nPublicly available apps, 36\nPublish-subscribe connector, 336\nPublish-subscribe pattern, 226–229\nPublisher role, 336\nQAW (Quality Attribute Workshop), 294–296\nQt framework, 215\nQuality attribute modeling and analysis, \n251–252\nanalytic model space, 259–260\navailability analysis, 255–259\nchecklists, 260–262\nexperiments, simulations, and prototypes, \n264–265\nlife cycle stages, 265–266\nperformance analysis, 252–255\nsummary, 266–267\nthought experiments and back-of-the-\nenvelope analysis, 262–264\nQuality Attribute Workshop (QAW), 294–296\nQuality attributes, 185\nADD method, 322–323\nASRs, 294–296\nATAM, 407\ncapture scenarios, 196–197\ncategories, 189–190\nchecklists, 199, 260–262\nconsiderations, 65–67\ndesign approaches, 197\nand documentation, 354–355\ngrand unified theory, 261\nimportant, 185–188\ninhibiting and enabling, 26–27\nintroduction, 63–64\nLightweight Architecture Evaluation, 416\nmodels, 197–198\nNASA ECS project, 453\npeer review, 398\nquality design decisions, 72–76\nrequirements, 64, 68–70\nsoftware and system, 190–193\nstandard lists, 193–196\nsummary, 76–77\ntactics, 70–72, 198–199\ntechnical context, 40–41\nvariability, 488–489\nX-ability, 196–199\nQuality design decisions, 72–73\nallocation of responsibilities, 73\nbinding time, 75–76\ncoordination models, 73–74\ndata models, 74\nelement mapping, 75\nresource management, 74–75\ntechnology choices, 76\nQuality management in global development, \n424\nQuality of products as business goal, 299\nQuality requirements, mapping decisions to, \n402–403\nQuality views, 340–341\nQuestioners on ATAM teams, 401\nQuestions for organizational competence, \n470, 472–474\nQueue sizes tactic, 139\nQueuing models for performance, 198–199, \n252–255\nQuick Test Pro tool, 172\n\n\n580 \nIndex\t\nRace conditions, 133\nRandom access in equipment utilization, 508\nRapid elasticity, 504–505\nRate monotonic prioritization strategy, 140\nRational Unified Process, 44\nRationale in documentation, 347, 349\nRaw view extraction in reconstruction, \n382–386\nRDBMSs (relational database management \nsystems), 518\nReact to attacks tactics, 153\nReactive enforcement in Metropolis model, \n536\nReactive product line models, 495\nReader role in component-and-connector \nviews, 335\nReconfiguration tactic, 93\nReconstruction and conformance, 380–381\ndatabase construction, 386–387\nfinding violations, 389–392\nguidelines, 392–393\nprocess, 381–382\nraw view extraction, 382–386\nsummary, 393–394\nview fusion, 388–389\nRecord/playback method for system state, \n165\nRecover from attacks tactics, 153–154\nRecover-from-faults tactics, 91–94\nReduce computational overhead tactic, 142\nReduce function in map-reduce pattern, \n232–235\nReduce overhead tactic, 138\nRedundancy tactics, 90, 256–259\nRefactor tactic, 124\nRefined scenarios\nNASA ECS project, 451–452\nQAW, 296\nReflection for variation, 491\nReflection pattern, 262\nRegistry of services, 225\nRegression testing, 372\nReintroduction tactics, 91, 93–94\nRejuvenation tactic, 95\nRelational database management systems \n(RDBMSs), 518\nRelations\nallocation views, 339–340\narchitectural structures, 14, 16–17\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector views, 337\nconformance, 390\nin documentation, 346\nlayered pattern, 207\nmap-reduce patterns, 235\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nview extraction, 384\nRelease strategy for documentation, 350\nReliability\ncloud, 507\ncomponent-and-connector views, 336\ncore, 532\nindependently developed components \nfor, 36\nvs. safety, 188\nSOAP, 109\nviews, 341\nReliability quality attribute, 195\nRemote procedure call (RPC) model, 109\nRemoval from service tactic, 94–95\nReplicated elements in variation, 491\nReplication tactic, 90\nReport method for system state, 165\nReporting tests, 374\nRepository patterns, 19\nRepresentation of architecture, 6\nRepresentational State Transfer (REST), \n108–110, 223–225\nReputation of products as business goal, 299\nRequest/reply connectors\nclient-server pattern, 218\npeer-to-peer pattern, 222\nRequirements\nASRs. See Architecturally significant \nrequirements (ASRs)\ncategories, 64–65\nfrom goals, 49\nmapping to, 355, 402–403\nMetropolis model, 534\nproduct reuse, 483\nprototypes for, 47\nquality attributes, 68–70\nsoftware development life cycle changes, \n530\nsummary, 308–310\ntying methods together, 308\nRequirements documents\nASRs from, 292–293\n",
      "page_number": 592
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 601-608)",
      "start_page": 601,
      "end_page": 608,
      "detection_method": "topic_boundary",
      "content": "Index\n581\nWaterfall model, 56\nReset method for system state, 165\nResisting attacks tactics, 152–153\nRESL scale factor, 279\nResource management category\nASRs, 293\navailability, 97\ninteroperability, 115\nmodifiability, 127\nperformance, 144\nquality design decisions, 74–75\nsecurity, 155\nsoftware development life cycle changes, \n530\ntestability, 170\nusability, 182\nResources\ncomponent-and-connector views, 336\nequipment utilization, 508\npooling, 504\nsandboxing, 166\nsoftware development life cycle changes, \n530\nResponse\navailability, 85–86\ninteroperability, 105, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntestability, 162–163\nusability, 176\nvariability, 489\nResponse measure\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntestability, 162–163\nusability, 176\nvariability, 489\nResponsibilities\nas business goal, 299\nmodules, 333\nquality design decisions, 73\nREST (Representational State Transfer), \n108–110, 223–225\nRestart tactic, 94\nRestrict dependencies tactic, 124, 239, \n246–247\nRestrictions on vocabulary, 36\nResults\nATAM, 411\nCBAM, 447, 456–457\nevaluation, 400\nLightweight Architecture Evaluation, 416\nRetry tactic, 93\nReusable models, 35\nReuse of software architecture, 479, 483–486\nReviews\nback door, 544–545\npeer, 398–399\nRevision history of modules, 334\nRevoke access tactic, 153\nRework in agility, 279\nRisk\nADD method, 320\nATAM, 402\nglobal development, 425\nprogress tracking, 429\nRisk-based testing, 373–374\nRobustness of core, 532\nRoles\ncomponent-and-connector views, 335\nproduct line architecture, 488–490\nsoftware development teams, 422\ntesting, 375–376\nRollback tactic, 92\nRound-robin scheduling strategy, 140–141\nRozanski, Nick, 170\nRPC (remote procedure call) model, 109\nRuntime conditionals, 492\nRutan, Burt, 159\nSaaS (Software as a Service) model, 505\nSafety\nchecklists, 260, 268\nuse cases, 46\nSafety attribute, 188\nSafety Integrity Level, 268\nSalesforce.com, 509\nSample technologies in cloud, 514–520\nSampling rate tactic, 137\nSandbox tactic, 165–166\nSanity checking tactic, 89\nSatisfaction in usability, 175\nSaxe, John Godfrey, 379\nScalability\nkinds, 187\npeer-to-peer systems, 220\nWebArrow web-conferencing system, 285\nScalability attribute, 187\nScaling, automatic, 516\nScenario scribes, 401\n\n\n582 \nIndex\t\nScenarios\nATAM, 408, 410\navailability, 85–86\nbusiness goals, 301–303\nCBAM, 445–446\ninteroperability, 107–110\nLightweight Architecture Evaluation, 416\nmodifiability, 119–120\nNASA ECS project, 451–452\nperformance, 132–134\nQAW, 295–296\nquality attributes, 67–70, 196–197\nsecurity, 148–150\nfor structures, 12\ntestability, 162–163\nusability, 176\nweighting, 441, 444\nSchedule resources tactic\nperformance, 139\nquality attributes, 72\nScheduled downtimes, 81\nSchedulers, hypervisor, 512\nSchedules\ndeviation measurements, 429\nestimates, 34\npolicies, 140–141\npolicy tactic, 244–245\ntop-down and bottom-up, 420–421\nSchemas, database, 519\nScope, product line, 486–488\nScope and summary section in \ndocumentation maps, 347\nScrum development methodology, 44\nSDL (Specification and Description \nLanguage), 354\nSecurity\nanalytic model space, 259\nbroker pattern, 242\ncloud, 507, 520–521\ncomponent-and-connector views, 336\ndesign checklist, 154–156\ngeneral scenario, 148–150\nintroduction, 147–148\nping/echo, 243\nquality attributes checklists, 260\nsummary, 156\ntactics, 150–154\nviews, 341\nSecurity Monkey, 161\nSecurity quality attribute, 195, 307\nSEI (Software Engineering Institute), 59\nSelecting\narchitecture, 47\ntools and technology, 463\nSelenium tool, 172\nSelf-organization in Agile, 277\nSelf-test tactic, 91\nSemantic coherence, 178\nSemantic importance, 140\nSemiformal documentation notations, 330\nSensitivity points in ATAM, 403\nSeparate entities tactic, 153\nSeparation of concerns in testability, 167\nSequence diagrams\nthought experiments, 263\nfor traces, 351–352\nServers\nclient-server pattern, 217–219\nproxy, 146, 211\nSAO pattern, 223, 225\nService consumer components, 222, 225\nService discovery in SOAP, 108\nService impact of faults, 81\nService-level agreements (SLAs)\nAmazon, 81, 522\navailability in, 81\nIaaS, 506\nPaaS, 505\nSOA, 222\nService-oriented architecture (SOA) pattern, \n222–226\nService providers, 222–225\nService registry, 223\nService structure, 13\nServices for platforms, 532–533\nSet method for system state, 165\nShadow tactic, 93\nShared-data patterns, 19, 230–231\nShared documents in documentation, 350\nShareholders, responsibilities to, 299\nSiberian hydroelectric plant catastrophe, 188, \n192\nSiddhartha, Gautama, 251\nSide-channel attacks, 521\nSide effects in economic analysis, 439, 441\nSimian Army, 160–161\nSimulations, 264–265\nSize\nmodules, 121\nqueue, 139\nSkeletal systems, 34\nSkeletal view of human body, 9\nSkills\narchitects, 460, 463, 465\nglobal development, 423\nprofessional context, 51\n\n\nIndex\n583\nSLAs. See Service-level agreements (SLAs)\nSmall victories, 544\nSmart pointers, 95\nSOA (service-oriented architecture) pattern, \n222–226\nSOAP\nvs. REST, 108–110\nSOA pattern, 223–225\nSocial networks in publish-subscribe pattern, \n229\nSocializing in Incremental Commitment \nModel, 286\nSociety\nas goal-object, 302\nservice to, 299\nSoftware architecture importance, 25–26\nchange management, 27–28\nconstraints, 32–33\ncost and schedule estimates, 34\ndesign decisions, 31–32\nevolutionary prototyping, 33–34\nindependently developed components, \n35–36\norganizational structure, 33\nquality attributes, 26–27\nstakeholder communication, 29–31\nsummary, 37\nsystem qualities prediction, 28\ntraining basis, 37\ntransferable, reusable models, 35\nvocabulary restrictions, 36\nSoftware architecture overview, 3–4. See also \nArchitecture\nas abstraction, 5–6\nbehavior in, 6–7\ncompetence, 467–475\ncontexts. See Contexts\ndefinitions, 4\ngood and bad, 19–21\npatterns, 18–19\nselecting, 7\nas set of software structures, 4–5\nstructures and views, 9–18\nsummary, 21–22\nsystem architecture vs. enterprise, 7–8\nSoftware as a Service (SaaS) model, 505\nSoftware Engineering Body of Knowledge \n(SWEBOK), 292\nSoftware Engineering Institute (SEI), 59, 479\nSoftware Product Line Conference (SPLC), \n498\nSoftware Product Line Hall of Fame, 498\nSoftware product lines\nadoption strategies, 494–496\nevaluating, 493–494\nevolving, 496–497\nfailures, 481–482\nintroduction, 479–481\nkey issues, 494–497\norganizational structure, 497\nquality attribute of variability, 488\nreuse potential, 483–486\nrole of, 488–490\nscope, 486–488\nsuccessful, 483–486\nsummary, 497–498\nvariability, 482–483\nvariation mechanisms, 490–493\nSoftware quality attributes, 190–193\nSoftware rejuvenation tactic, 95\nSoftware upgrade tactic, 92–93\nSolutions in relationships, 204–205\nSonarJ tool, 387–391\nSorting in map-reduce pattern, 232\nSoS (system of systems), 106\nSource code\nKSLOC, 279–281\nmapping to, 334\nSource in security scenario, 150\nSource of stimulus\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 148\ntestability, 162–163\nusability, 176\nvariability, 489\nSpare tactics, 91–92, 256–259\nSpecialized interfaces tactic, 165\nSpecification and Description Language \n(SDL), 354\nSpikes in Agile, 284–285\nSPLC (Software Product Line Conference), 498\nSplit module tactic, 123\nSporadic events, 133\nSpring framework, 166\nStaging views, 343\nStakeholders\non ATAM teams, 401\ncommunication among, 29–31, 329\ndocumentation for, 348–349\nevaluation process, 400\ninterests, 52–55\ninterviewing, 294–296\n\n\n584 \nIndex\t\nStakeholders, continued\nfor methods, 272\nutility tree reviews, 306\nviews, 342\nStandard lists for quality attributes, 193–196\nStandards and interoperability, 112–113\nState, system, 164–167\nState machine diagrams, 353\nState resynchronization tactic, 93\nStateless services in cloud, 522\nStates, responsibilities to, 299\nStatic allocation views, 340\nStatic scheduling, 141\nStatus meetings, 428\nStein, Gertrude, 142\nSteinberg, Saul, 39\nStimulus\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 148, 150\nsource. See Source of stimulus\ntestability, 162–163\nusability, 176\nvariability, 489\nStochastic events, 133\nStonebraker, Michael, 518\nStorage\nfor testability, 165\nvirtualization, 512–513\nStrategies in NASA ECS project, 452–456\nStrictly layered patterns, 19\nStructural complexity in testability, 167–168\nStructure101 tool, 387\nStuxnet virus, 80\nSubarchitecture in component-and-connector \nviews, 335\nSubmodules, 333\nSubscriber role, 336\nSubsystems, 9\nSupernodes in peer-to-peer pattern, 220\nSupport and development software, 358–359\nSupport system initiative tactic, 180–181\nSupport user initiative tactic, 179–180\nSWEBOK (Software Engineering Body of \nKnowledge), 292\nSwing classes, 215\nSyncing code and architecture, 368\nSystem analysis and construction, \ndocumentation for, 329\nSystem architecture vs. enterprise \narchitecture, 7–8\nSystem as goal-object, 302\nSystem availability requirements, 81\nSystem efficiency in usability, 175\nSystem engineers, 55\nSystem exceptions tactic, 90\nSystem Generation Module, 358\nSystem initiative in usability, 177\nSystem of systems (SoS), 106\nSystem overview in documentation, 349\nSystem qualities, predicting, 28\nSystem quality attributes, 190–193\nSystem test manager roles, 422\nSystem testing, 371\nTactics\navailability, 87–96\ninteractions, 242–247\ninteroperability, 110–113\nmodifiability, 121–125\npatterns relationships with, 238–242\nperformance, 135–142\nquality attributes, 70–72, 198–199\nsecurity, 150–154\ntestability, 164–168\nusability, 177–181\nTailor interface tactic, 111\nTeam building skills, 463, 465\nTeam leader roles, 422\nTeamCity tool, 172\nTeams\nATAM, 400–401\norganizing, 422\nTechnical contexts\narchitecture influence, 57\nenvironment, 41–42\nquality attributes, 40–41\nVasa ship, 42–43\nTechnical debt, 286\nTechnical processes in security, 157\nTechnology choices, 76\nTechnology knowledge of architects, 467\nTemplates\nATAM, 406\ncode, 365–367\nscenarios. See Scenarios\nvariation mechanism, 492\n10-18 Monkey, 161\nTerminating generate and test process, 316\nTerms and concepts, 368–369\nTest harnesses, 160\n\n\nIndex\n585\nTestability\nanalytic model space, 259\nautomation, 171–172\nbroker pattern, 241\ndesign checklist, 169–170\ngeneral scenario, 162–163\nintroduction, 159–162\nsummary, 172\ntactics, 164–168\ntest data, 170–171\nTestable requirements, 292\nTestComplete tool, 172\nTesters, 55\nTests and testing\nactivities, 374–375\narchitect role, 375–376, 463\nblack-box and white-box, 372–373\nchoices, 315\nin incremental development, 428\nlevels, 370–372\nmodules, 334\nproduct reuse, 484\nrisk-based, 373–374\nsummary, 376\nTherac-25 fatal overdose, 192\nThought experiments, 262–264\nThousands of source lines of code (KSLOC), \n279–281\nThreads in concurrency, 132–133\nThroughput of systems, 134\nTiers\ncomponent-and-connector views, 337\nmulti-tier pattern, 235–237\nTime and time management\nbasis sets, 261\nglobal development, 424\nperformance, 131\nTime boxing, 264\nTime of day factor in equipment utilization, 508\nTime of year factor in equipment utilization, \n508\nTime-sharing, 503\nTime stamp tactic, 89\nTime to market\nindependently developed components \nfor, 36\nand modifiability, 284\nTimeout tactic, 91\nTiming in availability, 85\nTMR (triple modular redundancy), 89\nTools\nfor product reuse, 484\nselecting, 463\nTop-down adoption, 495\nTop-down analysis mode, 284\nTop-down schedules, 420–421\nTopic-based publish-subscribe patterns, 229\nTopological constraints, 236\nTorvalds, Linus, 530, 535, 538\nTotal benefit in CBAM, 446\nTraces for behavior documentation, 351–353\nTracking progress, 428–429\nTradeoffs\nATAM, 403\nimplementation, 427\nTraffic systems, 142\nTraining, architecture for, 37\nTransactions\navailability, 95\ndatabases, 519–520\nSOAP, 108\nTransferable models, 35\nTransformation systems, 215\nTransforming existing systems, 462\nTransitions in state machine diagrams, 354\nTriple modular redundancy (TMR), 89\nTroeh, Eve, 190\nTurner, R., 279, 281, 288\nTwitter, 528\nTwo-phase commits, 95\nUbiquitous network access, 504\nUDDI (Universal Description, Discovery and \nIntegration) language, 108\nUML\nactivity diagrams, 353\ncommunication diagrams, 353\ncomponent-and-connector views, \n338–339\nconnectors, 369\nsequence diagrams, 351–352\nstate machine diagrams, 353\nUnambiguous requirements, 292\nUncertainty in equipment utilization, \n508–509\nUndo command, 179\nUnified Process, 44\nUnit testing, 370–371\nUnity of purpose in modules, 121\nUniversal Description, Discovery and \nIntegration (UDDI) language, 108\nUp-front planning vs. agility, 278–281\nUsability\nanalytic model space, 259\ndesign checklist, 181–182\ngeneral scenario, 176\n\n\n586 \nIndex\t\nUsability, continued\nintroduction, 175\nquality attributes checklists, 260\ntactics, 177–181\nUsability quality attribute, 193, 307\nUsage\nallocation views, 339\ncomponent-and-connector views, 337\nmodular views, 333\nUse an intermediary tactic, 245\nmodifiability, 123\nquality attributes, 72\nUse cases\nATAM presentations, 406\nthought experiments, 263\nfor traces, 351\n“User beware” proviso, 372\nUser initiative in usability, 177\nUser interface\nexchanging information via, 104–105\nseparating, 178\nUser needs in usability, 175\nUser stories in Agile, 278\nUsers\ncommunication with, 29\ndescription and interests, 55\nUses\nfor documentation, 328–329\nviews for, 332\nUses relation in layered patterns, 19\nUses structure in decomposition, 12\nUtility\nassigning, 452\nCBAM, 448\nUtility-response curves, 439–443\nUtility trees\nASRs, 304–307\nATAM, 407, 410\nLightweight Architecture Evaluation, 416\nUtilization of equipment in cloud, 508–509\nValue component\nbusiness goals, 301\nutility trees, 306\nValue for cost (VFC), 438, 442\nVariability\nproduct line, 482–483\nquality attributes, 488–489\nVariability attribute, 186\nVariability guides, 347, 493\nVariation\nbinding time, 75\nsoftware product lines, 490–493\nVariation points\nCBAM, 448–450\nidentifying, 490\nVasa ship, 42–43\nVascular view of human body, 9\nVehicle cruise control systems, 353\nVerify and refine requirements in ADD, \n321–323\nVerify message integrity tactic, 151\nVertical scalability, 187\nVFC (value for cost), 438, 442\nViews, 331–332\nallocation, 339–340\narchitectural structures, 9–10\nchoosing, 341–343\ncombining, 343–345\ncomponent-and-connector, 335–339, 344, \n406\ndocumenting, 345–347\nfused, 388–389\nModel-View-Controller pattern, 213–214\nmodule, 332–335, 406\nquality, 340–341\nViews and Beyond approach, 282, 356–357\nVilla, Pancho, 541\nViolations, finding, 389–392\nVirtual resource managers, 515\nVirtual system of systems, 106\nVirtualization and virtual machines\ncloud, 509–514, 520–521\nlayers as, 13\nin sandboxing, 166\nVisibility of interfaces, 333\nVitruvius, 459\nVlissides, J., 212\nVocabulary\nquality attributes, 67\nrestrictions, 36\nVoting tactic, 89\nVulnerabilities in security views, 341\nWalking skeleton method, 287\nWar ship example, 42–43\nWarm spare tactic, 91–92\nWatchdogs, 89\nWaterfall model\ndescription, 44\nrequirements documents, 56\nWeaknesses\nbroker pattern, 211, 240–242\nclient-server pattern, 218\nlayered pattern, 207\nmap-reduce patterns, 235\n\n\nIndex\n587\nModel-View-Controller pattern, 213\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nWealth of Networks (Benkler), 528\nWeb 2.0 movement, 527\nWeb-based system events, 131\nWeb-conferencing systems\nAgile example, 283–285\nconsiderations, 265\nWeb Services Description Language \n(WSDL), 110\nWebArrow web-conferencing system, \n284–285\nWebSphere MQ product, 224\nWeighting scenarios, 441, 444\nWells, H. G., 117\nWest, Mae, 131\n“What if” questions in performance \nanalysis, 255\nWhite-box testing, 372–373\nWhitney, Eli, 35–36, 480\nWikipedia, 528\nWikis for documentation, 350\nWisdom of crowds, 537\nWoods, Eoin, 25, 170\nWork assignment structures, 14\nWork-breakdown structures, 33\nWork skills of architect, 465\nWorld Wide Web as client-server pattern, 219\nWrappers, 129\nWriter role in component-and-connector \nviews, 335\nWS*, 108–110\nWSDL (Web Services Description \nLanguage), 110\nX-ability, 196–199\nX-ray view of human body, 9\nYAGNI principle, 282\nYahoo! map-reduce patterns, 234\nYoung, Toby, 39\nYouTube, 528\nZoning policies analogy in Metropolis \nmodel, 536\n\n\n588\nSpecial permission to reproduce portions of the following works copyright by \nCarnegie Mellon University is granted by the Software Engineering Institute:\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Software Architecture Documentation \nin Practice: Documenting Architectural Layers,” CMU/SEI-2000-SR-004, March \n2000.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architectures: \nOrganization of Documentation Package,” CMU/SEI-2001-TN-010, August \n2001.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architecture: \nDocumenting Behavior,” CMU/SEI-2002-TN-001, January 2002.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architecture: \nDocumenting Interfaces,” CMU/SEI-2002-TN-015, June 2002.\nFelix Bachmann and Paul Clements. “Variability in Product Lines,” CMU/SEI-\n2005-TR-012, September 2005. \nFelix Bachmann, Len Bass, and Robert Nord. “Modifiability Tactics,” CMU/SEI-\n2007-TR-002, September 2007.\nMario R. Barbacci, Robert Ellison, Anthony J. Lattanze, Judith A. Stafford, \nCharles B. Weinstock, and William G. Wood. “Quality Attribute Workshops \n(QAWs), Third Edition,” CMU/SEI-2003-TR-016, August 2003.\nLen Bass, Paul Clements, Rick Kazman, and Mark Klein. “Models for Evaluat-\ning and Improving Architecture Competence,” CMU/SEI-2008-TR-006, March \n2008.\nLen Bass, Paul Clements, Rick Kazman, John Klein, Mark Klein, and Jeannine \nSiviy. “A Workshop on Architecture Competence,” CMU/SEI-2009-TN-005, \nApril 2009.\nLisa Brownsword, David Carney, David Fisher, Grace Lewis, Craig Meyers, Ed-\nwin Morris, Patrick Place, James Smith, and Lutz Wrage. “Current Perspectives \non Interoperability,” CMU/SEI-2004-TR-009, March 2004.\nPaul Clements and Len Bass. “Relating Business Goals to Architecturally Signif-\nicant Requirements for Software Systems,” CMU/SEI-2010-TN-018, May 2010.\nRick Kazman and Jeromy Carriere, “Playing Detective: Reconstructing Software \nArchitecture from Available Evidence,” CMU/SEI-97-TR-010, October 1997.\n",
      "page_number": 601
    },
    {
      "number": 66,
      "title": "Segment 66 (pages 609-617)",
      "start_page": 609,
      "end_page": 617,
      "detection_method": "topic_boundary",
      "content": "589\nRick Kazman, Mark Klein, and Paul Clements. “ATAM: Method for Architecture \nEvaluation,” CMU/SEI-2000-TR-004, August 2000.\nRick Kazman, Jai Asundi, and Mark Klein, “Making Architecture Design Deci-\nsions, An Economic Approach,” CMU/SEI-2002-TR-035, September 2002.\nRick Kazman, Liam O’Brien, and Chris Verhoef, “Architecture Reconstruction \nGuidelines, Third Edition,” CMU/SEI-2002-TR-034, November 2003. \nRobert L. Nord, Paul C. Clements, David Emery, and Rich Hilliard. “A Structured \nApproach for Reviewing Architecture Documentation,” CMU/SEI-2009-TN-030, \nDecember 2009.\nJames Scott and Rick Kazman. “Realizing and Refining Architectural Tactics: \nAvailability,” CMU/SEI-2009-TR-006 and ESC-TR-2009-006, August 2009.\n\n\nThis page intentionally left blank \n\n\nBuild Your Credentials with the SEI Software \nArchitecture Professional Certificate\nAs a software architect, you know \nthat complexity is rampant and always \nincreasing. It simply takes more \nskills now to deliver the richly featured, \nhigh-performing products that \ncustomers demand. \nThe SEI Software Architecture \nProfessional Certificate helps you \nkeep pace. From this four-course \nsequence, you’ll gain the ability to \n• apply architecture-centric\n practices throughout the life cycle\n•  produce and understand documen-\ntation for software architecture\n• understand the relationships among\n system qualities such as security \n and performance, architecture, and \n your organization’s business goals \nTake the first step in building credentials \nby registering for the SEI Software \nArchitecture: Principles and Practices \ncourse. Visit www.sei.cmu.edu/go/\nsapptraining/ for details.  \nThe SEI also offers an Architecture \nTradeoff Analysis Method® (ATAM®) \nEvaluator Certificate and an ATAM \nLeader Certification. \n \nFor more information on the Software \nArchitecture Professional Certificate \nand other SEI software architecture \ncredentials, visit www.sei.cmu.edu/go/\narchitecture-credentials/.\n \nArchitecture Tradeoff Analysis Method and ATAM  \nare registered in the U.S. Patent and Trademark Office  \nby Carnegie Mellon University. \n\n\nRegister the Addison-Wesley, Exam \nCram, Prentice Hall, Que, and \nSams products you own to unlock \ngreat beneﬁ ts. \nTo begin the registration process, \nsimply go to informit.com/register \nto sign in or create an account. \nYou will then be prompted to enter \nthe 10- or 13-digit ISBN that appears \non the back cover of your product.\ninformIT.com \nTHE TRUSTED TECHNOLOGY LEARNING SOURCE\nAddison-Wesley  |  Cisco Press  |  Exam Cram  \nIBM Press   |   Que   |   Prentice Hall   |   Sams \nSAFARI BOOKS ONLINE\nAbout InformIT — THE TRUSTED TECHNOLOGY LEARNING SOURCE\nINFORMIT IS HOME TO THE LEADING TECHNOLOGY PUBLISHING IMPRINTS \nAddison-Wesley Professional, Cisco Press, Exam Cram, IBM Press, Prentice Hall \nProfessional, Que, and Sams. Here you will gain access to quality and trusted content and \nresources from the authors, creators, innovators, and leaders of technology. Whether you’re \nlooking for a book on a new technology, a helpful article, timely newsletters, or access to \nthe Safari Books Online digital library, InformIT has a solution for you.\nRegistering your products can unlock \nthe following beneﬁ ts:\n•  Access to supplemental content, \nincluding bonus chapters, \nsource code, or project ﬁ les. \n•  A coupon to be used on your \nnext purchase.\nRegistration beneﬁ ts vary by product.  \nBeneﬁ ts will be listed on your Account \npage under Registered Products.\ninformit.com/register\nTHIS PRODUCT\n\n\n InformIT is a brand of Pearson and the online presence \nfor the world’s leading technology publishers. It’s your source \nfor reliable and qualified content and knowledge, providing \naccess to the top brands, authors, and contributors from \nthe tech community.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nLearnIT at InformIT\nLooking for a book, eBook, or training video on a new technology? Seek-\ning timely and relevant information and tutorials? Looking for expert opin-\nions, advice, and tips?  InformIT has the solution.\n•  Learn about new releases and special promotions by \nsubscribing to a wide variety of newsletters. \nVisit informit.com/newsletters.\n•   Access FREE podcasts from experts at informit.com/podcasts.\n•   Read the latest author articles and sample chapters at \ninformit.com/articles.\n•  Access thousands of books and videos in the Safari Books \nOnline digital library at safari.informit.com.\n• Get tips from expert blogs at informit.com/blogs.\nVisit informit.com/learn to discover all the ways you can access the \nhottest technology content.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nAre You Part of the IT Crowd?\nConnect with Pearson authors and editors via RSS feeds, Facebook, \nTwitter, YouTube, and more! Visit informit.com/socialconnect.\n\n\nSample concrete interoperability scenario\nStimulus:\nResponse:\nEnvironment:\nSystems known\nprior to run-time\nArtifact:\nResponse\nMeasure:\nSource\nof Stimulus:\n3\n2\n1\n4\nOur Vehicle \nInformation \nSystem\nCurrent\nLocation\nSent\nTraffic Monitor \nCombines Current \nLocation with Other \nInformation, \nOverlays on Google \nMaps, and \nBroadcasts\nOur Information \nIncluded Correctly \n99.9% of the Time\nTraffic Monitoring \nSystem\nSample concrete availability scenario\nStimulus:\nServer\nUnresponsive\nResponse:\nInform \nOperator\nContinue\nto Operate\nResponse\nMeasure:\nNo Downtime\nSource:\nHeartbeat\nMonitor\nArtifact:\nProcess\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSample concrete modiﬁability scenario\nStimulus:\nWishes\nto Change\nthe UI\nResponse:\nChange Made \nand Unit Tested \nSource:\nDeveloper\nArtifact:\nCode\nEnvironment:\n \nDesign\nTime\nResponse\nMeasure:\nIn Three\nHours\n3\n2\n1\n4\n\n\nSample concrete security scenario \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nNormal\nOperations\n3\n2\n1\n4\nDisgruntled \nEmployee from \nRemote Location\nAttempts to \nModify Pay \nRate\nSystem\nMaintains\nAudit Trail\nCorrect Data Is \nRestored within a\nDay and Source \nof Tampering \nIdentified\nArtifact:\nData within\nthe System\nSample performance scenario\nStimulus:\nInitiate\nTransactions\nResponse:\nTransactions\nAre Processed\nResponse\nMeasure:\nAverage\nLatency\nof Two\nSource:\nUsers\nArtifact:\nSystem\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSeconds\n\n\nSample concrete testability scenario\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nDevelopment\n3\n2\n1\n4\nUnit Tester\nCode Unit \nCompleted\nResults Captured\n85% Path Coverage \nin Three Hours\nArtifact:\nCode Unit\nSample concrete usability scenario \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nRuntime\n3\n2\n1\n4\nUser\nDownloads \na New \nApplication\nUser Uses\nApplication\nProductively\nWithin Two\nMinutes of\nExperimentation\nArtifact:\nSystem\n\n\nInteroperability Tactics\nLocate\nManage Interfaces\nDiscover\nService\nOrchestrate\nTailor Interface\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nAvailability Tactics\nDetect Faults\nPrevent Faults\nPing / Echo\nRemoval from\nService\nMonitor\nTransactions\nPredictive\nModel\nRecover from Faults\nHeartbeat\nPreparation\nand Repair\nReintroduction\nActive\nRedundancy\nPassive\nRedundancy\nSpare\nEscalating\nRestart\nException\nHandling\nShadow\nNon-Stop\nForwarding\nState\nResynchronization\nException\nPrevention\nFault\nFault\nMasked\nor\nRepair\nMade\nTimestamp\nSanity\nChecking\nCondition\nMonitoring\nVoting\nException\nDetection\nSelf-Test\nRollback\nSoftware\nUpgrade\nRetry\nIgnore Faulty\nBehavior\nDegradation\nReconfiguration\nIncrease\nCompetence Set\n",
      "page_number": 609
    },
    {
      "number": 67,
      "title": "Segment 67 (pages 618-620)",
      "start_page": 618,
      "end_page": 620,
      "detection_method": "topic_boundary",
      "content": "Modifiability Tactics\nIncrease\nCohesion\nReduce\nCoupling\nSplit Module\nEncapsulate\nUse an\nIntermediary\nChange\nArrives\nChange Made\nwithin Time \nand Budget\nReduce Size\nof a Module\nIncrease\nSemantic\nCoherence\nRestrict\nDependencies\nRefactor\nAbstract Common\nServices\nDefer\nBinding\nPerformance Tactics\nControl Resource Demand\nManage Resources\nManage Sampling Rate\nLimit Event Response\nPrioritize Events\nReduce Overhead\nBound Execution Times\nIncrease Resource\nEfficiency\nEvent\nArrives\nResponse\nGenerated within\nTime Constraints\nIncrease Resources\nIntroduce Concurrency\nMaintain Multiple\nCopies of Computations\nMaintain Multiple\nCopies of Data\nBound Queue Sizes\nSchedule Resources\n\n\nUsability Tactics\nSupport User\nInitiative\nSupport System\nInitiative\nCancel\nMaintain User\nModel\nMaintain System\nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nUndo\nPause/Resume\nAggregate\nMaintain Task\nModel\nTestability Tactics\nControl and Observe\nSystem State\nLimit Complexity\nSpecialized\nInterfaces\nLimit Structural\nComplexity\nLimit\nNondeterminism\nTests\nExecuted\nFaults\nDetected\nRecord/\nPlayback\nLocalize State\nStorage\nSandbox\nExecutable\nAssertions\nAbstract Data\nSources\nSecurity Tactics\nResist Attacks\nEncrypt Data\nAttack\nSystem Detects,\nResists, Reacts,\nor Recovers\nDetect Attacks\nMaintain\nAudit Trail\nLimit Exposure\nRecover\nfrom Attacks\nReact to\nAttacks\nRevoke\nAccess\nLock\nComputer\nDetect\nIntrustion\nDetect Service\nDenial\nVerify Message\nIntegrity\nDetect Message\nDelay\nChange Default\nSettings\nSeparate\nEntities\nRestore\nSee\nAvailability\nIdentify\nActors\nAuthenticate\nActors\nAuthorize\nActors\nLimit Access\nInform\nActors\n",
      "page_number": 618
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Software\nArchitecture\nin Practice\nThird Edition \n",
      "content_length": 49,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "T\nhe SEI Series in Software Engineering represents is a collaborative \nundertaking of the Carnegie Mellon Software Engineering Institute (SEI) and \nAddison-Wesley to develop and publish books on software engineering and \nrelated topics. The common goal of the SEI and Addison-Wesley is to provide \nthe most current information on these topics in a form that is easily usable by \npractitioners and students.\nBooks in the series describe frameworks, tools, methods, and technologies \ndesigned to help organizations, teams, and individuals improve their technical \nor management capabilities. Some books describe processes and practices for \ndeveloping higher-quality software, acquiring programs for complex systems, or \ndelivering services more effectively. Other books focus on software and system \narchitecture and product-line development. Still others, from the SEI’s CERT \nProgram, describe technologies and practices needed to manage software \nand network security risk. These and all books in the series address critical \nproblems in software engineering for which practical solutions are available. \nVisit informit.com/sei for a complete list of available products.\nThe SEI Series in \nSoftware Engineering\n",
      "content_length": 1213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Software\nArchitecture\nin Practice\nThird Edition \nLen Bass\nPaul Clements\nRick Kazman\n▼\n▲\n▼ Addison-Wesley\nUpper Saddle River, NJ  •  Boston  •  Indianapolis  •  San Francisco\nNew York  •  Toronto  •  Montreal  •  London  •  Munich  •  Paris  •  Madrid\nCapetown  •  Sydney  •  Tokyo  •  Singapore  •  Mexico City\n",
      "content_length": 311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "The SEI Series in Software Engineering\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nCMM, CMMI, Capability Maturity Model, Capability Maturity Modeling, Carnegie Mellon, CERT, \nand CERT Coordination Center are registered in the U.S. Patent and Trademark Office by Carnegie \nMellon University. \nATAM; Architecture Tradeoff Analysis Method; CMM Integration; COTS Usage-Risk Evaluation; \nCURE; EPIC; Evolutionary Process for Integrating COTS Based Systems; Framework for Software \nProduct Line Practice; IDEAL; Interim Profile; OAR; OCTAVE; Operationally Critical Threat, Asset, \nand Vulnerability Evaluation; Options Analysis for Reengineering; Personal Software Process; PLTP; \nProduct Line Technical Probe; PSP; SCAMPI; SCAMPI Lead Appraiser; SCAMPI Lead Assessor; \nSCE; SEI; SEPG; Team Software Process; and TSP are service marks of Carnegie Mellon University. \nSpecial permission to reproduce portions of works copyright by Carnegie Mellon University, as listed \non page 588, is granted by the Software Engineering Institute.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in this book, and the publisher was aware of a trade-\nmark claim, the designations have been printed with initial capital letters or in all capitals.\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or \nimplied warranty of any kind and assume no responsibility for errors or omissions. No liability is \nassumed for incidental or consequential damages in connection with or arising out of the use of the \ninformation or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales opportunities (which may \ninclude electronic versions; custom cover designs; and content particular to your business, training \ngoals, marketing focus, or branding interests), please contact our corporate sales department at corp-\nsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the U.S., please contact international@pearsoned.com.\nVisit us on the Web: informit.com/aw\nLibrary of Congress Cataloging-in-Publication Data\nBass, Len.\n    Software architecture in practice / Len Bass, Paul Clements, Rick Kazman.—3rd ed.\n    p.  cm.—(SEI series in software engineering)\n    Includes bibliographical references and index.\n    ISBN 978-0-321-81573-6 (hardcover : alk. paper) 1. Software architecture. 2. System  \ndesign. I. Clements, Paul, 1955– II. Kazman, Rick. III. Title.\n    QA76.754.B37 2012\n    005.1—dc23\n\t\n2012023744\nCopyright © 2013 Pearson Education, Inc.\nAll rights reserved. Printed in the United States of America. This publication is protected by copy-\nright, and permission must be obtained from the publisher prior to any prohibited reproduction, stor-\nage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, pho-\ntocopying, recording, or likewise. To obtain permission to use material from this work, please submit \na written request to Pearson Education, Inc., Permissions Department, 200 Old Tappan Road, Old \nTappan, New Jersey 07657, or you may fax your request to (201) 236-3290.\nISBN-13: 978-0-321-81573-6 \nISBN-10: 0-321-81573-4\nText printed in the United States on recycled paper at Courier in Westford, Massachusetts. \nFifth printing, September 2015\n",
      "content_length": 3720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "v\nContents\nPreface    xv\nReader’s Guide    xvii\nAcknowledgments    xix\n\tPart ONE\t Introduction    1\nCHAPTER  1\nWhat Is Software Architecture?    3\n1.1\tWhat Software Architecture Is and What It \nIsn’t        4\n1.2  Architectural Structures and Views        9\n1.3  Architectural Patterns        18\n1.4  What Makes a “Good” Architecture?        19\n1.5  Summary        21\n1.6  For Further Reading        22\n1.7  Discussion Questions        23\nCHAPTER  2\nWhy Is Software Architecture Important?    25\n2.1  Inhibiting or Enabling a System’s Quality \nAttributes        26\n2.2  \u0007Reasoning About and Managing \nChange        27\n2.3  Predicting System Qualities         28\n2.4  \u0007Enhancing Communication among \nStakeholders        29\n2.5  Carrying Early Design Decisions        31\n2.6  \u0007Defining Constraints on an \nImplementation        32\n2.7  Influencing the Organizational Structure         33\n2.8  Enabling Evolutionary Prototyping        33\n",
      "content_length": 934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "vi \nContents\t\n2.9  Improving Cost and Schedule Estimates        34\n2.10  Supplying a Transferable, Reusable \nModel        35\n2.11  \u0007Allowing Incorporation of Independently \nDeveloped Components        35\n2.12  Restricting the Vocabulary of Design \nAlternatives        36\n2.13  Providing a Basis for Training        37\n2.14  Summary        37\n2.15  For Further Reading        38\n2.16  Discussion Questions        38\nCHAPTER  3\nThe Many Contexts of Software \nArchitecture    39\n3.1  Architecture in a Technical Context        40\n3.2  Architecture in a Project Life-Cycle \nContext        44\n3.3  Architecture in a Business Context        49\n3.4  Architecture in a Professional Context        51\n3.5  Stakeholders        52\n3.6  How Is Architecture Influenced?        56\n3.7  What Do Architectures Influence?         57\n3.8  Summary         59\n3.9  For Further Reading        59\n3.10  Discussion Questions        60\n\tPart TWO\t Quality Attributes    61\nCHAPTER  4\nUnderstanding Quality Attributes    63\n4.1  Architecture and Requirements        64\n4.2  Functionality        65\n4.3  Quality Attribute Considerations         65\n4.4  \tSpecifying Quality Attribute \nRequirements        68\n4.5  \tAchieving Quality Attributes through \nTactics        70\n4.6  Guiding Quality Design Decisions        72\n4.7  Summary        76\n",
      "content_length": 1313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Contents\nvii\n4.8  For Further Reading        77\n4.9  Discussion Questions        77\nCHAPTER  5\nAvailability    79\n5.1  Availability General Scenario        85\n5.2  Tactics for Availability        87\n5.3  A Design Checklist for Availability        96\n5.4  Summary        98\n5.5  For Further Reading        99\n5.6  Discussion Questions        100\nCHAPTER  6\nInteroperability    103\n6.1  Interoperability General Scenario        107\n6.2  Tactics for Interoperability        110\n6.3  A Design Checklist for Interoperability        114\n6.4  Summary        115\n6.5  For Further Reading        116\n6.6  Discussion Questions        116\nCHAPTER  7\nModifiability    117\n7.1 Modifiability General Scenario        119\n7.2  Tactics for Modifiability        121\n7.3  A Design Checklist for Modifiability        125\n7.4  Summary        128\n7.5  For Further Reading        128\n7.6  Discussion Questions        128\nCHAPTER  8\nPerformance    131\n8.1  Performance General Scenario        132\n8.2  Tactics for Performance        135\n8.3  A Design Checklist for Performance        142\n8.4  Summary        145\n8.5  For Further Reading        145\n8.6  Discussion Questions        145\nCHAPTER  9\nSecurity    147\n9.1  Security General Scenario        148\n9.2  Tactics for Security        150\n",
      "content_length": 1267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "viii \nContents\t\n9.3  A Design Checklist for Security        154\n9.4  Summary        156\n9.5  For Further Reading        157\n9.6  Discussion Questions        158\nCHAPTER  10\nTestability    159\n10.1  Testability General Scenario        162\n10.2  Tactics for Testability        164\n10.3  A Design Checklist for Testability        169\n10.4  Summary        172\n10.5  For Further Reading        172\n10.6  Discussion Questions        173\nCHAPTER  11\nUsability    175\n11.1  Usability General Scenario        176\n11.2  Tactics for Usability        177\n11.3  A Design Checklist for Usability        181\n11.4  Summary        183\n11.5  For Further Reading        183\n11.6  Discussion Questions        183\nCHAPTER  12\nOther Quality Attributes    185\n12.1  Other Important Quality Attributes        185\n12.2  Other Categories of Quality Attributes        189\n12.3  \u0007Software Quality Attributes and System \nQuality Attributes        190\n12.4  \u0007Using Standard Lists of Quality Attributes— \nor Not         193\n12.5  \u0007Dealing with “X-ability”: Bringing a New \nQuality Attribute into the Fold        196\n12.6  For Further Reading        200\n12.7  Discussion Questions        201\nCHAPTER  13\nArchitectural Tactics and Patterns    203\n13.1  Architectural Patterns        204\n13.2  Overview of the Patterns Catalog        205\n13.3  \u0007Relationships between Tactics and \nPatterns         238\n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Contents\nix\n13.4  Using Tactics Together         242\n13.5  Summary        247\n13.6  For Further Reading        248\n13.7  Discussion Questions        249\nCHAPTER  14\nQuality Attribute Modeling and Analysis    251\n14.1  Modeling Architectures to Enable Quality \nAttribute Analysis        252\n14.2  Quality Attribute Checklists        260\n14.3  \u0007Thought Experiments and  \nBack-of-the-Envelope Analysis        262\n14.4  Experiments, Simulations, and \nPrototypes        264\n14.5  Analysis at Different Stages of the Life \nCycle        265\n14.6  Summary        266\n14.7  For Further Reading        267\n14.8  Discussion Questions        269\n\t Part THREE\t Architecture in the Life \nCycle    271\nCHAPTER  15\nArchitecture in Agile Projects    275\n15.1  How Much Architecture?        277\n15.2  Agility and Architecture Methods        281\n15.3  A Brief Example of Agile Architecting        283\n15.4  Guidelines for the Agile Architect        286\n15.5  Summary        287\n15.6  For Further Reading        288\n15.7  Discussion Questions        289\nCHAPTER  16\nArchitecture and Requirements    291\n16.1  Gathering ASRs from Requirements \nDocuments        292\n16.2  Gathering ASRs by Interviewing \nStakeholders        294\n16.3  \u0007Gathering ASRs by Understanding the \nBusiness Goals        296\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "x \nContents\t\n16.4  Capturing ASRs in a Utility Tree        304\n16.5  Tying the Methods Together        308\n16.6  Summary        308\n16.7  For Further Reading        309\n16.8  Discussion Questions        309\nCHAPTER  17\nDesigning an Architecture    311\n17.1  Design Strategy        311\n17.2  The Attribute-Driven Design Method        316\n17.3  The Steps of ADD        318\n17.4  Summary        325\n17.5  For Further Reading        325\n17.6  Discussion Questions        326\nCHAPTER  18\nDocumenting Software Architectures    327\n18.1  \u0007Uses and Audiences for Architecture \nDocumentation        328\n18.2  Notations for Architecture \nDocumentation        329\n18.3  Views        331\n18.4  Choosing the Views        341\n18.5  Combining Views         343\n18.6  Building the Documentation Package        345\n18.7  Documenting Behavior        351\n18.8  Architecture Documentation and Quality \nAttributes         354\n18.9  \u0007Documenting Architectures That Change \nFaster Than You Can Document Them        355\n18.10  \u0007Documenting Architecture in an Agile \nDevelopment Project        356\n18.11  Summary        359\n18.12  For Further Reading        360\n18.13  Discussion Questions        360\nCHAPTER  19\nArchitecture, Implementation, and \nTesting    363\n19.1  Architecture and Implementation        363\n19.2  Architecture and Testing        370\n",
      "content_length": 1329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Contents\nxi\n19.3  Summary        376\n19.4  For Further Reading        376\n19.5  Discussion Questions        377\nCHAPTER  20\nArchitecture Reconstruction and \nConformance    379\n20.1  Architecture Reconstruction Process         381\n20.2  Raw View Extraction        382\n20.3  Database Construction        386\n20.4  View Fusion        388\n20.5  Architecture Analysis: Finding \nViolations        389\n20.6  Guidelines        392\n20.7  Summary        393\n20.8  For Further Reading        394\n20.9  Discussion Questions         395\nCHAPTER  21\nArchitecture Evaluation    397\n21.1  Evaluation Factors        397\n21.2  The Architecture Tradeoff Analysis \nMethod        400\n21.3  Lightweight Architecture Evaluation        415\n21.4  Summary        417\n21.5  For Further Reading        417\n21.6  Discussion Questions         418\nCHAPTER  22\nManagement and Governance    419\n22.1  Planning        420\n22.2  Organizing        422\n22.3  Implementing        427\n22.4  Measuring        429\n22.5  Governance        430\n22.6  Summary        432\n22.7  For Further Reading        432\n22.8  Discussion Questions        433\n",
      "content_length": 1101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xii \nContents\t\n\t\nPart FOUR\t Architecture and \nBusiness    435\nCHAPTER  23\nEconomic Analysis of Architectures    437\n23.1  Decision-Making Context        438\n23.2  The Basis for the Economic Analyses        439\n23.3  Putting Theory into Practice:  \nThe CBAM        442\n23.4  Case Study: The NASA ECS Project        450\n23.5  Summary        457\n23.6  For Further Reading        458\n23.7  Discussion Questions        458\nCHAPTER  24\nArchitecture Competence    459\n24.1  \u0007Competence of Individuals: Duties, Skills, and \nKnowledge of Architects        460\n24.2  Competence of a Software Architecture \nOrganization        467\n24.3  Summary        475\n24.4  For Further Reading        475\n24.5  Discussion Questions        477\nCHAPTER  25\nArchitecture and Software Product Lines    479\n25.1  An Example of Product Line \nVariability        482\n25.2  What Makes a Software Product Line \nWork?        483\n25.3  Product Line Scope        486\n25.4  The Quality Attribute of Variability        488\n25.5  The Role of a Product Line \nArchitecture        488\n25.6  Variation Mechanisms        490\n25.7  Evaluating a Product Line \nArchitecture        493\n25.8  Key Software Product Line Issues        494\n25.9  Summary        497\n25.10  For Further Reading        498\n25.11  Discussion Questions        498\n",
      "content_length": 1290,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Contents\nxiii\n\tPart FIVE\t The Brave New World    501\nCHAPTER  26\nArchitecture in the Cloud    503\n26.1  Basic Cloud Definitions        504\n26.2  Service Models and Deployment \nOptions        505\n26.3  Economic Justification        506\n26.4  Base Mechanisms        509\n26.5  Sample Technologies        514\n26.6  Architecting in a Cloud Environment        520\n26.7  Summary        524\n26.8  For Further Reading        524\n26.9  Discussion Questions        525\nCHAPTER  27\nArchitectures for the Edge    527\n27.1  The Ecosystem of Edge-Dominant \nSystems        528\n27.2  Changes to the Software Development Life \nCycle        530\n27.3  Implications for Architecture        531\n27.4  Implications of the Metropolis Model        533\n27.5  Summary        537\n27.6  For Further Reading        538\n27.7  Discussion Questions        538\nCHAPTER  28\nEpilogue    541\nReferences    547\nAbout the Authors    561\nIndex    563\n",
      "content_length": 911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xv\nPreface\nI should have no objection to go over the same \nlife from its beginning to the end: requesting only \nthe advantage authors have, of correcting in a \n[third] edition the faults of the first [two].\n— Benjamin Franklin\nIt has been a decade since the publication of the second edition of this book. \nDuring that time, the field of software architecture has broadened its focus \nfrom being primarily internally oriented—How does one design, evaluate, \nand document software?—to including external impacts as well—a deeper \nunderstanding of the influences on architectures and a deeper understanding of \nthe impact architectures have on the life cycle, organizations, and management.\nThe past ten years have also seen dramatic changes in the types of systems \nbeing constructed. Large data, social media, and the cloud are all areas that, at \nmost, were embryonic ten years ago and now are not only mature but extremely \ninfluential.\nWe listened to some of the criticisms of the previous editions and have \nincluded much more material on patterns, reorganized the material on quality \nattributes, and made interoperability a quality attribute worthy of its own chapter. \nWe also provide guidance about how you can generate scenarios and tactics for \nyour own favorite quality attributes.\nTo accommodate this plethora of new material, we had to make difficult \nchoices. In particular, this edition of the book does not include extended \ncase studies as the prior editions did. This decision also reflects the maturing \nof the field, in the sense that case studies about the choices made in software \narchitectures are more prevalent than they were ten years ago, and they are less \nnecessary to convince readers of the importance of software architecture. The \ncase studies from the first two editions are available, however, on the book’s \nwebsite, at www.informit.com/title/9780321815736. In addition, on the same \nwebsite, we have slides that will assist instructors in presenting this material.\nWe have thoroughly reworked many of the topics covered in this edition. \nIn particular, we realize that the methods we present—for architecture design, \nanalysis, and documentation—are one version of how to achieve a particular \ngoal, but there are others. This led us to separate the methods that we present \n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xvi \nPreface\t\nin detail from their underlying theory. We now present the theory first with \nspecific methods given as illustrations of possible realizations of the theories. \nThe new topics in this edition include architecture-centric project management; \narchitecture competence; requirements modeling and analysis; Agile methods; \nimplementation and testing; the cloud; and the edge.\nAs with the prior editions, we firmly believe that the topics are best discussed \nin either reading groups or in classroom settings, and to that end we have included \na collection of discussion questions at the end of each chapter. Most of these \nquestions are open-ended, with no absolute right or wrong answers, so you, as a \nreader, should emphasize how you justify your answer rather than just answer the \nquestion itself.\n",
      "content_length": 813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xvii\nReader’s Guide\nWe have structured this book into five distinct portions. Part One introduces \narchitecture and the various contextual lenses through which it could be viewed. \nThese are the following:\n■\n■Technical. What technical role does the software architecture play in the \nsystem or systems of which it’s a part? \n■\n■Project. How does a software architecture relate to the other phases of a \nsoftware development life cycle?\n■\n■Business. How does the presence of a software architecture affect an \norganization’s business environment?\n■\n■Professional. What is the role of a software architect in an organization or a \ndevelopment project?\nPart Two is focused on technical background. Part Two describes how \ndecisions are made. Decisions are based on the desired quality attributes for a \nsystem, and Chapters 5–11 describe seven different quality attributes and the \ntechniques used to achieve them. The seven are availability, interoperability, \nmaintainability, performance, security, testability, and usability. Chapter 12 \ntells you how to add other quality attributes to our seven, Chapter 13 discusses \npatterns and tactics, and Chapter 14 discusses the various types of modeling and \nanalysis that are possible.\nPart Three is devoted to how a software architecture is related to the other \nportions of the life cycle. Of special note is how architecture can be used in Agile \nprojects. We discuss individually other aspects of the life cycle: requirements, \ndesign, implementation and testing, recovery and conformance, and evaluation.\nPart Four deals with the business of architecting from an economic \nperspective, from an organizational perspective, and from the perspective of \nconstructing a series of similar systems.\nPart Five discusses several important emerging technologies and how \narchitecture relates to these technologies.\n",
      "content_length": 1856,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "xix\nAcknowledgments\nWe had a fantastic collection of reviewers for this edition, and their assistance \nhelped make this a better book. Our reviewers were Muhammad Ali Babar, Felix \nBachmann, Joe Batman, Phil Bianco, Jeromy Carriere, Roger Champagne, Steve \nChenoweth, Viktor Clerc, Andres Diaz Pace, George Fairbanks, Rik Farenhorst, \nIan Gorton, Greg Hartman, Rich Hilliard, James Ivers, John Klein, Philippe \nKruchten, Phil Laplante, George Leih, Grace Lewis, John McGregor, Tommi \nMikkonen, Linda Northrop, Ipek Ozkaya, Eltjo Poort, Eelco Rommes, Nick \nRozanski, Jungwoo Ryoo, James Scott, Antony Tang, Arjen Uittenbogaard, Hans \nvan Vliet, Hiroshi Wada, Rob Wojcik, Eoin Woods, and Liming Zhu.\nIn addition, we had significant contributions from Liming Zhu, Hong-\nMei Chen, Jungwoo Ryoo, Phil Laplante, James Scott, Grace Lewis, and Nick \nRozanski that helped give the book a richer flavor than one written by just the \nthree of us.\nThe issue of build efficiency in Chapter 12 came from Rolf Siegers and John \nMcDonald of Raytheon. John Klein and Eltjo Poort contributed the “abstract \nsystem clock” and “sandbox mode” tactics, respectively, for testability. The list \nof stakeholders in Chapter 3 is from Documenting Software Architectures: Views \nand Beyond, Second Edition. Some of the material in Chapter 28 was inspired by a \ntalk given by Anthony Lattanze called “Organizational Design Thinking” in 2011.\nJoe Batman was instrumental in the creation of the seven categories of design \ndecisions we describe in Chapter 4. In addition, the descriptions of the security \nview, communications view, and exception view in Chapter 18 are based on material \nthat Joe wrote while planning the documentation for a real system’s architecture. \nMuch of the new material on modifiability tactics was based on the work of Felix \nBachmann and Rod Nord. James Ivers helped us with the security tactics.\nBoth Paul Clements and Len Bass have taken new positions since the \nlast edition was published, and we thank their new respective managements \n(BigLever Software for Paul and NICTA for Len) for their willingness to support \nour work on this edition. We would also like to thank our (former) colleagues at \nthe Software Engineering Institute for multiple contributions to the evolution of \nthe ideas expressed in this edition.\nFinally, as always, we thank our editor at Addison-Wesley, Peter Gordon, \nfor providing guidance and support during the writing and production processes.\n",
      "content_length": 2476,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "1\nPart  O N E\nIntroduction\nWhat is a software architecture? What is it good for? How does it come to be? \nWhat effect does its existence have? These are the questions we answer in Part I.\nChapter 1 deals with a technical perspective on software architecture. We \ndefine it and relate it to system and enterprise architectures. We discuss how the \narchitecture can be represented in different views to emphasize different perspec-\ntives on the architecture. We define patterns and discuss what makes a “good” \narchitecture.\nIn Chapter 2, we discuss the uses of an architecture. You may be surprised \nthat we can find so many—ranging from a vehicle for communication among \nstakeholders to a blueprint for implementation, to the carrier of the system’s \nquality attributes. We also discuss how the architecture provides a reasoned basis \nfor schedules and how it provides the foundation for training new members on a \nteam.\nFinally, in Chapter 3, we discuss the various contexts in which a software ar-\nchitecture exists. It exists in a technical context, in a project life-cycle context, in \na business context, and in a professional context. Each of these contexts defines a \nrole for the software architecture to play, or an influence on it. These impacts and \ninfluences define the Architecture Influence Cycle.\n",
      "content_length": 1314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "3\n1\nWhat Is Software \nArchitecture?\nGood judgment is usually the result of experience. \nAnd experience is frequently the result of bad \njudgment. But to learn from the experience of \nothers requires those who have the experience to \nshare the knowledge with those who follow.\n—Barry LePatner\nWriting (on our part) and reading (on your part) a book about software architec-\nture, which distills the experience of many people, presupposes that \n1.\t\nhaving a software architecture is important to the successful development \nof a software system and \n2.\t\nthere is a sufficient, and sufficiently generalizable, body of knowledge \nabout software architecture to fill up a book.\nOne purpose of this book is to convince you that both of these assumptions are \ntrue, and once you are convinced, give you a basic knowledge so that you can \napply it yourself.\nSoftware systems are constructed to satisfy organizations’ business goals. \nThe architecture is a bridge between those (often abstract) business goals and \nthe final (concrete) resulting system. While the path from abstract goals to con-\ncrete systems can be complex, the good news is that software architectures can be \ndesigned, analyzed, documented, and implemented using known techniques that \nwill support the achievement of these business and mission goals. The complex-\nity can be tamed, made tractable.\nThese, then, are the topics for this book: the design, analysis, documenta-\ntion, and implementation of architectures. We will also examine the influences, \nprincipally in the form of business goals and quality attributes, which inform \nthese activities. \n",
      "content_length": 1617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "4 \nPart One  Introduction\t\n1—What Is Software Architecture?\nIn this chapter we will focus on architecture strictly from a software engineer-\ning point of view. That is, we will explore the value that a software architecture \nbrings to a development project. (Later chapters will take a business and organi-\nzational perspective.)\n1.1  What Software Architecture Is and What It Isn’t\nThere are many definitions of software architecture, easily discoverable with \na web search, but the one we like is this one:\nThe software architecture of a system is the set of structures needed to \nreason about the system, which comprise software elements, relations \namong them, and properties of both. \nThis definition stands in contrast to other definitions that talk about the sys-\ntem’s “early” or “major” design decisions. While it is true that many architectural \ndecisions are made early, not all are—especially in Agile or spiral-development \nprojects. It’s also true that very many decisions are made early that are not archi-\ntectural. Also, it’s hard to look at a decision and tell whether or not it’s “major.” \nSometimes only time will tell. And since writing down an architecture is one of \nthe architect’s most important obligations, we need to know now which decisions \nan architecture comprises.\nStructures, on the other hand, are fairly easy to identify in software, and they \nform a powerful tool for system design. \nLet us look at some of the implications of our definition. \nArchitecture Is a Set of Software Structures\nThis is the first and most obvious implication of our definition. A structure is sim-\nply a set of elements held together by a relation. Software systems are composed \nof many structures, and no single structure holds claim to being the architecture. \nThere are three categories of architectural structures, which will play an import-\nant role in the design, documentation, and analysis of architectures:\n1.\t\nFirst, some structures partition systems into implementation units, which \nin this book we call modules. Modules are assigned specific computational \nresponsibilities, and are the basis of work assignments for programming \nteams (Team A works on the database, Team B works on the business rules, \nTeam C works on the user interface, etc.). In large projects, these elements \n(modules) are subdivided for assignment to subteams. For example, the da-\ntabase for a large enterprise resource planning (ERP) implementation might \nbe so complex that its implementation is split into many parts. The structure \nthat captures that decomposition is a kind of module structure, the module \n",
      "content_length": 2615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "1.1  What Software Architecture Is and What It Isn’t\n5\ndecomposition structure in fact. Another kind of module structure emerges \nas an output of object-oriented analysis and design—class diagrams. If you \naggregate your modules into layers, you’ve created another (and very use-\nful) module structure. Module structures are static structures, in that they \nfocus on the way the system’s functionality is divided up and assigned to \nimplementation teams. \n2.\t\nOther structures are dynamic, meaning that they focus on the way the el-\nements interact with each other at runtime to carry out the system’s func-\ntions. Suppose the system is to be built as a set of services. The services, \nthe infrastructure they interact with, and the synchronization and interaction \nrelations among them form another kind of structure often used to describe \na system. These services are made up of (compiled from) the programs in \nthe various implementation units that we just described. In this book we \nwill call runtime structures component-and-connector (C&C) structures. \nThe term component is overloaded in software engineering. In our use, a \ncomponent is always a runtime entity.\n3.\t\nA third kind of structure describes the mapping from software structures \nto the system’s organizational, developmental, installation, and execution \nenvironments. For example, modules are assigned to teams to develop, and \nassigned to places in a file structure for implementation, integration, and \ntesting. Components are deployed onto hardware in order to execute. These \nmappings are called allocation structures.\nAlthough software comprises an endless supply of structures, not all of them \nare architectural. For example, the set of lines of source code that contain the let-\nter “z,” ordered by increasing length from shortest to longest, is a software struc-\nture. But it’s not a very interesting one, nor is it architectural. A structure is archi-\ntectural if it supports reasoning about the system and the system’s properties. The \nreasoning should be about an attribute of the system that is important to some \nstakeholder. These include functionality achieved by the system, the availability \nof the system in the face of faults, the difficulty of making specific changes to the \nsystem, the responsiveness of the system to user requests, and many others. We \nwill spend a great deal of time in this book on the relationship between architec-\nture and quality attributes like these.\nThus, the set of architectural structures is not fixed or limited. What is archi-\ntectural is what is useful in your context for your system.\nArchitecture Is an Abstraction\nBecause architecture consists of structures and structures consist of elements1 \nand relations, it follows that an architecture comprises software elements and \n1.  In this book we use the term “element” when we mean either a module or a component, and don’t \nwant to distinguish.\n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "6 \nPart One  Introduction\t\n1—What Is Software Architecture?\nhow the elements relate to each other. This means that architecture specifically \nomits certain information about elements that is not useful for reasoning about \nthe system—in particular, it omits information that has no ramifications outside \nof a single element. Thus, an architecture is foremost an abstraction of a system \nthat selects certain details and suppresses others. In all modern systems, elements \ninteract with each other by means of interfaces that partition details about an el-\nement into public and private parts. Architecture is concerned with the public \nside of this division; private details of elements—details having to do solely with \ninternal implementation—are not architectural. Beyond just interfaces, though, \nthe architectural abstraction lets us look at the system in terms of its elements, \nhow they are arranged, how they interact, how they are composed, what their \nproperties are that support our system reasoning, and so forth. This abstraction \nis essential to taming the complexity of a system—we simply cannot, and do not \nwant to, deal with all of the complexity all of the time. \nEvery Software System Has a Software Architecture\nEvery system can be shown to comprise elements and relations among them to \nsupport some type of reasoning. In the most trivial case, a system is itself a single \nelement—an uninteresting and probably non-useful architecture, but an architec-\nture nevertheless. \nEven though every system has an architecture, it does not necessarily follow \nthat the architecture is known to anyone. Perhaps all of the people who designed \nthe system are long gone, the documentation has vanished (or was never pro-\nduced), the source code has been lost (or was never delivered), and all we have is \nthe executing binary code. This reveals the difference between the architecture of \na system and the representation of that architecture. Because an architecture can \nexist independently of its description or specification, this raises the importance \nof architecture documentation, which is described in Chapter 18, and architec-\nture reconstruction, discussed in Chapter 20.\nArchitecture Includes Behavior\nThe behavior of each element is part of the architecture insofar as that behavior \ncan be used to reason about the system. This behavior embodies how elements \ninteract with each other, which is clearly part of our definition of architecture. \nThis tells us that box-and-line drawings that are passed off as architectures \nare in fact not architectures at all. When looking at the names of the boxes (da-\ntabase, graphical user interface, executive, etc.), a reader may well imagine the \nfunctionality and behavior of the corresponding elements. This mental image \napproaches an architecture, but it springs from the imagination of the observ-\ner’s mind and relies on information that is not present. This does not mean that \nthe exact behavior and performance of every element must be documented in \nall circumstances—some aspects of behavior are fine-grained and below the \n",
      "content_length": 3101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "1.1  What Software Architecture Is and What It Isn’t\n7\narchitect’s level of concern. But to the extent that an element’s behavior influ-\nences another element or influences the acceptability of the system as a whole, \nthis behavior must be considered, and should be documented, as part of the soft-\nware architecture. \nNot All Architectures Are Good Architectures\nThe definition is indifferent as to whether the architecture for a system is a good \none or a bad one. An architecture may permit or preclude a system’s achievement \nof its behavioral, quality attribute, and life-cycle requirements. Assuming that we \ndo not accept trial and error as the best way to choose an architecture for a sys-\ntem—that is, picking an architecture at random, building the system from it, and \nthen hacking away and hoping for the best—this raises the importance of archi-\ntecture design, which is treated in Chapter 17, and architecture evaluation, which \nwe deal with in Chapter 21.\nSystem and Enterprise Architectures\nTwo disciplines related to software architecture are system architecture \nand enterprise architecture. Both of these disciplines have broader con-\ncerns than software and affect software architecture through the estab-\nlishment of constraints within which a software system must live. In both \ncases, the software architect for a system should be on the team that pro-\nvides input into the decisions made about the system or the enterprise. \nSystem architecture\nA system’s architecture is a representation of a system in which there \nis a mapping of functionality onto hardware and software components, \na mapping of the software architecture onto the hardware architecture, \nand a concern for the human interaction with these components. That is, \nsystem architecture is concerned with a total system, including hardware, \nsoftware, and humans.\nA system architecture will determine, for example, the functionality that \nis assigned to different processors and the type of network that connects \nthose processors. The software architecture on each of those processors \nwill determine how this functionality is implemented and how the various \nprocessors interact through the exchange of messages on the network.\nA description of the software architecture, as it is mapped to hardware \nand networking components, allows reasoning about qualities such as per-\nformance and reliability. A description of the system architecture will allow \nreasoning about additional qualities such as power consumption, weight, \nand physical footprint.\nWhen a particular system is designed, there is frequently negotiation be-\ntween the system architect and the software architect as to the distribution \n",
      "content_length": 2693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "8 \nPart One  Introduction\t\n1—What Is Software Architecture?\nof functionality and, consequently, the constraints placed on the software \narchitecture.\nEnterprise architecture\nEnterprise architecture is a description of the structure and behavior of an \norganization’s processes, information flow, personnel, and organizational \nsubunits, aligned with the organization’s core goals and strategic direction. \nAn enterprise architecture need not include information systems—clearly \norganizations had architectures that fit the preceding definition prior to the \nadvent of computers—but these days, enterprise architectures for all but the \nsmallest businesses are unthinkable without information system support. \nThus, a modern enterprise architecture is concerned with how an enter-\nprise’s software systems support the business processes and goals of the \nenterprise. Typically included in this set of concerns is a process for deciding \nwhich systems with which functionality should be supported by an enterprise.\nAn enterprise architecture will specify the data model that various sys-\ntems use to interact, for example. It will specify rules for how the enter-\nprise’s systems interact with external systems.\nSoftware is only one concern of enterprise architecture. Two other com-\nmon concerns addressed by enterprise architecture are how the software \nis used by humans to perform business processes, and the standards that \ndetermine the computational environment.\nSometimes the software infrastructure that supports communication \namong systems and with the external world is considered a portion of the \nenterprise architecture; other times, this infrastructure is considered one \nof the systems within an enterprise. (In either case, the architecture of that \ninfrastructure is a software architecture!) These two views will result in dif-\nferent management structures and spheres of influence for the individuals \nconcerned with the infrastructure.\nThe system and the enterprise provide environments for, and constraints \non, the software architecture. The software architecture must live within \nthe system and enterprise, and increasingly it is the focus for achieving the \norganization’s business goals. But all three forms of architecture share im-\nportant commonalities: They are concerned with major elements taken as \nabstractions, the relationships among the elements, and how the elements \ntogether meet the behavioral and quality goals of the thing being built.\nAre these in scope for this book? Yes! (Well, no.)\nSystem and enterprise architectures share a great deal with software ar-\nchitectures. All can be designed, evaluated, and documented; all answer \nto requirements; all are intended to satisfy stakeholders; all consist of \nstructures, which in turn consist of elements and relationships; all have a \nrepertoire of patterns and styles at their respective architects’ disposal; \nand the list goes on. So to the extent that these architectures share com-\nmonalities with software architecture, they are in the scope of this book. \nBut like all technical disciplines, each has its own specialized vocabulary \nand techniques, and we won’t cover those. Copious other sources do.\n",
      "content_length": 3202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "1.2  Architectural Structures and Views\n9\n1.2  Architectural Structures and Views\nThe neurologist, the orthopedist, the hematologist, and the dermatologist all have \ndifferent views of the structure of a human body. Ophthalmologists, cardiolo-\ngists, and podiatrists concentrate on specific subsystems. And the kinesiologist \nand psychiatrist are concerned with different aspects of the entire arrangement’s \nbehavior. Although these views are pictured differently and have very different \nproperties, all are inherently related, interconnected: together they describe the \narchitecture of the human body. Figure 1.1 shows several different views of the \nhuman body: the skeletal, the vascular, and the X-ray.\nFIGURE 1.1  Physiological structures (Getty images: Brand X Pictures [skeleton], \nDon Farrall [woman], Mads Abildgaard [man])\nSo it is with software. Modern systems are frequently too complex to grasp \nall at once. Instead, we restrict our attention at any one moment to one (or a \nsmall number) of the software system’s structures. To communicate meaningfully \nabout an architecture, we must make clear which structure or structures we are \ndiscussing at the moment—which view we are taking of the architecture. \n",
      "content_length": 1224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "10 \nPart One  Introduction\t\n1—What Is Software Architecture?\nStructures and Views\nWe will be using the related terms structure and view when discussing architec-\nture representation. \n■\n■A view is a representation of a coherent set of architectural elements, as \nwritten by and read by system stakeholders. It consists of a representation \nof a set of elements and the relations among them. \n■\n■A structure is the set of elements itself, as they exist in software or \nhardware.\nIn short, a view is a representation of a structure. For example, a module \nstructure is the set of the system’s modules and their organization. A module view \nis the representation of that structure, documented according to a template in a \nchosen notation, and used by some system stakeholders. \nSo: Architects design structures. They document views of those structures.\nThree Kinds of Structures\nAs we saw in the previous section, architectural structures can be divided into \nthree major categories, depending on the broad nature of the elements they show. \nThese correspond to the three broad kinds of decisions that architectural design \ninvolves: \n1.\t\nModule structures embody decisions as to how the system is to be struc-\ntured as a set of code or data units that have to be constructed or procured. \nIn any module structure, the elements are modules of some kind (perhaps \nclasses, or layers, or merely divisions of functionality, all of which are units \nof implementation). Modules represent a static way of considering the sys-\ntem. Modules are assigned areas of functional responsibility; there is less \nemphasis in these structures on how the resulting software manifests itself \nat runtime. Module structures allow us to answer questions such as these: \n■\n■What is the primary functional responsibility assigned to each module? \n■\n■What other software elements is a module allowed to use? \n■\n■What other software does it actually use and depend on? \n■\n■What modules are related to other modules by generalization or special-\nization (i.e., inheritance) relationships? \nModule structures convey this information directly, but they can also be \nused by extension to ask questions about the impact on the system when the \nresponsibilities assigned to each module change. In other words, examining \na system’s module structures—that is, looking at its module views—is an \nexcellent way to reason about a system’s modifiability. \n2.\t\nComponent-and-connector structures embody decisions as to how the \nsystem is to be structured as a set of elements that have runtime behav-\nior (components) and interactions (connectors). In these structures, the \n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "1.2  Architectural Structures and Views\n11\nelements are runtime components (which are the principal units of compu-\ntation and could be services, peers, clients, servers, filters, or many other \ntypes of runtime elements) and connectors (which are the communication \nvehicles among components, such as call-return, process synchronization \noperators, pipes, or others). Component-and-connector views help us an-\nswer questions such as these: \n■\n■What are the major executing components and how do they interact at \nruntime? \n■\n■What are the major shared data stores? \n■\n■Which parts of the system are replicated? \n■\n■How does data progress through the system? \n■\n■What parts of the system can run in parallel? \n■\n■Can the system’s structure change as it executes and, if so, how? \nBy extension, component-and-connector views are crucially important \nfor asking questions about the system’s runtime properties such as \nperformance, security, availability, and more.\n3.\t\nAllocation structures embody decisions as to how the system will relate \nto nonsoftware structures in its environment (such as CPUs, file systems, \nnetworks, development teams, etc.). These structures show the relationship \nbetween the software elements and elements in one or more external envi-\nronments in which the software is created and executed. Allocation views \nhelp us answer questions such as these: \n■\n■What processor does each software element execute on? \n■\n■In what directories or files is each element stored during development, \ntesting, and system building? \n■\n■What is the assignment of each software element to development teams?\nStructures Provide Insight\nStructures play such an important role in our perspective on software architec-\nture because of the analytical and engineering power they hold. Each structure \nprovides a perspective for reasoning about some of the relevant quality attributes. \nFor example:\n■\n■The module “uses” structure, which embodies what modules use what other \nmodules, is strongly tied to the ease with which a system can be extended \nor contracted. \n■\n■The concurrency structure, which embodies parallelism within the system, \nis strongly tied to the ease with which a system can be made free of \ndeadlock and performance bottlenecks. \n■\n■The deployment structure is strongly tied to the achievement of \nperformance, availability, and security goals. \n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "12 \nPart One  Introduction\t\n1—What Is Software Architecture?\nAnd so forth. Each structure provides the architect with a different insight \ninto the design (that is, each structure can be analyzed for its ability to deliver a \nquality attribute). But perhaps more important, each structure presents the archi-\ntect with an engineering leverage point: By designing the structures appropri-\nately, the desired quality attributes emerge.\nScenarios, described in Chapter 4, are useful for exercising a given structure \nas well as its connections to other structures. For example, a software engineer \nwanting to make a change to the concurrency structure of a system would need \nto consult the concurrency and deployment views, because the affected mecha-\nnisms typically involve processes and threads, and physical distribution might \ninvolve different control mechanisms than would be used if the processes were \nco-located on a single machine. If control mechanisms need to be changed, the \nmodule decomposition would need to be consulted to determine the extent of the \nchanges.\nSome Useful Module Structures\nUseful module structures include the following:\n■\n■Decomposition structure. The units are modules that are related to each \nother by the is-a-submodule-of relation, showing how modules are decom-\nposed into smaller modules recursively until the modules are small enough \nto be easily understood. Modules in this structure represent a common \nstarting point for design, as the architect enumerates what the units of \nsoftware will have to do and assigns each item to a module for subsequent \n(more detailed) design and eventual implementation. Modules often have \nproducts (such as interface specifications, code, test plans, etc.) associated \nwith them. The decomposition structure determines, to a large degree, the \nsystem’s modifiability, by assuring that likely changes are localized. That \nis, changes fall within the purview of at most a few (preferably small) mod-\nules. This structure is often used as the basis for the development project’s \norganization, including the structure of the documentation, and the project’s \nintegration and test plans. The units in this structure tend to have names that \nare organization-specific such as “segment” or “subsystem.”\n■\n■Uses structure. In this important but overlooked structure, the units here are \nalso modules, perhaps classes. The units are related by the uses relation, \na specialized form of dependency. A unit of software uses another if the \ncorrectness of the first requires the presence of a correctly functioning \nversion (as opposed to a stub) of the second. The uses structure is used to \nengineer systems that can be extended to add functionality, or from which \nuseful functional subsets can be extracted. The ability to easily create a \nsubset of a system allows for incremental development.\n",
      "content_length": 2870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "1.2  Architectural Structures and Views\n13\n■\n■Layer structure. The modules in this structure are called layers. A layer \nis an abstract “virtual machine” that provides a cohesive set of services \nthrough a managed interface. Layers are allowed to use other layers in a \nstrictly managed fashion; in strictly layered systems, a layer is only allowed \nto use the layer immediately below. This structure is used to imbue a system \nwith portability, the ability to change the underlying computing platform. \n■\n■Class (or generalization) structure. The module units in this structure are \ncalled classes. The relation is inherits from or is an instance of. This view \nsupports reasoning about collections of similar behavior or capability (e.g., \nthe classes that other classes inherit from) and parameterized differences. \nThe class structure allows one to reason about reuse and the incremental \naddition of functionality. If any documentation exists for a project that has \nfollowed an object-oriented analysis and design process, it is typically this \nstructure.\n■\n■Data model. The data model describes the static information structure in \nterms of data entities and their relationships. For example, in a banking \nsystem, entities will typically include Account, Customer, and Loan. \nAccount has several attributes, such as account number, type (savings or \nchecking), status, and current balance. A relationship may dictate that one \ncustomer can have one or more accounts, and one account is associated to \none or two customers.\nSome Useful C&C Structures\nComponent-and-connector structures show a runtime view of the system. In these \nstructures the modules described above have all been compiled into executable \nforms. All component-and-connector structures are thus orthogonal to the mod-\nule-based structures and deal with the dynamic aspects of a running system. The \nrelation in all component-and-connector structures is attachment, showing how \nthe components and the connectors are hooked together. (The connectors them-\nselves can be familiar constructs such as “invokes.”) Useful C&C structures in-\nclude the following:\n■\n■Service structure. The units here are services that interoperate with each \nother by service coordination mechanisms such as SOAP (see Chapter 6). \nThe service structure is an important structure to help engineer a system \ncomposed of components that may have been developed anonymously and \nindependently of each other. \n■\n■Concurrency structure. This component-and-connector structure allows the \narchitect to determine opportunities for parallelism and the locations where \nresource contention may occur. The units are components and the connec-\ntors are their communication mechanisms. The components are arranged \ninto logical threads; a logical thread is a sequence of computations that \n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "14 \nPart One  Introduction\t\n1—What Is Software Architecture?\ncould be allocated to a separate physical thread later in the design process. \nThe concurrency structure is used early in the design process to identify the \nrequirements to manage the issues associated with concurrent execution.\nSome Useful Allocation Structures\nAllocation structures define how the elements from C&C or module structures \nmap onto things that are not software: typically hardware, teams, and file sys-\ntems. Useful allocation structures include these:\n■\n■Deployment structure. The deployment structure shows how software is \nassigned to hardware processing and communication elements. The ele-\nments are software elements (usually a process from a C&C view), hard-\nware entities (processors), and communication pathways. Relations are \nallocated-to, showing on which physical units the software elements reside, \nand migrates-to if the allocation is dynamic. This structure can be used to \nreason about performance, data integrity, security, and availability. It is of \nparticular interest in distributed and parallel systems. \n■\n■Implementation structure. This structure shows how software elements \n(usually modules) are mapped to the file structure(s) in the system’s devel-\nopment, integration, or configuration control environments. This is critical \nfor the management of development activities and build processes. (In prac-\ntice, a screenshot of your development environment tool, which manages \nthe implementation environment, often makes a very useful and sufficient \ndiagram of your implementation view.)\n■\n■Work assignment structure. This structure assigns responsibility for im-\nplementing and integrating the modules to the teams who will carry it out. \nHaving a work assignment structure be part of the architecture makes it \nclear that the decision about who does the work has architectural as well as \nmanagement implications. The architect will know the expertise required \non each team. Also, on large multi-sourced distributed development proj-\nects, the work assignment structure is the means for calling out units of \nfunctional commonality and assigning those to a single team, rather than \nhaving them implemented by everyone who needs them. This structure will \nalso determine the major communication pathways among the teams: regu-\nlar teleconferences, wikis, email lists, and so forth.\nTable 1.1 summarizes these structures. The table lists the meaning of the \nelements and relations in each structure and tells what each might be used for. \nRelating Structures to Each Other\nEach of these structures provides a different perspective and design handle on a \nsystem, and each is valid and useful in its own right. Although the structures give \n",
      "content_length": 2750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "1.2  Architectural Structures and Views\n15\nTABLE 1.1  Useful Architectural Structures\nSoftware \nStructure\nElement  \nTypes\n \nRelations\n \nUseful For\nQuality Attributes \nAffected\nModule \nStructures\nDecomposition\nModule\nIs a submodule of\nResource allocation and project structuring and \nplanning; information hiding, encapsulation; \nconfiguration control\nModifiability\nUses\nModule\nUses (i.e., requires the correct \npresence of)\nEngineering subsets, engineering extensions\n“Subsetability,” \nextensibility\nLayers\nLayer\nRequires the correct presence \nof, uses the services of, \nprovides abstraction to\nIncremental development, implementing systems \non top of “virtual machines”\nPortability\nClass\nClass, object\nIs an instance of, shares access \nmethods of\nIn object-oriented design systems, factoring out \ncommonality; planning extensions of functionality\nModifiability, \nextensibility\nData model\nData entity\n{one, many}-to-{one, many}, \ngeneralizes, specializes\nEngineering global data structures for consistency \nand performance\nModifiability, \nperformance\nC&C \nStructures\nService\nService, ESB, registry, \nothers\nRuns concurrently with, may \nrun concurrently with, excludes, \nprecedes, etc.\nScheduling analysis, performance analysis\nInteroperability, \nmodifiability\nConcurrency\nProcesses, threads\nCan run in parallel\nIdentifying locations where resource contention \nexists, or where threads may fork, join, be created, \nor be killed\nPerformance, \navailability\nAllocation  \nStructures\nDeployment\nComponents, hardware \nelements\nAllocated to, migrates to\nPerformance, availability, security analysis\nPerformance, \nsecurity, availability\nImplementation\nModules, file structure\nStored in\nConfiguration control, integration, test activities\nDevelopment \nefficiency\nWork assignment Modules, organizational \nunits\nAssigned to\nProject management, best use of expertise and \navailable resources, management of commonality\nDevelopment \nefficiency \n",
      "content_length": 1931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "16 \nPart One  Introduction\t\n1—What Is Software Architecture?\ndifferent system perspectives, they are not independent. Elements of one structure \nwill be related to elements of other structures, and we need to reason about these \nrelations. For example, a module in a decomposition structure may be manifested \nas one, part of one, or several components in one of the component-and-con-\nnector structures, reflecting its runtime alter ego. In general, mappings between \nstructures are many to many. \nFigure 1.2 shows a very simple example of how two structures might relate \nto each other. The figure on the left shows a module decomposition view of a \ntiny client-server system. In this system, two modules must be implemented: The \nclient software and the server software. The figure on the right shows a compo-\nnent-and-connector view of the same system. At runtime there are ten clients run-\nning and accessing the server. Thus, this little system has two modules and eleven \ncomponents (and ten connectors).\nWhereas the correspondence between the elements in the decomposition \nstructure and the client-server structure is obvious, these two views are used for \nvery different things. For example, the view on the right could be used for perfor-\nmance analysis, bottleneck prediction, and network traffic management, which \nwould be extremely difficult or impossible to do with the view on the left.\n(In Chapter 13 we’ll learn about the map-reduce pattern, in which copies \nof simple, identical functionality are distributed across hundreds or thousands \nof processing nodes—one module for the whole system, but one component per \nnode.) \nIndividual projects sometimes consider one structure dominant and cast \nother structures, when possible, in terms of the dominant structure. Often the \ndominant structure is the module decomposition structure. This is for a good \nClient\nServer\nModule\nSystem\nDecomposition View\nKey:\nClient-Server View\nKey:\nComponent\nRequest-Reply\nC7\nC8\nC2\nC3\nC1\nC4\nC6\nC9\nC10\nC5\nS1\nFIGURE 1.2  Two views of a client-server system\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "1.2  Architectural Structures and Views\n17\nreason: it tends to spawn the project structure, because it mirrors the team struc-\nture of development. In other projects, the dominant structure might be a C&C \nstructure that shows how the system’s functionality and/or critical quality attri-\nbutes are achieved.\nFewer Is Better\nNot all systems warrant consideration of many architectural structures. The larger \nthe system, the more dramatic the difference between these structures tends to be; \nbut for small systems we can often get by with fewer. Instead of working with \neach of several component-and-connector structures, usually a single one will do. \nIf there is only one process, then the process structure collapses to a single node \nand need not be explicitly represented in the design. If there is to be no distribu-\ntion (that is, if the system is implemented on a single processor), then the deploy-\nment structure is trivial and need not be considered further. In general, design and \ndocument a structure only if doing so brings a positive return on the investment, \nusually in terms of decreased development or maintenance costs.\nWhich Structures to Choose?\nWe have briefly described a number of useful architectural structures, and there \nare many more. Which ones shall an architect choose to work on? Which ones \nshall the architect choose to document? Surely not all of them. Chapter 18 will \ntreat this topic in more depth, but for now a good answer is that you should think \nabout how the various structures available to you provide insight and leverage \ninto the system’s most important quality attributes, and then choose the ones that \nwill play the best role in delivering those attributes. \nAsk Cal\nMore than a decade ago I went to a customer site to do an architecture \nevaluation—one of the first instances of the Architecture Tradeoff Analysis \nMethod (ATAM) that I had ever performed (you can read about the ATAM, \nand other architecture evaluation topics, in Chapter 21). In those early \ndays, we were still figuring out how to make architecture evaluations re-\npeatable and predictable, and how to guarantee useful outcomes from \nthem. One of the ways that we ensured useful outcomes was to enforce \ncertain preconditions on the evaluation. A precondition that we figured out \nrather quickly was this: if the architecture has not been documented, we \nwill not proceed with the evaluation. The reason for this precondition was \nsimple: we could not evaluate the architecture by reading the code—we \ndidn’t have the time for that—and we couldn’t just ask the architect to \n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "18 \nPart One  Introduction\t\n1—What Is Software Architecture?\nsketch the architecture in real time, since that would produce vague and \nvery likely erroneous representations.\nOkay, it’s not completely true to say that they had no architecture docu-\nmentation. They did produce a single-page diagram, with a few boxes and \nlines. Some of those boxes were, however, clouds. Yes, they actually used \na cloud as one of their icons. When I pressed them on the meaning of this \nicon—Was it a process? A class? A thread?—they waffled. This was not, in \nfact, architecture documentation. It was, at best, “marketecture.”\nBut in those early days we had no preconditions and so we didn’t stop \nthe evaluation there. We just blithely waded in to whatever swamp we \nfound, and we enforced nothing. As I began this evaluation, I interviewed \nsome of the key project stakeholders: the project manager and several of \nthe architects (this was a large project with one lead architect and several \nsubordinates). As it happens, the lead architect was away, and so I spent \nmy time with the subordinate architects. Every time I asked the subor-\ndinates a tough question—“How do you ensure that you will meet your \nlatency goal along this critical execution path?” or “What are your rules for \nlayering?”—they would answer: “Ask Cal. Cal knows that.” Cal was the lead \narchitect. Immediately I noted a risk for this system: What if Cal gets hit by \na bus? What then?\nIn the end, because of my pestering, the architecture team did in fact \nproduce respectable architecture documentation. About halfway through \nthe evaluation, the project manager came up to me and shook my hand \nand thanked me for the great job I had done. I was dumbstruck. In my \nmind I hadn’t done anything, at that point; the evaluation was only partially \ncomplete and I hadn’t produced a single report or finding. I said that to the \nmanager and he said: “You got those guys to document the architecture. \nI’ve never been able to get them to do that. So . . . thanks!”\nIf Cal had been hit by a bus or just left the company, they would have \nhad a serious problem on their hands: all of that architectural knowledge \nlocated in one guy’s head and he is no longer with the organization. In can \nhappen. It does happen.\nThe moral of this story? An architecture that is not documented, and not \ncommunicated, may still be a good architecture, but the risks surrounding it \nare enormous.\n—RK\n1.3  Architectural Patterns\nIn some cases, architectural elements are composed in ways that solve particular \nproblems. The compositions have been found useful over time, and over many \ndifferent domains, and so they have been documented and disseminated. These \ncompositions of architectural elements, called architectural patterns, provide \npackaged strategies for solving some of the problems facing a system.\n",
      "content_length": 2853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "1.4  What Makes a “Good” Architecture?\n19\nAn architectural pattern delineates the element types and their forms of in-\nteraction used in solving the problem. Patterns can be characterized according to \nthe type of architectural elements they use. For example, a common module type \npattern is this:\n■\n■Layered pattern. When the uses relation among software elements is \nstrictly unidirectional, a system of layers emerges. A layer is a coherent \nset of related functionality. In a strictly layered structure, a layer can only \nuse the services of the layer immediately below it. Many variations of this \npattern, lessening the structural restriction, occur in practice. Layers are \noften designed as abstractions (virtual machines) that hide implementation \nspecifics below from the layers above, engendering portability. \nCommon component-and-connector type patterns are these:\n■\n■Shared-data (or repository) pattern. This pattern comprises components \nand connectors that create, store, and access persistent data. The repository \nusually takes the form of a (commercial) database. The connectors are \nprotocols for managing the data, such as SQL. \n■\n■Client-server pattern. The components are the clients and the servers, and \nthe connectors are protocols and messages they share among each other to \ncarry out the system’s work. \nCommon allocation patterns include the following:\n■\n■Multi-tier pattern, which describes how to distribute and allocate the \ncomponents of a system in distinct subsets of hardware and software, \nconnected by some communication medium. This pattern specializes the \ngeneric deployment (software-to-hardware allocation) structure.\n■\n■Competence center and platform, which are patterns that specialize a \nsoftware system’s work assignment structure. In competence center, work \nis allocated to sites depending on the technical or domain expertise located \nat a site. For example, user-interface design is done at a site where usability \nengineering experts are located. In platform, one site is tasked with \ndeveloping reusable core assets of a software product line (see Chapter 25), \nand other sites develop applications that use the core assets.\nArchitectural patterns will be investigated much further in Chapter 13. \n1.4  What Makes a “Good” Architecture?\nThere is no such thing as an inherently good or bad architecture. Architectures \nare either more or less fit for some purpose. A three-tier layered service-oriented \narchitecture may be just the ticket for a large enterprise’s web-based B2B system \n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "20 \nPart One  Introduction\t\n1—What Is Software Architecture?\nbut completely wrong for an avionics application. An architecture carefully \ncrafted to achieve high modifiability does not make sense for a throwaway proto-\ntype (and vice versa!). One of the messages of this book is that architectures can \nin fact be evaluated—one of the great benefits of paying attention to them—but \nonly in the context of specific stated goals. \nNevertheless, there are rules of thumb that should be followed when design-\ning most architectures. Failure to apply any of these does not automatically mean \nthat the architecture will be fatally flawed, but it should at least serve as a warn-\ning sign that should be investigated.\nWe divide our observations into two clusters: process recommendations and \nproduct (or structural) recommendations. Our process recommendations are the \nfollowing:\n1.\t\nThe architecture should be the product of a single architect or a small \ngroup of architects with an identified technical leader. This approach \ngives the architecture its conceptual integrity and technical consistency. \nThis recommendation holds for Agile and open source projects as well \nas “traditional” ones. There should be a strong connection between the \narchitect(s) and the development team, to avoid ivory tower designs that are \nimpractical.\n2.\t\nThe architect (or architecture team) should, on an ongoing basis, base the \narchitecture on a prioritized list of well-specified quality attribute require-\nments. These will inform the tradeoffs that always occur. Functionality mat-\nters less.\n3.\t\nThe architecture should be documented using views. The views should \naddress the concerns of the most important stakeholders in support of the \nproject timeline. This might mean minimal documentation at first, elaborat-\ned later. Concerns usually are related to construction, analysis, and main-\ntenance of the system, as well as education of new stakeholders about the \nsystem. \n4.\t\nThe architecture should be evaluated for its ability to deliver the system’s \nimportant quality attributes. This should occur early in the life cycle, when \nit returns the most benefit, and repeated as appropriate, to ensure that \nchanges to the architecture (or the environment for which it is intended) \nhave not rendered the design obsolete. \n5.\t\nThe architecture should lend itself to incremental implementation, to avoid \nhaving to integrate everything at once (which almost never works) as well \nas to discover problems early. One way to do this is to create a “skeletal” \nsystem in which the communication paths are exercised but which at first \nhas minimal functionality. This skeletal system can be used to “grow” the \nsystem incrementally, refactoring as necessary.\nOur structural rules of thumb are as follows:\n1.\t\nThe architecture should feature well-defined modules whose functional \nresponsibilities are assigned on the principles of information hiding and \n",
      "content_length": 2943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "1.5  Summary\n21\nseparation of concerns. The information-hiding modules should encapsulate \nthings likely to change, thus insulating the software from the effects of \nthose changes. Each module should have a well-defined interface that \nencapsulates or “hides” the changeable aspects from other software \nthat uses its facilities. These interfaces should allow their respective \ndevelopment teams to work largely independently of each other. \n2.\t\nUnless your requirements are unprecedented—possible, but unlikely—your \nquality attributes should be achieved using well-known architectural pat-\nterns and tactics (described in Chapter 13) specific to each attribute. \n3.\t\nThe architecture should never depend on a particular version of a commer-\ncial product or tool. If it must, it should be structured so that changing to a \ndifferent version is straightforward and inexpensive. \n4.\t\nModules that produce data should be separate from modules that consume \ndata. This tends to increase modifiability because changes are frequently \nconfined to either the production or the consumption side of data. If new \ndata is added, both sides will have to change, but the separation allows for a \nstaged (incremental) upgrade. \n5.\t\nDon’t expect a one-to-one correspondence between modules and compo-\nnents. For example, in systems with concurrency, there may be multiple in-\nstances of a component running in parallel, where each component is built \nfrom the same module. For systems with multiple threads of concurrency, \neach thread may use services from several components, each of which was \nbuilt from a different module.\n6.\t\nEvery process should be written so that its assignment to a specific proces-\nsor can be easily changed, perhaps even at runtime. \n7.\t\nThe architecture should feature a small number of ways for components \nto interact. That is, the system should do the same things in the same way \nthroughout. This will aid in understandability, reduce development time, \nincrease reliability, and enhance modifiability. \n8.\t\nThe architecture should contain a specific (and small) set of resource con-\ntention areas, the resolution of which is clearly specified and maintained. \nFor example, if network utilization is an area of concern, the architect \nshould produce (and enforce) for each development team guidelines that \nwill result in a minimum of network traffic. If performance is a concern, the \narchitect should produce (and enforce) time budgets for the major threads. \n1.5  Summary\nThe software architecture of a system is the set of structures needed to reason \nabout the system, which comprise software elements, relations among them, and \nproperties of both.\n",
      "content_length": 2675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "22 \nPart One  Introduction\t\n1—What Is Software Architecture?\nA structure is a set of elements and the relations among them.\nA view is a representation of a coherent set of architectural elements, as \nwritten by and read by system stakeholders. A view is a representation of one or \nmore structures. \nThere are three categories of structures:\n■\n■Module structures show how a system is to be structured as a set of code or \ndata units that have to be constructed or procured. \n■\n■Component-and-connector structures show how the system is to be \nstructured as a set of elements that have runtime behavior (components) and \ninteractions (connectors). \n■\n■Allocation structures show how the system will relate to nonsoftware \nstructures in its environment (such as CPUs, file systems, networks, \ndevelopment teams, etc.). \nStructures represent the primary engineering leverage points of an architec-\nture. Each structure brings with it the power to manipulate one or more quality \nattributes. They represent a powerful approach for creating the architecture (and \nlater, for analyzing it and explaining it to its stakeholders). And as we will see \nin Chapter 18, the structures that the architect has chosen as engineering lever-\nage points are also the primary candidates to choose as the basis for architecture \ndocumentation. \nEvery system has a software architecture, but this architecture may be docu-\nmented and disseminated, or it may not be.\nThere is no such thing as an inherently good or bad architecture. Architec-\ntures are either more or less fit for some purpose. \n1.6  For Further Reading\nThe early work of David Parnas laid much of the conceptual foundation for what \nbecame the study of software architecture. A quintessential Parnas reader would \ninclude his foundational article on information hiding [Parnas 72] as well as his \nworks on program families [Parnas 76], the structures inherent in software sys-\ntems [Parnas 74], and introduction of the uses structure to build subsets and sup-\nersets of systems [Parnas 79]. All of these papers can be found in the more easily \naccessible collection of his important papers [Hoffman 00].\nAn early paper by Perry and Wolf [Perry 92] drew an analogy between soft-\nware architecture views and structures and the structures one finds in a house \n(plumbing, electrical, and so forth). \nSoftware architectural patterns have been extensively catalogued in the se-\nries Pattern-Oriented Software Architecture [Buschmann 96] and others. Chapter \n13 of this book also deals with architectural patterns.\n",
      "content_length": 2555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "1.7  Discussion Questions\n23\nEarly papers on architectural views as used in industrial development proj-\nects are [Soni 95] and [Kruchten 95]. The former grew into a book [Hofmeister \n00] that presents a comprehensive picture of using views in development and \nanalysis. The latter grew into the Rational Unified Process, about which there is \nno shortage of references, both paper and online. A good one is [Kruchten 03].\nCristina Gacek and her colleagues discuss the process issues surrounding \nsoftware architecture in [Gacek 95].\nGarlan and Shaw’s seminal work on software architecture [Garlan 93] \nprovides many excellent examples of architectural styles (a concept similar to \npatterns).\nIn [Clements 10a] you can find an extended discussion on the difference be-\ntween an architectural pattern and an architectural style. (It argues that a pattern \nis a context-problem-solution triple; a style is simply a condensation that focuses \nmost heavily on the solution part.)\nSee [Taylor 09] for a definition of software architecture based on decisions \nrather than on structure.\n1.7  Discussion Questions\n1.\t\nSoftware architecture is often compared to the architecture of buildings as a \nconceptual analogy. What are the strong points of that analogy? What is the \ncorrespondence in buildings to software architecture structures and views? \nTo patterns? What are the weaknesses of the analogy? When does it break \ndown?\n2.\t\nDo the architectures you’ve been exposed to document different structures \nand relations like those described in this chapter? If so, which ones? If not, \nwhy not? \n3.\t\nIs there a different definition of software architecture that you are familiar \nwith? If so, compare and contrast it with the definition given in this chapter. \nMany definitions include considerations like “rationale” (stating the reasons \nwhy the architecture is what it is) or how the architecture will evolve over \ntime. Do you agree or disagree that these considerations should be part of \nthe definition of software architecture?\n4.\t\nDiscuss how an architecture serves as a basis for analysis. What about \ndecision-making? What kinds of decision-making does an architecture \nempower? \n5.\t\nWhat is architecture’s role in project risk reduction?\n",
      "content_length": 2244,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "24 \nPart One  Introduction\t\n1—What Is Software Architecture?\n6.\t\nFind a commonly accepted definition of system architecture and discuss \nwhat it has in common with software architecture. Do the same for enter-\nprise architecture.\n7.\t\nFind a published example of an architecture. What structure or structures \nare shown? Given its purpose, what structure or structures should have \nbeen shown? What analysis does the architecture support? Critique it: What \nquestions do you have that the representation does not answer?\n8.\t\nSailing ships have architectures, which means they have “structures” that \nlend themselves to reasoning about the ship’s performance and other qual-\nity attributes. Look up the technical definitions for barque, brig, cutter, \nfrigate, ketch, schooner, and sloop. Propose a useful set of “structures” for \ndistinguishing and reasoning about ship architectures.\n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "25\n2\nWhy Is Software \nArchitecture Important?\nSoftware architecture is the set of design \ndecisions which, if made incorrectly, may \ncause your project to be cancelled.\n—Eoin Woods\nIf architecture is the answer, what was the question?\nWhile Chapter 3 will cover the business importance of architecture to an \nenterprise, this chapter focuses on why architecture matters from a technical per-\nspective. We will examine a baker’s dozen of the most important reasons.\n1.\t An architecture will inhibit or enable a system’s driving quality attributes.\n2.\t The decisions made in an architecture allow you to reason about and man-\nage change as the system evolves.\n3.\t The analysis of an architecture enables early prediction of a system’s \nqualities.\n4.\t A documented architecture enhances communication among stakeholders.\n5.\t The architecture is a carrier of the earliest and hence most fundamental, \nhardest-to-change design decisions.\n6.\t An architecture defines a set of constraints on subsequent implementation.\n7.\t The architecture dictates the structure of an organization, or vice versa.\n8.\t An architecture can provide the basis for evolutionary prototyping.\n9.\t An architecture is the key artifact that allows the architect and project man-\nager to reason about cost and schedule.\n10.\t An architecture can be created as a transferable, reusable model that forms \nthe heart of a product line.\n11.\t Architecture-based development focuses attention on the assembly of com-\nponents, rather than simply on their creation.\n12.\t By restricting design alternatives, architecture channels the creativity of \ndevelopers, reducing design and system complexity.\n13.\t An architecture can be the foundation for training a new team member.\n",
      "content_length": 1730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "26 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nEven if you already believe us that architecture is important and don’t need the \npoint hammered thirteen more times, think of these thirteen points (which form \nthe outline for this chapter) as thirteen useful ways to use architecture in a project.\n2.1  \u0007Inhibiting or Enabling a System’s Quality \nAttributes\nWhether a system will be able to exhibit its desired (or required) quality attri-\nbutes is substantially determined by its architecture. \nThis is such an important message that we’ve devoted all of Part 2 of this \nbook to expounding that message in detail. Until then, keep these examples in \nmind as a starting point:\n■\n■If your system requires high performance, then you need to pay attention \nto managing the time-based behavior of elements, their use of shared \nresources, and the frequency and volume of inter-element communication.\n■\n■If modifiability is important, then you need to pay careful attention to \nassigning responsibilities to elements so that the majority of changes to the \nsystem will affect a small number of those elements. (Ideally each change \nwill affect just a single element.)\n■\n■If your system must be highly secure, then you need to manage and protect \ninter-element communication and control which elements are allowed to \naccess which information; you may also need to introduce specialized \nelements (such as an authorization mechanism) into the architecture.\n■\n■If you believe that scalability will be important to the success of your \nsystem, then you need to carefully localize the use of resources to facilitate \nintroduction of higher-capacity replacements, and you must avoid hard-\ncoding in resource assumptions or limits.\n■\n■If your projects need the ability to deliver incremental subsets of the \nsystem, then you must carefully manage intercomponent usage.\n■\n■If you want the elements from your system to be reusable in other systems, \nthen you need to restrict inter-element coupling, so that when you extract \nan element, it does not come out with too many attachments to its current \nenvironment to be useful.\nThe strategies for these and other quality attributes are supremely architectural. \nBut an architecture alone cannot guarantee the functionality or quality required of \na system. Poor downstream design or implementation decisions can always under-\nmine an adequate architectural design. As we like to say (mostly in jest): The archi-\ntecture giveth and the implementation taketh away. Decisions at all stages of the \nlife cycle—from architectural design to coding and implementation—affect system \nquality. Therefore, quality is not completely a function of an architectural design. \n",
      "content_length": 2720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "2.2  Reasoning About and Managing Change\n27\nA good architecture is necessary, but not sufficient, to ensure quality. Achiev-\ning quality attributes must be considered throughout design, implementation, and \ndeployment. No quality attribute is entirely dependent on design, nor is it entirely \ndependent on implementation or deployment. Satisfactory results are a matter of \ngetting the big picture (architecture) as well as the details (implementation) correct. \nFor example, modifiability is determined by how functionality is divided \nand coupled (architectural) and by coding techniques within a module (nonar-\nchitectural). Thus, a system is typically modifiable if changes involve the fewest \npossible number of distinct elements. In spite of having the ideal architecture, \nhowever, it is always possible to make a system difficult to modify by writing \nobscure, tangled code. \n2.2  Reasoning About and Managing Change\nThis point is a corollary to the previous point.\nModifiability—the ease with which changes can be made to a system—is \na quality attribute (and hence covered by the arguments in the previous section), \nbut it is such an important quality that we have awarded it its own spot in the List \nof Thirteen. The software development community is coming to grips with the \nfact that roughly 80 percent of a typical software system’s total cost occurs after \ninitial deployment. A corollary of this statistic is that most systems that people \nwork on are in this phase. Many programmers and software designers never get \nto work on new development; they work under the constraints of the existing \narchitecture and the existing body of code. Virtually all software systems change \nover their lifetime, to accommodate new features, to adapt to new environments, \nto fix bugs, and so forth. But these changes are often fraught with difficulty. \nEvery architecture partitions possible changes into three categories: local, \nnonlocal, and architectural. \n■\n■A local change can be accomplished by modifying a single element. For \nexample, adding a new business rule to a pricing logic module. \n■\n■A nonlocal change requires multiple element modifications but leaves \nthe underlying architectural approach intact. For example, adding a new \nbusiness rule to a pricing logic module, then adding new fields to the \ndatabase that this new business rule requires, and then revealing the results \nof the rule in the user interface. \n■\n■An architectural change affects the fundamental ways in which the \nelements interact with each other and will probably require changes all \nover the system. For example, changing a system from client-server to \npeer-to-peer. \n",
      "content_length": 2667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "28 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nObviously, local changes are the most desirable, and so an effective architec-\nture is one in which the most common changes are local, and hence easy to make. \nDeciding when changes are essential, determining which change paths have \nthe least risk, assessing the consequences of proposed changes, and arbitrating \nsequences and priorities for requested changes all require broad insight into rela-\ntionships, performance, and behaviors of system software elements. These activ-\nities are in the job description for an architect. Reasoning about the architecture \nand analyzing the architecture can provide the insight necessary to make deci-\nsions about anticipated changes. \n2.3  Predicting System Qualities \nThis point follows from the previous two. Architecture not only imbues systems \nwith qualities, but it does so in a predictable way. \nWere it not possible to tell that the appropriate architectural decisions have \nbeen made (i.e., if the system will exhibit its required quality attributes) without \nwaiting until the system is developed and deployed, then choosing an architec-\nture would be a hopeless task—randomly making architecture selections would \nperform as well as any other method. Fortunately, it is possible to make quality \npredictions about a system based solely on an evaluation of its architecture. If we \nknow that certain kinds of architectural decisions lead to certain quality attributes \nin a system, then we can make those decisions and rightly expect to be rewarded \nwith the associated quality attributes. After the fact, when we examine an archi-\ntecture, we can look to see if those decisions have been made, and confidently \npredict that the architecture will exhibit the associated qualities.\nThis is no different from any mature engineering discipline, where design \nanalysis is a standard part of the development process. The earlier you can find \na problem in your design, the cheaper, easier, and less disruptive it will be to fix.\nEven if you don't do the quantitative analytic modeling sometimes necessary \nto ensure that an architecture will deliver its prescribed benefits, this principle of \nevaluating decisions based on their quality attribute implications is invaluable for \nat least spotting potential trouble spots early.\nThe architecture modeling and analysis techniques described in Chap-\nter 14, as well as the architecture evaluation techniques covered in Chapter 21, \nallow early insight into the software product qualities made possible by software \narchitectures.\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "2.4  Enhancing Communication among Stakeholders\n29\n2.4  Enhancing Communication among Stakeholders\nSoftware architecture represents a common abstraction of a system that most, \nif not all, of the system’s stakeholders can use as a basis for creating mutual under-\nstanding, negotiating, forming consensus, and communicating with each other. The \narchitecture—or at least parts of it—is sufficiently abstract that most nontechnical \npeople can understand it adequately, particularly with some coaching from the archi-\ntect, and yet that abstraction can be refined into sufficiently rich technical specifica-\ntions to guide implementation, integration, test, and deployment.\nEach stakeholder of a software system—customer, user, project manager, \ncoder, tester, and so on—is concerned with different characteristics of the system \nthat are affected by its architecture. For example:\n■\n■The user is concerned that the system is fast, reliable, and available when \nneeded. \n■\n■The customer is concerned that the architecture can be implemented on \nschedule and according to budget.\n■\n■The manager is worried (in addition to concerns about cost and schedule) \nthat the architecture will allow teams to work largely independently, \ninteracting in disciplined and controlled ways.\n■\n■The architect is worried about strategies to achieve all of those goals. \nArchitecture provides a common language in which different concerns can \nbe expressed, negotiated, and resolved at a level that is intellectually manageable \neven for large, complex systems. Without such a language, it is difficult to under-\nstand large systems sufficiently to make the early decisions that influence both \nquality and usefulness. Architectural analysis, as we will see in Chapter 21, both \ndepends on this level of communication and enhances it.\nSection 3.5 covers stakeholders and their concerns in greater depth.\n“What Happens When I Push This Button?” Architecture as a \nVehicle for Stakeholder Communication\nThe project review droned on and on. The government-sponsored devel-\nopment was behind schedule and over budget and was large enough that \nthese lapses were attracting congressional attention. And now the govern-\nment was making up for past neglect by holding a marathon come-one-\ncome-all review session. The contractor had recently undergone a buyout, \nwhich hadn’t helped matters. It was the afternoon of the second day, and \nthe agenda called for the software architecture to be presented. The young \narchitect—an apprentice to the chief architect for the system—was bravely \nexplaining how the software architecture for the massive system would \nenable it to meet its very demanding real-time, distributed, high-reliability \n",
      "content_length": 2711,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "30 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nrequirements. He had a solid presentation and a solid architecture to pres-\nent. It was sound and sensible. But the audience—about 30 government \nrepresentatives who had varying roles in the management and oversight of \nthis sticky project—was tired. Some of them were even thinking that per-\nhaps they should have gone into real estate instead of enduring another one \nof these marathon let’s-finally-get-it-right-this-time reviews. \nThe viewgraph showed, in semiformal box-and-line notation, what the \nmajor software elements were in a runtime view of the system. The names \nwere all acronyms, suggesting no semantic meaning without explanation, \nwhich the young architect gave. The lines showed data flow, message \npassing, and process synchronization. The elements were internally re-\ndundant, the architect was explaining. “In the event of a failure,” he began, \nusing a laser pointer to denote one of the lines, “a restart mechanism \ntriggers along this path when—”\n“What happens when the mode select button is pushed?” interrupted \none of the audience members. He was a government attendee represent-\ning the user community for this system.\n“Beg your pardon?” asked the architect. \n“The mode select button,” he said. “What happens when you push it?”\n“Um, that triggers an event in the device driver, up here,” began the \narchitect, laser-pointing. “It then reads the register and interprets the event \ncode. If it’s mode select, well, then, it signals the blackboard, which in turns \nsignals the objects that have subscribed to that event. . . . ”\n“No, I mean what does the system do,” interrupted the questioner. “Does \nit reset the displays? And what happens if this occurs during a system \nreconfiguration?”\nThe architect looked a little surprised and flicked off the laser pointer. \nThis was not an architectural question, but since he was an architect and \ntherefore fluent in the requirements, he knew the answer. “If the command \nline is in setup mode, the displays will reset,” he said. “Otherwise an error \nmessage will be put on the control console, but the signal will be ignored.” \nHe put the laser pointer back on. “Now, the restart mechanism that I was \ntalking about—”\n“Well, I was just wondering,” said the users’ delegate. “Because I see \nfrom your chart that the display console is sending signal traffic to the \ntarget location module.”\n“What should happen?” asked another member of the audience, \naddressing the first questioner. “Do you really want the user to get mode \ndata during its reconfiguring?” And for the next 45 minutes, the architect \nwatched as the audience consumed his time slot by debating what the cor-\nrect behavior of the system was supposed to be in various esoteric states. \nThe debate was not architectural, but the architecture (and the graphical \nrendition of it) had sparked debate. It is natural to think of architecture as \nthe basis for communication among some of the stakeholders besides the \narchitects and developers: Managers, for example, use the architecture to \ncreate teams and allocate resources among them. But users? The architec-\nture is invisible to users, after all; why should they latch on to it as a tool for \nunderstanding the system?\n",
      "content_length": 3280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "2.5  Carrying Early Design Decisions\n31\nThe fact is that they do. In this case, the questioner had sat through two \ndays of viewgraphs all about function, operation, user interface, and testing. \nBut it was the first slide on architecture that—even though he was tired and \nwanted to go home—made him realize he didn’t understand something. \nAttendance at many architecture reviews has convinced me that seeing \nthe system in a new way prods the mind and brings new questions to the \nsurface. For users, architecture often serves as that new way, and the \nquestions that a user poses will be behavioral in nature. In a memorable \narchitecture evaluation exercise a few years ago, the user representatives \nwere much more interested in what the system was going to do than in how \nit was going to do it, and naturally so. Up until that point, their only contact \nwith the vendor had been through its marketers. The architect was the first \nlegitimate expert on the system to whom they had access, and they didn’t \nhesitate to seize the moment.\nOf course, careful and thorough requirements specifications would ame-\nliorate this situation, but for a variety of reasons they are not always created \nor available. In their absence, a specification of the architecture often \nserves to trigger questions and improve clarity. It is probably more prudent \nto recognize this reality than to resist it. \nSometimes such an exercise will reveal unreasonable requirements, \nwhose utility can then be revisited. A review of this type that emphasizes \nsynergy between requirements and architecture would have let the young \narchitect in our story off the hook by giving him a place in the overall review \nsession to address that kind of information. And the user representative \nwouldn’t have felt like a fish out of water, asking his question at a clearly \ninappropriate moment. \n—PCC\n2.5  Carrying Early Design Decisions\nSoftware architecture is a manifestation of the earliest design decisions about a \nsystem, and these early bindings carry enormous weight with respect to the sys-\ntem’s remaining development, its deployment, and its maintenance life. It is also \nthe earliest point at which these important design decisions affecting the system \ncan be scrutinized.\nAny design, in any discipline, can be viewed as a set of decisions. When \npainting a picture, an artist decides on the material for the canvas, on the media \nfor recording—oil paint, watercolor, crayon—even before the picture is begun. \nOnce the picture is begun, other decisions are immediately made: Where is the \nfirst line? What is its thickness? What is its shape? All of these early design de-\ncisions have a strong influence on the final appearance of the picture. Each deci-\nsion constrains the many decisions that follow. Each decision, in isolation, might \nappear innocent enough, but the early ones in particular have disproportionate \nweight simply because they influence and constrain so much of what follows.\n",
      "content_length": 2981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "32 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nSo it is with architecture design. An architecture design can also be viewed \nas a set of decisions. The early design decisions constrain the decisions that fol-\nlow, and changing these decisions has enormous ramifications. Changing these \nearly decisions will cause a ripple effect, in terms of the additional decisions that \nmust now be changed. Yes, sometimes the architecture must be refactored or re-\ndesigned, but this is not a task we undertake lightly (because the “ripple” might \nturn into a tsunami).\nWhat are these early design decisions embodied by software architecture? \nConsider:\n■\n■Will the system run on one processor or be distributed across multiple \nprocessors?\n■\n■Will the software be layered? If so, how many layers will there be? What \nwill each one do?\n■\n■Will components communicate synchronously or asynchronously? Will \nthey interact by transferring control or data or both?\n■\n■Will the system depend on specific features of the operating system or \nhardware? \n■\n■Will the information that flows through the system be encrypted or not?\n■\n■What operating system will we use?\n■\n■What communication protocol will we choose?\nImagine the nightmare of having to change any of these or a myriad other \nrelated decisions. Decisions like these begin to flesh out some of the structures of \nthe architecture and their interactions. In Chapter 4, we describe seven categories \nof these early design decisions. In Chapters 5–11 we show the implications of \nthese design decision categories on achieving quality attributes.\n2.6  Defining Constraints on an Implementation\nAn implementation exhibits an architecture if it conforms to the design decisions \nprescribed by the architecture. This means that the implementation must be im-\nplemented as the set of prescribed elements, these elements must interact with \neach other in the prescribed fashion, and each element must fulfill its responsibil-\nity to the other elements as dictated by the architecture. Each of these prescrip-\ntions is a constraint on the implementer.\nElement builders must be fluent in the specifications of their individual ele-\nments, but they may not be aware of the architectural tradeoffs—the architecture \n(or architect) simply constrains them in such a way as to meet the tradeoffs. A \nclassic example of this phenomenon is when an architect assigns performance \nbudget to the pieces of software involved in some larger piece of functionality. \nIf each software unit stays within its budget, the overall transaction will meet its \n",
      "content_length": 2594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "2.8  Enabling Evolutionary Prototyping\n33\nperformance requirement. Implementers of each of the constituent pieces may \nnot know the overall budget, only their own.\nConversely, the architects need not be experts in all aspects of algorithm \ndesign or the intricacies of the programming language—although they should \ncertainly know enough not to design something that is difficult to build—but they \nare the ones responsible for establishing, analyzing, and enforcing the architec-\ntural tradeoffs. \n2.7  Influencing the Organizational Structure \nNot only does architecture prescribe the structure of the system being developed, \nbut that structure becomes engraved in the structure of the development project (and \nsometimes the structure of the entire organization). The normal method for divid-\ning up the labor in a large project is to assign different groups different portions of \nthe system to construct. This is called the work-breakdown structure of a system. \nBecause the architecture includes the broadest decomposition of the system, it is \ntypically used as the basis for the work-breakdown structure. The work-breakdown \nstructure in turn dictates units of planning, scheduling, and budget; interteam com-\nmunication channels; configuration control and file-system organization; integration \nand test plans and procedures; and even project minutiae such as how the project \nintranet is organized and who sits with whom at the company picnic. Teams commu-\nnicate with each other in terms of the interface specifications for the major elements. \nThe maintenance activity, when launched, will also reflect the software structure, \nwith teams formed to maintain specific structural elements from the architecture: the \ndatabase, the business rules, the user interface, the device drivers, and so forth.\nA side effect of establishing the work-breakdown structure is to freeze some \naspects of the software architecture. A group that is responsible for one of the \nsubsystems will resist having its responsibilities distributed across other groups. \nIf these responsibilities have been formalized in a contractual relationship, chang-\ning responsibilities could become expensive or even litigious. \nThus, once the architecture has been agreed on, it becomes very costly—for \nmanagerial and business reasons—to significantly modify it. This is one argu-\nment (among many) for carrying out extensive analysis before settling on the \nsoftware architecture for a large system—because so much depends on it.\n2.8  Enabling Evolutionary Prototyping\nOnce an architecture has been defined, it can be analyzed and prototyped \nas a skeletal system. A skeletal system is one in which at least some of the \n",
      "content_length": 2701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "34 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\ninfrastructure—how the elements initialize, communicate, share data, access re-\nsources, report errors, log activity, and so forth—is built before much of the sys-\ntem’s functionality has been created. (The two can go hand in hand: build a little \ninfrastructure to support a little end-to-end functionality; repeat until done.) \nFor example, systems built as plug-in architectures are skeletal systems: the \nplug-ins provide the actual functionality. This approach aids the development \nprocess because the system is executable early in the product’s life cycle. The \nfidelity of the system increases as stubs are instantiated, or prototype parts are \nreplaced with complete versions of these parts of the software. In some cases the \nprototype parts can be low-fidelity versions of the final functionality, or they can \nbe surrogates that consume and produce data at the appropriate rates but do little \nelse. Among other things, this approach allows potential performance problems \nto be identified early in the product’s life cycle.\nThese benefits reduce the potential risk in the project. Furthermore, if the ar-\nchitecture is part of a family of related systems, the cost of creating a framework \nfor prototyping can be distributed over the development of many systems.\n2.9  Improving Cost and Schedule Estimates\nCost and schedule estimates are important tools for the project manager both to \nacquire the necessary resources and to monitor progress on the project, to know \nif and when a project is in trouble. One of the duties of an architect is to help \nthe project manager create cost and schedule estimates early in the project life \ncycle. Although top-down estimates are useful for setting goals and apportion-\ning budgets, cost estimations that are based on a bottom-up understanding of the \nsystem’s pieces are typically more accurate than those that are based purely on \ntop-down system knowledge. \nAs we have said, the organizational and work-breakdown structure of a proj-\nect is almost always based on its architecture. Each team or individual responsi-\nble for a work item will be able to make more-accurate estimates for their piece \nthan a project manager and will feel more ownership in making the estimates \ncome true. But the best cost and schedule estimates will typically emerge from a \nconsensus between the top-down estimates (created by the architect and project \nmanager) and the bottom-up estimates (created by the developers). The discus-\nsion and negotiation that results from this process creates a far more accurate \nestimate than either approach by itself.\nIt helps if the requirements for a system have been reviewed and validated. \nThe more up-front knowledge you have about the scope, the more accurate the \ncost and schedule estimates will be.\nChapter 22 delves into the use of architecture in project management.\n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "2.11  Allowing Incorporation of Independently Developed Components\n35\n2.10  Supplying a Transferable, Reusable Model\nThe earlier in the life cycle that reuse is applied, the greater the benefit that can \nbe achieved. While code reuse provides a benefit, reuse of architectures provides \ntremendous leverage for systems with similar requirements. Not only can code be \nreused, but so can the requirements that led to the architecture in the first place, \nas well as the experience and infrastructure gained in building the reused archi-\ntecture. When architectural decisions can be reused across multiple systems, all \nof the early-decision consequences we just described are also transferred.\nA software product line or family is a set of software systems that are all \nbuilt using the same set of reusable assets. Chief among these assets is the archi-\ntecture that was designed to handle the needs of the entire family. Product-line \narchitects choose an architecture (or a family of closely related architectures) that \nwill serve all envisioned members of the product line. The architecture defines \nwhat is fixed for all members of the product line and what is variable. Software \nproduct lines represent a powerful approach to multi-system development that \nis showing order-of-magnitude payoffs in time to market, cost, productivity, \nand product quality. The power of architecture lies at the heart of the paradigm. \nSimilar to other capital investments, the architecture for a product line becomes \na developing organization’s core asset. Software product lines are explained in \nChapter 25. \n2.11  \u0007Allowing Incorporation of Independently \nDeveloped Components\nWhereas earlier software paradigms have focused on programming as the prime \nactivity, with progress measured in lines of code, architecture-based development \noften focuses on composing or assembling elements that are likely to have been \ndeveloped separately, even independently, from each other. This composition is \npossible because the architecture defines the elements that can be incorporated \ninto the system. The architecture constrains possible replacements (or additions) \naccording to how they interact with their environment, how they receive and re-\nlinquish control, what data they consume and produce, how they access data, and \nwhat protocols they use for communication and resource sharing. \nIn 1793, Eli Whitney’s mass production of muskets, based on the principle \nof interchangeable parts, signaled the dawn of the industrial age. In the days be-\nfore physical measurements were reliable, manufacturing interchangeable parts \nwas a daunting notion. Today in software, until abstractions can be reliably de-\nlimited, the notion of structural interchangeability is just as daunting and just as \nsignificant. \n",
      "content_length": 2799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "36 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\nCommercial off-the-shelf components, open source software, publicly avail-\nable apps, and networked services are all modern-day software instantiations of \nWhitney’s basic idea. Whitney’s musket parts had “interfaces” (having to do with \nfit and durability) and so do today’s interchangeable software components.\nFor software, the payoff can be \n■\n■Decreased time to market (it should be easier to use someone else’s ready \nsolution than build your own)\n■\n■Increased reliability (widely used software should have its bugs ironed out \nalready)\n■\n■Lower cost (the software supplier can amortize development cost across \ntheir customer base)\n■\n■Flexibility (if the component you want to buy is not terribly special-\npurpose, it’s likely to be available from several sources, thus increasing \nyour buying leverage)\n2.12  Restricting the Vocabulary of Design Alternatives\nAs useful architectural patterns are collected, it becomes clear that although soft-\nware elements can be combined in more or less infinite ways, there is something \nto be gained by voluntarily restricting ourselves to a relatively small number of \nchoices of elements and their interactions. By doing so we minimize the design \ncomplexity of the system we are building. \nA software engineer is not an artiste, whose creativity and freedom are \nparamount. Engineering is about discipline, and discipline comes in part by re-\nstricting the vocabulary of alternatives to proven solutions. Advantages of this \napproach include enhanced reuse, more regular and simpler designs that are more \neasily understood and communicated, more capable analysis, shorter selection \ntime, and greater interoperability. Architectural patterns guide the architect and \nfocus the architect on the quality attributes of interest in large part by restricting \nthe vocabulary of design alternatives to a relatively small number.\nProperties of software design follow from the choice of an architectural pat-\ntern. Those patterns that are more desirable for a particular problem should im-\nprove the implementation of the resulting design solution, perhaps by making it \neasier to arbitrate conflicting design constraints, by increasing insight into poorly \nunderstood design contexts, or by helping to surface inconsistencies in require-\nments. We will discuss architectural patterns in more detail in Chapter 13.\n",
      "content_length": 2427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "2.14  Summary\n37\n2.13  Providing a Basis for Training\nThe architecture, including a description of how the elements interact with each \nother to carry out the required behavior, can serve as the first introduction to the \nsystem for new project members. This reinforces our point that one of the im-\nportant uses of software architecture is to support and encourage communication \namong the various stakeholders. The architecture is a common reference point.\nModule views are excellent for showing someone the structure of a project: \nWho does what, which teams are assigned to which parts of the system, and so \nforth. Component-and-connector views are excellent for explaining how the sys-\ntem is expected to work and accomplish its job.\nWe will discuss these views in more detail in Chapter 18.\n2.14  Summary\nSoftware architecture is important for a wide variety of technical and nontechni-\ncal reasons. Our list includes the following:\n1.\t An architecture will inhibit or enable a system’s driving quality attributes.\n2.\t The decisions made in an architecture allow you to reason about and man-\nage change as the system evolves.\n3.\t The analysis of an architecture enables early prediction of a system’s \nqualities.\n4.\t A documented architecture enhances communication among stakeholders.\n5.\t The architecture is a carrier of the earliest and hence most fundamental, \nhardest-to-change design decisions.\n6.\t An architecture defines a set of constraints on subsequent implementation.\n7.\t The architecture dictates the structure of an organization, or vice versa.\n8.\t An architecture can provide the basis for evolutionary prototyping.\n9.\t An architecture is the key artifact that allows the architect and project man-\nager to reason about cost and schedule.\n10.\t An architecture can be created as a transferable, reusable model that forms \nthe heart of a product line.\n11.\t Architecture-based development focuses attention on the assembly of com-\nponents, rather than simply on their creation.\n12.\t An architecture channels the creativity of developers, reducing design and \nsystem complexity.\n13.\t An architecture can be the foundation for training of a new team member.\n",
      "content_length": 2175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "38 \nPart One  Introduction\t\n2—Why Is Software Architecture Important?\n2.15  For Further Reading\nRebecca Grinter has observed architects from a sociological standpoint. In \n[Grinter 99] she argues eloquently that the architect’s primary role is to facilitate \nstakeholder communication. The way she puts it is that architects enable com-\nmunication among parties who would otherwise not be able to talk to each other.\nThe granddaddy of papers about architecture and organization is [Conway \n68]. Conway’s law states that “organizations which design systems . . . are con-\nstrained to produce designs which are copies of the communication structures of \nthese organizations.” \nThere is much about software development through composition that re-\nmains unresolved. When the components that are candidates for importation and \nreuse are distinct subsystems that have been built with conflicting architectural \nassumptions, unanticipated complications can increase the effort required to inte-\ngrate their functions. David Garlan and his colleagues coined the term architec-\ntural mismatch to describe this situation, and their paper on it is worth reading \n[Garlan 95].\nPaulish [Paulish 02] discusses architecture-based project management, and \nin particular the ways in which an architecture can help in the estimation of proj-\nect cost and schedule.\n2.16  Discussion Questions\n1.\t\nFor each of the thirteen reasons articulated in this chapter why architecture \nis important, take the contrarian position: Propose a set of circumstances \nunder which architecture is not necessary to achieve the result indicated. \nJustify your position. (Try to come up with different circumstances for each \nof the thirteen.)\n2.\t\nThis chapter argues that architecture brings a number of tangible benefits. \nHow would you measure the benefits, on a particular project, of each of the \nthirteen points?\n3.\t\nSuppose you want to introduce architecture-centric practices to your orga-\nnization. Your management is open to the idea, but wants to know the ROI \nfor doing so. How would you respond?\n4.\t\nPrioritize the list of thirteen points in this chapter according to some criteria \nmeaningful to you. Justify your answer. Or, if you could choose only two \nor three of the reasons to promote the use of architecture in a project, which \nwould you choose and why?\n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "39\n3\nThe Many Contexts of \nSoftware Architecture\nPeople in London think of London as the center \nof the world, whereas New Yorkers think the \nworld ends three miles outside of Manhattan.\n—Toby Young \nIn 1976, a New Yorker magazine cover featured a cartoon by Saul Steinberg \nshowing a New Yorker’s view of the world. You’ve probably seen it; if not, you \ncan easily find it online. Looking to the west from 9th Avenue in Manhattan, the \nillustration shows 10th Avenue, then the wide Hudson River, then a thin strip \nof completely nondescript land called “Jersey,” followed by a somewhat thicker \nstrip of land representing the entire rest of the United States. The mostly empty \nUnited States has a cartoon mountain or two here and there and a few cities hap-\nhazardly placed “out there,” and is flanked by featureless “Canada” on the right \nand “Mexico” on the left. Beyond is the Pacific Ocean, only slightly wider than \nthe Hudson, and beyond that lie tiny amorphous shapes for Japan and China and \nRussia, and that’s pretty much the world from a New Yorker’s perspective.\nIn a book about architecture, it is tempting to view architecture in the same \nway, as the most important piece of the software universe. And in some chapters, \nwe unapologetically will do exactly that. But in this chapter we put software ar-\nchitecture in its place, showing how it supports and is informed by other critical \nforces and activities in the various contexts in which it plays a role. \nThese contexts, around which we structured this book, are as follows:\n■\n■Technical. What technical role does the software architecture play in the \nsystem or systems of which it’s a part? \n■\n■Project life cycle. How does a software architecture relate to the other \nphases of a software development life cycle?\n■\n■Business. How does the presence of a software architecture affect an orga-\nnization’s business environment?\n",
      "content_length": 1898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "40 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n■\n■Professional. What is the role of a software architect in an organization or a \ndevelopment project? \nThese contexts all play out throughout the book, but this chapter introduces each \none. Although the contexts are unchanging, the specifics for your system may \nchange over time. One challenge for the architect is to envision what in their \ncontext might change and to adopt mechanisms to protect the system and its de-\nvelopment if the envisioned changes come to pass.\n3.1  Architecture in a Technical Context\nArchitectures inhibit or enable the achievement of quality attributes, and one use \nof an architecture is to support reasoning about the consequences of change in the \nparticular quality attributes important for a system at its inception.\nArchitectures Inhibit or Enable the \nAchievement of Quality Attributes\nChapter 2 listed thirteen reasons why software architecture is important and mer-\nits study. Several of those reasons deal with exigencies that go beyond the bounds \nof a particular development project (such as communication among stakehold-\ners, many of whom may reside outside the project’s organization). Others deal \nwith nontechnical aspects of a project (such as the architecture’s influence on a \nproject’s team structure, or its contribution to accurate budget and schedule esti-\nmation). The first three reasons in that List of Thirteen deal specifically with an \narchitecture’s technical impact on every system that uses it:\n1.\t\nAn architecture will inhibit or enable the achievement of a system’s quality \nattributes.\n2.\t\nYou can predict many aspects of a system’s qualities by studying its \narchitecture.\n3.\t\nAn architecture makes it easier for you to reason about and manage change.\nThese are all about the architecture’s effect on a system’s quality attributes, \nalthough the first one states it the most explicitly. While all of the reasons enu-\nmerated in Chapter  2 are valid statements of the contribution of architecture, \nprobably the most important reason that it warrants attention is its critical effect \non quality attributes.\nThis is such a critical point that, with your indulgence, we’ll add a few more \npoints to the bullet list that we gave in Section 2.1. Remember? The one that \nstarted like this:\n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "3.1  Architecture in a Technical Context\n41\n■\n■If your system requires high performance, then you need to pay attention \nto managing the time-based behavior of elements, their use of shared \nresources, and the frequency and volume of interelement communication.\nTo that list, we’ll add the following:\n■\n■If you care about a system’s availability, you have to be concerned with \nhow components take over for each other in the event of a failure, and how \nthe system responds to a fault.\n■\n■If you care about usability, you have to be concerned about isolating the \ndetails of the user interface and those elements responsible for the user \nexperience from the rest of the system, so that those things can be tailored \nand improved over time.\n■\n■If you care about the testability of your system, you have to be concerned \nabout the testability of individual elements, which means making their state \nobservable and controllable, plus understanding the emergent behavior of \nthe elements working together.\n■\n■If you care about the safety of your system, you have to be concerned about \nthe behavioral envelope of the elements and the emergent behavior of the \nelements working in concert.\n■\n■If you care about interoperability between your system and another, you \nhave to be concerned about which elements are responsible for external \ninteractions so that you can control those interactions.\nThese and other representations are all saying the same thing in different \nways: If you care about this quality attribute, you have to be concerned with these \ndecisions, all of which are thoroughly architectural in nature. An architecture in-\nhibits or enables a system’s quality attributes. And conversely, nothing else influ-\nences an architecture more than the quality attribute requirements it must satisfy.\nIf you care about architecture for no other reason, you should care about it for \nthis one. We feel so strongly about architecture’s importance with respect to achiev-\ning system quality attributes that all of Part II of this book is devoted to the topic.\nWhy is functionality missing from the preceding list? It is missing because \nthe architecture mainly provides containers into which the architect places func-\ntionality. Functionality is not so much a driver for the architecture as it is a conse-\nquence of it. We return to this point in more detail in Part II.\nArchitectures and the Technical Environment\nThe technical environment that is current when an architecture is designed will \ninfluence that architecture. It might include standard industry practices or soft-\nware engineering techniques prevalent in the architect’s professional community. \nIt is a brave architect who, in today’s environment, does not at least consider \na web-based, object-oriented, service-oriented, mobility-aware, cloud-based, \n",
      "content_length": 2822,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "42 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nsocial-networking-friendly design for an information system. It wasn’t always so, \nand it won’t be so ten years from now when another crop of technological trends \nhas come to the fore.\nThe Swedish Ship Vasa\nIn the 1620s, Sweden and Poland were at war. The king of Sweden, \nGustavus Adolphus, was determined to put a swift and favorable end to it \nand commissioned a new warship the likes of which had never been seen \nbefore. The Vasa, shown in Figure 3.1, was to be the world’s most formi-\ndable instrument of war: 70 meters long, able to carry 300 soldiers, and \nwith an astonishing 64 heavy guns mounted on two gun decks. Seeking to \nadd overwhelming firepower to his navy to strike a decisive blow, the king \ninsisted on stretching the Vasa’s armaments to the limits. Her architect, \nHenrik Hybertsson, was a seasoned Dutch shipbuilder with an impeccable \nreputation, but the Vasa was beyond even his broad experience. Two-\ngun-deck ships were rare, and none had been built of the Vasa’s size and \narmament. \nLike all architects of systems that push the envelope of experience, \nHybertsson had to balance many concerns. Swift time to deployment was \ncritical, but so were performance, functionality, safety, reliability, and cost. \nFigure 3.1  The warship. Used with permission of The Vasa Museum, \nStockholm, Sweden.\n",
      "content_length": 1396,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "3.1  Architecture in a Technical Context\n43\nHe was also responsible to a variety of stakeholders. In this case, the \nprimary customer was the king, but Hybertsson also was responsible to \nthe crew that would sail his creation. Also like all architects, Hybertsson \nbrought his experience with him to the task. In this case, his experience \ntold him to design the Vasa as though it were a single-gun-deck ship and \nthen extrapolate, which was in accordance with the technical environment \nof the day. Faced with an impossible task, Hybertsson had the good sense \nto die about a year before the ship was finished.\nThe project was completed to his specifications, however, and on \nSunday morning, August 10, 1628, the mighty ship was ready. She set her \nsails, waddled out into Stockholm’s deep-water harbor, fired her guns in sa-\nlute, and promptly rolled over. Water poured in through the open gun ports, \nand the Vasa plummeted. A few minutes later her first and only voyage \nended 30 meters beneath the surface. Dozens among her 150-man crew \ndrowned.\nInquiries followed, which concluded that the ship was well built but “badly \nproportioned.” In other words, its architecture was flawed. Today we know \nthat Hybertsson did a poor job of balancing all of the conflicting constraints \nlevied on him. In particular, he did a poor job of risk management and a \npoor job of customer management (not that anyone could have fared bet-\nter). He simply acquiesced in the face of impossible requirements. \nThe story of the Vasa, although more than 375 years old, well illustrates \nthe Architecture Influence Cycle: organization goals beget requirements, \nwhich beget an architecture, which begets a system. The architecture flows \nfrom the architect’s experience and the technical environment of the day. \nHybertsson suffered from the fact that neither of those were up to the task \nbefore him. \nIn this book, we provide three things that Hybertsson could have used:\n1.\t\nExamples of successful architectural practices that work under \ndemanding requirements, so as to help set the technical \nplaying field of the day.\n2.\t\nMethods to assess an architecture before any system is built \nfrom it, so as to mitigate the risks associated with launching \nunprecedented designs.\n3.\t\nTechniques for incremental architecture-based development, \nso as to uncover design flaws before it is too late to correct \nthem.\nOur goal is to give architects another way out of their design dilemmas \nthan the one that befell the ill-fated Dutch ship designer. Death before de-\nployment is not nearly so admired these days. \n—PCC\n",
      "content_length": 2599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "44 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.2  Architecture in a Project Life-Cycle Context\nSoftware development processes are standard approaches for developing software \nsystems. They impose a discipline on software engineers and, more important, \nteams of software engineers. They tell the members of the team what to do next. \nThere are four dominant software development processes, which we describe in \nroughly the order in which they came to prominence: \n1.\t\nWaterfall. For many years the Waterfall model dominated the field of \nsoftware development. The Waterfall model organized the life cycle into a \nseries of connected sequential activities, each with entry and exit conditions \nand a formalized relationship with its upstream and downstream neighbors. \nThe process began with requirements specification, followed by design, \nthen implementation, then integration, then testing, then installation, \nall followed by maintenance. Feedback paths from later to earlier steps \nallowed for the revision of artifacts (requirements documents, design \ndocuments, etc.) on an as-needed basis, based on the knowledge acquired \nin the later stage. For example, designers might push back against overly \nstringent requirements, which would then be reworked and flow back down. \nTesting that uncovered defects would trigger reimplementation (and maybe \neven redesign). And then the cycle continued.\n2.\t\nIterative. Over time the feedback paths of the Waterfall model became \nso pronounced that it became clear that it was better to think of software \ndevelopment as a series of short cycles through the steps—some \nrequirements lead to some design, which can be implemented and tested \nwhile the next cycle’s worth of requirements are being captured and \ndesigned. These cycles are called iterations, in the sense of iterating toward \nthe ultimate software solution for the given problem. Each iteration should \ndeliver something working and useful. The trick here is to uncover early \nthose requirements that have the most far-reaching effect on the design; the \ncorresponding danger is to overlook requirements that, when discovered \nlater, will capsize the design decisions made so far. An especially well-\nknown iterative process is called the Unified Process (originally named the \nRational Unified Process, after Rational Software, which originated it). It \ndefines four phases of each iteration: inception, elaboration, construction, \nand transition. A set of chosen use cases defines the goals for each iteration, \nand the iterations are ordered to address the greatest risks first.\n3.\t\nAgile. The term “Agile software development” refers to a group of \nsoftware development methodologies, the best known of which include \nScrum, Extreme Programming, and Crystal Clear. These methodologies \nare all incremental and iterative. As such, one can consider some iterative \n",
      "content_length": 2904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "3.2  Architecture in a Project Life-Cycle Context\n45\nmethodologies as Agile. What distinguishes Agile practices is early \nand frequent delivery of working software, close collaboration between \ndevelopers and customers, self-organizing teams, and a focus on adaptation \nto changing circumstances (such as late-arriving requirements). All Agile \nmethodologies focus on teamwork, adaptability, and close collaboration \n(both within the team and between team members and customers/end \nusers). These methodologies typically eschew substantial up-front work, \non the assumption that requirements always change, and they continue to \nchange throughout the project’s life cycle. As such, it might seem that Agile \nmethodologies and architecture cannot happily coexist. As we will show in \nChapter 15, this is not so.\n4.\t\nModel-driven development. Model-driven development is based on the \nidea that humans should not be writing code in programming languages, \nbut they should be creating models of the domain, from which code is \nautomatically generated. Humans create a platform-independent model \n(PIM), which is combined with a platform-definition model (PDM) to \ngenerate running code. In this way the PIM is a pure realization of the \nfunctional requirements while the PDM addresses platform specifics and \nquality attributes.\nAll of these processes include design among their obligations, and because \narchitecture is a special kind of design, architecture finds a home in each one. \nChanging from one development process to another in the middle of a project re-\nquires the architect to save useful information from the old process and determine \nhow to integrate it into the new process.\nNo matter what software development process or life-cycle model you’re \nusing, there are a number of activities that are involved in creating a software \narchitecture, using that architecture to realize a complete design, and then imple-\nmenting or managing the evolution of a target system or application. The process \nyou use will determine how often and when you revisit and elaborate each of \nthese activities. These activities include: \n1.\t\nMaking a business case for the system \n2.\t\nUnderstanding the architecturally significant requirements \n3.\t\nCreating or selecting the architecture\n4.\t\nDocumenting and communicating the architecture \n5.\t\nAnalyzing or evaluating the architecture\n6.\t\nImplementing and testing the system based on the architecture\n7.\t\nEnsuring that the implementation conforms to the architecture\nEach of these activities is covered in a chapter in Part III of this book, and \ndescribed briefly below.\n",
      "content_length": 2616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "46 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nMaking a Business Case for the System\nA business case is, briefly, a justification of an organizational investment. It is a \ntool that helps you make business decisions by predicting how they will affect \nyour organization. Initially, the decision will be a go/no-go for pursuing a new \nbusiness opportunity or approach. After initiation, the business case is reviewed \nto assess the accuracy of initial estimates and then updated to examine new or al-\nternative angles on the opportunity. By documenting the expected costs, benefits, \nand risks, the business case serves as a repository of the business and market-\ning data. In this role, management uses the business case to determine possible \ncourses of action. \nKnowing the business goals for the system—Chapter 16 will show you how \nto elicit and capture them in a systematic way—is also critical in the creation of a \nbusiness case for a system. \nCreating a business case is broader than simply assessing the market need \nfor a system. It is an important step in shaping and constraining any future re-\nquirements. How much should the product cost? What is its targeted market? \nWhat is its targeted time to market and lifetime? Will it need to interface with \nother systems? Are there system limitations that it must work within?\nThese are all questions about which the system’s architects have specialized \nknowledge; they must contribute to the answers. These questions cannot be de-\ncided solely by an architect, but if an architect is not consulted in the creation of \nthe business case, the organization may be unable to achieve its business goals. \nTypically, a business case is created prior to the initiation of a project, but it also \nmay be revisited during the course of the project for the organization to deter-\nmine whether to continue making investments in the project. If the circumstances \nassumed in the initial version of the business case change, the architect may be \ncalled upon to establish how the system will change to reflect the new set of \ncircumstances.\nUnderstanding the Architecturally Significant Requirements\nThere are a variety of techniques for eliciting requirements from the stakeholders. \nFor example, object-oriented analysis uses use cases and scenarios to embody \nrequirements. Safety-critical systems sometimes use more rigorous approaches, \nsuch as finite-state-machine models or formal specification languages. In Part II \nof this book, which covers quality attributes, we introduce a collection of quality \nattribute scenarios that aid in the brainstorming, discussion, and capture of qual-\nity attribute requirements for a system. \nOne fundamental decision with respect to the system being built is the extent \nto which it is a variation on other systems that have been constructed. Because \nit is a rare system these days that is not similar to other systems, requirements \n",
      "content_length": 2952,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "3.2  Architecture in a Project Life-Cycle Context\n47\nelicitation techniques involve understanding these prior systems’ characteristics. \nWe discuss the architectural implications of software product lines in Chapter 25. \nAnother technique that helps us understand requirements is the creation of \nprototypes. Prototypes may help to model and explore desired behavior, design \nthe user interface, or analyze resource utilization. This helps to make the system \n“real” in the eyes of its stakeholders and can quickly build support for the project \nand catalyze decisions on the system’s design and the design of its user interface. \nCreating or Selecting the Architecture\nIn the landmark book The Mythical Man-Month, Fred Brooks argues forcefully \nand eloquently that conceptual integrity is the key to sound system design and \nthat conceptual integrity can only be had by a small number of minds coming \ntogether to design the system’s architecture. We firmly believe this as well. Good \narchitecture almost never results as an emergent phenomenon. \nChapters 5–12 and 17 will provide practical techniques that will aid you in \ncreating an architecture to achieve its behavioral and quality requirements. \nDocumenting and Communicating the Architecture\nFor the architecture to be effective as the backbone of the project’s design, it \nmust be communicated clearly and unambiguously to all of the stakeholders. De-\nvelopers must understand the work assignments that the architecture requires of \nthem, testers must understand the task structure that the architecture imposes on \nthem, management must understand the scheduling implications it contains, and \nso forth. \nToward this end, the architecture’s documentation should be informative, \nunambiguous, and readable by many people with varied backgrounds. Architec-\ntural documentation should also be minimal and aimed at the stakeholders who \nwill use it; we are no fans of documentation for documentation’s sake. We dis-\ncuss the documentation of architectures and provide examples of good documen-\ntation practices in Chapter 18. We will also discuss keeping the architecture up to \ndate when there is a change in something on which the architecture documenta-\ntion depends.\nAnalyzing or Evaluating the Architecture\nIn any design process there will be multiple candidate designs considered. Some \nwill be rejected immediately. Others will contend for primacy. Choosing among \nthese competing designs in a rational way is one of the architect’s greatest \nchallenges. \n",
      "content_length": 2520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "48 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nEvaluating an architecture for the qualities that it supports is essential to \nensuring that the system constructed from that architecture satisfies its stake-\nholders’ needs. Analysis techniques to evaluate the quality attributes that an ar-\nchitecture imparts to a system have become much more widespread in the past \ndecade. Scenario-based techniques provide one of the most general and effective \napproaches for evaluating an architecture. The most mature methodological ap-\nproach is found in the Architecture Tradeoff Analysis Method (ATAM) of Chap-\nter 21, while the economic implications of architectural decisions are explored in \nChapter 23.\nImplementing and Testing the System \nBased on the Architecture\nIf the architect designs and analyzes a beautiful, conceptually sound architec-\nture which the implementers then ignore, what was the point? If architecture is \nimportant enough to devote the time and effort of your best minds to, then it is \njust as important to keep the developers faithful to the structures and interaction \nprotocols constrained by the architecture. Having an explicit and well-commu-\nnicated architecture is the first step toward ensuring architectural conformance. \nHaving an environment or infrastructure that actively assists developers in creat-\ning and maintaining the architecture (as opposed to just the code) is better. \nThere are many reasons why developers might not be faithful to the archi-\ntecture: It might not have been properly documented and disseminated. It might \nbe too confusing. It might be that the architect has not built ground-level support \nfor the architecture (particularly if it presents a different way of “doing business” \nthan the developers are used to), and so the developers resist it. Or the developers \nmay sincerely want to implement the architecture but, being human, they occa-\nsionally slip up. This is not to say that the architecture should not change, but it \nshould not change purely on the basis of the whims of the developers, because \nthey may not have the overall picture.\nEnsuring That the Implementation \nConforms to the Architecture\nFinally, when an architecture is created and used, it goes into a maintenance \nphase. Vigilance is required to ensure that the actual architecture and its repre-\nsentation remain faithful to each other during this phase. And when they do get \nsignificantly out of sync, effort must be expended to either fix the implementation \nor update the architectural documentation.\nAlthough work in this area is still relatively immature, it has been an area of \nintense activity in recent years. Chapter 20 will present the current state of recov-\nering an architecture from an existing system and ensuring that it conforms to the \nspecified architecture. \n",
      "content_length": 2843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "3.3  Architecture in a Business Context\n49\n3.3  Architecture in a Business Context\nArchitectures and systems are not constructed frivolously. They serve some business \npurposes, although as mentioned before, these purposes may change over time. \nArchitectures and Business Goals\nSystems are created to satisfy the business goals of one or more organizations. \nDevelopment organizations want to make a profit, or capture market, or stay in \nbusiness, or help their customers do their jobs better, or keep their staff gainfully \nemployed, or make their stockholders happy, or a little bit of each. Customers \nhave their own goals for acquiring a system, usually involving some aspect of \nmaking their lives easier or more productive. Other organizations involved in a \nproject’s life cycle, such as subcontractors or government regulatory agencies, \nhave their own goals dealing with the system.\nArchitects need to understand who the vested organizations are and what their \ngoals are. Many of these goals will have a profound influence on the architecture. \nMany business goals will be manifested as quality attribute requirements. \nIn fact, every quality attribute—such as a user-visible response time or platform \nflexibility or ironclad security or any of a dozen other needs—should originate \nfrom some higher purpose that can be described in terms of added value. If we \nask, for example, “Why do you want this system to have a really fast response \ntime?” we might hear that this will differentiate the product from its competition \nand let the developing organization capture market share. \nSome business goals, however, will not show up in the form of requirements. \nWe know of one software architect who was informed by his manager that the \narchitecture should include a database. The architect was perplexed, because the \nrequirements for the system really didn’t warrant a database and the architect’s \ndesign had nicely avoided putting one in, thereby simplifying the design and \nlowering the cost of the product. The architect was perplexed, that is, until the \nmanager reminded the architect that the company’s database department was cur-\nrently overstaffed and underworked. They needed something to do! The architect \nput in the database, and all was well. That kind of business goal—keeping staff \ngainfully employed—is not likely to show up in any requirements document, but \nif the architect had failed to meet it, the manager would have considered the ar-\nchitecture as unacceptable, just as the customer would have if it failed to provide \na key piece of functionality.\nStill other business goals have no effect on the architecture whatsoever. A \nbusiness goal to lower costs might be realized by asking employees to work from \nhome, or turn the office thermostats down in the winter, or using less paper in the \nprinters. Chapter 16 will deal with uncovering business goals and the require-\nments they lead to.\n",
      "content_length": 2933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "50 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nFigure 3.2 illustrates the major points from the preceding discussion. In the \nfigure, the arrows mean “leads to.” The solid arrows highlight the relationships of \nmost interest to us.\nArchitectures and the Development Organization\nA development organization contributes many of the business goals that influ-\nence an architecture. For example, if the organization has an abundance of ex-\nperienced and idle programmers skilled in peer-to-peer communications, then \na peer-to-peer architecture might be the approach supported by management. If \nnot, it may well be rejected. This would support the business goal, perhaps left \nimplicit, of not wanting to hire new staff or lay off existing staff, or not wanting \nto invest significantly in the retraining of existing staff. \nMore generally, an organization often has an investment in assets, such as \nexisting architectures and the products based on them. The foundation of a de-\nvelopment project may be that the proposed system is the next in a sequence of \nsimilar systems, and the cost estimates assume a high degree of asset reuse and a \nhigh degree of skill and productivity from the programmers. \nAdditionally, an organization may wish to make a long-term business in-\nvestment in an infrastructure to pursue strategic goals and may view the proposed \nsystem as one means of financing and extending that infrastructure. For example, \nan organization may decide that it wants to develop a reputation for supporting \nsolutions based on cloud computing or service-oriented architecture or high-per-\nformance real-time computing. This long-term goal would be supported, in part, \nby infrastructural investments that will affect the developing organization: a \ncloud-computing group needs to be hired or grown, infrastructure needs to be \npurchased, or perhaps training needs to be planned.\nBusiness Goals\nQuality Attributes\nArchitecture\nNonarchitectural Solutions\nFigure 3.2  Some business goals may lead to quality attribute requirements \n(which lead to architectures), or lead directly to architectural decisions, or lead to \nnonarchitectural solutions.\n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "3.4  Architecture in a Professional Context\n51\nFinally, the organizational structure can shape the software architecture, and \nvice versa. Organizations are often organized around technology and application \nconcepts: a database group, a networking group, a business rules team, a user-in-\nterface group. So the explicit identification of a distinct subsystem in the archi-\ntecture will frequently lead to the creation of a group with the name of the sub-\nsystem. Furthermore, if the user-interface team frequently needs to communicate \nwith the business rules team, these teams will need to either be co-located or they \nwill need some regular means of communicating and coordinating. \n3.4  Architecture in a Professional Context\nWhat do architects do? How do you become an architect? In this section we talk \nabout the many facets of being an architect that go beyond what you learned in a \nprogramming or software engineering course.\nYou probably know by now that architects need more than just technical \nskills. Architects need to explain to one stakeholder or another the chosen prior-\nities of different properties, and why particular stakeholders are not having all of \ntheir expectations fulfilled. To be an effective architect, then, you will need diplo-\nmatic, negotiation, and communication skills.\nYou will perform many activities beyond directly producing an architecture. \nThese activities, which we call duties, form the backbone of individual architec-\nture competence. We surveyed the broad body of information aimed at architects \n(such as websites, courses, books, and position descriptions for architects), as \nwell as practicing architects, and duties are but one aspect. Writers about archi-\ntects also speak of skills and knowledge. For example, architects need the ability \nto communicate ideas clearly and need to have up-to-date knowledge about (for \nexample) patterns, or database platforms, or web services standards. \nDuties, skills, and knowledge form a triad on which architecture compe-\ntence rests. You will need to be involved in supporting management and deal-\ning with customers. You will need to manage a diverse workload and be able to \nswitch contexts frequently. You will need to know business considerations. You \nwill need to be a leader in the eyes of developers and management. In Chapter 24 \nwe examine at length the architectural competence of organizations and people.\nArchitects’ Background and Experience\nWe are all products of our experiences, architects included. If you have had good \nresults using a particular architectural approach, such as three-tier client-server \nor publish-subscribe, chances are that you will try that same approach on a new \ndevelopment effort. Conversely, if your experience with an approach was disas-\ntrous, you may be reluctant to try it again. \n",
      "content_length": 2829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "52 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nArchitectural choices may also come from your education and training, \nexposure to successful architectural patterns, or exposure to systems that have \nworked particularly poorly or particularly well. You may also wish to experiment \nwith an architectural pattern or technique learned from a book (such as this one) \nor a training course. \nWhy do we mention this? Because you (and your organization) must be \naware of this influence, so that you can manage it to the best of your abilities. This \nmay mean that you will critically examine proposed architectural solutions, to \nensure that they are not simply the path of least resistance. It may mean that you \nwill take training courses in interesting new technologies. It may mean that you \nwill invest in exploratory projects, to “test the water” of a new technology. Each \nof these steps is a way to proactively manage your background and experience.\n3.5  Stakeholders\nMany people and organizations are interested in a software system. We call these \nentities stakeholders. A stakeholder is anyone who has a stake in the success of \nthe system: the customer, the end users, the developers, the project manager, the \nmaintainers, and even those who market the system, for example. But stakehold-\ners, despite all having a shared stake in the success of the system, typically have \ndifferent specific concerns that they wish the system to guarantee or optimize. \nThese concerns are as diverse as providing a certain behavior at runtime, perform-\ning well on a particular piece of hardware, being easy to customize, achieving \nshort time to market or low cost of development, gainfully employing program-\nmers who have a particular specialty, or providing a broad range of functions. \nFigure 3.3 shows the architect receiving a few helpful stakeholder “suggestions.” \nYou will need to know and understand the nature, source, and priority of \nconstraints on the project as early as possible. Therefore, you must identify and \nactively engage the stakeholders to solicit their needs and expectations. Early en-\ngagement of stakeholders allows you to understand the constraints of the task, \nmanage expectations, negotiate priorities, and make tradeoffs. Architecture eval-\nuation (covered in Part III of this book) and iterative prototyping are two means \nfor you to achieve stakeholder engagement.\nHaving an acceptable system involves appropriate performance, reliability, \navailability, platform compatibility, memory utilization, network usage, security, \nmodifiability, usability, and interoperability with other systems as well as behav-\nior. All of these qualities, and others, affect how the delivered system is viewed \nby its eventual recipients, and so such quality attributes will be demanded by one \nor more of the system’s stakeholders. \nThe underlying problem, of course, is that each stakeholder has different \nconcerns and goals, some of which may be contradictory. It is a rare requirements \n",
      "content_length": 3030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "3.5  Stakeholders\n53\ndocument that does a good job of capturing all of a system’s quality requirements \nin testable detail (a property is testable if it is falsifiable; “make the system easy \nto use” is not falsifiable but “deliver audio packets with no more than 10 ms. \njitter” is falsifiable). The architect often has to fill in the blanks—the quality attri-\nbute requirements that have not been explicitly stated—and mediate the conflicts \nthat frequently emerge.\nTherefore, one of the best pieces of advice we can give to architects is this: \nKnow your stakeholders. Talk to them, engage them, listen to them, and put your-\nself in their shoes. Table 3.1 enumerates a set of stakeholders. Notice the remark-\nable variety and length of this set, but remember that not every stakeholder named \nin this list may play a role in every system, and one person may play many roles. \nArchitect\nDeveloping\nOrganization’s\nManagement\nStakeholder\nMarketing\nStakeholder\nEnd User\nStakeholder\nMaintenance\nOrganization\nStakeholder\nCustomer\nStakeholder\nlow cost,\nkeeping people\nemployed!\nbehavior,\nperformance,\nsecurity,\nreliability,\nusability!\nNeat features,\nshort time to market,\nlow cost, parity with\ncompeting products!\nModifiability!\nlow cost, timely\ndelivery, not changed\nvery often!\nOhhhhhh...\nFigure 3.3  Influence of stakeholders on the architect\n",
      "content_length": 1343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "54 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\nTable 3.1  Stakeholders for a System and Their Interests\nName\nDescription\nInterest in Architecture\nAnalyst\nResponsible for analyzing the architecture to make sure it meets certain \ncritical quality attribute requirements. Analysts are often specialized; for \ninstance, performance analysts, safety analysts, and security analysts \nmay have well-defined positions in a project.\nAnalyzing satisfaction of quality attribute requirements of the system \nbased on its architecture.\nArchitect\nResponsible for the development of the architecture and its \ndocumentation. Focus and responsibility is on the system.\nNegotiating and making tradeoffs among competing requirements \nand design approaches. A vessel for recording design decisions. \nProviding evidence that the architecture satisfies its requirements.\nBusiness  \nManager\nResponsible for the functioning of the business/organizational entity \nthat owns the system. Includes managerial/executive responsibility, \nresponsibility for defining business processes, etc. \nUnderstanding the ability of the architecture to meet business goals.\nConformance \nChecker\nResponsible for assuring conformance to standards and processes to \nprovide confidence in a product’s suitability.\nBasis for conformance checking, for assurance that implementations \nhave been faithful to the architectural prescriptions.\nCustomer\nPays for the system and ensures its delivery. The customer often speaks \nfor or represents the end user, especially in a government acquisition \ncontext. \nAssuring required functionality and quality will be delivered; gauging \nprogress; estimating cost; and setting expectations for what will be \ndelivered, when, and for how much.\nDatabase \nAdministrator\nInvolved in many aspects of the data stores, including database design, \ndata analysis, data modeling and optimization, installation of database \nsoftware, and monitoring and administration of database security.\nUnderstanding how data is created, used, and updated by other \narchitectural elements, and what properties the data and database \nmust have for the overall system to meet its quality goals.\nDeployer\nResponsible for accepting the completed system from the development \neffort and deploying it, making it operational, and fulfilling its allocated \nbusiness function.\nUnderstanding the architectural elements that are delivered and \nto be installed at the customer or end user’s site, and their overall \nresponsibility toward system function.\nDesigner\nResponsible for systems and/or software design downstream of the \narchitecture, applying the architecture to meet specific requirements of the \nparts for which they are responsible.\nResolving resource contention and establishing performance and \nother kinds of runtime resource consumption budgets. Understand-\ning how their part will communicate and interact with other parts of \nthe system.\nEvaluator\nResponsible for conducting a formal evaluation of the architecture (and its \ndocumentation) against some clearly defined criteria.\nEvaluating the architecture’s ability to deliver required behavior and \nquality attributes.\n",
      "content_length": 3169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "3.5  Stakeholders\n55\nName\nDescription\nInterest in Architecture\nImplementer\nResponsible for the development of specific elements according to \ndesigns, requirements, and the architecture.\nUnderstanding inviolable constraints and exploitable freedoms on \ndevelopment activities.\nIntegrator\nResponsible for taking individual components and integrating them, \naccording to the architecture and system designs.\nProducing integration plans and procedures, and locating the source \nof integration failures.\nMaintainer\nResponsible for fixing bugs and providing enhancements to the system \nthroughout its life (including adaptation of the system for uses not originally \nenvisioned).\nUnderstanding the ramifications of a change.\nNetwork \nAdministrator\nResponsible for the maintenance and oversight of computer hardware \nand software in a computer network. This may include the deployment, \nconfiguration, maintenance, and monitoring of network components.\nDetermining network loads during various use profiles, understanding \nuses of the network.\nProduct-Line \nManager\nResponsible for development of an entire family of products, all built using \nthe same core assets (including the architecture).\nDetermining whether a potential new member of a product family is in \nor out of scope and, if out, by how much.\nProject Manager\nResponsible for planning, sequencing, scheduling, and allocating \nresources to develop software components and deliver components to \nintegration and test activities.\nHelping to set budget and schedule, gauging progress against \nestablished budget and schedule, identifying and resolving \ndevelopment-time resource contention.\nRepresentative of \nExternal Systems \nResponsible for managing a system with which this one must interoperate, \nand its interface with our system.\nDefining the set of agreement between the systems. \nSystem Engineer\nResponsible for design and development of systems or system \ncomponents in which software plays a role.\nAssuring that the system environment provided for the software is \nsufficient.\nTester\nResponsible for the (independent) test and verification of the system or its \nelements against the formal requirements and the architecture.\nCreating tests based on the behavior and interaction of the software \nelements. \nUser\nThe actual end users of the system. There may be distinguished kinds of \nusers, such as administrators, superusers, etc.\nUsers, in the role of reviewers, might use architecture documentation \nto check whether desired functionality is being delivered. Users might \nalso use the documentation to understand what the major system \nelements are, which can aid them in emergency field maintenance.\n",
      "content_length": 2668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "56 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.6  How Is Architecture Influenced?\nFor decades, software designers have been taught to build systems based on the \nsoftware’s technical requirements. In the older Waterfall model, the requirements \ndocument is “tossed over the wall” into the designer’s cubicle, and the designer \nmust come forth with a satisfactory design. Requirements beget design, which \nbegets system. In an iterative or Agile approach to development, an increment of \nrequirements begets an increment of design, and so forth.\nThis vision of software development is short-sighted. In any development \neffort, the requirements make explicit some—but only some—of the desired \nproperties of the final system. Not all requirements are focused directly on de-\nsired system properties; some requirements might mandate a development pro-\ncess or the use of a particular tool. Furthermore, the requirements specification \nonly begins to tell the story. Failure to satisfy other constraints may render the \nsystem just as problematic as if it functioned poorly. \nWhat do you suppose would happen if two different architects, working in two \ndifferent organizations, were given the same requirements specification for a sys-\ntem? Do you think they would produce the same architecture or different ones? \nThe answer is that they would very likely produce different ones, which im-\nmediately belies the notion that requirements determine architecture. Other fac-\ntors are at work. \nA software architecture is a result of business and social influences, as well \nas technical ones. The existence of an architecture in turn affects the technical, \nbusiness, and social environments that subsequently influence future architec-\ntures. In particular, each of the contexts for architecture that we just covered—\ntechnical, project, business, and professional—plays a role in influencing an ar-\nchitect and the architecture, as shown in Figure 3.4. \nArchitect’s Influences\nArchitect\nBusiness\nTechnical\nProject\nProfessional\nStakeholders\nArchitecture\nSystem\nFigure 3.4  Influences on the architect\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "3.7  What Do Architectures Influence? \n57\nAn architect designing a system for which the real-time deadlines are tight \nwill make one set of design choices; the same architect, designing a similar sys-\ntem in which the deadlines can be easily satisfied, will make different choices. \nAnd the same architect, designing a non-real-time system, is likely to make quite \ndifferent choices still. Even with the same requirements, hardware, support soft-\nware, and human resources available, an architect designing a system today is \nlikely to design a different system than might have been designed five years ago. \n3.7  What Do Architectures Influence? \nThe story about contexts influencing architectures has a flip side. It turns out that \narchitectures have an influence on the very factors that influence them. Specifi-\ncally, the existence of an architecture affects the technical, project, business, and \nprofessional contexts that subsequently influence future architectures.\nHere is how the cycle works:\n■\n■Technical context. The architecture can affect stakeholder requirements \nfor the next system by giving the customer the opportunity to receive a \nsystem (based on the same architecture) in a more reliable, timely, and \neconomical manner than if the subsequent system were to be built from \nscratch, and typically with fewer defects. A customer may in fact be willing \nto relax some of their requirements to gain these economies. Shrink-\nwrapped software has clearly affected people’s requirements by providing \nsolutions that are not tailored to any individual’s precise needs but are \ninstead inexpensive and (in the best of all possible worlds) of high quality. \nSoftware product lines have the same effect on customers who cannot be so \nflexible with their requirements.\n■\n■Project context. The architecture affects the structure of the developing \norganization. An architecture prescribes a structure for a system; as we will \nsee, it particularly prescribes the units of software that must be implemented \n(or otherwise obtained) and integrated to form the system. These units \nare the basis for the development project’s structure. Teams are formed \nfor individual software units; and the development, test, and integration \nactivities all revolve around the units. Likewise, schedules and budgets \nallocate resources in chunks corresponding to the units. If a company \nbecomes adept at building families of similar systems, it will tend to invest \nin each team by nurturing each area of expertise. Teams become embedded \nin the organization’s structure. This is feedback from the architecture to \nthe developing organization. In any design undertaken by the organization \nat large, these groups have a strong voice in the system’s decomposition, \npressuring for the continued existence of the portions they control.\n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "58 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n■\n■Business context. The architecture can affect the business goals of the \ndeveloping organization. A successful system built from an architecture can \nenable a company to establish a foothold in a particular market segment—\nthink of the iPhone or Android app platforms as examples. The architecture \ncan provide opportunities for the efficient production and deployment of \nsimilar systems, and the organization may adjust its goals to take advantage \nof its newfound expertise to plumb the market. This is feedback from the \nsystem to the developing organization and the systems it builds.\n■\n■Professional context. The process of system building will affect the \narchitect’s experience with subsequent systems by adding to the corporate \nexperience base. A system that was successfully built around a particular \ntechnical approach will make the architect more inclined to build systems \nusing the same approach in the future. On the other hand, architectures that \nfail are less likely to be chosen for future projects.\nThese and other feedback mechanisms form what we call the Architecture \nInfluence Cycle, or AIC, illustrated in Figure 3.5, which depicts the influences of \nthe culture and business of the development organization on the software archi-\ntecture. That architecture is, in turn, a primary determinant of the properties of \nthe developed system or systems. But the AIC is also based on a recognition that \nshrewd organizations can take advantage of the organizational and experiential \neffects of developing an architecture and can use those effects to position their \nbusiness strategically for future projects.\nArchitect’s Influences\nArchitect\nBusiness\nTechnical\nProject\nProfessional\nStakeholders\nArchitecture\nSystem\nFigure 3.5  Architecture Influence Cycle\n",
      "content_length": 1854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "3.9  For Further Reading\n59\n3.8  Summary \nArchitectures exist in four different contexts.\n1.\t\nTechnical. The technical context includes the achievement of quality \nattribute requirements. We spend Part II discussing how to do this. The \ntechnical context also includes the current technology. The cloud (discussed \nin Chapter 26) and mobile computing (discussed in Chapter 27) are \nimportant current technologies.\n2.\t\nProject life cycle. Regardless of the software development methodology \nyou use, you must make a business case for the system, understand the \narchitecturally significant requirements, create or select the architecture, \ndocument and communicate the architecture, analyze or evaluate the archi-\ntecture, implement and test the system based on the architecture, and ensure \nthat the implementation conforms to the architecture.\n3.\t\nBusiness. The system created from the architecture must satisfy the busi-\nness goals of a wide variety of stakeholders, each of whom has different \nexpectations for the system. The architecture is also influenced by and in-\nfluences the structure of the development organization.\n4.\t\nProfessional. You must have certain skills and knowledge to be an architect, \nand there are certain duties that you must perform as an architect. These \nare influenced not only by coursework and reading but also by your \nexperiences.\nAn architecture has some influences that lead to its creation, and its exis-\ntence has an impact on the architect, the organization, and, potentially, the indus-\ntry. We call this cycle the Architecture Influence Cycle.\n3.9  For Further Reading\nThe product line framework produced by the Software Engineering Institute in-\ncludes a discussion of business cases from which we drew [SEI 12].\nThe SEI has also published a case study of Celsius Tech that includes an ex-\nample of how organizations and customers change over time [Brownsword 96]. \nSeveral other SEI reports discuss how to find business goals and the busi-\nness goals that have been articulated by certain organizations [Kazman 05, Cle-\nments 10b]. \nRuth Malan and Dana Bredemeyer provide a description of how an architect \ncan build buy-in within an organization [Malan 00].\n",
      "content_length": 2204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "60 \nPart One  Introduction\t\n3—The Many Contexts of Software Architecture\n3.10  Discussion Questions\n1.\t\nEnumerate six different software systems used by your organization. For \neach of these systems:\na.\t What are the contextual influences?\nb.\t Who are the stakeholders?\nc.\t How do these systems reflect or impact the organizational structure?\n2.\t\nWhat kinds of business goals have driven the construction of the following:\na.\t The World Wide Web\nb.\t Amazon’s EC2 cloud infrastructure\nc.\t Google’s Android platform\n3.\t\nWhat mechanisms are available to improve your skills and knowledge? \nWhat skills are you lacking?\n4.\t\nDescribe a system you are familiar with and place it into the AIC. Specifi-\ncally, identify the forward and reverse influences on contextual factors.\n",
      "content_length": 770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "61\nPart  T WO\nQuality Attributes\nIn Part II, we provide the technical foundations for you to design or analyze an \narchitecture to achieve particular quality attributes. We do not discuss design or \nanalysis processes here; we cover those topics in Part III. It is impossible, how-\never, to understand how to improve the performance of a design, for example, \nwithout understanding something about performance. \nIn Chapter 4 we describe how to specify a quality attribute requirement and \nmotivate design techniques called tactics to enable you to achieve a particular qual-\nity attribute requirement. We also enumerate seven categories of design decisions. \nThese are categories of decisions that are universally important, and so we provide \nmaterial to help an architect focus on these decisions. In Chapter 4, we describe \nthese categories, and in each of the following chapters devoted to a particular quality \nattribute—Chapters 5–11—we use those categories to develop a checklist that tells \nyou how to focus your attention on the important aspects associated with that quality \nattribute. Many of the items in our checklists may seem obvious, but the purpose of \na checklist is to help ensure the completeness of your design and analysis process.\nIn addition to providing a treatment of seven specific quality attributes \n(availability, interoperability, modifiability, performance, security, testability, and \nusability), we also describe how you can generate the material provided in Chap-\nters 5–11 for other quality attributes that we have not covered.\nArchitectural patterns provide known solutions to a number of common \nproblems in design. In Chapter 13, we present some of the most important pat-\nterns and discuss the relationship between patterns and tactics.\nBeing able to analyze a design for a particular quality attribute is a key skill \nthat you as an architect will need to acquire. In Chapter 14, we discuss modeling \ntechniques for some of the quality attributes.\n",
      "content_length": 1990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "63\n4\nUnderstanding Quality \nAttributes\nBetween stimulus and response, there is a space. In \nthat space is our power to choose our response. In \nour response lies our growth and our freedom.\n— Viktor E. Frankl, Man’s Search for Meaning\nAs we have seen in the Architecture Influence Cycle (in Chapter 3), many fac-\ntors determine the qualities that must be provided for in a system’s architecture. \nThese qualities go beyond functionality, which is the basic statement of the sys-\ntem’s capabilities, services, and behavior. Although functionality and other qual-\nities are closely related, as you will see, functionality often takes the front seat in \nthe development scheme. This preference is shortsighted, however. Systems are \nfrequently redesigned not because they are functionally deficient—the replace-\nments are often functionally identical—but because they are difficult to maintain, \nport, or scale; or they are too slow; or they have been compromised by hackers. \nIn Chapter 2, we said that architecture was the first place in software creation in \nwhich quality requirements could be addressed. It is the mapping of a system’s \nfunctionality onto software structures that determines the architecture’s support \nfor qualities. In Chapters 5–11 we discuss how various qualities are supported by \narchitectural design decisions. In Chapter 17 we show how to integrate all of the \nquality attribute decisions into a single design. \nWe have been using the term “quality attribute” loosely, but now it is time to \ndefine it more carefully. A quality attribute (QA) is a measurable or testable prop-\nerty of a system that is used to indicate how well the system satisfies the needs of \nits stakeholders. You can think of a quality attribute as measuring the “goodness” \nof a product along some dimension of interest to a stakeholder.\nIn this chapter our focus is on understanding the following:\n■\n■How to express the qualities we want our architecture to provide to the sys-\ntem or systems we are building from it \n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "64 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n■\n■How to achieve those qualities \n■\n■How to determine the design decisions we might make with respect to those \nqualities \nThis chapter provides the context for the discussion of specific quality attributes \nin Chapters 5–11.\n4.1  Architecture and Requirements\nRequirements for a system come in a variety of forms: textual requirements, \nmockups, existing systems, use cases, user stories, and more. Chapter 16 dis-\ncusses the concept of an architecturally significant requirement, the role such re-\nquirements play in architecture, and how to identify them. No matter the source, \nall requirements encompass the following categories: \n1.\t\nFunctional requirements. These requirements state what the system must \ndo, and how it must behave or react to runtime stimuli. \n2.\t\nQuality attribute requirements. These requirements are qualifications of \nthe functional requirements or of the overall product. A qualification of a \nfunctional requirement is an item such as how fast the function must be \nperformed, or how resilient it must be to erroneous input. A qualification \nof the overall product is an item such as the time to deploy the product or a \nlimitation on operational costs.\n3.\t\nConstraints. A constraint is a design decision with zero degrees of freedom. \nThat is, it’s a design decision that’s already been made. Examples include \nthe requirement to use a certain programming language or to reuse a certain \nexisting module, or a management fiat to make your system service ori-\nented. These choices are arguably in the purview of the architect, but ex-\nternal factors (such as not being able to train the staff in a new language, or \nhaving a business agreement with a software supplier, or pushing business \ngoals of service interoperability) have led those in power to dictate these \ndesign outcomes.\nWhat is the “response” of architecture to each of these kinds of requirements?\n1.\t\nFunctional requirements are satisfied by assigning an appropriate sequence \nof responsibilities throughout the design. As we will see later in this chap-\nter, assigning responsibilities to architectural elements is a fundamental \narchitectural design decision.\n2.\t\nQuality attribute requirements are satisfied by the various structures de-\nsigned into the architecture, and the behaviors and interactions of the ele-\nments that populate those structures. Chapter 17 will show this approach in \nmore detail. \n",
      "content_length": 2477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "4.3  Quality Attribute Considerations \n65\n3.\t\nConstraints are satisfied by accepting the design decision and reconciling it \nwith other affected design decisions.\n4.2  Functionality\nFunctionality is the ability of the system to do the work for which it was in-\ntended. Of all of the requirements, functionality has the strangest relationship to \narchitecture.\nFirst of all, functionality does not determine architecture. That is, given a \nset of required functionality, there is no end to the architectures you could create \nto satisfy that functionality. At the very least, you could divide up the function-\nality in any number of ways and assign the subpieces to different architectural \nelements. \nIn fact, if functionality were the only thing that mattered, you wouldn’t have \nto divide the system into pieces at all; a single monolithic blob with no internal \nstructure would do just fine. Instead, we design our systems as structured sets \nof cooperating architectural elements—modules, layers, classes, services, data-\nbases, apps, threads, peers, tiers, and on and on—to make them understandable \nand to support a variety of other purposes. Those “other purposes” are the other \nquality attributes that we’ll turn our attention to in the remaining sections of this \nchapter, and the remaining chapters of Part II. \nBut although functionality is independent of any particular structure, func-\ntionality is achieved by assigning responsibilities to architectural elements, re-\nsulting in one of the most basic of architectural structures.\nAlthough responsibilities can be allocated arbitrarily to any modules, soft-\nware architecture constrains this allocation when other quality attributes are im-\nportant. For example, systems are frequently divided so that several people can \ncooperatively build them. The architect’s interest in functionality is in how it in-\nteracts with and constrains other qualities. \n4.3  Quality Attribute Considerations \nJust as a system’s functions do not stand on their own without due consideration of \nother quality attributes, neither do quality attributes stand on their own; they pertain \nto the functions of the system. If a functional requirement is “When the user presses \nthe green button, the Options dialog appears,” a performance QA annotation might \ndescribe how quickly the dialog will appear; an availability QA annotation might \ndescribe how often this function will fail, and how quickly it will be repaired; a us-\nability QA annotation might describe how easy it is to learn this function.\n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "66 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\nFunctional Requirements\nAfter more than 15 years of writing and discussing the distinction between \nfunctional requirements and quality requirements, the definition of func-\ntional requirements still eludes me. Quality attribute requirements are well \ndefined: performance has to do with the timing behavior of the system, \nmodifiability has to do with the ability of the system to support changes in \nits behavior or other qualities after initial deployment, availability has to do \nwith the ability of the system to survive failures, and so forth.\nFunction, however, is much more slippery. An international standard \n(ISO 25010) defines functional suitability as “the capability of the software \nproduct to provide functions which meet stated and implied needs when \nthe software is used under specified conditions.” That is, functionality is the \nability to provide functions. One interpretation of this definition is that func-\ntionality describes what the system does and quality describes how well \nthe system does its function. That is, qualities are attributes of the system \nand function is the purpose of the system.\nThis distinction breaks down, however, when you consider the nature \nof some of the “function.” If the function of the software is to control engine \nbehavior, how can the function be correctly implemented without consid-\nering timing behavior? Is the ability to control access through requiring a \nuser name/password combination not a function even though it is not the \npurpose of any system?\nI like much better the use of the word “responsibility” to describe com-\nputations that a system must perform. Questions such as “What are the \ntiming constraints on that set of responsibilities?”, “What modifications are \nanticipated with respect to that set of responsibilities?”, and “What class of \nusers is allowed to execute that set of responsibilities?” make sense and \nare actionable.\nThe achievement of qualities induces responsibility; think of the user \nname/password example just mentioned. Further, one can identify respon-\nsibilities as being associated with a particular set of requirements.\nSo does this mean that the term “functional requirement” shouldn’t be \nused? People have an understanding of the term, but when precision is \ndesired, we should talk about sets of specific responsibilities instead.\nPaul Clements has long ranted against the careless use of the term \n“nonfunctional,” and now it’s my turn to rant against the careless use of the \nterm “functional”—probably equally ineffectually.\n—LB\nQuality attributes have been of interest to the software community at least \nsince the 1970s. There are a variety of published taxonomies and definitions, and \nmany of them have their own research and practitioner communities. From an \n",
      "content_length": 2851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "4.3  Quality Attribute Considerations \n67\narchitect’s perspective, there are three problems with previous discussions of sys-\ntem quality attributes: \n1.\t\nThe definitions provided for an attribute are not testable. It is meaningless \nto say that a system will be “modifiable.” Every system may be modifiable \nwith respect to one set of changes and not modifiable with respect to an-\nother. The other quality attributes are similar in this regard: a system may \nbe robust with respect to some faults and brittle with respect to others. And \nso forth.\n2.\t\nDiscussion often focuses on which quality a particular concern belongs to. \nIs a system failure due to a denial-of-service attack an aspect of availability, \nan aspect of performance, an aspect of security, or an aspect of usability? \nAll four attribute communities would claim ownership of a system failure \ndue to a denial-of-service attack. All are, to some extent, correct. But this \ndoesn’t help us, as architects, understand and create architectural solutions \nto manage the attributes of concern.\n3.\t\nEach attribute community has developed its own vocabulary. The perfor-\nmance community has “events” arriving at a system, the security com-\nmunity has “attacks” arriving at a system, the availability community has \n“failures” of a system, and the usability community has “user input.” All \nof these may actually refer to the same occurrence, but they are described \nusing different terms.\nA solution to the first two of these problems (untestable definitions and \noverlapping concerns) is to use quality attribute scenarios as a means of charac-\nterizing quality attributes (see the next section). A solution to the third problem \nis to provide a discussion of each attribute—concentrating on its underlying con-\ncerns—to illustrate the concepts that are fundamental to that attribute community.\nThere are two categories of quality attributes on which we focus. The first is \nthose that describe some property of the system at runtime, such as availability, \nperformance, or usability. The second is those that describe some property of the \ndevelopment of the system, such as modifiability or testability. \nWithin complex systems, quality attributes can never be achieved in isola-\ntion. The achievement of any one will have an effect, sometimes positive and \nsometimes negative, on the achievement of others. For example, almost every \nquality attribute negatively affects performance. Take portability. The main tech-\nnique for achieving portable software is to isolate system dependencies, which \nintroduces overhead into the system’s execution, typically as process or proce-\ndure boundaries, and this hurts performance. Determining the design that sat-\nisfies all of the quality attribute requirements is partially a matter of making the \nappropriate tradeoffs; we discuss design in Chapter 17. Our purpose here is to \nprovide the context for discussing each quality attribute. In particular, we focus \non how quality attributes can be specified, what architectural decisions will en-\nable the achievement of particular quality attributes, and what questions about \nquality attributes will enable the architect to make the correct design decisions.\n",
      "content_length": 3217,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "68 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n4.4  Specifying Quality Attribute Requirements\nA quality attribute requirement should be unambiguous and testable. We use a \ncommon form to specify all quality attribute requirements. This has the advantage \nof emphasizing the commonalities among all quality attributes. It has the disad-\nvantage of occasionally being a force-fit for some aspects of quality attributes.\nOur common form for quality attribute expression has these parts:\n■\n■Stimulus. We use the term “stimulus” to describe an event arriving at the \nsystem. The stimulus can be an event to the performance community, a \nuser operation to the usability community, or an attack to the security \ncommunity. We use the same term to describe a motivating action for de-\nvelopmental qualities. Thus, a stimulus for modifiability is a request for \na modification; a stimulus for testability is the completion of a phase of \ndevelopment.\n■\n■Stimulus source. A stimulus must have a source—it must come from some-\nwhere. The source of the stimulus may affect how it is treated by the sys-\ntem. A request from a trusted user will not undergo the same scrutiny as a \nrequest by an untrusted user.\n■\n■Response. How the system should respond to the stimulus must also be \nspecified. The response consists of the responsibilities that the system \n(for runtime qualities) or the developers (for development-time qualities) \nshould perform in response to the stimulus. For example, in a performance \nscenario, an event arrives (the stimulus) and the system should process \nthat event and generate a response. In a modifiability scenario, a request \nfor a modification arrives (the stimulus) and the developers should imple-\nment the modification—without side effects—and then test and deploy the \nmodification.\n■\n■Response measure. Determining whether a response is satisfactory—\nwhether the requirement is satisfied—is enabled by providing a response \nmeasure. For performance this could be a measure of latency or throughput; \nfor modifiability it could be the labor or wall clock time required to make, \ntest, and deploy the modification.\nThese four characteristics of a scenario are the heart of our quality attribute \nspecifications. But there are two more characteristics that are important: environ-\nment and artifact.\n■\n■Environment. The environment of a requirement is the set of circumstances \nin which the scenario takes place. The environment acts as a qualifier on \nthe stimulus. For example, a request for a modification that arrives after \nthe code has been frozen for a release may be treated differently than one \nthat arrives before the freeze. A failure that is the fifth successive failure \n",
      "content_length": 2728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "4.4  Specifying Quality Attribute Requirements\n69\nof a component may be treated differently than the first failure of that \ncomponent.\n■\n■Artifact. Finally, the artifact is the portion of the system to which the \nrequirement applies. Frequently this is the entire system, but occasion-\nally specific portions of the system may be called out. A failure in a \ndata store may be treated differently than a failure in the metadata store. \nModifications to the user interface may have faster response times than \nmodifications to the middleware. \nTo summarize how we specify quality attribute requirements, we capture \nthem formally as six-part scenarios. While it is common to omit one or more of \nthese six parts, particularly in the early stages of thinking about quality attributes, \nknowing that all parts are there forces the architect to consider whether each part \nis relevant. \nIn summary, here are the six parts:\n1.\t\nSource of stimulus. This is some entity (a human, a computer system, or \nany other actuator) that generated the stimulus.\n2.\t\nStimulus. The stimulus is a condition that requires a response when it ar-\nrives at a system.\n3.\t\nEnvironment. The stimulus occurs under certain conditions. The system \nmay be in an overload condition or in normal operation, or some other rele-\nvant state. For many systems, “normal” operation can refer to one of a num-\nber of modes. For these kinds of systems, the environment should specify in \nwhich mode the system is executing.\n4.\t\nArtifact. Some artifact is stimulated. This may be a collection of systems, \nthe whole system, or some piece or pieces of it.\n5.\t\nResponse. The response is the activity undertaken as the result of the arrival \nof the stimulus. \n6.\t\nResponse measure. When the response occurs, it should be measurable in \nsome fashion so that the requirement can be tested. \nWe distinguish general quality attribute scenarios (which we call “general \nscenarios” for short)—those that are system independent and can, potentially, \npertain to any system—from concrete quality attribute scenarios (concrete sce-\nnarios)—those that are specific to the particular system under consideration. \nWe can characterize quality attributes as a collection of general scenarios. \nOf course, to translate these generic attribute characterizations into requirements \nfor a particular system, the general scenarios need to be made system specific. \nDetailed examples of these scenarios will be given in Chapters 5–11. Figure 4.1 \nshows the parts of a quality attribute scenario that we have just discussed. Fig-\nure 4.2 shows an example of a general scenario, in this case for availability.\n",
      "content_length": 2642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "70 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n4.5  Achieving Quality Attributes through Tactics\nThe quality attribute requirements specify the responses of the system that, with a \nbit of luck and a dose of good planning, realize the goals of the business. We now \nturn to the techniques an architect can use to achieve the required quality attri-\nbutes. We call these techniques architectural tactics. A tactic is a design decision \nthat influences the achievement of a quality attribute response—tactics directly \naffect the system’s response to some stimulus. Tactics impart portability to one \ndesign, high performance to another, and integrability to a third.\nStimulus\nResponse\nResponse\nMeasure\nSource\nof Stimulus\nArtifact\nEnvironment\n3\n2\n1\n4\nFigure 4.1  The parts of a quality attribute scenario\nFigure 4.2  A general scenario for availability\nStimulus\nResponse\nResponse\nMeasure\nSource\nof Stimulus\n3\n2\n1\n4\nInternal/External: \npeople, hardware, \nsoftware, physical \ninfrastructure, \nphysical \nenvironment\nFault: \nomission, \ncrash, \nincorrect \ntiming, \nincorrect \nresponse\nPrevent fault from \nbecoming failure\nDetect fault: log, notify \nRecover from fault:\ndisable event source, \nbe unavailable, \nfix/mask, degraded \nmode\nTime or time interval \nsystem must be available\nAvailability percentage \nTime in degraded mode\nTime to detect fault \nRepair time\nProportion of faults \nsystem handles\nArtifact\nProcessors, \ncommunication \nchannels, persistent \nstorage, processes\nEnvironment\nNormal operation, \nstartup, shutdown, \nrepair mode, \ndegraded \noperation, \noverloaded \noperation\n",
      "content_length": 1602,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "4.5  Achieving Quality Attributes through Tactics\n71\nNot My Problem\nOne time I was doing an architecture analysis on a complex system cre-\nated by and for Lawrence Livermore National Laboratory. If you visit their \nwebsite (www.llnl.gov) and try to figure out what Livermore Labs does, you \nwill see the word “security” mentioned over and over. The lab focuses on \nnuclear security, international and domestic security, and environmental \nand energy security. Serious stuff . . .\nKeeping this emphasis in mind, I asked them to describe the quality \nattributes of concern for the system that I was analyzing. I’m sure you can \nimagine my surprise when security wasn’t mentioned once! The system \nstakeholders mentioned performance, modifiability, evolvability, interoper-\nability, configurability, and portability, and one or two more, but the word \nsecurity never passed their lips. \nBeing a good analyst, I questioned this seemingly shocking and obvious \nomission. Their answer was simple and, in retrospect, straightforward: “We \ndon’t care about it. Our systems are not connected to any external net-\nwork and we have barbed-wire fences and guards with machine guns.” Of \ncourse, someone at Livermore Labs was very interested in security. But it \nwas clearly not the software architects.\n—RK\nThe focus of a tactic is on a single quality attribute response. Within a tactic, \nthere is no consideration of tradeoffs. Tradeoffs must be explicitly considered \nand controlled by the designer. In this respect, tactics differ from architectural \npatterns, where tradeoffs are built into the pattern. (We visit the relation between \ntactics and patterns in Chapter 14. Chapter 13 explains how sets of tactics for a \nquality attribute can be constructed, which are the steps we used to produce the \nset in this book.)\nA system design consists of a collection of decisions. Some of these deci-\nsions help control the quality attribute responses; others ensure achievement of \nsystem functionality. We represent the relationship between stimulus, tactics, and \nresponse in Figure 4.3. The tactics, like design patterns, are design techniques \nthat architects have been using for years. Our contribution is to isolate, catalog, \nand describe them. We are not inventing tactics here, we are just capturing what \narchitects do in practice. \nWhy do we do this? There are three reasons: \n1.\t\nDesign patterns are complex; they typically consist of a bundle of design \ndecisions. But patterns are often difficult to apply as is; architects need to \nmodify and adapt them. By understanding the role of tactics, an architect \ncan more easily assess the options for augmenting an existing pattern to \nachieve a quality attribute goal. \n",
      "content_length": 2719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "72 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n2.\t\nIf no pattern exists to realize the architect’s design goal, tactics allow the \narchitect to construct a design fragment from “first principles.” Tactics give \nthe architect insight into the properties of the resulting design fragment. \n3.\t\nBy cataloging tactics, we provide a way of making design more systematic \nwithin some limitations. Our list of tactics does not provide a taxonomy. We \nonly provide a categorization. The tactics will overlap, and you frequently \nwill have a choice among multiple tactics to improve a particular quality at-\ntribute. The choice of which tactic to use depends on factors such as tradeoffs \namong other quality attributes and the cost to implement. These consider-\nations transcend the discussion of tactics for particular quality attributes. \nChapter 17 provides some techniques for choosing among competing tactics.\nThe tactics that we present can and should be refined. Consider perfor-\nmance: Schedule resources is a common performance tactic. But this tactic needs \nto be refined into a specific scheduling strategy, such as shortest-job-first, round-\nrobin, and so forth, for specific purposes. Use an intermediary is a modifiability \ntactic. But there are multiple types of intermediaries (layers, brokers, and prox-\nies, to name just a few). Thus there are refinements that a designer will employ to \nmake each tactic concrete. \nIn addition, the application of a tactic depends on the context. Again consid-\nering performance: Manage sampling rate is relevant in some real-time systems \nbut not in all real-time systems and certainly not in database systems.\n4.6  Guiding Quality Design Decisions\nRecall that one can view an architecture as the result of applying a collection of \ndesign decisions. What we present here is a systematic categorization of these \nFigure 4.3  Tactics are intended to control responses to stimuli.\nStimulus\nResponse\nTactics\nto Control\nResponse\n",
      "content_length": 1992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "4.6  Guiding Quality Design Decisions\n73\ndecisions so that an architect can focus attention on those design dimensions \nlikely to be most troublesome. \nThe seven categories of design decisions are\n1.\t\nAllocation of responsibilities\n2.\t\nCoordination model\n3.\t\nData model\n4.\t\nManagement of resources\n5.\t\nMapping among architectural elements\n6.\t\nBinding time decisions\n7.\t\nChoice of technology\nThese categories are not the only way to classify architectural design deci-\nsions, but they do provide a rational division of concerns. These categories might \noverlap, but it’s all right if a particular decision exists in two different categories, \nbecause the concern of the architect is to ensure that every important decision is \nconsidered. Our categorization of decisions is partially based on our definition \nof software architecture in that many of our categories relate to the definition of \nstructures and the relations among them.\nAllocation of Responsibilities\nDecisions involving allocation of responsibilities include the following:\n■\n■Identifying the important responsibilities, including basic system functions, \narchitectural infrastructure, and satisfaction of quality attributes. \n■\n■Determining how these responsibilities are allocated to non-runtime and \nruntime elements (namely, modules, components, and connectors). \nStrategies for making these decisions include functional decomposition, \nmodeling real-world objects, grouping based on the major modes of system oper-\nation, or grouping based on similar quality requirements: processing frame rate, \nsecurity level, or expected changes.\nIn Chapters 5–11, where we apply these design decision categories to a \nnumber of important quality attributes, the checklists we provide for the alloca-\ntion of responsibilities category is derived systematically from understanding the \nstimuli and responses listed in the general scenario for that QA.\nCoordination Model\nSoftware works by having elements interact with each other through designed \nmechanisms. These mechanisms are collectively referred to as a coordination \nmodel. Decisions about the coordination model include these:\n",
      "content_length": 2142,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "74 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n■\n■Identifying the elements of the system that must coordinate, or are prohib-\nited from coordinating.\n■\n■Determining the properties of the coordination, such as timeliness, cur-\nrency, completeness, correctness, and consistency.\n■\n■Choosing the communication mechanisms (between systems, between our \nsystem and external entities, between elements of our system) that realize \nthose properties. Important properties of the communication mechanisms \ninclude stateful versus stateless, synchronous versus asynchronous, guar-\nanteed versus nonguaranteed delivery, and performance-related properties \nsuch as throughput and latency.\nData Model\nEvery system must represent artifacts of system-wide interest—data—in some \ninternal fashion. The collection of those representations and how to interpret \nthem is referred to as the data model. Decisions about the data model include the \nfollowing:\n■\n■Choosing the major data abstractions, their operations, and their properties. \nThis includes determining how the data items are created, initialized, ac-\ncessed, persisted, manipulated, translated, and destroyed.\n■\n■Compiling metadata needed for consistent interpretation of the data.\n■\n■Organizing the data. This includes determining whether the data is going \nto be kept in a relational database, a collection of objects, or both. If both, \nthen the mapping between the two different locations of the data must be \ndetermined.\nManagement of Resources\nAn architect may need to arbitrate the use of shared resources in the architec-\nture. These include hard resources (e.g., CPU, memory, battery, hardware buffers, \nsystem clock, I/O ports) and soft resources (e.g., system locks, software buffers, \nthread pools, and non-thread-safe code). \nDecisions for management of resources include the following:\n■\n■Identifying the resources that must be managed and determining the limits \nfor each.\n■\n■Determining which system element(s) manage each resource. \n■\n■Determining how resources are shared and the arbitration strategies em-\nployed when there is contention.\n■\n■Determining the impact of saturation on different resources. For example, \nas a CPU becomes more heavily loaded, performance usually just degrades \nfairly steadily. On the other hand, when you start to run out of memory, at \n",
      "content_length": 2352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "4.6  Guiding Quality Design Decisions\n75\nsome point you start paging/swapping intensively and your performance \nsuddenly crashes to a halt.\nMapping among Architectural Elements \nAn architecture must provide two types of mappings. First, there is mapping \nbetween elements in different types of architecture structures—for example, \nmapping from units of development (modules) to units of execution (threads or \nprocesses). Next, there is mapping between software elements and environment \nelements—for example, mapping from processes to the specific CPUs where \nthese processes will execute.\nUseful mappings include these:\n■\n■The mapping of modules and runtime elements to each other—that is, the \nruntime elements that are created from each module; the modules that con-\ntain the code for each runtime element.\n■\n■The assignment of runtime elements to processors.\n■\n■The assignment of items in the data model to data stores.\n■\n■The mapping of modules and runtime elements to units of delivery.\nBinding Time Decisions\nBinding time decisions introduce allowable ranges of variation. This variation \ncan be bound at different times in the software life cycle by different entities—\nfrom design time by a developer to runtime by an end user. A binding time de-\ncision establishes the scope, the point in the life cycle, and the mechanism for \nachieving the variation. \nThe decisions in the other six categories have an associated binding time \ndecision. Examples of such binding time decisions include the following:\n■\n■For allocation of responsibilities, you can have build-time selection of mod-\nules via a parameterized makefile. \n■\n■For choice of coordination model, you can design runtime negotiation of \nprotocols.\n■\n■For resource management, you can design a system to accept new periph-\neral devices plugged in at runtime, after which the system recognizes them \nand downloads and installs the right drivers automatically.\n■\n■For choice of technology, you can build an app store for a smartphone that \nautomatically downloads the version of the app appropriate for the phone of \nthe customer buying the app.\nWhen making binding time decisions, you should consider the costs to im-\nplement the decision and the costs to make a modification after you have im-\nplemented the decision. For example, if you are considering changing platforms \n",
      "content_length": 2343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "76 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\nat some time after code time, you can insulate yourself from the effects caused \nby porting your system to another platform at some cost. Making this decision \ndepends on the costs incurred by having to modify an early binding compared to \nthe costs incurred by implementing the mechanisms involved in the late binding. \nChoice of Technology\nEvery architecture decision must eventually be realized using a specific tech-\nnology. Sometimes the technology selection is made by others, before the in-\ntentional architecture design process begins. In this case, the chosen technology \nbecomes a constraint on decisions in each of our seven categories. In other cases, \nthe architect must choose a suitable technology to realize a decision in every one \nof the categories.\nChoice of technology decisions involve the following:\n■\n■Deciding which technologies are available to realize the decisions made in \nthe other categories.\n■\n■Determining whether the available tools to support this technology choice \n(IDEs, simulators, testing tools, etc.) are adequate for development to \nproceed.\n■\n■Determining the extent of internal familiarity as well as the degree of exter-\nnal support available for the technology (such as courses, tutorials, exam-\nples, and availability of contractors who can provide expertise in a crunch) \nand deciding whether this is adequate to proceed.\n■\n■Determining the side effects of choosing a technology, such as a required \ncoordination model or constrained resource management opportunities.\n■\n■Determining whether a new technology is compatible with the existing \ntechnology stack. For example, can the new technology run on top of or \nalongside the existing technology stack? Can it communicate with the exist-\ning technology stack? Can the new technology be monitored and managed?\n4.7  Summary\nRequirements for a system come in three categories:\n1.\t\nFunctional. These requirements are satisfied by including an appropriate set \nof responsibilities within the design.\n2.\t\nQuality attribute. These requirements are satisfied by the structures and \nbehaviors of the architecture.\n3.\t\nConstraints. A constraint is a design decision that’s already been made.\n",
      "content_length": 2250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "4.9  Discussion Questions\n77\nTo express a quality attribute requirement, we use a quality attribute sce-\nnario. The parts of the scenario are these:\n1.\t\nSource of stimulus\n2.\t\nStimulus\n3.\t\nEnvironment\n4.\t\nArtifact\n5.\t\nResponse \n6.\t\nResponse measure\nAn architectural tactic is a design decision that affects a quality attribute \nresponse. The focus of a tactic is on a single quality attribute response. Architec-\ntural patterns can be seen as “packages” of tactics.\nThe seven categories of architectural design decisions are these:\n1.\t\nAllocation of responsibilities\n2.\t\nCoordination model\n3.\t\nData model\n4.\t\nManagement of resources\n5.\t\nMapping among architectural elements\n6.\t\nBinding time decisions\n7.\t\nChoice of technology\n4.8  For Further Reading\nPhilippe Kruchten [Kruchten 04] provides another categorization of design \ndecisions.\nPena [Pena 87] uses categories of Function/Form/Economy/Time as a way \nof categorizing design decisions. \nBinding time and mechanisms to achieve different types of binding times \nare discussed in [Bachmann 05].\nTaxonomies of quality attributes can be found in [Boehm 78], [McCall 77], \nand [ISO 11].\nArguments for viewing architecture as essentially independent from func-\ntion can be found in [Shaw 95].\n4.9  Discussion Questions\n1.\t\nWhat is the relationship between a use case and a quality attribute scenario? \nIf you wanted to add quality attribute information to a use case, how would \nyou do it?\n",
      "content_length": 1439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "78 \nPart Two  Quality Attributes\t\n4—Understanding Quality Attributes\n2.\t\nDo you suppose that the set of tactics for a quality attribute is finite or in-\nfinite? Why?\n3.\t\nDiscuss the choice of programming language (an example of choice of \ntechnology) and its relation to architecture in general, and the design \ndecisions in the other six categories? For instance, how can certain pro-\ngramming languages enable or inhibit the choice of particular coordination \nmodels?\n4.\t\nWe will be using the automatic teller machine as an example throughout \nthe chapters on quality attributes. Enumerate the set of responsibilities that \nan automatic teller machine should support and propose an initial design to \naccommodate that set of responsibilities. Justify your proposal.\n5.\t\nThink about the screens that your favorite automatic teller machine uses. \nWhat do those screens tell you about binding time decisions reflected in the \narchitecture?\n6.\t\nConsider the choice between synchronous and asynchronous communica-\ntion (a choice in the coordination mechanism category). What quality attri-\nbute requirements might lead you to choose one over the other?\n7.\t\nConsider the choice between stateful and stateless communication (a choice \nin the coordination mechanism category). What quality attribute require-\nments might lead you to choose one over the other?\n8.\t\nMost peer-to-peer architecture employs late binding of the topology. What \nquality attributes does this promote or inhibit?\n",
      "content_length": 1482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "79\n5\nAvailability\nWith James Scott\nNinety percent of life is just showing up.\n—Woody Allen\nAvailability refers to a property of software that it is there and ready to carry \nout its task when you need it to be. This is a broad perspective and encompasses \nwhat is normally called reliability (although it may encompass additional con-\nsiderations such as downtime due to periodic maintenance). In fact, availability \nbuilds upon the concept of reliability by adding the notion of recovery—that is, \nwhen the system breaks, it repairs itself. Repair may be accomplished by various \nmeans, which we’ll see in this chapter. More precisely, Avižienis and his col-\nleagues have defined dependability:\nDependability is the ability to avoid failures that are more frequent and \nmore severe than is acceptable.\nOur definition of availability as an aspect of dependability is this: “Availabil-\nity refers to the ability of a system to mask or repair faults such that the cumula-\ntive service outage period does not exceed a required value over a specified time \ninterval.” These definitions make the concept of failure subject to the judgment of \nan external agent, possibly a human. They also subsume concepts of reliability, \nconfidentiality, integrity, and any other quality attribute that involves a concept of \nunacceptable failure. \nAvailability is closely related to security. A denial-of-service attack is ex-\nplicitly designed to make a system fail—that is, to make it unavailable. Availabil-\nity is also closely related to performance, because it may be difficult to tell when \na system has failed and when it is simply being outrageously slow to respond. \nFinally, availability is closely allied with safety, which is concerned with keeping \n",
      "content_length": 1744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "80 \nPart Two  Quality Attributes\t\n5—Availability\nthe system from entering a hazardous state and recovering or limiting the damage \nwhen it does.\nFundamentally, availability is about minimizing service outage time by mit-\nigating faults. Failure implies visibility to a system or human observer in the en-\nvironment. That is, a failure is the deviation of the system from its specification, \nwhere the deviation is externally visible. One of the most demanding tasks in \nbuilding a high-availability, fault-tolerant system is to understand the nature of \nthe failures that can arise during operation (see the sidebar “Planning for Fail-\nure”). Once those are understood, mitigation strategies can be designed into the \nsoftware.\nA failure’s cause is called a fault. A fault can be either internal or external to \nthe system under consideration. Intermediate states between the occurrence of a \nfault and the occurrence of a failure are called errors. Faults can be prevented, tol-\nerated, removed, or forecast. In this way a system becomes “resilient” to faults.\nAmong the areas with which we are concerned are how system faults are \ndetected, how frequently system faults may occur, what happens when a fault \noccurs, how long a system is allowed to be out of operation, when faults or fail-\nures may occur safely, how faults or failures can be prevented, and what kinds of \nnotifications are required when a failure occurs. \nBecause a system failure is observable by users, the time to repair is the time \nuntil the failure is no longer observable. This may be a brief delay in the response \ntime or it may be the time it takes someone to fly to a remote location in the An-\ndes to repair a piece of mining machinery (as was recounted to us by a person \nresponsible for repairing the software in a mining machine engine). The notion \nof “observability” can be a tricky one: the Stuxnet virus, as an example, went un-\nobserved for a very long time even though it was doing damage. In addition, we \nare often concerned with the level of capability that remains when a failure has \noccurred—a degraded operating mode.\nThe distinction between faults and failures allows discussion of automatic \nrepair strategies. That is, if code containing a fault is executed but the system is \nable to recover from the fault without any deviation from specified behavior be-\ning observable, there is no failure. \nThe availability of a system can be calculated as the probability that it will \nprovide the specified services within required bounds over a specified time inter-\nval. When referring to hardware, there is a well-known expression used to derive \nsteady-state availability:\nMTBF\n(MTBF + MTTR)\nwhere MTBF refers to the mean time between failures and MTTR refers to the \nmean time to repair. In the software world, this formula should be interpreted \nto mean that when thinking about availability, you should think about what will \nmake your system fail, how likely that is to occur, and that there will be some \ntime required to repair it.\n",
      "content_length": 3034,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "\t\n5—Availability  81\nFrom this formula it is possible to calculate probabilities and make claims \nlike “99.999 percent availability,” or a 0.001 percent probability that the system \nwill not be operational when needed. Scheduled downtimes (when the system is \nintentionally taken out of service) may not be considered when calculating avail-\nability, because the system is deemed “not needed” then; of course, this depends \non the specific requirements for the system, often encoded in service-level agree-\nments (SLAs). This arrangement may lead to seemingly odd situations where the \nsystem is down and users are waiting for it, but the downtime is scheduled and so \nis not counted against any availability requirements. \nIn operational systems, faults are detected and correlated prior to being re-\nported and repaired. Fault correlation logic will categorize a fault according to \nits severity (critical, major, or minor) and service impact (service-affecting or \nnon-service-affecting) in order to provide the system operator with timely and ac-\ncurate system status and allow for the appropriate repair strategy to be employed. \nThe repair strategy may be automated or may require manual intervention.\nThe availability provided by a computer system or hosting service is fre-\nquently expressed as a service-level agreement. This SLA specifies the availabil-\nity level that is guaranteed and, usually, the penalties that the computer system or \nhosting service will suffer if the SLA is violated. The SLA that Amazon provides \nfor its EC2 cloud service is\nAWS will use commercially reasonable efforts to make Amazon EC2 \navailable with an Annual Uptime Percentage [defined elsewhere] of at \nleast 99.95% during the Service Year. In the event Amazon EC2 does \nnot meet the Annual Uptime Percentage commitment, you will be \neligible to receive a Service Credit as described below.\nTable 5.1 provides examples of system availability requirements and associated \nthreshold values for acceptable system downtime, measured over observation pe-\nriods of 90 days and one year. The term high availability typically refers to de-\nsigns targeting availability of 99.999 percent (“5 nines”) or greater. By definition \nor convention, only unscheduled outages contribute to system downtime.\nTable 5.1  System Availability Requirements\nAvailability\nDowntime/90 Days\nDowntime/Year\n99.0%\n21 hours, 36 minutes\n3 days, 15.6 hours\n99.9%\n2 hours, 10 minutes\n8 hours, 0 minutes, 46 seconds\n99.99%\n12 minutes, 58 seconds\n52 minutes, 34 seconds\n99.999%\n1 minute, 18 seconds\n5 minutes, 15 seconds\n99.9999%\n8 seconds\n32 seconds\n",
      "content_length": 2607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "82 \nPart Two  Quality Attributes\t\n5—Availability\nPlanning for Failure\nWhen designing a high-availability or safety-critical system, it’s tempting to \nsay that failure is not an option. It’s a catchy phrase, but it’s a lousy design \nphilosophy. In fact, failure is not only an option, it’s almost inevitable. What \nwill make your system safe and available is planning for the occurrence of \nfailure or (more likely) failures, and handling them with aplomb. The first \nstep is to understand what kinds of failures your system is prone to, and \nwhat the consequences of each will be. Here are three well-known tech-\nniques for getting a handle on this.\nHazard analysis\nHazard analysis is a technique that attempts to catalog the hazards that \ncan occur during the operation of a system. It categorizes each hazard \naccording to its severity. For example, the DO-178B standard used in the \naeronautics industry defines these failure condition levels in terms of their \neffects on the aircraft, crew, and passengers:\n■\n■\nCatastrophic. This kind of failure may cause a crash. This failure represents \nthe loss of critical function required to safely fly and land aircraft.\n■\n■\nHazardous. This kind of failure has a large negative impact on safety or \nperformance, or reduces the ability of the crew to operate the aircraft due \nto physical distress or a higher workload, or causes serious or fatal injuries \namong the passengers. \n■\n■\nMajor. This kind of failure is significant, but has a lesser impact than a \nHazardous failure (for example, leads to passenger discomfort rather than \ninjuries) or significantly increases crew workload to the point where safety \nis affected.\n■\n■\nMinor. This kind of failure is noticeable, but has a lesser impact than a Ma-\njor failure (for example, causing passenger inconvenience or a routine flight \nplan change).\n■\n■\nNo effect. This kind of failure has no impact on safety, aircraft operation, or \ncrew workload.\nOther domains have their own categories and definitions. Hazard anal-\nysis also assesses the probability of each hazard occurring. Hazards for \nwhich the product of cost and probability exceed some threshold are then \nmade the subject of mitigation activities.\nFault tree analysis\nFault tree analysis is an analytical technique that specifies a state of the \nsystem that negatively impacts safety or reliability, and then analyzes the \nsystem’s context and operation to find all the ways that the undesired state \ncould occur. The technique uses a graphic construct (the fault tree) that \nhelps identify all sequential and parallel sequences of contributing faults \nthat will result in the occurrence of the undesired state, which is listed at \nthe top of the tree (the “top event”). The contributing faults might be hard-\nware failures, human errors, software errors, or any other pertinent events \nthat can lead to the undesired state. \n",
      "content_length": 2885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Part Two  Quality Attributes\t\n5—Availability  83\nFigure 5.1, taken from a NASA handbook on fault tree analysis, shows \na very simple fault tree for which the top event is failure of component D. It \nshows that component D can fail if A fails and either B or C fails.\nThe symbols that connect the events in a fault tree are called gate symbols, \nand are taken from Boolean logic diagrams. Figure 5.2 illustrates the notation.\nA fault tree lends itself to static analysis in various ways. For example, a \n“minimal cut set” is the smallest combination of events along the bottom of \nthe tree that together can cause the top event. The set of minimal cut sets \nshows all the ways the bottom events can combine to cause the overarch-\ning failure. Any singleton minimal cut set reveals a single point of failure, \nwhich should be carefully scrutinized. Also, the probabilities of various con-\ntributing failures can be combined to come up with a probability of the top \nevent occurring. Dynamic analysis occurs when the order of contributing \nfailures matters. In this case, techniques such as Markov analysis can be \nused to calculate probability of failure over different failure sequences. \nFault trees aid in system design, but they can also be used to diagnose \nfailures at runtime. If the top event has occurred, then (assuming the fault \ntree model is complete) one or more of the contributing failures has oc-\ncurred, and the fault tree can be used to track it down and initiate repairs.\nFailure Mode, Effects, and Criticality Analysis (FMECA) catalogs the \nkinds of failures that systems of a given type are prone to, along with how \nsevere the effects of each one can be. FMECA relies on the history of\nD Fails\nA Fails\nB or C Fail\nB Fails\nC Fails\nG1\nA\nG2\nC\nB\nFigure 5.1  A simple fault tree. D fails if A fails and either B or C fails.\n",
      "content_length": 1840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "84 \nPart Two  Quality Attributes\t\n5—Availability\nfailure of similar systems in the past. Table 5.2, also taken from the NASA \nhandbook, shows the data for a system of redundant amplifiers. Historical \ndata shows that amplifiers fail most often when there is a short circuit or \nthe circuit is left open, but there are several other failure modes as well \n(lumped together as “Other”).\nn\n \nGATE SYMBOLS\nAND   Output fault occurs if all of the input faults occur\nOR   Output fault occurs if a least one of the input faults occurs\nCOMBINATION   Output fault occurs if n of the input faults occur\nEXCLUSIVE OR   Output fault occurs if exactly one of the input \nfaults occurs\nPRIORITY AND   Output fault occurs if all of the input faults occur in a \nspecific sequence (the sequence is represented by a CONDITIONING \nEVENT drawn to the right of the gate)\nINHIBIT   Output fault occurs if the (single) input fault occurs in the \npresence of an enabling condition (the enabling condition is represented \nby a CONDITIONING EVENT drawn to the right of the gate)\nFigure 5.2  Fault tree gate symbols\nTable 5.2  Failure Probabilities and Effects\n \nComponent\nFailure \nProbability\nFailure \nMode\n% Failures \nby Mode\nEffects\nCritical\nNoncritical\nA\n1 × 10–3\nOpen\n90\nX\nShort\n5\nX (5 × 10–5)\nOther\n5\nX (5 × 10–5)\nB\n1 × 10–3\nOpen\n90\nX\nShort\n5\nX (5 × 10–5)\nOther\n5\nX (5 × 10–5)\n",
      "content_length": 1355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "5.1  Availability General Scenario\n85\nAdding up the critical column gives us the probability of a critical system \nfailure: 5 × 10–5 + 5 × 10–5 + 5 × 10–5 + 5 × 10–5 = 2 × 10–4. \nThese techniques, and others, are only as good as the knowledge and \nexperience of the people who populate their respective data structures. \nOne of the worst mistakes you can make, according to the NASA hand-\nbook, is to let form take priority over substance. That is, don’t let safety \nengineering become a matter of just filling out the tables. Instead, keep \npressing to find out what else can go wrong, and then plan for it.\n5.1  Availability General Scenario\nFrom these considerations we can now describe the individual portions of an \navailability general scenario. These are summarized in Table 5.3: \n■\n■Source of stimulus. We differentiate between internal and external origins of \nfaults or failure because the desired system response may be different. \n■\n■Stimulus. A fault of one of the following classes occurs: \n■\n■Omission. A component fails to respond to an input.\n■\n■Crash. The component repeatedly suffers omission faults.\n■\n■Timing. A component responds but the response is early or late.\n■\n■Response. A component responds with an incorrect value.\n■\n■Artifact. This specifies the resource that is required to be highly available, \nsuch as a processor, communication channel, process, or storage.\n■\n■Environment. The state of the system when the fault or failure occurs may \nalso affect the desired system response. For example, if the system has al-\nready seen some faults and is operating in other than normal mode, it may \nbe desirable to shut it down totally. However, if this is the first fault ob-\nserved, some degradation of response time or function may be preferred. \n■\n■Response. There are a number of possible reactions to a system fault. \nFirst, the fault must be detected and isolated (correlated) before any other \nresponse is possible. (One exception to this is when the fault is prevented \nbefore it occurs.) After the fault is detected, the system must recover from \nit. Actions associated with these possibilities include logging the failure, \nnotifying selected users or other systems, taking actions to limit the damage \ncaused by the fault, switching to a degraded mode with either less capacity \nor less function, shutting down external systems, or becoming unavailable \nduring repair.\n■\n■Response measure. The response measure can specify an availability per-\ncentage, or it can specify a time to detect the fault, time to repair the fault, \ntimes or time intervals during which the system must be available, or the \nduration for which the system must be available.\n",
      "content_length": 2686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "86 \nPart Two  Quality Attributes\t\n5—Availability\nFigure 5.3 shows a concrete scenario generated from the general scenario: The \nheartbeat monitor determines that the server is nonresponsive during normal opera-\ntions. The system informs the operator and continues to operate with no downtime.\nTable 5.3  Availability General Scenario \nPortion of \nScenario\nPossible Values\nSource\nInternal/external: people, hardware, software, physical infrastructure, \nphysical environment\nStimulus\nFault: omission, crash, incorrect timing, incorrect response\nArtifact\nProcessors, communication channels, persistent storage, processes\nEnvironment Normal operation, startup, shutdown, repair mode, degraded operation, \noverloaded operation\nResponse\nPrevent the fault from becoming a failure\nDetect the fault:\n■\n■\nLog the fault\n■\n■\n\u0007Notify appropriate entities (people or systems)\nRecover from the fault:\n■\n■\nDisable source of events causing the fault\n■\n■\nBe temporarily unavailable while repair is being effected\n■\n■\nFix or mask the fault/failure or contain the damage it causes\n■\n■\nOperate in a degraded mode while repair is being effected\nResponse \nMeasure\nTime or time interval when the system must be available\nAvailability percentage (e.g., 99.999%)\nTime to detect the fault\nTime to repair the fault\nTime or time interval in which system can be in degraded mode\nProportion (e.g., 99%) or rate (e.g., up to 100 per second) of a certain \nclass of faults that the system prevents, or handles without failing\nStimulus:\nServer\nUnresponsive\nResponse:\nInform \nOperator\nContinue\nto Operate\nResponse \nMeasure:\nNo Downtime\nSource:\nHeartbeat\nMonitor\nArtifact:\nProcess\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nFigure 5.3  Sample concrete availability scenario\n",
      "content_length": 1732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "5.2  Tactics for Availability\n87\n5.2  Tactics for Availability\nA failure occurs when the system no longer delivers a service that is consistent \nwith its specification; this failure is observable by the system’s actors. A fault \n(or combination of faults) has the potential to cause a failure. Availability tac-\ntics, therefore, are designed to enable a system to endure system faults so that a \nservice being delivered by the system remains compliant with its specification. \nThe tactics we discuss in this section will keep faults from becoming failures or \nat least bound the effects of the fault and make repair possible. We illustrate this \napproach in Figure 5.4.\nAvailability tactics may be categorized as addressing one of three catego-\nries: fault detection, fault recovery, and fault prevention. The tactics categoriza-\ntion for availability is shown in Figure 5.5 (on the next page). Note that it is often \nthe case that these tactics will be provided for you by a software infrastructure, \nsuch as a middleware package, so your job as an architect is often one of choos-\ning and assessing (rather than implementing) the right availability tactics and the \nright combination of tactics. \nFault\nFault Masked\nor Repair Made\nTactics\nto Control\nAvailability\nFigure 5.4  Goal of availability tactics\nDetect Faults\nBefore any system can take action regarding a fault, the presence of the fault \nmust be detected or anticipated. Tactics in this category include the following:\n■\n■Ping/echo refers to an asynchronous request/response message pair ex-\nchanged between nodes, used to determine reachability and the round-trip \ndelay through the associated network path. But the echo also determines \nthat the pinged component is alive and responding correctly. The ping is \n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "88 \nPart Two  Quality Attributes\t\n5—Availability\noften sent by a system monitor. Ping/echo requires a time threshold to be \nset; this threshold tells the pinging component how long to wait for the \necho before considering the pinged component to have failed (“timed out”). \nStandard implementations of ping/echo are available for nodes intercon-\nnected via IP.\n■\n■Monitor. A monitor is a component that is used to monitor the state of \nhealth of various other parts of the system: processors, processes, I/O, \nmemory, and so on. A system monitor can detect failure or congestion in \nthe network or other shared resources, such as from a denial-of-service \nattack. It orchestrates software using other tactics in this category to detect \nAvailability Tactics\nDetect Faults\nPrevent Faults\nPing / Echo\nRemoval from\nService\nMonitor\nTransactions\nPredictive\nModel\nRecover from Faults\nHeartbeat\nPreparation\nand Repair\nReintroduction\nActive\nRedundancy\nPassive\nRedundancy\nSpare\nEscalating\nRestart\nException\nHandling\nShadow\nNon-Stop\nForwarding\nState\nResynchronization\nException\nPrevention\nFault\nFault\nMasked\nor\nRepair\nMade\nTimestamp\nSanity\nChecking\nCondition\nMonitoring\nVoting\nException\nDetection\nSelf-Test\nRollback\nSoftware\nUpgrade\nRetry\nIgnore Faulty\nBehavior\nDegradation\nReconfiguration\nIncrease\nCompetence Set\nFigure 5.5  Availability tactics\n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "5.2  Tactics for Availability\n89\nmalfunctioning components. For example, the system monitor can initiate \nself-tests, or be the component that detects faulty time stamps or missed \nheartbeats.1\n■\n■Heartbeat is a fault detection mechanism that employs a periodic message \nexchange between a system monitor and a process being monitored. A \nspecial case of heartbeat is when the process being monitored periodically \nresets the watchdog timer in its monitor to prevent it from expiring and thus \nsignaling a fault. For systems where scalability is a concern, transport and \nprocessing overhead can be reduced by piggybacking heartbeat messages \non to other control messages being exchanged between the process being \nmonitored and the distributed system controller. The big difference between \nheartbeat and ping/echo is who holds the responsibility for initiating the \nhealth check—the monitor or the component itself.\n■\n■Time stamp. This tactic is used to detect incorrect sequences of events, pri-\nmarily in distributed message-passing systems. A time stamp of an event \ncan be established by assigning the state of a local clock to the event imme-\ndiately after the event occurs. Simple sequence numbers can also be used \nfor this purpose, if time information is not important.\n■\n■Sanity checking checks the validity or reasonableness of specific operations \nor outputs of a component. This tactic is typically based on a knowledge of \nthe internal design, the state of the system, or the nature of the information \nunder scrutiny. It is most often employed at interfaces, to examine a specific \ninformation flow. \n■\n■Condition monitoring involves checking conditions in a process or device, \nor validating assumptions made during the design. By monitoring condi-\ntions, this tactic prevents a system from producing faulty behavior. The \ncomputation of checksums is a common example of this tactic. However, \nthe monitor must itself be simple (and, ideally, provable) to ensure that it \ndoes not introduce new software errors. \n■\n■Voting. The most common realization of this tactic is referred to as triple \nmodular redundancy (TMR), which employs three components that do the \nsame thing, each of which receives identical inputs, and forwards their out-\nput to voting logic, used to detect any inconsistency among the three output \nstates. Faced with an inconsistency, the voter reports a fault. It must also \ndecide what output to use. It can let the majority rule, or choose some com-\nputed average of the disparate outputs. This tactic depends critically on the \nvoting logic, which is usually realized as a simple, rigorously reviewed and \ntested singleton so that the probability of error is low. \n1.  When the detection mechanism is implemented using a counter or timer that is periodically reset, \nthis specialization of system monitor is referred to as a “watchdog.” During nominal operation, the \nprocess being monitored will periodically reset the watchdog counter/timer as part of its signal that \nit’s working correctly; this is sometimes referred to as “petting the watchdog.”\n",
      "content_length": 3093,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "90 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Replication is the simplest form of voting; here, the components are exact \nclones of each other. Having multiple copies of identical components can \nbe effective in protecting against random failures of hardware, but this \ncannot protect against design or implementation errors, in hardware or \nsoftware, because there is no form of diversity embedded in this tactic. \n■\n■Functional redundancy is a form of voting intended to address the issue \nof common-mode failures (design or implementation faults) in hardware \nor software components. Here, the components must always give the \nsame output given the same input, but they are diversely designed and \ndiversely implemented. \n■\n■Analytic redundancy permits not only diversity among components’ pri-\nvate sides, but also diversity among the components’ inputs and outputs. \nThis tactic is intended to tolerate specification errors by using separate \nrequirement specifications. In embedded systems, analytic redundancy \nalso helps when some input sources are likely to be unavailable at times. \nFor example, avionics programs have multiple ways to compute aircraft \naltitude, such as using barometric pressure, the radar altimeter, and geo-\nmetrically using the straight-line distance and look-down angle of a point \nahead on the ground. The voter mechanism used with analytic redun-\ndancy needs to be more sophisticated than just letting majority rule or \ncomputing a simple average. It may have to understand which sensors are \ncurrently reliable or not, and it may be asked to produce a higher-fidelity \nvalue than any individual component can, by blending and smoothing \nindividual values over time. \n■\n■Exception detection refers to the detection of a system condition that alters \nthe normal flow of execution. The exception detection tactic can be further \nrefined:\n■\n■System exceptions will vary according to the processor hardware architec-\nture employed and include faults such as divide by zero, bus and address \nfaults, illegal program instructions, and so forth. \n■\n■The parameter fence tactic incorporates an a priori data pattern (such as \n0xDEADBEEF) placed immediately after any variable-length parameters \nof an object. This allows for runtime detection of overwriting the memory \nallocated for the object’s variable-length parameters. \n■\n■Parameter typing employs a base class that defines functions that add, \nfind, and iterate over type-length-value (TLV) formatted message param-\neters. Derived classes use the base class functions to implement functions \nthat provide parameter typing according to each parameter’s structure. \nUse of strong typing to build and parse messages results in higher avail-\nability than implementations that simply treat messages as byte buckets. \nOf course, all design involves tradeoffs. When you employ strong typing, \nyou typically trade higher availability against ease of evolution.\n",
      "content_length": 2943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "5.2  Tactics for Availability\n91\n■\n■Timeout is a tactic that raises an exception when a component detects \nthat it or another component has failed to meet its timing constraints. For \nexample, a component awaiting a response from another component can \nraise an exception if the wait time exceeds a certain value. \n■\n■Self-test. Components (or, more likely, whole subsystems) can run proce-\ndures to test themselves for correct operation. Self-test procedures can be \ninitiated by the component itself, or invoked from time to time by a system \nmonitor. These may involve employing some of the techniques found in \ncondition monitoring, such as checksums.\nRecover from Faults\nRecover-from-faults tactics are refined into preparation-and-repair tactics and \nreintroduction tactics. The latter are concerned with reintroducing a failed (but \nrehabilitated) component back into normal operation.\nPreparation-and-repair tactics are based on a variety of combinations of re-\ntrying a computation or introducing redundancy. They include the following:\n■\n■Active redundancy (hot spare). This refers to a configuration where all of \nthe nodes (active or redundant spare) in a protection group2 receive and \nprocess identical inputs in parallel, allowing the redundant spare(s) to main-\ntain synchronous state with the active node(s). Because the redundant spare \npossesses an identical state to the active processor, it can take over from a \nfailed component in a matter of milliseconds. The simple case of one active \nnode and one redundant spare node is commonly referred to as 1+1 (“one \nplus one”) redundancy. Active redundancy can also be used for facilities \nprotection, where active and standby network links are used to ensure high-\nly available network connectivity. \n■\n■Passive redundancy (warm spare). This refers to a configuration where \nonly the active members of the protection group process input traffic; \none of their duties is to provide the redundant spare(s) with periodic state \nupdates. Because the state maintained by the redundant spares is only \nloosely coupled with that of the active node(s) in the protection group \n(with the looseness of the coupling being a function of the checkpointing \nmechanism employed between active and redundant nodes), the redundant \nnodes are referred to as warm spares. Depending on a system’s availability \nrequirements, passive redundancy provides a solution that achieves a bal-\nance between the more highly available but more compute-intensive (and \nexpensive) active redundancy tactic and the less available but significantly \nless complex cold spare tactic (which is also significantly cheaper). (For an \n2.  A protection group is a group of processing nodes where one or more nodes are “active,” with the \nremaining nodes in the protection group serving as redundant spares.\n",
      "content_length": 2833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "92 \nPart Two  Quality Attributes\t\n5—Availability\nexample of implementing passive redundancy, see the section on code tem-\nplates in Chapter 19.) \n■\n■Spare (cold spare). Cold sparing refers to a configuration where the re-\ndundant spares of a protection group remain out of service until a fail-over \noccurs, at which point a power-on-reset procedure is initiated on the re-\ndundant spare prior to its being placed in service. Due to its poor recovery \nperformance, cold sparing is better suited for systems having only high-re-\nliability (MTBF) requirements as opposed to those also having high-avail-\nability requirements.\n■\n■Exception handling. Once an exception has been detected, the system must \nhandle it in some fashion. The easiest thing it can do is simply to crash, but \nof course that’s a terrible idea from the point of availability, usability, test-\nability, and plain good sense. There are much more productive possibilities. \nThe mechanism employed for exception handling depends largely on the \nprogramming environment employed, ranging from simple function return \ncodes (error codes) to the use of exception classes that contain information \nhelpful in fault correlation, such as the name of the exception thrown, the \norigin of the exception, and the cause of the exception thrown. Software \ncan then use this information to mask the fault, usually by correcting the \ncause of the exception and retrying the operation.\n■\n■Rollback. This tactic permits the system to revert to a previous known good \nstate, referred to as the “rollback line”—rolling back time—upon the detec-\ntion of a failure. Once the good state is reached, then execution can contin-\nue. This tactic is often combined with active or passive redundancy tactics \nso that after a rollback has occurred, a standby version of the failed compo-\nnent is promoted to active status. Rollback depends on a copy of a previous \ngood state (a checkpoint) being available to the components that are rolling \nback. Checkpoints can be stored in a fixed location and updated at regular \nintervals, or at convenient or significant times in the processing, such as at \nthe completion of a complex operation. \n■\n■Software upgrade is another preparation-and-repair tactic whose goal is to \nachieve in-service upgrades to executable code images in a non-service-af-\nfecting manner. This may be realized as a function patch, a class patch, \nor a hitless in-service software upgrade (ISSU). A function patch is used \nin procedural programming and employs an incremental linker/loader to \nstore an updated software function into a pre-allocated segment of target \nmemory. The new version of the software function will employ the entry \nand exit points of the deprecated function. Also, upon loading the new \nsoftware function, the symbol table must be updated and the instruction \ncache invalidated. The class patch tactic is applicable for targets executing \nobject-oriented code, where the class definitions include a back-door mech-\nanism that enables the runtime addition of member data and functions. Hit-\nless in-service software upgrade leverages the active redundancy or passive \n",
      "content_length": 3151,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "5.2  Tactics for Availability\n93\nredundancy tactics to achieve non-service-affecting upgrades to software \nand associated schema. In practice, the function patch and class patch are \nused to deliver bug fixes, while the hitless in-service software upgrade is \nused to deliver new features and capabilities.\n■\n■Retry. The retry tactic assumes that the fault that caused a failure is tran-\nsient and retrying the operation may lead to success. This tactic is used in \nnetworks and in server farms where failures are expected and common. \nThere should be a limit on the number of retries that are attempted before a \npermanent failure is declared.\n■\n■Ignore faulty behavior. This tactic calls for ignoring messages sent from a \nparticular source when we determine that those messages are spurious. For \nexample, we would like to ignore the messages of an external component \nlaunching a denial-of-service attack by establishing Access Control List \nfilters, for example.\n■\n■The degradation tactic maintains the most critical system functions in the \npresence of component failures, dropping less critical functions. This is \ndone in circumstances where individual component failures gracefully re-\nduce system functionality rather than causing a complete system failure. \n■\n■Reconfiguration attempts to recover from component failures by reassign-\ning responsibilities to the (potentially restricted) resources left functioning, \nwhile maintaining as much functionality as possible.\nReintroduction is where a failed component is reintroduced after it has been \ncorrected. Reintroduction tactics include the following:\n■\n■The shadow tactic refers to operating a previously failed or in-service up-\ngraded component in a “shadow mode” for a predefined duration of time \nprior to reverting the component back to an active role. During this duration \nits behavior can be monitored for correctness and it can repopulate its state \nincrementally.\n■\n■State resynchronization is a reintroduction partner to the active redun-\ndancy and passive redundancy preparation-and-repair tactics. When used \nalongside the active redundancy tactic, the state resynchronization occurs \norganically, because the active and standby components each receive and \nprocess identical inputs in parallel. In practice, the states of the active and \nstandby components are periodically compared to ensure synchronization. \nThis comparison may be based on a cyclic redundancy check calculation \n(checksum) ‎or, for systems providing safety-critical services, a message \ndigest calculation (a one-way hash function). When used alongside the pas-\nsive redundancy (warm spare) tactic, state resynchronization is based solely \non periodic state information transmitted from the active component(s) to \nthe standby component(s), typically via checkpointing. A special case of \nthis tactic is found in stateless services, whereby any resource can handle a \nrequest from another (failed) resource.\n",
      "content_length": 2956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "94 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Escalating restart is a reintroduction tactic that allows the system to recov-\ner from faults by varying the granularity of the component(s) restarted and \nminimizing the level of service affected. For example, consider a system \nthat supports four levels of restart, as follows. The lowest level of restart \n(call it Level 0), and hence having the least impact on services, employs \npassive redundancy (warm spare), where all child threads of the faulty \ncomponent are killed and recreated. In this way, only data associated with \nthe child threads is freed and reinitialized. The next level of restart (Level \n1) frees and reinitializes all unprotected memory (protected memory would \nremain untouched). The next level of restart (Level 2) frees and reinitializes \nall memory, both protected and unprotected, forcing all applications to re-\nload and reinitialize. And the final level of restart (Level 3) would involve \ncompletely reloading and reinitializing the executable image and associated \ndata segments. Support for the escalating restart tactic is particularly useful \nfor the concept of graceful degradation, where a system is able to degrade \nthe services it provides while maintaining support for mission-critical or \nsafety-critical applications.\n■\n■Non-stop forwarding (NSF) is a concept that originated in router design. In \nthis design functionality is split into two parts: supervisory, or control plane \n(which manages connectivity and routing information), and data plane \n(which does the actual work of routing packets from sender to receiver). If \na router experiences the failure of an active supervisor, it can continue for-\nwarding packets along known routes—with neighboring routers—while the \nrouting protocol information is recovered and validated. When the control \nplane is restarted, it implements what is sometimes called “graceful restart,” \nincrementally rebuilding its routing protocol database even as the data \nplane continues to operate.\nPrevent Faults\nInstead of detecting faults and then trying to recover from them, what if your sys-\ntem could prevent them from occurring in the first place? Although this sounds \nlike some measure of clairvoyance might be required, it turns out that in many \ncases it is possible to do just that.3\n■\n■Removal from service. This tactic refers to temporarily placing a system \ncomponent in an out-of-service state for the purpose of mitigating potential \nsystem failures. One example involves taking a component of a system out \nof service and resetting the component in order to scrub latent faults (such \n3.  These tactics deal with runtime means to prevent faults from occurring. Of course, an excellent \nway to prevent faults—at least in the system you’re building, if not in systems that your system must \ninteract with—is to produce high-quality code. This can be done by means of code inspections, pair \nprogramming, solid requirements reviews, and a host of other good engineering practices. \n",
      "content_length": 3028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "5.2  Tactics for Availability\n95\nas memory leaks, fragmentation, or soft errors in an unprotected cache) be-\nfore the accumulation of faults affects service (resulting in system failure). \nAnother term for this tactic is software rejuvenation.\n■\n■Transactions. Systems targeting high-availability services leverage transac-\ntional semantics to ensure that asynchronous messages exchanged between \ndistributed components are atomic, consistent, isolated, and durable. These \nfour properties are called the “ACID properties.” The most common realiza-\ntion of the transactions tactic is “two-phase commit” (a.k.a. 2PC) protocol. \nThis tactic prevents race conditions caused by two processes attempting to \nupdate the same data item.\n■\n■Predictive model. A predictive model, when combined with a monitor, is \nemployed to monitor the state of health of a system process to ensure that \nthe system is operating within its nominal operating parameters, and to take \ncorrective action when conditions are detected that are predictive of likely \nfuture faults. The operational performance metrics monitored are used to \npredict the onset of faults; examples include session establishment rate (in \nan HTTP server), threshold crossing (monitoring high and low water marks \nfor some constrained, shared resource), or maintaining statistics for process \nstate (in service, out of service, under maintenance, idle), message queue \nlength statistics, and so on.\n■\n■Exception prevention. This tactic refers to techniques employed for the pur-\npose of preventing system exceptions from occurring. The use of exception \nclasses, which allows a system to transparently recover from system excep-\ntions, was discussed previously. Other examples of exception prevention \ninclude abstract data types, such as smart pointers, and the use of wrappers \nto prevent faults, such as dangling pointers and semaphore access violations \nfrom occurring. Smart pointers prevent exceptions by doing bounds check-\ning on pointers, and by ensuring that resources are automatically deallocat-\ned when no data refers to it. In this way resource leaks are avoided.\n■\n■Increase competence set. A program’s competence set is the set of states in \nwhich it is “competent” to operate. For example, the state when the denom-\ninator is zero is outside the competence set of most divide programs. When \na component raises an exception, it is signaling that it has discovered itself \nto be outside its competence set; in essence, it doesn’t know what to do and \nis throwing in the towel. Increasing a component’s competence set means \ndesigning it to handle more cases—faults—as part of its normal operation. \nFor example, a component that assumes it has access to a shared resource \nmight throw an exception if it discovers that access is blocked. Another \ncomponent might simply wait for access, or return immediately with an \nindication that it will complete its operation on its own the next time it does \nhave access. In this example, the second component has a larger compe-\ntence set than the first. \n",
      "content_length": 3062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "96 \nPart Two  Quality Attributes\t\n5—Availability\n5.3  A Design Checklist for Availability\nTable 5.4 is a checklist to support the design and analysis process for availability.\nTable 5.4  Checklist to Support the Design and Analysis Process for \nAvailability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine the system responsibilities that need to be highly \navailable. Within those responsibilities, ensure that additional \nresponsibilities have been allocated to detect an omission, \ncrash, incorrect timing, or incorrect response. Additionally, \nensure that there are responsibilities to do the following:\n■\n■\nLog the fault\n■\n■\nNotify appropriate entities (people or systems)\n■\n■\nDisable the source of events causing the fault\n■\n■\nBe temporarily unavailable\n■\n■\nFix or mask the fault/failure\n■\n■\nOperate in a degraded mode\nCoordination Model\nDetermine the system responsibilities that need to be highly \navailable. With respect to those responsibilities, do the \nfollowing: \n■\n■\nEnsure that coordination mechanisms can detect an \nomission, crash, incorrect timing, or incorrect response. \nConsider, for example, whether guaranteed delivery is \nnecessary. Will the coordination work under conditions of \ndegraded communication?\n■\n■\nEnsure that coordination mechanisms enable the logging \nof the fault, notification of appropriate entities, disabling of \nthe source of the events causing the fault, fixing or masking \nthe fault, or operating in a degraded mode.\n■\n■\nEnsure that the coordination model supports the replace-\nment of the artifacts used (processors, communications \nchannels, persistent storage, and processes). For exam-\nple, does replacement of a server allow the system to \ncontinue to operate? \n■\n■\nDetermine if the coordination will work under conditions \nof degraded communication, at startup/shutdown, in re-\npair mode, or under overloaded operation. For example, \nhow much lost information can the coordination model \nwithstand and with what consequences?\nData Model\nDetermine which portions of the system need to be highly \navailable. Within those portions, determine which data \nabstractions, along with their operations or their properties, \ncould cause a fault of omission, a crash, incorrect timing \nbehavior, or an incorrect response.\nFor those data abstractions, operations, and properties, \nensure that they can be disabled, be temporarily unavailable, \nor be fixed or masked in the event of a fault.\nFor example, ensure that write requests are cached if a \nserver is temporarily unavailable and performed when the \nserver is returned to service.\n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "5.3  A Design Checklist for Availability\n97\nCategory\nChecklist\nMapping among \nArchitectural Elements\nDetermine which artifacts (processors, communication \nchannels, persistent storage, or processes) may produce \na fault: omission, crash, incorrect timing, or incorrect \nresponse. \nEnsure that the mapping (or remapping) of architectural \nelements is flexible enough to permit the recovery from the \nfault. This may involve a consideration of the following:\n■\n■\nWhich processes on failed processors need to be reas-\nsigned at runtime \n■\n■\nWhich processors, data stores, or communication chan-\nnels can be activated or reassigned at runtime\n■\n■\nHow data on failed processors or storage can be served \nby replacement units\n■\n■\nHow quickly the system can be reinstalled based on the \nunits of delivery provided\n■\n■\nHow to (re)assign runtime elements to processors, com-\nmunication channels, and data stores\n■\n■\nWhen employing tactics that depend on redundancy of \nfunctionality, the mapping from modules to redundant \ncomponents is important. For example, it is possible to \nwrite one module that contains code appropriate for both \nthe active component and backup components in a pro-\ntection group. \nResource  \nManagement\nDetermine what critical resources are necessary to \ncontinue operating in the presence of a fault: omission, \ncrash, incorrect timing, or incorrect response. Ensure \nthere are sufficient remaining resources in the event of a \nfault to log the fault; notify appropriate entities (people or \nsystems); disable the source of events causing the fault; \nbe temporarily unavailable; fix or mask the fault/failure; \noperate normally, in startup, shutdown, repair mode, \ndegraded operation, and overloaded operation.\nDetermine the availability time for critical resources, what \ncritical resources must be available during specified time \nintervals, time intervals during which the critical resources \nmay be in a degraded mode, and repair time for critical \nresources. Ensure that the critical resources are available \nduring these time intervals.\nFor example, ensure that input queues are large enough \nto buffer anticipated messages if a server fails so that the \nmessages are not permanently lost.\ncontinues\n",
      "content_length": 2224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "98 \nPart Two  Quality Attributes\t\n5—Availability\nTable 5.4  Checklist to Support the Design and Analysis Process for \nAvailability, continued\nCategory\nChecklist\nBinding Time\nDetermine how and when architectural elements are bound. \nIf late binding is used to alternate between components \nthat can themselves be sources of faults (e.g., processes, \nprocessors, communication channels), ensure the chosen \navailability strategy is sufficient to cover faults introduced by \nall sources. For example:\n■\n■\nIf late binding is used to switch between artifacts such \nas processors that will receive or be the subject of faults, \nwill the chosen fault detection and recovery mechanisms \nwork for all possible bindings?\n■\n■\nIf late binding is used to change the definition or toler-\nance of what constitutes a fault (e.g., how long a process \ncan go without responding before a fault is assumed), \nis the recovery strategy chosen sufficient to handle all \ncases? For example, if a fault is flagged after 0.1 millisec-\nonds, but the recovery mechanism takes 1.5 seconds to \nwork, that might be an unacceptable mismatch.\n■\n■\nWhat are the availability characteristics of the late bind-\ning mechanism itself? Can it fail?\nChoice of Technology\nDetermine the available technologies that can (help) detect \nfaults, recover from faults, or reintroduce failed components. \nDetermine what technologies are available that help the \nresponse to a fault (e.g., event loggers). \nDetermine the availability characteristics of chosen \ntechnologies themselves: What faults can they recover \nfrom? What faults might they introduce into the system? \n5.4  Summary\nAvailability refers to the ability of the system to be available for use, especially \nafter a fault occurs. The fault must be recognized (or prevented) and then the \nsystem must respond in some fashion. The response desired will depend on the \ncriticality of the application and the type of fault and can range from “ignore it” \nto “keep on going as if it didn’t occur.”\nTactics for availability are categorized into detect faults, recover from faults \nand prevent faults. Detection tactics depend, essentially, on detecting signs of life \nfrom various components. Recovery tactics are some combination of retrying an \noperation or maintaining redundant data or computations. Prevention tactics de-\npend either on removing elements from service or utilizing mechanisms to limit \nthe scope of faults.\n",
      "content_length": 2435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "5.5  For Further Reading\n99\nAll of the availability tactics involve the coordination model because the \ncoordination model must be aware of faults that occur to generate an appropriate \nresponse. \n5.5  For Further Reading\nPatterns for availability:\n■\n■You can find patterns for fault tolerance in [Hanmer 07]. \nTactics for availability, overall:\n■\n■A more detailed discussion of some of the availability tactics in this chapter is \ngiven in [Scott 09]. This is the source of much of the material in this chapter.\n■\n■The Internet Engineering Task Force has promulgated a number of stan-\ndards supporting availability tactics. These standards include non-stop for-\nwarding [IETF 04], ping/echo ICMPv6 [IETF 06b], echo request/response), \nand MPLS (LSP Ping) networks [IETF 06a].\nTactics for availability, fault detection:\n■\n■The parameter fence tactic was first used (to our knowledge) in the Control \nData Series computers of the late 1960s. \n■\n■Triple modular redundancy (TMR), part of the voting tactic, was developed \nin the early 1960s by Lyons [Lyons 62].\n■\n■The fault detection tactic of voting is based on the fundamental contribu-\ntions to automata theory by Von Neumann, who demonstrated how systems \nhaving a prescribed reliability could be built from unreliable components \n[Von Neumann 56]. \nTactics for availability, fault recovery:\n■\n■Standards-based realizations of active redundancy exist for protecting net-\nwork links (i.e., facilities) at both the physical layer [Bellcore 99, Telcordia \n00] and the network/link layer [IETF 05].\n■\n■Exception handlinghas been written about by [Powel Douglass 99]. Soft-\nware can then use this information to mask the fault, usually by correcting \nthe cause of the exception and retrying the operation.\n■\n■[Morelos-Zaragoza 06] and [Schneier 96] have written about the compari-\nson of state during resynchronization. \n■\n■Some examples of how a system can degrade through use (degradation) are \ngiven in [Nygard 07].\n■\n■[Utas 05] has written about escalating restart. \n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "100 \nPart Two  Quality Attributes\t\n5—Availability\n■\n■Mountains of papers have been written about parameter typing, but [Utas \n05] writes about it in the context of availability (as opposed to bug preven-\ntion, its usual context).\n■\n■Hardware engineers often use preparation-and-repair tactics. Examples in-\nclude error detection and correction (EDAC) coding, forward error correction \n(FEC), and temporal redundancy. EDAC coding is typically used to protect \ncontrol memory structures in high-availability distributed real-time embedded \nsystems [Hamming 80]. Conversely, FEC coding is typically employed to \nrecover from physical-layer errors occurring on external network links More-\nlos-Zaragoza 06]. Temporal redundancy involves sampling spatially redundant \nclock or data lines at time intervals that exceed the pulse width of any transient \npulse to be tolerated, and then voting out any defects detected [Mavis 02].\nTactics for availability, fault prevention:\n■\n■Parnas and Madey have written about increasing an element’s competence \nset [Parnas 95].\n■\n■The ACID properties, important in the transactions tactic, were introduced \nby Gray in the 1970s and discussed in depth in [Gray 93]. \nAnalysis:\n■\n■Fault tree analysis dates from the early 1960s, but the granddaddy of re-\nsources for it is the U.S. Nuclear Regulatory Commission’s “Fault Tree \nHandbook,” published in 1981 [Vesely 81]. NASA’s 2002 “Fault Tree \nHandbook with Aerospace Applications” [Vesely 02] is an updated compre-\nhensive primer of the NRC handbook, and the source for the notation used \nin this chapter. Both are available online as downloadable PDF files. \n5.6  Discussion Questions\n1.\t\nWrite a set of concrete scenarios for availability using each of the possible \nresponses in the general scenario.\n2.\t\nWrite a concrete availability scenario for the software for a (hypothetical) \npilotless passenger aircraft.\n3.\t\nWrite a concrete availability scenario for a program like Microsoft Word.\n4.\t\nRedundancy is often cited as a key strategy for achieving high availability. \nLook at the tactics presented in this chapter and decide how many of them \nexploit some form of redundancy and how many do not.\n5.\t\nHow does availability trade off against modifiability? How would you make \na change to a system that is required to have “24/7” availability (no sched-\nuled or unscheduled downtime, ever)?\n",
      "content_length": 2377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "5.6  Discussion Questions\n101\n6.\t\nCreate a fault tree for an automatic teller machine. Include faults dealing \nwith hardware component failure, communications failure, software failure, \nrunning out of supplies, user errors, and security attacks. How would you \nmodify your automatic teller machine design to accommodate these faults?\n7.\t\nConsider the fault detection tactics (ping/echo, heartbeat, system monitor, \nvoting, and exception detection). What are the performance implications of \nusing these tactics?\n",
      "content_length": 513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "103\n6\nInteroperability\nWith Liming Zhu\nThe early bird (A) arrives and catches worm (B), pulling \nstring (C) and shooting off pistol (D). Bullet (E) bursts \nballoon (F), dropping brick (G) on bulb (H) of atomizer \n(I) and shooting perfume (J) on sponge (K). As sponge \ngains in weight, it lowers itself and pulls string (L), \nraising end of board (M). Cannon ball (N) drops on nose \nof sleeping gentleman. String tied to cannon ball releases \ncork (O) of vacuum bottle (P) and ice water falls on \nsleeper’s face to assist the cannon ball in its good work.\n—Rube Goldberg, instructions for “a simple alarm clock”\nInteroperability is about the degree to which two or more systems can usefully \nexchange meaningful information via interfaces in a particular context. The defi-\nnition includes not only having the ability to exchange data (syntactic interoper-\nability) but also having the ability to correctly interpret the data being exchanged \n(semantic interoperability). A system cannot be interoperable in isolation. Any \ndiscussion of a system’s interoperability needs to identify with whom, with what, \nand under what circumstances—hence, the need to include the context.\nInteroperability is affected by the systems expected to interoperate. If we \nalready know the interfaces of external systems with which our system will in-\nteroperate, then we can design that knowledge into the system. Or we can design \nour system to interoperate in a more generic fashion, so that the identity and the \nservices that another system provides can be bound later in the life cycle, at build \ntime or runtime.\nLike all quality attributes, interoperability is not a yes-or-no proposition but \nhas shades of meaning. There are several characterizing frameworks for interop-\nerability, all of which seem to define five levels of interoperability “maturity” \n(see the “For Further Reading” section at the end of this chapter for a pointer). \nThe lowest level signifies systems that do not share data at all, or do not do so \n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "104 \nPart Two  Quality Attributes\t\n6—Interoperability\nwith any success. The highest level signifies systems that work together seam-\nlessly, never make any mistakes interpreting each other’s communications, and \nshare the same underlying semantic model of the world in which they work. \n“Exchanging Information via Interfaces”\nInteroperability, as we said, is about two or more systems exchanging \ninformation via interfaces. \nAt this point, we need to clarify two critical concepts central to this dis-\ncussion and emphasize that we are taking a broad view of each.\nThe first is what it means to “exchange information.” This can mean \nsomething as simple as program A calling program B with some param-\neters. However, two systems (or parts of a system) can exchange infor-\nmation even if they never communicate directly with each other. Did you \never have a conversation like the following in junior high school? “Charlene \nsaid that Kim told her that Trevor heard that Heather wants to come to \nyour party.” Of course, junior high school protocol would preclude the \npossibility of responding directly to Heather. Instead, your response (if you \nlike Heather) might be, “Cool,” which would make its way back through \nCharlene, Kim, and Trevor. You and Heather exchanged information, but \nnever talked to each other. (We hope you got to talk to each other at the \nparty.)\nEntities can exchange information in even less direct ways. If I have an \nidea of a program’s behavior, and I design my program to work assuming \nthat behavior, the two programs have also exchanged information—just not \nat runtime.\nOne of the more infamous software disasters in history occurred when \nan antimissile system failed to intercept an incoming ballistic rocket in \nOperation Desert Storm in 1991, resulting in 28 fatalities. One of the mis-\nsile’s software components “expected” to be shut down and restarted peri-\nodically, so it could recalibrate its orientation framework from a known initial \npoint. The software had been running for some 100 hours when the missile \nwas launched, and calculation errors had accumulated to the point where \nthe software component’s idea of its orientation had wandered hopelessly \naway from truth.\nSystems (or components within systems) often have or embody ex-\npectations about the behaviors of its “information exchange” partners. \nThe assumption of everything interacting with the errant component in the \npreceding example was that its accuracy did not degrade over time. The \nresult was a system of parts that did not work together correctly to solve \nthe problem they were supposed to.\nThe second concept we need to stress is what we mean by “interface.” \nOnce again, we mean something beyond the simple case—a syntactic \ndescription of a component’s programs and the type and number of their \nparameters, most commonly realized as an API. That’s necessary for \n",
      "content_length": 2891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "\t\n6—Interoperability  105\ninteroperability—heck, it’s necessary if you want your software to compile \nsuccessfully—but it’s not sufficient. To illustrate this concept, we’ll use an-\nother “conversation” analogy. Has your partner or spouse ever come home, \nslammed the door, and when you ask what’s wrong, replied “Nothing!”? \nIf so, then you should be able to appreciate the keen difference between \nsyntax and semantics and the role of expectations in understanding how an \nentity behaves. Because we want interoperable systems and components, \nand not simply ones that compile together nicely, we require a higher bar \nfor interfaces than just a statement of syntax. By “interface,” we mean the \nset of assumptions that you can safely make about an entity. For example, \nit’s a safe assumption that whatever’s wrong with your spouse/partner, \nit’s not “Nothing,” and you know that because that “interface” extends way \nbeyond just the words they say. And it’s also a safe assumption that nothing \nabout our missile component’s accuracy degradation over time was in its \nAPI, and yet that was a critical part of its interface.\n—PCC\nHere are some of the reasons you might want systems to interoperate:\n■\n■Your system provides a service to be used by a collection of unknown \nsystems. These systems need to interoperate with your system even though \nyou may know nothing about them. An example is a service such as Google \nMaps.\n■\n■You are constructing capabilities from existing systems. For example, one \nof the existing systems is responsible for sensing its environment, another \none is responsible for processing the raw data, a third is responsible for \ninterpreting the data, and a final one is responsible for producing and \ndistributing a representation of what was sensed. An example is a traffic \nsensing system where the input comes from individual vehicles, the raw \ndata is processed into common units of measurement, is interpreted and \nfused, and traffic congestion information is broadcast.\nThese examples highlight two important aspects of interoperability:\n1.\t\nDiscovery. The consumer of a service must discover (possibly at runtime, \npossibly prior to runtime) the location, identity, and the interface of the \nservice.\n2.\t\nHandling of the response. There are three distinct possibilities:\n■\n■The service reports back to the requester with the response.\n■\n■The service sends its response on to another system. \n■\n■The service broadcasts its response to any interested parties.\nThese elements, discovery and disposition of response, along with management \nof interfaces, govern our discussion of scenarios and tactics for interoperability.\n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "106 \nPart Two  Quality Attributes\t\n6—Interoperability\nSystems of Systems\nIf you have a group of systems that are interoperating to achieve a joint \npurpose, you have what is called a system of systems (SoS). An SoS is \nan arrangement of systems that results when independent and useful sys-\ntems are integrated into a larger system that delivers unique capabilities. \nTable 6.1 shows a categorization of SoSs.\nTable 6.1  Taxonomy of Systems of Systems*\nDirected\nSoS objectives, centralized management, funding, and \nauthority for the overall SoS are in place. Systems are \nsubordinated to the SoS.\nAcknowledged\nSoS objectives, centralized management, funding, and \nauthority in place. However, systems retain their own \nmanagement, funding, and authority in parallel with the \nSoS.\nCollaborative\nThere are no overall objectives, centralized \nmanagement, authority, responsibility, or funding at the \nSoS level. Systems voluntarily work together to address \nshared or common interests.\nVirtual\nLike collaborative, but systems don’t know about each \nother.\n*  The taxonomy shown is an extension of work done by Mark Maier in 1998.\nIn directed and acknowledged SoSs, there is a deliberate attempt to \ncreate an SoS. The key difference is that in the former, there is SoS-level \nmanagement that exercises control over the constituent systems, while in \nthe latter, the constituent systems retain a high degree of autonomy in their \nown evolution. Collaborative and virtual systems of systems are more ad \nhoc, absent an overarching authority or source of funding and, in the case \nof a virtual SoS, even absent the knowledge about the scope and member-\nship of the SoS.\nThe collaborative case is quite common. Consider the Google Maps ex-\nample from the introduction. Google is the manager and funding authority \nfor the map service. Each use of the maps in an application (an SoS) has \nits own management and funding authority, and there is no overall manage-\nment of all of the applications that use Google Maps. The various organiza-\ntions involved in the applications collaborate (either explicitly or implicitly) to \nenable the applications to work correctly.\nA virtual SoS involves large systems and is much more ad hoc. For \nexample, there are over 3,000 electric companies in the U.S. electric grid, \neach state has a public utility commission that oversees the utility companies \noperating in its state, and the federal Department of Energy provides some \nlevel of policy guidance. Many of the systems within the electric grid must \ninteroperate, but there is no management authority for the overall system.\n",
      "content_length": 2613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "6.1  Interoperability General Scenario\n107\n6.1  Interoperability General Scenario\nThe following are the portions of an interoperability general scenario:\n■\n■Source of stimulus. A system that initiates a request. \n■\n■Stimulus. A request to exchange information among systems.\n■\n■Artifacts. The systems that wish to interoperate.\n■\n■Environment. The systems that wish to interoperate are discovered at run-\ntime or are known prior to runtime.\n■\n■Response. The request to interoperate results in the exchange of informa-\ntion. The information is understood by the receiving party both syntactical-\nly and semantically. Alternatively, the request is rejected and appropriate \nentities are notified. In either case, the request may be logged.\n■\n■Response measure. The percentage of information exchanges correctly \nprocessed or the percentage of information exchanges correctly rejected.\nFigure 6.1 gives an example: Our vehicle information system sends our cur-\nrent location to the traffic monitoring system. The traffic monitoring system com-\nbines our location with other information, overlays this information on a Goo-\ngle Map, and broadcasts it. Our location information is correctly included with a \nprobability of 99.9%.\nTable 6.2 presents the possible values for each portion of an interoperability \nscenario.\nStimulus:\nResponse:\nEnvironment:\nSystems known\nprior to run-time\nArtifact:\nResponse\nMeasure:\nSource\nof Stimulus:\n3\n2\n1\n4\nOur Vehicle \nInformation \nSystem\nCurrent \nLocation \nSent\nTraffic Monitor \nCombines Current \nLocation with Other \nInformation, \nOverlays on Google \nMaps, and \nBroadcasts\nOur Information \nIncluded Correctly \n99.9% of the Time\nTraffic Monitoring \nSystem\nFigure 6.1  Sample concrete interoperability scenario\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "108 \nPart Two  Quality Attributes\t\n6—Interoperability\nTable 6.2  General Interoperability Scenario\nPortion of Scenario Possible Values\nSource\nA system initiates a request to interoperate with another \nsystem.\nStimulus\nA request to exchange information among system(s).\nArtifact\nThe systems that wish to interoperate.\nEnvironment\nSystem(s) wishing to interoperate are discovered at runtime or \nknown prior to runtime.\nResponse\nOne or more of the following:\n■\n■\nThe request is (appropriately) rejected and appropriate \nentities (people or systems) are notified.\n■\n■\nThe request is (appropriately) accepted and information is \nexchanged successfully.\n■\n■\nThe request is logged by one or more of the involved \nsystems.\nResponse Measure\nOne or more of the following:\n■\n■\nPercentage of information exchanges correctly processed \n■\n■\nPercentage of information exchanges correctly rejected \nSOAP vs. REST\nIf you want to allow web-based applications to interoperate, you have \ntwo major off-the-shelf technology options today: (1) WS* and SOAP \n(which once stood for “Simple Object Access Protocol,” but that acronym \nis no longer blessed) and (2) REST (which stands for “Representation \nState Transfer,” and therefore is sometimes spelled ReST). How can we \ncompare these technologies? What is each good for? What are the road \nhazards you need to be aware of? This is a bit of an apples-and-oranges \ncomparison, but I will try to sketch the landscape. \nSOAP is a protocol specification for XML-based information that distrib-\nuted applications can use to exchange information and hence interoperate. \nIt is most often accompanied by a set of SOA middleware interoperability \nstandards and compliant implementations, referred to (collectively) as WS*. \nSOAP and WS* together define many standards, including the following:\n■\n■\nAn infrastructure for service composition. SOAP can employ the Business \nProcess Execution Language (BPEL) as a way to let developers express \nbusiness processes that are implemented as WS* services. \n■\n■\nTransactions. There are several web-service standards for ensuring \nthat transactions are properly managed: WS-AT, WS-BA, WS-CAF, and \nWS-Transaction.\n■\n■\nService discovery. The Universal Description, Discovery and Integration \n(UDDI) language enables businesses to publish service listings and \ndiscover each other.\n",
      "content_length": 2341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "6.1  Interoperability General Scenario\n109\n■\n■\nReliability. SOAP, by itself, does not ensure reliable message delivery. \nApplications that require such guarantees must use services compliant with \nSOAP’s reliability standard: WS-Reliability.\nSOAP is quite general and has its roots in a remote procedure call \n(RPC) model of interacting applications, although other models are cer-\ntainly possible. SOAP has a simple type system, comparable to that found \nin the major programming languages. SOAP relies on HTTP and RPC for \nmessage transmission, but it could, in theory, be implemented on top of \nany communication protocol. SOAP does not mandate a service’s method \nnames, addressing model, or procedural conventions. Thus, choosing \nSOAP buys little actual interoperability between applications—it is just \nan information exchange standard. The interacting applications need to \nagree on how to interpret the payload, which is where you get semantic \ninteroperability. \nREST, on the other hand, is a client-server-based architectural style that \nis structured around a small set of create, read, update, delete (CRUD) op-\nerations (called POST, GET, PUT, DELETE respectively in the REST world) \nand a single addressing scheme (based on a URI, or uniform resource \nidentifier). REST imposes few constraints on an architecture: SOAP offers \ncompleteness; REST offers simplicity. \nREST is about state and state transfer and views the web (and the ser-\nvices that service-oriented systems can string together) as a huge network \nof information that is accessible by a single URI-based addressing scheme. \nThere is no notion of type and hence no type checking in REST—it is up to \nthe applications to get the semantics of interaction right.\nBecause REST interfaces are so simple and general, any HTTP client \ncan talk to any HTTP server, using the REST operations (POST, GET, PUT, \nDELETE) with no further configuration. That buys you syntactic interopera-\nbility, but of course there must be organization-level agreement about what \nthese programs actually do and what information they exchange. That is, \nsemantic interoperability is not guaranteed between services just because \nboth have REST interfaces.\nREST, on top of HTTP, is meant to be self-descriptive and in the best \ncase is a stateless protocol. Consider the following example, in REST, of a \nphone book service that allows someone to look up a person, given some \nunique identifier for that person:\nhttp://www.XYZdirectory.com/phonebook/UserInfo/99999\nThe same simple lookup, implemented in SOAP, would be specified as \nsomething like the following:\n<?xml version=”1.0”?>\n<soap:Envelope xmlns:soap=http://www.w3.org/2001/  \n\t\n\t\n12/soap-envelope \n soap:encodingStyle=”http://www.w3.org/2001/12/ \n\t\n\t\nsoap-encoding”>\n\t\n<soap:Body pb=”http://www.XYZdirectory.com/ \n\t\n\t\nphonebook”>\n",
      "content_length": 2841,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "110 \nPart Two  Quality Attributes\t\n6—Interoperability\n\t\n\t\n<pb:GetUserInfo>\n\t\n\t\n\t\n<pb:UserIdentifier>99999</pb:UserIdentifier>\n\t\n\t\n</pb:GetUserInfo>\n\t\n</soap:Body>\n</soap:Envelope>\nOne aspect of the choice between SOAP and REST is whether you \nwant to accept the complexity and restrictions of SOAP+WSDL (the Web \nServices Description Language) to get more standardized interoperability \nor if you want to avoid the overhead by using REST, but perhaps benefit \nfrom less standardization. What are the other considerations? \nA message exchange in REST has somewhat fewer characters than a \nmessage exchange in SOAP. So one of the tradeoffs in the choice between \nREST and SOAP is the size of the individual messages. For systems \nexchanging a large number of messages, another tradeoff is between per-\nformance (favoring REST) and structured messages (favoring SOAP).\nThe decision to implement WS* or REST will depend on aspects such \nas the quality of service (QoS) required—WS* implementation has greater \nsupport for security, availability, and so on—and type of functionality. A \nRESTful implementation, because of its simplicity, is more appropriate for \nread-only functionality, typical of mashups, where there are minimal QoS \nrequirements and concerns.\nOK, so if you are building a service-based system, how do you choose? \nThe truth is, you don’t have to make a single choice, once and for all time; \neach technology is reasonably easy to use, at least for simple applications. \nAnd each has its strengths and weaknesses. Like everything else in archi-\ntecture, it’s all about the tradeoffs; your decision will likely hinge on the way \nthose tradeoffs affect your system in your context.\n—RK\n6.2  Tactics for Interoperability\nFigure 6.2 shows the goal of the set of interoperability tactics.\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nTactics\nto Control\nInteroperability\nFigure 6.2  Goal of interoperability tactics \n",
      "content_length": 1936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "6.2  Tactics for Interoperability\n111\nWe identify two categories of interoperability tactics: locate and manage \ninterfaces. \nLocate\nThere is only one tactic in this category: discover service. It is used when the \nsystems that interoperate must be discovered at runtime.\n■\n■Discover service. Locate a service through searching a known directory ser-\nvice. (By “service,” we simply mean a set of capabilities that is accessible \nvia some kind of interface.) There may be multiple levels of indirection in \nthis location process—that is, a known location points to another location \nthat in turn can be searched for the service. The service can be located by \ntype of service, by name, by location, or by some other attribute.\nManage Interfaces\nManaging interfaces consists of two tactics: orchestrate and tailor interface. \n■\n■Orchestrate. Orchestrate is a tactic that uses a control mechanism to \ncoordinate and manage and sequence the invocation of particular services \n(which could be ignorant of each other). Orchestration is used when the \ninteroperating systems must interact in a complex fashion to accomplish a \ncomplex task; orchestration “scripts” the interaction. Workflow engines are \nan example of the use of the orchestrate tactic. The mediator design pattern \ncan serve this function for simple orchestration. Complex orchestration can \nbe specified in a language such as BPEL.\n■\n■Tailor interface. Tailor interface is a tactic that adds or removes capabilities \nto an interface. Capabilities such as translation, adding buffering, or \nsmoothing data can be added. Capabilities may be removed as well. An \nexample of removing capabilities is to hide particular functions from \nuntrusted users. The decorator pattern is an example of the tailor interface \ntactic. \nThe enterprise service bus that underlies many service-oriented architec-\ntures combines both of the manage interface tactics.\nFigure 6.3 shows a summary of the tactics to achieve interoperability.\n",
      "content_length": 1977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "112 \nPart Two  Quality Attributes\t\n6—Interoperability\nInteroperability Tactics\nLocate\nManage Interfaces\nDiscover\nService\nOrchestrate\nTailor Interface\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nFigure 6.3  Summary of interoperability tactics\nWhy Standards Are Not Enough to Guarantee Interoperability \nBy Grace Lewis\nDeveloper of System A needs to exchange product data with System B. \nDeveloper A finds that there is an existing WS* web service interface for \nsending product data that among other fields contains price expressed \nin XML Schema as a decimal with two fraction digits. Developer A writes \ncode to interact with the web service and the system works perfectly. \nHowever, after two weeks of operation, there is a huge discrepancy be-\ntween the totals reported by System A and the totals reported by System \nB. After conversations between the two developers, they discover that \nSystem B expected to receive a price that included tax and System A was \nsending it without tax. \nThis is a simple example of why standards are not enough. The sys-\ntems exchanged data perfectly because they both agreed that the price \nwas a decimal with two fractions digits expressed in XML Schema and the \nmessage was sent via SOAP over HTTP (syntax)—standards used in the \nimplementation of WS* web services—but they did not agree on whether \nthe price included tax or not (semantics).\nOf course, the only realistic approach to getting diverse applications to \nshare information is by reaching agreements on the structure and func-\ntion of the information to be shared. These agreements are often reflected \nin standards that provide a common interface that multiple vendors and \napplication builders support. Standards have indeed been instrumental \n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "6.2  Tactics for Interoperability\n113\nin achieving a significant level of interoperability that we rely on in almost \nevery domain. However, while standards are useful and in many ways in-\ndispensable, expectations of what can be achieved through standards are \nunrealistic. Here are some of the challenges that organizations face related \nto standards and interoperability:\n1.\t Ideally, every implementation of a standard should be identical \nand thus completely interoperable with any other implementation. \nHowever, this is far from reality. Standards, when incorporated into \nproducts, tools, and services, undergo customizations and exten-\nsions because every vendor wants to create a unique selling point as \na competitive advantage.\n2.\t Standards are often deliberately open-ended and provide exten-\nsion points. The actual implementation of these extension points \nis left to the discretion of implementers, leading to proprietary \nimplementations.\n3.\t Standards, like any technology, have a life cycle of their own and \nevolve over time in compatible and noncompatible ways. Deciding \nwhen to adopt a new or revised standard is a critical decision for or-\nganizations. Committing to a new standard that is not ready or even-\ntually not adopted by the community is a big risk for organizations. \nOn the other hand, waiting too long may also become a problem, \nwhich can lead to unsupported products, incompatibilities, and work-\narounds, because everyone else is using the standard.\n4.\t Within the software community, there are as many bad standards as \nthere are engineers with opinions. Bad standards include underspe-\ncified, overspecified, inconsistently specified, unstable, or irrelevant \nstandards. \n5.\t It is quite common for standards to be championed by competing \norganizations, resulting in conflicting standards due to overlap or mu-\ntual exclusion.\n6.\t For new and rapidly emerging domains, the argument often made is \nthat standardization will be destructive because it will hinder flexibil-\nity: premature standardization will force the use of an inadequate ap-\nproach and lead to abandoning other presumably better approaches. \nSo what do organizations do in the meantime?\nWhat these challenges illustrate is that because of the way in which \nstandards are usually created and evolved, we cannot let standards drive \nour architectures. We need to architect systems first and then decide which \nstandards can support desired system requirements and qualities. This ap-\nproach allows standards to change and evolve without affecting the overall \narchitecture of the system. \nI once heard someone in a keynote address say that “The nice thing \nabout standards is that there are so many to choose from.”\n",
      "content_length": 2725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "114 \nPart Two  Quality Attributes\t\n6—Interoperability\n6.3  A Design Checklist for Interoperability\nTable 6.3 is a checklist to support the design and analysis process for inter­operability.\nTable 6.3  Checklist to Support the Design and Analysis Process for \nInteroperability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which of your system responsibilities will need to \ninteroperate with other systems.\nEnsure that responsibilities have been allocated to detect \na request to interoperate with known or unknown external \nsystems. \nEnsure that responsibilities have been allocated to carry out the \nfollowing tasks: \n■\n■\nAccept the request\n■\n■\nExchange information\n■\n■\nReject the request\n■\n■\nNotify appropriate entities (people or systems)\n■\n■\nLog the request (for interoperability in an untrusted environ-\nment, logging for nonrepudiation is essential) \nCoordination Model\nEnsure that the coordination mechanisms can meet the critical \nquality attribute requirements. Considerations for performance \ninclude the following:\n■\n■\nVolume of traffic on the network both created by the sys-\ntems under your control and generated by systems not \nunder your control\n■\n■\nTimeliness of the messages being sent by your systems\n■\n■\nCurrency of the messages being sent by your systems\n■\n■\nJitter of the messages’ arrival times\n■\n■\nEnsure that all of the systems under your control make as-\nsumptions about protocols and underlying networks that are \nconsistent with the systems not under your control.\nData Model\nDetermine the syntax and semantics of the major data \nabstractions that may be exchanged among interoperating \nsystems.\nEnsure that these major data abstractions are consistent with \ndata from the interoperating systems. (If your system’s data \nmodel is confidential and must not be made public, you may \nhave to apply transformations to and from the data abstractions \nof systems with which yours interoperates.)\nMapping among \nArchitectural \nElements\nFor interoperability, the critical mapping is that of components \nto processors. Beyond the necessity of making sure that \ncomponents that communicate externally are hosted \non processors that can reach the network, the primary \nconsiderations deal with meeting the security, availability, and \nperformance requirements for the communication. These will \nbe dealt with in their respective chapters.\n",
      "content_length": 2371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "6.4  Summary\n115\nCategory\nChecklist\nResource \nManagement\nEnsure that interoperation with another system (accepting a \nrequest and/or rejecting a request) can never exhaust critical \nsystem resources (e.g., can a flood of such requests cause \nservice to be denied to legitimate users?).\nEnsure that the resource load imposed by the communication \nrequirements of interoperation is acceptable.\nEnsure that if interoperation requires that resources be shared \namong the participating systems, an adequate arbitration policy \nis in place.\nBinding Time\nDetermine the systems that may interoperate, and when they \nbecome known to each other. For each system over which you \nhave control:\n■\n■\nEnsure that it has a policy for dealing with binding to both \nknown and unknown external systems. \n■\n■\nEnsure that it has mechanisms in place to reject unaccept-\nable bindings and to log such requests.\n■\n■\nIn the case of late binding, ensure that mechanisms will \nsupport the discovery of relevant new services or protocols, \nor the sending of information using chosen protocols.\nChoice of \nTechnology\nFor any of your chosen technologies, are they “visible” at the \ninterface boundary of a system? If so, what interoperability \neffects do they have? Do they support, undercut, or have \nno effect on the interoperability scenarios that apply to your \nsystem? Ensure the effects they have are acceptable.\nConsider technologies that are designed to support \ninteroperability, such as web services. Can they be used to \nsatisfy the interoperability requirements for the systems under \nyour control?\n6.4  Summary\nInteroperability refers to the ability of systems to usefully exchange information. \nThese systems may have been constructed with the intention of exchanging infor-\nmation, they may be existing systems that are desired to exchange information, \nor they may provide general services without knowing the details of the systems \nthat wish to utilize those services.\nThe general scenario for interoperability provides the details of these dif-\nferent cases. In any interoperability case, the goal is to intentionally exchange \ninformation or reject the request to exchange information.\nAchieving interoperability involves the relevant systems locating each other \nand then managing the interfaces so that they can exchange information.\n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "116 \nPart Two  Quality Attributes\t\n6—Interoperability\n6.5  For Further Reading\nAn SEI report gives a good overview of interoperability, and it highlights some of \nthe “maturity frameworks” for interoperability [Brownsword 04].\nThe various WS* services are being developed under the auspices of the \nWorld Wide Web Consortium (W3C) and can be found at www.w3.org/2002/ws.\nSystems of systems are of particular interest to the U.S. Department of De-\nfense. An engineering guide can be found at [ODUSD 08].\n6.6  Discussion Questions\n1.\t\nFind a web service mashup. Write several concrete interoperability scenari-\nos for this system.\n2.\t\nWhat is the relationship between interoperability and the other quality \nattributes highlighted in this book? For example, if two systems fail to ex-\nchange information properly, could a security flaw result? What other quali-\nty attributes seem strongly related (at least potentially) to interoperability? \n3.\t\nIs a service-oriented system a system of systems? If so, describe a ser-\nvice-oriented system that is directed, one that is acknowledged, one that is \ncollaborative, and one that is virtual.\n4.\t\nUniversal Description, Discovery, and Integration (UDDI) was touted as a \ndiscovery service, but commercial support for UDDI is being withdrawn. \nWhy do you suppose this is? Does it have anything to do with the quality \nattributes delivered or not delivered by UDDI solutions?\n5.\t\nWhy has the importance of orchestration grown in recent years?\n6.\t\nIf you are a technology producer, what are the advantages and disadvan-\ntages of adhering to interoperability standards? Why would a producer not \nadhere to a standard?\n7.\t\nWith what other systems will an automatic teller machine need to interoper-\nate? How would you change your automatic teller system design to accom-\nmodate these other systems?\n",
      "content_length": 1837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "117\n7\nModifiability\nAdapt or perish, now as ever, is \nnature’s inexorable imperative.\n—H.G. Wells\nChange happens.\nStudy after study shows that most of the cost of the typical software system \noccurs after it has been initially released. If change is the only constant in the uni-\nverse, then software change is not only constant but ubiquitous. Changes happen \nto add new features, to change or even retire old ones. Changes happen to fix de-\nfects, tighten security, or improve performance. Changes happen to enhance the \nuser’s experience. Changes happen to embrace new technology, new platforms, \nnew protocols, new standards. Changes happen to make systems work together, \neven if they were never designed to do so. \nModifiability is about change, and our interest in it centers on the cost and \nrisk of making changes. To plan for modifiability, an architect has to consider \nfour questions: \n■\n■What can change? A change can occur to any aspect of a system: the \nfunctions that the system computes, the platform (the hardware, operating \nsystem, middleware), the environment in which the system operates \n(the systems with which it must interoperate, the protocols it uses to \ncommunicate with the rest of the world), the qualities the system exhibits \n(its performance, its reliability, and even its future modifications), and its \ncapacity (number of users supported, number of simultaneous operations). \n■\n■What is the likelihood of the change? One cannot plan a system for all \npotential changes—the system would never be done, or if it was done \nit would be far too expensive and would likely suffer quality attribute \nproblems in other dimensions. Although anything might change, the \narchitect has to make the tough decisions about which changes are likely, \nand hence which changes are to be supported, and which are not.\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "118 \nPart Two  Quality Attributes\t\n7—Modifiability\n■\n■When is the change made and who makes it? Most commonly in the \npast, a change was made to source code. That is, a developer had to make \nthe change, which was tested and then deployed in a new release. Now, \nhowever, the question of when a change is made is intertwined with the \nquestion of who makes it. An end user changing the screen saver is clearly \nmaking a change to one of the aspects of the system. Equally clear, it is \nnot in the same category as changing the system so that it can be used \nover the web rather than on a single machine. Changes can be made to the \nimplementation (by modifying the source code), during compile (using \ncompile-time switches), during build (by choice of libraries), during \nconfiguration setup (by a range of techniques, including parameter setting), \nor during execution (by parameter settings, plugins, etc.). A change can also \nbe made by a developer, an end user, or a system administrator.\n■\n■What is the cost of the change? Making a system more modifiable involves \ntwo types of cost:\n■\n■The cost of introducing the mechanism(s) to make the system more \nmodifiable\n■\n■The cost of making the modification using the mechanism(s) \nFor example, the simplest mechanism for making a change is to wait for \na change request to come in, then change the source code to accommodate the \nrequest. The cost of introducing the mechanism is zero; the cost of exercising \nit is the cost of changing the source code and revalidating the system. At the \nother end of the spectrum is an application generator, such as a user interface \nbuilder. The builder takes as input a description of the designer user interface \nproduced through direct manipulation techniques and produces (usually) source \ncode. The cost of introducing the mechanism is the cost of constructing the UI \nbuilder, which can be substantial. The cost of using the mechanism is the cost of \nproducing the input to feed the builder (cost can be substantial or negligible), the \ncost of running the builder (approximately zero), and then the cost of whatever \ntesting is performed on the result (usually much less than usual).\nFor N similar modifications, a simplified justification for a change mecha-\nnism is that \nN × Cost of making the change without the mechanism <_  \nCost of installing the mechanism +  \n(N × Cost of making the change using the mechanism).\nN is the anticipated number of modifications that will use the modifiability \nmechanism, but N is a prediction. If fewer changes than expected come in, then \nan expensive modification mechanism may not be warranted. In addition, the cost \nof creating the modifiability mechanism could be applied elsewhere—in adding \nfunctionality, in improving the performance, or even in nonsoftware investments \nsuch as buying tech stocks. Also, the equation does not take time into account. It \n",
      "content_length": 2900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "7.1  Modifiability General Scenario\n119\nmight be cheaper in the long run to build a sophisticated change-handling mecha-\nnism, but you might not be able to wait for that.\n7.1  Modifiability General Scenario\nFrom these considerations, we can see the portions of the modifiability general \nscenario:\n■\n■Source of stimulus. This portion specifies who makes the change: the \ndeveloper, a system administrator, or an end user. \n■\n■Stimulus. This portion specifies the change to be made. A change can be \nthe addition of a function, the modification of an existing function, or the \ndeletion of a function. (For this categorization, we regard fixing a defect \nas changing a function, which presumably wasn’t working correctly as \na result of the defect.) A change can also be made to the qualities of the \nsystem: making it more responsive, increasing its availability, and so forth. \nThe capacity of the system may also change. Accommodating an increasing \nnumber of simultaneous users is a frequent requirement. Finally, changes \nmay happen to accommodate new technology of some sort, the most \ncommon of which is porting the system to a different type of computer or \ncommunication network.\n■\n■Artifact. This portion specifies what is to be changed: specific components \nor modules, the system’s platform, its user interface, its environment, or \nanother system with which it interoperates.\n■\n■Environment. This portion specifies when the change can be made: design \ntime, compile time, build time, initiation time, or runtime. \n■\n■Response. Make the change, test it, and deploy it. \n■\n■Response measure. All of the possible responses take time and cost money; \ntime and money are the most common response measures. Although both \nsound simple to measure, they aren’t. You can measure calendar time or \nstaff time. But do you measure the time it takes for the change to wind its \nway through configuration control boards and approval authorities (some \nof whom may be outside your organization), or merely the time it takes \nyour engineers to make the change? Cost usually means direct outlay, but \nit might also include opportunity cost of having your staff work on changes \ninstead of other tasks. Other measures include the extent of the change \n(number of modules or other artifacts affected) or the number of new \ndefects introduced by the change, or the effect on other quality attributes. If \nthe change is being made by a user, you may wish to measure the efficacy \nof the change mechanisms provided, which somewhat overlaps with \nmeasures of usability (see Chapter 11). \n",
      "content_length": 2577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "120 \nPart Two  Quality Attributes\t\n7—Modifiability\nFigure 7.1 illustrates a concrete modifiability scenario: The developer \nwishes to change the user interface by modifying the code at design time. The \nmodifications are made with no side effects within three hours.\nTable 7.1 enumerates the elements of the general scenario that characterize \nmodifiability.\nStimulus:\nWishes\nto Change\nthe UI\nResponse:\nChange Made \nand Unit Tested \nSource:\nDeveloper\nArtifact:\nCode\nEnvironment:\n \nDesign\nTime\nResponse\nMeasure:\nIn Three\nHours\n3\n2\n1\n4\nFigure 7.1  Sample concrete modifiability scenario\nTable 7.1  Modifiability General Scenario\nPortion of Scenario Possible Values\nSource\nEnd user, developer, system administrator\nStimulus\nA directive to add/delete/modify functionality, or change a \nquality attribute, capacity, or technology\nArtifacts\nCode, data, interfaces, components, resources, configurations, \n. . . \nEnvironment\nRuntime, compile time, build time, initiation time, design time\nResponse\nOne or more of the following:\n■\n■\nMake modification \n■\n■\nTest modification\n■\n■\nDeploy modification\nResponse Measure\nCost in terms of the following:\n■\n■\nNumber, size, complexity of affected artifacts\n■\n■\nEffort\n■\n■\nCalendar time\n■\n■\nMoney (direct outlay or opportunity cost)\n■\n■\nExtent to which this modification affects other functions or \nquality attributes\n■\n■\nNew defects introduced\n",
      "content_length": 1377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "7.2  Tactics for Modifiability\n121\n7.2  Tactics for Modifiability\nTactics to control modifiability have as their goal controlling the complexity of \nmaking changes, as well as the time and cost to make changes. Figure 7.2 shows \nthis relationship.\nTo understand modifiability, we begin with coupling and cohesion.\nModules have responsibilities. When a change causes a module to be modi-\nfied, its responsibilities are changed in some way. Generally, a change that affects \none module is easier and less expensive than if it changes more than one mod-\nule. However, if two modules’ responsibilities overlap in some way, then a single \nchange may well affect them both. We can measure this overlap by measuring the \nprobability that a modification to one module will propagate to the other. This is \ncalled coupling, and high coupling is an enemy of modifiability.\nCohesion measures how strongly the responsibilities of a module are re-\nlated. Informally, it measures the module’s “unity of purpose.” Unity of purpose \ncan be measured by the change scenarios that affect a module. The cohesion of a \nmodule is the probability that a change scenario that affects a responsibility will \nalso affect other (different) responsibilities. The higher the cohesion, the lower \nthe probability that a given change will affect multiple responsibilities. High co-\nhesion is good; low cohesion is bad. The definition allows for two modules with \nsimilar purposes each to be cohesive.\nGiven this framework, we can now identify the parameters that we will use \nto motivate modifiability tactics:\n■\n■Size of a module. Tactics that split modules will reduce the cost of making \na modification to the module that is being split as long as the split is chosen \nto reflect the type of change that is likely to be made.\nChange \nArrives\nChange Made within\nTime and Budget\nTactics\nto Control\nModifiability\nFigure 7.2  The goal of modifiability tactics\n",
      "content_length": 1928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "122 \nPart Two  Quality Attributes\t\n7—Modifiability\n■\n■Coupling. Reducing the strength of the coupling between two modules A \nand B will decrease the expected cost of any modification that affects A. \nTactics that reduce coupling are those that place intermediaries of various \nsorts between modules A and B.\n■\n■Cohesion. If module A has a low cohesion, then cohesion can be improved \nby removing responsibilities unaffected by anticipated changes. \nFinally we need to be concerned with when in the software development \nlife cycle a change occurs. If we ignore the cost of preparing the architecture for \nthe modification, we prefer that a change is bound as late as possible. Changes \ncan only be successfully made (that is, quickly and at lowest cost) late in the \nlife cycle if the architecture is suitably prepared to accommodate them. Thus the \nfourth and final parameter in a model of modifiability is this:\n■\n■Binding time of modification. An architecture that is suitably equipped to \naccommodate modifications late in the life cycle will, on average, cost less \nthan an architecture that forces the same modification to be made earlier. \nThe preparedness of the system means that some costs will be zero, or very \nlow, for late life-cycle modifications. This, however, neglects the cost of \npreparing the architecture for the late binding.\nNow we may understand tactics and their consequences as affecting one or \nmore of the previous parameters: reducing the size of a module, increasing cohe-\nsion, reducing coupling, and deferring binding time. These tactics are shown in \nFigure 7.3.\nModifiability Tactics\nIncrease\nCohesion\nReduce\nCoupling\nSplit Module\nEncapsulate\nUse an\nIntermediary\nChange\nArrives\nChange Made\nwithin Time \nand Budget\nReduce Size\nof a Module\nIncrease\nSemantic\nCoherence\nRestrict\nDependencies\nRefactor\nAbstract Common\nServices\nDefer\nBinding\nFigure 7.3  Modifiability tactics\n",
      "content_length": 1905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "7.2  Tactics for Modifiability\n123\nReduce the Size of a Module\n■\n■Split module. If the module being modified includes a great deal of capa-\nbility, the modification costs will likely be high. Refining the module into \nseveral smaller modules should reduce the average cost of future changes. \nIncrease Cohesion\nSeveral tactics involve moving responsibilities from one module to another. The \npurpose of moving a responsibility from one module to another is to reduce the \nlikelihood of side effects affecting other responsibilities in the original module.\n■\n■Increase semantic coherence. If the responsibilities A and B in a module \ndo not serve the same purpose, they should be placed in different modules. \nThis may involve creating a new module or it may involve moving a re-\nsponsibility to an existing module. One method for identifying responsibil-\nities to be moved is to hypothesize likely changes that affect a module. If \nsome responsibilities are not affected by these changes, then those responsi-\nbilities should probably be removed.\nReduce Coupling\nWe now turn to tactics that reduce the coupling between modules.\n■\n■Encapsulate. Encapsulation introduces an explicit interface to a module. \nThis interface includes an application programming interface (API) and its \nassociated responsibilities, such as “perform a syntactic transformation on \nan input parameter to an internal representation.” Perhaps the most common \nmodifiability tactic, encapsulation reduces the probability that a change to \none module propagates to other modules. The strengths of coupling that \npreviously went to the module now go to the interface for the module. \nThese strengths are, however, reduced because the interface limits the ways \nin which external responsibilities can interact with the module (perhaps \nthrough a wrapper). The external responsibilities can now only directly in-\nteract with the module through the exposed interface (indirect interactions, \nhowever, such as dependence on quality of service, will likely remain un-\nchanged). Interfaces designed to increase modifiability should be abstract \nwith respect to the details of the module that are likely to change—that is, \nthey should hide those details. \n■\n■Use an intermediary breaks a dependency. Given a dependency between re-\nsponsibility A and responsibility B (for example, carrying out A first requires \ncarrying out B), the dependency can be broken by using an intermediary. \nThe type of intermediary depends on the type of dependency. For example, \na publish-subscribe intermediary will remove the data producer’s knowledge \n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "124 \nPart Two  Quality Attributes\t\n7—Modifiability\nof its consumers. So will a shared data repository, which separates readers of \na piece of data from writers of that data. In a service-oriented architecture in \nwhich services discover each other by dynamic lookup, the directory service \nis an intermediary.\n■\n■Restrict dependencies is a tactic that restricts the modules that a given mod-\nule interacts with or depends on. In practice this tactic is achieved by re-\nstricting a module’s visibility (when developers cannot see an interface, they \ncannot employ it) and by authorization (restricting access to only authorized \nmodules). This tactic is seen in layered architectures, in which a layer is only \nallowed to use lower layers (sometimes only the next lower layer) and in the \nuse of wrappers, where external entities can only see (and hence depend on) \nthe wrapper and not the internal functionality that it wraps.\n■\n■Refactor is a tactic undertaken when two modules are affected by the same \nchange because they are (at least partial) duplicates of each other. Code re-\nfactoring is a mainstay practice of Agile development projects, as a cleanup \nstep to make sure that teams have not produced duplicative or overly com-\nplex code; however, the concept applies to architectural elements as well. \nCommon responsibilities (and the code that implements them) are “factored \nout” of the modules where they exist and assigned an appropriate home of \ntheir own. By co-locating common responsibilities—that is, making them \nsubmodules of the same parent module—the architect can reduce coupling. \n■\n■Abstract common services. In the case where two modules provide not-\nquite-the-same but similar services, it may be cost-effective to implement \nthe services just once in a more general (abstract) form. Any modification \nto the (common) service would then need to occur just in one place, reduc-\ning modification costs. A common way to introduce an abstraction is by pa-\nrameterizing the description (and implementation) of a module’s activities. \nThe parameters can be as simple as values for key variables or as complex \nas statements in a specialized language that are subsequently interpreted. \nDefer Binding\nBecause the work of people is almost always more expensive than the work of \ncomputers, letting computers handle a change as much as possible will almost \nalways reduce the cost of making that change. If we design artifacts with built-in \nflexibility, then exercising that flexibility is usually cheaper than hand-coding a \nspecific change.\nParameters are perhaps the best-known mechanism for introducing \nflexibility, and that is reminiscent of the abstract common services tactic. A \nparameterized function f(a, b) is more general than the similar function f(a) that \nassumes b = 0. When we bind the value of some parameters at a different phase \nin the life cycle than the one in which we defined the parameters, we are applying \nthe defer binding tactic.\n",
      "content_length": 2980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "7.3  A Design Checklist for Modifiability\n125\nIn general, the later in the life cycle we can bind values, the better. However, \nputting the mechanisms in place to facilitate that late binding tends to be more \nexpensive—yet another tradeoff. And so the equation on page 118 comes into \nplay. We want to bind as late as possible, as long as the mechanism that allows it \nis cost-effective.\nTactics to bind values at compile time or build time include these:\n■\n■Component replacement (for example, in a build script or makefile)\n■\n■Compile-time parameterization\n■\n■Aspects\nTactics to bind values at deployment time include this:\n■\n■Configuration-time binding\nTactics to bind values at startup or initialization time include this:\n■\n■Resource files\nTactics to bind values at runtime include these:\n■\n■Runtime registration\n■\n■Dynamic lookup (e.g., for services)\n■\n■Interpret parameters\n■\n■Startup time binding\n■\n■Name servers\n■\n■Plug-ins\n■\n■Publish-subscribe\n■\n■Shared repositories\n■\n■Polymorphism\nSeparating building a mechanism for modifiability from using the \nmechanism to make a modification admits the possibility of different stakeholders \nbeing involved—one stakeholder (usually a developer) to provide the mechanism \nand another stakeholder (an installer, for example, or a user) to exercise it later, \npossibly in a completely different life-cycle phase. Installing a mechanism so that \nsomeone else can make a change to the system without having to change any \ncode is sometimes called externalizing the change.\n7.3  A Design Checklist for Modifiability\nTable 7.2 is a checklist to support the design and analysis process for modifiability.\n",
      "content_length": 1648,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "126 \nPart Two  Quality Attributes\t\n7—Modifiability\nTable 7.2  Checklist to Support the Design and Analysis Process for Modifiability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which changes or categories of changes are likely to \noccur through consideration of changes in technical, legal, social, \nbusiness, and customer forces. For each potential change or \ncategory of changes: \n■\n■\nDetermine the responsibilities that would need to be added, \nmodified, or deleted to make the change. \n■\n■\nDetermine what  responsibilities are impacted by the change.\n■\n■\nDetermine an allocation of responsibilities to modules that \nplaces, as much as possible, responsibilities that will be \nchanged (or impacted by the change) together in the same \nmodule, and places responsibilities that will be changed at \ndifferent times in separate modules.\nCoordination \nModel\nDetermine which functionality or quality attribute can change at \nruntime and how this affects coordination; for example, will the \ninformation being communicated change at runtime, or will the \ncommunication protocol change at runtime? If so, ensure that such \nchanges affect a small number set of modules.\nDetermine which devices, protocols, and communication paths \nused for coordination are likely to change. For those devices, \nprotocols, and communication paths, ensure that the impact of \nchanges will be limited to a small set of modules.\nFor those elements for which modifiability is a concern, use \na coordination model that reduces coupling such as publish-\nsubscribe, defers bindings such as enterprise service bus, or \nrestricts dependencies such as broadcast.\nData Model\nDetermine which changes (or categories of changes) to the data \nabstractions, their operations, or their properties are likely to \noccur. Also determine which changes or categories of changes \nto these data abstractions will involve their creation, initialization, \npersistence, manipulation, translation, or destruction.\nFor each change or category of change, determine if the \nchanges will be made by an end user, a system administrator, or \na developer. For those changes to be made by an end user or \nsystem administrator, ensure that the necessary attributes are \nvisible to that user and that the user has the correct privileges to \nmodify the data, its operations, or its properties.\nFor each potential change or category of change:\n■\n■\nDetermine which data abstractions would need to be added, \nmodified, or deleted to make the change.\n■\n■\nDetermine whether there would be any changes to the \ncreation, initialization, persistence, manipulation, translation, or \ndestruction of these data abstractions.\n■\n■\nDetermine which other data abstractions are impacted \nby the change. For these additional data abstractions, \ndetermine whether the impact would be on the operations, \ntheir properties, their creation, initialization, persistence, \nmanipulation, translation, or destruction.\n■\n■\nEnsure an allocation of data abstractions that minimizes the \nnumber and severity of modifications to the abstractions by the \npotential changes.\nDesign your data model so that items allocated to each element of \nthe data model are likely to change together.\n",
      "content_length": 3211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "7.3  A Design Checklist for Modifiability\n127\nCategory\nChecklist\nMapping among \nArchitectural \nElements\nDetermine if it is desirable to change the way in which functionality \nis mapped to computational elements (e.g., processes, threads, \nprocessors) at runtime, compile time, design time, or build time.\nDetermine the extent of modifications necessary to accommodate \nthe addition, deletion, or modification of a function or a quality \nattribute. This might involve a determination of the following, for \nexample:\n■\n■\nExecution dependencies\n■\n■\nAssignment of data to databases\n■\n■\nAssignment of runtime elements to processes, threads, or \nprocessors\nEnsure that such changes are performed with mechanisms that \nutilize deferred binding of mapping decisions.\nResource \nManagement\nDetermine how the addition, deletion, or modification of a \nresponsibility or quality attribute will affect resource usage. This \ninvolves, for example:\n■\n■\nDetermining what changes might introduce new resources or \nremove old ones or affect existing resource usage\n■\n■\nDetermining what resource limits will change and how\nEnsure that the resources after the modification are sufficient to \nmeet the system requirements.\nEncapsulate all resource managers and ensure that the policies \nimplemented by those resource managers are themselves \nencapsulated and bindings are deferred to the extent possible.\nBinding Time\nFor each change or category of change:\n■\n■\nDetermine the latest time at which the change will need to be \nmade. \n■\n■\nChoose a defer-binding mechanism (see Section 7.2) that \ndelivers the appropriate capability at the time chosen.\n■\n■\nDetermine the cost of introducing the mechanism and the cost \nof making changes using the chosen mechanism. Use the \nequation on page 118 to assess your choice of mechanism.\n■\n■\nDo not introduce so many binding choices that change is \nimpeded because the dependencies among the choices are \ncomplex and unknown. \nChoice of \nTechnology\nDetermine what modifications are made easier or harder by your \ntechnology choices. \n■\n■\nWill your technology choices help to make, test, and deploy \nmodifications?\n■\n■\nHow easy is it to modify your choice of technologies (in case \nsome of these technologies change or become obsolete)?\nChoose your technologies to support the most likely modifications. \nFor example, an enterprise service bus makes it easier to change \nhow elements are connected but may introduce vendor lock-in.\n",
      "content_length": 2447,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "128 \nPart Two  Quality Attributes\t\n7—Modifiability\n7.4  Summary\nModifiability deals with change and the cost in time or money of making a \nchange, including the extent to which this modification affects other functions or \nquality attributes. \nChanges can be made by developers, installers, or end users, and these \nchanges need to be prepared for. There is a cost of preparing for change as well \nas a cost of making a change. The modifiability tactics are designed to prepare for \nsubsequent changes.\nTactics to reduce the cost of making a change include making modules \nsmaller, increasing cohesion, and reducing coupling. Deferring binding will also \nreduce the cost of making a change.\nReducing coupling is a standard category of tactics that includes encapsulat-\ning, using an intermediary, restricting dependencies, co-locating related responsi-\nbilities, refactoring, and abstracting common services.\nIncreasing cohesion is another standard tactic that involves separating re-\nsponsibilities that do not serve the same purpose.\nDefer binding is a category of tactics that affect build time, load time, ini-\ntialization time, or runtime.\n7.5  For Further Reading\nSerious students of software engineering should read two early papers about \ndesigning for modifiability. The first is Edsger Dijkstra’s 1968 paper about the \nT.H.E. operating system [Dijkstra 68], which is the first paper that talks about de-\nsigning systems to be layered, and the modifiability benefits it brings. The second \nis David Parnas’s 1972 paper that introduced the concept of information hiding \n[Parnas 72]. Parnas prescribed defining modules not by their functionality but by \ntheir ability to internalize the effects of changes.\nThe tactics that we have presented in this chapter are a variant on those in-\ntroduced by [Bachmann 07].\nAdditional tactics for modifiability within the avionics domain can be found \nin [EOSAN 07], published by the European Organization for the Safety of Air \nNavigation. \n7.6  Discussion Questions\n1.\t\nModifiability comes in many flavors and is known by many names. Find \none of the IEEE or ISO standards dealing with quality attributes and \n",
      "content_length": 2158,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "7.6  Discussion Questions\n129\ncompile a list of quality attributes that refer to some form of modifiability. \nDiscuss the differences.\n2.\t\nFor each quality attribute that you discovered as a result of the previous \nquestion, write a modifiability scenario that expresses it.\n3.\t\nIn a certain metropolitan subway system, the ticket machines accept cash \nbut do not give change. There is a separate machine that dispenses change \nbut does not sell tickets. In an average station there are six or eight ticket \nmachines for every change machine. What modifiability tactics do you see \nat work in this arrangement? What can you say about availability?\n4.\t\nFor the subway system in the previous question, describe the specific form \nof modifiability (using a modifiability scenario) that seems to be the aim of \narranging the ticket and change machines as described.\n5.\t\nA wrapper is a common aid to modifiability. A wrapper for a component \nis the only element allowed to use that component; every other piece of \nsoftware uses the component’s services by going through the wrapper. The \nwrapper transforms the data or control information for the component it \nwraps. For example, a component may expect input using English measures \nbut find itself in a system in which all of the other components produce \nmetric measures. A wrapper could be employed to translate. What modifi-\nability tactics does a wrapper embody?\n6.\t\nOnce an intermediary has been introduced into an architecture, some mod-\nules may attempt to circumvent it, either inadvertently (because they are \nnot aware of the intermediary) or intentionally (for performance, for conve-\nnience, or out of habit). Discuss some architectural means to prevent inad-\nvertent circumvention of an intermediary.\n7.\t\nIn some projects, deployability is an important quality attribute that mea-\nsures how easy it is to get a new version of the system into the hands of its \nusers. This might mean a trip to your auto dealer or transmitting updates \nover the Internet. It also includes the time it takes to install the update once \nit arrives. In projects that measure deployability separately, should the cost \nof a modification stop when the new version is ready to ship? Justify your \nanswer.\n8.\t\nThe abstract common services tactic is intended to reduce coupling, but it \nalso might reduce cohesion. Discuss.\n9.\t\nIdentify particular change scenarios for an automatic teller machine. What \nmodifications would you make to your automatic teller machine design to \naccommodate these changes?\n",
      "content_length": 2539,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "131\n8\nPerformance\nAn ounce of performance is worth pounds of promises.\n—Mae West\nIt’s about time.\nPerformance, that is: It’s about time and the software system’s ability to meet \ntiming requirements. When events occur—interrupts, messages, requests from \nusers or other systems, or clock events marking the passage of time—the system, \nor some element of the system, must respond to them in time. Characterizing \nthe events that can occur (and when they can occur) and the system or element’s \ntime-based response to those events is the essence is discussing performance.\nWeb-based system events come in the form of requests from users (num-\nbering in the tens or tens of millions) via their clients such as web browsers. In \na control system for an internal combustion engine, events come from the opera-\ntor’s controls and the passage of time; the system must control both the firing of \nthe ignition when a cylinder is in the correct position and the mixture of the fuel \nto maximize power and efficiency and minimize pollution. \nFor a web-based system, the desired response might be expressed as number \nof transactions that can be processed in a minute. For the engine control system, \nthe response might be the allowable variation in the firing time. In each case, the \npattern of events arriving and the pattern of responses can be characterized, and \nthis characterization forms the language with which to construct performance \nscenarios.\nFor much of the history of software engineering, performance has been the \ndriving factor in system architecture. As such, it has frequently compromised the \nachievement of all other qualities. As the price/performance ratio of hardware \ncontinues to plummet and the cost of developing software continues to rise, other \nqualities have emerged as important competitors to performance.\nNevertheless, all systems have performance requirements, even if they are \nnot expressed. For example, a word processing tool may not have any explicit \nperformance requirement, but no doubt everyone would agree that waiting an \n",
      "content_length": 2062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "132 \nPart Two  Quality Attributes\t\n8—Performance\nhour (or a minute, or a second) before seeing a typed character appear on the \nscreen is unacceptable. Performance continues to be a fundamentally important \nquality attribute for all software.\nPerformance is often linked to scalability—that is, increasing your system’s \ncapacity for work, while still performing well. Technically, scalability is making \nyour system easy to change in a particular way, and so is a kind of modifiability. \nIn addition, we address scalability explicitly in Chapter 12.\n8.1  Performance General Scenario\nA performance scenario begins with an event arriving at the system. Responding \ncorrectly to the event requires resources (including time) to be consumed. While \nthis is happening, the system may be simultaneously servicing other events. \nConcurrency\nConcurrency is one of the more important concepts that an architect must \nunderstand and one of the least-taught in computer science courses. \nConcurrency refers to operations occurring in parallel. For example, sup-\npose there is a thread that executes the statements \nx := 1;\nx++;\nand another thread that executes the same statements. What is the value \nof x after both threads have executed those statements? It could be either \n2 or 3. I leave it to you to figure out how the value 3 could occur—or \nshould I say I interleave it to you?\nConcurrency occurs any time your system creates a new thread, be-\ncause threads, by definition, are independent sequences of control. Multi-\ntasking on your system is supported by independent threads. Multiple users \nare simultaneously supported on your system through the use of threads. \nConcurrency also occurs any time your system is executing on more than \none processor, whether the processors are packaged separately or as \nmulti-core processors. In addition, you must consider concurrency when \nparallel algorithms, parallelizing infrastructures such as map-reduce, or \nNoSQL databases are used by your system, or you utilize one of a variety \nof concurrent scheduling algorithms. In other words, concurrency is a tool \navailable to you in many ways.\nConcurrency, when you have multiple CPUs or wait states that can \nexploit it, is a good thing. Allowing operations to occur in parallel improves \nperformance, because delays introduced in one thread allow the processor \n",
      "content_length": 2356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "8.1  Performance General Scenario\n133\nto progress on another thread. But because of the interleaving phenome-\nnon just described (referred to as a race condition), concurrency must also \nbe carefully managed by the architect.\nAs the example shows, race conditions can occur when there are two \nthreads of control and there is shared state. The management of con-\ncurrency frequently comes down to managing how state is shared. One \ntechnique for preventing race conditions is to use locks to enforce sequen-\ntial access to state. Another technique is to partition the state based on the \nthread executing a portion of code. That is, if there are two instances of x in \nour example, x is not shared by the two threads and there will not be a race \ncondition.\nRace conditions are one of the hardest types of bugs to discover; the \noccurrence of the bug is sporadic and depends on (possibly minute) differ-\nences in timing. I once had a race condition in an operating system that I \ncould not track down. I put a test in the code so that the next time the race \ncondition occurred, a debugging process was triggered. It took over a year \nfor the bug to recur so that the cause could be determined.\nDo not let the difficulties associated with concurrency dissuade you from \nutilizing this very important technique. Just use it with the knowledge that \nyou must carefully identify critical sections in your code and ensure that \nrace conditions will not occur in those sections.\n—LB\nEvents can arrive in predictable patterns or mathematical distributions, or be \nunpredictable. An arrival pattern for events is characterized as periodic, stochastic, \nor sporadic:\n■\n■Periodic events arrive predictably at regular time intervals. For instance, an \nevent may arrive every 10 milliseconds. Periodic event arrival is most often \nseen in real-time systems. \n■\n■Stochastic arrival means that events arrive according to some probabilistic \ndistribution. \n■\n■Sporadic events arrive according to a pattern that is neither periodic \nnor stochastic. Even these can be characterized, however, in certain \ncircumstances. For example, we might know that at most 600 events will \noccur in a minute, or that there will be at least 200 milliseconds between \nthe arrival of any two events. (This might describe a system in which events \ncorrespond to keyboard strokes from a human user.) These are helpful \ncharacterizations, even though we don’t know when any single event will \narrive. \nThe response of the system to a stimulus can be measured by the following: \n■\n■Latency. The time between the arrival of the stimulus and the system’s \nresponse to it. \n",
      "content_length": 2634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "134 \nPart Two  Quality Attributes\t\n8—Performance\n■\n■Deadlines in processing. In the engine controller, for example, the fuel \nshould ignite when the cylinder is in a particular position, thus introducing \na processing deadline. \n■\n■The throughput of the system, usually given as the number of transactions \nthe system can process in a unit of time. \n■\n■The jitter of the response—the allowable variation in latency. \n■\n■The number of events not processed because the system was too busy to \nrespond. \nFrom these considerations we can now describe the individual portions of a \ngeneral scenario for performance:\n■\n■Source of stimulus. The stimuli arrive either from external (possibly \nmultiple) or internal sources. \n■\n■Stimulus. The stimuli are the event arrivals. The arrival pattern can be peri-\nodic, stochastic, or sporadic, characterized by numeric parameters. \n■\n■Artifact. The artifact is the system or one or more of its components.\n■\n■Environment. The system can be in various operational modes, such as nor-\nmal, emergency, peak load, or overload.\n■\n■Response. The system must process the arriving events. This may cause a \nchange in the system environment (e.g., from normal to overload mode). \n■\n■Response measure. The response measures are the time it takes to process \nthe arriving events (latency or a deadline), the variation in this time (jitter), \nthe number of events that can be processed within a particular time interval \n(throughput), or a characterization of the events that cannot be processed \n(miss rate).\nThe general scenario for performance is summarized in Table 8.1.\nFigure 8.1 gives an example concrete performance scenario: Users initiate \ntransactions under normal operations. The system processes the transactions with \nan average latency of two seconds.\nTable 8.1  Performance General Scenario\nPortion of Scenario\nPossible Values\nSource\nInternal or external to the system \nStimulus\nArrival of a periodic, sporadic, or stochastic event\nArtifact\nSystem or one or more components in the system\nEnvironment\nOperational mode: normal, emergency, peak load, overload\nResponse\nProcess events, change level of service\nResponse Measure\nLatency, deadline, throughput, jitter, miss rate\n",
      "content_length": 2212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "8.2  Tactics for Performance\n135\n8.2  Tactics for Performance\nThe goal of performance tactics is to generate a response to an event arriving \nat the system within some time-based constraint. The event can be single or a \nstream and is the trigger to perform computation. Performance tactics control the \ntime within which a response is generated, as illustrated in Figure 8.2. \nAt any instant during the period after an event arrives but before the sys-\ntem’s response t`o it is complete, either the system is working to respond to that \nevent or the processing is blocked for some reason. This leads to the two basic \ncontributors to the response time: processing time (when the system is working to \nrespond) and blocked time (when the system is unable to respond).\nStimulus:\nInitiate\nTransactions\nresponse:\nTransactions\nAre Processed\nresponse\nMeasure:\nAverage\nLatency\nof Two \nSource:\nUsers\nartifact:\nSystem\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSeconds\nFigure 8.1  Sample concrete performance scenario\nEvent\nArrives\nResponse\nGenerated\nwithin Time\nConstraints\nTactics\nto Control\nPerformance\nFigure 8.2  The goal of performance tactics\n",
      "content_length": 1137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "136 \nPart Two  Quality Attributes\t\n8—Performance\n■\n■Processing time. Processing consumes resources, which takes time. Events \nare handled by the execution of one or more components, whose time \nexpended is a resource. Hardware resources include CPU, data stores, \nnetwork communication bandwidth, and memory. Software resources \ninclude entities defined by the system under design. For example, buffers \nmust be managed and access to critical sections1 must be made sequential. \nFor example, suppose a message is generated by one component. It \nmight be placed on the network, after which it arrives at another compo-\nnent. It is then placed in a buffer; transformed in some fashion; processed \naccording to some algorithm; transformed for output; placed in an output \nbuffer; and sent onward to another component, another system, or some \nactor. Each of these steps consumes resources and time and contributes to \nthe overall latency of the processing of that event.\nDifferent resources behave differently as their utilization approaches \ntheir capacity—that is, as they become saturated. For example, as a CPU \nbecomes more heavily loaded, performance usually degrades fairly steadily. \nOn the other hand, when you start to run out of memory, at some point the \npage swapping becomes overwhelming and performance crashes suddenly.\n■\n■Blocked time. A computation can be blocked because of contention for some \nneeded resource, because the resource is unavailable, or because the compu-\ntation depends on the result of other computations that are not yet available:\n■\n■Contention for resources. Many resources can only be used by a single \nclient at a time. This means that other clients must wait for access to \nthose resources. Figure 8.2 shows events arriving at the system. These \nevents may be in a single stream or in multiple streams. Multiple streams \nvying for the same resource or different events in the same stream vying \nfor the same resource contribute to latency. The more contention for a \nresource, the more likelihood of latency being introduced. \n■\n■Availability of resources. Even in the absence of contention, computation \ncannot proceed if a resource is unavailable. Unavailability may be caused \nby the resource being offline or by failure of the component or for some \nother reason. In any case, you must identify places where resource un-\navailability might cause a significant contribution to overall latency. Some \nof our tactics are intended to deal with this situation.\n■\n■Dependency on other computation. A computation may have to wait \nbecause it must synchronize with the results of another computation or \nbecause it is waiting for the results of a computation that it initiated. If a \ncomponent calls another component and must wait for that component to \nrespond, the time can be significant if the called component is at the other \nend of a network (as opposed to co-located on the same processor).\n1.   A critical section is a section of code in a multi-threaded system in which at most one thread may \nbe active at any time.\n",
      "content_length": 3063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "8.2  Tactics for Performance\n137\nWith this background, we turn to our tactic categories. We can either reduce \ndemand for resources or make the resources we have handle the demand more \neffectively: \n■\n■Control resource demand. This tactic operates on the demand side to \nproduce smaller demand on the resources that will have to service the \nevents.\n■\n■Manage resources. This tactic operates on the response side to make the re-\nsources at hand work more effectively in handling the demands put to them.\nControl Resource Demand\nOne way to increase performance is to carefully manage the demand for re-\nsources. This can be done by reducing the number of events processed by en-\nforcing a sampling rate, or by limiting the rate at which the system responds to \nevents. In addition, there are a number of techniques for ensuring that the re-\nsources that you do have are applied judiciously:\n■\n■Manage sampling rate. If it is possible to reduce the sampling frequency \nat which a stream of environmental data is captured, then demand can be \nreduced, typically with some attendant loss of fidelity. This is common \nin signal processing systems where, for example, different codecs can be \nchosen with different sampling rates and data formats. This design choice \nis made to maintain predictable levels of latency; you must decide whether \nhaving a lower fidelity but consistent stream of data is preferable to losing \npackets of data.\n■\n■Limit event response. When discrete events arrive at the system (or element) \ntoo rapidly to be processed, then the events must be queued until they can \nbe processed. Because these events are discrete, it is typically not desirable \nto “downsample” them. In such a case, you may choose to process events \nonly up to a set maximum rate, thereby ensuring more predictable process-\ning when the events are actually processed. This tactic could be triggered \nby a queue size or processor utilization measure exceeding some warning \nlevel. If you adopt this tactic and it is unacceptable to lose any events, then \nyou must ensure that your queues are large enough to handle the worst case. \nIf, on the other hand, you choose to drop events, then you need to choose a \npolicy for handling this situation: Do you log the dropped events, or simply \nignore them? Do you notify other systems, users, or administrators?\n■\n■Prioritize events. If not all events are equally important, you can impose a \npriority scheme that ranks events according to how important it is to service \nthem. If there are not enough resources available to service them when they \narise, low-priority events might be ignored. Ignoring events consumes min-\nimal resources (including time), and thus increases performance compared \nto a system that services all events all the time. For example, a building \n",
      "content_length": 2810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "138 \nPart Two  Quality Attributes\t\n8—Performance\nmanagement system may raise a variety of alarms. Life-threatening alarms \nsuch as a fire alarm should be given higher priority than informational \nalarms such as a room is too cold.\n■\n■Reduce overhead. The use of intermediaries (so important for modifiability, \nas we saw in Chapter 7) increases the resources consumed in processing \nan event stream, and so removing them improves latency. This is a clas-\nsic modifiability/performance tradeoff. Separation of concerns, another \nlinchpin of modifiability, can also increase the processing overhead nec-\nessary to service an event if it leads to an event being serviced by a chain \nof components rather than a single component. The context switching and \nintercomponent communication costs add up, especially when the compo-\nnents are on different nodes on a network. A strategy for reducing compu-\ntational overhead is to co-locate resources. Co-location may mean hosting \ncooperating components on the same processor to avoid the time delay of \nnetwork communication; it may mean putting the resources in the same \nruntime software component to avoid even the expense of a subroutine call. \nA special case of reducing computational overhead is to perform a periodic \ncleanup of resources that have become inefficient. For example, hash tables \nand virtual memory maps may require recalculation and reinitialization. \nAnother common strategy is to execute single-threaded servers (for simplic-\nity and avoiding contention) and split workload across them. \n■\n■Bound execution times. Place a limit on how much execution time is used to \nrespond to an event. For iterative, data-dependent algorithms, limiting the \nnumber of iterations is a method for bounding execution times. The cost is \nusually a less accurate computation. If you adopt this tactic, you will need \nto assess its effect on accuracy and see if the result is “good enough.” This \nresource management tactic is frequently paired with the manage sampling \nrate tactic.\n■\n■Increase resource efficiency. Improving the algorithms used in critical areas \nwill decrease latency. \nManage Resources\nEven if the demand for resources is not controllable, the management of these re-\nsources can be. Sometimes one resource can be traded for another. For example, \nintermediate data may be kept in a cache or it may be regenerated depending on \ntime and space resource availability. This tactic is usually applied to the proces-\nsor but is also effective when applied to other resources such as a disk. Here are \nsome resource management tactics:\n■\n■Increase resources. Faster processors, additional processors, additional \nmemory, and faster networks all have the potential for reducing latency. \n",
      "content_length": 2749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "8.2  Tactics for Performance\n139\nCost is usually a consideration in the choice of resources, but increasing the \nresources is definitely a tactic to reduce latency and in many cases is the \ncheapest way to get immediate improvement. \n■\n■Introduce concurrency. If requests can be processed in parallel, the blocked \ntime can be reduced. Concurrency can be introduced by processing differ-\nent streams of events on different threads or by creating additional threads \nto process different sets of activities. Once concurrency has been intro-\nduced, scheduling policies can be used to achieve the goals you find desir-\nable. Different scheduling policies may maximize fairness (all requests get \nequal time), throughput (shortest time to finish first), or other goals. (See \nthe sidebar.)\n■\n■Maintain multiple copies of computations. Multiple servers in a client-serv-\ner pattern are replicas of computation. The purpose of replicas is to reduce \nthe contention that would occur if all computations took place on a single \nserver. A load balancer is a piece of software that assigns new work to one \nof the available duplicate servers; criteria for assignment vary but can be as \nsimple as round-robin or assigning the next request to the least busy server. \n■\n■Maintain multiple copies of data. Caching is a tactic that involves keeping \ncopies of data (possibly one a subset of the other) on storage with different \naccess speeds. The different access speeds may be inherent (memory versus \nsecondary storage) or may be due to the necessity for network communica-\ntion. Data replication involves keeping separate copies of the data to reduce \nthe contention from multiple simultaneous accesses. Because the data being \ncached or replicated is usually a copy of existing data, keeping the copies \nconsistent and synchronized becomes a responsibility that the system must \nassume. Another responsibility is to choose the data to be cached. Some \ncaches operate by merely keeping copies of whatever was recently request-\ned, but it is also possible to predict users’ future requests based on patterns \nof behavior, and begin the calculations or prefetches necessary to comply \nwith those requests before the user has made them.\n■\n■Bound queue sizes. This controls the maximum number of queued arrivals \nand consequently the resources used to process the arrivals. If you adopt \nthis tactic, you need to adopt a policy for what happens when the queues \noverflow and decide if not responding to lost events is acceptable. This tac-\ntic is frequently paired with the limit event response tactic.\n■\n■Schedule resources. Whenever there is contention for a resource, the \nresource must be scheduled. Processors are scheduled, buffers are \nscheduled, and networks are scheduled. Your goal is to understand the \ncharacteristics of each resource’s use and choose the scheduling strategy \nthat is compatible with it. (See the sidebar.)\nThe tactics for performance are summarized in Figure 8.3.\n",
      "content_length": 2980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "140 \nPart Two  Quality Attributes\t\n8—Performance\nScheduling Policies\nA scheduling policy conceptually has two parts: a priority assignment \nand dispatching. All scheduling policies assign priorities. In some cases \nthe assignment is as simple as first-in/first-out (or FIFO). In other cases, \nit can be tied to the deadline of the request or its semantic importance. \nCompeting criteria for scheduling include optimal resource usage, request \nimportance, minimizing the number of resources used, minimizing latency, \nmaximizing throughput, preventing starvation to ensure fairness, and so \nforth. You need to be aware of these possibly conflicting criteria and the \neffect that the chosen tactic has on meeting them.\nA high-priority event stream can be dispatched only if the resource to \nwhich it is being assigned is available. Sometimes this depends on pre-\nempting the current user of the resource. Possible preemption options are \nas follows: can occur anytime, can occur only at specific preemption points, \nand executing processes cannot be preempted. Some common scheduling \npolicies are these:\n■\n■\nFirst-in/first-out. FIFO queues treat all requests for resources as equals \nand satisfy them in turn. One possibility with a FIFO queue is that one \nrequest will be stuck behind another one that takes a long time to gener-\nate a response. As long as all of the requests are truly equal, this is not \na problem, but if some requests are of higher priority than others, it is \nproblematic.\n■\n■\nFixed-priority scheduling. Fixed-priority scheduling assigns each source \nof resource requests a particular priority and assigns the resources in \nthat priority order. This strategy ensures better service for higher priority \nrequests. But it admits the possibility of a lower priority, but important, \nrequest taking an arbitrarily long time to be serviced, because it is stuck \nbehind a series of higher priority requests. Three common prioritization \nstrategies are these:\n■\n■\nSemantic importance. Each stream is assigned a priority statically \naccording to some domain characteristic of the task that generates it. \n■\n■\nDeadline monotonic. Deadline monotonic. Deadline monotonic is a \nstatic priority assignment that assigns higher priority to streams with \nshorter deadlines. This scheduling policy is used when streams of \ndifferent priorities with real-time deadlines are to be scheduled. \n■\n■\nRate monotonic. Rate monotonic is a static priority assignment \nfor periodic streams that assigns higher priority to streams with \nshorter periods. This scheduling policy is a special case of deadline \nmonotonic but is better known and more likely to be supported by the \noperating system. \n■\n■\nDynamic priority scheduling. Strategies include these:\n■\n■\nRound-robin. Round-robin is a scheduling strategy that orders \nthe requests and then, at every assignment possibility, assigns \nthe resource to the next request in that order. A special form of \n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "8.2  Tactics for Performance\n141\nround-robin is a cyclic executive, where assignment possibilities are \nat fixed time intervals.\n■\n■\nEarliest-deadline-first. Earliest-deadline-first. Earliest-deadline-first \nassigns priorities based on the pending requests with the earliest \ndeadline.\n■\n■\nLeast-slack-first. This strategy assigns the highest priority to the job \nhaving the least “slack time,” which is the difference between the exe-\ncution time remaining and the time to the job’s deadline.\nFor a single processor and processes that are preemptible (that is, it is \npossible to suspend processing of one task in order to service a task \nwhose deadline is drawing near), both the earliest-deadline and least-\nslack scheduling strategies are optimal. That is, if the set of processes can \nbe scheduled so that all deadlines are met, then these strategies will be \nable to schedule that set successfully.\n■\n■\nStatic scheduling. A cyclic executive schedule is a scheduling strategy \nwhere the preemption points and the sequence of assignment to the \nresource are determined offline. The runtime overhead of a scheduler is \nthereby obviated.\nPerformance Tactics\nControl Resource Demand\nManage Resources\nManage Sampling Rate\nLimit Event Response\nPrioritize Events\nReduce Overhead\nBound Execution Times\nIncrease Resource\nEfficiency\nEvent \nArrives\nResponse\nGenerated within\nTime Constraints\nIncrease Resources\nIntroduce Concurrency\nMaintain Multiple\nCopies of Computations\nMaintain Multiple\nCopies of Data\nBound Queue Sizes\nSchedule Resources\nFigure 8.3  Performance tactics\n",
      "content_length": 1570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "142 \nPart Two  Quality Attributes\t\n8—Performance\nPerformance Tactics on the Road\nTactics are generic design principles. To exercise this point, think about \nthe design of the systems of roads and highways where you live. Traffic \nengineers employ a bunch of design “tricks” to optimize the performance \nof these complex systems, where performance has a number of mea-\nsures, such as throughput (how many cars per hour get from the suburbs \nto the football stadium), average-case latency (how long it takes, on aver-\nage, to get from your house to downtown), and worst-case latency (how \nlong does it take an emergency vehicle to get you to the hospital). What \nare these tricks? None other than our good old buddies, tactics.\nLet’s consider some examples:\n■\n■\nManage event rate. Lights on highway entrance ramps let cars onto the \nhighway only at set intervals, and cars must wait (queue) on the ramp for \ntheir turn.\n■\n■\nPrioritize events. Ambulances and police, with their lights and sirens \ngoing, have higher priority than ordinary citizens; some highways have \nhigh-occupancy vehicle (HOV) lanes, giving priority to vehicles with two \nor more occupants.\n■\n■\nMaintain multiple copies. Add traffic lanes to existing roads, or build \nparallel routes.\nIn addition, there are some tricks that users of the system can employ:\n■\n■\nIncrease resources. Buy a Ferrari, for example. All other things being \nequal, the fastest car with a competent driver on an open road will get \nyou to your destination more quickly.\n■\n■\nIncrease efficiency. Find a new route that is quicker and/or shorter than \nyour current route.\n■\n■\nReduce computational overhead. You can drive closer to the car in \nfront of you, or you can load more people into the same vehicle (that is, \ncarpooling).\nWhat is the point of this discussion? To paraphrase Gertrude Stein: per-\nformance is performance is performance. Engineers have been analyzing \nand optimizing systems for centuries, trying to improve their performance, \nand they have been employing the same design strategies to do so. So \nyou should feel some comfort in knowing that when you try to improve the \nperformance of your computer-based system, you are applying tactics that \nhave been thoroughly “road tested.” \n—RK\n8.3  A Design Checklist for Performance\nTable 8.2 is a checklist to support the design and analysis process for performance.\n",
      "content_length": 2374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "8.3  A Design Checklist for Performance\n143\nTable 8.2  Checklist to Support the Design and Analysis Process for \nPerformance\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine the system’s responsibilities that will involve heavy \nloading, have time-critical response requirements, are heavily \nused, or impact portions of the system where heavy loads or \ntime-critical events occur. \nFor those responsibilities, identify the processing requirements \nof each responsibility, and determine whether they may cause \nbottlenecks.\nAlso, identify additional responsibilities to recognize and process \nrequests appropriately, including\n■\n■\nResponsibilities that result from a thread of control crossing \nprocess or processor boundaries\n■\n■\nResponsibilities to manage the threads of control—allocation \nand deallocation of threads, maintaining thread pools, and so \nforth\n■\n■\nResponsibilities for scheduling shared resources or \nmanaging performance-related artifacts such as queues, \nbuffers, and caches\nFor the responsibilities and resources you identified, ensure that \nthe required performance response can be met (perhaps by \nbuilding a performance model to help in the evaluation).\nCoordination  \nModel\nDetermine the elements of the system that must coordinate with \neach other—directly or indirectly—and choose communication \nand coordination mechanisms that do the following:\n■\n■\nSupport any introduced concurrency (for example, is it thread \nsafe?), event prioritization, or scheduling strategy\n■\n■\nEnsure that the required performance response can be \ndelivered\n■\n■\nCan capture periodic, stochastic, or sporadic event arrivals, \nas needed \n■\n■\nHave the appropriate properties of the communication \nmechanisms; for example, stateful, stateless, synchronous, \nasynchronous, guaranteed delivery, throughput, or latency\nData Model\nDetermine those portions of the data model that will be heavily \nloaded, have time-critical response requirements, are heavily \nused, or impact portions of the system where heavy loads or \ntime-critical events occur. \nFor those data abstractions, determine the following:\n■\n■\nWhether maintaining multiple copies of key data would \nbenefit performance\n■\n■\nWhether partitioning data would benefit performance\n■\n■\nWhether reducing the processing requirements for the \ncreation, initialization, persistence, manipulation, translation, \nor destruction of the enumerated data abstractions is \npossible\n■\n■\nWhether adding resources to reduce bottlenecks for the \ncreation, initialization, persistence, manipulation, translation, \nor destruction of the enumerated data abstractions is feasible\ncontinues\n",
      "content_length": 2635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "144 \nPart Two  Quality Attributes\t\n8—Performance\nTable 8.2  Checklist to Support the Design and Analysis Process for \nPerformance, continued\nCategory\nChecklist\nMapping among \nArchitectural \nElements\nWhere heavy network loading will occur, determine whether \nco-locating some components will reduce loading and improve \noverall efficiency.\nEnsure that components with heavy computation requirements \nare assigned to processors with the most processing capacity. \nDetermine where introducing concurrency (that is, allocating \na piece of functionality to two or more copies of a component \nrunning simultaneously) is feasible and has a significant positive \neffect on performance.\nDetermine whether the choice of threads of control and their \nassociated responsibilities introduces bottlenecks. \nResource \nManagement\nDetermine which resources in your system are critical for \nperformance. For these resources, ensure that they will be \nmonitored and managed under normal and overloaded system \noperation. For example: \n■\n■\nSystem elements that need to be aware of, and manage, \ntime and other performance-critical resources\n■\n■\nProcess/thread models \n■\n■\nPrioritization of resources and access to resources \n■\n■\nScheduling and locking strategies \n■\n■\nDeploying additional resources on demand to meet increased \nloads\nBinding Time\nFor each element that will be bound after compile time, \ndetermine the following:\n■\n■\nTime necessary to complete the binding\n■\n■\nAdditional overhead introduced by using the late binding \nmechanism\nEnsure that these values do not pose unacceptable performance \npenalties on the system.\nChoice of \nTechnology\nWill your choice of technology let you set and meet hard, real-\ntime deadlines? Do you know its characteristics under load and \nits limits?\nDoes your choice of technology give you the ability to set the \nfollowing:\n■\n■\nScheduling policy\n■\n■\nPriorities\n■\n■\nPolicies for reducing demand\n■\n■\nAllocation of portions of the technology to processors\n■\n■\nOther performance-related parameters\nDoes your choice of technology introduce excessive overhead \nfor heavily used operations?\n",
      "content_length": 2109,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "8.6  Discussion Questions\n145\n8.4  Summary\nPerformance is about the management of system resources in the face of partic-\nular types of demand to achieve acceptable timing behavior. Performance can be \nmeasured in terms of throughput and latency for both interactive and embedded \nreal-time systems, although throughput is usually more important in interactive \nsystems, and latency is more important in embedded systems.\nPerformance can be improved by reducing demand or by managing re-\nsources more appropriately. Reducing demand will have the side effect of re-\nducing fidelity or refusing to service some requests. Managing resources more \nappropriately can be done through scheduling, replication, or just increasing the \nresources available.\n8.5  For Further Reading\nPerformance has a rich body of literature. Here are some books we recommend:\n■\n■Software Performance and Scalability: A Quantitative Approach [Liu 09]. \nThis books covers performance geared toward enterprise applications, with \nan emphasis on queuing theory and measurement.\n■\n■Performance Solutions: A Practical Guide to Creating Responsive, Scal-\nable Software [Smith 01]. This book covers designing with performance in \nmind, with emphasis on building (and populating with real data) practical \npredictive performance models.\n■\n■Real-Time Design Patterns: Robust Scalable Architecture for Real-Time \nSystems [Douglass 99].\n■\n■Real-Time Systems [Liu 00]. \n■\n■Pattern-Oriented Software Architecture Volume 3: Patterns for Resource \nManagement [Kircher 03].\n8.6  Discussion Questions\n1.\t\n“Every system has real-time performance constraints.” Discuss. Or provide \na counterexample.\n2.\t\nWrite a performance scenario that describes the average on-time flight ar-\nrival performance for an airline.\n",
      "content_length": 1767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "146 \nPart Two  Quality Attributes\t\n8—Performance\n3.\t\nWrite several performance scenarios for an automatic teller machine. Think \nabout whether your major concern is worst-case latency, average-case la-\ntency, throughput, or some other response measure. How would you modify \nyour automatic teller machine design to accommodate these scenarios?\n4.\t\nWeb-based systems often use proxy servers, which are the first element of \nthe system to receive a request from a client (such as your browser). Proxy \nservers are able to serve up often-requested web pages, such as a company’s \nhome page, without bothering the real application servers that carry out \ntransactions. There may be many proxy servers, and they are often located \ngeographically close to large user communities, to decrease response time \nfor routine requests. What performance tactics do you see at work here?\n5.\t\n A fundamental difference between coordination mechanisms is whether \ninteraction is synchronous or asynchronous. Discuss the advantages and \ndisadvantages of each with respect to each of the performance responses: \nlatency, deadline, throughput, jitter, miss rate, data loss, or any other re-\nquired performance-related response you may be used to.\n6.\t\nFind real-world (that is, nonsoftware) examples of applying each of the \nmanage-resources tactics. For example, suppose you were managing a \nbrick-and-mortar big-box retail store. How would you get people through \nthe checkout lines faster using these tactics?\n7.\t\nUser interface frameworks typically are single-threaded. Why is this so and \nwhat are the performance implications of this single-threading?\n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "147\n9\nSecurity\nWith Jungwoo Ryoo and Phil Laplante \nYour personal identity isn’t worth quite as much as \nit used to be—at least to thieves willing to swipe it. \nAccording to experts who monitor such markets, the \nvalue of stolen credit card data may range from $3 to \nas little as 40 cents. That’s down tenfold from a decade \nago—even though the cost to an individual who has a \ncredit card stolen can soar into the hundreds of dollars.\n—Forbes.com (Taylor Buley. “Hackonomics,” Forbes.com, \nOctober 27, 2008, www.forbes.com/2008/10/25/credit-card-\ntheft-tech-security-cz_tb1024theft.html)\nSecurity is a measure of the system’s ability to protect data and information from \nunauthorized access while still providing access to people and systems that are \nauthorized. An action taken against a computer system with the intention of do-\ning harm is called an attack and can take a number of forms. It may be an un-\nauthorized attempt to access data or services or to modify data, or it may be in-\ntended to deny services to legitimate users.\nThe simplest approach to characterizing security has three characteristics: \nconfidentiality, integrity, and availability (CIA):\n1.\t\nConfidentiality is the property that data or services are protected from \nunauthorized access. For example, a hacker cannot access your income tax \nreturns on a government computer.\n2.\t\nIntegrity is the property that data or services are not subject to unauthorized \nmanipulation. For example, your grade has not been changed since your \ninstructor assigned it.\n3.\t\nAvailability is the property that the system will be available for legitimate \nuse. For example, a denial-of-service attack won’t prevent you from order-\ning  book from an online bookstore.\n",
      "content_length": 1729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "148 \nPart Two  Quality Attributes\t\n9—Security\nOther characteristics that are used to support CIA are these:\n4.\t\nAuthentication verifies the identities of the parties to a transaction and \nchecks if they are truly who they claim to be. For example, when you get \nan email purporting to come from a bank, authentication guarantees that it \nactually comes from the bank.\n5.\t\nNonrepudiation guarantees that the sender of a message cannot later deny \nhaving sent the message, and that the recipient cannot deny having received \nthe message. For example, you cannot deny ordering something from the \nInternet, or the merchant cannot disclaim getting your order.\n6.\t\nAuthorization grants a user the privileges to perform a task. For example, an \nonline banking system authorizes a legitimate user to access his account.\nWe will use these characteristics in our general scenarios for security. Approaches \nto achieving security can be characterized as those that detect attacks, those that \nresist attacks, those that react to attacks, and those that recover from successful \nattacks. The objects that are being protected from attacks are data at rest, data in \ntransit, and computational processes.\n9.1  Security General Scenario\nOne technique that is used in the security domain is threat modeling. An “attack \ntree,” similar to a fault tree discussed in Chapter 5, is used by security engineers \nto determine possible threats. The root is a successful attack and the nodes are \npossible direct causes of that successful attack. Children nodes decompose the \ndirect causes, and so forth. An attack is an attempt to break CIA, and the leaves of \nattack trees would be the stimulus in the scenario. The response to the attack is to \npreserve CIA or deter attackers through monitoring of their activities. From these \nconsiderations we can now describe the individual portions of a security general \nscenario. These are summarized in Table 9.1, and an example security scenario is \ngiven in Figure 9.1.\n■\n■Source of stimulus. The source of the attack may be either a human or \nanother system. It may have been previously identified (either correctly or \nincorrectly) or may be currently unknown. A human attacker may be from \noutside the organization or from inside the organization. \n■\n■Stimulus. The stimulus is an attack. We characterize this as an unauthorized \nattempt to display data, change or delete data, access system services, \nchange the system’s behavior, or reduce availability.\n■\n■Artifact. The target of the attack can be either the services of the system, \nthe data within it, or the data produced or consumed by the system. Some \nattacks are made on particular components of the system known to be \nvulnerable. \n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "9.1  Security General Scenario\n149\n■\n■Environment. The attack can come when the system is either online or \noffline, either connected to or disconnected from a network, either behind a \nfirewall or open to a network, fully operational, partially operational, or not \noperational.\n■\n■Response. The system should ensure that transactions are carried out in a \nfashion such that data or services are protected from unauthorized access; \ndata or services are not being manipulated without authorization; parties \nto a transaction are identified with assurance; the parties to the transaction \ncannot repudiate their involvements; and the data, resources, and system \nservices will be available for legitimate use. \nThe system should also track activities within it by recording access \nor modification; attempts to access data, resources, or services; and noti-\nfying appropriate entities (people or systems) when an apparent attack is \noccurring.\n■\n■Response measure. Measures of a system’s response include how much \nof a system is compromised when a particular component or data value is \ncompromised, how much time passed before an attack was detected, how \nmany attacks were resisted, how long it took to recover from a successful \nattack, and how much data was vulnerable to a particular attack.\nTable 9.1 enumerates the elements of the general scenario, which charac-\nterize security, and Figure 9.1 shows a sample concrete scenario: A disgruntled \nemployee from a remote location attempts to modify the pay rate table during \nnormal operations. The system maintains an audit trail, and the correct data is \nrestored within a day. \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nNormal\nOperations\n3\n2\n1\n4\nDisgruntled \nEmployee from \nRemote Location\nAttempts to \nModify Pay \nRate\nSystem \nMaintains \nAudit Trail\nCorrect Data Is \nRestored within a\nDay and Source \nof Tampering \nIdentified\nArtifact:\nData within\nthe System\nFigure 9.1  Sample concrete security scenario \n",
      "content_length": 1981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "150 \nPart Two  Quality Attributes\t\n9—Security\nTable 9.1  Security General Scenario\nPortion of \nScenario\nPossible Values\nSource\nHuman or another system which may have been previously \nidentified (either correctly or incorrectly) or may be currently \nunknown. A human attacker may be from outside the organization or \nfrom inside the organization.\nStimulus\nUnauthorized attempt is made to display data, change or delete \ndata, access system services, change the system’s behavior, or \nreduce availability.\nArtifact\nSystem services, data within the system, a component or resources \nof the system, data produced or consumed by the system\nEnvironment\nThe system is either online or offline; either connected to or \ndisconnected from a network; either behind a firewall or open to a \nnetwork; fully operational, partially operational, or not operational.\nResponse\nTransactions are carried out in a fashion such that \n■\n■\nData or services are protected from unauthorized access. \n■\n■\nData or services are not being manipulated without authorization.\n■\n■\nParties to a transaction are identified with assurance. \n■\n■\nThe parties to the transaction cannot repudiate their \ninvolvements. \n■\n■\nThe data, resources, and system services will be available for \nlegitimate use. \nThe system tracks activities within it by\n■\n■\nRecording access or modification \n■\n■\nRecording attempts to access data, resources, or services \n■\n■\nNotifying appropriate entities (people or systems) when an \napparent attack is occurring\nResponse \nMeasure\nOne or more of the following:\n■\n■\nHow much of a system is compromised when a particular \ncomponent or data value is compromised\n■\n■\nHow much time passed before an attack was detected \n■\n■\nHow many attacks were resisted \n■\n■\nHow long does it take to recover from a successful attack \n■\n■\nHow much data is vulnerable to a particular attack\n9.2  Tactics for Security\nOne method for thinking about how to achieve security in a system is to think \nabout physical security. Secure installations have limited access (e.g., by using \nsecurity checkpoints), have means of detecting intruders (e.g., by requiring le-\ngitimate visitors to wear badges), have deterrence mechanisms such as armed \nguards, have reaction mechanisms such as automatic locking of doors, and have \nrecovery mechanisms such as off-site backup. These lead to our four categories \nof tactics: detect, resist, react, and recover. Figure 9.2 shows these categories as \nthe goal of security tactics.\n",
      "content_length": 2477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "9.2  Tactics for Security\n151\nAttack\nSystem Detects, Resists,\nReacts, or Recovers \nTactics\nto Control\nSecurity\nFigure 9.2  The goal of security tactics \nDetect Attacks\nThe detect attacks category consists of four tactics: detect intrusion, detect service \ndenial, verify message integrity, and detect message delay.\n■\n■Detect intrusion is the comparison of network traffic or service request \npatterns within a system to a set of signatures or known patterns of \nmalicious behavior stored in a database. The signatures can be based on \nprotocol, TCP flags, payload sizes, applications, source or destination \naddress, or port number.\n■\n■Detect service denial is the comparison of the pattern or signature of \nnetwork traffic coming into a system to historic profiles of known denial-of-\nservice attacks. \n■\n■Verify message integrity. This tactic employs techniques such as \nchecksums or hash values to verify the integrity of messages, resource \nfiles, deployment files, and configuration files. A checksum is a validation \nmechanism wherein the system maintains redundant information for \nconfiguration files and messages, and uses this redundant information \nto verify the configuration file or message when it is used. A hash value \nis a unique string generated by a hashing function whose input could be \nconfiguration files or messages. Even a slight change in the original files or \nmessages results in a significant change in the hash value.\n■\n■Detect message delay is intended to detect potential man-in-the-middle \nattacks, where a malicious party is intercepting (and possibly modifying) \nmessages. By checking the time that it takes to deliver a message, it is \npossible to detect suspicious timing behavior, where the time it takes to \ndeliver a message is highly variable.\n",
      "content_length": 1786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "152 \nPart Two  Quality Attributes\t\n9—Security\nResist Attacks\nThere are a number of well-known means of resisting an attack:\n■\n■Identify actors. Identifying “actors” is really about identifying the source of \nany external input to the system. Users are typically identified through user \nIDs. Other systems may be “identified” through access codes, IP addresses, \nprotocols, ports, and so on.\n■\n■Authenticate actors. Authentication means ensuring that an actor (a user or \na remote computer) is actually who or what it purports to be. Passwords, \none-time passwords, digital certificates, and biometric identification \nprovide a means for authentication.\n■\n■Authorize actors. Authorization means ensuring that an authenticated actor \nhas the rights to access and modify either data or services. This mechanism \nis usually enabled by providing some access control mechanisms within \na system. Access control can be by an actor or by an actor class. Classes \nof actors can be defined by actor groups, by actor roles, or by lists of \nindividuals.\n■\n■Limit access. Limiting access involves controlling what and who may access \nwhich parts of a system. This may include limiting access to resources such \nas processors, memory, and network connections, which may be achieved \nby using process management, memory protection, blocking a host, closing \na port, or rejecting a protocol. For example, a firewall is a single point of \naccess to an organization’s intranet. A demilitarized zone (DMZ) is a subnet \nbetween the Internet and an intranet, protected by two firewalls: one facing \nthe Internet and the other the intranet. A DMZ is used when an organization \nwants to let external users access services that should be publicly available \noutside the intranet. This way the number of open ports in the internal firewall \ncan be minimized. This tactic also limits access for actors (by identifying, \nauthenticating, and authorizing them).\n■\n■Limit exposure. Limiting exposure refers to ultimately and indirectly \nreducing the probability of a successful attack, or restricting the amount of \npotential damage. This can be achieved by concealing facts about a system \nto be protected (“security by obscurity”) or by dividing and distributing \ncritical resources so that the exploitation of a single weakness cannot fully \ncompromise any resource (“don’t put all your eggs in one basket”). For \nexample, a design decision to hide how many entry points a system has is a \nway of limiting exposure. A decision to distribute servers amongst several \ngeographically dispersed data centers is also a way of limiting exposure.\n■\n■Encrypt data. Data should be protected from unauthorized access. \nConfidentiality is usually achieved by applying some form of encryption \nto data and to communication. Encryption provides extra protection to \npersistently maintained data beyond that available from authorization. \nCommunication links, on the other hand, may not have authorization \ncontrols. In such cases, encryption is the only protection for passing data \nover publicly accessible communication links. The link can be implemented \nby a virtual private network (VPN) or by a Secure Sockets Layer (SSL) for \n",
      "content_length": 3194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "9.2  Tactics for Security\n153\na web-based link. Encryption can be symmetric (both parties use the same \nkey) or asymmetric (public and private keys).\n■\n■Separate entities. Separating different entities within the system can be \ndone through physical separation on different servers that are attached \nto different networks; the use of virtual machines (see Chapter 26 for \na discussion of virtual machines); or an “air gap,” that is, by having no \nconnection between different portions of a system. Finally, sensitive \ndata is frequently separated from nonsensitive data to reduce the attack \npossibilities from those who have access to nonsensitive data.\n■\n■Change default settings. Many systems have default settings assigned \nwhen the system is delivered. Forcing the user to change those settings will \nprevent attackers from gaining access to the system through settings that \nare, generally, publicly available.\nReact to Attacks\nSeveral tactics are intended to respond to a potential attack: \n■\n■Revoke access. If the system or a system administrator believes that \nan attack is underway, then access can be severely limited to sensitive \nresources, even for normally legitimate users and uses. For example, if your \ndesktop has been compromised by a virus, your access to certain resources \nmay be limited until the virus is removed from your system.\n■\n■Lock computer. Repeated failed login attempts may indicate a potential \nattack. Many systems limit access from a particular computer if there \nare repeated failed attempts to access an account from that computer. \nLegitimate users may make mistakes in attempting to log in. Therefore, the \nlimited access may only be for a certain time period.\n■\n■Inform actors. Ongoing attacks may require action by operators, other \npersonnel, or cooperating systems. Such personnel or systems—the set of \nrelevant actors—must be notified when the system has detected an attack.\nRecover from Attacks\nOnce a system has detected and attempted to resist an attack, it needs to recover. \nPart of recovery is restoration of services. For example, additional servers or net-\nwork connections may be kept in reserve for such a purpose. Since a successful \nattack can be considered a kind of failure, the set of availability tactics (from \nChapter 5) that deal with recovering from a failure can be brought to bear for this \naspect of security as well. \nIn addition to the availability tactics that permit restoration of services, we \nneed to maintain an audit trail. We audit—that is, keep a record of user and sys-\ntem actions and their effects—to help trace the actions of, and to identify, an at-\ntacker. We may analyze audit trails to attempt to prosecute attackers, or to create \nbetter defenses in the future.\nThe set of security tactics is shown in Figure 9.3.\n",
      "content_length": 2807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "154 \nPart Two  Quality Attributes\t\n9—Security\nSecurity Tactics\nResist Attacks\nEncrypt Data\nAttack\nSystem Detects,\nResists, Reacts,\nor Recovers\nDetect Attacks\nMaintain\nAudit Trail\nLimit Exposure\nRecover\nfrom Attacks\nReact to\nAttacks\nRevoke\nAccess\nLock\nComputer\nDetect\nIntrusion\nDetect Service\nDenial\nVerify Message\nIntegrity\nDetect Message\nDelay\nChange Default\nSettings\nSeparate\nEntities\nRestore\nSee\nAvailability\nIdentify\nActors\nAuthenticate\nActors\nAuthorize\nActors\nLimit Access\nInform\nActors\nFIGURE 9.3  Security tactics\n9.3  A Design Checklist for Security\nTable 9.2 is a checklist to support the design and analysis process for security.\nTABLE 9.2  Checklist to Support the Design and Analysis Process for Security\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which system responsibilities need to be secure. \nFor each of these responsibilities, ensure that additional \nresponsibilities have been allocated to do the following:\n■\n■\nIdentify the actor\n■\n■\nAuthenticate the actor\n■\n■\nAuthorize actors\n■\n■\nGrant or deny access to data or services\n■\n■\nRecord attempts to access or modify data or services\n■\n■\nEncrypt data\n■\n■\nRecognize reduced availability for resources or services and \ninform appropriate personnel and restrict access\n■\n■\nRecover from an attack\n■\n■\nVerify checksums and hash values\n",
      "content_length": 1315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "9.3  A Design Checklist for Security\n155\nCategory\nChecklist\nCoordination \nModel\nDetermine mechanisms required to communicate and coordinate \nwith other systems or individuals. For these communications, \nensure that mechanisms for authenticating and authorizing the \nactor or system, and encrypting data for transmission across \nthe connection, are in place. Ensure also that mechanisms exist \nfor monitoring and recognizing unexpectedly high demands for \nresources or services as well as mechanisms for restricting or \nterminating the connection.\nData Model\nDetermine the sensitivity of different data fields. For each data \nabstraction:\n■\n■\nEnsure that data of different sensitivity is separated.\n■\n■\nEnsure that data of different sensitivity has different access \nrights and that access rights are checked prior to access.\n■\n■\nEnsure that access to sensitive data is logged and that the log \nfile is suitably protected.\n■\n■\nEnsure that data is suitably encrypted and that keys are \nseparated from the encrypted data.\n■\n■\nEnsure that data can be restored if it is inappropriately \nmodified.\nMapping among \nArchitectural \nElements\nDetermine how alternative mappings of architectural elements \nthat are under consideration may change how an individual or \nsystem may read, write, or modify data; access system services or \nresources; or reduce availability to system services or resources. \nDetermine how alternative mappings may affect the recording \nof access to data, services or resources and the recognition of \nunexpectedly high demands for resources.\nFor each such mapping, ensure that there are responsibilities to do \nthe following:\n■\n■\nIdentify an actor\n■\n■\nAuthenticate an actor\n■\n■\nAuthorize actors\n■\n■\nGrant or deny access to data or services\n■\n■\nRecord attempts to access or modify data or services\n■\n■\nEncrypt data\n■\n■\nRecognize reduced availability for resources or services, inform \nappropriate personnel, and restrict access\n■\n■\nRecover from an attack\nResource \nManagement\nDetermine the system resources required to identify and monitor \na system or an individual who is internal or external, authorized or \nnot authorized, with access to specific resources or all resources.\nDetermine the resources required to authenticate the actor, grant \nor deny access to data or resources, notify appropriate entities \n(people or systems), record attempts to access data or resources, \nencrypt data, recognize inexplicably high demand for resources, \ninform users or systems, and restrict access. \nFor these resources consider whether an external entity can \naccess a critical resource or exhaust a critical resource; how to \nmonitor the resource; how to manage resource utilization; how \nto log resource utilization; and ensure that there are sufficient \nresources to perform the necessary security operations.\nEnsure that a contaminated element can be prevented from \ncontaminating other elements.\nEnsure that shared resources are not used for passing sensitive \ndata from an actor with access rights to that data to an actor \nwithout access rights to that data.\ncontinues\n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "156 \nPart Two  Quality Attributes\t\n9—Security\nTable 9.2  Checklist to Support the Design and Analysis Process for Security, \ncontinued\nCategory\nChecklist\nBinding Time\nDetermine cases where an instance of a late-bound component \nmay be untrusted. For such cases ensure that late-bound \ncomponents can be qualified; that is, if ownership certificates \nfor late-bound components are required, there are appropriate \nmechanisms to manage and validate them; that access to \nlate-bound data and services can be managed; that access by \nlate-bound components to data and services can be blocked; that \nmechanisms to record the access, modification, and attempts to \naccess data or services by late-bound components are in place; \nand that system data is encrypted where the keys are intentionally \nwithheld for late-bound components\nChoice of \nTechnology\nDetermine what technologies are available to help user \nauthentication, data access rights, resource protection, and data \nencryption.\nEnsure that your chosen technologies support the tactics relevant \nfor your security needs.\n9.4  Summary\nAttacks against a system can be characterized as attacks against the confidential-\nity, integrity, or availability of a system or its data. Confidentiality means keeping \ndata away from those who should not have access while granting access to those \nwho should. Integrity means that there are no unauthorized modifications to or \ndeletion of data, and availability means that the system is accessible to those who \nare entitled to use it.\nThe emphasis of distinguishing various classes of actors in the characteri-\nzation leads to many of the tactics used to achieve security. Identifying, authen-\nticating, and authorizing actors are tactics intended to determine which users or \nsystems are entitled to what kind of access to a system.\nAn assumption is made that no security tactic is foolproof and that systems \nwill be compromised. Hence, tactics exist to detect an attack, limit the spread of \nany attack, and to react and recover from an attack.\nRecovering from an attack involves many of the same tactics as availability \nand, in general, involves returning the system to a consistent state prior to any attack.\n",
      "content_length": 2208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "9.5  For Further Reading\n157\n9.5  For Further Reading\nThe architectural tactics that we have described in this chapter are only one as-\npect of making a system secure. Other aspects are these:\n■\n■Coding. Secure Coding in C and C++ [Seacord 05] describes how to code \nsecurely. The Common Weakness Enumeration [CWE 12] is a list of the \nmost common vulnerabilities discovered in systems. \n■\n■Organizational processes. Organizations must have processes that provide \nfor responsibility for various aspects of security, including ensuring that \nsystems are patched to put into place the latest protections. The National \nInstitute of Standards and Technology (NIST) provides an enumeration of \norganizational processes [NIST 09]. [Cappelli 12] discusses insider threats.\n■\n■Technical processes. Microsoft has a life-cycle development process (The \nSecure Development Life Cycle) that includes modeling of threats. Four \ntraining classes are publicly available. www.microsoft.com/download/en/\ndetails.aspx?id=16420\nNIST has several volumes that give definitions of security terms [NIST 04], \ncategories of security controls [NIST 06], and an enumeration of security con-\ntrols that an organization could employ [NIST 09]. A security control could be a \ntactic, but it could also be organizational, coding-related, or a technical process.\nThe attack surface of a system is the code that can be run by unauthorized \nusers. A discussion of how to minimize the attack surface for a system can be \nfound at [Howard 04].\nEncryption and certificates of various types and strengths are commonly \nused to resist certain types of attacks. Encryption algorithms are particularly dif-\nficult to code correctly. A document produced by NIST [NIST 02] gives require-\nments for these algorithms.\nGood books on engineering systems for security have been written by Ross \nAnderson [Anderson 08] and Bruce Schneier [Schneier 08].\nDifferent domains have different specific sets of practices. The Payment \nCard Industry (PCI) has a set of standards intended for those involved in credit \ncard processing (www.pcisecuritystandards.org). There is also a set of recom-\nmendations for securing various portions of the electric grid (www.smartgridipe-\ndia.org/index.php/ASAP-SG).\nData on the various sources of data breaches can be found in the Verizon \n2012 Data Breach Investigations Report [Verizon 12].\nJohn Viega has written several books about secure software development in \nvarious environments. See, for example, [Viega 01].\n",
      "content_length": 2504,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "158 \nPart Two  Quality Attributes\t\n9—Security\n9.6  Discussion Questions\n1.\t\nWrite a set of concrete scenarios for security for an automatic teller ma-\nchine. How would you modify your design for the automatic teller machine \nto satisfy these scenarios?\n2.\t\nOne of the most sophisticated attacks on record was carried out by a virus \nknown as Stuxnet. Stuxnet first appeared in 2009 but became widely known \nin 2011 when it was revealed that it had apparently severely damaged or \nincapacitated the high-speed centrifuges involved in Iran’s uranium en-\nrichment program. Read about Stuxnet and see if you can devise a defense \nstrategy against it based on the tactics in this chapter.\n3.\t\nSome say that inserting security awareness into the software develop-\nment life cycle is at least as important as designing software with security \ncountermeasures. What are some examples of software development pro-\ncesses that can lead to more-secure systems?\n4.\t\nSecurity and usability are often seen to be at odds with each other. Security \noften imposes procedures and processes that seem like needless overhead to \nthe casual user. But some say that security and usability go (or should go) \nhand in hand and argue that making the system easy to use securely is the \nbest way to promote security to the user. Discuss.\n5.\t\nList some examples of critical resources for security that might become \nexhausted.\n6.\t\nList an example of a mapping of architectural elements that has strong se-\ncurity implications. Hint: think of where data is stored.\n7.\t\nWhich of the tactics in our list will protect against an insider threat? Can \nyou think of any that should be added?\n8.\t\nIn the United States, Facebook can account for more than 5 percent of all \nInternet traffic in a given week. How would you recognize a denial-of-ser-\nvice attack on Facebook.com?\n9.\t\nThe public disclosure of vulnerabilities in production systems is a matter of \ncontroversy. Discuss why this is so and the pros and cons of public disclo-\nsure of vulnerabilities.\n",
      "content_length": 2025,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "159\n10\nTestability\nTesting leads to failure, and failure \nleads to understanding\n—Burt Rutan\nIndustry estimates indicate that between 30 and 50 percent (or in some cases, \neven more) of the cost of developing well-engineered systems is taken up by test-\ning. If the software architect can reduce this cost, the payoff is large.\nSoftware testability refers to the ease with which software can be made to \ndemonstrate its faults through (typically execution-based) testing. Specifically, \ntestability refers to the probability, assuming that the software has at least one \nfault, that it will fail on its next test execution. Intuitively, a system is testable if it \n“gives up” its faults easily. If a fault is present in a system, then we want it to fail \nduring testing as quickly as possible. Of course, calculating this probability is not \neasy and, as you will see when we discuss response measures for testability, other \nmeasures will be used.\nFigure 10.1 shows a model of testing in which a program processes input \nand produces output. An oracle is an agent (human or mechanical) that decides \nwhether the output is correct or not by comparing the output to the program’s \nspecification. Output is not just the functionally produced value, but it also can \ninclude derived measures of quality attributes such as how long it took to produce \nthe output. Figure 10.1 also shows that the program’s internal state can also be \nshown to the oracle, and an oracle can decide whether that is correct or not—that \nis, it can detect whether the program has entered an erroneous state and render a \njudgment as to the correctness of the program. \nSetting and examining a program’s internal state is an aspect of testing that \nwill figure prominently in our tactics for testability.\n",
      "content_length": 1779,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "160 \nPart Two  Quality Attributes\t\n10—Testability\nProgram\nOracle\n{ \n}\ninput\noutput\napproved\nrejected\ninternal state\nFigure 10.1  A model of testing\nFor a system to be properly testable, it must be possible to control each compo-\nnent’s inputs (and possibly manipulate its internal state) and then to observe its \noutputs (and possibly its internal state, either after or on the way to computing \nthe outputs). Frequently this control and observation is done through the use of a \ntest harness, which is specialized software (or in some cases, hardware) designed \nto exercise the software under test. Test harnesses come in various forms, such \nas a record-and-playback capability for data sent across various interfaces, or a \nsimulator for an external environment in which a piece of embedded software is \ntested, or even during production (see sidebar). The test harness can provide as-\nsistance in executing the test procedures and recording the output. A test harness \ncan be a substantial piece of software in its own right, with its own architecture, \nstakeholders, and quality attribute requirements. \nTesting is carried out by various developers, users, or quality assurance per-\nsonnel. Portions of the system or the entire system may be tested. The response \nmeasures for testability deal with how effective the tests are in discovering faults \nand how long it takes to perform the tests to some desired level of coverage. Test \ncases can be written by the developers, the testing group, or the customer. The \ntest cases can be a portion of acceptance testing or can drive the development as \nthey do in certain types of Agile methodologies.\nNetflix’s Simian Army\nNetflix distributes movies and television shows both via DVD and via \nstreaming video. Their streaming video service has been extremely suc-\ncessful. In May 2011 Netflix streaming video accounted for 24 percent of the \nInternet traffic in North America. Naturally, high availability is important to \nNetflix.\nNetflix hosts their computer services in the Amazon EC2 cloud, and they \nutilize what they call a “Simian Army” as a portion of their testing process. \nThey began with a Chaos Monkey, which randomly kills processes in the \n",
      "content_length": 2206,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "\t\n10—Testability  161\nrunning system. This allows the monitoring of the effect of failed processes \nand gives the ability to ensure that the system does not fail or suffer serious \ndegradation as a result of a process failure. \nRecently, the Chaos Monkey got some friends to assist in the testing. \nCurrently, the Netflix Simian Army includes these:\n■\n■\nThe Latency Monkey induces artificial delays in the client-server \ncommunication layer to simulate service degradation and measures if \nupstream services respond appropriately. \n■\n■\nThe Conformity Monkey finds instances that don’t adhere to best \npractices and shuts them down. For example, if an instance does not \nbelong to an auto-scaling group, it will not appropriately scale when \ndemand goes up.\n■\n■\nThe Doctor Monkey taps into health checks that run on each instance as \nwell as monitors other external signs of health (e.g., CPU load) to detect \nunhealthy instances. \n■\n■\nThe Janitor Monkey ensures that the Netflix cloud environment is \nrunning free of clutter and waste. It searches for unused resources and \ndisposes of them.\n■\n■\nThe Security Monkey is an extension of Conformity Monkey. It finds \nsecurity violations or vulnerabilities, such as improperly configured \nsecurity groups, and terminates the offending instances. It also ensures \nthat all the SSL and digital rights management (DRM) certificates are \nvalid and are not coming up for renewal.\n■\n■\nThe 10-18 Monkey (localization-internationalization) detects \nconfiguration and runtime problems in instances serving customers in \nmultiple geographic regions, using different languages and character \nsets. The name 10-18 comes from L10n-i18n, a sort of shorthand for the \nwords localization and internationalization.\nSome of the members of the Simian Army use fault injection to place \nfaults into the running system in a controlled and monitored fashion. \nOther members monitor various specialized aspects of the system and its \nenvironment. Both of these techniques have broader applicability than just \nNetflix.\nNot all faults are equal in terms of severity. More emphasis should be \nplaced on finding the most severe faults than on finding other faults. The \nSimian Army reflects a determination by Netflix that the faults they look for \nare the most serious in terms of their impact.\nThis strategy illustrates that some systems are too complex and adap-\ntive to be tested fully, because some of their behaviors are emergent. An \naspect of testing in that arena is logging of operational data produced by \nthe system, so that when failures occur, the logged data can be analyzed in \nthe lab to try to reproduce the faults. Architecturally this can require mecha-\nnisms to access and log certain system state. The Simian Army is one way \nto discover and log behavior in systems of this ilk.\n—LB\n",
      "content_length": 2825,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "162 \nPart Two  Quality Attributes\t\n10—Testability\nTesting of code is a special case of validation, which is making sure that an \nengineered artifact meets the needs of its stakeholders or is suitable for use. In \nChapter 21 we will discuss architectural design reviews. This is another kind of \nvalidation, where the artifact being tested is the architecture. In this chapter we \nare concerned only with the testability of a running system and of its source code. \n10.1  Testability General Scenario\nWe can now describe the general scenario for testability.\n■\n■Source of stimulus. The testing is performed by unit testers, integration \ntesters, or system testers (on the developing organization side), or \nacceptance testers and end users (on the customer side). The source could \nbe human or an automated tester.\n■\n■Stimulus. A set of tests is executed due to the completion of a coding incre-\nment such as a class layer or service, the completed integration of a subsys-\ntem, the complete implementation of the whole system, or the delivery of \nthe system to the customer.\n■\n■Artifact. A unit of code (corresponding to a module in the architecture), a \nsubsystem, or the whole system is the artifact being tested. \n■\n■Environment. The test can happen at development time, at compile time, at \ndeployment time, or while the system is running (perhaps in routine use). The \nenvironment can also include the test harness or test environments in use.\n■\n■Response. The system can be controlled to perform the desired tests and the \nresults from the test can be observed. \n■\n■Response measure. Response measures are aimed at representing how eas-\nily a system under test “gives up” its faults. Measures might include the \neffort involved in finding a fault or a particular class of faults, the effort \nrequired to test a given percentage of statements, the length of the longest \ntest chain (a measure of the difficulty of performing the tests), measures of \neffort to perform the tests, measures of effort to actually find faults, esti-\nmates of the probability of finding additional faults, and the length of time \nor amount of effort to prepare the test environment. \nMaybe one measure is the ease at which the system can be brought into \na specific state. In addition, measures of the reduction in risk of the remain-\ning errors in the system can be used. Not all faults are equal in terms of \ntheir possible impact. Measures of risk reduction attempt to rate the severity \nof faults found (or to be found). \nFigure 10.2 shows a concrete scenario for testability. The unit tester com-\npletes a code unit during development and performs a test sequence whose results \nare captured and that gives 85 percent path coverage within three hours of testing. \n",
      "content_length": 2752,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "10.1  Testability General Scenario\n163\nTable 10.1 enumerates the elements of the general scenario that characterize \ntestability.\nTable 10.1  Testability General Scenario\nPortion of Scenario\nPossible Values\nSource\nUnit testers, integration testers, system testers, acceptance \ntesters, end users, either running tests manually or using \nautomated testing tools\nStimulus \nA set of tests is executed due to the completion of a coding \nincrement such as a class layer or service, the completed \nintegration of a subsystem, the complete implementation of the \nwhole system, or the delivery of the system to the customer.\nEnvironment \nDesign time, development time, compile time, integration time, \ndeployment time, run time\nArtifacts \nThe portion of the system being tested\nResponse \nOne or more of the following: execute test suite and capture \nresults, capture activity that resulted in the fault, control and \nmonitor the state of the system \nResponse Measure \nOne or more of the following: effort to find a fault or class of \nfaults, effort to achieve a given percentage of state space \ncoverage, probability of fault being revealed by the next \ntest, time to perform tests, effort to detect faults, length of \nlongest dependency chain in test, length of time to prepare \ntest environment, reduction in risk exposure (size(loss) × \nprob(loss))\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nDevelopment\n3\n2\n1\n4\nUnit Tester\nCode Unit \nCompleted\nResults Captured\n85% Path Coverage \nin Three Hours\nArtifact:\nCode Unit\nFigure 10. 2  Sample concrete testability scenario\n",
      "content_length": 1578,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "164 \nPart Two  Quality Attributes\t\n10—Testability\n10.2  Tactics for Testability\nThe goal of tactics for testability is to allow for easier testing when an increment \nof software development is completed. Figure 10.3 displays the use of tactics for \ntestability. Architectural techniques for enhancing the software testability have not \nreceived as much attention as more mature quality attribute disciplines such as \nmodifiability, performance, and availability, but as we stated before, anything the \narchitect can do to reduce the high cost of testing will yield a significant benefit. \nThere are two categories of tactics for testability. The first category deals \nwith adding controllability and observability to the system. The second deals \nwith limiting complexity in the system’s design. \nControl and Observe System State\nControl and observation are so central to testability that some authors even define \ntestability in those terms. The two go hand-in-hand; it makes no sense to control \nsomething if you can’t observe what happens when you do. The simplest form of \ncontrol and observation is to provide a software component with a set of inputs, \nlet it do its work, and then observe its outputs. However, the control and observe \nsystem state category of testability tactics provides insight into software that goes \nbeyond its inputs and outputs. These tactics cause a component to maintain some \nsort of state information, allow testers to assign a value to that state information, \nand/or make that information accessible to testers on demand. The state infor-\nmation might be an operating state, the value of some key variable, performance \nload, intermediate process steps, or anything else useful to re-creating component \nbehavior. Specific tactics include the following:\nTests\nExecuted\nFaults\nDetected\nTactics\nto Control\nTestability\nFigure 10.3  The goal of testability tactics\n",
      "content_length": 1899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "10.2  Tactics for Testability\n165\n■\n■Specialized interfaces. Having specialized testing interfaces allows you \nto control or capture variable values for a component either through a test \nharness or through normal execution. Examples of specialized test routines \ninclude these:\n■\n■A set and get method for important variables, modes, or attributes \n(methods that might otherwise not be available except for testing \npurposes)\n■\n■A report method that returns the full state of the object \n■\n■A reset method to set the internal state (for example, all the attributes of a \nclass) to a specified internal state\n■\n■A method to turn on verbose output, various levels of event logging, \nperformance instrumentation, or resource monitoring\nSpecialized testing interfaces and methods should be clearly identified or \nkept separate from the access methods and interfaces for required function-\nality, so that they can be removed if needed. (However, in performance-crit-\nical and some safety-critical systems, it is problematic to field different \ncode than that which was tested. If you remove the test code, how will you \nknow the code you field has the same behavior, particularly the same timing \nbehavior, as the code you tested? For other kinds of systems, however, this \nstrategy is effective.)\n■\n■Record/playback. The state that caused a fault is often difficult to re-create. \nRecording the state when it crosses an interface allows that state to be used \nto “play the system back” and to re-create the fault. Record/playback refers \nto both capturing information crossing an interface and using it as input for \nfurther testing. \n■\n■Localize state storage. To start a system, subsystem, or module in an arbi-\ntrary state for a test, it is most convenient if that state is stored in a single \nplace. By contrast, if the state is buried or distributed, this becomes difficult \nif not impossible. The state can be fine-grained, even bit-level, or coarse-\ngrained to represent broad abstractions or overall operational modes. The \nchoice of granularity depends on how the states will be used in testing. A \nconvenient way to “externalize” state storage (that is, to make it able to be \nmanipulated through interface features) is to use a state machine (or state \nmachine object) as the mechanism to track and report current state.\n■\n■Abstract data sources. Similar to controlling a program’s state, easily con-\ntrolling its input data makes it easier to test. Abstracting the interfaces lets \nyou substitute test data more easily. For example, if you have a database of \ncustomer transactions, you could design your architecture so that it is easy \nto point your test system at other test databases, or possibly even to files of \ntest data instead, without having to change your functional code.\n■\n■Sandbox. “Sandboxing” refers to isolating an instance of the system from \nthe real world to enable experimentation that is unconstrained by the worry \n",
      "content_length": 2948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "166 \nPart Two  Quality Attributes\t\n10—Testability\nabout having to undo the consequences of the experiment. Testing is helped \nby the ability to operate the system in such a way that it has no permanent \nconsequences, or so that any consequences can be rolled back. This can \nbe used for scenario analysis, training, and simulation. (The Spring frame-\nwork, which is quite popular in the Java community, comes with a set of \ntest utilities that support this. Tests are run as a “transaction,” which is \nrolled back at the end.)\nA common form of sandboxing is to virtualize resources. Testing a \nsystem often involves interacting with resources whose behavior is outside \nthe control of the system. Using a sandbox, you can build a version of the \nresource whose behavior is under your control. For example, the system \nclock’s behavior is typically not under our control—it increments one \nsecond each second—which means that if we want to make the system \nthink it’s midnight on the day when all of the data structures are supposed \nto overflow, we need a way to do that, because waiting around is a poor \nchoice. By having the capability to abstract system time from clock time, \nwe can allow the system (or components) to run at faster than wall-clock \ntime, and to allow the system (or components) to be tested at critical time \nboundaries (such as the next shift on or off Daylight Savings Time). Similar \nvirtualizations could be done for other resources, such as memory, battery, \nnetwork, and so on. Stubs, mocks, and dependency injection are simple but \neffective forms of virtualization.\n■\n■Executable assertions. Using this tactic, assertions are (usually) hand-coded \nand placed at desired locations to indicate when and where a program is in \na faulty state. The assertions are often designed to check that data values \nsatisfy specified constraints. Assertions are defined in terms of specific data \ndeclarations, and they must be placed where the data values are referenced \nor modified. Assertions can be expressed as pre- and post-conditions for \neach method and also as class-level invariants. This results in increasing \nobservability, when an assertion is flagged as having failed. Assertions \nsystematically inserted where data values change can be seen as a manual \nway to produce an “extended” type. Essentially, the user is annotating \na type with additional checking code. Any time an object of that type is \nmodified, the checking code is automatically executed, and warnings are \ngenerated if any conditions are violated. To the extent that the assertions \ncover the test cases, they effectively embed the test oracle in the code—\nassuming the assertions are correct and correctly coded.\nAll of these tactics add capability or abstraction to the software that (were we \nnot interested in testing) otherwise would not be there. They can be seen as replac-\ning bare-bones, get-the-job-done software with more elaborate software that has \nbells and whistles for testing. There are a number of techniques for effecting this \nreplacement. These are not testability tactics, per se, but techniques for replacing \none component with a different version of itself. They include the following:\n",
      "content_length": 3211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "10.2  Tactics for Testability\n167\n■\n■Component replacement, which simply swaps the implementation of a \ncomponent with a different implementation that (in the case of testability) \nhas features that facilitate testing. Component replacement is often \naccomplished in a system’s build scripts.\n■\n■Preprocessor macros that, when activated, expand to state-reporting code or \nactivate probe statements that return or display information, or return con-\ntrol to a testing console.\n■\n■Aspects (in aspect-oriented programs) that handle the cross-cutting concern \nof how state is reported.\nLimit Complexity\nComplex software is harder to test. This is because, by the definition of complex-\nity, its operating state space is very large and (all else being equal) it is more dif-\nficult to re-create an exact state in a large state space than to do so in a small state \nspace. Because testing is not just about making the software fail but about finding \nthe fault that caused the failure so that it can be removed, we are often concerned \nwith making behavior repeatable. This category has three tactics:\n■\n■Limit structural complexity. This tactic includes avoiding or resolving \ncyclic dependencies between components, isolating and encapsulating \ndependencies on the external environment, and reducing dependencies \nbetween components in general (for example, reduce the number of \nexternal accesses to a module’s public data). In object-oriented systems, \nyou can simplify the inheritance hierarchy: Limit the number of classes \nfrom which a class is derived, or the number of classes derived from a \nclass. Limit the depth of the inheritance tree, and the number of children of \na class. Limit polymorphism and dynamic calls. One structural metric that \nhas been shown empirically to correlate to testability is called the response \nof a class. The response of class C is a count of the number of methods \nof C plus the number of methods of other classes that are invoked by the \nmethods of C. Keeping this metric low can increase testability.\nHaving high cohesion, loose coupling, and separation of concerns—all \nmodifiability tactics (see Chapter 7)—can also help with testability. They \nare a form of limiting the complexity of the architectural elements by \ngiving each element a focused task with limited interaction with other ele-\nments. Separation of concerns can help achieve controllability and observ-\nability (as well as reducing the size of the overall program’s state space). \nControllability is critical to making testing tractable, as Robert Binder has \nnoted: “A component that can act independently of others is more readily \ncontrollable. . . . With high coupling among classes it is typically more \ndifficult to control the class under test, thus reducing testability. . . . If user \ninterface capabilities are entwined with basic functions it will be more \ndifficult to test each function” [Binder 94].\n",
      "content_length": 2921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "168 \nPart Two  Quality Attributes\t\n10—Testability\nAlso, systems that require complete data consistency at all times are of-\nten more complex than those that do not. If your requirements allow it, con-\nsider building your system under the “eventual consistency” model, where \nsooner or later (but maybe not right now) your data will reach a consistent \nstate. This often makes system design simpler, and therefore easier to test.\nFinally, some architectural styles lend themselves to testability. In a \nlayered style, you can test lower layers first, then test higher layers with \nconfidence in the lower layers. \n■\n■Limit nondeterminism. The counterpart to limiting structural complexity \nis limiting behavioral complexity, and when it comes to testing, \nnondeterminism is a very pernicious form of complex behavior. \nNondeterministic systems are harder to test than deterministic systems. \nThis tactic involves finding all the sources of nondeterminism, such as \nunconstrained parallelism, and weeding them out as much as possible. \nSome sources of nondeterminism are unavoidable—for instance, in multi-\nthreaded systems that respond to unpredictable events—but for such \nsystems, other tactics (such as record/playback) are available. \nFigure 10.4 provides a summary of the tactics used for testability.\nTestability Tactics\nControl and Observe\nSystem State\nLimit Complexity\nSpecialized\nInterfaces\nLimit Structural\nComplexity\nLimit\nNondeterminism\nTests\nExecuted\nFaults\nDetected\nRecord/\nPlayback\nLocalize State\nStorage\nSandbox\nExecutable\nAssertions\nAbstract Data\nSources\nFigure 10.4  Testability tactics\n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "10.3  A Design Checklist for Testability\n169\n10.3  A Design Checklist for Testability\nTable 10.2 is a checklist to support the design and analysis process for testability.\nTable 10.2  Checklist to Support the Design and Analysis Process for Testability\nCategory\nChecklist\nAllocation of \nResponsibilities\nDetermine which system responsibilities are most critical \nand hence need to be most thoroughly tested.\nEnsure that additional system responsibilities have been \nallocated to do the following:\n■\n■\nExecute test suite and capture results (external test or \nself-test)\n■\n■\nCapture (log) the activity that resulted in a fault or that \nresulted in unexpected (perhaps emergent) behavior \nthat was not necessarily a fault\n■\n■\nControl and observe relevant system state for testing\nMake sure the allocation of functionality provides high \ncohesion, low coupling, strong separation of concerns, and \nlow structural complexity.\nCoordination Model\nEnsure the system’s coordination and communication \nmechanisms:\n■\n■\nSupport the execution of a test suite and capture the \nresults within a system or between systems\n■\n■\nSupport capturing activity that resulted in a fault within \na system or between systems\n■\n■\nSupport injection and monitoring of state into the \ncommunication channels for use in testing, within a \nsystem or between systems\n■\n■\nDo not introduce needless nondeterminism\nData Model\nDetermine the major data abstractions that must be tested \nto ensure the correct operation of the system.\n■\n■\nEnsure that it is possible to capture the values of \ninstances of these data abstractions \n■\n■\nEnsure that the values of instances of these data \nabstractions can be set when state is injected into the \nsystem, so that system state leading to a fault may be \nre-created\n■\n■\nEnsure that the creation, initialization, persistence, \nmanipulation, translation, and destruction of instances \nof these data abstractions can be exercised and \ncaptured \nMapping among \nArchitectural Elements\nDetermine how to test the possible mappings of \narchitectural elements (especially mappings of processes \nto processors, threads to processes, and modules to \ncomponents) so that the desired test response is achieved \nand potential race conditions identified.\nIn addition, determine whether it is possible to test for \nillegal mappings of architectural elements.\ncontinues\n",
      "content_length": 2357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "170 \nPart Two  Quality Attributes\t\n10—Testability\nTable 10.2  Checklist to Support the Design and Analysis Process for \nTestability, continued\nCategory\nChecklist\nResource Management\nEnsure there are sufficient resources available to execute \na test suite and capture the results. Ensure that your test \nenvironment is representative of (or better yet, identical to) \nthe environment in which the system will run. Ensure that \nthe system provides the means to do the following:\n■\n■\nTest resource limits \n■\n■\nCapture detailed resource usage for analysis in the \nevent of a failure\n■\n■\nInject new resource limits into the system for the \npurposes of testing\n■\n■\nProvide virtualized resources for testing\nBinding Time\nEnsure that components that are bound later than compile \ntime can be tested in the late-bound context. \nEnsure that late bindings can be captured in the event of a \nfailure, so that you can re-create the system’s state leading \nto the failure. \nEnsure that the full range of binding possibilities can be \ntested.\nChoice of Technology\nDetermine what technologies are available to help achieve \nthe testability scenarios that apply to your architecture. Are \ntechnologies available to help with regression testing, fault \ninjection, recording and playback, and so on?\nDetermine how testable the technologies are that you have \nchosen (or are considering choosing in the future) and \nensure that your chosen technologies support the level of \ntesting appropriate for your system. For example, if your \nchosen technologies do not make it possible to inject state, \nit may be difficult to re-create fault scenarios.\nNow That Your Architecture Is Set to Help You Test . . . \nBy Nick Rozanski, coauthor (with Eoin Woods) of Software Systems \nArchitecture: Working With Stakeholders Using Viewpoints and \nPerspectives\nIn addition to architecting your system to make it amenable to testing, \nyou will need to overcome two more specific and daunting challenges \nwhen testing very large or complex systems, namely test data and test \nautomation.\nTest Data\nYour first challenge is how to create large, consistent and useful test \ndata sets. This is a significant problem in my experience, particularly for \nintegration testing (that is, testing a number of components to confirm that \nthey work together correctly) and performance testing (confirming that \n",
      "content_length": 2360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "10.3  A Design Checklist for Testability\n171\nthe system meets it requirements for throughput, latency, and response \ntime). For unit tests, and usually for user acceptance tests, the test data is \ntypically created by hand.\nFor example, you might need 50 products, 100 customers, and 500 \norders in your test database, so that you can test the functional steps \ninvolved in creating, amending, or deleting orders. This data has to be \nsufficiently varied to make testing worthwhile, it has to conform to all the \nreferential integrity rules and other constraints of your data model, and you \nneed to be able to calculate and specify the expected results of the tests.\nI’ve seen—and been involved in—two ways of doing this: you either \nwrite a system to generate your test data, or you capture a representative \ndata set from the production environment and anonymize it as necessary. \n(Anonymizing test data involves removing any sensitive information, such as \npersonal data about people or organizations, financial details, and so on.)\nCreating your own test data is the ideal, because you know what data \nyou are using and can ensure that it covers all of your edge cases, but it is \na lot of effort. Capturing data from the live environment is easier, assum-\ning that there is a system there already, but you don’t know what data and \nhence what coverage you’re going to get, and you may have to take extra \ncare to conform to privacy and data protection legislation.\nThis can have an impact on the system’s architecture in a number of \nways, and should be given due consideration early on by the architect. For \nexample, the system may need to be able to capture live transactions, or \ntake “snapshots” of live data, which can be used to generate test data. In ad-\ndition, the test-data-generation system may need an architecture of its own.\nTest Automation\nYour second challenge is around test automation. In practice it is not pos-\nsible to test large systems by hand because of the number of tests, their \ncomplexity, and the amount of checking of results that’s required. In the \nideal world, you create a test automation framework to do this automati-\ncally, which you feed with test data, and set running every night, or even \nrun every time you check in something (the continuous integration model).\nThis is an area that is given too little attention on many large software \ndevelopment projects. It is often not budgeted for in the project plan, with \nan unwritten assumption that the effort needed to build it can be somehow \n“absorbed” into the development costs. A test automation framework can \nbe a significantly complex thing in its own right (which raises the question \nof how you test it!). It should be scoped and planned like any other project \ndeliverable.\nDue consideration should be given to how the framework will invoke \nfunctions on the system under test, particularly for testing user interfaces, \nwhich is almost without exception a nightmare. (The execution of a UI test \nis highly dependent on the layout of the windows, the ordering of fields, \nand so on, which usually changes a lot in heavily user-focused systems. \nIt is sometimes possible to execute window controls programmatically, but \nin the worst case you may have to record and replay keystrokes or mouse \nmovements.)\n",
      "content_length": 3311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "172 \nPart Two  Quality Attributes\t\n10—Testability\nThere are lots of tools to help with this nowadays, such as Quick Test \nPro, TestComplete, or Selenium for testing, and CruiseControl, Hudson, \nand TeamCity for continuous integration. A comprehensive list on the web \ncan be found here: en.wikipedia.org/wiki/Test_automation.\n10.4  Summary\nEnsuring that a system is easily testable has payoffs both in terms of the cost of \ntesting and the reliability of the system. A vehicle often used to execute the tests \nis the test harness. Test harnesses are software systems that encapsulate test re-\nsources such as test cases and test infrastructure so that it is easy to reapply tests \nacross iterations and it is easy to apply the test infrastructure to new increments \nof the system. Another vehicle is the creation of test cases prior to the develop-\nment of a component, so that developers know which tests their component must \npass.\nControlling and observing the system state is a major class of testability \ntactics. Providing the ability to do fault injection, to record system state at key \nportions of the system, to isolate the system from its environment, and to abstract \nvarious resources are all different tactics to support the control and observation of \na system and its components.\nComplex systems are difficult to test because of the large state space in \nwhich their computations take place, and because of the larger number of inter-\nconnections among the elements of the system. Consequently, keeping the sys-\ntem simple is another class of tactics that supports testability.\n10.5  For Further Reading\nAn excellent general introduction to software testing is [Beizer 90]. For a more \nmodern take on testing, and from the software developer’s perspective rather than \nthe tester’s, Freeman and Pryce cover test-driven development in the object-ori-\nented realm [Freeman 09].\nBertolino and Strigini [Bertolino 96] are the developers of the model of test-\ning shown in Figure 10.1. \nYin and Bieman [Yin 94] have written about executable assertions. Hartman \n[Hartman 10] describes a technique for using executable assertions as a means \nfor detecting race conditions.\nBruntink and van Deursen [Bruntink 06] write about the impact of structure \non testing. \n",
      "content_length": 2272,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "10.6  Discussion Questions\n173\nJeff Voas’s foundational work on testability and the relationship between \ntestability and reliability is worthwhile. There are several papers to choose from, \nbut [Voas 95] is a good start that will point you to others.\n10.6  Discussion Questions\n1.\t\nA testable system is one that gives up its faults easily. That is, if a system \ncontains a fault, then it doesn’t take long or much effort to make that fault \nshow up. On the other hand, fault tolerance is all about designing systems \nthat jealously hide their faults; there, the whole idea is to make it very diffi-\ncult for a system to reveal its faults. Is it possible to design a system that is \nboth highly testable and highly fault tolerant, or are these two design goals \ninherently incompatible? Discuss.\n2.\t\n“Once my system is in routine use by end users, it should not be highly \ntestable, because if it still contains faults—and all systems probably do—\nthen I don’t want them to be easily revealed.” Discuss.\n3.\t\nMany of the tactics for testability are also useful for achieving modifiabili-\nty. Why do you think that is?\n4.\t\nWrite some concrete testability scenarios for an automatic teller machine. \nHow would you modify your design for the automatic teller machine to ac-\ncommodate these scenarios?\n5.\t\nWhat other quality attributes do you think testability is most in conflict \nwith? What other quality attributes do you think testability is most compati-\nble with?\n6.\t\nOne of our tactics is to limit nondeterminism. One method is to use locking \nto enforce synchronization. What impact does the use of locks have on oth-\ner quality attributes?\n7.\t\nSuppose you’re building the next great social networking system. You antic-\nipate that within a month of your debut, you will have half a million users. \nYou can’t pay half a million people to test your system, and yet it has to be \nrobust and easy to use when all half a million are banging away at it. What \nshould you do? What tactics will help you? Write a testability scenario for \nthis social networking system.\n8.\t\nSuppose you use executable assertions to improve testability. Make a case \nfor, and then a case against, allowing the assertions to run in the production \nsystem as opposed to removing them after testing.\n",
      "content_length": 2275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "175\n11\nUsability\nAny darn fool can make something complex; it \ntakes a genius to make something simple.\n—Albert Einstein\nUsability is concerned with how easy it is for the user to accomplish a desired \ntask and the kind of user support the system provides. Over the years, a focus on \nusability has shown itself to be one of the cheapest and easiest ways to improve a \nsystem’s quality (or more precisely, the user’s perception of quality). \nUsability comprises the following areas:\n■\n■Learning system features. If the user is unfamiliar with a particular system \nor a particular aspect of it, what can the system do to make the task of \nlearning easier? This might include providing help features.\n■\n■Using a system efficiently. What can the system do to make the user more \nefficient in its operation? This might include the ability for the user to redi-\nrect the system after issuing a command. For example, the user may wish to \nsuspend one task, perform several operations, and then resume that task.\n■\n■Minimizing the impact of errors. What can the system do so that a user \nerror has minimal impact? For example, the user may wish to cancel a com-\nmand issued incorrectly.\n■\n■Adapting the system to user needs. How can the user (or the system itself) \nadapt to make the user’s task easier? For example, the system may automat-\nically fill in URLs based on a user’s past entries.\n■\n■Increasing confidence and satisfaction. What does the system do to give the \nuser confidence that the correct action is being taken? For example, pro-\nviding feedback that indicates that the system is performing a long-running \ntask and the extent to which the task is completed will increase the user’s \nconfidence in the system.\n",
      "content_length": 1720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "176 \nPart Two  Quality Attributes\t\n11—Usability\n11.1  Usability General Scenario\nThe portions of the usability general scenarios are these:\n■\n■Source of stimulus. The end user (who may be in a specialized role, such as \na system or network administrator) is always the source of the stimulus for \nusability.\n■\n■Stimulus. The stimulus is that the end user wishes to use a system efficient-\nly, learn to use the system, minimize the impact of errors, adapt the system, \nor configure the system. \n■\n■Environment. The user actions with which usability is concerned always \noccur at runtime or at system configuration time. \n■\n■Artifact. The artifact is the system or the specific portion of the system with \nwhich the user is interacting.\n■\n■Response. The system should either provide the user with the features need-\ned or anticipate the user’s needs. \n■\n■Response measure. The response is measured by task time, number of \nerrors, number of tasks accomplished, user satisfaction, gain of user \nknowledge, ratio of successful operations to total operations, or amount of \ntime or data lost when an error occurs. \nTable 11.1 enumerates the elements of the general scenario that characterize \nusability.\nFigure 11.1 gives an example of a concrete usability scenario that you could \ngenerate using Table 11.1: The user downloads a new application and is using it \nproductively after two minutes of experimentation.\nTable 11.1  Usability General Scenario\nPortion of Scenario\nPossible Values\nSource \nEnd user, possibly in a specialized role\nStimulus\nEnd user tries to use a system efficiently, learn to use the \nsystem, minimize the impact of errors, adapt the system, or \nconfigure the system.\nEnvironment\nRuntime or configuration time \nArtifacts\nSystem or the specific portion of the system with which the \nuser is interacting\nResponse\nThe system should either provide the user with the features \nneeded or anticipate the user’s needs.\nResponse Measure \nOne or more of the following: task time, number of errors, \nnumber of tasks accomplished, user satisfaction, gain of user \nknowledge, ratio of successful operations to total operations, \nor amount of time or data lost when an error occurs \n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "11.2  Tactics for Usability\n177\n11.2  Tactics for Usability\nRecall that usability is concerned with how easy it is for the user to accomplish \na desired task, as well as the kind of support the system provides to the user. \nResearchers in human-computer interaction have used the terms user initiative, \nsystem initiative, and mixed initiative to describe which of the human-computer \npair takes the initiative in performing certain actions and how the interaction pro-\nceeds. Usability scenarios can combine initiatives from both perspectives. For \nexample, when canceling a command, the user issues a cancel—user initiative—\nand the system responds. During the cancel, however, the system may put up a \nprogress indicator—system initiative. Thus, cancel may demonstrate mixed ini-\ntiative. We use this distinction between user and system initiative to discuss the \ntactics that the architect uses to achieve the various scenarios.\nFigure 11.2 shows the goal of the set of runtime usability tactics.\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nRuntime\n3\n2\n1\n4\nUser\nDownloads \na New \nApplication\nUser Uses\nApplication\nProductively\nWithin Two\nMinutes of\nExperimentation\nArtifact:\nSystem\nFigure 11.1  Sample concrete usability scenario \nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nTactics\nto Control\nUsability\nFigure 11.2  The goal of runtime usability tactics\n",
      "content_length": 1390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "178 \nPart Two  Quality Attributes\t\n11—Usability\nSeparate the User Interface!\nOne of the most helpful things an architect can do to make a system \nusable is to facilitate experimentation with the user interface via the con-\nstruction of rapid prototypes. Building a prototype, or several prototypes, \nto let real users experience the interface and give their feedback pays \nenormous dividends. The best way to do this is to design the software so \nthat the user interface can be quickly changed.\nTactics for modifiability that we saw in Chapter 7 support this goal per-\nfectly well, especially these:\n■\n■\nIncrease semantic coherence, encapsulate, and co-locate related re-\nsponsibilities, which localize user interface responsibilities to a single \nplace\n■\n■\nRestrict dependencies, which minimizes the ripple effect to other soft-\nware when the user interface changes\n■\n■\nDefer binding, which lets you make critical user interface choices without \nhaving to recode\nDefer binding is especially helpful here, because you can expect that \nyour product’s user interface will face pressure to change during testing \nand even after it goes to market.\nUser interface generation tools are consistent with these tactics; most \nproduce a single module with an abstract interface to the rest of the soft-\nware. Many provide the capability to change the user interface after compile \ntime. You can do your part by restricting dependencies on the generated \nmodule, should you later decide to adopt a different tool.\nMuch work in different user interface separation patterns occurred in the \n1980s and 90s. With the advent of the web and the modernization of the \nmodel-view-controller (MVC) pattern to reflect web interfaces, MVC has \nbecome the dominant separation pattern. Now the MVC pattern is built into \na wide variety of different frameworks. (See Chapter 14 for a discussion of \nMVC.) MVC makes it easy to provide multiple views of the data, supporting \nuser initiative, as we discuss next.\nMany times quality attributes are in conflict with each other. Usability \nand modifiability, on the other hand, often complement each other, \nbecause one of the best ways to make a system more usable is to make \nit modifiable. However, this is not always the case. In many systems busi-\nness rules drive the UI—for example, specifying how to validate input. To \nrealize this validation, the UI may need to call a server (which can neg-\natively affect performance). To get around this performance penalty, the \narchitect may choose to duplicate these rules in the client and the server, \nwhich then makes evolution difficult. Alas, the architect’s life is never easy!\n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "11.2  Tactics for Usability\n179\nThere is a connection between the achievement of usability and modifiabil-\nity. The user interface design process consists of generating and then testing a \nuser interface design. Deficiencies in the design are corrected and the process \nrepeats. If the user interface has already been constructed as a portion of the sys-\ntem, then the system must be modified to reflect the latest design. Hence the con-\nnection with modifiability. This connection has resulted in standard patterns to \nsupport user interface design (see sidebar).\nSupport User Initiative\nOnce a system is executing, usability is enhanced by giving the user feed-\nback as to what the system is doing and by allowing the user to make appro-\npriate responses. For example, the tactics described next—cancel, undo, pause/ \nresume, and aggregate—support the user in either correcting errors or being more \nefficient.\nThe architect designs a response for user initiative by enumerating and al-\nlocating the responsibilities of the system to respond to the user command. Here \nare some common examples of user initiative: \n■\n■Cancel. When the user issues a cancel command, the system must be \nlistening for it (thus, there is the responsibility to have a constant listener \nthat is not blocked by the actions of whatever is being canceled); the \ncommand being canceled must be terminated; any resources being \nused by the canceled command must be freed; and components that are \ncollaborating with the canceled command must be informed so that they \ncan also take appropriate action.\n■\n■Undo. To support the ability to undo, the system must maintain a sufficient \namount of information about system state so that an earlier state may be \nrestored, at the user’s request. Such a record may be in the form of state \n“snapshots”—for example, checkpoints—or as a set of reversible oper-\nations. Not all operations can be easily reversed: for example, changing \nall occurrences of the letter “a” to the letter “b” in a document cannot be \nreversed by changing all instances of “b” to “a”, because some of those in-\nstances of “b” may have existed prior to the original change. In such a case \nthe system must maintain a more elaborate record of the change. Of course, \nsome operations, such as ringing a bell, cannot be undone. \n■\n■Pause/resume. When a user has initiated a long-running operation—say, \ndownloading a large file or set of files from a server—it is often useful to \nprovide the ability to pause and resume the operation. Effectively pausing a \nlong-running operation requires the ability to temporarily free resources so \nthat they may be reallocated to other tasks. \n",
      "content_length": 2672,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "180 \nPart Two  Quality Attributes\t\n11—Usability\n■\n■Aggregate. When a user is performing repetitive operations, or operations \nthat affect a large number of objects in the same way, it is useful to provide \nthe ability to aggregate the lower-level objects into a single group, so that \nthe operation may be applied to the group, thus freeing the user from the \ndrudgery (and potential for mistakes) of doing the same operation repeated-\nly. For example, aggregate all of the objects in a slide and change the text to \n14-point font.\nSupport System Initiative\nWhen the system takes the initiative, it must rely on a model of the user, the \ntask being undertaken by the user, or the system state itself. Each model requires \nvarious types of input to accomplish its initiative. The support system initiative \ntactics are those that identify the models the system uses to predict either its \nown behavior or the user’s intention. Encapsulating this information will make \nit easier for it to be tailored or modified. Tailoring and modification can be either \ndynamically based on past user behavior or offline during development. These \ntactics are the following:\n■\n■Maintain task model. The task model is used to determine context so the \nsystem can have some idea of what the user is attempting and provide \nassistance. For example, knowing that sentences start with capital letters \nwould allow an application to correct a lowercase letter in that position.\n■\n■Maintain user model. This model explicitly represents the user’s knowledge \nof the system, the user’s behavior in terms of expected response time, and \nother aspects specific to a user or a class of users. For example, maintaining \na user model allows the system to pace mouse selection so that not all of \nthe document is selected when scrolling is required. Or a model can control \nthe amount of assistance and suggestions automatically provided to a user. \nA special case of this tactic is commonly found in user interface customiza-\ntion, wherein a user can explicitly modify the system’s user model.\n■\n■Maintain system model. Here the system maintains an explicit model \nof itself. This is used to determine expected system behavior so that \nappropriate feedback can be given to the user. A common manifestation of \na system model is a progress bar that predicts the time needed to complete \nthe current activity.\nFigure 11.3 shows a summary of the tactics to achieve usability.\n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "11.3  A Design Checklist for Usability\n181\nUsability Tactics\nSupport User\nInitiative\nSupport System\nInitiative\nCancel\nMaintain User\nModel\nMaintain System\nModel\nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nUndo\nPause/Resume\nAggregate\nMaintain Task\nModel\nFigure 11.3  Usability tactics\n11.3  A Design Checklist for Usability\nTable 11.2 is a checklist to support the design and analysis process for usability.\nTable 11.2  Checklist to Support the Design and Analysis Process for Usability\nCategory\nChecklist\nAllocation of \nResponsibilities\nEnsure that additional system responsibilities have been \nallocated, as needed, to assist the user in the following:\n■\n■\nLearning how to use the system\n■\n■\nEfficiently achieving the task at hand \n■\n■\nAdapting and configuring the system\n■\n■\nRecovering from user and system errors\nCoordination Model\nDetermine whether the properties of system elements’ \ncoordination—timeliness, currency, completeness, \ncorrectness, consistency—affect how a user learns to use \nthe system, achieves goals or completes tasks, adapts \nand configures the system, recovers from user and system \nerrors, and gains increased confidence and satisfaction. \nFor example, can the system respond to mouse events \nand give semantic feedback in real time? Can long-running \nevents be canceled in a reasonable amount of time?\ncontinues\n",
      "content_length": 1356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "182 \nPart Two  Quality Attributes\t\n11—Usability\nTable 11.2  Checklist to Support the Design and Analysis Process for Usability, \ncontinued\nCategory\nChecklist\nData Model\nDetermine the major data abstractions that are involved \nwith user-perceivable behavior. Ensure these major data \nabstractions, their operations, and their properties have \nbeen designed to assist the user in achieving the task at \nhand, adapting and configuring the system, recovering from \nuser and system errors, learning how to use the system, and \nincreasing satisfaction and user confidence.\nFor example, the data abstractions should be designed \nto support undo and cancel operations: the transaction \ngranularity should not be so great that canceling or undoing \nan operation takes an excessively long time.\nMapping among \nArchitectural  \nElements\nDetermine what mapping among architectural elements is \nvisible to the end user (for example, the extent to which the \nend user is aware of which services are local and which \nare remote). For those that are visible, determine how this \naffects the ways in which, or the ease with which, the user \nwill learn how to use the system, achieve the task at hand, \nadapt and configure the system, recover from user and \nsystem errors, and increase confidence and satisfaction.\nResource  \nManagement\nDetermine how the user can adapt and configure the \nsystem’s use of resources. Ensure that resource limitations \nunder all user-controlled configurations will not make users \nless likely to achieve their tasks. For example, attempt to \navoid configurations that would result in excessively long \nresponse times. Ensure that the level of resources will not \naffect the users’ ability to learn how to use the system, or \ndecrease their level of confidence and satisfaction with the \nsystem.\nBinding Time\nDetermine which binding time decisions should be under \nuser control and ensure that users can make decisions \nthat aid in usability. For example, if the user can choose, at \nruntime, the system’s configuration, or its communication \nprotocols, or its functionality via plug-ins, you need to ensure \nthat such choices do not adversely affect the user’s ability to \nlearn system features, use the system efficiently, minimize \nthe impact of errors, further adapt and configure the system, \nor increase confidence and satisfaction.\nChoice of Technology\nEnsure the chosen technologies help to achieve the usability \nscenarios that apply to your system. For example, do these \ntechnologies aid in the creation of online help, the production \nof training materials, and the collection of user feedback? \nHow usable are any of your chosen technologies? Ensure \nthe chosen technologies do not adversely affect the usability \nof the system (in terms of learning system features, using the \nsystem efficiently, minimizing the impact of errors, adapting/\nconfiguring the system, and increasing confidence and \nsatisfaction).\n",
      "content_length": 2935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "11.6  Discussion Questions\n183\n11.4  Summary\nArchitectural support for usability involves both allowing the user to take the ini-\ntiative—in circumstances such as canceling a long-running command or undoing \na completed command—and aggregating data and commands. \nTo be able to predict user or system responses, the system must keep an ex-\nplicit model of the user, the system, and the task.\nThere is a strong relationship between supporting the user interface design \nprocess and supporting modifiability; this relation is promoted by patterns that \nenforce separation of the user interface from the rest of the system, such as the \nMVC pattern.\n11.5  For Further Reading\nClaire Marie Karat has investigated the relation between usability and business \nadvantage [Karat 94].\nJakob Nielsen has also written extensively on this topic, including a calcula-\ntion on the ROI of usability [Nielsen 08].\nBonnie John and Len Bass have investigated the relation between usabil-\nity and software architecture. They have enumerated around two dozen usability \nscenarios that have architectural impact and given associated patterns for these \nscenarios [Bass 03].\nGreg Hartman has defined attentiveness as the ability of the system to sup-\nport user initiative and allow cancel or pause/resume [Hartman 10].\nSome of the patterns for separating the user interface are Arch/Slinky, See-\nheim, and PAC. These are discussed in Chapter 8 of Human-Computer Interac-\ntion [Dix 04].\n11.6  Discussion Questions\n1.\t\nWrite a concrete usability scenario for your automobile that specifies how \nlong it takes you to set your favorite radio stations? Now consider another \npart of the driver experience and create scenarios that test other aspects of \nthe response measures from the general scenario table.\n2.\t\nWrite a concrete usability scenario for an automatic teller machine. How \nwould your design be modified to satisfy these scenarios?\n",
      "content_length": 1918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "184 \nPart Two  Quality Attributes\t\n11—Usability\n3.\t\nHow might usability trade off against security? How might it trade off \nagainst performance? \n4.\t\nPick a few of your favorite web sites that do similar things, such as social \nnetworking or online shopping. Now pick one or two appropriate responses \nfrom the usability general scenario (such as “achieve the task at hand”) and \na correspondingly appropriate response measure. Using the response and \nresponse measure you chose, compare the web sites’ usability.\n5.\t\nSpecify the data model for a four-function calculator that allows undo.\n6.\t\nWhy is it that in so many systems, the cancel button in a dialog box appears \nto be unresponsive? What architectural principles do you think were ig-\nnored in these systems?\n7.\t\nWhy do you think that progress bars frequently behave erratically, moving \nfrom 10 to 90 percent in one step and then getting stuck on 90 percent?\n8.\t\nResearch the crash of Air France Flight 296 into the forest at Habsheim, \nFrance, on June 26, 1988. The pilots said they were unable to read the dig-\nital display of the radio altimeter or hear its audible readout. If they could \nhave, do you believe the crash would have been averted? In this context, \ndiscuss the relationship between usability and safety.\n",
      "content_length": 1282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "185\n12\nOther Quality Attributes\nQuality is not an act, it is a habit.\n—Aristotle\nChapters 5–11 each dealt with a particular quality attribute important to software \nsystems. Each of those chapters discussed how its particular quality attribute is \ndefined, gave a general scenario for that quality attribute, and showed how to \nwrite specific scenarios to express precise shades of meaning concerning that \nquality attribute. And each gave a collection of techniques to achieve that quality \nattribute in an architecture. In short, each chapter presented a kind of portfolio for \nspecifying and designing to achieve a particular quality attribute.\nThose seven chapters covered seven of the most important quality attributes, in \nterms of their occurrence in modern software-reliant systems. However, as is no \ndoubt clear, seven only begins to scratch the surface of the quality attributes that \nyou might find needed in a software system you’re working on. \nIs cost a quality attribute? It is not a technical quality attribute, but it certainly \naffects fitness for use. We consider economic factors in Chapter 23. \nThis chapter will give a brief introduction to a few other quality attributes—a \nsort of “B list” of quality attributes—but, more important, show how to build the \nsame kind of specification or design portfolio for a quality attribute not covered \nin our list.\n12.1  Other Important Quality Attributes\nBesides the quality attributes we’ve covered in depth in Chapters 5–11, some oth-\ners that arise frequently are variability, portability, development distributability, \nscalability and elasticity, deployability, mobility, and monitorability. We discuss \n“green” computing in Section 12.3.\n",
      "content_length": 1708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "186 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nVariability\nVariability is a special form of modifiability. It refers to the ability of a system \nand its supporting artifacts such as requirements, test plans, and configuration \nspecifications to support the production of a set of variants that differ from each \nother in a preplanned fashion. Variability is an especially important quality at-\ntribute in a software product line (this will be explored in depth in Chapter 25), \nwhere it means the ability of a core asset to adapt to usages in the different prod-\nuct contexts that are within the product line scope. The goal of variability in a \nsoftware product line is to make it easy to build and maintain products in the \nproduct line over a period of time. Scenarios for variability will deal with the \nbinding time of the variation and the people time to achieve it.\nPortability\nPortability is also a special form of modifiability. Portability refers to the ease \nwith which software that was built to run on one platform can be changed to \nrun on a different platform. Portability is achieved by minimizing platform de-\npendencies in the software, isolating dependencies to well-identified locations, \nand writing the software to run on a “virtual machine” (such as a Java Virtual \nMachine) that encapsulates all the platform dependencies within. Scenarios de-\nscribing portability deal with moving software to a new platform by expending \nno more than a certain level of effort or by counting the number of places in the \nsoftware that would have to change.\nDevelopment Distributability\nDevelopment distributability is the quality of designing the software to support \ndistributed software development. Many systems these days are developed using \nglobally distributed teams. One problem that must be overcome when develop-\ning with distributed teams is coordinating their activities. The system should be \ndesigned so that coordination among teams is minimized. This minimal coor-\ndination needs to be achieved both for the code and for the data model. Teams \nworking on modules that communicate with each other may need to negotiate \nthe interfaces of those modules. When a module is used by many other mod-\nules, each developed by a different team, communication and negotiation become \nmore complex and burdensome. Similar considerations apply for the data model. \nScenarios for development distributability will deal with the compatibility of the \ncommunication structures and data model of the system being developed and the \ncoordination mechanisms of the organizations doing the development.\n",
      "content_length": 2624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "12.1  Other Important Quality Attributes\n187\nScalability \nTwo kinds of scalability are horizontal scalability and vertical scalability. Hori-\nzontal scalability (scaling out) refers to adding more resources to logical units, \nsuch as adding another server to a cluster of servers. Vertical scalability (scaling \nup) refers to adding more resources to a physical unit, such as adding more mem-\nory to a single computer. The problem that arises with either type of scaling is \nhow to effectively utilize the additional resources. Being effective means that the \nadditional resources result in a measurable improvement of some system quality, \ndid not require undue effort to add, and did not disrupt operations. In cloud en-\nvironments, horizontal scalability is called elasticity. Elasticity is a property that \nenables a customer to add or remove virtual machines from the resource pool (see \nChapter 26 for further discussion of such environments). These virtual machines \nare hosted on a large collection of upwards of 10,000 physical machines that are \nmanaged by the cloud provider. Scalability scenarios will deal with the impact of \nadding or removing resources, and the measures will reflect associated availabil-\nity and the load assigned to existing and new resources. \nDeployability\nDeployability is concerned with how an executable arrives at a host platform and \nhow it is subsequently invoked. Some of the issues involved in deploying soft-\nware are: How does it arrive at its host (push, where updates are sent to users un-\nbidden, or pull, where users must explicitly request updates)? How is it integrated \ninto an existing system? Can this be done while the existing system is executing? \nMobile systems have their own problems in terms of how they are updated, be-\ncause of concerns about bandwidth. Deployment scenarios will deal with the type \nof update (push or pull), the form of the update (medium, such as DVD or Inter-\nnet download, and packaging, such as executable, app, or plug-in), the resulting \nintegration into an existing system, the efficiency of executing the process, and \nthe associated risk.\nMobility\nMobility deals with the problems of movement and affordances of a platform \n(e.g., size, type of display, type of input devices, availability and volume of \nbandwidth, and battery life). Issues in mobility include battery management, \nreconnecting after a period of disconnection, and the number of different user \ninterfaces necessary to support multiple platforms. Scenarios will deal with spec-\nifying the desired effects of mobility or the various affordances. Scenarios may \nalso deal with variability, where the same software is deployed on multiple (per-\nhaps radically different) platforms.\n",
      "content_length": 2738,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "188 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nMonitorability\nMonitorability deals with the ability of the operations staff to monitor the system \nwhile it is executing. Items such as queue lengths, average transaction processing \ntime, and the health of various components should be visible to the operations \nstaff so that they can take corrective action in case of potential problems. Sce-\nnarios will deal with a potential problem and its visibility to the operator, and \npotential corrective action. \nSafety\nIn 2009 an employee of the Shushenskaya hydroelectric power station in Siberia \nsent commands over a network to remotely, and accidentally, activate an unused \nturbine. The offline turbine created a “water hammer” that flooded and then de-\nstroyed the plant and killed dozens of workers.\nThe thought that software could kill people used to belong in the realm of \nkitschy computers-run-amok science fiction. Sadly, it didn’t stay there. As soft-\nware has come to control more and more of the devices in our lives, software \nsafety has become a critical concern.\nSafety is not purely a software concern, but a concern for any system that \ncan affect its environment. As such it receives mention in Section 12.3, where we \ndiscuss system quality attributes. But there are means to address safety that are \nwholly in the software realm, which is why we discuss it here as well. \nSoftware safety is about the software’s ability to avoid entering states that \ncause or lead to damage, injury, or loss of life to actors in the software’s envi-\nronment, and to recover and limit the damage when it does enter into bad states. \nAnother way to put this is that safety is concerned with the prevention of and \nrecovery from hazardous failures. Because of this, the architectural concerns with \nsafety are almost identical to those for availability, which is also about avoiding \nand recovering from failures. Tactics for safety, then, overlap with those for avail-\nability to a large degree. Both comprise tactics to prevent failures and to detect \nand recover from failures that do occur.\nSafety is not the same as reliability. A system can be reliable (consistent \nwith its specification) but still unsafe (for example, when the specification ig-\nnores conditions leading to unsafe action). In fact, paying careful attention to the \nspecification for safety-critical software is perhaps the most powerful thing you \ncan do to produce safe software. Failures and hazards cannot be detected, pre-\nvented, or ameliorated if the software has not been designed with them in mind. \nSafety is frequently engineered by performing failure mode and effects analy-\nsis, hazard analysis, and fault tree analysis. (These techniques are discussed in \nChapter 5.) These techniques are intended to discover possible hazards that could \nresult from the system’s operation and provide plans to cope with these hazards.\n",
      "content_length": 2922,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "12.2  Other Categories of Quality Attributes\n189\n12.2  Other Categories of Quality Attributes\nWe have primarily focused on product qualities in our discussions of quality at-\ntributes, but there are other types of quality attributes that measure “goodness” of \nsomething other than the final product. Here are three: \nConceptual Integrity of the Architecture\nConceptual integrity refers to consistency in the design of the architecture, and it \ncontributes to the understandability of the architecture and leads to fewer errors \nof confusion. Conceptual integrity demands that the same thing is done in the \nsame way through the architecture. In an architecture with conceptual integrity, \nless is more. For example, there are countless ways that components can send \ninformation to each other: messages, data structures, signaling of events, and so \nforth. An architecture with conceptual integrity would feature one way only, and \nonly provide alternatives if there was a compelling reason to do so. Similarly, \ncomponents should all report and handle errors in the same way, log events or \ntransactions in the same way, interact with the user in the same way, and so forth.\nQuality in Use\nISO/IEC 25010, which we discuss in Section 12.4, has a category of qualities that \npertain to the use of the system by various stakeholders. For example, time-to-\nmarket is an important characteristic of a system, but it is not discernible from an \nexamination of the product itself. Some of the qualities in this category are these:\n■\n■Effectiveness. This refers to the distinction between building the system \ncorrectly (the system performs according to its requirements) and building \nthe correct system (the system performs in the manner the user wishes). \nEffectiveness is a measure of whether the system is correct.\n■\n■Efficiency. The effort and time required to develop a system. Put another \nway, what is the architecture’s impact on the project’s cost and schedule? \nWould a different set of architectural choices have resulted in a system \nthat would be faster or cheaper to bring to fruition? Efficiency can include \ntraining time for developers; an architecture that uses technology unfamiliar \nto the staff on hand is less buildable. Is the architecture appropriate for the \norganization in terms of its experience and its available supporting infra-\nstructure (such as test facilities or development environments)?\n■\n■Freedom from risk. The degree to which a product or system affects \neconomic status, human life, health, or the environment.\n",
      "content_length": 2549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "190 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nA special case of efficiency is how easy it is to build (that is, compile and \nassemble) the system after a change. This becomes critical during testing. A \nrecompile process that takes hours or overnight is a schedule-killer. Architects \nhave control over this by managing dependencies among modules. If the archi-\ntect doesn’t do this, then what often happens is that some bright-eyed developer \nwrites a makefile early on, it works, and people add to it and add to it. Eventually \nthe project ends up with a seven-hour compile step and very unhappy integrators \nand testers who are already behind schedule (because they always are).\nMarketability\nAn architecture’s marketability is another quality attribute of concern. Some sys-\ntems are well known by their architectures, and these architectures sometimes \ncarry a meaning all their own, independent of what other quality attributes they \nbring to the system. The current craze in building cloud-based systems has taught \nus that the perception of an architecture can be more important than the qualities \nthe architecture brings. Many organizations have felt they had to build cloud-\nbased systems (or some other technology du jour) whether or not that was the \ncorrect technical choice. \n12.3  \u0007Software Quality Attributes and \nSystem Quality Attributes\nPhysical systems, such as aircraft or automobiles or kitchen appliances, that rely \non software embedded within are designed to meet a whole other litany of qual-\nity attributes: weight, size, electric consumption, power output, pollution output, \nweather resistance, battery life, and on and on. For many of these systems, safety \ntops the list (see the sidebar).\nSometimes the software architecture can have a surprising effect on the sys-\ntem’s quality attributes. For example, software that makes inefficient use of com-\nputing resources might require additional memory, a faster processor, a bigger \nbattery, or even an additional processor. Additional processors can add to a sys-\ntem’s power consumption, weight, required cabinet space, and of course expense.\nGreen computing is an issue of growing concern. Recently there was a con-\ntroversy about how much greenhouse gas was pumped into the atmosphere by \nGoogle’s massive processor farms. Given the daily output and the number of \ndaily requests, it is possible to estimate how much greenhouse gas you cause to be \nemitted each time you ask Google to perform a search. (Current estimates range \nfrom 0.2 grams to 7 grams of CO2.) Green computing is all the rage. Eve Troeh, \non the American Public Media show “Marketplace” (July 5, 2011), reports:\n",
      "content_length": 2682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "12.3  Software Quality Attributes and System Quality Attributes\n191\nTwo percent of all U.S. electricity now goes to data centers, according \nto the Environmental Protection Agency. Electricity has become the \nbiggest cost for processing data—more than the equipment to do it, \nmore than the buildings to house that equipment. . . . Google’s making \ndata servers that can float offshore, cooled by ocean breezes. HP has \nplans to put data servers near farms, and power them with methane gas \nfrom cow pies.\nThe lesson here is that if you are the architect for software that resides in a \nlarger system, you will need to understand the quality attributes that are import-\nant for the containing system to achieve, and work with the system architects and \nengineers to see how your software architecture can contribute to achieving them.\nThe Vanishing Line between Software and System Qualities\nThis is a book about software architecture, and so we treat quality attri-\nbutes from a software architect’s perspective. But you may have already \nnoticed that the quality attributes that the software architect can bring to \nthe party are limited by the architecture of the system in which the soft-\nware runs. \nFor example: \n■\n■\nThe performance of a piece of software is fundamentally constrained \nby the performance of the computer that runs it. No matter how well you \ndesign the software, you just can’t run the latest whole-earth weather \nforecasting models on Grampa’s Commodore 64 and hope to know if it’s \ngoing to rain tomorrow.\n■\n■\nPhysical security is probably more important and more effective than \nsoftware security at preventing fraud and theft. If you don’t believe this, \nwrite your laptop’s password on a slip of paper, tape it to your laptop, \nand leave it in an unlocked car with the windows down. (Actually, don’t \nreally do that. Consider this a thought experiment.) \n■\n■\nIf we’re being perfectly honest here, how usable is a device for web \nbrowsing that has a screen smaller than a credit card and keys the size \nof a raisin?\nFor me, nowhere is the barrier between software and system more \nnebulous than in the area of safety. The thought that software—strings \nof 0’s and 1’s—can kill or maim or destroy is still an unnatural notion. Of \ncourse, it’s not the 0’s and 1’s that wreak havoc. At least, not directly. It’s \nwhat they’re connected to. Software, and the system in which it runs, has \nto be connected to the outside world in some way before it can do damage. \nThat’s the good news. The bad news is that the good news isn’t all that \ngood. Software is connected to the outside world, always. If your program \n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "192 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nhas no effect whatsoever that is observable outside of itself, it probably \nserves no purpose.\nThere are notorious examples of software-related failures. The Siberian \nhydroelectric plant catastrophe mentioned in the text, the Therac-25 fatal \nradiation overdose, the Ariane 5 explosion, and a hundred lesser known \naccidents all caused harm because the software was part of a system that \nincluded a turbine, an X-ray emitter, or a rocket’s steering controls, in the \nexamples just cited. In these cases, flawed software commanded some \nhardware in the system to take a disastrous action, and the hardware sim-\nply obeyed. Actuators are devices that connect hardware to software; they \nare the bridge between the world of 0’s and 1’s and the world of motion and \ncontrol. Send a digital value to an actuator (or write a bit string in the hard-\nware register corresponding to the actuator) and that value is translated to \nsome mechanical action, for better or worse. \nBut connection to an actuator is not required for software-related disas-\nters. Sometimes all the computer has to do is send erroneous information \nto its human operators. In September 1983, a Soviet satellite sent data \nto its ground system computer, which interpreted that data as a missile \nlaunched from the United States aimed at Moscow. Seconds later, the \ncomputer reported a second missile in flight. Soon, a third, then a fourth, \nand then a fifth appeared. Soviet Strategic Rocket Forces lieutenant colonel \nStanislav Yevgrafovich Petrov made the astonishing decision to ignore the \nwarning system, believing it to be in error. He thought it extremely unlikely \nthat the U.S. would have fired just a few missiles, thereby inviting total \nretaliatory destruction. He decided to wait it out, to see if the missiles were \nreal—that is, to see if his country’s capital city was going to be incinerated. \nAs we know, it wasn’t. The Soviet system had mistaken a rare sunlight con-\ndition for missiles in flight. Similar mistakes have occurred on the U.S. side.\nOf course, the humans don’t always get it right. On the dark and stormy \nnight of June 1, 2009, Air France flight 447 from Rio de Janeiro to Paris \nplummeted into the Atlantic Ocean, killing all on board. The Airbus A-330’s \nflight recorders were not recovered until May 2011, and as this book goes \nto publication it appears that the pilots never knew that the aircraft had en-\ntered a high-altitude stall. The sensors that measure airspeed had become \nclogged with ice and therefore unreliable. The software was required to dis-\nengage the autopilot in this situation, which it did. The human pilots thought \nthe aircraft was going too fast (and in danger of structural failure) when in \nfact it was going too slow (and falling). During the entire three-minute-plus \nplunge from 38,000 feet, the pilots kept trying to pull the nose up and throt-\ntles back to lower the speed. It’s a good bet that adding to the confusion \nwas the way the A-330’s stall warning system worked. When the system \ndetects a stall, it emits a loud audible alarm. The computers deactivate the \nstall warning when they “think” that the angle of attack measurements are \ninvalid. This can occur when the airspeed readings are very low. That is ex-\nactly what happened with Air France 447: Its forward speed dropped below \n60 knots, and the angle of attack was extremely high. As a consequence \nof a rule in the flight control software, the stall warning stopped and started \n",
      "content_length": 3548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "12.4  Using Standard Lists of Quality Attributes—or Not \n193\nseveral times. Worse, it came on whenever the pilot let the nose fall a bit \n(increasing the airspeed and taking the readings into the “valid” range, but \nstill in stall) and then stopped when he pulled back. That is, doing the right \nthing resulted in the wrong feedback and vice versa. \nWas this an unsafe system, or a safe system unsafely operated? \nUltimately the courts will decide. \nSoftware that can physically harm us is a fact of our modern life. \nSometimes the link between software and physical harm is direct, as in \nthe Ariane example, and sometimes it’s much more tenuous, as in the Air \nFrance 447 example. But as software professionals, we cannot take refuge \nin the fact that our software can’t actually inflict harm any more than the \nperson who shouts “Fire!” in a crowded theater can claim it was the stam-\npede, not the shout, that caused injury.\n—PCC\n12.4  Using Standard Lists of Quality Attributes—or Not \nArchitects have no shortage of lists of quality attributes for software systems at \ntheir disposal. The standard with the pause-and-take-a-breath title of “ISO/IEC \nFCD 25010: Systems and software engineering—Systems and software product \nQuality Requirements and Evaluation (SQuaRE)—System and software quality \nmodels,” is a good example. The standard divides quality attributes into those \nsupporting a “quality in use” model and those supporting a “product quality” \nmodel. That division is a bit of a stretch in some places, but nevertheless begins \na divide-and-conquer march through a breathtaking array of qualities. See Figure \n12.1 for this array.\nThe standard lists the following quality attributes that deal with product \nquality:\n■\n■Functional suitability. The degree to which a product or system provides \nfunctions that meet stated and implied needs when used under specified \nconditions\n■\n■Performance efficiency. Performance relative to the amount of resources \nused under stated conditions\n■\n■Compatibility. The degree to which a product, system, or component can \nexchange information with other products, systems, or components, and/or \nperform its required functions, while sharing the same hardware or software \nenvironment\n■\n■Usability. The degree to which a product or system can be used by specified \nusers to achieve specified goals with effectiveness, efficiency, and satisfac-\ntion in a specified context of use\n",
      "content_length": 2431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "194 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nSystem Software \nProduct Quality\n \nFunctional \nsuitability\nFunctional \ncompleteness\nFunctional \ncorrectness\nFunctional \nappropriateness\nPerformance \nefficiency\nTime behavior\n \nResource \nutilization\nCapacity\n \nCompatibility\n \nCoexistence\n \nInteroperability\n \nLearnability\n \nOperability\nUser interface \naesthetics\nAccessibility\n \nReliability\n \nMaturity\n \nAvailability\n \n \nFault tolerance\n \nRecoverability\n \nSecurity\n \nConfidentiality\n \nIntegrity\n \n \nNonrepudiation\n \nAccountability\n \nAuthenticity\n \nMaintainability\n \nModularity\n \nReusability\n \nAnalyzability\n \nModifiability\n \nTestability\n \nUsability\n \nAppropriateness \nrecognizability\nUser error \nprediction\nPortability\n \nAdaptability\n \nInstallability\n \nReplaceability\nFigure 12.1  The ISO/IEC FCD 25010 product quality standard\n",
      "content_length": 840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "12.4  Using Standard Lists of Quality Attributes—or Not \n195\n■\n■Reliability. The degree to which a system, product, or component performs \nspecified functions under specified conditions for a specified period of time\n■\n■Security. The degree to which a product or system protects information and \ndata so that persons or other products or systems have the degree of data \naccess appropriate to their types and levels of authorization\n■\n■Maintainability. The degree of effectiveness and efficiency with which a \nproduct or system can be modified by the intended maintainers\n■\n■Portability. The degree of effectiveness and efficiency with which a system, \nproduct, or component can be transferred from one hardware, software, or \nother operational or usage environment to another\nIn ISO 25010, these “quality characteristics” are each composed of “qual-\nity subcharacteristics” (for example, nonrepudiation is a subcharacteristic of se-\ncurity). The standard slogs through almost five dozen separate descriptions of \nquality subcharacteristics in this way. It defines for us the qualities of “pleasure” \nand “comfort.” It distinguishes among “functional correctness” and “functional \ncompleteness,” and then adds “functional appropriateness” for good measure. To \nexhibit “compatibility,” systems must either have “interoperability” or just plain \n“coexistence.” “Usability” is a product quality, not a quality-in-use quality, al-\nthough it includes “satisfaction,” which is a quality-in-use quality. “Modifiabil-\nity” and “testability” are both part of “maintainability.” So is “modularity,” which \nis a strategy for achieving a quality rather than a goal in its own right. “Avail-\nability” is part of “reliability.” “Interoperability” is part of “compatibility.” And \n“scalability” isn’t mentioned at all.\nGot all that?\nLists like these—and there are many—do serve a purpose. They can be help-\nful checklists to assist requirements gatherers in making sure that no important \nneeds were overlooked. Even more useful than standalone lists, they can serve \nas the basis for creating your own checklist that contains the quality attributes \nof concern in your domain, your industry, your organization, and your products. \nQuality attribute lists can also serve as the basis for establishing measures. If \n“pleasure” turns out to be an important concern in your system, how do you mea-\nsure it to know if your system is providing enough of it?\nHowever, general lists like these also have drawbacks. First, no list will ever \nbe complete. As an architect, you will be called upon to design a system to meet \na stakeholder concern not foreseen by any list-maker. For example, some writers \nspeak of “manageability,” which expresses how easy it is for system administra-\ntors to manage the application. This can be achieved by inserting useful instru-\nmentation for monitoring operation and for debugging and performance tuning. \nWe know of an architecture that was designed with the conscious goal of retain-\ning key staff and attracting talented new hires to a quiet region of the American \nMidwest. That system’s architects spoke of imbuing the system with “Iowabil-\nity.” They achieved it by bringing in state-of-the-art technology and giving their \ndevelopment teams wide creative latitude. Good luck finding “Iowability” in any \n",
      "content_length": 3327,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "196 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nstandard list of quality attributes, but that QA was as important to that organiza-\ntion as any other.\nSecond, lists often generate more controversy than understanding. You \nmight argue persuasively that “functional correctness” should be part of “reliabil-\nity,” or that “portability” is just a kind of “modifiability,” or that “maintainability” \nis a kind of “modifiability” (not the other way around). The writers of ISO 25010 \napparently spent time and effort deciding to make security its own characteristic, \ninstead of a subcharacteristic of functionality, which it was in a previous version. \nWe believe that effort in making these arguments could be better spent elsewhere.\nThird, these lists often purport to be taxonomies, which are lists with the \nspecial property that every member can be assigned to exactly one place. Quality \nattributes are notoriously squishy in this regard. We discussed denial of service as \nbeing part of security, availability, performance, and usability in Chapter 4.\nFinally, these lists force architects to pay attention to every quality attribute \non the list, even if only to finally decide that the particular quality attribute is ir-\nrelevant to their system. Knowing how to quickly decide that a quality attribute is \nirrelevant to a specific system is a skill gained over time.\nThese observations reinforce the lesson introduced in Chapter 4 that quality \nattribute names, by themselves, are largely useless and are at best invitations to \nbegin a conversation; that spending time worrying about what qualities are sub-\nqualities of what other qualities is also almost useless; and that scenarios provide \nthe best way for us to specify precisely what we mean when we speak of a quality \nattribute. \nUse standard lists of quality attributes to the extent that they are helpful as \nchecklists, but don’t feel the need to slavishly adhere to their terminology. \n12.5  \u0007Dealing with “X-ability”: Bringing a New \nQuality Attribute into the Fold\nSuppose, as an architect, you must deal with a quality attribute for which there \nis no compact body of knowledge, no “portfolio” like Chapters 5–11 provided \nfor those seven QAs? Suppose you find yourself having to deal with a quality \nattribute like “green computing” or “manageability” or even “Iowability”? What \ndo you do?\nCapture Scenarios for the New Quality Attribute\nThe first thing to do is interview the stakeholders whose concerns have led to the \nneed for this quality attribute. You can work with them, either individually or as \na group, to build a set of attribute characterizations that refine what is meant by \n",
      "content_length": 2680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "12.5  Dealing with “X-ability”: Bringing a New Quality Attribute into the Fold\n197\nthe QA. For example, security is often decomposed into concerns such as confi-\ndentiality, integrity, availability, and others. After that refinement, you can work \nwith the stakeholders to craft a set of specific scenarios that characterize what is \nmeant by that QA. \nOnce you have a set of specific scenarios, then you can work to generalize \nthe collection. Look at the set of stimuli you’ve collected, the set of responses, \nthe set of response measures, and so on. Use those to construct a general scenario \nby making each part of the general scenario a generalization of the specific in-\nstances you collected. \nIn our experience, the steps described so far tend to consume about half a day.\nAssemble Design Approaches for the New Quality Attribute\nAfter you have a set of guiding scenarios for the QA, you can assemble a set of \ndesign approaches for dealing with it. You can do this by \n1.\t\nRevisiting a body of patterns you’re familiar with and asking yourself how \neach one affects the QA of interest.\n2.\t\nSearching for designs that have had to deal with this QA. You can search on \nthe name you’ve given the QA itself, but you can also search for the terms \nyou chose when you refined the QA into subsidiary attribute characteriza-\ntions (such as “confidentiality” for the QA of security). \n3.\t\nFinding experts in this area and interviewing them or simply writing and \nasking them for advice. \n4.\t\nUsing the general scenario to try to catalog a list of design approaches to \nproduce the responses in the response category.\n5.\t\nUsing the general scenario to catalog a list of ways in which a problematic \narchitecture would fail to produce the desired responses, and thinking of \ndesign approaches to head off those cases.\nModel the New Quality Attribute\nIf you can build a conceptual model of the quality attribute, this can be helpful in \ncreating a set of design approaches for it. By “model,” we don’t mean anything \nmore than understanding the set of parameters to which the quality attribute is \nsensitive. For example, a model of modifiability might tell us that modifiability \nis a function of how many places in a system have to be changed in response to \na modification, and the interconnectedness of those places. A model for perfor-\nmance might tell us that throughput is a function of transactional workload, the \ndependencies among the transactions, and the number of transactions that can be \nprocessed in parallel. \n",
      "content_length": 2526,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "198 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nOnce you have a model for your QA, then you can work to catalog the ar-\nchitectural approaches (tactics and patterns) open to you for manipulating each of \nthe relevant parameters in your favor.\nAssemble a Set of Tactics for the New Quality Attribute\nThere are two sources that can be used to derive tactics for any quality attribute: \nmodels and experts. \nFigure 12.2 shows a queuing model for performance. Such models are \nwidely used to analyze the latency and throughput of various types of queuing \nsystems, including manufacturing and service environments, as well as computer \nsystems. \nWithin this model, there are seven parameters that can affect the latency that \nthe model predicts: \n■\n■Arrival rate\n■\n■Queuing discipline\n■\n■Scheduling algorithm\n■\n■Service time\n■\n■Topology \n■\n■Network bandwidth\n■\n■Routing algorithm\nResults\nRouting of \nmessages\nArrivals\nQueue\nServer\nScheduling \nalgorithm\nFigure 12.2  A generic queuing model\n",
      "content_length": 1001,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "12.5  Dealing with “X-ability”: Bringing a New Quality Attribute into the Fold\n199\nThese are the only parameters that can affect latency within this model. This \nis what gives the model its power. Furthermore, each of these parameters can be \naffected by various architectural decisions. This is what makes the model useful \nfor an architect. For example, the routing algorithm can be fixed or it could be a \nload-balancing algorithm. A scheduling algorithm must be chosen. The topology \ncan be affected by dynamically adding or removing new servers. And so forth.\nThe process of generating tactics based on a model is this:\n■\n■Enumerate the parameters of the model\n■\n■For each parameter, enumerate the architectural decisions that can affect \nthis parameter\nWhat results is a list of tactics to, in the example case, control performance \nand, in the more general case, to control the quality attribute that the model is \nconcerned with. This makes the design problem seem much more tractable. This \nlist of tactics is finite and reasonably small, because the number of parameters of \nthe model is bounded, and for each parameter, the number of architectural deci-\nsions to affect the parameter is limited.\nDeriving tactics from models is fine as long as the quality attribute in ques-\ntion has a model. Unfortunately, the number of such models is limited and is a \nsubject of active research. There are no good architectural models for usability or \nsecurity, for example. In the cases where we had no model to work from, we did \nfour things to catalog the tactics: \n1.\t\nWe interviewed experts in the field, asking them what they do as architects \nto improve the quality attribute response. \n2.\t\nWe examined systems that were touted as having high usability (or testabil-\nity, or whatever tactic we were focusing on). \n3.\t\nWe scoured the relevant design literature looking for common themes in \ndesign. \n4.\t\nWe examined documented architectural patterns to look for ways they \nachieved the quality attribute responses touted for them.\nConstruct Design Checklists for the New Quality Attribute\nFinally, examine the seven categories of design decisions in Chapter 4 and ask \nyourself (or your experts) how to specialize your new quality of interest to these \ncategories. In particular, think about reviewing a software architecture and trying \nto figure out how well it satisfies your new qualities in these seven categories. \nWhat questions would you ask the architect of that system to understand how \nthe design attempts to achieve the new quality? These are the basis for the design \nchecklist.\n",
      "content_length": 2598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "200 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\n12.6  For Further Reading\nFor most of the quality attributes we discussed in this chapter, the Internet is your \nfriend. You can find reasonable discussions of scalability, portability, and deploy-\nment strategies using your favorite search engine. Mobility is harder to find be-\ncause it has so many meanings, but look under “mobile computing” as a start.\nDistributed development is a topic covered in the International Conference \non Global Software Engineering, and looking at the proceedings of this confer-\nence will give you access to the latest research in this area (www.icgse.org).\nRelease It! [Nygard 07] has a good discussion of monitorability (which he \ncalls transparency) as well as potential problems that are manifested after ex-\ntended operation of a system. The book also includes various patterns for dealing \nwith some of the problems.\nTo gain an appreciation for the importance of software safety, we suggest \nreading some of the disaster stories that arise when software fails. A vener-\nable source is the ACM Risks Forum newsgroup, known as comp.risks in the \nUSENET community, available at www.risks.org. This list has been moderated \nby Peter Neumann since 1985 and is still going strong.\nNancy Leveson is an undisputed thought leader in the area of software and \nsafety. If you’re working in safety-critical systems, you should become familiar \nwith her work. You can start small with a paper like [Leveson 04], which dis-\ncusses a number of software-related factors that have contributed to spacecraft \naccidents. Or you can start at the top with [Leveson 11], a book that treats safety \nin the context of today’s complex, sociotechnical, software-intensive systems. \nThe Federal Aviation Administration is the U.S. government agency charged \nwith oversight of the U.S. airspace system, and the agency is extremely concerned \nabout safety. Their 2000 System Safety Handbook is a good practical overview of \nthe topic [FAA 00].\nIEEE STD-1228-1994 (“Software Safety Plans”) defines best practices for \nconducting software safety hazard analyses, to help ensure that requirements and \nattributes are specified for safety-critical software [IEEE 94]. The aeronautical \nstandard DO-178B (due to be replaced by DO-178C as this book goes to publica-\ntion) covers software safety requirements for aerospace applications.\nA discussion of safety tactics can be found in the work of Wu and Kelly \n[Wu 06]. \nIn particular, interlocks are an important tactic for safety. They enforce some \nsafe sequence of events, or ensure that a safe condition exists before an action is \ntaken. Your microwave oven shuts off when you open the door because of a hard-\nware interlock. Interlocks can be implemented in software also. For an interesting \ncase study of this, see [Wozniak 07]. \n",
      "content_length": 2854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "12.7  Discussion Questions\n201\n12.7  Discussion Questions\n1.\t\nThe Kingdom of Bhutan measures the happiness of its population, and \ngovernment policy is formulated to increase Bhutan’s GNH (gross national \nhappiness). Go read about how the GNH is measured (try www.grossna-\ntionalhappiness.com) and then sketch a general scenario for the quality \nattribute of happiness that will let you express concrete happiness require-\nments for a software system.\n2.\t\nChoose a quality attribute not described in Chapters 5–11. For that quality \nattribute, assemble a set of specific scenarios that describe what you mean \nby it. Use that set of scenarios to construct a general scenario for it.\n3.\t\nFor the QA you chose for discussion question 2, assemble a set of design \napproaches (patterns and tactics) that help you achieve it.\n4.\t\nFor the QA you chose for discussion question 2, develop a design checklist \nfor that quality attribute using the seven categories of guiding quality de-\nsign decisions outlined in Chapter 4.\n5.\t\nWhat might cause you to add a tactic or pattern to the sets of quality attri-\nbutes already described in Chapters 5–11 (or any other quality attribute, for \nthat matter)? \n6.\t\nAccording to slate.com and other sources, a teenage girl in Germany “went \ninto hiding after she forgot to set her Facebook birthday invitation to private \nand accidentally invited the entire Internet. After 15,000 people confirmed \nthey were coming, the girl’s parents canceled the party, notified police, and \nhired private security to guard their home.” Fifteen hundred people showed \nup anyway; several minor injuries ensued. Is Facebook “unsafe”? Discuss.\n7.\t\nAuthor James Gleick (“A Bug and a Crash,” www.around.com/ariane.html) \nwrites that “It took the European Space Agency 10 years and $7 billion to \nproduce Ariane 5, a giant rocket capable of hurling a pair of three-ton sat-\nellites into orbit with each launch. . . . All it took to explode that rocket less \nthan a minute into its maiden voyage . . . was a small computer program \ntrying to stuff a 64-bit number into a 16-bit space. One bug, one crash. Of \nall the careless lines of code recorded in the annals of computer science, \nthis one may stand as the most devastatingly efficient.” Write a safety sce-\nnario that addresses the Ariane 5 disaster and discuss tactics that might have \nprevented it.\n8.\t\nDiscuss how you think development distributability tends to “trade off” \nagainst the quality attributes of performance, availability, modifiability, and \ninteroperability.\n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "202 \nPart Two  Quality Attributes\t\n12—Other Quality Attributes\nExtra Credit: Close your eyes and, without peeking, spell “distributability.” \nBonus points for successfully saying “development distributability” three \ntimes as fast as you can.\n9.\t What is the relationship between mobility and security?\n10.\t Relate monitorability to observability and controllability, the two parts of \ntestability. Are they the same? If you want to make your system more of \none, can you just optimize for the other?\n",
      "content_length": 501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "203\n13\nArchitectural Tactics \nand Patterns\nI have not failed. I’ve just found \n10,000 ways that won’t work. \n—Thomas Edison\nThere are many ways to do design badly, and just a few ways to do it well. Be-\ncause success in architectural design is complex and challenging, designers have \nbeen looking for ways to capture and reuse hard-won architectural knowledge. \nArchitectural patterns and tactics are ways of capturing proven good design \nstructures, so that they can be reused. \nArchitectural patterns have seen increased interest and attention, from both \nsoftware practitioners and theorists, over the past 15 years or more. An architec-\ntural pattern \n■\n■is a package of design decisions that is found repeatedly in practice,\n■\n■has known properties that permit reuse, and \n■\n■describes a class of architectures.\nBecause patterns are (by definition) found in practice, one does not invent \nthem; one discovers them. Cataloging patterns is akin to the job of a Linnaean \nbotanist or zoologist: “discovering” patterns and describing their shared charac-\nteristics. And like the botanist, zoologist, or ecologist, the pattern cataloger strives \nto understand how the characteristics lead to different behaviors and different re-\nsponses to environmental conditions. For this reason there will never be a com-\nplete list of patterns: patterns spontaneously emerge in reaction to environmental \nconditions, and as long as those conditions change, new patterns will emerge. \nArchitectural design seldom starts from first principles. Experienced architects \ntypically think of creating an architecture as a process of selecting, tailoring, and \ncombining patterns. The software architect must decide how to instantiate a pat-\ntern—how to make it fit with the specific context and the constraints of the problem.\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "204 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nIn Chapters 5–11 we have seen a variety of architectural tactics. These are \nsimpler than patterns. Tactics typically use just a single structure or computa-\ntional mechanism, and they are meant to address a single architectural force. For \nthis reason they give more precise control to an architect when making design \ndecisions than patterns, which typically combine multiple design decisions into \na package. Tactics are the “building blocks” of design, from which architectural \npatterns are created. Tactics are atoms and patterns are molecules. Most patterns \nconsist of (are constructed from) several different tactics. For this reason we say \nthat patterns package tactics. \nIn this chapter we will take a very brief tour through the patterns universe, \ntouching on some of the most important and most commonly used patterns for ar-\nchitecture, and we will then look at the relationships between patterns and tactics: \nshowing how a pattern is constructed from tactics, and showing how tactics can \nbe used to tailor patterns when the pattern that you find in a book or on a website \ndoesn’t quite address your design needs.\n13.1  Architectural Patterns\nAn architectural pattern establishes a relationship between:\n■\n■A context. A recurring, common situation in the world that gives rise to a \nproblem.\n■\n■A problem. The problem, appropriately generalized, that arises in the given \ncontext. The pattern description outlines the problem and its variants, and \ndescribes any complementary or opposing forces. The description of the \nproblem often includes quality attributes that must be met.\n■\n■A solution. A successful architectural resolution to the problem, appro-\npriately abstracted. The solution describes the architectural structures \nthat solve the problem, including how to balance the many forces at \nwork. The solution will describe the responsibilities of and static rela-\ntionships among elements (using a module structure), or it will describe \nthe runtime behavior of and interaction between elements (laying out a \ncomponent-and-connector or allocation structure). The solution for a pat-\ntern is determined and described by:\n■\n■A set of element types (for example, data repositories, processes, and \nobjects)\n■\n■A set of interaction mechanisms or connectors (for example, method \ncalls, events, or message bus)\n■\n■A topological layout of the components \n■\n■A set of semantic constraints covering topology, element behavior, and \ninteraction mechanisms\n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "13.2  Overview of the Patterns Catalog\n205\nThe solution description should also make clear what quality attributes are \nprovided by the static and runtime configurations of elements.\nThis {context, problem, solution} form constitutes a template for document-\ning a pattern. \nComplex systems exhibit multiple patterns at once. A web-based system \nmight employ a three-tier client-server architectural pattern, but within this pat-\ntern it might also use replication (mirroring), proxies, caches, firewalls, MVC, \nand so forth, each of which may employ more patterns and tactics. And all of \nthese parts of the client-server pattern likely employ layering to internally struc-\nture their software modules.\n13.2  Overview of the Patterns Catalog\nIn this section we list an assortment of useful and widely used patterns. This cata-\nlog is not meant to be exhaustive—in fact no such catalog is possible. Rather it is \nmeant to be representative. We show patterns of runtime elements (such as broker \nor client-server) and of design-time elements (such as layers). For each pattern \nwe list the context, problem, and solution. As part of the solution, we briefly de-\nscribe the elements, relations, and constraints of each pattern. \nApplying a pattern is not an all-or-nothing proposition. Pattern definitions \ngiven in catalogs are strict, but in practice architects may choose to violate them \nin small ways when there is a good design tradeoff to be had (sacrificing a little \nof whatever the violation cost, but gaining something that the deviation gained). \nFor example, the layered pattern expressly forbids software in lower layers from \nusing software in upper layers, but there may be cases (such as to gain some per-\nformance) when an architecture might allow a few specific exceptions. \nPatterns can be categorized by the dominant type of elements that they \nshow: module patterns show modules, component-and-connector (C&C) patterns \nshow components and connectors, and allocation patterns show a combination \nof software elements (modules, components, connectors) and nonsoftware ele-\nments. Most published patterns are C&C patterns, but there are module patterns \nand allocation patterns as well. We’ll begin with the granddaddy of module pat-\nterns, the layered pattern.\nModule Patterns\nLayered Pattern\nContext: All complex systems experience the need to develop and evolve por-\ntions of the system independently. For this reason the developers of the system \nneed a clear and well-documented separation of concerns, so that modules of the \nsystem may be independently developed and maintained. \n",
      "content_length": 2605,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "206 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nProblem: The software needs to be segmented in such a way that the modules \ncan be developed and evolved separately with little interaction among the parts, \nsupporting portability, modifiability, and reuse.\nSolution: To achieve this separation of concerns, the layered pattern divides the \nsoftware into units called layers. Each layer is a grouping of modules that offers a \ncohesive set of services. There are constraints on the allowed-to-use relationship \namong the layers: the relations must be unidirectional. Layers completely parti-\ntion a set of software, and each partition is exposed through a public interface. \nThe layers are created to interact according to a strict ordering relation. If (A,B) \nis in this relation, we say that the implementation of layer A is allowed to use any \nof the public facilities provided by layer B. In some cases, modules in one layer \nmight be required to directly use modules in a nonadjacent lower layer; normally \nonly next-lower-layer uses are allowed. This case of software in a higher layer \nusing modules in a nonadjacent lower layer is called layer bridging. If many in-\nstances of layer bridging occur, the system may not meet its portability and modi-\nfiability goals that strict layering helps to achieve. Upward usages are not allowed \nin this pattern. \nOf course, none of this comes for free. Someone must design and build the \nlayers, which can often add up-front cost and complexity to a system. Also, if the \nlayering is not designed correctly, it may actually get in the way, by not provid-\ning the lower-level abstractions that programmers at the higher levels need. And \nlayering always adds a performance penalty to a system. If a call is made to a \nfunction in the top-most layer, this may have to traverse many lower layers before \nbeing executed by the hardware. Each of these layers adds some overhead of their \nown, at minimum in the form of context switching.\nTable 13.1 summarizes the solution of the layered pattern.\nLayers are almost always drawn as a stack of boxes. The allowed-to-use \nrelation is denoted by geometric adjacency and is read from the top down, as in \nFigure 13.1. \nA\nB\nC\nKey:\nLayer\nA layer is allowed to use \nthe next lower layer.\nFigure 13.1  Stack-of-boxes notation for layered designs\n",
      "content_length": 2353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "13.2  Overview of the Patterns Catalog\n207\nTable 13.1  Layered Pattern Solution\nOverview\nThe layered pattern defines layers (groupings of modules that offer \na cohesive set of services) and a unidirectional allowed-to-use \nrelation among the layers. The pattern is usually shown graphically \nby stacking boxes representing layers on top of each other. \nElements\nLayer, a kind of module. The description of a layer should define \nwhat modules the layer contains and a characterization of the \ncohesive set of services that the layer provides.\nRelations\nAllowed to use, which is a specialization of a more generic \ndepends-on relation. The design should define what the layer usage \nrules are (e.g., “a layer is allowed to use any lower layer” or “a layer \nis allowed to use only the layer immediately below it”) and any \nallowable exceptions. \nConstraints\n■\n■\nEvery piece of software is allocated to exactly one layer.\n■\n■\nThere are at least two layers (but usually there are three or \nmore).\n■\n■\nThe allowed-to-use relations should not be circular (i.e., a lower \nlayer cannot use a layer above).\nWeaknesses\n■\n■\nThe addition of layers adds up-front cost and complexity to a \nsystem.\n■\n■\nLayers contribute a performance penalty.\nSome Finer Points of Layers\nA layered architecture is one of the few places where connections among \ncomponents can be shown by adjacency, and where “above” and “below” \nmatter. If you turn Figure 13.1 upside-down so that C is on top, this would \nrepresent a completely different design. Diagrams that use arrows among \nthe boxes to denote relations retain their semantic meaning no matter the \norientation. \nThe layered pattern is one of the most commonly used patterns in all of \nsoftware engineering, but I’m often surprised by how many people still get \nit wrong.\nFirst, it is impossible to look at a stack of boxes and tell whether layer \nbridging is allowed or not. That is, can a layer use any lower layer, or just \nthe next lower one? It is the easiest thing in the world to resolve this; all the \narchitect has to do is include the answer in the key to the diagram’s nota-\ntion (something we recommend for all diagrams). For example, consider the \nlayered pattern presented in Figure 13.2 on the next page.\nBut I’m still surprised at how few architects actually bother to do this. \nAnd if they don’t, their layer diagrams are ambiguous.\nSecond, any old set of boxes stacked on top of each other does not \nconstitute a layered architecture. For instance, look at the design shown \nin Figure 13.3, which uses arrows instead of adjacency to indicate the \n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "208 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nrelationships among the boxes. Here, everything is allowed to use every-\nthing. This is decidedly not a layered architecture. The reason is that if \nLayer A is replaced by a different version, Layer C (which uses it in this fig-\nure) might well have to change. We don’t want our virtual machine layer to \nchange every time our application layer changes. But I’m still surprised at \nhow many people call a stack of boxes lined up with each other “layers” (or \nthink that layers are the same as tiers in a multi-tier architecture).\nKey:\nApplications\nServices\nData Bank\nEnvironmental Models\nEnvironment Sensing\nJVM\nOS and Hardware\nSecurity\nlayer\nSoftware in a layer is allowed to use software \nin the same layer, or any layer immediately \nbelow or to the right.\nFigure 13.2  A simple layer diagram, with a simple key answering the uses \nquestion\nLayer\nAllowed to use\nA\nB\nC\nKey:\nFigure 13.3  A wolf in layer’s clothing\n",
      "content_length": 988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "13.2  Overview of the Patterns Catalog\n209\nThird, many architectures that purport to be layered look something \nlike Figure 13.4. This diagram probably means that modules in A, B, or C \ncan use modules in D, but without a key to tell us for sure, it could mean \nanything. “Sidecars” like this often contain common utilities (sometimes \nimported), such as error handlers, communication protocols, or database \naccess mechanisms. This kind of diagram makes sense only in the case \nwhere no layer bridging is allowed in the main stack. Otherwise, D could \nsimply be made the bottom-most layer in the main stack, and the “sidecar” \ngeometry would be unnecessary. But I’m still surprised at how often I see \nthis layout go unexplained.\nSometimes layers are divided into segments denoting a finer-grained \ndecomposition of the modules. Sometimes this occurs when a preexisting \nset of units, such as imported modules, share the same allowed-to-use \nrelation. When this happens, you have to specify what usage rules are in \neffect among the segments. Many usage rules are possible, but they must \nbe made explicit. In Figure 13.5, the top and the bottom layers are\nA\nB\nC\nD\nFigure 13.4  Layers with a “sidecar” \nKey:\nLayer\nUI\nBusiness Logic\nData Access\nLocal Data\nAccess\nRemote Data\nAccess\nWeb UI\nRich\nClient\nCommand\nLine\nLayer\nsegment\nAllowed to use\nFigure 13.5  Layered design with segmented layers \n",
      "content_length": 1394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "210 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nsegmented. Segments of the top layer are not allowed to use each other, \nbut segments of the bottom layer are. If you draw the same diagram with-\nout the arrows, it will be harder to differentiate the different usage rules \nwithin segmented layers. Layered diagrams are often a source of hidden \nambiguity because the diagram does not make explicit the allowed-to-use \nrelations. \nFinally, the most important point about layering is that a layer isn’t \nallowed to use any layer above it. A module “uses” another module when it \ndepends on the answer it gets back. But a layer is allowed to make upward \ncalls, as long as it isn’t expecting an answer from them. This is how the \ncommon error-handling scheme of callbacks works. A program in layer A \ncalls a program in a lower layer B, and the parameters include a pointer to \nan error-handling program in A that the lower layer should call in case of \nerror. The software in B makes the call to the program in A, but cares not in \nthe least what it does. By not depending in any way on the contents of A, B \nis insulated from changes in A. \n—PCC\nOther Module Patterns\nDesigners in a particular domain often publish “standard” module decomposi-\ntions for systems in that domain. These standard decompositions, if put in the \n“context, problem, solution” form, constitute module decomposition patterns.\nSimilarly in the object-oriented realm, “standard” or published class/object \ndesign solutions for a class of system constitute object-oriented patterns.\nComponent-and-Connector Patterns\nBroker Pattern\nContext: Many systems are constructed from a collection of services distributed \nacross multiple servers. Implementing these systems is complex because you \nneed to worry about how the systems will interoperate—how they will connect to \neach other and how they will exchange information—as well as the availability of \nthe component services.\nProblem: How do we structure distributed software so that service users do not \nneed to know the nature and location of service providers, making it easy to dy-\nnamically change the bindings between users and providers?\nSolution: The broker pattern separates users of services (clients) from providers \nof services (servers) by inserting an intermediary, called a broker. When a client \nneeds a service, it queries a broker via a service interface. The broker then for-\nwards the client’s service request to a server, which processes the request. The ser-\nvice result is communicated from the server back to the broker, which then returns \n",
      "content_length": 2609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "13.2  Overview of the Patterns Catalog\n211\nthe result (and any exceptions) back to the requesting client. In this way the client \nremains completely ignorant of the identity, location, and characteristics of the \nserver. Because of this separation, if a server becomes unavailable, a replacement \ncan be dynamically chosen by the broker. If a server is replaced with a different \n(compatible) service, again, the broker is the only component that needs to know \nof this change, and so the client is unaffected. Proxies are commonly introduced as \nintermediaries in addition to the broker to help with details of the interaction with \nthe broker, such as marshaling and unmarshaling messages. \nThe down sides of brokers are that they add complexity (brokers and \npossibly proxies must be designed and implemented, along with messaging \nprotocols) and add a level of indirection between a client and a server, which will \nadd latency to their communication. Debugging brokers can be difficult because \nthey are involved in highly dynamic environments where the conditions leading \nto a failure may be difficult to replicate. The broker would be an obvious point of \nattack, from a security perspective, and so it needs to be hardened appropriately. \nAlso a broker, if it is not designed carefully, can be a single point of failure for \na large and complex system. And brokers can potentially be bottlenecks for \ncommunication.\nTable 13.2 summarizes the solution of the broker pattern.\nTable 13.2  Broker Pattern Solution\nOverview\nThe broker pattern defines a runtime component, called a broker, that \nmediates the communication between a number of clients and servers. \nElements\nClient, a requester of services\nServer, a provider of services\nBroker, an intermediary that locates an appropriate server to fulfill a \nclient’s request, forwards the request to the server, and returns the \nresults to the client\nClient-side proxy, an intermediary that manages the actual \ncommunication with the broker, including marshaling, sending, and \nunmarshaling of messages\nServer-side proxy, an intermediary that manages the actual \ncommunication with the broker, including marshaling, sending, and \nunmarshaling of messages\nRelations\nThe attachment relation associates clients (and, optionally, client-side \nproxies) and servers (and, optionally, server-side proxies) with brokers.\nConstraints\nThe client can only attach to a broker (potentially via a client-side \nproxy). The server can only attach to a broker (potentially via a server-\nside proxy).\nWeaknesses Brokers add a layer of indirection, and hence latency, between clients \nand servers, and that layer may be a communication bottleneck.\nThe broker can be a single point of failure.\nA broker adds up-front complexity.\nA broker may be a target for security attacks.\nA broker may be difficult to test.\n",
      "content_length": 2846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "212 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThe broker is, of course, the critical component in this pattern. The pattern \nprovides all of the modifiability benefits of the use-an-intermediary tactic \n(described in Chapter 7), an availability benefit (because the broker pattern \nmakes it easy to replace a failed server with another), and a performance benefit \n(because the broker pattern makes it easy to assign work to the least-busy server). \nHowever, the pattern also carries with it some liabilities. For example, the use of \na broker precludes performance optimizations that you might make if you knew \nthe precise location and characteristics of the server. Also the use of this pattern \nadds the overhead of the intermediary and thus latency.\nThe original version of the broker pattern, as documented by Gamma, Helm, \nJohnson, and Vlissides [Gamma 94], is given in Figure 13.6.\nThe first widely used implementation of the broker pattern was in the \nCommon Object Request Broker Architecture (CORBA). Other common uses \nof this pattern are found in Enterprise Java Beans (EJB) and Microsoft’s .NET \nplatform—essentially any modern platform for distributed service providers and \nconsumers implements some form of a broker. The service-oriented architecture \n(SOA) approach depends crucially on brokers, most commonly in the form of an \nenterprise service bus. \nModel-View-Controller Pattern\nContext: User interface software is typically the most frequently modified portion \nof an interactive application. For this reason it is important to keep modifications \n+pack_data()\n+unpack_data()\n+send_request()\n+return()\nClient-S ide Proxy\n+initialize()\n+enter_main_loop()\n+run_service()\n+use_Broker_API()\nServer\n+call_server()\n+start_task()\n+use_Broker_API()\nClient\n+pack_data()\n+unpack_data()\n+call_service()\n+send_response()\nServer-Side Proxy\n+locateServer()\n+locateClient()\n+registerServer()\n+unregisterServer()\nBroker\n+pack_data()\n+unpack_data()\n+forward_message()\n+transmit_message()\nBridge\n-transfers\n*\n1\n*\n-call\n1\n-uses\n*\n1\n0..1\n-call\n1\n-transfers\n*\n1\n*\n-call\n1\n-uses\n*\n1\nFigure 13.6  The broker pattern \n",
      "content_length": 2145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "13.2  Overview of the Patterns Catalog\n213\nto the user interface software separate from the rest of the system. Users often \nwish to look at data from different perspectives, such as a bar graph or a pie chart. \nThese representations should both reflect the current state of the data. \nProblem: How can user interface functionality be kept separate from application \nfunctionality and yet still be responsive to user input, or to changes in the under-\nlying application’s data? And how can multiple views of the user interface be cre-\nated, maintained, and coordinated when the underlying application data changes?\nSolution: The model-view-controller (MVC) pattern separates application func-\ntionality into three kinds of components: \n■\n■A model, which contains the application’s data\n■\n■A view, which displays some portion of the underlying data and interacts \nwith the user\n■\n■A controller, which mediates between the model and the view and manages \nthe notifications of state changes\nMVC is not appropriate for every situation. The design and implementation \nof three distinct kinds of components, along with their various forms of \ninteraction, may be costly, and this cost may not make sense for relatively \nsimple user interfaces. Also, the match between the abstractions of MVC and \ncommercial user interface toolkits is not perfect. The view and the controller split \napart input and output, but these functions are often combined into individual \nwidgets. This may result in a conceptual mismatch between the architecture and \nthe user interface toolkit.\nTable 13.3 summarizes the solution of the MVC pattern.\nTable 13.3  Model-View-Controller Pattern Solution\nOverview\nThe MVC pattern breaks system functionality into three components: a \nmodel, a view, and a controller that mediates between the model and \nthe view.\nElements\nThe model is a representation of the application data or state, and it \ncontains (or provides an interface to) application logic.\nThe view is a user interface component that either produces a \nrepresentation of the model for the user or allows for some form of \nuser input, or both.\nThe controller manages the interaction between the model and the \nview, translating user actions into changes to the model or changes to \nthe view.\nRelations\nThe notifies relation connects instances of model, view, and controller, \nnotifying elements of relevant state changes. \nConstraints\nThere must be at least one instance each of model, view, and \ncontroller.\nThe model component should not interact directly with the controller.\nWeaknesses\nThe complexity may not be worth it for simple user interfaces.\nThe model, view, and controller abstractions may not be good fits for \nsome user interface toolkits.\n",
      "content_length": 2731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "214 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThere may, in fact, be many views and many controllers associated with \na model. For example, a set of business data may be represented as columns of \nnumbers in a spreadsheet, as a scatter plot, or as a pie chart. Each of these is a \nseparate view, and this view can be dynamically updated as the model changes \n(for example, showing live transactions in a transaction processing system). A \nmodel may be updated by different controllers; for example, a map could be \nzoomed and panned via mouse movements, trackball movements, keyboard \nclicks, or voice commands; each of these different forms of input needs to be \nmanaged by a controller. \nThe MVC components are connected to each other via some flavor of \nnotification, such as events or callbacks. These notifications contain state updates. \nA change in the model needs to be communicated to the views so that they may \nbe updated. An external event, such as a user input, needs to be communicated to \nthe controller, which may in turn update the view and/or the model. Notifications \nmay be either push or pull.\nBecause these components are loosely coupled, it is easy to develop and \ntest them in parallel, and changes to one have minimal impact on the others. The \nrelationships between the components of MVC are shown in Figure 13.7.\n• Encapsulates application state\n• Responds to state queries\n• Exposes application functionality\n• Notifies views of changes\nModel\n• Renders the models\n• Requests updates from models\n• Sends user gestures to controller\n• Allows controller to select view\nView\n• Defines application behavior\n• Maps user actions to model updates\n• Selects view for response\n• One for each functionality\nController\nState\nQuery\nState\nChange\nUser Gestures\nView Selection\nChange\nNotification\nKey:\nEvents\nMethod \nInvocations\nFigure 13.7  The model-view-controller pattern\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "13.2  Overview of the Patterns Catalog\n215\nThe MVC pattern is widely used in user interface libraries such as Java’s \nSwing classes, Microsoft’s ASP.NET framework, Adobe’s Flex software \ndevelopment kit, Nokia’s Qt framework, and many others. As such, it is common \nfor a single application to contain many instances of MVC (often one per user \ninterface object).\nPipe-and-Filter Pattern\nContext: Many systems are required to transform streams of discrete data items, \nfrom input to output. Many types of transformations occur repeatedly in practice, \nand so it is desirable to create these as independent, reusable parts. \nProblem: Such systems need to be divided into reusable, loosely coupled com-\nponents with simple, generic interaction mechanisms. In this way they can be \nflexibly combined with each other. The components, being generic and loosely \ncoupled, are easily reused. The components, being independent, can execute in \nparallel.\nSolution: The pattern of interaction in the pipe-and-filter pattern is characterized \nby successive transformations of streams of data. Data arrives at a filter’s input \nport(s), is transformed, and then is passed via its output port(s) through a pipe to \nthe next filter. A single filter can consume data from, or produce data to, one or \nmore ports. \nThere are several weaknesses associated with the pipe-and-filter pattern. For \ninstance, this pattern is typically not a good choice for an interactive system, as \nit disallows cycles (which are important for user feedback). Also, having large \nnumbers of independent filters can add substantial amounts of computational \noverhead, because each filter runs as its own thread or process. Also, pipe-and-\nfilter systems may not be appropriate for long-running computations, without the \naddition of some form of checkpoint/restore functionality, as the failure of any \nfilter (or pipe) can cause the entire pipeline to fail.\nThe solution of the pipe-and-filter pattern is summarized in Table 13.4. \nPipes buffer data during communication. Because of this property, filters can \nexecute asynchronously and concurrently. Moreover, a filter typically does not \nknow the identity of its upstream or downstream filters. For this reason, pipeline \npipe-and-filter systems have the property that the overall computation can be \ntreated as the functional composition of the computations of the filters, making it \neasier for the architect to reason about end-to-end behavior.\nData transformation systems are typically structured as pipes and filters, \nwith each filter responsible for one part of the overall transformation of the input \ndata. The independent processing at each step supports reuse, parallelization, and \nsimplified reasoning about overall behavior. Often such systems constitute the \nfront end of signal-processing applications. These systems receive sensor data at \na set of initial filters; each of these filters compresses the data and performs initial \nprocessing (such as smoothing). Downstream filters reduce the data further and \n",
      "content_length": 3044,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "216 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\ndo synthesis across data derived from different sensors. The final filter typically \npasses its data to an application, for example providing input to modeling or \nvisualization tools. \nOther systems that use pipe-and-filter include those built using UNIX pipes, \nthe request processing architecture of the Apache web server, the map-reduce \npattern (presented later in this chapter), Yahoo! Pipes for processing RSS feeds, \nmany workflow engines, and many scientific computation systems that have to \nprocess and analyze large streams of captured data. Figure 13.8 shows a UML \ndiagram of a pipe-and-filter system.\nTable 13.4  Pipe-and-Filter Pattern Solution\nOverview\nData is transformed from a system’s external inputs to its external \noutputs through a series of transformations performed by its filters \nconnected by pipes.\nElements\nFilter, which is a component that transforms data read on its input \nport(s) to data written on its output port(s). Filters can execute \nconcurrently with each other. Filters can incrementally transform \ndata; that is, they can start producing output as soon as they start \nprocessing input. Important characteristics include processing rates, \ninput/output data formats, and the transformation executed by the \nfilter.\nPipe, which is a connector that conveys data from a filter’s output \nport(s) to another filter’s input port(s). A pipe has a single source \nfor its input and a single target for its output. A pipe preserves the \nsequence of data items, and it does not alter the data passing \nthrough. Important characteristics include buffer size, protocol of \ninteraction, transmission speed, and format of the data that passes \nthrough a pipe.\nRelations\nThe attachment relation associates the output of filters with the input \nof pipes and vice versa. \nConstraints\nPipes connect filter output ports to filter input ports. \nConnected filters must agree on the type of data being passed along \nthe connecting pipe.\nSpecializations of the pattern may restrict the association of \ncomponents to an acyclic graph or a linear sequence, sometimes \ncalled a pipeline.\nOther specializations may prescribe that components have certain \nnamed ports, such as the stdin, stdout, and stderr ports of UNIX \nfilters.\nWeaknesses\nThe pipe-and-filter pattern is typically not a good choice for an \ninteractive system.\nHaving large numbers of independent filters can add substantial \namounts of computational overhead.\nPipe-and-filter systems may not be appropriate for long-running \ncomputations.\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "13.2  Overview of the Patterns Catalog\n217\ncapacity = 40\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 30 sec and retry\ncapacity = 50\nend-of-data = ”EOT” String\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 20 sec and retry\ncapacity = 10\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 60 sec and retry\ncapacity = 40\nend-of-data = empty record\nwhen-full = block for 2 sec and retry\nwhen-empty = block for 30 sec and retry\n«pipe»\n«pipe»\n«pipe»\n«pipe»\nout\nin\nout\nin\nout\nout\nin\nin\n«filter»\n:XmlToObject\n«filter»\n:Process\nPayment\n«filter»\n:FormatRejected\nRecords\n«filter»\n:Calculate\nDirectDeposit\n«filter»\n:Format\nDirectDeposit\nFigure 13.8  A UML diagram of a pipe-and-filter-based system\nClient-Server Pattern\nContext: There are shared resources and services that large numbers of distrib-\nuted clients wish to access, and for which we wish to control access or quality of \nservice.\nProblem: By managing a set of shared resources and services, we can promote \nmodifiability and reuse, by factoring out common services and having to modify \nthese in a single location, or a small number of locations. We want to improve \nscalability and availability by centralizing the control of these resources and ser-\nvices, while distributing the resources themselves across multiple physical servers. \nSolution: Clients interact by requesting services of servers, which provide a set \nof services. Some components may act as both clients and servers. There may be \none central server or multiple distributed ones. \nThe client-server pattern solution is summarized in Table 13.5; the \ncomponent types are clients and servers; the principal connector type for the \nclient-server pattern is a data connector driven by a request/reply protocol used \nfor invoking services. \nSome of the disadvantages of the client-server pattern are that the server \ncan be a performance bottleneck and it can be a single point of failure. Also, \ndecisions about where to locate functionality (in the client or in the server) are \noften complex and costly to change after a system has been built.\n",
      "content_length": 2159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "218 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.5  Client-Server Pattern Solution\nOverview\nClients initiate interactions with servers, invoking services as \nneeded from those servers and waiting for the results of those \nrequests.\nElements\nClient, a component that invokes services of a server \ncomponent. Clients have ports that describe the services they \nrequire. \nServer, a component that provides services to clients. Servers \nhave ports that describe the services they provide. Important \ncharacteristics include information about the nature of the \nserver ports (such as how many clients can connect) and \nperformance characteristics (e.g., maximum rates of service \ninvocation). \nRequest/reply connector, a data connector employing a \nrequest/reply protocol, used by a client to invoke services on a \nserver. Important characteristics include whether the calls are \nlocal or remote, and whether data is encrypted.\nRelations\nThe attachment relation associates clients with servers.\nConstraints\nClients are connected to servers through request/reply \nconnectors.\nServer components can be clients to other servers. \nSpecializations may impose restrictions:\n■\n■\nNumbers of attachments to a given port\n■\n■\nAllowed relations among servers\nComponents may be arranged in tiers, which are logical \ngroupings of related functionality or functionality that will share \na host computing environment (covered more later in this \nchapter).\nWeaknesses\nServer can be a performance bottleneck.\nServer can be a single point of failure.\nDecisions about where to locate functionality (in the client or \nin the server) are often complex and costly to change after a \nsystem has been built.\nSome common examples of systems that use the client-server pattern are these: \n■\n■Information systems running on local networks where the clients are GUI-\nlaunched applications and the server is a database management system\n■\n■Web-based applications where the clients are web browsers and the servers \nare components running on an e-commerce site \nThe computational flow of pure client-server systems is asymmetric: \nclients initiate interactions by invoking services of servers. Thus, the client must \nknow the identity of a service to invoke it, and clients initiate all interactions. \nIn contrast, servers do not know the identity of clients in advance of a service \nrequest and must respond to the initiated client requests. \nIn early forms of client-server, service invocation is synchronous: the \nrequester of a service waits, or is blocked, until a requested service completes its \n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "13.2  Overview of the Patterns Catalog\n219\nactions, possibly providing a return result. However, variants of the client-server \npattern may employ more-sophisticated connector protocols. For example:\n■\n■Web browsers don’t block until the data request is served up.\n■\n■In some client-server patterns, servers are permitted to initiate certain \nactions on their clients. This might be done by allowing a client to register \nnotification procedures, or callbacks, that the server calls at specific times. \n■\n■In other systems service calls over a request/reply connector are bracketed \nby a “session” that delineates the start and end of a set of a client-server \ninteraction.\nThe client-server pattern separates client applications from the services they \nuse. This pattern simplifies systems by factoring out common services, which are \nreusable. Because servers can be accessed by any number of clients, it is easy \nto add new clients to a system. Similarly, servers may be replicated to support \nscalability or availability. \nThe World Wide Web is the best-known example of a system that is based on \nthe client-server pattern, allowing clients (web browsers) to access information \nfrom servers across the Internet using HyperText Transfer Protocol (HTTP). \nHTTP is a request/reply protocol. HTTP is stateless; the connection between the \nclient and the server is terminated after each response from the server.\nFigure 13.9 uses an informal notation to describe the client-server view of \nan automatic teller machine (ATM) banking system.\nserver\nServer\nTCP socket connector with\nclient and server ports\nFTX server\ndaemon\nATM OS/2\nclient process\nWindows\napplication\nclient\nclient\nclient\nClient\nclient\nclient\nserver server\nserver\nserver\nKey:\nBank\ntransaction\nauthorizer\nATM\nmonitoring\nserver\nATM\nreconfiguration\nserver\nATM main\nprocess\nReconfigure\nand update\nprocess\nMonitoring\nstation\nprogram\nFigure 13.9  The client-server architecture of an ATM banking system \n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "220 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nPeer-to-Peer Pattern\nContext: Distributed computational entities—each of which is considered \nequally important in terms of initiating an interaction and each of which provides \nits own resources—need to cooperate and collaborate to provide a service to a \ndistributed community of users.\nProblem: How can a set of “equal” distributed computational entities be con-\nnected to each other via a common protocol so that they can organize and share \ntheir services with high availability and scalability?\nSolution: In the peer-to-peer (P2P) pattern, components directly interact as \npeers. All peers are “equal” and no peer or group of peers can be critical for \nthe health of the system. Peer-to-peer communication is typically a request/\nreply interaction without the asymmetry found in the client-server pattern. \nThat is, any component can, in principle, interact with any other component by \nrequesting its services. The interaction may be initiated by either party—that \nis, in client-server terms, each peer component is both a client and a server. \nSometimes the interaction is just to forward data without the need for a reply. \nEach peer provides and consumes similar services and uses the same protocol. \nConnectors in peer-to-peer systems involve bidirectional interactions, reflecting \nthe two-way communication that may exist between two or more peer-to-peer \ncomponents. \nPeers first connect to the peer-to-peer network on which they discover other \npeers they can interact with, and then initiate actions to achieve their computation \nby cooperating with other peers by requesting services. Often a peer’s search for \nanother peer is propagated from one peer to its connected peers for a limited \nnumber of hops. A peer-to-peer architecture may have specialized peer nodes \n(called supernodes) that have indexing or routing capabilities and allow a regular \npeer’s search to reach a larger number of peers. \nPeers can be added and removed from the peer-to-peer network with no sig-\nnificant impact, resulting in great scalability for the whole system. This provides \nflexibility for deploying the system across a highly distributed platform.\nTypically multiple peers have overlapping capabilities, such as providing \naccess to the same data or providing equivalent services. Thus, a peer acting as \nclient can collaborate with multiple peers acting as servers to complete a certain \ntask. If one of these multiple peers becomes unavailable, the others can still pro-\nvide the services to complete the task. The result is improved overall availability. \nThere are also performance advantages: The load on any given peer component \nacting as a server is reduced, and the responsibilities that might have required \nmore server capacity and infrastructure to support it are distributed. This can de-\ncrease the need for other communication for updating data and for central server \nstorage, but at the expense of storing the data locally.\n",
      "content_length": 3020,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "13.2  Overview of the Patterns Catalog\n221\nThe drawbacks of the peer-to-peer pattern are strongly related to its \nstrengths. Because peer-to-peer systems are decentralized, managing security, \ndata consistency, data and service availability, backup, and recovery are all more \ncomplex. In many cases it is difficult to provide guarantees with peer-to-peer \nsystems because the peers come and go; instead, the architect can, at best, offer \nprobabilities that quality goals will be met, and these probabilities typically in-\ncrease with the size of the population of peers. \nTable 13.6 on the next page summarizes the peer-to-peer pattern solution. \nPeer-to-peer computing is often used in distributed computing applications \nsuch as file sharing, instant messaging, desktop grid computing, routing, and \nwireless ad hoc networking. Examples of peer-to-peer systems include file-shar-\ning networks such as BitTorrent and eDonkey, and instant messaging and VoIP \napplications such as Skype. Figure 13.10 shows an example of an instantiation of \nthe peer-to-peer pattern.\nA\nB\nmoldy\n69.95.63.49\namidala\n70.116.152.15\nanakin\n207.192.20.13\nlambda\n50.64.16.14\noutrider\n74.12.41.111\nnaboo\n157.66.24.26\nKey:\nLeaf peer\nUltrapeer\nGnutella port\nHTTP file transfer\nfrom A to B\nRequest/reply using Gnutella\nprotocol over TCP or UDP\nFigure 13.10  A peer-to-peer view of a Gnutella network using an informal C&C \nnotation. For brevity, only a few peers are identified. Each of the identified leaf \npeers uploads and downloads files directly from other peers.\n",
      "content_length": 1543,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "222 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.6  Peer-to-Peer Pattern Solution\nOverview\nComputation is achieved by cooperating peers that request service \nfrom and provide services to one another across a network.\nElements\nPeer, which is an independent component running on a network \nnode. Special peer components can provide routing, indexing, and \npeer search capability.\nRequest/reply connector, which is used to connect to the peer \nnetwork, search for other peers, and invoke services from other \npeers. In some cases, the need for a reply is done away with.\nRelations\nThe  relation associates peers with their connectors. Attachments \nmay change at runtime.\nConstraints\nRestrictions may be placed on the following:\n■\n■\nThe number of allowable attachments to any given peer\n■\n■\nThe number of hops used for searching for a peer\n■\n■\nWhich peers know about which other peers\nSome P2P networks are organized with star topologies, in which \npeers only connect to supernodes.\nWeaknesses\nManaging security, data consistency, data/service availability, \nbackup, and recovery are all more complex.\nSmall peer-to-peer systems may not be able to consistently achieve \nquality goals such as performance and availability.\nService-Oriented Architecture Pattern\nContext: A number of services are offered (and described) by service provid-\ners and consumed by service consumers. Service consumers need to be able \nto understand and use these services without any detailed knowledge of their \nimplementation.\nProblem: How can we support interoperability of distributed components run-\nning on different platforms and written in different implementation languages, \nprovided by different organizations, and distributed across the Internet? How can \nwe locate services and combine (and dynamically recombine) them into meaning-\nful coalitions while achieving reasonable performance, security, and availability?\nSolution: The service-oriented architecture (SOA) pattern describes a collection \nof distributed components that provide and/or consume services. In an SOA, ser-\nvice provider components and service consumer components can use different \nimplementation languages and platforms. Services are largely standalone: service \nproviders and service consumers are usually deployed independently, and often \nbelong to different systems or even different organizations. Components have in-\nterfaces that describe the services they request from other components and the \nservices they provide. A service’s quality attributes can be specified and guar-\nanteed with a service-level agreement (SLA). In some cases, these are legally \nbinding. Components achieve their computation by requesting services from one \nanother.\n",
      "content_length": 2742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "13.2  Overview of the Patterns Catalog\n223\nThe elements in this pattern include service providers and service consum-\ners, which in practice can take different forms, from JavaScript running on a \nweb browser to CICS transactions running on a mainframe. In addition to the \nservice provider and service consumer components, an SOA application may \nuse specialized components that act as intermediaries and provide infrastruc-\nture services:\n■\n■Service invocation can be mediated by an enterprise service bus (ESB). An \nESB routes messages between service consumers and service providers. In \naddition, an ESB can convert messages from one protocol or technology to \nanother, perform various data transformations (e.g., format, content, split-\nting, merging), perform security checks, and manage transactions. Using an \nESB promotes interoperability, security, and modifiability. Of course, com-\nmunicating through an ESB adds overhead thereby lowering performance, \nand introduces an additional point of failure. When an ESB is not in place, \nservice providers and consumers communicate with each other in a point-\nto-point fashion.\n■\n■To improve the independence of service providers, a service registry can be \nused in SOA architectures. The registry is a component that allows services \nto be registered at runtime. This enables runtime discovery of services, \nwhich increases system modifiability by hiding the location and identity of \nthe service provider. A registry can even permit multiple live versions of the \nsame service.\n■\n■An orchestration server (or orchestration engine) orchestrates the interac-\ntion among various service consumers and providers in an SOA system. It \nexecutes scripts upon the occurrence of a specific event (e.g., a purchase \norder request arrived). Applications with well-defined business processes or \nworkflows that involve interactions with distributed components or systems \ngain in modifiability, interoperability, and reliability by using an orches-\ntration server. Many commercially available orchestration servers support \nvarious workflow or business process language standards.\nThe basic types of connectors used in SOA are these: \n■\n■SOAP. The standard protocol for communication in the web services tech-\nnology. Service consumers and providers interact by exchanging request/\nreply XML messages typically on top of HTTP.\n■\n■Representational State Transfer (REST). A service consumer sends non-\nblocking HTTP requests. These requests rely on the four basic HTTP com-\nmands (POST, GET, PUT, DELETE) to tell the service provider to create, \nretrieve, update, or delete a resource.\n■\n■Asynchronous messaging, a “fire-and-forget” information exchange. \nParticipants do not have to wait for an acknowledgment of receipt, because \nthe infrastructure is assumed to have delivered the message successfully. \nThe messaging connector can be point-to-point or publish-subscribe.\n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "224 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nIn practice, SOA environments may involve a mix of the three connectors \njust listed, along with legacy protocols and other communication alternatives \n(e.g., SMTP). Commercial products such as IBM’s WebSphere MQ, Microsoft’s \nMSMQ, or Apache’s ActiveMQ are infrastructure components that provide asyn-\nchronous messaging. SOAP and REST are described in more detail in Chapter 6.\nAs you can see, the SOA pattern can be quite complex to design and im-\nplement (due to dynamic binding and the concomitant use of metadata). Other \npotential problems with this pattern include the performance overhead of the \nmiddleware that is interposed between services and clients and the lack of perfor-\nmance guarantees (because services are shared and, in general, not under control \nof the requester). These weaknesses are all shared with the broker pattern, which \nis not surprising because the SOA pattern shares many of the design concepts and \ngoals of broker. In addition, because you do not, in general, control the evolution \nof the services that you use, you may have to endure high and unplanned-for \nmaintenance costs. \nTable 13.7 summarizes the SOA pattern.\nThe main benefit and the major driver of SOA is interoperability. Because \nservice providers and service consumers may run on different platforms, ser-\nvice-oriented architectures often integrate a variety of systems, including legacy \nsystems. SOA also offers the necessary elements to interact with external ser-\nvices available over the Internet. Special SOA components such as the registry or \nthe ESB also allow dynamic reconfiguration, which is useful when there’s a need \nto replace or add versions of components with no system interruption. \nFigure 13.11 shows the SOA view of a system called Adventure Builder. \nAdventure Builder allows a customer on the web to assemble a vacation by \nchoosing an activity and lodging at and transportation to a destination. The Ad-\nventure Builder system interacts with external service providers to construct the \nvacation, and with bank services to process payment. The central OPC (Order \nProcessing Center) component coordinates the interaction with internal and ex-\nternal service consumers and providers. Note that the external providers can be \nlegacy mainframe systems, Java systems, .NET systems, and so on. The nature of \nthese external components is transparent because SOAP provides the necessary \ninteroperability. \n",
      "content_length": 2503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "13.2  Overview of the Patterns Catalog\n225\nTable 13.7  Service-Oriented Architecture Pattern Solution\nOverview\nComputation is achieved by a set of cooperating components \nthat provide and/or consume services over a network. The \ncomputation is often described using a workflow language.\nElements\nComponents:\n■\n■\nService providers, which provide one or more services \nthrough published interfaces. Concerns are often tied to \nthe chosen implementation technology, and include perfor-\nmance, authorization constraints, availability, and cost. In \nsome cases these properties are specified in a service-level \nagreement.\n■\n■\nService consumers, which invoke services directly or through \nan intermediary.\n■\n■\nService providers may also be service consumers.\n■\n■\nESB, which is an intermediary element that can route and \ntransform messages between service providers and consum-\ners.\n■\n■\nRegistry of services, which may be used by providers to \nregister their services and by consumers to discover services \nat runtime.\n■\n■\nOrchestration server, which coordinates the interactions \nbetween service consumers and providers based on \nlanguages for business processes and workflows.\nConnectors:\n■\n■\nSOAP connector, which uses the SOAP protocol for \nsynchronous communication between web services, typically \nover HTTP. \n■\n■\nREST connector, which relies on the basic request/reply \noperations of the HTTP protocol.\n■\n■\nAsynchronous messaging connector, which uses a \nmessaging system to offer point-to-point or publish-subscribe \nasynchronous message exchanges.\nRelations\nAttachment of the different kinds of components available to the \nrespective connectors\nConstraints\nService consumers are connected to service providers, but \nintermediary components (e.g., ESB, registry, orchestration \nserver) may be used. \nWeaknesses\nSOA-based systems are typically complex to build.\nYou don’t control the evolution of independent services.\nThere is a performance overhead associated with the \nmiddleware, and services may be performance bottlenecks, and \ntypically do not provide performance guarantees.\n",
      "content_length": 2086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "226 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nKey:\nAdventure Builder\njdbc\njdbc\nTBD\nOpcOrder\nTrackingService\nOpcPurchase\nOrderService\nWeb\nService\nBroker\nWeb\nbrowser\nConsumer\nWeb site\nOPC\nBank\nAdventure\nCatalog\nDB\nUser’s\ne-mail\nclient\nAirline\nProvider\nLodging\nProvider\nActivity\nProvider\nAdventure\nOPC DB\nService\nRegistry\nActivityPO\n Service\nLodgingPO\nService\nAirlinePO\nService\nCreditCard\nService\nClient-side\napplication\nJava EE\napplication\nWeb services\nendpoint\nData\nrepository\nHTTP/HTTPS\nSOAP call\nData access\nSMTP\nScope of the\napplication (not \na component)\nExternal Web\nservice provider\nFigure 13.11  Diagram of the SOA view for the Adventure Builder system. OPC \nstands for “Order Processing Center.”\nPublish-Subscribe Pattern\nContext: There are a number of independent producers and consumers of data \nthat must interact. The precise number and nature of the data producers and con-\nsumers are not predetermined or fixed, nor is the data that they share. \n",
      "content_length": 986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "13.2  Overview of the Patterns Catalog\n227\nProblem: How can we create integration mechanisms that support the ability to \ntransmit messages among the producers and consumers in such a way that they \nare unaware of each other’s identity, or potentially even their existence? \nSolution: In the publish-subscribe pattern, summarized in Table 13.8, compo-\nnents interact via announced messages, or events. Components may subscribe \nto a set of events. It is the job of the publish-subscribe runtime infrastructure to \nmake sure that each published event is delivered to all subscribers of that event. \nThus, the main form of connector in these patterns is an event bus. Publisher \ncomponents place events on the bus by announcing them; the connector then de-\nlivers those events to the subscriber components that have registered an interest in \nthose events. Any component may be both a publisher and a subscriber.\nPublish-subscribe adds a layer of indirection between senders and receivers. \nThis has a negative effect on latency and potentially scalability, depending on \nhow it is implemented. One would typically not want to use publish-subscribe in \na system that had hard real-time deadlines to meet, as it introduces uncertainty in \nmessage delivery times.\nAlso, the publish-subscribe pattern suffers in that it provides less control \nover ordering of messages, and delivery of messages is not guaranteed (because \nthe sender cannot know if a receiver is listening). This can make the publish-sub-\nscribe pattern inappropriate for complex interactions where shared state is critical.\nTable 13.8  Publish-Subscribe Pattern Solution\nOverview\nComponents publish and subscribe to events. When an event is \nannounced by a component, the connector infrastructure dispatches \nthe event to all registered subscribers.\nElements\nAny C&C component with at least one publish or subscribe port. \nConcerns include which events are published and subscribed to, and \nthe granularity of events.\nThe publish-subscribe connector, which will have announce and listen \nroles for components that wish to publish and subscribe to events.\nRelations\nThe attachment relation associates components with the publish-\nsubscribe connector by prescribing which components announce \nevents and which components are registered to receive events.\nConstraints\nAll components are connected to an event distributor that may be \nviewed as either a bus—connector—or a component. Publish ports \nare attached to announce roles and subscribe ports are attached to \nlisten roles. Constraints may restrict which components can listen to \nwhich events, whether a component can listen to its own events, and \nhow many publish-subscribe connectors can exist within a system.\nA component may be both a publisher and a subscriber, by having \nports of both types.\nWeaknesses\nTypically increases latency and has a negative effect on scalability and \npredictability of message delivery time.\nLess control over ordering of messages, and delivery of messages is \nnot guaranteed.\n",
      "content_length": 3028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "228 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThere are some specific refinements of this pattern that are in common use. \nWe will describe several of these later in this section.\nThe computational model for the publish-subscribe pattern is best thought of \nas a system of independent processes or objects, which react to events generated \nby their environment, and which in turn cause reactions in other components as \na side effect of their event announcements. An example of the publish-subscribe \npattern, implemented on top of the Eclipse platform, is shown in Figure 13.12.\nTypical examples of systems that employ the publish-subscribe pattern are \nthe following:\n■\n■Graphical user interfaces, in which a user’s low-level input actions are \ntreated as events that are routed to appropriate input handlers\n■\n■MVC-based applications, in which view components are notified when the \nstate of a model object changes\n■\n■Enterprise resource planning (ERP) systems, which integrate many compo-\nnents, each of which is only interested in a subset of system events\n■\n■Extensible programming environments, in which tools are coordinated \nthrough events\n■\n■Mailing lists, where a set of subscribers can register interest in specific \ntopics \nKey:\nEclipse UI event manager\nRegister \naction\nhandlers\nUI\nevent\nhandle \nUI event\nCRUD\nfact data\nassert/modify/\nretract fact\nSEI.ArchE.UI\nplug-in config\nviews and\neditors\nFact\ndata in\nmemory\nArchE\ncore\nlistener\naction\nhandler\nArchE\ncore\nfaçade\nJess\nnew or\nsetField()\nnotify data\nchange\nregister views as \nobserver of facts\nregister to fact \ndata changes\nnotify fact\ndata change\nAction\nhandler\nobject\nUI screen\nobject\nJava\nobject\nExternal\nprogram\nXML file\nEvent manager\n(part of Eclipse\nplatform)\nRegister to\nlisten for event\nEvent send/\nreceive\nJava method\ncall\nFigure 13.12  A typical publish-subscribe pattern realization\n",
      "content_length": 1888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "13.2  Overview of the Patterns Catalog\n229\n■\n■Social networks, where “friends” are notified when changes occur to a \nperson’s website\nThe publish-subscribe pattern is used to send events and messages to an un-\nknown set of recipients. Because the set of event recipients is unknown to the \nevent producer, the correctness of the producer cannot, in general, depend on \nthose recipients. Thus, new recipients can be added without modification to the \nproducers. \nHaving components be ignorant of each other’s identity results in easy mod-\nification of the system (adding or removing producers and consumers of data) but \nat the cost of runtime performance, because the publish-subscribe infrastructure \nis a kind of indirection, which adds latency. In addition, if the publish-subscribe \nconnector fails completely, this is a single point of failure for the entire system.\nThe publish-subscribe pattern can take several forms: \n■\n■List-based publish-subscribe is a realization of the pattern where every \npublisher maintains a subscription list—a list of subscribers that have \nregistered an interest in receiving the event. This version of the pattern is \nless decoupled than others, as we shall see below, and hence it does not \nprovide as much modifiability, but it can be quite efficient in terms of \nruntime overhead. Also, if the components are distributed, there is no single \npoint of failure.\n■\n■Broadcast-based publish-subscribe differs from list-based publish-\nsubscribe in that publishers have less (or no) knowledge of the subscribers. \nPublishers simply publish events, which are then broadcast. Subscribers \n(or in a distributed system, services that act on behalf of the subscribers) \nexamine each event as it arrives and determine whether the published event \nis of interest. This version has the potential to be very inefficient if there \nare lots of messages and most messages are not of interest to a particular \nsubscriber.\n■\n■Content-based publish-subscribe is distinguished from the previous two \nvariants, which are broadly categorized as “topic-based.” Topics are \npredefined events, or messages, and a component subscribes to all events \nwithin the topic. Content, on the other hand, is much more general. Each \nevent is associated with a set of attributes and is delivered to a subscriber \nonly if those attributes match subscriber-defined patterns.\nIn practice the publish-subscribe pattern is typically realized by some form \nof message-oriented middleware, where the middleware is realized as a broker, \nmanaging the connections and channels of information between producers and \nconsumers. This middleware is often responsible for the transformation of mes-\nsages (or message protocols), in addition to routing and sometimes storing the \nmessages. Thus the publish-subscribe pattern inherits the strengths and weak-\nnesses of the broker pattern.\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "230 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nShared-Data Pattern\nContext: Various computational components need to share and manipulate large \namounts of data. This data does not belong solely to any one of those components.\nProblem: How can systems store and manipulate persistent data that is accessed \nby multiple independent components? \nSolution: In the shared-data pattern, interaction is dominated by the exchange of \npersistent data between multiple data accessors and at least one shared-data store. \nExchange may be initiated by the accessors or the data store. The connector type is \ndata reading and writing. The general computational model associated with shared-\ndata systems is that data accessors perform operations that require data from the data \nstore and write results to one or more data stores. That data can be viewed and acted \non by other data accessors. In a pure shared-data system, data accessors interact only \nthrough one or more shared-data stores. However, in practice shared-data systems \nalso allow direct interactions between data accessors. The data-store components of \na shared-data system provide shared access to data, support data persistence, man-\nage concurrent access to data through transaction management, provide fault toler-\nance, support access control, and handle the distribution and caching of data values.\nSpecializations of the shared-data pattern differ with respect to the nature \nof the stored data—existing approaches include relational, object structures, lay-\nered, and hierarchical structures. \nAlthough the sharing of data is a critical task for most large, complex sys-\ntems, there are a number of potential problems associated with this pattern. For \none, the shared-data store may be a performance bottleneck. For this reason, \nperformance optimization has been a common theme in database research. The \nshared-data store is also potentially a single point of failure. Also, the producers \nand consumers of the shared data may be tightly coupled, through their knowl-\nedge of the structure of the shared data.\nThe shared-data pattern solution is summarized in Table 13.9.\nThe shared-data pattern is useful whenever various data items are persistent and \nhave multiple accessors. Use of this pattern has the effect of decoupling the producer \nof the data from the consumers of the data; hence, this pattern supports modifiabil-\nity, as the producers do not have direct knowledge of the consumers. Consolidating \nthe data in one or more locations and accessing it in a common fashion facilitates \nperformance tuning. Analyses associated with this pattern usually center on qualities \nsuch as data consistency, performance, security, privacy, availability, scalability, and \ncompatibility with, for example, existing repositories and their data. \nWhen a system has more than one data store, a key architecture concern is the \nmapping of data and computation to the data. Use of multiple stores may occur be-\ncause the data is naturally, or historically, partitioned into separable stores. In other \ncases data may be replicated over several stores to improve performance or availabil-\nity through redundancy. Such choices can strongly affect the qualities noted above. \nFigure 13.13 shows the diagram of a shared-data view of an enterprise access \nmanagement system. There are three types of accessor components: Windows \n",
      "content_length": 3412,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "13.2  Overview of the Patterns Catalog\n231\napplications, web applications, and headless programs (programs or scripts that \nrun in background and don’t provide any user interface).\nTable 13.9  Shared-Data Pattern Solution\nOverview\nCommunication between data accessors is mediated by a shared-\ndata store. Control may be initiated by the data accessors or the \ndata store. Data is made persistent by the data store.\nElements\nShared-data store. Concerns include types of data stored, data \nperformance-oriented properties, data distribution, and number of \naccessors permitted. \nData accessor component. \nData reading and writing connector. An important choice here is \nwhether the connector is transactional or not, as well as the read/\nwrite language, protocols, and semantics.\nRelations\nAttachment relation determines which data accessors are \nconnected to which data stores.\nConstraints\nData accessors interact with the data store(s). \nWeaknesses\nThe shared-data store may be a performance bottleneck.\nThe shared-data store may be a single point of failure.\nProducers and consumers of data may be tightly coupled.\nKey:\nPassword\nsynchronizer\nWindows\nAD\nMicrosoft\nExchange\nServer\nAuthentication\nApplication\nWeb\nsign-in\nWeb\napplication\nPassword\nreset\nSelf\nregistration\nHR database\nAccount\nprovisioning\ncentralized security realm\nRights\nenablement\nEntitlement\nmanagement\nDelegated\nadministration\nRequest\ntracking\nAudit and\nmonitoring\nWindows GUI\napplication\nHeadless\nprogram\nWeb\napplication\nData\nrepository\nData\nread\nData\nwrite\nData \nread & write\nFigure 13.13  The shared-data diagram of an enterprise access management \nsystem\n",
      "content_length": 1626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "232 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nAllocation Patterns\nMap-Reduce Pattern\nContext: Businesses have a pressing need to quickly analyze enormous volumes \nof data they generate or access, at petabyte scale. Examples include logs of inter-\nactions in a social network site, massive document or data repositories, and pairs \nof <source, target> web links for a search engine. Programs for the analysis of \nthis data should be easy to write, run efficiently, and be resilient with respect to \nhardware failure.\nProblem: For many applications with ultra-large data sets, sorting the data and \nthen analyzing the grouped data is sufficient. The problem the map-reduce pat-\ntern solves is to efficiently perform a distributed and parallel sort of a large data \nset and provide a simple means for the programmer to specify the analysis to be \ndone. \nSolution: The map-reduce pattern requires three parts: First, a specialized infra-\nstructure takes care of allocating software to the hardware nodes in a massively \nparallel computing environment and handles sorting the data as needed. A node \nmay be a standalone processor or a core in a multi-core chip. Second and third are \ntwo programmer-coded functions called, predictably enough, map and reduce. \nThe map function takes as input a key (key1) and a data set. The purpose of \nthe map function is to filter and sort the data set. All of the heavy analysis takes \nplace in the reduce function. The input key in the map function is used to filter \nthe data. Whether a data record is to be involved in further processing is deter-\nmined by the map function. A second key (key2) is also important in the map \nfunction. This is the key that is used for sorting. The output of the map function \nconsists of a <key2, value> pair, where the key2 is the sorting value and the value \nis derived from the input record.\nSorting is performed by a combination of the map and the infrastructure. \nEach record output by map is hashed by key2 into a disk partition. The infra-\nstructure maintains an index file for key2 on the disk partition. This allows for the \nvalues on the disk partition to be retrieved in key2 order.\nThe performance of the map phase of map-reduce is enhanced by having \nmultiple map instances, each processing a different portion of the disk file being \nprocessed. Figure 13.14 shows how the map portion of map-reduce processes \ndata. An input file is divided into portions, and a number of map instances are \ncreated to process each portion. The map function processes its portion into a \nnumber of partitions, based on programmer-specified logic.\nThe reduce function is provided with all the sets of <key2, value> pairs emit-\nted by all the map instances in sorted order. Reduce does some programmer-spec-\nified analysis and then emits the results of that analysis. The output set is almost \nalways much smaller than the input sets, hence the name “reduce.” The term \n“load” is sometimes used to describe the final set of data emitted. Figure 13.14 \nalso shows one instance (of many possible instances) of the reduce processing, \n",
      "content_length": 3121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "13.2  Overview of the Patterns Catalog\n233\ncalled Reduce Instance 2. Reduce Instance 2 is receiving data from all of the \nPartition 2s produced by the various map instances. It is possible that there are \nseveral iterations of reduce for large files, but this is not shown in Figure 13.14.\nA classic teaching problem for map-reduce is counting word occurrences \nin a document. This example can be carried out with a single map function. The \ndocument is the data set. The map function will find every word in the document \nand output a <word, 1> pair for each. For example, if the document begins with \nthe words “Having a whole book . . . ,” then the first results of map will be\n<Having, 1>\n<a, 1>\n<whole, 1>\n<book, 1>\nIn practice, the “a” would be one of the words filtered by map. \nPseudocode for map might look like this:\nmap(String key, String value):\n// key: document name\n// value: document contents\nfor each word w in value:\nEmit (w, “1”);\nPortion i of\ninput file\nPortion j of\ninput file\nReduce\ninstance 2\nOutput \nfrom\ninstance 2\nMap instance j\nPartition 1 Partition 2 Partition 3\nPartition 1 Partition 2 Partition 3\nComponent\nDisk file\nOutput\nKey:\nMerge\nMap instance i\nFigure 13.14  A component-and-connector view of map-reduce showing how the \ndata processed by map is partitioned and subsequently processed by reduce\n",
      "content_length": 1329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "234 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nThe reduce function will take that list in sorted order, add up the 1s for each \nword to get a count, and output the result. \nThe corresponding reduce function would look like this:\nreduce(List <key, value>):\n// key: a word\n// value: an integer \nint result = 0;\nsort input\nfor each input value:\nfor each input pair with same word\nresult ++ ;\nEmit (word, result)\nresult = 0\nLarger data sets lead to a much more interesting solution. Suppose we want \nto continuously analyze Twitter posts over the last hour to see what topics are \ncurrently “trending.” This is analogous to counting word occurrences in millions \nof documents. In that case, each document (tweet) can be assigned to its own in-\nstance of the map function. (If you don’t have millions of processors handy, you \ncan break the tweet collection into groups that match the number of processors \nin your processor farm, and process the collection in waves, one group after the \nother.) Or we can use a dictionary to give us a list of words, and each map func-\ntion can be assigned its own word to look for across all tweets. \nThere can also be multiple instances of reduce. These are usually arranged \nso that the reduction happens in stages, with each stage processing a smaller list \n(with a smaller number of reduce instances) than the previous stage. The final \nstage is handled by a single reduce function that produces the final output. \nOf course, the map-reduce pattern is not appropriate in all instances. Some \nconsiderations that would argue against adopting this pattern are these: \n■\n■If you do not have large data sets, then the overhead of map-reduce is not \njustified.\n■\n■If you cannot divide your data set into similar sized subsets, the advantages \nof parallelism are lost.\n■\n■If you have operations that require multiple reduces, this will be complex to \norchestrate.\nCommercial implementations of map-reduce provide infrastructure that \ntakes care of assignment of function instances to hardware, recovery and reas-\nsignment in case of hardware failure (a common occurrence in massively parallel \ncomputing environments), and utilities like sorting of the massive lists that are \nproduced along the way. \nTable 13.10 summarizes the solution of the map-reduce pattern.\nMap-reduce is a cornerstone of the software of some of the most familiar \nnames on the web, including Google, Facebook, eBay, and Yahoo!\n",
      "content_length": 2457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "13.2  Overview of the Patterns Catalog\n235\nTable 13.10  Map-Reduce Pattern Solution\nOverview\nThe map-reduce pattern provides a framework for analyzing a \nlarge distributed set of data that will execute in parallel, on a set \nof processors. This parallelization allows for low latency and high \navailability. The map performs the extract and transform portions \nof the analysis and the reduce performs the loading of the results. \n(Extract-transform-load is sometimes used to describe the functions of \nthe map and reduce.)\nElements\nMap is a function with multiple instances deployed across multiple \nprocessors that performs the extract and transformation portions of \nthe analysis.\nReduce is a function that may be deployed as a single instance or as \nmultiple instances across processors to perform the load portion of \nextract-transform-load.\nThe infrastructure is the framework responsible for deploying map and \nreduce instances, shepherding the data between them, and detecting \nand recovering from failure.\nRelations\nDeploy on is the relation between an instance of a map or reduce \nfunction and the processor onto which it is installed.\nInstantiate, monitor, and control is the relation between the \ninfrastructure and the instances of map and reduce. \nConstraints\nThe data to be analyzed must exist as a set of files.\nThe map functions are stateless and do not communicate with each \nother.\nThe only communication between the map instances and the reduce \ninstances is the data emitted from the map instances as <key, value> \npairs.\nWeaknesses\nIf you do not have large data sets, the overhead of map-reduce is not \njustified.\nIf you cannot divide your data set into similar sized subsets, the \nadvantages of parallelism are lost.\nOperations that require multiple reduces are complex to orchestrate.\nMulti-tier Pattern\nThe multi-tier pattern is a C&C pattern or an allocation pattern, depending on the \ncriteria used to define the tiers. Tiers can be created to group components of similar \nfunctionality, in which case it is a C&C pattern. However, in many, if not most, \ncases tiers are defined with an eye toward the computing environment on which \nthe software will run: A client tier in an enterprise system will not be running on \nthe computer that hosts the database. That makes it an allocation pattern, mapping \nsoftware elements—perhaps produced by applying C&C patterns—to computing \nelements. Because of that reason, we have chosen to list it as an allocation pattern.\nContext: In a distributed deployment, there is often a need to distribute a sys-\ntem’s infrastructure into distinct subsets. This may be for operational or business \nreasons (for example, different parts of the infrastructure may belong to different \norganizations).\n",
      "content_length": 2756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "236 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nProblem: How can we split the system into a number of computationally inde-\npendent execution structures—groups of software and hardware—connected by \nsome communications media? This is done to provide specific server environ-\nments optimized for operational requirements and resource usage. \nSolution: The execution structures of many systems are organized as a set of \nlogical groupings of components. Each grouping is termed a tier. The grouping \nof components into tiers may be based on a variety of criteria, such as the type of \ncomponent, sharing the same execution environment, or having the same runtime \npurpose.\nThe use of tiers may be applied to any collection (or pattern) of runtime \ncomponents, although in practice it is most often used in the context of cli-\nent-server patterns. Tiers induce topological constraints that restrict which com-\nponents may communicate with other components. Specifically, connectors may \nexist only between components in the same tier or residing in adjacent tiers. The \nmulti-tier pattern found in many Java EE and Microsoft .NET applications is an \nexample of organization in tiers derived from the client-server pattern. \nAdditionally, tiers may constrain the kinds of communication that can take \nplace across adjacent tiers. For example, some tiered patterns require call-return \ncommunication in one direction but event-based notification in the other.\nThe main weakness with the multi-tier architecture is its cost and complex-\nity. For simple systems, the benefits of the multi-tier architecture may not justify \nits up-front and ongoing costs, in terms of hardware, software, and design and \nimplementation complexity.\nTiers are not components, but rather logical groupings of components. Also, \ndon’t confuse tiers with layers! Layering is a pattern of modules (a unit of imple-\nmentation), while tiers applies only to runtime entities. \nTable 13.11 summarizes the solution part of the multi-tier pattern.\nTiers make it easier to ensure security, and to optimize performance and \navailability in specialized ways. They also enhance the modifiability of the sys-\ntem, as the computationally independent subgroups need to agree on protocols \nfor interaction, thus reducing their coupling.\nFigure 13.15 uses an informal notation to describe the multi-tier architecture \nof the Consumer Website Java EE application. This application is part of the Ad-\nventure Builder system. Many component-and-connector types are specific to the \nsupporting platform, which is Java EE in this case.\n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "13.2  Overview of the Patterns Catalog\n237\nTable 13.11  Multi-tier Pattern Solution\nOverview\nThe execution structures of many systems are organized as a \nset of logical groupings of components. Each grouping is termed \na tier. The grouping of components into tiers may be based on a \nvariety of criteria, such as the type of component, sharing the same \nexecution environment, or having the same runtime purpose.\nElements\nTier, which is a logical grouping of software components. \nTiers may be formed on the basis of common computing platforms, \nin which case those platforms are also elements of the pattern.\nRelations\nIs part of, to group components into tiers.\nCommunicates with, to show how tiers and the components they \ncontain interact with each other. \nAllocated to, in the case that tiers map to computing platforms. \nConstraints\nA software component belongs to exactly one tier.\nWeaknesses\nSubstantial up-front cost and complexity.\nKey\nWeb\nbrowser\nSignOnFilter\n*.do\n*.screen\nMain\nServlet\nTemplate\nServlet\nScreen\nJSP\nindex.jsp\nSign On\nNotifier\nmappings.xml\nscreen\ndefinitions.xml\nsign-on-\nconfig.xml\nOrder\nFacade\nEJB tier\nBack end\nWeb tier\nClient tier\nCatalog\nFacade\nOPC\nAdventure\nCatalog\nDB\nUser\nMgmt\nFacade\nOpcOrder\nTrackingService\nOpcPurchase\nOrderService\nClient-side\napplication\nJava\nEE\nfilter\nStateless\nsession\nbean\nJava EE\napplication\nContext\nlistener\nData\nstore\nFile\nServlet\nContainer\nWeb services\nendpoint\nSOAP\ncall\nFile\nI/O\nJava\ncall\nHTTP/\nHTTPS\nJDBC\nFigure 13.15  A multi-tier view of the Consumer Website Java EE application, \nwhich is part of the Adventure Builder system\n",
      "content_length": 1593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "238 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nOther Allocation Patterns.  There are several published deployment styles. \nMicrosoft publishes a “Tiered Distribution” pattern, which prescribes a particular \nallocation of components in a multi-tier architecture to the hardware they will run \non. Similarly, IBM’s WebSphere handbooks describe a number of what they call \n“topologies” along with the quality attribute criteria for choosing among them. \nThere are 11 topologies (specialized deployment patterns) described for Web-\nSphere version 6, including the “single machine topology (stand-alone server),” \n“reverse proxy topology,” “vertical scaling topology,” “horizontal scaling topol-\nogy,” and “horizontal scaling with IP sprayer topology.”\nThere are also published work assignment patterns. These take the form of \noften-used team structures. For example, patterns for globally distributed Agile \nprojects include these:\n■\n■Platform. In software product line development, one site is tasked with \ndeveloping reusable core assets of the product line, and other sites develop \napplications that use the core assets.\n■\n■Competence center. Work is allocated to sites depending on the technical \nor domain expertise located at a site. For example, user interface design is \ndone at a site where usability engineering experts are located.\n■\n■Open source. Many independent contributors develop the software product \nin accordance with a technical integration strategy. Centralized control is \nminimal, except when an independent contributor integrates his code into \nthe product line.\n13.3 Relationships between Tactics and Patterns \nPatterns and tactics together constitute the software architect’s primary tools of \nthe trade. How do they relate to each other?\nPatterns Comprise Tactics\nAs we said in the introduction to this chapter, tactics are the “building blocks” \nof design from which architectural patterns are created. Tactics are atoms and \npatterns are molecules. Most patterns consist of (are constructed from) several \ndifferent tactics, and although these tactics might all serve a common purpose—\nsuch as promoting modifiability, for example—they are often chosen to promote \ndifferent quality attributes. For example, a tactic might be chosen that makes an \navailability pattern more secure, or that mitigates the performance impact of a \nmodifiability pattern.\nConsider the example of the layered pattern, the most common pattern in all \nof software architecture (virtually all nontrivial systems employ layering). The \n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "13.3 Relationships between Tactics and Patterns \n239\nlayered pattern can be seen as the amalgam of several tactics—increase semantic \ncoherence, abstract common services, encapsulate, restrict communication paths, \nand use an intermediary. For example:\n■\n■Increase semantic coherence. The goal of ensuring that a layer’s respon-\nsibilities all work together without excessive reliance on other layers \nis achieved by choosing responsibilities that have semantic coherence. \nDoing so binds responsibilities that are likely to be affected by a change. \nFor example, responsibilities that deal with hardware should be allocated \nto a hardware layer and not to an application layer; a hardware respon-\nsibility typically does not have semantic coherence with the application \nresponsibilities.\n■\n■Restrict dependencies. Layers define an ordering and only allow a layer to \nuse the services of its adjacent lower layer. The possible communication \npaths are reduced to the number of layers minus one. This limitation has a \ngreat influence on the dependencies between the layers and makes it much \neasier to limit the side effects of replacing a layer. \nWithout any one of its tactics, the pattern might be ineffective. For example, \nif the restrict dependencies tactic is not employed, then any function in any layer \ncan call any other function in any other layer, destroying the low coupling that \nmakes the layering pattern effective. If the increase semantic coherence tactic \nis not employed, then functionality could be randomly sprinkled throughout the \nlayers, destroying the separation of concerns, and hence ease of modification, \nwhich is the prime motivation for employing layers in the first place.\nTable 13.12 shows a number of the architectural patterns described in the \nbook Pattern-Oriented Software Architecture Volume 1: A System of Patterns, by \nBuschmann et al., and shows which modifiability tactics they employ.\nUsing Tactics to Augment Patterns\nA pattern is described as a solution to a class of problems in a general context. \nWhen a pattern is chosen and applied, the context of its application becomes very \nspecific. A documented pattern is therefore underspecified with respect to apply-\ning it in a specific situation.\nTo make a pattern work in a given architectural context, we need to examine \nit from two perspectives:\n■\n■The inherent quality attribute tradeoffs that the pattern makes. Patterns exist \nto achieve certain quality attributes, and we need to compare the ones they \npromote (and the ones they diminish) with our needs.\n■\n■Other quality attributes that the pattern isn’t directly concerned with, but \nwhich it nevertheless affects, and which are important in our application.\n",
      "content_length": 2719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "240 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nTable 13.12  Architecture Patterns and Corresponding Tactics ([Bachmann 07])\nPattern\nModifiability\nIncrease \nCohesion\nReduce Coupling\nDefer Binding \nTime\nIncrease Semantic \nCoherence\nAbstract Common \nServices\nEncapsulate\nUse a Wrapper\nRestrict Comm. \nPaths\nUse an \nIntermediary\nRaise the \nAbstraction Level\nUse Runtime \nRegistration\nUse Startup-Time \nBinding\nUse Runtime \nBinding\nLayered\nX\nX \nX \nX\nX \nX \nPipes and Filters\nX\nX\nX\nX\nX\nBlackboard\nX\nX\nX\nX\nX\nX\nX\nBroker\nX\nX\nX\nX\nX\nX\nX\nModel View \nController\nX\nX\nX\nX\nPresentation \nAbstraction Control\nX\nX\nX\nX\nMicrokernel\nX\nX\nX\nX\nX\nReflection\nX\nX\nTo illustrate these concerns in particular, and how to use tactics to augment \npatterns in general, we’ll use the broker pattern as a starting point. \nThe broker pattern is widely used in distributed systems and dates back at \nleast to its critical role in CORBA-based systems. Broker is a crucial component \nof any large-scale, dynamic, service-oriented architecture. \nUsing this pattern, a client requesting some information from a server does \nnot need to know the location or APIs of the server. The client simply contacts \nthe broker (typically through a client-side proxy); this is illustrated in the UML \nsequence diagram in Figure 13.16.\nWeaknesses of the Broker Pattern.  In Section 13.2 we enumerated sev-\neral weaknesses of the broker pattern. Here we will examine these weaknesses \nin more detail. The broker pattern has several weaknesses with respect to certain \nquality attributes. For example: \n■\n■Availability. The broker, if implemented as suggested in Figure 13.6, is a \nsingle point of failure. The liveness of servers, the broker, and perhaps even \nthe clients need to be monitored, and repair mechanisms must be provided.\n",
      "content_length": 1805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "13.3 Relationships between Tactics and Patterns \n241\n:Client\n:ClientProxy\n:ServerProxy\n:Server\nprocess boundary\nregisterServer()\nmarshallRequest()\nunmarshallRequest()\nclientID\nserverID\nOK\nresultA\nresultA\nsendRequest()\nmarshallResponse()\nunmarshallResponse()\nsendResponse()\nperformFunctionA()\nperformFunctionA()\nlocateServer()\nlocateClient()\nprocess boundary\n:Broker\nFigure 13.16  A sequence diagram showing a typical client-server interaction \nmediated by a broker\n■\n■Performance. The levels of indirection between the client (requesting \nthe information or service) and the server (providing the information or \nservice) add overhead, and hence add latency. Also, the broker is a potential \nperformance bottleneck if direct communication between the client and \nserver is not desired (for example, for security reasons).\n■\n■Testability. Brokers are employed in complex multi-process and multi-\nprocessor systems. Such systems are typically highly dynamic. Requests \nand responses are typically asynchronous. All of this makes testing and \ndebugging such systems extremely difficult. But the description of the \nbroker pattern provides no testing functionality, such as testing interfaces, \nstate or activity capture and playback capabilities, and so forth.\n",
      "content_length": 1258,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "242 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\n■\n■Security. Because the broker pattern is primarily used when the system \nspans process and processor boundaries—such as on web-based systems—\nsecurity is a legitimate concern. However, the broker pattern as presented \ndoes not offer any means to authenticate or authorize clients or servers, and \nprovides no means of protecting the communication between clients and \nservers.\nOf these quality attributes, the broker pattern is mainly associated with \npoor performance (the well-documented price for the loose coupling it brings to \nsystems). It is largely unconcerned with the other quality attributes in this list; \nthey aren’t mentioned in most published descriptions. But as the other bullets \nshow, they can be unacceptable “collateral damage” that come with the broker’s \nbenefits.\nImproving the Broker Pattern with Tactics.  How can we use tactics to \nplug the gaps between the “out of the box” broker pattern and a version of it that \nwill let us meet the requirements of a demanding distributed system? Here are \nsome options:\n■\n■The increase available resources performance tactic would lead to multiple \nbrokers, to help with performance and availability. \n■\n■The maintain multiple copies tactic would allow each of these brokers to \nshare state, to ensure that they respond identically to client requests.\n■\n■Load balancing (an application of the scheduling resources tactic) would \nensure that one broker is not overloaded while another one sits idle.\n■\n■Heartbeat, exception detection, or ping/echo would give the replicated \nbrokers a way of notifying clients and notifying each other when one of \nthem is out of service, as a means of detecting faults. \nOf course, each of these tactics brings a tradeoff. Each complicates the de-\nsign, which will now take longer to implement, be more costly to acquire, and \nbe more costly to maintain. Load balancing introduces indirection that will add \nlatency to each transaction, thus giving back some of the performance it was in-\ntended to increase. And the load balancer is a single point of failure, so it too \nmust be replicated, further increasing the design cost and complexity.\n13.4  Using Tactics Together \nTactics, as described in Chapters 5–11, are design primitives aimed at managing \na single quality attribute response. Of course, this is almost never true in prac-\ntice; every tactic has its main effect—to manage modifiability or performance \nor safety, and so on—and it has its side effects, its tradeoffs. On the face of it, \nthe situation for an architect sounds hopeless. Whatever you do to improve one \n",
      "content_length": 2655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "13.4  Using Tactics Together \n243\nquality attribute endangers another. We are able to use tactics profitably because \nwe can gauge the direct and side effects of a tactic, and when the tradeoff is ac-\nceptable, we employ the tactic. In doing so we gain some benefit in our quality \nattribute of interest while giving up something else (with respect to a different \nquality attribute and, we hope, of a much smaller magnitude).\nThis section will walk through an example that shows how applying tactics \nto a pattern can produce negative effects in one area, but how adding other tactics \ncan bring relief and put you back in an acceptable design space. The point is to \nshow the interplay between tactics that you can use to your advantage. Just as \nsome combinations of liquids are noxious whereas others yield lovely things like \nstrawberry lemonade, tactics can either make things worse or put you in a happy \ndesign space. Here, then, is a walkthrough of tactic mixology.\nConsider a system that needs to detect faults in its components. A common \ntactic for detecting faults is ping/echo. Let us assume that the architect has de-\ncided to employ ping/echo as a way to detect failed components in the system. \nEvery tactic has one or more side effects, and ping/echo is no different. Common \nconsiderations associated with ping/echo are these:\n■\n■Security. How to prevent a ping flood attack?\n■\n■Performance. How to ensure that the performance overhead of ping/echo is \nsmall?\n■\n■Modifiability. How to add ping/echo to the existing architecture?\nWe can represent the architect’s reasoning and decisions thus far as shown \nin Figure 13.17.\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nFigure 13.17  Partial availability decisions\n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "244 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nSuppose the architect determines that the performance tradeoff (the overhead \nof adding ping/echo to the system) is the most severe. A tactic to address the \nperformance side effect is increase available resources. Considerations associated \nwith increase available resources are these:\n■\n■Cost. Increased resources cost more.\n■\n■Performance. How to utilize the increased resources efficiently?\nThis set of design decisions can now be represented as shown in Figure 13.18.\nNow the architect chooses to deal with the resource utilization consequence \nof employing increase available resources. These resources must be used efficiently \nor else they are simply adding cost and complexity to the system. A tactic that can \naddress the efficient use of resources is the employment of a scheduling policy. Con-\nsiderations associated with the scheduling policy tactic are these:\n■\n■Modifiability. How to add the scheduling policy to the existing architecture?\n■\n■Modifiability. How to change the scheduling policy in the future?\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease Available\nResources\nFigure 13.18  More availability decisions\n",
      "content_length": 1257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "13.4  Using Tactics Together \n245\nThe set of design decisions that includes the scheduling policy tactic can \nnow be represented as in Figure 13.19.\nNext the architect chooses to deal with the modifiability consequence of \nemploying a scheduling policy tactic. A tactic to address the addition of the \nscheduler to the system is to use an intermediary, which will insulate the choice \nof scheduling policy from the rest of the system. One consideration associated \nwith use an intermediary is this:\n■\n■Modifiability. How to ensure that all communication passes through the \nintermediary?\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease Available\nResources\nAdd to \nsystem\nModify\npolicy\nScheduling\nPolicy\nFigure 13.19  Still more availability decisions\n",
      "content_length": 801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "246 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\nWe can now represent the tactics-based set of architectural design decisions \nmade thus far as in Figure 13.20.\nA tactic to address the concern that all communication passes through the \nintermediary is restrict dependencies. One consideration associated with the \nrestrict dependencies tactic is this:\n■\n■Performance. How to ensure that the performance overhead of the \nintermediary is not excessive?\nThis design problem has now become recursive! At this point (or in fact, \nat any point in the tree of design decisions that we have described) the architect \nmight determine that the performance overhead of the intermediary is small \nenough that no further design decisions need to be made.\nSystem\nPing/Echo\nAdd to\nsystem\nPing\nflood\nPerformance\noverhead\nCost\nResource\nutilization\nIncrease available\nResources\nAdd to \nsystem\nModify\npolicy\nScheduling\nPolicy\nEnsure usage\nUse an intermediary\nFigure 13.20  As far as we go with availability decisions\n",
      "content_length": 1022,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "13.5  Summary\n247\nApplying successive tactics is like moving through a game space, and it’s a \nlittle like chess: Good players are able to see the consequences of the move they’re \nconsidering, and the very good players are able to look several moves ahead. In \nChapter 17 we’ll see the activity of design treated as an exercise of “generate and \ntest”: propose a design and test it to see if it’s satisfactory. Applying tactics to \nan existing design solution, such as a pattern, is one technique for generating a \ndesign for subsequent testing.\n13.5  Summary\nAn architectural pattern\n■\n■is a package of design decisions that is found repeatedly in practice,\n■\n■has known properties that permit reuse, and \n■\n■describes a class of architectures. \nBecause patterns are (by definition) found repeatedly in practice, one does \nnot invent them; one discovers them. \nTactics are simpler than patterns. Tactics typically use just a single structure \nor computational mechanism, and they are meant to address a single architectural \nforce. For this reason they give more precise control to an architect when \nmaking design decisions than patterns, which typically combine multiple design \ndecisions into a package. Tactics are the “building blocks” of design from which \narchitectural patterns are created. Tactics are atoms and patterns are molecules. \nAn architectural pattern establishes a relationship between:\n■\n■A context. A recurring, common situation in the world that gives rise to a \nproblem.\n■\n■A problem. The problem, appropriately generalized, that arises in the given \ncontext. \n■\n■A solution. A successful architectural resolution to the problem, \nappropriately abstracted. \nComplex systems exhibit multiple patterns at once. \nPatterns can be categorized by the dominant type of elements that they show: \nmodule patterns show modules, component-and-connector patterns show compo-\nnents and connectors, and allocation patterns show a combination of software \nelements (modules, components, connectors) and nonsoftware elements. Most \npublished patterns are C&C patterns, but there are module patterns and allocation \npatterns as well. This chapter showed examples of each type.\nA pattern is described as a solution to a class of problems in a general con-\ntext. When a pattern is chosen and applied, the context of its application becomes \nvery specific. A documented pattern is therefore underspecified with respect to \n",
      "content_length": 2429,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "248 \nPart Two  Quality Attributes\t\n13—Architectural Tactics and Patterns\napplying it in a specific situation. We can make a pattern more specific to our \nproblem by augmenting it with tactics. Applying successive tactics is like mov-\ning through a game space, and is a little like chess: the consequences of the next \nmove are important, and looking several moves ahead is helpful.\n13.6  For Further Reading\nThere are many existing repositories of patterns and books written about patterns. \nThe original and most well-known work on object-oriented design patterns is by \nthe “Gang of Four” [Gamma 94].\nThe Gang of Four’s discussion of patterns included patterns at many levels \nof abstraction. In this chapter we have focused entirely on architectural patterns. \nThe patterns that we have presented here are intended as representative examples. \nThis chapter’s inventory of patterns is in no way meant to be exhaustive. For \nexample, while we describe the SOA pattern, entire repositories of SOA patterns \n(refinements of the basic SOA pattern) have been created. A good place to start is \nwww.soapatterns.org.\nSome good references for pattern-oriented architecture are [Buschmann 96], \n[Hanmer 07], [Schmidt 00], and [Kircher 03].\nA good place to learn more about the map-reduce pattern is Google’s foun-\ndational paper on it [Dean 04].\nMap-reduce is the tip of the spear of the so-called “NoSQL” movement, \nwhich seeks to displace the relational database from its venerable and taken-for-\ngranted status in large data-processing systems. The movement has some of the \nrevolutionary flavor of the Agile movement, except that NoSQL advocates are \nclaiming a better (for them) technology, as opposed to a better process. You can \neasily find NoSQL podcasts, user forums, conferences, and blogs; it’s also dis-\ncussed in Chapter 26.\n[Bachmann 07] discusses the use of tactics in the layered pattern and is the \nsource for some of our discussion of that.\nThe passage in this chapter about augmenting ping/echo with other tactics \nto achieve the desired combination of quality attributes is based on the work of \nKiran Kumar and TV Prabhakar [Kumar 10a] and [Kumar 10b]. \n[Urdangarin 08] is the source of the work assignment patterns described in \nSection 13.2.\nThe Adventure Builder system shown in Figures 13.11 and 13.15 comes \nfrom [AdvBuilder 10].\n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "13.7  Discussion Questions\n249\n13.7  Discussion Questions\n1.\t\nWhat’s the difference between an architectural pattern, such as those de-\nscribed in this chapter and in the Pattern-Oriented Software Architecture \nseries of books, and design patterns, such as those collected by the Gang of \nFour in 1994 and many other people subsequently? Given a pattern, how \nwould you decide whether it was an architectural pattern, a design pattern, \na code pattern, or something else?\n2.\t\nSOA systems feature dynamic service registration and discovery. Which \nquality attributes does this capability enhance and which does it threaten? \nIf you had to make a recommendation to your boss about whether your \ncompany’s SOA system should use external services it discovers at runtime, \nwhat would you say?\n3.\t\nWrite a complete pattern description for the “competence center” work as-\nsignment pattern mentioned in Section 13.2.\n4.\t\nFor a data set that is a set of web pages, sketch a map function and a reduce \nfunction that together provide a basic search engine capability. \n5.\t\nDescribe how the layered pattern makes use of these tactics: abstract com-\nmon services, encapsulate, and use an intermediary.\n",
      "content_length": 1191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "251\n14\nQuality Attribute \nModeling and Analysis\nDo not believe in anything simply because you have \nheard it . . . Do not believe in anything merely on \nthe authority of your teachers and elders. Do not \nbelieve in traditions because they have been handed \ndown for many generations. But after observation \nand analysis, when you find that anything agrees \nwith reason and is conducive to the good and benefit \nof one and all, then accept it and live up to it.\n—Prince Gautama Siddhartha\nIn Chapter  2 we listed thirteen reasons why architecture is important, worth \nstudying, and worth practicing. Reason 6 is that the analysis of an architecture \nenables early prediction of a system’s qualities. This is an extraordinarily pow-\nerful reason! Without it, we would be reduced to building systems by choosing \nvarious structures, implementing the system, measuring the system for its quality \nattribute responses, and all along the way hoping for the best. Architecture lets \nus do better than that, much better. We can analyze an architecture to see how \nthe system or systems we build from it will perform with respect to their quality \nattribute goals, even before a single line of code has been written. This chapter \nwill explore how.\nThe methods available depend, to a large extent, on the quality attribute to \nbe analyzed. Some quality attributes, especially performance and availability, \nhave well-understood and strongly validated analytic modeling techniques. Other \nquality attributes, for example security, can be analyzed through checklists. Still \nothers can be analyzed through back-of-the-envelope calculations and thought \nexperiments. \nOur topics in this chapter range from the specific, such as creating models \nand analyzing checklists, to the general, such as how to generate and carry out the \nthought experiments to perform early (and necessarily crude) analysis. Models \n",
      "content_length": 1897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "252 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nand checklists are focused on particular quality attributes but can aid in the anal-\nysis of any system with respect to those attributes. Thought experiments, on the \nother hand, can consider multiple quality attributes simultaneously but are only \napplicable to the specific system under consideration.\n14.1  \u0007Modeling Architectures to Enable \nQuality Attribute Analysis\nSome quality attributes, most notably performance and availability, have well-un-\nderstood, time-tested analytic models that can be used to assist in an analysis. \nBy analytic model, we mean one that supports quantitative analysis. Let us first \nconsider performance.\nAnalyzing Performance\nIn Chapter 12 we discussed the fact that models have parameters, which are val-\nues you can set to predict values about the entity being modeled (and in Chap-\nter 12 we showed how to use the parameters to help us derive tactics for the \nquality attribute associated with the model). As an example we showed a queuing \nmodel for performance as Figure 12.2, repeated here as Figure 14.1. The parame-\nters of this model are the following: \n■\n■The arrival rate of events\n■\n■The chosen queuing discipline\n■\n■The chosen scheduling algorithm\n■\n■The service time for events\n■\n■The network topology \n■\n■The network bandwidth\n■\n■The routing algorithm chosen\nIn this section, we discuss how such a model can be used to understand the \nlatency characteristics of an architectural design. \nTo apply this model in an analytical fashion, we also need to have previ-\nously made some architecture design decisions. We will use model-view-control-\nler as our example here. MVC, as presented in Section 13.2, says nothing about \nits deployment. That is, there is no specification of how the model, the view, and \nthe controller are assigned to processes and processors; that’s not part of the pat-\ntern’s concern. These and other design decisions have to be made to transform \na pattern into an architecture. Until that happens, one cannot say anything with \nauthority about how an MVC-based implementation will perform. For this exam-\nple we will assume that there is one instance each of the model, the view, and the \ncontroller, and that each instance is allocated to a separate processor. Figure 14.2 \nshows MVC following this allocation scheme.\n",
      "content_length": 2371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "14.1  Modeling Architectures to Enable Quality Attribute Analysis\n253\nResults\nRouting of \nmessages\nArrivals\nQueue\nServer\nScheduling \nalgorithm\nFigure 14.1  A queuing model of performance\nInternet\nintranet\n<<deploy>>\n<<deploy>>\n<<deploy>>\nDatabase \nhost\n<<component>>\nModel\nUser’s \nmachine\n<<component>>\nView\nApp\nserver\n<<component>>\nController\nKey: UML 2.0\nFigure 14.2  An allocation view, in UML, of a model-view-controller architecture\nGiven that quality attribute models such as the performance model shown \nin Figure 14.1 already exist, the problem becomes how to map these allocation \nand coordination decisions onto Figure 14.1. Doing this yields Figure 14.3. \nThere are requests coming from users outside the system—labeled as 1 in Fig-\nure 14.3—arriving at the view. The view processes the requests and sends some \n",
      "content_length": 823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "254 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\ntransformation of the requests on to the controller—labeled as 2. Some actions of \nthe controller are returned to the view—labeled as 3. The controller sends other \nactions on to the model—labeled 4. The model performs its activities and sends \ninformation back to the view—labeled 5.\nTo analyze the model in Figure 14.3, a number of items need to be known \nor estimated: \n■\n■The frequency of arrivals from outside the system\n■\n■The queuing discipline used at the view queue\n■\n■The time to process a message within the view\n■\n■The number and size of messages that the view sends to the controller\n■\n■The bandwidth of the network that connects the view and the controller\n■\n■The queuing discipline used by the controller\n■\n■The time to process a message within the controller\n■\n■The number and size of messages that the controller sends back to the view\n■\n■The bandwidth of the network used for messages from the controller to the \nview\n■\n■The number and size of messages that the controller sends to the model\n■\n■The queuing discipline used by the model\n■\n■The time to process a message within the model\n■\n■The number and size of messages the model sends to the view\n■\n■The bandwidth of the network connecting the model and the view\nUsers\ngenerate \nrequests\n1\n2\n3\n4\n5\nController\nModel\nView\nFigure 14.3  A queuing model of performance for MVC\n",
      "content_length": 1420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "14.1  Modeling Architectures to Enable Quality Attribute Analysis\n255\nGiven all of these assumptions, the latency for the system can be estimated. \nSometimes well-known formulas from queuing theory apply. For situations where \nthere are no closed-form solutions, estimates can often be obtained through sim-\nulation. Simulations can be used to make more-realistic assumptions such as the \ndistribution of the event arrivals. The estimates are only as good as the assump-\ntions, but they can serve to provide rough values that can be used either in design \nor in evaluation; as better information is obtained, the estimates will improve.\nA reasonably large number of parameters must be known or estimated to \nconstruct the queuing model shown in Figure 14.3. The model must then be \nsolved or simulated to derive the expected latency. This is the cost side of the \ncost/benefit of performing a queuing analysis. The benefit side is that as a result \nof the analysis, there is an estimate for latency, and “what if” questions can be \neasily answered. The question for you to decide is whether having an estimate of \nthe latency and the ability to answer “what if” questions is worth the cost of per-\nforming the analysis. One way to answer this question is to consider the impor-\ntance of having an estimate for the latency prior to constructing either the system \nor a prototype that simulates an architecture under an assumed load. If having a \nsmall latency is a crucial requirement upon which the success of the system re-\nlies, then producing an estimate is appropriate. \nPerformance is a well-studied quality attribute with roots that extend beyond \nthe computer industry. For example, the queuing model given in Figure 14.1 \ndates from the 1930s. Queuing theory has been applied to factory floors, to bank-\ning queues, and to many other domains. Models for real-time performance, such \nas rate monotonic analysis, also exist and have sophisticated analysis techniques. \nAnalyzing Availability\nAnother quality attribute with a well-understood analytic framework is availability. \nModeling an architecture for availability—or to put it more carefully, mod-\neling an architecture to determine the availability of a system based on that archi-\ntecture—is a matter of determining the failure rate and the recovery time. As you \nmay recall from Chapter 5, availability can be expressed as\nMTBF\n(MTBF + MTTR)\nThis models what is known as steady-state availability, and it is used to \nindicate the uptime of a system (or component of a system) over a sufficiently \nlong duration. In the equation, MTBF is the mean time between failure, which is \nderived based on the expected value of the implementation’s failure probability \ndensity function (PDF), and MTTR refers to the mean time to repair. \nJust as for performance, to model an architecture for availability, we need \nan architecture to analyze. So, suppose we want to increase the availability of a \nsystem that uses the broker pattern, by applying redundancy tactics. Figure 14.4 \n",
      "content_length": 3035,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "256 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nillustrates three well-known redundancy tactics from Chapter  5: active redun-\ndancy, passive redundancy, and cold spare. Our goal is to analyze each redun-\ndancy option for its availability, to help us choose one.\nAs you recall, each of these tactics introduces a backup copy of a compo-\nnent that will take over in case the primary component suffers a failure. In our \ncase, a broker replica is employed as the redundant spare. The difference among \nthem is how up to date with current events each backup keeps itself:\n■\n■In the case of active redundancy, the active and redundant brokers both \nreceive identical copies of the messages received from the client and server \nproxies. The internal broker state is synchronously maintained between the \nactive and redundant spare in order to facilitate rapid failover upon detec-\ntion of a fault in the active broker. \n■\n■For the passive redundancy implementation, only the active broker receives \nand processes messages from the client and server proxies. When using this \ntactic, checkpoints of internal broker state are periodically transmitted from \nthe active broker process to the redundant spare, using the checkpoint-based \nrollback tactic. \n■\n■Finally, when using the cold spare tactic, only the active broker receives \nand processes messages from the client and server proxies, because the \nredundant spare is in a dormant or even powered-off state. Recovery strate-\ngies using this tactic involve powering up, booting, and loading the broker \nimplementation on the spare. In this scenario, the internal broker state is \nrebuilt organically, rather than via synchronous operation or checkpointing, \nas described for the other two redundancy tactics.\nSuppose further that we will detect failure with the heartbeat tactic, where \neach broker (active and spare) periodically transmits a heartbeat message to a \nseparate process responsible for fault detection, correlation, reporting, and recov-\nery. This fault manager process is responsible for coordinating the transition of \nthe active broker role from the failed broker process to the redundant spare. \nYou can now use the steady state model of availability to assign values for \nMTBF and MTTR for each of the three redundancy tactics we are considering. \nDoing so will be an exercise left to the reader (as you’ll see when you reach the \ndiscussion questions for this chapter). Because the three tactics differ primarily in \nhow long it takes to bring the backup copy up to speed, MTTR will be where the \ndifference among the tactics shows up.\nMore sophisticated models of availability exist, based on probability. In \nthese models, we can express a probability of failure during a period of time. \nGiven a particular MTBF and a time duration T, the probability of failure R is \ngiven by\nR(T ) = e(    )\n–T\nMTBF\n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "14.1  Modeling Architectures to Enable Quality Attribute Analysis\n257\nBrokerACTIVE\nBrokerSPARE\n(Cold) Spare\nClient-Server\nProxy Traffic\nBrokerACTIVE\nBrokerSPARE\nPassive \nRedundancy\nClient-Server\nProxy Traffic\nPeriodic\nCheckpoint Data\nKey:\nmessage\nprocess\nBrokerACTIVE\nBrokerSPARE\nActive\nRedundancy\nClient-Server\nProxy Traffic\nState\nSynchronization\nFigure 14.4  Redundancy tactics, as applied to a broker pattern\nYou will recall from Statistics 101 that:\n■\n■When two events A and B are independent, the probability that A or B will \noccur is the sum of the probability of each event: P(A or B) = P(A) \n+ P(B).\n■\n■When two events A and B are independent, the probability of both occur-\nring is P(A and B) = P(A) • P(B).\n■\n■When two events A and B are dependent, the probability of both occurring \nis P(A and B) = P(A) • P(B|A), where the last term means “the \nprobability of B occurring, given that A occurs.”\n",
      "content_length": 908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "258 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nWe can apply simple probability arithmetic to an architecture pattern for \navailability to determine the probability of failure of the pattern given the proba-\nbility of failure of the individual components (and an understanding of their de-\npendency relations). For example, in an architecture pattern employing the pas-\nsive redundancy tactic, let’s assume that the failure of a component (which at any \nmoment might be acting as either the primary or backup copy) is independent of \na failure of its counterpart, and that the probability of failure of either is the same. \nThen the probability that both will fail is F = (1 – a) **2, where a is the \navailability of an individual component (assuming that failures are independent). \nStill other models take into account different levels of failure severity and \ndegraded operating states of the system. Although the derivation of these for-\nmulas is outside the scope of this chapter, you end up with formulas that look \nlike the following for the three redundancy tactics we’ve been discussing, where \nthe values C2 through C5 are references to the MTBF column of Table 14.1, D2 \nthrough D4 refer to the Active column, E2 through E3 refer to the Passive col-\numn, and F2 through F3 refer to the Spare column.\n■\n■Active redundancy:\n■\n■Availability(MTTR): 1 –((SUM(C2:C5) + D3) × D2)/((C2 × (C2 + C4 + \nD3) + ((C2 + C4 + D2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))\n■\n■P(Degraded) = ((C3 + C5) × D2)/((C2 × (C2 + C4 + D3) + ((C2 + C4 + \nD2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))\n■\n■Passive redundancy:\n■\n■Availability(MTTR_passive) = 1 – ((SUM(C2:C5) + E3) × E2)/((C2 × \n(C2 + C4 + E3) + ((C2 + C4 + E2) × (C3 + C5)) + ((C2 + C4) × (C2 + \nC4 + E3))))\n■\n■P(Degraded) = ((C3 + C5) × E2)/((C2 × (C2 + C4 + E3) + ((C2 + C4 + \nE2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + E3))))\n■\n■Spare:\n■\n■Availability(MTTR) = 1 – ((SUM(C2:C5) + F3) × F2)/((C2 × (C2 + C4 + \nF3) + ((C2 + C4 + F2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3))))\n■\n■P(Degraded) = ((C3 + C5) × F2)/((C2 × (C2 + C4 + F3) + ((C2 + C4 + \nF2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3)))) \nPlugging in these values for the parameters to the equations listed above \nresults in a table like Table 14.1, which can be easily calculated using any spread-\nsheet tool. Such a calculation can help in the selection of tactics.\n",
      "content_length": 2416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "14.1  Modeling Architectures to Enable Quality Attribute Analysis\n259\n TABLE 14.1  Calculated Availability for an Availability-Enhanced Broker \nImplementation\nFunction\nFailure \nSeverity\nMTBF \n(Hours)\nMTTR (Seconds)\nActive \nRedundancy\n(Hot Spare)\nPassive \nRedundancy\n(Warm Spare)\nSpare\n(Cold Spare)\nHardware\n1\n250,000\n1\n5\n900\n2\n50,000\n30\n30\n30\nSoftware\n1\n50,000\n1\n5\n900\n2\n10,000\n30\n30\n30\nAvailability\n0.9999998\n0.999990\n0.9994\nThe Analytic Model Space\nAs we discussed in the preceding sections, there are a growing number of analytic \nmodels for some aspects of various quality attributes. One of the quests of software \nengineering is to have a sufficient number of analytic models for a sufficiently large \nnumber of quality attributes to enable prediction of the behavior of a designed sys-\ntem based on these analytic models. Table 14.2 shows our current status with respect \nto this quest for the seven quality attributes discussed in Chapters 5–11.\nTABLE 14.2  A Summary of the Analytic Model Space\nQuality \nAttribute\nIntellectual Basis\nMaturity/Gaps\nAvailability\nMarkov models; \nstatistical models\nModerate maturity; mature in the \nhardware reliability domain, less mature \nin the software domain. Requires models \nthat speak to state recovery and for which \nfailure percentages can be attributed to \nsoftware.\nInteroperability\nConceptual framework\nLow maturity; models require substantial \nhuman interpretation and input.\nModifiability\nCoupling and cohesion \nmetrics; cost models\nSubstantial research in academia; still \nrequires more empirical support in real-\nworld environments.\nPerformance\nQueuing theory; real-\ntime scheduling theory\nHigh maturity; requires considerable \neducation and training to use properly.\nSecurity\nNo architectural models\n \nTestability\nComponent interaction \nmetrics\nLow maturity; little empirical validation.\nUsability\nNo architectural models\n \n",
      "content_length": 1881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "260 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nAs the table shows, the field still has a great deal of work to do to achieve \nthe quest for well-validated analytic models to predict behavior, but there is a \ngreat deal of activity in this area (see the “For Further Reading” section for ad-\nditional papers). The remainder of this chapter deals with techniques that can be \nused in addition to analytic models.\n14.2  Quality Attribute Checklists\nFor some quality attributes, checklists exist to enable the architect to test com-\npliance or to guide the architect when making design decisions. Quality attribute \nchecklists can come from industry consortia, from government organizations, \nor from private organizations. In large organizations they may be developed in \nhouse. \n These checklists can be specific to one or more quality attributes; checklists \nfor safety, security, and usability are common. Or they may be focused on a par-\nticular domain; there are security checklists for the financial industry, industrial \nprocess control, and the electric energy sector. They may even focus on some \nspecific aspect of a single quality attribute: cancel for usability, as an example. \nFor the purposes of certification or regulation, the checklists can be used by \nauditors as well as by the architect. For example, two of the items on the checklist \nof the Payment Card Industry (PCI) are to only persist credit card numbers in an \nencrypted form and to never persist the security code from the back of the credit \ncard. An auditor can ask to examine stored credit card data to determine whether \nit has been encrypted. The auditor can also examine the schema for data being \nstored to see whether the security code has been included.\nThis example reveals that design and analysis are often two sides of the \nsame coin. By considering the kinds of analysis to which a system will be sub-\njected (in this case, an audit), the architect will be led into making important \nearly architectural decisions (making the decisions the auditors will want to find).\nSecurity checklists usually have heavy process components. For example, a \nsecurity checklist might say that there should be an explicit security policy within \nan organization, and a cognizant security officer to ensure compliance with the \npolicy. They also have technical components that the architect needs to examine \nto determine the implications on the architecture of the system being designed or \nevaluated. For example, the following is an item from a security checklist gener-\nated by a group chartered by an organization of electric producers and distribu-\ntors. It pertains to embedded systems delivering electricity to your home:\nA designated system or systems shall daily or on request obtain current \nversion numbers, installation date, configuration settings, patch level \non all elements of a [portion of the electric distribution] system, \n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "14.1  Modeling Architectures to Enable Quality Attribute Analysis\n261\nIn Search of a Grand Unified Theory for Quality Attributes\nHow do we create analytic models for those quality attribute aspects for \nwhich none currently exist? I do not know the answer to this question, but \nif we had a basis set for quality attributes, we would be in a better position \nto create and validate quality attribute models. By basis set I mean a set \nof orthogonal concepts that allow one to define the existing set of quality \nattributes. Currently there is much overlap among quality attributes; a \nbasis set would enable discussion of tradeoffs in terms of a common set \nof fundamental and possibly quantifiable concepts. Once we have a basis \nset, we could develop analytic models for each of the elements of the set, \nand then an analytic model for a particular quality attribute becomes a \ncomposition of the models of the portions of the basis set that make up \nthat quality attribute. \nWhat are some of the elements of this basis set? Here are some of my \ncandidates:\n■\n■\nTime. Time is the basis for performance, some aspects of availability, \nand some aspects of usability. Time will surely be one of the fundamen-\ntal concepts for defining quality attributes. \n■\n■\nDependencies among structural elements. Modifiability, security, avail-\nability, and performance depend in some form or another on the strength \nof connections among various structural elements. Coupling is a form \nof dependency. Attacks depend on being able to move from one com-\npromised element to a currently uncompromised element through some \ndependency. Fault propagation depends on dependencies. And one of \nthe key elements of performance analysis is the dependency of one \ncomputation on another. Enumeration of the fundamental forms of de-\npendency and their properties will enable better understanding of many \nquality attributes and their interaction. \n■\n■\nAccess. How does a system promote or deny access through various \nmechanisms? Usability is concerned with allowing smooth access for \nhumans; security is concerned with allowing smooth access for some set \nof requests but denying access to another set of requests. Interoperabili-\nty is concerned with establishing connections and accessing information. \nRace conditions, which undermine availability, come about through un-\nmediated access to critical computations.\nThese are some of my candidates. I am sure there are others. The \ngeneral problem is to define a set of candidates for the basis set and then \nshow how current definitions of various quality attributes can be recast in \nterms of the elements of the basis set. I am convinced that this is a problem \nthat needs to be solved prior to making substantial progress in the quest for \na rich enough set of analytic models to enable prediction of system behav-\nior across the quality attributes important for a system.\n—LB\n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "262 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\ncompare these with inventory and configuration databases, and log all \ndiscrepancies. \nThis kind of rule is intended to detect malware masquerading as legitimate \ncomponents of a system. The architect will look at this item and conclude the \nfollowing:\n■\n■The embedded portions of the system should be able to report their version \nnumber, installation date, configuration settings, and patch levels. One tech-\nnique for doing this is to use “reflection” for each component in the system. \nReflection now becomes one of the important patterns used in this system.\n■\n■Each software update or patch should maintain this information. One tech-\nnique for doing this is to have automated update and patch mechanisms. \nThe architecture could also realize this functionality through reflection.\n■\n■A system must be designated to query the embedded components and per-\nsist the information. This means\n■\n■There must be overall inventory and configuration databases.\n■\n■Logs of discrepancies between current values and overall inventory must \nbe generated and sent to appropriate recipients.\n■\n■There must be network connections to the embedded components. This \naffects the network topology.\nThe creation of quality attribute checklists is usually a time-consuming ac-\ntivity, undertaken by multiple individuals and typically refined and evolved over \ntime. Domain specialists, quality attribute specialists, and architects should all \ncontribute to the development and validation of these checklists.\nThe architect should treat the items on an applicable checklist as require-\nments, in that they need to be understood and prioritized. Under particular cir-\ncumstances, an item in a checklist may not be met, but the architect should have a \ncompelling case as to why it is not.\n14.3  \u0007Thought Experiments and  \nBack-of-the-Envelope Analysis\nA thought experiment is a fancy name for the kinds of discussions that develop-\ners and architects have on a daily basis in their offices, in their meetings, over \nlunch, over whiteboards, in hallways, and around the coffee machine. One of the \nparticipants might draw two circles and an arrow on the whiteboard and make an \nassertion about the quality attribute behavior of these two circles and the arrow in \na particular context; a discussion ensues. The discussion can last for a long time, \nespecially if the two circles are augmented with a third and one more arrow, or if \nsome of the assumptions underlying a circle or an arrow are still in flux. In this \nsection, we describe this process somewhat more formally. \n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "14.3  Thought Experiments and Back-of-the-Envelope Analysis \n263\nThe level of formality one would use in performing a thought experiment \nis, as with most techniques discussed in this book, a question of context. If two \npeople with a shared understanding of the system are performing the thought ex-\nperiment for their own private purposes, then circles and lines on a whiteboard \nare adequate, and the discussion proceeds in a kind of shorthand. If a third person \nis to review the results and the third person does not share the common under-\nstanding, then sufficient details must be captured to enable the third person to un-\nderstand the argument—perhaps a quick legend and a set of properties need to be \nadded to the diagram. If the results are to be included in documentation as design \nrationale, then even more detail must be captured, as discussed in Chapter 18. \nFrequently such thought experiments are accompanied by rough analyses—back-\nof-the-envelope analyses—based on the best data available, based on past expe-\nriences, or even based on the guesses of the architects, without too much concern \nfor precision.\nThe purpose of thought experiments and back-of-the-envelope analysis is \nto find problems or confirmation of the nonexistence of problems in the quality \nattribute requirements as applied to sunny-day use cases. That is, for each use \ncase, consider the quality attribute requirements that pertain to that use case and \nanalyze the use case with the quality attribute requirements in mind. Models and \nchecklists focus on one quality attribute. To consider other quality attributes, one \nmust model or have a checklist for the second quality attribute and understand \nhow those models interact. A thought experiment may consider several of the \nquality attribute requirements simultaneously; typically it will focus on just the \nmost important ones.\nThe process of creating a thought experiment usually begins with listing the \nsteps associated with carrying out the use case under consideration; perhaps a se-\nquence diagram is employed. At each step of the sequence diagram, the (mental) \nquestion is asked: What can go wrong with this step with respect to any of the \nquality attribute requirements? For example, if the step involves user input, then \nthe possibility of erroneous input must be considered. Also the user may not have \nbeen properly authenticated and, even if authenticated, may not be authorized to \nprovide that particular input. If the step involves interaction with another system, \nthen the possibility that the input format will change after some time must be \nconsidered. The network passing the input to a processor may fail; the processor \nperforming the step may fail; or the computation to provide the step may fail, \ntake too long, or be dependent on another computation that may have had prob-\nlems. In addition, the architect must ask about the frequency of the input, and the \nanticipated distribution of requests (e.g., Are service requests regular and predict-\nable or irregular and “bursty”?), other processes that might be competing for the \nsame resources, and so forth. These questions go on and on.\nFor each possible problem with respect to a quality attribute requirement, \nthe follow-on questions consist of things like these: \n■\n■Are there mechanisms to detect that problem? \n",
      "content_length": 3349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "264 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\n■\n■Are there mechanisms to prevent or avoid that problem? \n■\n■Are there mechanisms to repair or recover from that problem if it occurs? \n■\n■Is this a problem we are willing to live with? \nThe problems hypothesized are scrutinized in terms of a cost/benefit analy-\nsis. That is, what is the cost of preventing this problem compared to the benefits \nthat accrue if the problem does not occur?\nAs you might have gathered, if the architects are being thorough and if the \nproblems are significant (that is, they present a large risk for the system), then \nthese discussions can continue for a long time. The discussions are a normal por-\ntion of design and analysis and will naturally occur, even if only in the mind of a \nsingle designer. On the other hand, the time spent performing a particular thought \nexperiment should be bounded. This sounds obvious, but every grey-haired archi-\ntect can tell you war stories about being stuck in endless meetings, trapped in the \npurgatory of “analysis paralysis.” \nAnalysis paralysis can be avoided with several techniques:\n■\n■“Time boxing”: setting a deadline on the length of a discussion.\n■\n■Estimating the cost if the problem occurs and not spending more than that \ncost in the analysis. In other words, do not spend an inordinate amount of \ntime in discussing minor or unlikely potential problems. \nPrioritizing the requirements will help both with the cost estimation and \nwith the time estimation.\n14.4  Experiments, Simulations, and Prototypes\nIn many environments it is virtually impossible to do a purely top-down architec-\ntural design; there are too many considerations to weigh at once and it is too hard \nto predict all of the relevant technological barriers. Requirements may change in \ndramatic ways, or a key assumption may not be met: We have seen cases where a \nvendor-provided API did not work as specified, or where an API exposing a criti-\ncal function was simply missing. \nFinding the sweet spot within the enormous architectural design space of \ncomplex systems is not feasible by reflection and mathematical analysis alone; \nthe models either aren’t precise enough to deal with all of the relevant details or \nare so complicated that they are impractical to analyze with tractable mathemat-\nical techniques. \nThe purpose of experiments, simulations, and prototypes is to provide al-\nternative ways of analyzing the architecture. These techniques are invaluable in \n",
      "content_length": 2507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "14.5  Analysis at Different Stages of the Life Cycle\n265\nresolving tradeoffs, by helping to turn unknown architectural parameters into \nconstants or ranges. For example, consider just a few of the questions that might \noccur when creating a web-conferencing system—a distributed client-server in-\nfrastructure with real-time constraints:\n■\n■Would moving to a distributed database from local flat files negatively im-\npact feedback time (latency) for users?\n■\n■How many participants could be hosted by a single conferencing server?\n■\n■What is the correct ratio between database servers and conferencing \nservers?\nThese sorts of questions are difficult to answer analytically. The answers to \nthese questions rely on the behavior and interaction of third-party components \nsuch as commercial databases, and on performance characteristics of software for \nwhich no standard analytical models exist. The approach used for the web-con-\nferencing architecture was to build an extensive testing infrastructure that sup-\nported simulations, experiments, and prototypes, and use it to compare the per-\nformance of each incremental modification to the code base. This allowed the \narchitect to determine the effect of each form of improvement before committing \nto including it in the final system. The infrastructure includes the following:\n■\n■A client simulator that makes it appear as though tens of thousands of cli-\nents are simultaneously interacting with a conferencing server.\n■\n■Instrumentation to measure load on the conferencing server and database \nserver with differing numbers of clients.\nThe lesson from this experience is that experimentation can often be a criti-\ncal precursor to making significant architectural decisions. Experimentation must \nbe built into the development process: building experimental infrastructure can \nbe time-consuming, possibly requiring the development of custom tools. Carry-\ning out the experiments and analyzing their results can require significant time. \nThese costs must be recognized in project schedules. \n14.5  Analysis at Different Stages of the Life Cycle\nDepending on your project’s state of development, different forms of analysis are \npossible. Each form of analysis comes with its own costs. And there are different \nlevels of confidence associated with each analysis technique. These are summa-\nrized in Table 14.3.\n",
      "content_length": 2369,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "266 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nTable 14.3  Forms of Analysis, Their Life-Cycle Stage, Cost, and Confidence in \nTheir Outputs\nLife-Cycle Stage\nForm of Analysis\nCost\nConfidence\nRequirements\nExperience-based analogy\nLow\nLow–High\nRequirements\nBack-of-the-envelope\nLow\nLow–Medium\nArchitecture\nThought experiment\nLow\nLow–Medium\nArchitecture\nChecklist\nLow\nMedium\nArchitecture\nAnalytic model\nLow–Medium\nMedium\nArchitecture\nSimulation\nMedium\nMedium\nArchitecture\nPrototype\nMedium\nMedium–High\nImplementation \nExperiment\nMedium–High\nMedium–High\nFielded System\nInstrumentation\nMedium–High\nHigh\nThe table shows that analysis performed later in the life cycle yields results that \nmerit high confidence. However, this confidence comes at a price. First, the cost of \nperforming the analysis also tends to be higher. But the cost of changing the system \nto fix a problem uncovered by analysis skyrockets later in the life cycle.\nChoosing an appropriate form of analysis requires a consideration of all of \nthe factors listed in Table 14.3: What life-cycle stage are you currently in? How \nimportant is the achievement of the quality attribute in question and how worried \nare you about being able to achieve the goals for this attribute? And finally, how \nmuch budget and schedule can you afford to allocate to this form of risk miti-\ngation? Each of these considerations will lead you to choose one or more of the \nanalysis techniques described in this chapter.\n14.6  Summary\nAnalysis of an architecture enables early prediction of a system’s qualities. We can \nanalyze an architecture to see how the system or systems we build from it will per-\nform with respect to their quality attribute goals. Some quality attributes, most nota-\nbly performance and availability, have well-understood, time-tested analytic models \nthat can be used to assist in quantitative analysis. Other quality attributes have less \nsophisticated models that can nevertheless help with predictive analysis. \nFor some quality attributes, checklists exist to enable the architect to test \ncompliance or to guide the architect when making design decisions. Quality at-\ntribute checklists can come from industry consortia, from government organiza-\ntions, or from private organizations. In large organizations they may be devel-\noped in house. The architect should treat the items on an applicable checklist as \nrequirements, in that they need to be understood and prioritized. \n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "14.7  For Further Reading\n267\nThought experiments and back-of-the-envelope analysis can often quickly \nhelp find problems or confirm the nonexistence of problems with respect to qual-\nity attribute requirements. A thought experiment may consider several of the \nquality attribute requirements simultaneously; typically it will focus on just the \nmost important ones. Experiments, simulations, and prototypes allow the explo-\nration of tradeoffs, by helping to turn unknown architectural parameters into con-\nstants or ranges whose values may be measured rather than estimated. \nDepending on your project’s state of development, different forms of analy-\nsis are possible. Each form of analysis comes with its own costs and its own level \nof confidence associated with each analysis technique.\n14.7  For Further Reading\nThere have been many papers and books published describing how to build and \nanalyze architectural models for quality attributes. Here are just a few examples.\nAvailability\nMany availability models have been proposed that operate at the architecture \nlevel of analysis. Just a few of these are [Gokhale 05] and [Yacoub 02].\nA discussion and comparison of different black-box and white-box models \nfor determining software reliability can be found in [Chandran 10].\nA book relating availability to disaster recovery and business recovery is \n[Schmidt 10].\nInteroperability\nAn overview of interoperability activities can be found in [Brownsword 04].\nModifiability\nModifiability is typically measured through complexity metrics. The classic work \non this topic is [Chidamber 94].\nMore recently, analyses based on design structure matrices have begun to \nappear [MacCormack 06].\nPerformance\nTwo of the classic works on software performance evaluation are [Smith 01] and \n[Klein 93].\n",
      "content_length": 1798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "268 \nPart Two  Quality Attributes\t\n14—Quality Attribute Modeling and Analysis\nA broad survey of architecture-centric performance evaluation approaches \ncan be found in [Koziolek 10].\nSecurity\nChecklists for security have been generated by a variety of groups for different \ndomains. See for example:\n■\n■Credit cards, generated by the Payment Card Industry: www.pcisecurity-\nstandards.org/security_standards/\n■\n■Information security, generated by the National Institute of Standards and \nTechnology (NIST): [NIST 09].\n■\n■Electric grid, generated by Advanced Security Acceleration Project for the \nSmart Grid: www.smartgridipedia.org/index.php/ASAP-SG\n■\n■Common Criteria. An international standard (ISO/IEC 15408) for computer \nsecurity certification: www.commoncriteriaportal.org\nTestability\nWork in measuring testability from an architectural perspective includes measur-\ning testability as the measured complexity of a class dependency graph derived \nfrom UML class diagrams, and identifying class diagrams that can lead to code \nthat is difficult to test [Baudry 05]; and measuring controllability and observabil-\nity as a function of data flow [Le Traon 97]. \nUsability\nA checklist for usability can be found at www.stcsig.org/usability/topics/articles/\nhe-checklist.html\nSafety\nA checklist for safety is called the Safety Integrity Level: en.wikipedia.org/wiki/\nSafety_Integrity_Level\nApplications of Modeling and Analysis\nFor a detailed discussion of a case where quality attribute modeling and analysis \nplayed a large role in determining the architecture as it evolved through a number \nof releases, see [Graham 07].\n",
      "content_length": 1624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "14.8  Discussion Questions\n269\n14.8  Discussion Questions\n1.\t\nBuild a spreadsheet for the steady-state availability equation MTBF / \n(MTBF + MTTR). Plug in different but reasonable values for MTBF and \nMTTR for each of the active redundancy, passive redundancy, and cold \nspare tactics. Try values for MTBF that are very large compared to MTTR, \nand also try values for MTBF that are much closer in size to MTTR. \nWhat do these tell you about which tactics you might want to choose for \navailability?\n2.\t\nEnumerate as many responsibilities as you can that need to be carried out \nfor providing a “cancel” operation in a user interface. Hint: There are at \nleast 21 of them, as indicated in a publication by (strong hint!) one of the \nauthors of this book whose last name (unbelievably strong hint!) begins \nwith “B.”\n3.\t\nThe M/M/1 (look it up!) queuing model has been employed in computing \nsystems for decades. Where in your favorite computing system would this \nmodel be appropriate to use to predict latency?\n4.\t\nSuppose an architect produced Figure 14.5 while you were sitting watching \nhim. Using thought experiments, how can you determine the performance \nand availability of this system? What assumptions are you making and what \nconclusions can you draw? How definite are your conclusions?\nFigure 14.5  Capture of a whiteboard sketch from an architect\n",
      "content_length": 1360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "271\nPart  T H R EE\nArchitecture in \nthe Life Cycle\nPart I of this book introduced architecture and the various contextual lenses \nthrough which it could be viewed. To recap from Chapter 3, those contexts in-\nclude the following:\n■\n■Technical. What technical role does the software architecture play in the sys-\ntem or systems of which it’s a part? Part of the answer to this is what Part II \nof our book is about and the rest is included in Part IV. Part II describes how \ndecisions are made, and Part IV describes the environment that determines \nwhether the results of the decisions satisfy the needs of the organization.\n■\n■Project life cycle. How does a software architecture relate to the other \nphases of a software development life cycle? The answer to this is what \nPart III of our book is about.\n■\n■Business. How does the presence of a software architecture affect an orga-\nnization’s business environment? The answer to this is what Part IV of our \nbook is about.\n■\n■Professional. What is the role of a software architect in an organization or \na development project? The answer to this is threaded throughout the entire \nbook, but especially in Chapter 24, where we treat the duties, skills, and \nknowledge of software architects.\nPart II concentrated on the technical context of software architecture. In our \nphilosophy, this is tantamount to understanding quality attributes. If you have a \ndeep understanding of how architecture affects quality attributes, then you have \nmastered most of what you need to know about making design decisions.\n",
      "content_length": 1557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "272\nHere in Part III we turn our attention to how to constructively apply that \nknowledge within the context of a particular software development project. Here \nis where software architecture meets software engineering: How do architecture \nconcerns affect the gathering of requirements, the carrying out of design deci-\nsions, the validation and capturing of the design, and the transformation of design \ninto implementation? In Part III, we’ll find out.\nA Word about Methods\nBecause this is a book about software architecture in practice, we’ve tried to spell \nout specific methods in enough detail so that you can emulate them. You’ll see \nPALM, a method for eliciting business goals that an architecture should accom-\nmodate. You’ll see Views and Beyond, an approach for documenting architecture \nin a set of views that serve stakeholders and their concerns. You’ll see ATAM, a \nmethod for evaluating an architecture against stakeholders’ ideas of what quality \nattributes it should provide. You’ll see CBAM, a method for assessing which evo-\nlutionary path of an architecture will best serve stakeholders’ needs. \nAll of these methods rely in some way or another on tapping stakehold-\ners’ knowledge about what an architecture under development should provide. As \npresented in their respective chapters, each of these methods includes a similar \nprocess of identifying the relevant stakeholders, putting them in a room together, \npresenting a briefing about the method that the stakeholders have been assembled \nto participate in, and then launching into the method. \nSo why is it necessary to put all of the stakeholders in the same room? The \nshort answer is that it isn’t. There are (at least) three major engagement models for \nconducting an architecture-focused method. Why three? Because we have identified \ntwo important factors, each of which has two values, that describe four potential en-\ngagement models for gathering information from stakeholders. These two factors are\n1.\t\nLocation (co-located or distributed)\n2.\t\nSynchronicity (synchronous or asynchronous) \nOne option (co-located and asynchronous) makes no sense, and so we are \nleft with three viable engagement models. The advantages and disadvantages \nwe’ve observed of each engagement model follow. \nWhy has the big-meeting format (co-located, synchronous) tended to pre-\nvail? There are several reasons:\n",
      "content_length": 2381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "273\n■\n■It compresses the time required for the method. Time on site for remote \nparticipants is minimized, although as we will see, travel time is not con-\nsidered in this argument. All of the stakeholders are available with minimal \nexternal distractions.\n■\n■It emphasizes the importance of the method. Any meeting important \nenough to bring multiple people together for an extended time must be \njudged by management to be important. \n■\n■It benefits from the helpful group mentality that emerges when people are \nin the same room working toward a common goal. The group mentality \nfosters buy-in to the architecture and buy-in to the reasons it exists. Putting \nstakeholders in the same room lets them open communication paths with \nthe architect and with each other, paths that will often remain open long \nModel\nAdvantages\nDisadvantages\nAll stakeholders in \nthe same room for \nthe duration of the \nexercise (co-located \nand synchronous). \nAll stakeholders participate \nequally.\nGroup mentality produces \nbuy-in for architecture and \nthe results of the exercise.\nEnduring communication \npaths are opened among \nstakeholders.\nThis option takes the \nshortest calendar time.\nScheduling can be \nproblematic.\nSome stakeholders might not \nbe forthcoming in a crowd.\nStakeholders might incur \nsubstantial travel costs to \nattend.\nSome stakeholders \nparticipate in exercise \nremotely (distributed \nand synchronous). \nSaves travel costs for remote \nparticipants; this option \nmight permit participation by \nstakeholders who otherwise \nwould not be able to \ncontribute.\nTechnology is a limiting \nfactor; remote participants \nalmost always are second-\nclass citizens in terms of \ntheir participation and after-\nexercise “connection” to other \nparticipants.\nFacilitators \ninterviewing \nstakeholders \nindividually or in small \ngroups (distributed \nand asynchronous).\nAllows for in-depth \ninteraction between \nfacilitators and stakeholders.\nEliminates group factors that \nmight inhibit a stakeholder \nfrom speaking in public.\nIf stakeholders are widely \ndistributed, increased travel \ncosts incurred by facilitator(s).\nReduced group buy-in.\nReduced group mentality.\nReduced after-exercise \ncommunication among \nstakeholders.\nExercise stretched out over a \nlonger period of calendar time.\n",
      "content_length": 2277,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "274\nafter the meeting has run its course. We always enjoy seeing business cards \nexchanged with handshakes when stakeholders meet each other for the first \ntime. Putting the architect in a room full of stakeholders for a couple of \ndays is a very healthy thing for any project. \nBut there are, as ever, tradeoffs. The big-meeting format can be costly and \ndifficult to fit into an already crowded project schedule. Often the hardest aspect \nof executing any of our methods is finding two contiguous days when all the \nimportant stakeholders are available. Also, the travel costs associated with a big \nmeeting can be substantial in a distributed organization. And some stakeholders \nmight not be as forthcoming as we would like if they are in a room surrounded \nby strong-willed peers or higher-ups (although our methods use facilitation tech-\nniques to try to correct for this). \nSo which model is best? You already know the answer: It depends. You can \nsee the tradeoffs among the different approaches. Pick the one that does the best \njob for your organization and its particularities. \nConclusion\nAs you read Part III and learn about architecture methods, remember that the \nform of the method we present is the one in which the most practical experience \nresides. But:\n1.\t\nYou can always adjust the engagement model to be something other than \neverybody-in-the-same-room if that will work better for you. \n2.\t\nWhereas the steps of a method are nominally carried out in sequential order \naccording to a set agenda, sometimes there must be dynamic modifications \nto the schedule to accommodate personnel availability or architectural \ninformation. Every situation is unique, and there may be times when you \nneed to return briefly to an earlier step, jump forward to a later step, or \niterate among steps, as the need dictates. \nP.S.: We do provide one example of a shortened version of one of our methods \n—the ATAM. We call this Lightweight Architecture Evaluation, and it is de-\nscribed in Chapter 21.\n",
      "content_length": 2008,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "275\n15\nArchitecture in Agile \nProjects\nIt is not the strongest of the species that survives, \nnor the most intelligent that survives. It is the \none that is the most adaptable to change.\n—Charles Darwin\nSince their first appearance over a decade ago, the various flavors of Agile meth-\nods and processes have received increasing attention and adoption by the world-\nwide software community. New software processes do not just emerge out of thin \nair; they evolve in response to a palpable need. In this case, the software develop-\nment world was responding to a need for projects to be more responsive to their \nstakeholders, to be quicker to develop functionality that users care about, to show \nmore and earlier progress in a project’s life cycle, and to be less burdened by doc-\numenting aspects of a project that would inevitably change. Is any of this inimical \nto the use of architecture? We emphatically say “no.” In fact, the question for a \nsoftware project is not “Should I do Agile or architecture?”, but rather questions \nsuch as “How much architecture should I do up front versus how much should \nI defer until the project’s requirements have solidified somewhat?”, “When and \nhow should I refactor?”, and “How much of the architecture should I formally \ndocument, and when?” We believe that there are good answers to all of these \nquestions, and that Agile and architecture are not just well suited to live together \nbut in fact critical companions for many software projects.\nThe Agile software movement began to receive considerable public atten-\ntion approximately a decade ago, with the release of the “Agile Manifesto.” Its \nroots extend at least a decade earlier than that, in practices such as Extreme Pro-\ngramming and Scrum. The Agile Manifesto, originally signed by 17 developers, \nwas however a brilliant public relations move; it is brief, pithy, and sensible:\n",
      "content_length": 1887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "276 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nManifesto for Agile Software Development\nWe are uncovering better ways of developing software by doing it and \nhelping others do it. Through this work we have come to value:\nIndividuals and interactions\nover\nprocesses and tools\nWorking software \nover\ncomprehensive documentation\nCustomer collaboration \nover\ncontract negotiation\nResponding to change\nover\nfollowing a plan\nThat is, while there is value in the items on the right, we value the items \non the left more. [agilemanifesto.org]\nThe authors of the Manifesto go on to describe the twelve principles that underlie \ntheir reasoning:\n1.\t Our highest priority is to satisfy the customer through early and continuous \ndelivery of valuable software.\n2.\t Welcome changing requirements, even late in development. Agile processes \nharness change for the customer’s competitive advantage.\n3.\t Deliver working software frequently, from a couple of weeks to a couple of \nmonths, with a preference to the shorter timescale.\n4.\t Business people and developers must work together daily throughout the \nproject.\n5.\t Build projects around motivated individuals. Give them the environment \nand support they need, and trust them to get the job done.\n6.\t The most efficient and effective method of conveying information to and \nwithin a development team is face-to-face conversation.\n7.\t Working software is the primary measure of progress.\n8.\t Agile processes promote sustainable development. The sponsors, develop-\ners, and users should be able to maintain a constant pace indefinitely.\n9.\t Continuous attention to technical excellence and good design enhances \nagility.\n10.\t Simplicity—the art of maximizing the amount of work not done—is \nessential.\n11.\t The best architectures, requirements, and designs emerge from self-organiz-\ning teams.\n12.\t At regular intervals, the team reflects on how to become more effective, \nthen tunes and adjusts its behavior accordingly.\nThere has been considerable elaboration of the Agile Manifesto, and Agile \nprocesses, since its first release, but the basic principles have remained solid. The \nAgile movement (and its predecessors) have gained considerable attention and \nhave enjoyed widespread adoption over the past two decades. These processes \n",
      "content_length": 2312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "15.1  How Much Architecture?\n277\nwere initially employed on small- to medium-sized projects with short time \nframes and enjoyed considerable success. They were not often used for larger \nprojects, particularly those with distributed development. This is not surprising, \ngiven the twelve principles. \nIn particular principles 4 and 6 imply the need for co-location or, if co-lo-\ncation is not possible, then at least a high level of communication among the \ndistributed teams. Indeed, one of the core practices of Agile projects is frequent \n(often daily) face-to-face meetings. Principle 11 says that, for best results, teams \nshould be self-organizing. But self-organization is a social process that is much \nmore cumbersome if those teams are not physically co-located. In this case we \nbelieve that the creators of the twelve Agile principles got it wrong. The best \nteams may be self-organizing, but the best architectures still require much more \nthan this—technical skill, deep experience, and deep knowledge.\nPrinciple 1 argues for “early and continuous delivery of valuable software” \nand principle 7 claims that “Working software is the primary measure of prog-\nress.” One might argue that a focus on early and continuous release of software, \nwhere “working” is measured in terms of customer-facing features, leaves little \ntime for addressing the kinds of cross-cutting concerns and infrastructure critical \nto a high-quality large-scale system. \nIt has been claimed by some that there is an inherent tension between being \nagile and doing a conscientious job of architecting. But is there truly a tension? \nAnd if so, how do you go about characterizing it and reasoning about it? In short, \nhow much architecture is the “right” amount of architecture?\nOur brief answer, in this chapter, is that there is no tension. This issue is not \n“Agile versus Architecture” but rather “how best to blend Agile and Architecture.”\nOne more point, before we dive into the details: The Agile Manifesto is it-\nself a compromise: a pronouncement created by a committee. The fact that ar-\nchitecture doesn’t clearly live anywhere within it is most likely because they had \nno consensus opinion on this topic and not because there is any inherent conflict.\n15.1  How Much Architecture?\nWe often think of the early software development methods that emerged in the \n1970s—such as the Waterfall method—as being plan-driven and inflexible. But \nthis inflexibility is not for nothing. Having a strong up-front plan provides for \nconsiderable predictability (as long as the requirements don’t change too much) \nand makes it easier to coordinate large numbers of teams. Can you imagine a \nlarge construction or aerospace project without heavy up-front planning? Agile \nmethods and practitioners, on the other hand, often scorn planning, preferring in-\nstead teamwork, frequent face-to-face communication, flexibility, and adaptation. \nThis enhances invention and creativity. \n",
      "content_length": 2963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "278 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nGarden Shed or Skyscraper?\nA few years ago I built a small shed in my back yard, for holding gardening \ntools, the lawn mower, the fertilizer cart, and so forth. I had a plan in my \nhead, a small team of physically co-located “developers,” and excellent \naccess to the customer (me) for making any last-minute decisions and for \nincorporating any late-breaking feature requests. What was my architec-\nture? For sure, nothing was written down; I had an image in my head. I \nwent to the local big-box hardware store/lumberyard and bought a bunch \nof building materials, primarily wood. I already owned a fine collection of \nhammers, saws, and drills. The boys and I started hammering and sawing \nand drilling. In short order I had a garden shed which has served its \npurpose, with the occasional repair, for quite a few years. My process was \nagile: I was able to accommodate the knowledge, skills, and characteris-\ntics of my developers; we were a self-organizing team; and I was able to \neasily accommodate feature requests that emerged late in the process.\nWould I recommend this process for the construction of a 20-story office \nbuilding, or even a building-code-compliant single-family house? Of course \nnot. All of these are built using the much-maligned BDUF (Big Design Up \nFront) process. \nMy ad hoc process for building the shed was ultimately agile, but it had \nlittle analysis or forethought. It did, however, have just enough forethought \nand planning. Doing BDUF—hiring an architect, a structural engineer, and \na surveyor, and doing a detailed analysis of soil conditions, potential snow \nloads, and options for future modifications—would have been folly; really \nexpensive folly! \nSo too with software. As with everything that we recommend in this \nbook, the amount of up-front planning and analysis should be justified by \nthe potential risks. In the end, everything in architecture is about cost/bene-\nfit tradeoffs.\n—RK\nLet us consider a specific case, to illustrate the tradeoff between up-front \nplanning and agility: the Agile technique of employing user stories. User stories \nare a cornerstone of the Agile approach. Each user story describes a set of features \nvisible to the user. Implementing user stories is a way of demonstrating progress \nto the customer. This can easily lead to an architecture in which every feature is \nindependently designed and implemented. In such an environment, concerns that \ncut across more than one feature become hard to capture. For example, suppose \nthere is a utility function that supports multiple features. To identify this utility \nfunction, coordination is required among the teams that develop the different fea-\ntures, and it also requires a role in which a broad overview across all of the fea-\ntures is maintained. If the development team is geographically distributed and the \n",
      "content_length": 2931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "15.1  How Much Architecture?\n279\nsystem being developed is a large one, then emphasis on delivering features early \nwill cause massive coordination problems. In an architecture-centric project, a \nlayered architecture is a way to solve this problem, with features on upper layers \nusing shared functionality of the lower layers, but that requires up-front planning \nand design and feature analysis. \nSuccessful projects clearly need a successful blend of the two approaches. For \nthe vast majority of nontrivial projects, this is not and never should be an either/or \nchoice. Too much up-front planning and commitment can stifle creativity and the \nability to adapt to changing requirements. Too much agility can be chaos. No one \nwould want to fly in an aircraft where the flight control software had not been rig-\norously planned and thoroughly analyzed. Similarly, no one would want to spend \n18 months planning an e-commerce website for their latest cell-phone model, or \nvideo game, or lipstick (all of which are guaranteed to be badly out of fashion in \n18 months). What we all want is the sweet spot—what George Fairbanks calls “just \nenough architecture.” This is not just a matter of doing the right amount of architec-\nture work, but also doing it at the right time. Agile projects tend to want to evolve the \narchitecture, as needed, in real time, whereas large software projects have tradition-\nally favored considerable up-front analysis and planning. \nAn Analytic Perspective on Up-front Work vs. Agility\nBoehm and Turner, analyzing historical data from 161 industrial projects, \nexamine the effects of up-front architecture and risk resolution effort. This \ncorresponds to the COnstructive COst MOdel II (COCOMO II) scale factor \ncalled “RESL.” There are two activities that can add time to the basic \nproject schedule:\n■\n■\nUp-front design work on the architecture and up-front risk identification, \nplanning, and resolution work\n■\n■\nRework due to fixing defects and addressing modification requests. \nIntuitively, these two trade off against each other: The more we invest in \nplanning, the less (we hope) rework is needed. \nSo Boehm and Turner synthesized a model that allowed them to plot \nthese two values against each other. The axes of their graph (Figure 15.1) \nshow percent of time added for RESL and percent of time added to the \nschedule. The amount of architecture and risk resolution effort is plotted \nas the dashed line, moving up and to the right from near the origin, and \nranges from 5 to 50 percent of project effort. This effort is plotted against \nthree hypothetical projects, measured in thousands of source lines of code \n(KSLOC):\n■\n■\nOne project of 10 KSLOC \n■\n■\nOne project of 100 KSLOC\n■\n■\nOne project of 10,000 KSLOC\n",
      "content_length": 2758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "280 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n0\n10\n20\n30\n40\n50\n60\nPercent of time added for architecture and risk resolution\nPercent of Project Schedule Devoted to \nInitial Architecture and Risk Resolution\nAdded Schedule Devoted to Rework\n(COCOMO II RESL Factor)\nTotal Percent Added to Schedule\nSweet Spot drivers:\nRapid Change: Leftward\nHigh Assurance: Rightward\nSweet Spot\nPercent of time added to Overall Schedule   \n10 KSLOC\n100 KSLOC\n10,000 KSLOC\nFigure 15.1  Architecture effort vs. rework\n",
      "content_length": 566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "15.2  Agility and Architecture Methods\n281\nThere is one line representing each of these three projects, starting \nnear the Y axis and descending, at different rates, to the X axis at the 50 \nmark. This shows that adding time for up-front work reduces later rework. \nNo surprise: that is exactly the point of doing more up-front work. However, \nwhen you sum each of those downward-trending lines (for the 10, 100, and \n1,000 KSLOC projects) with the upward sloping line for the up-front (initial \narchitecture and risk resolution) work, you get the second set of three lines, \nwhich start at the Y axis and meet the upward sloping line at the 50 mark \non the X axis. \nThese lines show that there is a sweet spot for each project. For the 10 \nKSLOC project, the sweet spot is at the far left. This says that devoting \nmuch, if any, time to up-front work is a waste for a small project (assuming \nthat the inherent domain complexity is the same for all three sets of lines). \nFor the 100 KSLOC project, the sweet spot is at around 20 percent of the \nproject schedule. And for the 1,000 KSLOC project, the sweet spot is at \naround 40 percent of the project schedule. These results are fairly intui-\ntive. A project with a million lines of code is enormously complex, and it is \ndifficult to imagine how Agile principles alone can cope with this complexity \nif there is no architecture to guide and organize the effort. \nThe graph shows that no one answer is appropriate for all situations, \nso you need methods to guide you to decide how much up-front work is \nright for you. Boehm and Turner’s work is a start, but expected lines of \ncode is not the only determinant for appropriateness of up-front planning. \nThe domain, the reliability or safety required, and the experience of your \ndevelopment team all play a role.\nThe whole point of choosing how much time to budget for architecture is to \nreduce risk. Risk may be financial, political, operational, or reputational. Some \nrisks might involve human life or the chance of legal action. Chapter 22 covers \nrisk management and budgets for planning in the context of architecture.\n15.2  Agility and Architecture Methods\nThroughout this book we emphasize methods for architecture design, analysis, \nand documentation. We unabashedly like methods! And so does the Agile com-\nmunity: dozens of books have been written on Scrum, Extreme Programming, \nCrystal Clear, and other Agile methods. But how should we think of architec-\nture-centric techniques and methods in an Agile context? How well do they fit \nwith the twelve Agile principles, for example?\nWe believe that they fit very well. The methods we present are based on the \nessential elements needed to perform the activity. If you believe that architecture \n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "282 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nneeds to be designed, analyzed, and documented, then the techniques we present \nare essential regardless of the project in which they are embedded. The methods \nwe present are essentially driven by the motivation to reduce risk, and by consid-\nerations of costs and benefits. \nAmong all of our methods—for extracting architecturally significant re-\nquirements, for architecture design, for architecture evaluation, for architecture \ndocumentation—that you’ll see in subsequent chapters, one might expect the \ngreatest Agile friction from evaluation and documentation. And so the rest of this \nsection will examine those two practices in an Agile context.\nArchitecture Documentation and YAGNI\nOur approach to architecture documentation is called Views and Beyond, and it \nwill be discussed in Chapter 18. Views and Beyond and Agile agree emphatically \non the following point: If information isn’t needed, don’t spend the resources \nto document it. All documentation should have an intended use and audience in \nmind, and be produced in a way that serves both. \nOne of our fundamental principles of technical documentation is “Write \nfor the reader.” That means understanding who will read the documentation \nand how they will use it. If there is no audience, there is no need to produce the \ndocumentation. This principle is so important in Agile methods that it has been \ngiven its own name: YAGNI. YAGNI means “you ain’t gonna need it,” and it \nrefers to the idea that you should only implement or document something when \nyou actually have the need for it. Do not spend time attempting to anticipate all \npossible needs. \nThe Views and Beyond approach uses the architectural view as the “unit” of \ndocumentation to be produced. Selecting the views to document is an example of \napplying this principle. The Views and Beyond approach prescribes producing a \nview if and only if it addresses substantial concerns of an important stakeholder \ncommunity. And because documentation is not a monolithic activity that holds up \nall other progress until it is complete, the view selection method prescribes pro-\nducing the documentation in prioritized stages to satisfy the needs of the stake-\nholders who need it now.\nWe document the portions of the architecture that we need to teach to \nnewcomers, that embody significant potential risks if not properly managed, \nand that we need to change frequently. We document what we need to convey \nto readers so they can do their job. Although “classic” Agile emphasizes doc-\numenting the minimum amount needed to let the current team of developers \nmake progress, our approach emphasizes that the reader might be a maintainer \nassigned to make a technology upgrade years after the original development \nteam has disbanded.\n",
      "content_length": 2845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "15.3  A Brief Example of Agile Architecting\n283\nArchitecture Evaluation \nCould an architecture evaluation work as part of an Agile process? Absolutely. \nIn fact, doing so is perfectly Agile-consistent, because meeting stakeholders’ im-\nportant concerns is a cornerstone of Agile philosophy. \nOur approach to architecture evaluation is exemplified by the Architecture \nTradeoff Analysis Method (ATAM) of Chapter 21. It does not endeavor to an-\nalyze all, or even most, of an architecture. Rather, the focus is determined by \na set of quality attribute scenarios that represent the most important (but by no \nmeans all) of the concerns of the stakeholders. “Most important” is judged by \nthe amount of value the scenario brings to the architecture’s stakeholders, or the \namount of risk present in achieving the scenario. Once these scenarios have been \nelicited, validated, and prioritized, they give us an evaluation agenda based on \nwhat is important to the success of the system, and what poses the greatest risk \nfor the system’s success. Then we only delve into those areas that pose high risk \nfor the achievement of the system’s main functions and qualities.\nAnd as we will see in Chapter 21, it is easy to tailor a lightweight architec-\nture evaluation, for quicker and less-costly analysis and feedback whenever in the \nproject it is called for. \n15.3  A Brief Example of Agile Architecting\nOur claim is that architecture and agility are quite compatible. Now we will look \nat a brief case study of just that. This project, which one of the authors worked \non, involved the creation and evolution of a web-conferencing system. Through-\nout this project we practiced “agile architecting” and, we believe, hit the sweet \nspot between up-front planning where possible, and agility where needed.\nWeb-conferencing systems are complex and demanding systems. They must \nprovide real-time responsiveness, competitive features, ease of installation and \nuse, lightweight footprint, and much more. For example:\n■\n■They must work on a wide variety of hardware and software platforms, the \ndetails of which are not under the control of the architect.\n■\n■They must be reliable and provide low-latency response times, particularly \nfor real-time functionality such as voice over IP (VoIP) and screen sharing.\n■\n■They must provide high security, but do so over an unknown network topol-\nogy and an unknown set of firewalls and firewall policies.\n■\n■They must be easily modified and easily integrated into a wide variety of \nenvironments and applications.\n■\n■They must be highly usable and easily installed and learned by users with \nwidely varying IT skills. \n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "284 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nMany of the above-mentioned goals trade off against each other. Typically \nsecurity (in the form of encryption) comes at the expense of real-time perfor-\nmance (latency). Modifiability comes at the expense of time-to-market. Avail-\nability and performance typically come at the expense of modifiability and cost.\nEven if it is possible to collect, analyze, and prioritize all relevant data, func-\ntional requirements, and quality attribute requirements, the stringent time-to-mar-\nket constraints that prevail in a competitive climate such as web-conferencing \nwould have prevented us from doing this. Trying to support all possible uses is \nintractable, and the users themselves were poorly equipped for envisioning all \npossible potential uses of the system. So just asking the users what they wanted, \nin the fashion of a traditional requirements elicitation, was not likely to work.\nThis results in a classic “agility versus commitment” problem. On the one \nhand the architect wants to provide new capabilities quickly, and to respond to \ncustomer needs rapidly. On the other hand, long-term survival of the system and \nthe company means that it must be designed for extensibility, modifiability, and \nportability. This can best be achieved by having a simple conceptual model for \nthe architecture, based on a small number of regularly applied patterns and tac-\ntics. It was not obvious how we would “evolve” our way to such an architecture. \nSo, how is it possible to find the “sweet spot” between these opposing forces?\nThe WebArrow web-conferencing system faced precisely this dilemma. It \nwas impossible for the architect and lead designers to do purely top-down ar-\nchitectural design; there were too many considerations to weigh at once, and it \nwas too hard to predict all of the relevant technological challenges. For example, \nthey had cases where they discovered that a vendor-provided API did not work \nas specified—imagine that!—or that an API exposing a critical function was sim-\nply missing. In such cases, these problems rippled through the architecture, and \nworkarounds needed to be fashioned . . . fast! \nTo address the complexity of this domain, the WebArrow architect and de-\nvelopers found that they needed to think and work in two different modes at the \nsame time:\n■\n■Top-down—designing and analyzing architectural structures to meet the \ndemanding quality attribute requirements and tradeoffs \n■\n■Bottom-up—analyzing a wide array of implementation-specific and \nenvironment-specific constraints and fashioning solutions to them \nTo compensate for the difficulty in analyzing architectural tradeoffs with \nany precision, the team adopted an agile architecture discipline combined with a \nrigorous program of experiments aimed at answering specific tradeoff questions. \nThese experiments are what are called “spikes” in Agile terminology. And these \nexperiments proved to be the key in resolving tradeoffs, by helping to turn un-\nknown architectural parameters into constants or ranges. Here’s how it worked:\n",
      "content_length": 3120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "15.3  A Brief Example of Agile Architecting\n285\n1.\t\nFirst, the WebArrow team quickly created and crudely analyzed an initial \nsoftware and system architecture concept, and then they implemented and \nfleshed it out incrementally, starting with the most critical functionality that \ncould be shown to a customer.\n2.\t\nThey adapted the architecture and refactored the design and code whenever \nnew requirements popped up or a better understanding of the problem do-\nmain emerged. \n3.\t\nContinuous experimentation, empirical evaluation, and architecture analysis \nwere used to help determine architectural decisions as the product evolved.\nFor example, incremental improvement in the scalability and fault-tolerance \nof WebArrow was guided by significant experimentation. The sorts of questions \nthat our experiments (spikes) were designed to answer were these:\n■\n■Would moving to a distributed database from local flat files negatively im-\npact feedback time (latency) for users?\n■\n■What (if any) scalability improvement would result from using mod_perl \nversus standard Perl? How difficult would the development and quality as-\nsurance effort be to convert to mod_perl?\n■\n■How many participants could be hosted by a single meeting server?\n■\n■What was the correct ratio between database servers and meeting servers?\nQuestions like these are difficult to answer analytically. The answers rely \non the behavior and interactions of third-party components, and on performance \ncharacteristics of software for which no standard analytic models exist. The Web­\nArrow team’s approach was to build an extensive testing infrastructure (including \nboth simulation and instrumentation), and to use this infrastructure to compare \nthe performance of each modification to the base system. This allowed the team \nto determine the effect of each proposed improvement before committing it to the \nfinal system. \nThe lesson here is that making architecture processes agile does not require \na radical re-invention of either Agile practices or architecture methods. The Web­\nArrow team’s emphasis on experimentation proved the key factor; it was our \nway of achieving an agile form of architecture conception, implementation, and \nevaluation. \nThis approach meant that the WebArrow architecture development approach \nwas in line with many of the twelve principles, including:\n■\n■Principle 1, providing early and continuous delivery of working software\n■\n■Principle 2, welcoming changing requirements\n■\n■Principle 3, delivering working software frequently\n■\n■Principle 8, promoting sustainable development at a constant pace \n■\n■Principle 9, giving continuous attention to technical excellence and good \ndesign\n",
      "content_length": 2687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "286 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\n15.4  Guidelines for the Agile Architect\nBarry Boehm and colleagues have developed the Incremental Commitment \nModel—a hybrid process model framework that attempts to find the balance \nbetween agility and commitment. This model is based upon the following six \nprinciples:\n1.\t\nCommitment and accountability of success-critical stakeholders\n2.\t\nStakeholder “satisficing” (meeting an acceptability threshold) based on suc-\ncess-based negotiations and tradeoffs\n3.\t\nIncremental and evolutionary growth of system definition and stakeholder \ncommitment\n4.\t\nIterative system development and definition\n5.\t\nInterleaved system definition and development allowing early fielding of \ncore capabilities, continual adaptation to change, and timely growth of \ncomplex systems without waiting for every requirement and subsystem to \nbe defined\n6.\t\nRisk management—risk-driven anchor point milestones, which are key to \nsynchronizing and stabilizing all of this concurrent activity\nGrady Booch has also provided a set of guidelines for an agile architecture \n(which in turn imply some duties for the agile architect). Booch claims that all \ngood software-intensive architectures are agile. What does he mean by this? He \nmeans that a successful architecture is resilient and loosely coupled. It is com-\nposed of a core set of well-reasoned design decisions but still contains some \n“wiggle room” that allows modifications to be made and refactorings to be done, \nwithout ruining the original structure. \nBooch also notes that an effective agile process will allow the architecture to \ngrow incrementally as the system is developed and matures. The key to success \nis to have decomposability, separation of concerns, and near-independence of the \nparts. (Sound familiar? These are all modifiability tactics.)\nFinally, Booch notes that to be agile, the architecture should be visible and \nself-evident in the code; this means making the design patterns, cross-cutting \nconcerns, and other important decisions obvious, well communicated, and de-\nfended. This may, in turn, require documentation. But whatever architectural de-\ncisions are made, the architect must make an effort to “socialize” the architecture.\nWard Cunningham has coined the term “technical debt.” Technical debt is \nan analogy to the normal debt that we acquire as consumers: we purchase some-\nthing now and (hope to) pay for it later. In software the equivalent of “purchas-\ning something now” is quick-and-dirty implementation. Such implementation \nfrequently leaves technical debt that incurs penalties in the future, in terms of \nincreased maintenance costs. When technical debt becomes unacceptably high, \nprojects need to pay down some of this debt, in the form of refactoring, which is \na key part of every agile architecting process.\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "15.5  Summary\n287\nWhat is our advice? \n1.\t\nIf you are building a large and complex system with relatively stable and \nwell-understood requirements, it is probably optimal to do a large amount \nof architecture work up front (see Figure 15.1 for some sample values for \n“large”).\n2.\t\nOn big projects with vague or unstable requirements, start by quickly de-\nsigning a complete candidate architecture even if it is just a “PowerPoint \narchitecture,” even if it leaves out many details, and even if you design it \nin just a couple of days. Alistair Cockburn has introduced a similar idea \nin his Crystal Clear method, called a “walking skeleton,” which is enough \narchitecture to be able to demonstrate end-to-end functionality, linking \ntogether the major system functions. Be prepared to change and elaborate \nthis architecture as circumstances dictate, as you perform your spikes and \nexperiments, and as functional and quality attribute requirements emerge \nand solidify. This early architecture will help guide development, help with \nearly problem understanding and analysis, help in requirements elicitation, \nhelp teams coordinate, and help in the creation of coding templates and oth-\ner project standards.\n3.\t\nOn smaller projects with uncertain requirements, at least try to get agree-\nment on the central patterns to be employed, but don’t spend too much time \non construction, documentation, or analysis up front. In Chapter 21 we will \nshow how analysis can be done in a relatively lightweight and “just-in-\ntime” fashion. \n15.5  Summary\nThe Agile software movement is emblemized by the Agile Manifesto and a set \nof principles that assign high value to close-knit teams and continuous and fre-\nquent delivery of working software. Agile processes were initially employed on \nsmall- to medium-sized projects with short time frames and enjoyed considerable \nsuccess. They were not often used for larger projects, particularly those with dis-\ntributed development. \nAlthough there might appear to be an inherent tension between being ag-\nile and architecture practices of the sort prescribed in this book, the underlying \nphilosophies are not at odds and can be married to great effect. Successful proj-\nects need a successful blend of the two approaches. Too much up-front planning \nand commitment can be stifling and unresponsive to customers’ needs, whereas \ntoo much agility can simply result in chaos. Agile architects tend to take a middle \nground, proposing an initial architecture and running with that, until its technical \ndebt becomes too great, at which point they need to refactor.\n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "288 \nPart Three  Architecture in the Life Cycle\t\n15—Architecture in Agile Projects\nBoehm and Turner, analyzing historical data from 161 industrial projects, \nexamined the effects of up-front architecture and risk resolution effort. They \nfound that projects tend to have a “sweet spot” where some up-front architecture \nplanning pays off and is not wasteful. \nAmong this book’s architecture methods, documentation and evaluation \nmight seem to be where the most friction with Agile philosophies might lie. \nHowever, our approaches to these activities are risk-based and embodied in meth-\nods that help you focus effort where it will most pay off.\nThe WebArrow example showed how adding experimentation to the proj-\nect’s processes enabled it to obtain benefits from both architecture and classic \nAgile practices, and be responsive to ever-changing requirements and domain \nunderstanding.\n15.6  For Further Reading\nAgile comes in many flavors. Here are some of the better-known ones:\n■\n■Extreme Programming [Beck 04]\n■\n■Scrum [Schwaber 04]\n■\n■Feature-Driven Development [Palmer 02]\n■\n■Crystal Clear [Cockburn 04] \nThe journal IEEE Software devoted an entire special issue in 2010 to the \ntopic of agility and architecture. The editor’s introduction [Abrahamsson 10] dis-\ncusses many of the issues that we have raised here.\nGeorge Fairbanks in his book Just Enough Architecture [Fairbanks 10] pro-\nvides techniques that are very compatible with Agile methods.\nBarry Boehm and Richard Turner [Boehm 04] offer a data- and analy-\nsis-driven perspective on the risks and tradeoffs involved in the continuum of \nchoices regarding agility and what they called “discipline.” The choice of “agil-\nity versus discipline” in the title of the book has angered and alienated many \npractitioners of Agile methods, most of which are quite disciplined. While this \nbook does not focus specifically on architecture, it does touch on the subject in \nmany ways. This work was expanded upon in 2010, when Boehm, Lane, Kool-\nmanojwong, and Turner described the Incremental Commitment Model and its \nrelationship to agility and architecture [Boehm 10]. All of Boehm and colleagues’ \nwork is informed by an active attention to risk. The seminal article on software \nrisk management [Boehm 91] was written by Barry Boehm, more than 20 years \nago, and it is still relevant and compelling reading today.\nCarriere, Kazman, and Ozkaya [Carriere 10] provide a way to reason about \nwhen and where in an architecture you should do refactoring—to reduce techni-\ncal debt—based on an analysis of the propagation cost of anticipated changes.\n",
      "content_length": 2608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "15.7  Discussion Questions\n289\nThe article by Graham, Kazman, and Walmsley [Graham 07] provides sub-\nstantially more detail on the WebArrow case study of agile architecting, includ-\ning a number of architectural diagrams and additional description of the experi-\nmentation performed. \nWard Cunningham first coined the term “technical debt” in 1992 [Cunning-\nham 92]. Brown et al. [Brown 10], building in part on Cunningham’s work, offer \nan economics-driven perspective on how to enable agility through architecture. \nRobert Nord, Jim Tomayko, and Rob Wojcik [Nord 04] have analyzed the \nrelationship between several of the Software Engineering Institute’s architecture \nmethods and Extreme Programming. Grady Booch has blogged extensively on the \nrelationship between architecture and Agile in his blog, for example [Booch 11].\nFelix Bachmann [Bachmann 11] has provided a concrete example of a light-\nweight version of the ATAM that fits well with Agile projects and principles.\n15.7  Discussion Questions\n1.\t\nHow would you employ the Agile practices of pair programming, frequent \nteam interaction, and dedicated customer involvement in a distributed de-\nvelopment environment?\n2.\t\nSuppose, as a supporter of architecture practices, you were asked to write \nan Architecture Manifesto that was modeled on the Agile Manifesto. What \nwould it look like?\n3.\t\nAgile projects must be budgeted and scheduled like any other. How would \nyou do that? Does an architecture help or hinder this process?\n4.\t\nWhat do you think are the essential skills for an architect operating in an \nAgile context? How do you suppose they differ for an architect working in \na non-Agile project?\n5.\t\nThe Agile Manifesto professes to value individuals and interactions over \nprocesses and tools. Rationalize this statement in terms of the role of tools \nin the modern software development process: compilers, integrated devel-\nopment environments, debuggers, configuration managers, automatic test \ntools, and build and configuration tools.\n6.\t\nCritique the Agile Manifesto in the context of a 200-developer, 5-mil-\nlion-line project with an expected lifetime of 20 years.\n",
      "content_length": 2146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "291\n16\nArchitecture and \nRequirements\nThe two most important requirements for major \nsuccess are: first, being in the right place at the \nright time, and second, doing something about it.\n—Ray Kroc\nArchitectures exist to build systems that satisfy requirements. That’s obvious. \nWhat may be less obvious is that, to an architect, not all requirements are created \nequal. Some have a much more profound effect on the architecture than others. \nAn architecturally significant requirement (ASR) is a requirement that will have \na profound effect on the architecture—that is, the architecture might well be dra-\nmatically different in the absence of such a requirement. \nYou cannot hope to design a successful architecture if you do not know the \nASRs. ASRs often, but not always, take the form of quality attribute require-\nments—the performance, security, modifiability, availability, usability, and so \nforth, that the architecture must provide to the system. In Chapters 5–13 we in-\ntroduced patterns and tactics to achieve quality attributes. Each time you select \na pattern or tactic to use in your architecture, you are changing the architecture \nas a result of the need to meet quality attribute requirements. The more difficult \nand important the QA requirement, the more likely it is to significantly affect the \narchitecture, and hence to be an ASR. \nArchitects have to identify ASRs, usually after doing a significant bit of \nwork to uncover candidate ASRs. Competent architects know this, and as we ob-\nserve experienced architects going about their duties, we notice that the first thing \nthey do is start talking to the important stakeholders. They’re gathering the in-\nformation they need to produce the architecture that will respond to the project’s \nneeds—whether or not this information has already been identified. \nThis chapter provides some systematic means for identifying the ASRs and \nother factors that will shape the architecture.\n",
      "content_length": 1955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "292 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.1  Gathering ASRs from Requirements Documents\nAn obvious location to look for candidate ASRs is in the requirements documents \nor in user stories. After all, we are looking for requirements, and requirements \nshould be in requirements documents. Unfortunately, this is not usually the case, \nalthough as we will see, there is information in the requirements documents that \ncan be of use. \nDon’t Get Your Hopes Up\nMany projects don’t create or maintain the kind of requirements document that \nprofessors in software engineering classes or authors of traditional software en-\ngineering books love to prescribe. Whether requirements are specified using the \n“MoSCoW” style (must, should, could, won’t), or as a collection of “user sto-\nries,” neither of these is much help in nailing down quality attributes.\nFurthermore, no architect just sits and waits until the requirements are “fin-\nished” before starting work. The architect must begin while the requirements are \nstill in flux. Consequently, the QA requirements are quite likely to be up in the \nair when the architect starts work. Even where they exist and are stable, require-\nments documents often fail an architect in two ways.\nFirst, most of what is in a requirements specification does not affect the \narchitecture. As we’ve seen over and over, architectures are mostly driven or \n“shaped” by quality attribute requirements. These determine and constrain the \nmost important architectural decisions. And yet the vast bulk of most require-\nments specifications is focused on the required features and functionality of a \nsystem, which shape the architecture the least. The best software engineering \npractices do prescribe capturing quality attribute requirements. For example, the \nSoftware Engineering Body of Knowledge (SWEBOK) says that quality attribute \nrequirements are like any other requirements. They must be captured if they are \nimportant, and they should be specified unambiguously and be testable. \nIn practice, though, we rarely see adequate capture of quality attribute re-\nquirements. How many times have you seen a requirement of the form “The \nsystem shall be modular” or “The system shall exhibit high usability” or “The \nsystem shall meet users’ performance expectations”? These are not requirements, \nbut in the best case they are invitations for the architect to begin a conversation \nabout what the requirements in these areas really are. \nSecond, much of what is useful to an architect is not in even the best re-\nquirements document. Many concerns that drive an architecture do not manifest \nthemselves at all as observables in the system being specified, and so are not \nthe subject of requirements specifications. ASRs often derive from business goals \nin the development organization itself; we’ll explore this in Section 16.3. De-\nvelopmental qualities are also out of scope; you will rarely see a requirements \n",
      "content_length": 2987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "16.1  Gathering ASRs from Requirements Documents\n293\ndocument that describes teaming assumptions, for example. In an acquisition \ncontext, the requirements document represents the interests of the acquirer, not \nthat of the developer. But as we saw in Chapter 3, stakeholders, the technical en-\nvironment, and the organization itself all play a role in influencing architectures. \nSniffing Out ASRs from a Requirements Document\nAlthough requirements documents won’t tell an architect the whole story, they \nare an important source of ASRs. Of course, ASRs aren’t going to be conve-\nniently labeled as such; the architect is going to have to perform a bit of excava-\ntion and archaeology to ferret them out.\nChapter 4 categorizes the design decisions that architects have to make. Ta-\nble 16.1 summarizes each category of architectural design decision, and it gives \na list of requirements to look for that might affect that kind of decision. If a re-\nquirement affects the making of a critical architectural design decision, it is by \ndefinition an ASR.\nTable 16.1  Early Design Decisions and Requirements That Can Affect Them\nDesign Decision Category\nLook for Requirements Addressing . . .\nAllocation of Responsibilities\nPlanned evolution of responsibilities, user roles, \nsystem modes, major processing steps, commercial \npackages\nCoordination Model\nProperties of the coordination (timeliness, currency, \ncompleteness, correctness, and consistency)\nNames of external elements, protocols, sensors \nor actuators (devices), middleware, network \nconfigurations (including their security properties)\nEvolution requirements on the list above \nData Model\nProcessing steps, information flows, major domain \nentities, access rights, persistence, evolution \nrequirements\nManagement of Resources\nTime, concurrency, memory footprint, scheduling, \nmultiple users, multiple activities, devices, energy \nusage, soft resources (buffers, queues, etc.) \nScalability requirements on the list above\nMapping among Architectural \nElements\nPlans for teaming, processors, families of \nprocessors, evolution of processors, network \nconfigurations\nBinding Time Decisions\nExtension of or flexibility of functionality, regional \ndistinctions, language distinctions, portability, \ncalibrations, configurations\nChoice of Technology\nNamed technologies, changes to technologies \n(planned and unplanned)\n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "294 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.2  Gathering ASRs by Interviewing Stakeholders\nSay your project isn’t producing a comprehensive requirements document. Or it \nis, but it’s not going to have the QAs nailed down by the time you need to start \nyour design work. What do you do?\nArchitects are often called upon to help set the quality attribute requirements \nfor a system. Projects that recognize this and encourage it are much more likely \nto be successful than those that don’t. Relish the opportunity. Stakeholders often \nhave no idea what QAs they want in a system, and no amount of nagging is going \nto suddenly instill the necessary insight. If you insist on quantitative QA require-\nments, you’re likely to get numbers that are arbitrary, and there’s a good chance \nthat you’ll find at least some of those requirements will be very difficult to satisfy. \nArchitects often have very good ideas about what QAs are exhibited by sim-\nilar systems, and what QAs are reasonable (and reasonably straightforward) to \nprovide. Architects can usually provide quick feedback as to which quality attri-\nbutes are going to be straightforward to achieve and which are going to be prob-\nlematic or even prohibitive. And architects are the only people in the room who \ncan say, “I can actually deliver an architecture that will do better than what you \nhad in mind—would that be useful to you?”\nInterviewing the relevant stakeholders is the surest way to learn what they \nknow and need. Once again, it behooves a project to capture this critical informa-\ntion in a systematic, clear, and repeatable way. Gathering this information from \nstakeholders can be achieved by many methods. One such method is the Quality \nAttribute Workshop (QAW), described in the sidebar.\nThe results of stakeholder interviews should include a list of architectural \ndrivers and a set of QA scenarios that the stakeholders (as a group) prioritized. \nThis information can be used to do the following:\n■\n■Refine system and software requirements\n■\n■Understand and clarify the system’s architectural drivers\n■\n■Provide rationale for why the architect subsequently made certain design \ndecisions\n■\n■Guide the development of prototypes and simulations\n■\n■Influence the order in which the architecture is developed\nThe Quality Attribute Workshop\nThe QAW is a facilitated, stakeholder-focused method to generate, prior-\nitize, and refine quality attribute scenarios before the software architec-\nture is completed. The QAW is focused on system-level concerns and \n",
      "content_length": 2574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "16.2  Gathering ASRs by Interviewing Stakeholders\n295\nspecifically the role that software will play in the system. The QAW is \nkeenly dependent on the participation of system stakeholders.1\nThe QAW involves the following steps:\nStep 1: QAW Presentation and Introductions. QAW facilitators describe \nthe motivation for the QAW and explain each step of the method. Everyone \nintroduces themselves, briefly stating their background, their role in the \norganization, and their relationship to the system being built.\nStep 2: Business/Mission Presentation. The stakeholder representing the \nbusiness concerns behind the system (typically a manager or management \nrepresentative) spends about one hour presenting the system’s business \ncontext, broad functional requirements, constraints, and known quality \nattribute requirements. The quality attributes that will be refined in later \nsteps will be derived largely from the business/mission needs presented in \nthis step.\nStep 3: Architectural Plan Presentation. Although a detailed system \nor software architecture might not exist, it is possible that broad system \ndescriptions, context drawings, or other artifacts have been created that \ndescribe some of the system’s technical details. At this point in the work-\nshop, the architect will present the system architectural plans as they stand. \nThis lets stakeholders know the current architectural thinking, to the extent \nthat it exists.\nStep 4: Identification of Architectural Drivers. The facilitators will share \ntheir list of key architectural drivers that they assembled during steps 2 and \n3, and ask the stakeholders for clarifications, additions, deletions, and cor-\nrections. The idea is to reach a consensus on a distilled list of architectural \ndrivers that includes overall requirements, business drivers, constraints, \nand quality attributes. \nStep 5: Scenario Brainstorming. Each stakeholder expresses a scenario \nrepresenting his or her concerns with respect to the system. Facilitators \nensure that each scenario has an explicit stimulus and response. The \nfacilitators ensure that at least one representative scenario exists for each \narchitectural driver listed in step 4.\nStep 6: Scenario Consolidation. After the scenario brainstorming, similar \nscenarios are consolidated where reasonable. Facilitators ask stakeholders \nto identify those scenarios that are very similar in content. Scenarios that \nare similar are merged, as long as the people who proposed them agree \nand feel that their scenarios will not be diluted in the process. Consolidation \nhelps to prevent votes from being spread across several scenarios that \nare expressing the same concern. Consolidating almost-alike scenarios \nassures that the underlying concern will get all of the votes it is due. \nStep 7: Scenario Prioritization. Prioritization of the scenarios is ac-\ncomplished by allocating each stakeholder a number of votes equal to 30 \npercent of the total number of scenarios generated after consolidation. \nStakeholders can allocate any number of their votes to any scenario or \n1.  This material was adapted from [Barbacci 03].\n",
      "content_length": 3129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "296 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\ncombination of scenarios. The votes are counted, and the scenarios are \nprioritized accordingly.\nStep 8: Scenario Refinement. After the prioritization, the top scenar-\nios are refined and elaborated. Facilitators help the stakeholders put the \nscenarios in the six-part scenario form of source-stimulus-artifact-environ-\nment-response-response measure that we described in Chapter 4. As the \nscenarios are refined, issues surrounding their satisfaction will emerge. \nThese are also recorded. Step 8 lasts as long as time and resources allow.\n16.3  \u0007Gathering ASRs by Understanding \nthe Business Goals\nBusiness goals are the raison d’être for building a system. No organization builds \na system without a reason; rather, the organization’s leaders want to further the \nmission and ambitions of their organization and themselves. Common business \ngoals include making a profit, of course, but most organizations have many more \nconcerns than simply profit, and in other organizations (e.g., nonprofits, charities, \ngovernments), profit is the farthest thing from anyone’s mind. \nBusiness goals are of interest to architects because they often are the precursor \nor progenitor of requirements that may or may not be captured in a requirements \nspecification but whose achievement (or lack) signals a successful (or less than suc-\ncessful) architectural design. Business goals frequently lead directly to ASRs.\nThere are three possible relationships between business goals and an \narchitecture:\n1.\t\nBusiness goals often lead to quality attribute requirements. Or to put it \nanother way, every quality attribute requirement—such as user-visible \nresponse time or platform flexibility or ironclad security or any of a \ndozen other needs—should originate from some higher purpose that can \nbe described in terms of added value. If we ask, for example, “Why do \nyou want this system to have a really fast response time?”, we might hear \nthat this will differentiate the product from its competition and let the \ndeveloping organization capture market share; or that this will make the \nsoldier a more effective warfighter, which is the mission of the acquiring \norganization; or other reasons having to do with the satisfaction of some \nbusiness goal. \n2.\t\nBusiness goals may directly affect the architecture without precipitating \na quality attribute requirement at all. In Chapter 3 we told the story of \nthe architect who designed a system without a database until the manager \ninformed him that the database team needed work. The architecture was \nimportantly affected without any relevant quality attribute requirement.\n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "16.3  Gathering ASRs by Understanding the Business Goals\n297\nBusiness Goals\nQuality Attributes\nArchitecture\nNonarchitectural Solutions\nFigure 16.1  Some business goals may lead to quality attribute requirements \n(which lead to architectures), or lead directly to architectural decisions, or lead to \nnonarchitectural solutions.\n3.\t\nNo influence at all. Not all business goals lead to quality attributes. For \nexample, a business goal to “reduce cost” may be realized by lowering the \nfacility’s thermostats in the winter or reducing employees’ salaries or pensions. \nFigure 16.1 illustrates the major points just described. In the figure, the ar-\nrows mean “leads to.” The solid arrows are the ones highlighting relationships of \nmost interest to architects.\nArchitects often become aware of an organization’s business and business \ngoals via osmosis—working, listening, talking, and soaking up the goals that are \nat work in an organization. Osmosis is not without its benefits, but more system-\natic ways are possible. We describe one such way in the sidebar “A Method for \nCapturing Business Goals.”\nA Categorization of Business Goals\nBusiness goals are worth capturing explicitly. This is because they often imply \nASRs that would otherwise go undetected until it is too late or too expensive to \naddress them. Capturing business goals is well served by having a set of candi-\ndate business goals handy to use as conversation starters. If you know that many \nbusinesses want to gain market share, for instance, you can use that to engage \nthe right stakeholders in your organization to ask, “What are our ambitions about \nmarket share for this product, and how could the architecture contribute to meet-\ning them?”\nOur research in business goals has led us to adopt the categories shown in \nTable 16.2. These categories can be used as an aid to brainstorming and elici-\ntation. By employing the list of categories, and asking the stakeholders about \npossible business goals in each category, some assurance of coverage is gained. \n",
      "content_length": 2035,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "298 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nTable 16.2  A List of Standard Business Goal Categories\n1.\nContributing to the growth and continuity of the organization\n2.\nMeeting financial objectives\n3.\nMeeting personal objectives\n4.\nMeeting responsibility to employees\n5.\nMeeting responsibility to society\n6.\nMeeting responsibility to state\n7.\nMeeting responsibility to shareholders\n8.\nManaging market position\n9.\nImproving business processes\n10.\nManaging the quality and reputation of products \n11.\nManaging change in environmental factors\nThese categories are not completely orthogonal. Some business goals may \nfit into more than one category, and that’s all right. In an elicitation method, the \ncategories should prompt questions about the existence of organizational busi-\nness goals that fall into that category. If the categories overlap, then this might \ncause us to ask redundant questions. This is not harmful and could well be help-\nful. The utility of these categories is to help identify all business goals, not to \nprovide a taxonomy.\n1.\t Contributing to the growth and continuity of the organization. How does \nthe system being developed contribute to the growth and continuity of \nthe organization? In one experience using this business goal category, \nthe system being developed was the sole reason for the existence of the \norganization. If the system was not successful, the organization would \ncease to exist. Other topics that might come up in this category deal with \nmarket share, product lines, and international sales.\n2.\t Meeting financial objectives. This category includes revenue generated or \nsaved by the system. The system may be for sale, either in standalone form \nor by providing a service, in which case it generates revenue. The system \nmay be for use in an internal process, in which case it should make those \nprocesses more effective or more efficient. Also in this category is the cost \nof development, deployment, and operation of the system. But this category \ncan also include financial objectives of individuals: a manager hoping for a \nraise, for example, or a shareholder expecting a dividend.\n3.\t Meeting personal objectives. Individuals have various goals associated with \nthe construction of a system. They may range from “I want to enhance my \nreputation by the success of this system” to “I want to learn new technol-\nogies” to “I want to gain experience with a different portion of the devel-\nopment process than in the past.” In any case, it is possible that technical \ndecisions are influenced by personal objectives.\n",
      "content_length": 2610,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "16.3  Gathering ASRs by Understanding the Business Goals\n299\n4.\t Meeting responsibility to employees. In this category, the employees in \nquestion are usually those employees involved in development or those \ninvolved in operation. Responsibility to employees involved in develop-\nment might include ensuring that certain types of employees have a role \nin the development of this system, or it might include providing employ-\nees the opportunities to learn new skills. Responsibility to employees \ninvolved in operating the system might include safety, workload, or skill \nconsiderations.\n5.\t Meeting responsibility to society. Some organizations see themselves as \nbeing in business to serve society. For these organizations, the system under \ndevelopment is helping them meet those responsibilities. But all organiza-\ntions must discharge a responsibility to society by obeying relevant laws \nand regulations. Other topics that might come up under this category are \nresource usage, “green computing,” ethics, safety, open source issues, secu-\nrity, and privacy.\n6.\t Meeting responsibility to state. Government systems, almost by definition, \nare intended to meet responsibility to a state or country. Other topics that \nmight come up in this category deal with export controls, regulatory confor-\nmance, or supporting government initiatives.\n7.\t Meeting responsibility to shareholders. There is overlap between this cate-\ngory and the financial objectives category, but additional topics that might \ncome up here are liability protection and certain types of regulatory confor-\nmance such as, in the United States, adherence to the Sarbanes-Oxley Act.\n8.\t Managing market position. Topics that might come up in this category are \nthe strategy used to increase or hold market share, various types of intellec-\ntual property protection, or the time to market.\n9.\t Improving business processes. Although this category partially overlaps \nwith meeting financial objectives, reasons other than cost reduction exist for \nimproving business processes. It may be that improved business processes \nenable new markets, new products, or better customer support.\n10.\t Managing the quality and reputation of products. Topics that might come \nup in this category include branding, recalls, types of potential users, quali-\nty of existing products, and testing support and strategies.\n11.\t Managing change in environmental factors. As we said in Chapter 3, the \nbusiness context for a system might change. This item is intended to en-\ncourage the stakeholders to consider what might change in the business \ngoals for a system.\nExpressing Business Goals \nHow will you write down a business goal once you’ve learned it? Just as for \nquality attributes, a scenario makes a convenient, uniform, and clarifying way \nto express business goals. It helps ensure that all business goals are expressed \n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "300 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nclearly, in a consistent fashion, and contain sufficient information to enable their \nshared understanding by relevant stakeholders. Just as a quality attribute scenario \nadds precision and meaning to an otherwise vague need for, say, “modifiability,” \na business goal scenario will add precision and meaning to a desire to “meet fi-\nnancial objectives.”\nOur business goal scenario template has seven parts. They all relate to the \nsystem under development, the identity of which is implicit. The parts are these:\n1.\t\nGoal-source. These are the people or written artifacts providing the goal.\n2.\t\nGoal-subject. These are the stakeholders who own the goal and wish it to \nbe true. Each stakeholder might be an individual or (in the case of a goal \nthat has no one owner and has been assimilated into an organization) the or-\nganization itself. If the business goal is, for example, “Maximize dividends \nfor the shareholders,” who is it that cares about that? It is probably not the \nprogrammers or the system’s end users (unless they happen to own stock). \nGoal-subjects can and do belong to different organizations. The developing \norganization, the customer organizations, subcontractors, vendors and sup-\npliers, standards bodies, regulatory agencies, and organizations responsible \nfor systems with which ours must interact are all potential goal-subjects.\n3.\t\nGoal-object. These are the entities to which the goal applies. “Object” \nis used in the sense of the object of a verb in a sentence. All goals have \ngoal-objects: we want something to be true about something (or someone) \nthat (or whom) we care about. For example, for goals we would character-\nize as furthering one’s self-interest, the goal-object can be “myself or my \nfamily.” For some goals the goal-object is clearly the development orga-\nnization, but for some goals the goal-object can be more refined, such as \nthe rank-and-file employees of the organization or the shareholders of the \norganization. Table 16.3 is a representative cross-section of goal-objects. \nGoal-objects in the table start small, where the goal-object is a single indi-\nvidual, and incrementally grow until the goal-object is society at large.\n4.\t\nEnvironment. This is the context for this goal. For example, there are social, \nlegal, competitive, customer, and technological environments. Sometimes \nthe political environment is key; this is as a kind of social factor. Upcoming \ntechnology may be a major factor.\n5.\t\nGoal. This is any business goal articulated by the goal-source.\n6.\t\nGoal-measure. This is a testable measurement to determine how one would \nknow if the goal has been achieved. The goal-measure should usually \ninclude a time component, stating the time by which the goal should be \nachieved.\n",
      "content_length": 2839,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "16.3  Gathering ASRs by Understanding the Business Goals\n301\n7.\t\nPedigree and value. The pedigree of the goal tells us the degree of \nconfidence the person who stated the goal has in it, and the goal’s volatility \nand value. The value of a goal can be expressed by how much its owner \nis willing to spend to achieve it or its relative importance compared to \nother goals. Relative importance may be given by a ranking from 1 (most \nimportant) to n (least important), or by assigning each goal a value on \na fixed scale such as 1 to 10 or high-medium-low. We combine value \nand pedigree into one part although it certainly is possible to treat them \nseparately. The important concern is that both are captured.\nElements 2–6 can be combined into a sentence that reads: \nFor the system being developed, <goal-subject> desires that <goal-\nobject> achieve <goal> in the context of <environment> and will be \nsatisfied if <goal-measure>. \nThe sentence can be augmented by the goal’s source (element 1) and the \ngoal’s pedigree and value (element 7). Some sample business goal scenarios in-\nclude the following:\n■\n■For MySys, the project manager has the goal that his family’s stock in the \ncompany will rise by 5 percent (as a result of the success of MySys).\n■\n■For MySys, the developing organization’s CEO has the goal that MySys \nwill make it 50 percent less likely that his nation will be attacked.\n■\n■For MySys, the portfolio manager has the goal that MySys will make the \nportfolio 30 percent more profitable.\n■\n■For MySys, the project manager has the goal that customer satisfaction will \nrise by 10 percent (as a result of the increased quality of MySys).\nIn many contexts, the goals of different stakeholders may conflict. By iden-\ntifying the stakeholder who owns the goal, the sources of conflicting goals can be \nidentified.\nA General Scenario for Business Goals\nA general scenario (see Chapter 4) is a template for constructing specific or “con-\ncrete” scenarios. It uses the generic structure of a scenario to supply a list of \npossible values for each non-boilerplate part of a scenario. See Table 16.4 for a \ngeneral scenario for business goals. \nFor each of these scenarios you might want to additionally capture its source \n(e.g., Did this come directly from the goal-subject, a document, a third party, a \nlegal requirement?), its volatility, and its importance.\n",
      "content_length": 2376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "Table 16.3  Business Goals and Their Goal-Objects\nGoal-Object\nBusiness Goals That Often Have This Goal-Object\nRemarks\nIndividual\nPersonal wealth, power, honor/face/reputation, game and gambling spirit, \nmaintain or improve reputation (personal), family interests\nThe individual who has these goals has them for \nhim/herself or his/her family.\nSystem\nManage flexibility, distributed development, portability, open systems/standards, \ntestability, product lines, integrability, interoperability, ease of installation and \nease of repair, flexibility/configurability, performance, reliability/availability, ease \nof use, security, safety, scalability/extendibility, functionality, system constraints, \ninternationalization, reduce time to market\nThese can be goals for a system being developed or \nacquired. The list applies to systems in general, but \nthe quantification of any one item likely applies to a \nsingle system being developed or acquired.\nPortfolio\nReduce cost of development, cost leadership, differentiation, reduce cost of \nretirement, smooth transition to follow-on systems, replace legacy systems, \nreplace labor with automation, diversify operational sequence, eliminate \nintermediate stages, automate tracking of business events, collect/communicate/\nretrieve operational knowledge, improve decision making, coordinate across \ndistance, align task and process, manage on basis of process measurements, \noperate effectively within the competitive environment, the technological \nenvironment, or the customer environment\nCreate something new, provide the best quality products and services possible, \nbe the leading innovator in the industry\nThese goals live on the cusp between an individual \nsystem and the entire organization. They apply either \nto a single system or to an organization’s entire \nportfolio that the organization is building or acquiring \nto achieve its goals. \nOrganization’s \nEmployees\nProvide high rewards and benefits to employees, create a pleasant and friendly \nworkplace, have satisfied employees, fulfill responsibility toward employees, \nmaintain jobs of workforce on legacy systems\nBefore we get to the organization as a whole, there \nare some goals aimed at specific subsets of the \norganization.\nOrganization’s \nShareholders\nMaximize dividends for the shareholders\nOrganization\nGrowth of the business, continuity of the business, maximize profits over the \nshort run, maximize profits over the long run, survival of the organization, \nmaximize the company’s net assets and reserves, be a market leader, maximize \nthe market share, expand or retain market share, enter new markets, maximize \nthe company’s rate of growth, keep tax payments to a minimum, increase sales \ngrowth, maintain or improve reputation, achieve business goals through financial \nobjectives, run a stable organization\nThese are goals for the organization as a whole. The \norganization can be a development or acquisition \norganization, although most were undoubtedly \ncreated with the former in mind.\nNation\nPatriotism, national pride, national security, national welfare\nBefore we get to society at large, this goal-object is \nspecifically limited to the goal owner’s own country.\nSociety\nRun an ethical organization, responsibility toward society, be a socially \nresponsible company, be of service to the community, operate effectively within \nsocial environment, operate effectively within legal environment\nSome interpret “society” as “my society,” which puts \nthis category closer to the nation goal-object, but we \nare taking a broader view.\n",
      "content_length": 3565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "Table 16.4  General Scenario Generation Table for Business Goals\n2. Goal-subject\n. . . has the goal that . . .\n3. Goal-object\n. . . achieves . . .\n5. Goal\n. . . in the context of . . .\n4. Environment\n. . . and will be satisfied if . . .\n6. Goal-measure (examples, \nbased on goal categories)\n7. Value\nAny stakeholder \nor stakeholder \ngroup identified \nas having a \nlegitimate \ninterest in the \nsystem\nIndividual\nSystem\nPortfolio\nOrganization’s \nemployees\nOrganization’s \nshareholders\nOrganization\nNation\nSociety\nContributing to the \ngrowth and continuity \nof the organization\nMeeting financial \nobjectives\nMeeting personal \nobjectives\nMeeting responsibility \nto employees\nMeeting responsibility \nto society\nMeeting responsibility \nto state\nMeeting responsibility \nto shareholders\nManaging market \nposition\nImproving business \nprocesses\nManaging quality and \nreputation of products \nManaging change in \nenvironmental factors\nSocial (includes \npolitical)\nLegal\nCompetitive\nCustomer\nTechnological\nTime that business remains \nviable\nFinancial performance vs. \nobjectives\nPromotion or raise achieved in \nperiod\nEmployee satisfaction; turnover \nrate\nContribution to trade deficit/\nsurplus\nStock price, dividends\nMarket share\nTime to carry out a business \nprocess\nQuality measures of products\nTechnology-related problems\nTime window for achievement\n1–n\n1–10\nH-M-L\nResources \nwilling to \nexpend\n",
      "content_length": 1386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "304 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nCapturing Business Goals\nBusiness goals are worth capturing because they can hold the key to discovering \nASRs that emerge in no other context. One method for eliciting and documenting \nbusiness goals is the Pedigreed Attribute eLicitation Method, or PALM. The word \n“pedigree” means that the business goal has a clear derivation or background. \nPALM uses the standard list of business goals and the business goal scenario \nformat we described earlier. \nPALM can be used to sniff out missing requirements early in the life cycle. \nFor example, having stakeholders subscribe to the business goal of improving the \nquality and reputation of their products may very well lead to (for example) se-\ncurity, availability, and performance requirements that otherwise might not have \nbeen considered.\nPALM can also be used to discover and carry along additional informa-\ntion about existing requirements. For example, a business goal might be to pro-\nduce a product that outcompetes a rival’s market entry. This might precipitate \na performance requirement for, say, half-second turnaround when the rival fea-\ntures one-second turnaround. But if the competitor releases a new product with \nhalf-second turnaround, then what does our requirement become? A conventional \nrequirements document will continue to carry the half-second requirement, but \nthe goal-savvy architect will know that the real requirement is to beat the compet-\nitor, which may mean even faster performance is needed.\nFinally, PALM can be used to examine particularly difficult quality attribute \nrequirements to see if they can be relaxed. We know of more than one system \nwhere a quality attribute requirement proved quite expensive to provide, and only \nafter great effort, money, and time were expended trying to meet it was it re-\nvealed that the requirement had no actual basis other than being someone’s best \nguess or fond wish at the time.\n16.4  Capturing ASRs in a Utility Tree\nAs we have seen, ASRs can be extracted from a requirements document, captured \nfrom stakeholders in a workshop such as a QAW, or derived from business goals. \nIt is helpful to record them in one place so that the list can be reviewed, refer-\nenced, used to justify design decisions, and revisited over time or in the case of \nmajor system changes.\nTo recap, an ASR must have the following characteristics:\n■\n■A profound impact on the architecture. Including this requirement will very \nlikely result in a different architecture than if it were not included.\n■\n■A high business or mission value. If the architecture is going to satisfy this \nrequirement—potentially at the expense of not satisfying others—it must be \nof high value to important stakeholders.\n",
      "content_length": 2790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "16.4  Capturing ASRs in a Utility Tree\n305\nUsing a single list can also help evaluate each potential ASR against these \ncriteria, and to make sure that no architectural drivers, stakeholder classes, or \nbusiness goals are lacking ASRs that express their needs.\nA Method for Capturing Business Goals\nPALM is a seven-step method, nominally carried out over a day and a half \nin a workshop attended by architects and stakeholders who can speak to \nthe business goals of the organizations involved. The steps are these:\n1.\t PALM overview presentation. Overview of PALM, the problem it \nsolves, its steps, and its expected outcomes. \n2.\t Business drivers presentation. Briefing of business drivers \nby project management. What are the goals of the customer \norganization for this system? What are the goals of the \ndevelopment organization? This is normally a lengthy discussion \nthat allows participants to ask questions about the business goals \nas presented by project management. \n3.\t Architecture drivers presentation. Briefing by the architect on the \ndriving business and quality attribute requirements: the ASRs. \n4.\t Business goals elicitation. Using the standard business goal \ncategories to guide discussion, we capture the set of important \nbusiness goals for this system. Business goals are elaborated and \nexpressed as scenarios. We consolidate almost-alike business \ngoals to eliminate duplication. Participants then prioritize the \nresulting set to identify the most important goals. \n5.\t Identification of potential quality attributes from business goals. \nFor each important business goal scenario, participants describe \na quality attribute that (if architected into the system) would help \nachieve it. If the QA is not already a requirement, this is recorded \nas a finding.\n6.\t Assignment of pedigree to existing quality attribute drivers. \nFor each architectural driver named in step 3, we identify which \nbusiness goals it is there to support. If none, that’s recorded \nas a finding. Otherwise, we establish its pedigree by asking \nfor the source of the quantitative part. For example: Why is \nthere a 40-millisecond performance requirement? Why not 60 \nmilliseconds? Or 80 milliseconds?\n7.\t Exercise conclusion. Review of results, next steps, and participant \nfeedback.\n",
      "content_length": 2288,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "306 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\nArchitects can use a construct called a utility tree for all of these purposes. A \nutility tree begins with the word “utility” as the root node. Utility is an expression \nof the overall “goodness” of the system. We then elaborate this root node by listing \nthe major quality attributes that the system is required to exhibit. (We said in Chap-\nter 4 that quality attribute names by themselves were not very useful. Never fear: \nwe are using them only as placeholders for subsequent elaboration and refinement!) \nUnder each quality attribute, record a specific refinement of that QA. For \nexample, performance might be decomposed into “data latency” and “transac-\ntion throughput.” Or it might be decomposed into “user wait time” and “time \nto refresh web page.” The refinements that you choose should be the ones that \nare relevant to your system. Under each refinement, record the appropriate ASRs \n(usually expressed as QA scenarios).\nSome ASRs might express more than one quality attribute and so might ap-\npear in more than one place in the tree. That is not necessarily a problem, but it \ncould be an indication that the ASR tries to cover too much diverse territory. Such \nASRs may be split into constituents that each attach to smaller concerns.\nOnce the ASRs are recorded and placed in the tree, you can now evaluate \nthem against the two criteria we listed above: the business value of the candidate \nASR and the architectural impact of including it. You can use any scale you like, \nbut we find that a simple “H” (high), “M” (medium), and “L” (low) suffice for \neach criterion.\nFor business value, High designates a must-have requirement, Medium is for a \nrequirement that is important but would not lead to project failure were it omitted. \nLow describes a nice requirement to have but not something worth much effort. \nFor architectural impact, High means that meeting this ASR will profoundly \naffect the architecture. Medium means that meeting this ASR will somewhat af-\nfect the architecture. Low means that meeting this candidate ASR will have little \neffect on the architecture. \nTable 16.5 shows a portion of a sample utility tree drawn from a health care ap-\nplication called Nightingale. Each ASR is labeled with a pair of “H,” “M,” and “L” \nvalues indicating (a) the ASR’s business value and (b) its effect on the architecture.\nOnce you have a utility tree filled out, you can use it to make important \nchecks. For instance:\n■\n■A QA or QA refinement without any ASR is not necessarily an error or \nomission that needs to be rectified, but it is an indication that attention should \nbe paid to finding out for sure if there are unrecorded ASRs in that area.\n■\n■ASRs that rate a (H,H) rating are obviously the ones that deserve the most \nattention from you; these are the most significant of the significant require-\nments. A very large number of these might be a cause for concern about \nwhether the system is achievable. \n■\n■Stakeholders can review the utility tree to make sure their concerns are ad-\ndressed. (An alternative to the organization we have described here is to use \nstakeholder roles rather than quality attributes as the organizing rule under \n“Utility.”)\n",
      "content_length": 3275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "16.4  Capturing ASRs in a Utility Tree\n307\nTABLE 16.5  Tabular Form of the Utility Tree for the Nightingale ATAM Exercise \nQuality \nAttribute\nAttribute \nRefinement \nASR\nPerformance \nTransaction \nresponse time\nA user updates a patient’s account in response to a \nchange-of-address notification while the system is under \npeak load, and the transaction completes in less than \n0.75 second. (H,M)\nA user updates a patient’s account in response to a \nchange-of-address notification while the system is under \ndouble the peak load, and the transaction completes in \nless than 4 seconds. (L,M)\nThroughput \nAt peak load, the system is able to complete 150 \nnormalized transactions per second. (M,M)\nUsability \nProficiency \ntraining\nA new hire with two or more years’ experience in the \nbusiness becomes proficient in Nightingale’s core \nfunctions in less than 1 week. (M,L)\nA user in a particular context asks for help, and the \nsystem provides help for that context, within 3 seconds. \n(H,M)\nNormal \noperations\nA hospital payment officer initiates a payment plan for a \npatient while interacting with that patient and completes \nthe process without the system introducing delays. (M,M)\nConfigurability \nUser-defined \nchanges\nA hospital increases the fee for a particular service. The \nconfiguration team makes the change in 1 working day; \nno source code needs to change. (H,L)\nMaintainability\nRoutine \nchanges\nA maintainer encounters search- and response-time \ndeficiencies, fixes the bug, and distributes the bug fix with \nno more than 3 person-days of effort. (H,M)\nA reporting requirement requires a change to the report-\ngenerating metadata. Change is made in 4 person-hours \nof effort. (M,L)\nUpgrades to \ncommercial \ncomponents\nThe database vendor releases a new version that must \nbe installed in less than 3 person-weeks. (H,M)\nExtensibility \nAdding new \nproduct\nA product that tracks blood bank donors is created within \n2 person-months. (M,M)\nSecurity \nConfidentiality A physical therapist is allowed to see that part of a \npatient’s record dealing with orthopedic treatment but not \nother parts nor any financial information. (H,M)\nIntegrity\nThe system resists unauthorized intrusion and reports the \nintrusion attempt to authorities within 90 seconds. (H,M)\nAvailability\nNo downtime\nThe database vendor releases new software, which is \nhot-swapped into place, with no downtime. (H,L)\nThe system supports 24/7 web-based account access by \npatients. (L,L)\n",
      "content_length": 2460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "308 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n16.5  Tying the Methods Together\nHow should you employ requirements documents, stakeholder interviews, Qual-\nity Attribute Workshops, PALM, and utility trees in concert with each other?\nAs for most complex questions, the answer to this one is “It depends.” If you \nhave a requirements process that gathers, identifies, and prioritizes ASRs, then \nuse that and consider yourself lucky.\nIf you feel your requirements fall short of this ideal state, then you can bring \nto bear one or more of the other approaches. For example, if nobody has captured \nthe business goals behind the system you’re building, then a PALM exercise \nwould be a good way to ensure that those goals are represented in the system’s \nASRs. \nIf you feel that important stakeholders have been overlooked in the require-\nments-gathering process, then it will probably behoove you to capture their con-\ncerns through interviews. A Quality Attribute Workshop is a structured method to \ndo that and capture their input.\nBuilding a utility tree is a good way to capture ASRs along with their priori-\ntization—something that many requirements processes overlook.\nFinally, you can blend all the methods together: PALM makes an excellent \n“subroutine call” from a Quality Attribute Workshop for the step that asks about \nbusiness goals, and a quality attribute utility tree makes an excellent repository \nfor the scenarios that are the workshop’s output. \nIt is unlikely, however, that your project will have the time and resources to \nsupport this do-it-all approach. Better to pick the approach that fills in the biggest \ngap in your existing requirements: stakeholder representation, business goal man-\nifestation, or ASR prioritization.\n16.6  Summary\nArchitectures are driven by architecturally significant requirements: requirements \nthat will have profound effects on the architecture. Architecturally significant \nrequirements may be captured from requirements documents, by interviewing \nstakeholders, or by conducting a Quality Attribute Workshop.\nIn gathering these requirements, we should be mindful of the business goals \nof the organization. Business goals can be expressed in a common, structured \nform and represented as scenarios. Business goals may be elicited and docu-\nmented using a structured facilitation method called PALM.\nA useful representation of quality attribute requirements is in a utility tree. \nThe utility tree helps to capture these requirements in a structured form, starting \nfrom coarse, abstract notions of quality attributes and gradually refining them to \n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "16.8  Discussion Questions\n309\nthe point where they are captured as scenarios. These scenarios are then priori-\ntized, and this prioritized set defines your “marching orders” as an architect.\n16.7  For Further Reading\nPALM can be used to capture the business goals that conform to a business goal \nviewpoint; that is, you can use PALM to populate a business goal view of your \nsystem, using the terminology of ISO Standard 42010. We discuss this in The \nBusiness Goals Viewpoint [Clements 10c]. Complete details of PALM can be \nfound in CMU/SEI-2010-TN-018, Relating Business Goals to Architecturally \nSignificant Requirements for Software Systems [Clements 10b].\nThe Open Group Architecture Framework, available at www.opengroup.org/\ntogaf/, provides a very complete template for documenting a business scenario \nthat contains a wealth of useful information. Although we believe architects can \nmake use of a lighter-weight means to capture a business goal, it’s worth a look.\nThe definitive reference source for the Quality Attribute Workshop is [Bar-\nbacci 03].\nThe term architecturally significant requirement was created by the Soft-\nware Architecture Review and Assessment (SARA) group [Obbink 02].\nWhen dealing with systems of systems (SoS), the interaction and handoff \nbetween the systems can be a source of problems. The Mission Thread Workshop \nand Business Thread Workshop focus on a single thread of activity within the \noverall SoS context and identify potential problems having to do with the inter-\naction of the disparate systems. Descriptions of these workshops can be found at \n[Klein 10] and [Gagliardi 09].\n16.8  Discussion Questions\n1.\t\nInterview representative stakeholders for your business’s or university’s ex-\npense recovery system. Capture the business goals that are motivating the sys-\ntem. Use the seven-part business goal scenario outline given in Section 16.3.\n2.\t\nDraw a relation between the business goals you uncovered for the previous \nquestion and ASRs.\n3.\t\nConsider an automated teller machine (ATM) system. Attempt to apply the \n11 categories of business goals to that system and infer what goals might \nhave been held by various stakeholders involved in its development.\n",
      "content_length": 2212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "310 \nPart Three  Architecture in the Life Cycle\t\n16—Architecture and Requirements\n4.\t\nCreate a utility tree for the ATM system above. (Interview some of your \nfriends and colleagues if you like, to have them contribute quality attribute \nconsiderations and scenarios.) Consider a minimum of four different quali-\nty attributes. Ensure that the scenarios that you create at the leaf nodes have \nexplicit responses and response measures.\n5.\t\nRestructure the utility tree given in Section 16.4 using stakeholder roles as \nthe organizing principle. What are the benefits and drawbacks of the two \nrepresentations?\n6.\t\nFind a software requirements specification that you consider to be of high \nquality. Using colored pens (real ones if the document is printed, virtual \nones if the document is online), color red all the material that you find com-\npletely irrelevant to a software architecture for that system. Color yellow all \nof the material that you think might be relevant, but not without further dis-\ncussion and elaboration. Color green all of the material that you are certain \nis architecturally significant. When you’re done, every part of the document \nthat’s not white space should be red, yellow, or green. Approximately what \npercentage of each color did your document end up being? Do the results \nsurprise you?\n",
      "content_length": 1325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "311\n17\nDesigning an \nArchitecture\nIn most people’s vocabularies, design means veneer. \nIt’s interior decorating. It’s the fabric of the curtains or \nthe sofa. But to me, nothing could be further from the \nmeaning of design. Design is the fundamental soul of \na human-made creation that ends up expressing itself \nin successive outer layers of the product or service. \n—Steve Jobs\nWe have discussed the building blocks for designing a software architecture, \nwhich principally are locating architecturally significant requirements; capturing \nquality attribute requirements; and choosing, generating, tailoring, and analyzing \ndesign decisions for achieving those requirements. All that’s missing is a way to \npull the pieces together. The purpose of this chapter is to provide that way. \nWe begin by describing our strategy for designing an architecture and then \npresent a packaging of these ideas into a method: the Attribute-Driven Design \nmethod. \n17.1  Design Strategy\nWe present three ideas that are key to architecture design methods: decomposi-\ntion, designing to architecturally significant requirements, and generate and test.\nDecomposition \nArchitecture determines the quality attributes of a system. Hopefully, we have \nconvinced you of that by now. The quality attributes are properties of the system \n",
      "content_length": 1315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "312 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nas a whole. Latency, for example, is the time between the arrival of an event and \nthe output of the processing of that event. Availability refers to the system provid-\ning services, and so forth.\nGiven the fact that quality attributes refer to the system as a whole, if we \nwish to design to achieve quality attribute requirements, we must begin with the \nsystem as a whole. As the design is decomposed, the quality attribute require-\nments can also be decomposed and assigned to the elements of the decomposition.\nA decomposition strategy does not mean that we are assuming the design is \na green-field design or that there are no constraints on the design to use partic-\nular preexisting components either externally developed or legacy. Just as when \nyou choose a route from one point to another, you may choose to stop at various \ndestinations along the route, constraints on the design can be accommodated by \na decomposition strategy. You as the designer must keep in mind the constraints \ngiven to you and arrange the decomposition so that it will accommodate those \nconstraints. In some contexts, the system may end up being constructed mostly \nfrom preexisting components; in others, the preexisting components may be a \nsmaller portion of the overall system. In either case, the goal of the design ac-\ntivity is to generate a design that accommodates the constraints and achieves the \nquality and business goals for the system. \nWe have already talked about module decomposition, but there are other \nkinds of decompositions that one regularly finds in an architecture, such as the \ndecomposition of a component in a components-and-connectors (C&C) pattern \ninto its subcomponents. For example, a user interface implemented using the \nmodel-view-controller (MVC) pattern would be decomposed into a number of \ncomponents for the model, one or more views, and one or more controllers.\nDesigning to Architecturally Significant Requirements\nIn Chapter 16, we discussed architecturally significant requirements (ASRs) and \ngave a technique for collecting and structuring them. These are the requirements \nthat drive the architectural design; that is why they are significant. Driving the \ndesign means that these requirements have a profound effect on the architecture. \nIn other words, you must design to satisfy these requirements. This raises two \nquestions: What happens to the other requirements? and Do I design for one ASR \nat a time or all at once? \n1.\t\nWhat about the non-ASR requirements? The choice of ASRs implies a \nprioritization of the requirements. Once you have produced a design that \nsatisfies the ASRs, you know that you are in good shape. However, in the \nreal world, there are other requirements that, while not ASRs, you would \nlike to be able to satisfy. You have three options with respect to meeting \nthese other requirements: (a) You can still meet the other requirements. \n(b) You can meet the other requirements with a slight adjustment of the \nexisting design, and this slight adjustment does not keep the higher priority \n",
      "content_length": 3137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "17.1  Design Strategy\n313\nrequirements from being met. (c) You cannot meet the other requirements \nunder the current design. In case (a) or (b), there is nothing more to be \ndone. You are happy. In case (c), you now have three options: (i) If you are \nclose to meeting the requirement, you can see if the requirement can be \nrelaxed. (ii) You can reprioritize the requirements and revisit the design. \n(iii) You can report that you cannot meet the requirement. All of these \nlatter three options involve adjusting either the requirement or its priority. \nDoing so may have a business impact, and it should be reported up the \nmanagement chain.\n2.\t\nDesign for all of the ASRs or one at a time? The answer to this question is a \nmatter of experience. When you learn chess, you begin by learning that the \nhorsey goes up two and over one. After you have been playing for a while, \nyou internalize the moves of the knight and you can begin to look further \nahead. The best players may look ahead a dozen or more moves. This situ-\nation applies to when you are designing to satisfy ASRs. Left to their own \ndevices, novice architects will likely focus on one ASR at a time. But you \ncan do better than that. Eventually, through experience and education, you \nwill develop an intuition for designing, and you will employ patterns to aid \nyou in designing for multiple ASRs.\nGenerate and Test\nOne way of viewing design is as a process of “generate and test.” This gener-\nate-and-test approach views a particular design as a hypothesis: namely, the de-\nsign satisfies the requirements. Testing is the process of determining whether the \ndesign hypothesis is correct. If it is not, then another design hypothesis must be \ngenerated. Figure 17.1 shows this iteration.\nGenerate \nInitial \nHypothesis \nTest \nHypothesis \nGenerate \nHypothesis \nFigure 17.1  The generate-and-test process of architecture design\n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "314 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nFor this process to be effective, the generation of the next design hypothesis \nmust build on the results of the tests. That is, the things wrong with the current \ndesign hypothesis are fixed in the next design hypothesis, and the things that are \nright are kept. If there is no coupling between the testing and the generation of \nthe next design hypothesis, then this process becomes “guess and test” and that \nis not effective.\nGenerate and test as a design strategy leads to the following questions:\n1.\t\nWhere does the initial hypothesis come from?\n2.\t\nWhat are the tests that are applied?\n3.\t\nHow is the next hypothesis generated?\n4.\t\nWhen are you done?\nWe have already seen many of the elements of the answers to these ques-\ntions. But now we can think about them and organize them more systematically.\nCreating the Initial Hypothesis\nDesign solutions are created using “collateral” that is available to the project. \nCollateral can include existing systems, frameworks available to the project, \nknown architecture patterns, design checklists, or a domain decomposition. \n■\n■Existing systems. Very few systems are completely unprecedented, even \nwithin a single organization. Organizations are in a particular business, their \nbusiness leads to specialization, and specialization leads to the development \nof variations on a theme. It is likely that systems already exist that are \nsimilar to the system being constructed in your company. \nExisting systems are likely to provide the most powerful collateral, \nbecause the business context and requirements for the existing system are \nlikely to be similar to the business context and requirements for the new \nsystem, and many of the problems that occur have already been solved in \nthe existing design.\nA common special case is when the existing system you’re drawing \non for knowledge is the same one that you’re building. This occurs when \nyou’re evolving a system, not building one from scratch. The existing \ndesign serves as the initial design hypothesis. The “test” part of this pro-\ncess will reveal the parts that don’t work under the current (presumably \nchanged) set of requirements and will therefore pinpoint the parts of the \nsystem’s design that need to change.\nAnother special case is when you have to combine existing legacy sys-\ntems into a single system. In this case, the collection of legacy systems can \nbe mined to determine the initial design hypothesis.\n■\n■Frameworks. A framework is a partial design (accompanied by code) that \nprovides services that are common in particular domains. Frameworks exist \nin a great many domains, ranging from web applications to middleware sys-\ntems to decision support systems. The design of the framework (especially \n",
      "content_length": 2812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "17.1  Design Strategy\n315\nthe architectural assumptions it makes) provides the initial design hypoth-\nesis. For example, a design framework might constrain all communication \nto be via a broker, or via a publish-subscribe bus, or via callbacks. In each \ncase this design framework has constrained your initial design hypothesis.\n■\n■Patterns and tactics. As we discussed in Chapter 13, a pattern is a known \nsolution to a common problem in a given context. Cataloged architectural \npatterns, possibly augmented with tactics, should be considered as candi-\ndates for the design hypothesis you’re building. \n■\n■Domain decomposition. Another option for the initial design hypothesis \ncomes from performing a domain decomposition. For example, most ob-\nject-oriented analysis and design processes begin this way, identifying ac-\ntors and entities in the domain. This decomposition will divide the respon-\nsibilities to make certain modifications easier, but by itself it does not speak \nto many other quality attribute requirements.\n■\n■Design checklists. The design checklists that we presented in Chapters 5–11 \ncan guide an architect to making quality-attribute-targeted design choices. \nThe point of using a checklist is to ensure completeness: Have I thought \nabout all of the issues that might arise with respect to the many quality \nattribute concerns that I have? The checklist will provide guidance and \nconfidence to an architect. \nChoosing the Tests\nThree sources provide the tests to be applied to the hypothesis:\n1.\t\nThe analysis techniques described in Chapter 14.\n2.\t\nThe design checklists for the quality attributes that we presented in Chap-\nters 5–11 can also be used to test the design decisions already made, from \nthe sources listed above. For the important quality attribute requirements, \nuse the design checklists to assess whether the decisions you’ve made so \nfar are sound and complete. For example, if testability is important for your \nsystem, the checklist says to ensure that the coordination model supports \ncapturing the activity that led to a fault. \n3.\t\nThe architecturally significant requirements. If the hypothesis does not \nprovide a solution for the ASRs, then it must be improved.\nGenerating the Next Hypothesis\nAfter applying the tests, you might be done—everything looks good. On the \nother hand, you might still have some concerns; specifically, you might have a list \nof quality attribute problems associated with your analysis of the current hypoth-\nesis. This is the problem that tactics are intended to solve: to improve a design \nwith respect to a particular quality attribute. Use the sets of tactics described in \neach of Chapters 5–11 to help you to choose the ones that will improve your de-\nsign so that you can satisfy these outstanding quality attribute requirements.\n",
      "content_length": 2818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "316 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nTerminating the Process\nYou are done with the generate-and-test process when you either have a design \nthat satisfies the ASRs or when you exhaust your budget for producing the de-\nsign. In Chapter 22, we discuss how much time should be budgeted for producing \nthe architecture. \nIf you do not produce such a design within budget, then you have two op-\ntions depending on the set of ASRs that are satisfied. Your first option is to pro-\nceed to implementation with the best hypothesis you were able to produce, with \nthe realization that some ASRs may not be met and may need to be relaxed or \neliminated. This is the most common case. Your second option is to argue for \nmore budget for design and analysis, potentially revisiting some of the major \nearly design decisions and resuming generate and test from that point. If all else \nfails, you could suggest that the project be terminated. If all of the ASRs are crit-\nical and you were not able to produce an acceptable or nearly acceptable design, \nthen the system you produce from the design will not be satisfactory and there is \nno sense in producing it.\n17.2  The Attribute-Driven Design Method\nThe Attribute-Driven Design (ADD) method is a packaging of the strategies that \nwe have just discussed. ADD is an iterative method that, at each iteration, helps \nthe architect to do the following:\n■\n■Choose a part of the system to design.\n■\n■Marshal all the architecturally significant requirements for that part.\n■\n■Create and test a design for that part.\nThe output of ADD is not an architecture complete in every detail, but an \narchitecture in which the main design approaches have been selected and vetted. \nIt produces a “workable” architecture early and quickly, one that can be given to \nother project teams so they can begin their work while the architect or architec-\nture team continues to elaborate and refine.\nInputs to ADD\nBefore beginning a design process, the requirements—functional, quality, and \nconstraints—should be known. In reality, waiting for all of the requirements to \nbe known means the project will never be finished, because requirements are con-\ntinually arriving to a project as a result of increased knowledge on the part of the \nstakeholders and changes in the environment (technical, social, legal, financial, \nor political) over time. ADD can begin when a set of architecturally significant \nrequirements is known. \n",
      "content_length": 2484,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "17.2  The Attribute-Driven Design Method\n317\nThis increases the importance of having the correct set of ASRs. If the set \nof ASRs changes after design has begun, then the design may well need to be \nreworked (a truth under any design method, not just ADD). To the extent that you \nhave any influence over the requirements-gathering process, it would behoove \nyou to lobby for collection of ASRs first. Although these can’t all be known a \npriori, as we saw in Chapter 16, quality attribute requirements are a good start.\nIn addition to the ASRs, input to ADD should include a context description. \nThe context description gives you two vital pieces of information as a designer:\n1.\t\nWhat are the boundaries of the system being designed? What is inside the \nsystem and what is outside the system must be known in order to constrain \nthe problem and establish the scope of the architecture you are designing. \nThe system’s scope is unknown or unclear surprisingly often, and it will \nhelp the architecture to nail down the scope as soon as you can. \n2.\t\nWhat are the external systems, devices, users, and environmental conditions \nwith which the system being designed must interact? By “environmental con-\nditions” here we are referring to the system’s runtime environment. The sys-\ntem’s environmental conditions are an enumeration of factors such as where \nthe input comes from, where the output goes, what forms they take, what \nquality attributes they have, and what forces may affect the operation of the \nsystem. It is possible that not all of the external systems are known at design \ntime. In this case, the system must have some discovery mechanisms, but the \ncontext description should enumerate the assumptions that can be made about \nthe external systems even if their specifics are not yet known. An example of \naccommodating environment conditions can be seen in a system that must \nbe sent into space. In addition to handling its inputs, outputs, and quality \nattributes, such a system must accommodate failures caused by stray gamma \nrays, certainly a force affecting the operation of the system. \nOutput of ADD\nThe output of ADD is a set of sketches of architectural views. The views together \nwill identify a collection of architectural elements and their relationships or in-\nteractions. One of the views produced will be a module decomposition view, and \nin that view each element will have an enumeration of its responsibilities listed. \nOther views will be produced according to the design solutions chosen \nalong the way. For example, if at one point in executing the method, you choose \nthe service-oriented architecture (SOA) pattern for part of the system, then you \nwill capture this in an SOA view (whose scope is that part of the system to which \nyou applied the pattern). \nThe interactions of the elements are described in terms of the information \nbeing passed between the elements. For example, we might specify protocol \nnames, synchronous, asynchronous, level of encryption, and so forth.\n",
      "content_length": 3021,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "318 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nThe reason we refer to “sketches” above is that ADD does not take the \ndesign so far as to include full-blown interface specifications, or even so far as \nchoosing the names and parameter types of interface programs (methods). That \ncan come later. ADD does identify the information that passes through the inter-\nfaces and important characteristics of the information. If any aspects of an inter-\nface have quality attribute implications, those are captured as annotations. \nWhen the method reaches the end, you will have a full-fledged architecture \nthat is roughly documented as a set of views. You can then polish this collection, \nperhaps merging some of the views as appropriate, to the extent required by your \nproject. In an Agile project, this set of rough sketches may be all you need for \nquite a while, or for the life of the project.\n17.3  The Steps of ADD\nADD is a five-step method:\n1.\t\nChoose an element of the system to design.\n2.\t\nIdentify the ASRs for the chosen element.\n3.\t\nGenerate a design solution for the chosen element.\n4.\t\nInventory remaining requirements and select the input for the next iteration.\n5.\t\nRepeat steps 1–4 until all the ASRs have been satisfied.\nStep 1: Choose an Element of the System to Design\nADD works by beginning with a part of the system that has not yet been de-\nsigned, and designing it. In this section, we’ll discuss how to make that choice.\nFor green-field designs, the “element” to begin with is simply the entire \nsystem. The first trip through the ADD steps will yield a broad, shallow design \nthat will produce a set of newly identified architectural elements and their inter-\nactions. These elements will almost certainly require more design decisions to \nflesh out what they do and how they satisfy the ASRs allocated to them; during \nthe next iteration of ADD, those elements become candidates for the “choose an \nelement” step.\nSo, nominally, the first iteration of ADD will create a collection of elements \nthat together constitute the entire system. The second iteration will take one of \nthese elements—what we call the “chosen element”—and design it, resulting in \nstill finer-grained elements. The third iteration will take another element—either \none of the children of the whole system or one of the children that was created \nfrom the design of one of the children of the whole system—and so forth. For ex-\nample, if you choose an SOA pattern in the first iteration, you might choose child \nelements such as service clients, service providers, and the SOA infrastructure \n",
      "content_length": 2620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "17.3  The Steps of ADD\n319\ncomponents. In the next iteration through the loop, you would refine one of these \nchild elements, perhaps the infrastructure components. In the next iteration you \nnow have a choice: refine another child of the SOA pattern, such as a service pro-\nvider, or refine one of the child elements of the infrastructure components. Figure \n17.2 shows these choices as a decomposition tree, annotated with the ADD itera-\ntion that applies to each node. (The example components are loosely based on the \nAdventure Builder system, introduced in Chapter 13.) Figure 17.2 is a decompo-\nsition view of our hypothetical system after two iterations of ADD.\nThere are cases when the first iteration of ADD is different. Perhaps you are \nnot creating a system but evolving an existing one. Perhaps you are required to \nuse a piece of software that your company already owns, and therefore must fit it \ninto the design. There are many reasons why some of the design might already be \ndone for you, and the first time through the steps of ADD you won’t pick “whole \nsystem” as the starting point. Nevertheless, step 1 still holds: All it requires is that \nat least one of the elements you know about needs further design.\nThere are two main refinement strategies to pursue with ADD: breadth first \nand depth first. Breadth first means that all of the second-level elements are de-\nsigned before any of the third-level elements, and so forth. Depth first means that \none downward chain is completed before beginning a second downward chain. \nThe order that you should work through ADD is influenced by the business and \ntechnical contexts within which the project is operating. Some of the important \nfactors include the following:\nIteration #2: \nSOA infrastructure \ncomponents refined\nOr an SOA infrastructure component?\nFigure 17.2  Iteration 1 applied the SOA pattern. Iteration 2 refined the \ninfrastructure components. Where will iteration 3 take you?\n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "320 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\n■\n■Personnel availability may dictate a refinement strategy. If an important \ngroup or team has a window of availability that will close soon and will \nwork on a particular part of the system, then it behooves the architect to \ndesign that part of the system to the point where it can be handed off for \nimplementation—depth first. But if the team is not currently available but \nwill be available at some definite time in the future, then you can defer their \npart of the design until later. \n■\n■Risk mitigation may dictate a refinement strategy. The idea is to design the \nrisky parts of the system to enough depth so that problems can be identified \nand solved early. For example, if an unfamiliar technology is being intro-\nduced on the project, prototypes using that technology will likely be devel-\noped to gain understanding of its implications. These prototypes are most \nuseful if they reflect the design of the actual system. A depth-first strategy \ncan provide a context for technology prototyping. Using this context you \ncan build the prototype in a fashion that allows for its eventual integration \ninto the architecture. On the other hand, if the risk is in how elements at the \nsame level of the design interact with each other to meet critical quality at-\ntributes, then a breadth-first strategy is in order.\n■\n■Deferral of some functionality or quality attribute concerns may dictate a \nmixed approach. For example, suppose the system being constructed has \na medium-priority availability requirement. In this case you might adopt \na strategy of employing redundancy for availability but defer detailed \nconsideration of this redundancy strategy to allow for the rapid generation \nof the high-priority functionality in an intermediate release. You might \ntherefore apply a breadth-first approach for everything but availability, \nand then in subsequent design iterations you revisit some of the elements \nto enable the addition of the responsibilities to support availability. In \nreality this approach will require some backtracking, where you revisit \nearlier decisions and refine them or modify them to accommodate this new \nrequirement.\nAll else being equal, a breadth-first refinement strategy is preferred because \nit allows you to apportion the most work to the most teams soonest. Breadth first \nallows for consideration of the interaction among the elements at the same level.\nStep 2: Identify the ASRs for This Element\nIn Chapter 16 we described a number of methods for discovering the ASRs for \na system. One of those methods involved building a utility tree. To support the \ndesign process, the utility tree has an advantage over the other methods: it guides \nthe stakeholders in prioritizing the QA requirements. The two factors used to pri-\noritize the ASRs in a utility tree are business value and architectural impact. The \nbusiness value of an ASR typically will not change throughout the design process \nand does not need to be reconsidered. \n",
      "content_length": 3057,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "17.3  The Steps of ADD\n321\nIf the chosen element for design in step 1 is the whole system, then a utility \ntree can be a good source for the ASRs. Otherwise, construct a utility tree spe-\ncifically focused on this chosen element, using the quality attribute requirements \nthat apply to this element (you’ll see how to assign those in step 4). Those that \nare labeled (High, High) are the ASRs for this element. As an architect you will \nalso need to pay attention to the (High, Medium) and (Medium, High) utility tree \nleaves as well. These will almost certainly also be ASRs for this element.\nStep 3: Generate a Design Solution for the Chosen Element\nThis step is the heart of the ADD. It is the application of the generate-and-test \nstrategy. Upon entry to this step, we have a chosen element for design and a list \nof ASRs that apply to it. For each ASR, we develop a solution by choosing a can-\ndidate design approach. \nYour initial candidate design will likely be inspired by a pattern, possibly \naugmented by one or more tactics. You may then refine this candidate design by \nconsidering the design checklists that we gave for the quality attributes in Chap-\nters 5–11. For ASRs that correspond to quality attributes, you can invoke those \nchecklists to help you instantiate or refine the major design approach (such as a \npattern) that you’ve chosen. For example, the layered pattern is helpful for build-\ning systems in which modifiability is important, but the pattern does not tell you \nhow many layers you should have or what each one’s responsibility should be. \nBut the checklist for the “allocation of responsibilities” design decision category \nfor modifiability in Chapter 7 will help you ask the right questions to make that \ndetermination.\nAlthough this step is performed for each ASR in turn, the sources of de-\nsign candidates outlined above—patterns, tactics, and checklists—will usually do \nmuch better than that. That is, you’re likely to find design candidates that address \nseveral of your ASRs at once. This is because to the extent that the system you’re \nbuilding is similar to others you know about, or to the extent that the problem you \nare solving is similar to the problems solved by patterns, it is likely that the solu-\ntions you choose will be solving a whole collection of ASRs simultaneously. If \nyou can bring a solution to bear that solves more than one of your ASRs at once, \nso much the better.\nThe design decisions made in this step now become constraints on all future \nsteps of the method.\nStep 4: Verify and Refine Requirements and \nGenerate Input for the Next Iteration\nIt’s possible that the design solution you came up with in the prior step won’t \nsatisfy all the ASRs. Step 4 of ADD is a test step that is applied to your design for \nthe element you chose to elaborate in step 1 of this iteration. One of the possible \n",
      "content_length": 2870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "322 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\noutcomes of step 4 is “backtrack,” meaning that an important requirement was \nnot satisfied and cannot be satisfied by further elaborating this design. In this \ncase, the design needs to be reconsidered.\nThe ASRs you have not yet satisfied could be related to the following:\n1.\t\nA quality attribute requirement allocated to the parent element\n2.\t\nA functional responsibility of the parent element\n3.\t\nOne or more constraints on the parent element\nTable 17.1 summarizes the types of problems and the actions we recom-\nmend for each.\nIn most real-world systems, requirements outstrip available time and re-\nsources. Consequently you will find yourself unable to meet some of the QA \nrequirements, functional requirements, and constraints. These kinds of decisions \nare outside the scope of the ADD method, but they are clearly important drivers \nof the design process, and as an architect you will be continually negotiating de-\ncisions of this form.\nStep 4 is about taking stock and seeing what requirements are left that still \nhave not been satisfied by our design so far. At this point you should sequence \nthrough the quality attribute requirements, responsibilities, and constraints for the \nelement just designed. For each one there are four possibilities:\nTable 17.1  Recommended Actions for Problems with the Current Hypothesis\nType of ASR Not Met\nAction Recommended\n1. Quality attribute requirement\nConsider applying (more) tactics to improve the \ndesign with respect to the quality attribute. For \neach candidate tactic, ask: \n■\n■\nWill this tactic improve the quality attribute \nbehavior of the current design sufficiently?\n■\n■\nShould this tactic be used in conjunction with \nanother tactic?\n■\n■\nWhat are the tradeoff considerations when \napplying this tactic?\n2. Functional responsibility\nAdd responsibilities either to existing modules or to \nnewly created modules:\n■\n■\nAssign the responsibility to a module containing \nsimilar responsibilities.\n■\n■\nBreak a module into portions when it is too \ncomplex.\n■\n■\nAssign the responsibility to a module containing \nresponsibilities with similar quality attribute \ncharacteristics—for example, similar timing \nbehavior, similar security requirements, or \nsimilar availability requirements. \n3. Constraint\nModify the design or try to relax the constraint:\n■\n■\nModify the design to accommodate the \nconstraint. \n■\n■\nRelax the constraint.\n",
      "content_length": 2467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "17.3  The Steps of ADD\n323\n1.\t\nThe quality attribute requirement, functional requirement, or constraint \nhas been satisfied. In this case, the design with respect to that requirement \nis complete; the next time around, when you further refine the design, \nthis requirement will not be considered. For example, if a constraint is to \nuse a particular middleware and the system is decomposed into elements \nthat all use this middleware, the constraint has been satisfied and can be \nremoved from consideration. An example of a quality attribute requirement \nbeing satisfied is a requirement to make it easy to modify elements and \ntheir interactions. If a publish-subscribe pattern can be shown to have been \nemployed throughout the system, then this QA requirement can be said to \nbe satisfied. \n2.\t\nThe quality attribute requirement, functional requirement, or constraint \nis delegated to one of the children. For example, if a constraint is to use a \nparticular middleware and the decomposition has a child element that acts \nas the infrastructure, then delegating that constraint to that child will retain \nthe constraint and have it be reconsidered when the infrastructure element \nis chosen for subsequent design. Similarly, with the example we gave ear-\nlier about providing extensibility, if there is as yet no identifiable plug-in \nmanager, then this requirement is delegated to the child where the plug-in \nmanager is likely to appear.\n3.\t\nThe quality attribute requirement, functional requirement, or constraint is \ndistributed among the children. For example, a constraint might be to use \n.NET. In this case, .NET Remoting might become a constraint on one child \nand ASP.NET on another. Or a quality attribute requirement that constrains \nend-to-end latency of a certain operation to 2 seconds could be distributed \namong the element’s three children so that the latency requirement for one \nelement is 0.8 seconds, the latency for a second element is 0.9 seconds, and \nthe latency for a third is 0.3 seconds. When those elements are subsequent-\nly chosen for further design, those times will serve as constraints on them \nindividually.\n4.\t\nThe quality attribute requirement, functional requirement, or constraint \ncannot be satisfied with the current design. In this case there are the same \ntwo options we discussed previously: you can either backtrack—revisit the \ndesign to see if the constraint or quality attribute requirement can be sat-\nisfied some other way—or push back on the requirement. This will almost \ncertainly involve the stakeholders who care about that requirement, and you \nshould have convincing arguments as to why the dropping of the require-\nment is necessary.\nReport to the project manager that the constraint cannot be satisfied \nwithout jeopardizing other requirements. You must be prepared to justify \nsuch an assertion. Essentially, this is asking, “What’s more important—the \nconstraint or these other requirements?”\n",
      "content_length": 2959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "324 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\nStep 5: Repeat Steps 1–4 Until Done\nAfter the prior steps, each element has a set of responsibilities, a set of quality \nattribute requirements, and a set of constraints assigned to it. If it’s clear that all \nof the requirements are satisfied, then this unequivocally ends the ADD process.\nIn projects in which there is a high degree of trust between you and the im-\nplementation teams, the ADD process can be terminated when only a sketch of \nthe architecture is available. This could be as soon as two levels of breadth-first \ndesign, depending on the size of the system. In this case, you trust the implemen-\ntation team to be able to flesh out the architecture design in a manner consistent \nwith the overall design approaches you have laid out. The test for this is if you \nbelieve that you could begin implementation with the level of detail available and \ntrust the implementation team to that extent. If you have less trust in the imple-\nmentation team, then an additional level (or levels) of design may be necessary. \n(And, of course, you will need to subsequently ensure that the implementation is \nfaithfully followed by the team.)\nOn the other hand, if there is a contractual arrangement between your or-\nganization and the implementation organization, then the specification of the \nportion of the system that the implementers are providing must be legally en-\nforceable. This means that the ADD process must continue until that level of \nspecificity has been achieved. \nFinally, another condition for terminating ADD is when the project’s design \nbudget has been exhausted. This happens more often than you might think. \nChoosing when to terminate ADD and when to start releasing the architec-\nture that you’ve sketched out are not the same decision. You can, and in many \ncases should, start releasing early architectural views based on the needs of the \nproject (such as scheduled design reviews or customer presentations) and your \nconfidence in the design so far. The unpalatable alternative is to make everyone \nwait until the architecture design is finished. You-can’t-have-it-until-it’s-done is \nparticularly unpalatable in Agile projects, as we discussed in Chapter 15. \nYou should release the documentation with a caveat as to how likely you \nthink it is to change. But even early broad-and-shallow architectural descriptions \ncan be enormously helpful to implementers and other project staff. A first- or sec-\nond-level module decomposition view, for instance, lets experts start scouring the \nmarketplace for commercial products that provide the responsibilities of the iden-\ntified modules. Managers can start making budgets and schedules for implemen-\ntation that are based on the architecture and not just the requirements. Support \nstaff can start building the infrastructure and file systems to hold project artifacts \n(these are often structured to mirror the module decomposition view). And early \nrelease invites early feedback.\n",
      "content_length": 3042,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "17.5  For Further Reading\n325\n17.4  Summary\nThe Attribute-Driven Design method is an application of the generate-and-test \nphilosophy. It keeps the number of requirements that must be satisfied to a hu-\nmanly achievable quantity. ADD is an iterative method that, at each iteration, \nhelps the architect to do the following:\n■\n■Choose an element of the system to design.\n■\n■Marshal all the architecturally significant requirements for the chosen \nelement.\n■\n■Create and test a design for that chosen element.\nThe output of ADD is not an architecture complete in every detail, but an ar-\nchitecture in which the main design approaches have been selected and validated. \nIt produces a “workable” architecture early and quickly, one that can be given to \nother project teams so they can begin their work while the architect or architec-\nture team continues to elaborate and refine.\nADD is a five-step method:\n1.\t\nChoose the element of the system to design. For green-field designs, the \n“part” to begin with is simply the entire system. For designs that are already \npartially completed (either by external constraints or by previous iterations \nthrough ADD), the part is an element that is not yet designed. Choosing the \nnext element can proceed in a breadth-first, depth-first, or mixed manner.\n2.\t\nIdentify the ASRs for the chosen element.\n3.\t\nGenerate a design solution for the chosen element, using design collateral \nsuch as existing systems, frameworks, patterns and tactics, and the design \nchecklists from Chapters 5–11.\n4.\t\nVerify and refine requirements and generate input for the next iteration. \nEither the design in step 3 will satisfy all of the chosen element’s ASRs or \nit won’t. If it doesn’t, then either they can be allocated to elements that will \nbe elaborated in future iterations of ADD, or the existing design is inad-\nequate and we must backtrack. Furthermore, non-ASR requirements will \neither be satisfied, allocated to children, or indicated as not achievable.\n5.\t\nRepeat steps 1–4 until all the ASRs have been satisfied, or until the archi-\ntecture has been elaborated sufficiently for the implementers to use it. \n17.5  For Further Reading\nYou can view design as the process of making decisions; this is another philoso-\nphy of design. This view of design leads to an emphasis on design rationale and \ntools to capture design rationale. The view of design as the process of making \n",
      "content_length": 2410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "326 \nPart Three  Architecture in the Life Cycle\t\n17—Designing an Architecture\ndecisions dates to the 1940s [Mettler 91], but it has been recently applied to ar-\nchitecture design most prominently by Philippe Kruchten [Kruchten 04], and \nHans van Vliet and Jan Bosch [van Vliet 05].\nThe Software Engineering Institute has produced a number of reports de-\nscribing the ADD method and its application in a variety of contexts. These in-\nclude [Wojcik 06], [Kazman 04], and [Wood 07].\nGeorge Fairbanks has written an engaging book that describes a risk-driven \nprocess of architecture design, entitled Just Enough Software Architecture: A \nRisk-Driven Approach [Fairbanks 10].\nTony Lattanze has created an Architecture-Centric Design Method (ACDM), \ndescribed in his book Architecting Software Intensive Systems: A Practitioners \nGuide [Lattanze 08].\nIan Gorton’s Essential Architecture, Second Edition, emphasizes the middle-\nware aspects of a design [Gorton 10].\nWoods and Rozanski have written Software Systems Architecture, Second \nEdition, which interprets the design process through the prism of different views \n[Woods 11].\nA number of authors have compared five different industrial architecture de-\nsign methods. You can find this comparison at [Hofmeister 07].\nRaghvinder Sangwan and his colleagues describe the design of a building \nmanagement system that was originally designed using object-oriented tech-\nniques and then was redesigned using ADD [Sangwan 08].\n17.6  Discussion Questions\n1.\t\nADD does not help with the detailed design of interfaces for the \narchitectural elements it identifies. Details of an interface include what each \nmethod does, whether you need to call a single all-encompassing method \nto perform the work of the element or many methods of finer-grained \nfunction, what exceptions are raised on the interface, and more. What are \nsome examples where the specific design of an interface might bring more \nor less performance, security, or availability to a system? (By the way, if \nthere are quality attribute implications to an interface, you can capture those \nas annotations on the element.)\n2.\t\nWhat sets a constraint apart from other (even high-priority) requirements is \nthat it is not negotiable. Should this consideration guide the design process? \nFor example, would it be wise to design to satisfy all of the constraints be-\nfore worrying about other ASRs?\n3.\t\nIn discussion question 4 of Chapter 16 you were asked to create a utility \ntree for an ATM. Now choose the two most important ASRs from that util-\nity tree and create a design fragment using the ADD method employing and \ninstantiating a pattern.\n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "327\n18\nDocumenting Software \nArchitectures\nIf it is not written down, it does not exist.\n—Philippe Kruchten\nEven the best architecture, the most perfectly suited for the job, will be essen-\ntially useless if the people who need to use it do not know what it is; cannot un-\nderstand it well enough to use, build, or modify it; or (worst of all) misunderstand \nit and apply it incorrectly. And all of the effort, analysis, hard work, and insight-\nful design on the part of the architecture team will have been wasted. They might \nas well have gone on vacation for all the good their architecture will do.\nCreating an architecture isn’t enough. It has to be communicated in a way \nto let its stakeholders use it properly to do their jobs. If you go to the trouble of \ncreating a strong architecture, one that you expect to stand the test of time, then \nyou must go to the trouble of describing it in enough detail, without ambiguity, \nand organizing it so that others can quickly find and update needed information. \nDocumentation speaks for the architect. It speaks for the architect today, \nwhen the architect should be doing other things besides answering a hundred \nquestions about the architecture. And it speaks for the architect tomorrow, when \nhe or she has left the project and now someone else is in charge of its evolution \nand maintenance.\nThe sad truth is that architectural documentation today, if it is done at all, \nis often treated as an afterthought, something people do because they have to. \nMaybe a contract requires it. Maybe a customer demands it. Maybe a compa-\nny’s standard process calls for it. In fact, these may all be legitimate reasons. \nBut none of them are compelling enough to produce high-quality documentation. \nWhy should the architect spend valuable time and energy just so a manager can \ncheck off a deliverable?\n",
      "content_length": 1849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "328  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe best architects produce good documentation not because it’s “required” \nbut because they see that it is essential to the matter at hand—producing a \nhigh-quality product, predictably and with as little rework as possible. They see \ntheir immediate stakeholders as the people most intimately involved in this un-\ndertaking: developers, deployers, testers, and analysts. \nBut architects also see documentation as delivering value to themselves. \nDocumentation serves as the receptacle to hold the results of major design deci-\nsions as they are confirmed. A well-thought-out documentation scheme can make \nthe process of design go much more smoothly and systematically. Documenta-\ntion helps the architect(s) reason about the architecture design and communicate \nit while the architecting is in progress, whether in a six-month design phase or a \nsix-day Agile sprint.\n18.1  \u0007Uses and Audiences for \nArchitecture Documentation\nArchitecture documentation must serve varied purposes. It should be sufficiently \ntransparent and accessible to be quickly understood by new employees. It should \nbe sufficiently concrete to serve as a blueprint for construction. It should have \nenough information to serve as a basis for analysis. \nArchitecture documentation is both prescriptive and descriptive. For some \naudiences, it prescribes what should be true, placing constraints on decisions yet \nto be made. For other audiences, it describes what is true, recounting decisions \nalready made about a system’s design.\nThe best architecture documentation for, say, performance analysis may well \nbe different from the best architecture documentation we would wish to hand to \nan implementer. And both of these will be different from what we put in a new \nhire’s “welcome aboard” package or a briefing we put together for an executive. \nWhen planning and reviewing documentation, you need to ensure support for all \nthe relevant needs.\nWe can see that many different kinds of people are going to have a vested \ninterest in an architecture document. They hope and expect that the architecture \ndocument will help them do their respective jobs. Understanding their uses of \narchitecture documentation is essential, as those uses determine the important \ninformation to capture. \nFundamentally, architecture documentation has three uses:\n1.\t\nArchitecture documentation serves as a means of education. The education-\nal use consists of introducing people to the system. The people may be new \nmembers of the team, external analysts, or even a new architect. In many \ncases, the “new” person is the customer to whom you’re showing your \n",
      "content_length": 2707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "18.2  Notations for Architecture Documentation\n329\nsolution for the first time, a presentation you hope will result in funding or \ngo-ahead approval.\n2.\t\nArchitecture documentation serves as a primary vehicle for communication \namong stakeholders. An architecture’s precise use as a communication ve-\nhicle depends on which stakeholders are doing the communicating. \nPerhaps one of the most avid consumers of architecture documentation \nis none other than the architect in the project’s future. The future architect \nmay be the same person or may be a replacement, but in either case he or \nshe is guaranteed to have an enormous stake in the documentation. New \narchitects are interested in learning how their predecessors tackled the dif-\nficult issues of the system and why particular decisions were made. Even if \nthe future architect is the same person, he or she will use the documentation \nas a repository of thought, a storehouse of design decisions too numerous \nand hopelessly intertwined to ever be reproducible from memory alone. See \nthe sidebar “Schmucks and Jerks.”\n3.\t\nArchitecture documentation serves as the basis for system analysis and con-\nstruction. Architecture tells implementers what to implement. Each module \nhas interfaces that must be provided and uses interfaces from other mod-\nules. Not only does this provide instructions about the provided and used \ninterfaces, but it also determines with what other teams the development \nteam for the module must communicate.\nDuring development, an architecture can be very complex, with many is-\nsues left to resolve. Documentation can serve as a receptacle for registering \nand communicating these issues that might otherwise be overlooked.\nFor those interested in the ability of the design to meet the system’s \nquality objectives, the architecture documentation serves as the fodder for \nevaluation. It must contain the information necessary to evaluate a vari-\nety of attributes, such as security, performance, usability, availability, and \nmodifiability.\nFor system builders who use automatic code-generation tools, the doc-\numentation may incorporate the models used for generation. These models \nprovide guidance to those who wish to understand the behavior of the mod-\nule in more detail than is normally documented but in less detail than exam-\nining the code would provide.\n18.2  Notations for Architecture Documentation\nNotations for documenting views differ considerably in their degree of formality. \nRoughly speaking, there are three main categories of notation:\n",
      "content_length": 2547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "330  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n■\n■Informal notations. Views are depicted (often graphically) using gener-\nal-purpose diagramming and editing tools and visual conventions chosen \nfor the system at hand. The semantics of the description are characterized in \nnatural language, and they cannot be formally analyzed. In our experience, \nthe most common tool for informal notations is PowerPoint.\n■\n■Semiformal notations. Views are expressed in a standardized notation that \nprescribes graphical elements and rules of construction, but it does not \nprovide a complete semantic treatment of the meaning of those elements. \nRudimentary analysis can be applied to determine if a description satisfies \nsyntactic properties. UML is a semiformal notation in this sense.\n■\n■Formal notations. Views are described in a notation that has a precise (usu-\nally mathematically based) semantics. Formal analysis of both syntax and \nsemantics is possible. There are a variety of formal notations for software \narchitecture available. Generally referred to as architecture description lan-\nguages (ADLs), they typically provide both a graphical vocabulary and an \nunderlying semantics for architecture representation. In some cases these \nnotations are specialized to particular architectural views. In others they allow \nmany views, or even provide the ability to formally define new views. The \nusefulness of ADLs lies in their ability to support automation through associ-\nated tools: automation to provide useful analysis of the architecture or assist \nin code generation. In practice, the use of such notations is rare.\nSchmucks and Jerks\nOne day I was sitting in a meeting with a well-known compiler guru. He \nwas recounting some of his favorite war stories from his long career. One of \nthese stories particularly stuck with me. He was talking about the time that \nhe was chasing down a very nasty and subtle bug in the code of a compiler \nthat he was maintaining. After a long and exasperating search, he finally \nlocated and eventually fixed the bug. But the search itself had gotten him \nso worked up, and he was so infuriated at the irresponsible thought and \nprogramming that led to the bug, that he decided to do a bit more detective \nwork and figure out who was the jerk responsible for that bug. \nBy going backward through the revision history, he found the culprit. It \nwas him. He was the jerk. It turns out that he was the one who—eight years \nearlier—had written that offending piece of code. The trouble was, he had \nno recollection of writing the code and no recollection of the rationale for \nwriting it the way he had done. Perhaps there was a good reason to do so \nat the time, but if so it was lost now. \nThat is why we document. The documentation helps the poor schmuck \nwho has to maintain your code in the future, and that schmuck might very \nwell be you!\n—RK\n",
      "content_length": 2924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "18.3  Views\n331\nDetermining which form of notation to use involves making several \ntradeoffs. Typically, more formal notations take more time and effort to create \nand understand, but they repay this effort in reduced ambiguity and more oppor-\ntunities for analysis. Conversely, more informal notations are easier to create, but \nthey provide fewer guarantees. \nRegardless of the level of formality, always remember that different no-\ntations are better (or worse) for expressing different kinds of information. For-\nmality aside, no UML class diagram will help you reason about schedulability, \nnor will a sequence chart tell you very much about the system’s likelihood of \nbeing delivered on time. You should choose your notations and representation \nlanguages always keeping in mind the important issues you need to capture and \nreason about.\n18.3  Views\nPerhaps the most important concept associated with software architecture docu-\nmentation is that of the view. A software architecture is a complex entity that can-\nnot be described in a simple one-dimensional fashion. A view is a representation \nof a set of system elements and relations among them—not all system elements, \nbut those of a particular type. For example, a layered view of a system would \nshow elements of type “layer”—that is, it would show the system’s decompo-\nsition into layers—and the relations among those layers. A pure layered view \nwould not, however, show the system’s services, or clients and servers, or data \nmodel, or any other type of element.\nThus, views let us divide the multidimensional entity that is a software ar-\nchitecture into a number of (we hope) interesting and manageable representations \nof the system. The concept of views gives us our most fundamental principle of \narchitecture documentation: \nDocumenting an architecture is a matter of documenting the relevant \nviews and then adding documentation that applies to more than one view.\nThis maxim gives our approach to documentation its name: Views and \nBeyond.\nWhat are the relevant views? This depends entirely on your goals. As we \nsaw previously, architecture documentation can serve many purposes: a mission \nstatement for implementers, a basis for analysis, the specification for automatic \ncode generation, the starting point for system understanding and asset recovery, \nor the blueprint for project planning. \nDifferent views also expose different quality attributes to different degrees. \nTherefore, the quality attributes that are of most concern to you and the other \nstakeholders in the system’s development will affect the choice of what views to \n",
      "content_length": 2618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "332  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\ndocument. For instance, a layered view will let you reason about your system’s \nportability, a deployment view will let you reason about your system’s perfor-\nmance and reliability, and so forth.\nDifferent views support different goals and uses. This is why we do not ad-\nvocate a particular view or collection of views. The views you should document \ndepend on the uses you expect to make of the documentation. Different views \nwill highlight different system elements and relations. How many different views \nto represent is the result of a cost/benefit decision. Each view has a cost and a \nbenefit, and you should ensure that the benefits of maintaining a particular view \noutweigh its costs.\nViews may be driven by the need to document a particular pattern in your \ndesign. Some patterns are composed of modules, others of components and con-\nnectors, and still others have deployment considerations. Module views, compo-\nnent-and-connector (C&C) views, and allocation views are the appropriate mech-\nanisms for representing these considerations.\nModule Views \nA module is an implementation unit that provides a coherent set of responsibili-\nties. A module might take the form of a class, a collection of classes, a layer, an \naspect, or any decomposition of the implementation unit. Example module views \nare decomposition, uses, and layers. Every module has a collection of properties \nassigned to it. These properties are intended to express the important information \nassociated with the module, as well as constraints on the module. Sample proper-\nties are responsibilities, visibility information, and revision history. The relations \nthat modules have to one another include is part of, depends on, and is a.\nThe way in which a system’s software is decomposed into manageable \nunits remains one of the important forms of system structure. At a minimum, \nthis determines how a system’s source code is decomposed into units, what kinds \nof assumptions each unit can make about services provided by other units, and \nhow those units are aggregated into larger ensembles. It also includes global data \nstructures that impact and are impacted by multiple units. Module structures of-\nten determine how changes to one part of a system might affect other parts and \nhence the ability of a system to support modifiability, portability, and reuse.\nIt is unlikely that the documentation of any software architecture can be \ncomplete without at least one module view.\nTable 18.1 summarizes the elements, relations, constraints, and purpose of \nthe module views in general. Later we provide this information specific to each \nof a number of often used module views. \nProperties of modules that help to guide implementation or are input to anal-\nysis should be recorded as part of the supporting documentation for a module \nview. The list of properties may vary but is likely to include the following:\n",
      "content_length": 2988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "18.3  Views\n333\nTable 18.1  Summary of the Module Views\nElements\nModules, which are implementation units of software that \nprovide a coherent set of responsibilities.\nRelations\n■\n■\nIs part of, which defines a part/whole relationship between \nthe submodule—the part—and the aggregate module—the \nwhole.\n■\n■\nDepends on, which defines a dependency relationship be-\ntween two modules. Specific module views elaborate what \ndependency is meant.\n■\n■\nIs a, which defines a generalization/specialization relation-\nship between a more specific module—the child—and a \nmore general module—the parent.\nConstraints\nDifferent module views may impose specific topological \nconstraints, such as limitations on the visibility between \nmodules.\nUsage\n■\n■\nBlueprint for construction of the code\n■\n■\nChange-impact analysis\n■\n■\nPlanning incremental development\n■\n■\nRequirements traceability analysis\n■\n■\nCommunicating the functionality of a system and the struc-\nture of its code base\n■\n■\nSupporting the definition of work assignments, implementa-\ntion schedules, and budget information\n■\n■\nShowing the structure of information that the system needs \nto manage\n■\n■Name. A module’s name is, of course, the primary means to refer to it. A \nmodule’s name often suggests something about its role in the system. In ad-\ndition, a module’s name may reflect its position in a decomposition hierar-\nchy; the name A.B.C, for example, refers to a module C that is a submodule \nof a module B, itself a submodule of A.\n■\n■Responsibilities. The responsibility property for a module is a way to iden-\ntify its role in the overall system and establishes an identity for it beyond \nthe name. Whereas a module’s name may suggest its role, a statement of \nresponsibility establishes it with much more certainty. Responsibilities \nshould be described in sufficient detail to make clear to the reader what \neach module does.\n■\n■Visibility of interface(s). When a module has submodules, some interfaces \nof the submodules are public and some may be private; that is, the inter-\nfaces are used only by the submodules within the enclosing parent module. \nThese private interfaces are not visible outside that context. \n■\n■Implementation information. Modules are units of implementation. It is \ntherefore useful to record information related to their implementation from \nthe point of view of managing their development and building the system \nthat contains them. This might include the following:\n",
      "content_length": 2454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "334  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n■\n■Mapping to source code units. This identifies the files that constitute the \nimplementation of a module. For example, a module Account, if imple-\nmented in Java, might have several files that constitute its implementa-\ntion: IAccount.java (an interface), AccountImpl.java (the implementation \nof Account functionality), AccountBean.java (a class to hold the state of \nan account in memory), AccountOrmMapping.xml (a file that defines the \nmapping between AccountBean and a database table—object-relational \nmapping), and perhaps even a unit test AccountTest.java.\n■\n■Test information. The module’s test plan, test cases, test scaffolding, and \ntest data are important to document. This information may simply be a \npointer to the location of these artifacts.\n■\n■Management information. A manager may need information about the \nmodule’s predicted schedule and budget. This information may simply be \na pointer to the location of these artifacts.\n■\n■Implementation constraints. In many cases, the architect will have an \nimplementation strategy in mind for a module or may know of constraints \nthat the implementation must follow.\n■\n■Revision history. Knowing the history of a module including authors and \nparticular changes may help when you perform maintenance activities.\nBecause modules partition the system, it should be possible to determine how \nthe functional requirements of a system are supported by module responsibilities. \nModule views that show dependencies among modules or layers (which are groups \nof modules that have a specific pattern of allowed usage) provide a good basis for \nchange-impact analysis. Modules are typically modified as a result of problem re-\nports or change requests. Impact analysis requires a certain degree of design com-\npleteness and integrity of the module description. In particular, dependency informa-\ntion has to be available and correct to be able to create useful results. \nA module view can be used to explain the system’s functionality to some-\none not familiar with it. The various levels of granularity of the module decom-\nposition provide a top-down presentation of the system’s responsibilities and \ntherefore can guide the learning process. For a system whose implementation is \nalready in place, module views, if kept up to date, are helpful, as they explain \nthe structure of the code base to a new developer on the team. Thus, up-to-date \nmodule views can simplify and regularize system maintenance. \nOn the other hand, it is difficult to use the module views to make inferences \nabout runtime behavior, because these views are just a static partition of the func-\ntions of the software. Thus, a module view is not typically used for analysis of \nperformance, reliability, and many other runtime qualities. For those, we rely on \ncomponent-and-connector and allocation views.\nModule views are commonly mapped to component-and-connector views. \nThe implementation units shown in module views have a mapping to components \nthat execute at runtime. Sometimes, the mapping is quite straightforward, even \none-to-one for small, simple applications. More often, a single module will be \n",
      "content_length": 3233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "18.3  Views\n335\nreplicated as part of many runtime components, and a given component could \nmap to several modules. Module views also provide the software elements that \nare mapped to the diverse nonsoftware elements of the system environment in the \nvarious allocation views.\nComponent-and-Connector Views \nComponent-and-connector views show elements that have some runtime pres-\nence, such as processes, objects, clients, servers, and data stores. These elements \nare termed components. Additionally, component-and-connector views include as \nelements the pathways of interaction, such as communication links and proto-\ncols, information flows, and access to shared storage. Such interactions are rep-\nresented as connectors in C&C views. Sample C&C views are service-oriented \narchitecture (SOA), client-server, or communicating process views.\nComponents have interfaces called ports. A port defines a point of potential \ninteraction of a component with its environment. A port usually has an explicit \ntype, which defines the kind of behavior that can take place at that point of in-\nteraction. A component may have many ports of the same type, each forming a \ndifferent input or output channel at runtime. In this respect ports differ from inter-\nfaces of modules, whose interfaces are never replicated. You can annotate a port \nwith a number or range of numbers to indicate replication; for example, “1..4” \nmight mean that an interface could be replicated up to four times. A component’s \nports should be explicitly documented, by showing them in the diagram and de-\nfining them in the diagram’s supporting documentation.\nA component in a C&C view may represent a complex subsystem, which \nitself can be described as a C&C subarchitecture. This subarchitecture can be \ndepicted graphically in situ when the substructure is not too complex, by showing \nit as nested inside the component that it refines. Often, however, it is documented \nseparately. A component’s subarchitecture may employ a different pattern than \nthe one in which the component appears. \nConnectors are the other kind of element in a C&C view. Simple examples \nof connectors are service invocation; asynchronous message queues; event multi-\ncast supporting publish-subscribe interactions; and pipes that represent asynchro-\nnous, order-preserving data streams. Connectors often represent much more com-\nplex forms of interaction, such as a transaction-oriented communication channel \nbetween a database server and a client, or an enterprise service bus that mediates \ninteractions between collections of service users and providers.\nConnectors have roles, which are its interfaces, defining the ways in which \nthe connector may be used by components to carry out interaction. For exam-\nple, a client-server connector might have invokes-services and provides-services \nroles. A pipe might have writer and reader roles. Like component ports, connec-\ntor roles differ from module interfaces in that they can be replicated, indicating \n",
      "content_length": 3008,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "336  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nhow many components can be involved in its interaction. A publish-subscribe \nconnector might have many instances of the publisher and subscriber roles. \nLike components, complex connectors may in turn be decomposed into col-\nlections of components and connectors that describe the architectural substruc-\nture of those connectors. Connectors need not be binary. That is, they need not \nhave exactly two roles. For example, a publish-subscribe connector might have \nan arbitrary number of publisher and subscriber roles. Even if the connector is ul-\ntimately implemented using binary connectors, such as a procedure call, it can be \nuseful to adopt n-ary connector representations in a C&C view. Connectors em-\nbody a protocol of interaction. When two or more components interact, they must \nobey conventions about order of interactions, locus of control, and handling of \nerror conditions and timeouts. The protocol of interaction should be documented.\nThe primary relation within a C&C view is attachment. Attachments indicate \nwhich connectors are attached to which components, thereby defining a system as a \ngraph of components and connectors. Specifically, an attachment is denoted by as-\nsociating (attaching) a component’s port to a connector’s role. A valid attachment is \none in which the ports and roles are compatible with each other, under the semantic \nconstraints defined by the view. Compatibility often is defined in terms of informa-\ntion type and protocol. For example, in a call-return architecture, you should check \nto make sure that all “calls” ports are attached to some call-return connector. At a \ndeeper semantic level, you should check to make sure that a port’s protocol is con-\nsistent with the behavior expected by the role to which it is attached.\nAn element (component or connector) of a C&C view will have various \nassociated properties. Every element should have a name and type. Additional \nproperties depend on the type of component or connector. Define values for the \nproperties that support the intended analyses for the particular C&C view. For \nexample, if the view will be used for performance analysis, latencies, queue ca-\npacities, and thread priorities may be necessary. The following are examples of \nsome typical properties and their uses:\n■\n■Reliability. What is the likelihood of failure for a given component or connec-\ntor? This property might be used to help determine overall system availability.\n■\n■Performance. What kinds of response time will the component provide un-\nder what loads? What kind of bandwidth, latency, jitter, transaction volume, \nor throughput can be expected for a given connector? This property can \nbe used with others to determine system-wide properties such as response \ntimes, throughput, and buffering needs.\n■\n■Resource requirements. What are the processing and storage needs of a \ncomponent or a connector? This property can be used to determine whether \na proposed hardware configuration will be adequate.\n■\n■Functionality. What functions does an element perform? This property can \nbe used to reason about overall computation performed by a system.\n■\n■Security. Does a component or a connector enforce or provide security fea-\ntures, such as encryption, audit trails, or authentication? This property can \nbe used to determine system security vulnerabilities.\n",
      "content_length": 3432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "18.3  Views\n337\n■\n■Concurrency. Does this component execute as a separate process or thread? \nThis property can help to analyze or simulate the performance of concur-\nrent components and identify possible deadlocks.\n■\n■Modifiability. Does the messaging structure support a structure to cater for \nevolving data exchanges? Can the components be adapted to process those \nnew messages? This property can be defined to extend the functionality of a \ncomponent.\n■\n■Tier. For a tiered topology, what tier does the component reside in? This \nproperty helps to define the build and deployment procedures, as well as \nplatform requirements for each tier.\nC&C views are commonly used to show to developers and other stakehold-\ners how the system works—one can “animate” or trace through a C&C view, \nshowing an end-to-end thread of activity. C&C views are also used to reason \nabout runtime system quality attributes, such as performance and availability. In \nparticular, a well-documented view allows architects to predict overall system \nproperties such as latency or reliability, given estimates or measurements of prop-\nerties of the individual elements and their interactions. \nTable 18.2 summarizes the elements, relations, and properties that can ap-\npear in C&C views. This table is followed by a more detailed discussion of these \nconcepts, together with guidelines concerning their documentation. \nTable 18.2  Summary of Component-and-Connector Views\nElements\n■\n■\nComponents. Principal processing units and data stores. A compo-\nnent has a set of ports through which it interacts with other compo-\nnents (via connectors).\n■\n■\nConnectors. Pathways of interaction between components. Connec-\ntors have a set of roles (interfaces) that indicate how components \nmay use a connector in interactions.\nRelations\n■\n■\nAttachments. Component ports are associated with connector roles \nto yield a graph of components and connectors.\n■\n■\nInterface delegation. In some situations component ports are associ-\nated with one or more ports in an “internal” subarchitecture. The case \nis similar for the roles of a connector. \nConstraints\n■\n■\nComponents can only be attached to connectors, not directly to other \ncomponents.\n■\n■\nConnectors can only be attached to components, not directly to other \nconnectors.\n■\n■\nAttachments can only be made between compatible ports and roles.\n■\n■\nInterface delegation can only be defined between two compatible \nports (or two compatible roles).\n■\n■\nConnectors cannot appear in isolation; a connector must be attached \nto a component.\nUsage\n■\n■\nShow how the system works.\n■\n■\nGuide development by specifying structure and behavior of runtime \nelements.\n■\n■\nHelp reason about runtime system quality attributes, such as perfor-\nmance and availability. \n",
      "content_length": 2770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "338 \nPart Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nNotations for C&C Views\nAs always, box-and-line drawings are available to represent C&C views. Al-\nthough informal notations are limited in the semantics that can be conveyed, \nfollowing some simple guidelines can lend rigor and depth to the descriptions. \nThe primary guideline is simple: assign each component type and each connector \ntype a separate visual form (symbol), and list each of the types in a key.\nUML components are a good semantic match to C&C components because \nthey permit intuitive documentation of important information like interfaces, \nproperties, and behavioral descriptions. UML components also distinguish be-\ntween component types and component instances, which is useful when defining \nview-specific component types.\nUML ports are a good semantic match to C&C ports. A UML port can be \ndecorated with a multiplicity, as shown in the left portion of Figure 18.1, though \nthis is typically only done on component types. The number of ports on compo-\nnent instances, as shown in the right portion of Figure 18.1, is typically bound to \na specific number. Components that dynamically create and manage a set of ports \nshould retain a multiplicity descriptor on instance descriptions.\nWhile C&C connectors are as semantically rich as C&C components, the \nsame is not true of UML connectors. UML connectors cannot have substructure, \nattributes, or behavioral descriptions. This makes choosing how to represent \nC&C connectors more difficult, as UML connectors are not always rich enough.\nYou should represent a “simple” C&C connector using a UML connec-\ntor—a line. Many commonly used C&C connectors have well-known, applica-\ntion-independent semantics and implementations, such as function calls or data \nread operations. If the only information you need to supply is the type of the \nconnector, then a UML connector is adequate. Call-return connectors can be rep-\nresented by a UML assembly connector, which links a component’s required in-\nterface (socket) to the other component’s provided interface (lollipop). You can \nuse a stereotype to denote the type of connector. If all connectors in a primary \npresentation are of the same type, you can note this once in a comment rather \nthan explicitly on each connector to reduce visual clutter. Attachment is shown \nby connecting the endpoints of the connector to the ports of components. Con-\nnector roles cannot be explicitly represented with a UML connector because the \nUML connector element does not allow the inclusion of interfaces (unlike the \nUML port, which does allow interfaces). The best approximation is to label the \nconnector ends and use these labels to identify role descriptions that must be doc-\numented elsewhere.\nYou should represent a “rich” C&C connector using a UML component, or \nby annotating a line UML connector with a tag or other auxiliary documentation \nthat explains the meaning of the complex connector. \n",
      "content_length": 3000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "18.3  Views\n339\n: Account\nDatabase\n«Repository»\nAccount Database\nServer [1..5]\nServer\nServer\nAdmin\nAdmin\nKey: UML\nFigure 18.1  A UML representation of the ports on a C&C component type (left) \nand component instance (right). The Account Database component type has \ntwo types of ports, Server and Admin (noted by the boxes on the component’s \nborder). The Server port is defined with a multiplicity, meaning that multiple \ninstances of the port are permitted on any corresponding component instance. \nAllocation Views \nAllocation views describe the mapping of software units to elements of an envi-\nronment in which the software is developed or in which it executes. The environ-\nment might be the hardware, the operating environment in which the software is \nexecuted, the file systems supporting development or deployment, or the devel-\nopment organization(s). \nTable 18.3 summarizes the characteristics of allocation views. Allocation \nviews consist of software elements and environmental elements. Examples of en-\nvironmental elements are a processor, a disk farm, a file or folder, or a group of \ndevelopers. The software elements come from a module or C&C view.\nTable 18.3  Summary of the Characteristics of Allocation Views\nElements\n■\n■\nSoftware element. A software element has properties that are \nrequired of the environment. \n■\n■\nEnvironmental element. An environmental element has properties \nthat are provided to the software.\nRelations\nAllocated to. A software element is mapped (allocated to) an \nenvironmental element. Properties are dependent on the particular \nview.\nConstraints\nVaries by view\nUsage\n■\n■\nFor reasoning about performance, availability, security, and safety. \n■\n■\nFor reasoning about distributed development and allocation of \nwork to teams. \n■\n■\nFor reasoning about concurrent access to software versions. \n■\n■\nFor reasoning about the form and mechanisms of system \ninstallation.\n",
      "content_length": 1912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "340  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe relation in an allocation view is allocated to. We usually talk about alloca-\ntion views in terms of a mapping from software elements to environmental elements, \nalthough the reverse mapping can also be relevant and interesting. A single software \nelement can be allocated to multiple environmental elements, and multiple software \nelements can be allocated to a single environmental element. If these allocations \nchange over time, either during development or execution of the system, then the \narchitecture is said to be dynamic with respect to that allocation. For example, pro-\ncesses might migrate from one processor or virtual machine to another. Similarly \nmodules might migrate from one development team to another.\nSoftware elements and environmental elements have properties in allocation \nviews. The usual goal of an allocation view is to compare the properties required \nby the software element with the properties provided by the environmental ele-\nments to determine whether the allocation will be successful or not. For example, \nto ensure a component’s required response time, it has to execute on (be allocated \nto) a processor that provides sufficiently fast processing power. For another ex-\nample, a computing platform might not allow a task to use more than 10 kilo-\nbytes of virtual memory. An execution model of the software element in question \ncan be used to determine the required virtual memory usage. Similarly, if you are \nmigrating a module from one team to another, you might want to ensure that the \nnew team has the appropriate skills and background knowledge.\nAllocation views can depict static or dynamic views. A static view depicts a \nfixed allocation of resources in an environment. A dynamic view depicts the condi-\ntions and the triggers for which allocation of resources changes according to load-\ning. Some systems recruit and utilize new resources as their load increases. An ex-\nample is a load-balancing system in which new processes or threads are created on \nanother machine. In this view, the conditions under which the allocation changes, \nthe allocation of runtime software, and the dynamic allocation mechanism need to \nbe documented. (Recall from Chapter 1 that one of the allocation structures is the \nwork assignment structure, which allocates modules to teams for development. That \nrelationship, too, can be allocated dynamically, depending on “load”—in this case, \nthe load on development teams.)\nQuality Views \nModule, C&C, and allocation views are all structural views: They primarily show \nthe structures that the architect has engineered into the architecture to satisfy \nfunctional and quality attribute requirements.\nThese views are excellent for guiding and constraining downstream develop-\ners, whose primary job it is to implement those structures. However, in systems in \nwhich certain quality attributes (or, for that matter, some other kind of stakeholder \nconcerns) are particularly important and pervasive, structural views may not be the \nbest way to present the architectural solution to those needs. The reason is that the \nsolution may be spread across multiple structures that are inconvenient to combine \n(for example, because the element types shown in each are different).\n",
      "content_length": 3344,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "18.4  Choosing the Views\n341\nAnother kind of view, which we call a quality view, can be tailored for spe-\ncific stakeholders or to address specific concerns. These quality views are formed \nby extracting the relevant pieces of structural views and packaging them together. \nHere are five examples:\n■\n■A security view can show all of the architectural measures taken to provide \nsecurity. It would show the components that have some security role or \nresponsibility, how those components communicate, any data repositories \nfor security information, and repositories that are of security interest. The \nview’s context information would show other security measures (such as \nphysical security) in the system’s environment. The behavior part of a se-\ncurity view would show the operation of security protocols and where and \nhow humans interact with the security elements. It would also capture how \nthe system would respond to specific threats and vulnerabilities. \n■\n■A communications view might be especially helpful for systems that are \nglobally dispersed and heterogeneous. This view would show all of the \ncomponent-to-component channels, the various network channels, quali-\nty-of-service parameter values, and areas of concurrency. This view can be \nused to analyze certain kinds of performance and reliability (such as dead-\nlock or race condition detection). The behavior part of this view could show \n(for example) how network bandwidth is dynamically allocated.\n■\n■An exception or error-handling view could help illuminate and draw atten-\ntion to error reporting and resolution mechanisms. Such a view would show \nhow components detect, report, and resolve faults or errors. It would help \nidentify the sources of errors and appropriate corrective actions for each. \nRoot-cause analysis in those cases could be facilitated by such a view.\n■\n■A reliability view would be one in which reliability mechanisms such as \nreplication and switchover are modeled. It would also depict timing issues \nand transaction integrity. \n■\n■A performance view would include those aspects of the architecture useful \nfor inferring the system’s performance. Such a view might show network \ntraffic models, maximum latencies for operations, and so forth.\nThese and other quality views reflect the documentation philosophy of ISO/\nIEC/IEEE standard 42010:2011, which prescribes creating views driven by stake-\nholder concerns about the architecture. \n18.4  Choosing the Views\nDocumenting decisions during the design process (something we strongly rec-\nommend) produces views, which are the heart of an architecture document. It \nis most likely that these views are rough sketches more than finished products \n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "342  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nready for public release; this will give you the freedom to back up and rethink \ndesign decisions that turn out to be problematic without having wasted time on \nbroad dissemination and cosmetic polish. They are documented purely as your \nown memory aid.\nBy the time you’re ready to release an architecture document, you’re likely \nto have a fairly well-worked-out collection of architecture views. At some point \nyou’ll need to decide which to take to completion, with how much detail, and \nwhich to include in a given release. You’ll also need to decide which views can \nbe usefully combined with others, so as to reduce the total number of views in the \ndocument and reveal important relations among the views.\nYou can determine which views are required, when to create them, and how \nmuch detail to include if you know the following:\n■\n■What people, and with what skills, are available\n■\n■Which standards you have to comply with\n■\n■What budget is on hand\n■\n■What the schedule is\n■\n■What the information needs of the important stakeholders are\n■\n■What the driving quality attribute requirements are\n■\n■What the size of the system is\nAt a minimum, expect to have at least one module view, at least one C&C \nview, and for larger systems, at least one allocation view in your architecture doc-\nument. Beyond that basic rule of thumb, however, there is a three-step method for \nchoosing the views: \n■\n■Step 1: Build a stakeholder/view table. Enumerate the stakeholders for \nyour project’s software architecture documentation down the rows. Be as \ncomprehensive as you can. For the columns, enumerate the views that ap-\nply to your system. (Use the structures discussed in Chapter 1, the views \ndiscussed in this chapter, and the views that your design work in ADD has \nsuggested as a starting list of candidates.) Some views (such as decom-\nposition, uses, and work assignment) apply to every system, while others \n(various C&C views, the layered view) only apply to some systems. For the \ncolumns, make sure to include the views or view sketches you already have \nas a result of your design work so far. \nOnce you have the rows and columns defined, fill in each cell to describe \nhow much information the stakeholder requires from the view: none, overview \nonly, moderate detail, or high detail. The candidate view list going into step 2 \nnow consists of those views for which some stakeholder has a vested interest.\n■\n■Step 2: Combine views. The candidate view list from step 1 is likely to \nyield an impractically large number of views. This step will winnow the list \nto manageable size. Look for marginal views in the table: those that require \nonly an overview, or that serve very few stakeholders. Combine each mar-\nginal view with another view that has a stronger constituency. \n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "18.5  Combining Views \n343\n■\n■Step 3: Prioritize and stage. After step 2 you should have the minimum \nset of views needed to serve your stakeholder community. At this point you \nneed to decide what to do first. What you do first depends on your project, \nbut here are some things to consider:\n■\n■The decomposition view (one of the module views) is a particularly \nhelpful view to release early. High-level (that is, broad and shallow) \ndecompositions are often easy to design, and with this information the \nproject manager can start to staff development teams, put training in \nplace, determine which parts to outsource, and start producing budgets \nand schedules.\n■\n■Be aware that you don’t have to satisfy all the information needs of all the \nstakeholders to the fullest extent. Providing 80 percent of the information \ngoes a long way, and this might be good enough so that the stakeholders \ncan do their job. Check with the stakeholder to see if a subset of informa-\ntion would be sufficient. They typically prefer a product that is delivered \non time and within budget over getting the perfect documentation.\n■\n■You don’t have to complete one view before starting another. People can \nmake progress with overview-level information, so a breadth-first ap-\nproach is often the best.\n18.5  Combining Views \nThe basic principle of documenting an architecture as a set of separate views \nbrings a divide-and-conquer advantage to the task of documentation, but if the \nviews were irrevocably different, with no association with one another, nobody \nwould be able to understand the system as a whole.\nBecause all views in an architecture are part of that same architecture and \nexist to achieve a common purpose, many of them have strong associations with \neach other. Managing how architectural structures are associated is an important \npart of the architect’s job, independent of whether any documentation of those \nstructures exists. \nSometimes the most convenient way to show a strong association between \ntwo views is to collapse them into a single combined view, as dictated by step 2 \nof the three-step method just presented to choose the views. A combined view \nis a view that contains elements and relations that come from two or more other \nviews. Combined views can be very useful as long as you do not try to overload \nthem with too many mappings.\nThe easiest way to merge views is to create an overlay that combines the in-\nformation that would otherwise have been in two separate views. This works well \nif the coupling between the two views is tight; that is, there are strong associations \n",
      "content_length": 2607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "344  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nbetween elements in one view and elements in the other view. If that is the case, \nthe structure described by the combined view will be easier to understand than \nthe two views seen separately. For an example, see the overlay of decomposition \nand uses sketches shown in Figure 18.2. In an overlay, the elements and the rela-\ntions keep the types as defined in their constituent views.\nThe views below often combine naturally:\n■\n■Various C&C views. Because C&C views all show runtime relations among \ncomponents and connectors of various types, they tend to combine well. \nDifferent (separate) C&C views tend to show different parts of the system, \nor tend to show decomposition refinements of components in other views. \nThe result is often a set of views that can be combined easily.\n«subsystem»\nCCS\nutils\n«subsystem»\nitc\nservlet\n«subsystem»\nadlsc\ncontroller\nbusiness\nfacades\nclient\nentity\ntest\ntaglibs\nobjects\n«subsystem»\ntdc\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\n<<uses>>\nServer-Side Application Modules\ncommon\nsecurity\nportal\nwebservice\nNotation: UML\nFigure 18.2  A decomposition view overlaid with “uses” information, to create a \ndecomposition/uses overlay. \n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "18.6  Building the Documentation Package\n345\n■\n■Deployment view with either SOA or communicating-processes views. An \nSOA view shows services, and a communicating-processes view shows pro-\ncesses. In both cases, these are components that are deployed onto proces-\nsors. Thus there is a strong association between the elements in these views.\n■\n■Decomposition view and any of work assignment, implementation, uses, or \nlayered views. The decomposed modules form the units of work, develop-\nment, and uses. In addition, these modules populate layers.\n18.6  Building the Documentation Package\nRemember the principle of architecture documentation, with which we started \nthis chapter. This principle tells us that our task is to document the relevant views \nand to document the information that applies to more than one view.\nDocumenting a View\nFigure 18.3 shows a template for documenting a view.\nSection 1. Primary Presentation\nSection 2. Element Catalog\n \nSection 2.A. Elements and Their Properties\n \nSection 2.B. Relations and Their Properties\n \nSection 2.C. Element Interfaces\n \nSection 2.D. Element Behavior\nSection 3. Context Diagram\nSection 4. Variability Guide\nSection 5. Rationale\nTemplate for a View\nFigure 18.3  View template\n",
      "content_length": 1234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "346  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nNo matter what the view, the documentation for a view can be placed into a \nstandard organization consisting of these parts:\n■\n■Section 1: The Primary Presentation. The primary presentation shows \nthe elements and relations of the view. The primary presentation should \ncontain the information you wish to convey about the system—in the vo-\ncabulary of that view. It should certainly include the primary elements and \nrelations but under some circumstances might not include all of them. For \nexample, you may wish to show the elements and relations that come into \nplay during normal operation but relegate error handling or exception pro-\ncessing to the supporting documentation. \nThe primary presentation is most often graphical. It might be a diagram \nyou’ve drawn in an informal notation using a simple drawing tool, or it \nmight be a diagram in a semiformal or formal notation imported from a \ndesign or modeling tool that you’re using. If your primary presentation is \ngraphical, make sure to include a key that explains the notation. Lack of \na key is the most common mistake that we see in documentation in practice.\nOccasionally the primary presentation will be textual, such as a table or \na list. If that text is presented according to certain stylistic rules, these rules \nshould be stated or incorporated by reference, as the analog to the graphi-\ncal notation key. Regardless of whether the primary presentation is textual \ninstead of graphical, its role is to present a terse summary of the most im-\nportant information in the view. \n■\n■Section 2: The Element Catalog. The element catalog details at least those \nelements depicted in the primary presentation. For instance, if a diagram \nshows elements A, B, and C, then the element catalog needs to explain what \nA, B, and C are. In addition, if elements or relations relevant to this view \nwere omitted from the primary presentation, they should be introduced and \nexplained in the catalog. Specific parts of the catalog include the following:\n■\n■Elements and their properties. This section names each element in the \nview and lists the properties of that element. Each view introduced in \nChapter 1 listed a set of suggested properties associated with that view. \nFor example, elements in a decomposition view might have the property \nof “responsibility”—an explanation of each module’s role in the sys-\ntem—and elements in a communicating-processes view might have tim-\ning parameters, among other things, as properties. Whether the properties \nare generic to the view chosen or the architect has introduced new ones, \nthis is where they are documented and given values.\n■\n■Relations and their properties. Each view has specific relation types that \nit depicts among the elements in that view. Mostly, these relations are \nshown in the primary presentation. However, if the primary presentation \ndoes not show all the relations or if there are exceptions to what is depict-\ned in the primary presentation, this is the place to record that information.\n■\n■Element interfaces. This section documents element interfaces.\n",
      "content_length": 3175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "18.6  Building the Documentation Package\n347\n■\n■Element behavior. This section documents element behavior that is not \nobvious from the primary presentation.\n■\n■Section 3: Context Diagram. A context diagram shows how the system or \nportion of the system depicted in this view relates to its environment. The \npurpose of a context diagram is to depict the scope of a view. Here “con-\ntext” means an environment with which the part of the system interacts. \nEntities in the environment may be humans, other computer systems, or \nphysical objects, such as sensors or controlled devices. \n■\n■Section 4: Variability Guide. A variability guide shows how to exercise \nany variation points that are a part of the architecture shown in this view.\n■\n■Section 5: Rationale. Rationale explains why the design reflected in the view \ncame to be. The goal of this section is to explain why the design is as it is and \nto provide a convincing argument that it is sound. The choice of a pattern in \nthis view should be justified here by describing the architectural problem that \nthe chosen pattern solves and the rationale for choosing it over another.\nDocumenting Information Beyond Views \nAs shown in Figure 18.4, documentation beyond views can be divided into two \nparts:\n1.\t\nOverview of the architecture documentation. This tells how the documen-\ntation is laid out and organized so that a stakeholder of the architecture can \nfind the information he or she needs efficiently and reliably.\n2.\t\nInformation about the architecture. Here, the information that remains to be \ncaptured beyond the views themselves is a short system overview to ground \nany reader as to the purpose of the system and the way the views are related \nto one another, an overview of and rationale behind system-wide design \napproaches, a list of elements and where they appear, and a glossary and an \nacronym list for the entire architecture. \nFigure 18.4 summarizes our template for documentation beyond views. \nDocumentation beyond views consists of the following sections:\n■\n■Document control information. List the issuing organization, the current \nversion number, date of issue and status, a change history, and the procedure \nfor submitting change requests to the document. Usually this is captured in \nthe front matter. Change control tools can provide much of this information.\n■\n■Section 1: Documentation Roadmap. The documentation roadmap tells \nthe reader what information is in the documentation and where to find it. A \ndocumentation map consists of four sections:\n■\n■Scope and summary. Explain the purpose of the document and briefly \nsummarize what is covered and (if you think it will help) what is not cov-\nered. Explain the relation to other documents (such as downstream design \ndocuments or upstream system engineering documents).\n",
      "content_length": 2812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "348  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nSection 1. Documentation Roadmap\nSection 2. How a View Is Documented\nSection 3. System Overview\nSection 4. Mapping Between Views\nSection 5. Rationale\nSection 6. Directory — index, glossary, \n \nacronym list\nTemplate for Documentation\nBeyond Views\nArchitecture\ndocumentation\ninformation\nArchitecture\ninformation\nFigure 18.4  Summary of documentation beyond views\n■\n■How the documentation is organized. For each section in the documenta-\ntion, give a short synopsis of the information that can be found there. An \nalternative to this is to use an annotated table of contents. This is a table \nthat doesn’t just list section titles and page numbers, but also gives a syn-\nopsis with each entry. It provides one-stop shopping for a reader attempt-\ning to look up a particular kind of information.\n■\n■View overview. The major part of the map describes the views that the \narchitect has included in the package. For each view, the map gives the \nfollowing information:\n■\n■The name of the view and what pattern it instantiates, if any.\n■\n■A description of the view’s element types, relation types, and property \ntypes. This lets a reader begin to understand the kind of information \nthat is presented in the view.\n■\n■A description of language, modeling techniques, or analytical methods \nused in constructing the view.\n■\n■How stakeholders can use the documentation. The map follows with a \nsection describing which stakeholders and concerns are addressed by \neach view; this is conveniently captured as a table. This section shows \nhow various stakeholders might use the documentation to help address \ntheir concerns. Include short scenarios, such as “A maintainer wishes to \nknow the units of software that are likely to be changed by a proposed \nmodification. The maintainer consults the decomposition view to under-\nstand the responsibilities of each module in order to identify the modules \nlikely to change. The maintainer then consults the uses view1 to see what \n1.   The uses view is a module view. It shows the uses structure discussed in Chapter 1.\n",
      "content_length": 2138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "18.6  Building the Documentation Package\n349\nmodules use the affected modules (and thus might also have to change).” \nTo be compliant with ISO/IEC 42010-2007, you must consider the con-\ncerns of at least users, acquirers, developers, and maintainers. \n■\n■Section 2: How a View Is Documented. This is where you explain the \nstandard organization you’re using to document views—either the one de-\nscribed in this chapter or one of your own. It tells your readers how to find \ninformation in a view. If your organization has standardized on a template \nfor a view, as it should, then you can simply refer to that standard. If you \nare lacking such a template, then text such as that given above describ-\ning our view template should appear in this section of your architecture \ndocumentation.\n■\n■Section 3: System Overview. This is a short prose description of the sys-\ntem’s function, its users, and any important background or constraints. This \nsection provides your readers with a consistent mental model of the system \nand its purpose. This might be just a pointer to a concept-of-operations \ndocument.\n■\n■Section 4: Mapping Between Views. Because all the views of an archi-\ntecture describe the same system, it stands to reason that any two views \nwill have much in common. Helping a reader understand the associations \nbetween views will help that reader gain a powerful insight into how the \narchitecture works as a unified conceptual whole. \nThe associations between elements across views in an architecture are, \nin general, many-to-many. For instance, each module may map to multiple \nruntime elements, and each runtime element may map to multiple modules. \nView-to-view associations can be conveniently captured as tables. List \nthe elements of the first view in some convenient lookup order. The table \nitself should be annotated or introduced with an explanation of the associa-\ntion that it depicts; that is, what the correspondence is between the elements \nacross the two views. Examples include “is implemented by” for mapping \nfrom a component-and-connector view to a module view, “implements” for \nmapping from a module view to a component-and-connector view, “in-\ncluded in” for mapping from a decomposition view to a layered view, and \nmany others.\n■\n■Section 5: Rationale. This section documents the architectural decisions \nthat apply to more than one view. Prime candidates include documentation \nof background or organizational constraints or major requirements that led \nto decisions of system-wide import. The decisions about which fundamen-\ntal architecture patterns to use are often described here.\n■\n■Section 6: Directory. The directory is a set of reference material that helps \nreaders find more information quickly. It includes an index of terms, a glos-\nsary, and an acronym list.\n",
      "content_length": 2812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "350  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nOnline Documentation, Hypertext, and Wikis \nA document can be structured as linked web pages. Compared with documents \nwritten with a text-editing tool, web-oriented documents typically consist of \nshort pages (created to fit on one screen) with a deeper structure. One page usu-\nally provides some overview information and has links to more detailed informa-\ntion. When done well, a web-based document is easier to use for people who just \nneed overview information. On the other hand, it can become more difficult for \npeople who need detail. Finding information can be more difficult in multi-page, \nweb-based documents than in a single-file, text-based document, unless a search \nengine is available.\nUsing readily available tools, it’s possible to create a shared document that \nmany stakeholders can contribute to. The hosting organization needs to decide \nwhat permissions it wants to give to various stakeholders; the tool used has to \nsupport the permissions policy. In the case of architecture documentation, we \nwould want all stakeholders to comment on and add clarifying information to the \narchitecture, but we would only want architects to be able to change the architec-\nture or at least provide architects with a “final approval” mechanism. A special \nkind of shared document that is ideal for this purpose is a wiki.\nFollow a Release Strategy\nYour project’s development plan should specify the process for keeping the im-\nportant documentation, including architecture documentation, current. The archi-\ntect should plan to issue releases of the documentation to support major project \nmilestones, which usually means far enough ahead of the milestone to give devel-\nopers time to put the architecture to work. For example, the end of each iteration \nor sprint or incremental release could be associated with providing revised docu-\nmentation to the development team.\nDocumenting Patterns\nArchitects can, and typically do, use patterns as a starting point for their design, \nas we have discussed in Chapter 13. These patterns might be published in exist-\ning catalogs or in an organization’s proprietary repository of standard designs, \nor created specifically for the problem at hand by the architect. In each of these \ncases, they provide a generic (that is, incomplete) solution approach that the ar-\nchitect will have to refine and instantiate. \nFirst, record the fact that the given pattern is being used. Then say why this \nsolution approach was chosen—why it is a good fit to the problem at hand. If the \nchosen approach comes from a pattern, this will consist essentially of showing \nthat the problem at hand fits the problem and context of the pattern.\nUsing a pattern means making successive design decisions that eventually \nresult in an architecture. These design decisions manifest themselves as newly \n",
      "content_length": 2920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "18.7  Documenting Behavior\n351\ninstantiated elements and relations among them. The architect can document a \nsnapshot of the architecture at each stage. How many stages there are depends on \nmany things, not the least of which is the ability of readers to follow the design \nprocess in case they have to revisit it in the future.\n18.7  Documenting Behavior\nDocumenting an architecture requires behavior documentation that complements \nstructural views by describing how architecture elements interact with each other. \nReasoning about characteristics such as a system’s potential to deadlock, a sys-\ntem’s ability to complete a task in the desired amount of time, or maximum mem-\nory consumption requires that the architecture description contain information \nabout both the characteristics of individual elements as well as patterns of inter-\naction among them—that is, how they behave with each other. In this section, we \nprovide guidance as to what types of things you will want to document in order \nto reap these benefits. In our architecture view template, behavior has its own \nsection in the element catalog.\nThere are two kinds of notations available for documenting behavior. The \nfirst kind of notation is called trace-oriented languages; the second is called com-\nprehensive languages.\nTraces are sequences of activities or interactions that describe the system’s \nresponse to a specific stimulus when the system is in a specific state. A trace \ndescribes a sequence of activities or interactions between structural elements of \nthe system. Although it is conceivable to describe all possible traces to generate \nthe equivalent of a comprehensive behavioral model, it is not the intention of \ntrace-oriented documentation to do so. Below we describe four notations for doc-\numenting traces: use cases, sequence diagrams, communication diagrams, and \nactivity diagrams. Although other notations are available (such as message se-\nquence charts, timing diagrams, and the Business Process Execution Language), \nwe have chosen these four as a representative sample of trace-oriented languages.\n■\n■Use cases describe how actors can use a system to accomplish their goals. \nUse cases are frequently used to capture the functional requirements for a \nsystem. UML provides a graphical notation for use case diagrams but does \nnot say how the text of a use case should be written. The UML use case dia-\ngram can be used effectively as an overview of the actors and the behavior \nof a system. The use case description is textual and should contain the use \ncase name and brief description, the actor or actors who initiate the use case \n(primary actors), other actors who participate in the use case (secondary \nactors), flow of events, alternative flows, and nonsuccess cases. \n■\n■A UML sequence diagram shows a sequence of interactions among in-\nstances of elements pulled from the structural documentation. It shows only \n",
      "content_length": 2928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "352  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nthe instances participating in the scenario being documented. A sequence \ndiagram has two dimensions: vertical, representing time, and horizontal, \nrepresenting the various instances. The interactions are arranged in time \nsequence from top to bottom. Figure 18.5 is an example of a sequence dia-\ngram that illustrates the basic UML notation. \nObjects (i.e., element instances) have a lifeline, drawn as a vertical \ndashed line along the time axis. The sequence is usually started by an ac-\ntor on the far left. The instances interact by sending messages, which are \nshown as horizontal arrows. A message can be a method or function call, an \nevent sent through a queue, or something else. The message usually maps \nto a resource (operation) in the interface of the receiver instance. A filled \narrowhead on a solid line represents a synchronous message, whereas the \nopen arrowhead represents an asynchronous message. The dashed arrow is \na return message. The execution occurrence bars along the lifeline indicate \nthat the instance is processing or blocked waiting for a return. \nKey (UML)\n:Login\nPage\n:Login\nController\n:UserDao\n:Logger\n:User\nlogin\nlogin(…)\ncheckPwd(…)\nnew\n:User\nSession\nActor\nObject\nLifeline\nExecution\noccurrence\nSynchronous\nmessage\nAsynchronous\nmessage\nReturn\nmessage\nregister User Login(…)\nFigure 18.5  A simple example of a UML sequence diagram\n",
      "content_length": 1456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "18.7  Documenting Behavior\n353\n■\n■A UML communication diagram shows a graph of interacting elements \nand annotates each interaction with a number denoting order. Similarly to \nsequence diagrams, instances shown in a communication diagram are ele-\nments described in the accompanying structural documentation. Commu-\nnication diagrams are useful when the task is to verify that an architecture \ncan fulfill the functional requirements. The diagrams are not useful if the \nunderstanding of concurrent actions is important, as when conducting a \nperformance analysis.\n■\n■UML activity diagrams are similar to flow charts. They show a business \nprocess as a sequence of steps (called actions) and include notation to ex-\npress conditional branching and concurrency, as well as to show sending \nand receiving events. Arrows between actions indicate the flow of control. \nOptionally, activity diagrams can indicate the architecture element or actor \nperforming the actions. Activity diagrams can express concurrency. A fork \nnode (depicted as a thick bar orthogonal to the flow arrows) splits the flow \ninto two or more concurrent flows of actions. The concurrent flows may lat-\ner be synchronized into a single flow through a join node (also depicted as \nan orthogonal bar). The join node waits for all incoming flows to complete \nbefore proceeding. Different from sequence and communication diagrams, \nactivity diagrams don’t show the actual operations being performed on spe-\ncific objects. Activity diagrams are useful to broadly describe the steps in a \nspecific workflow. Conditional branching (diamond symbol) allows a single \ndiagram to represent multiple traces, although it’s not usually the intent of \nan activity diagram to show all possible traces or the complete behavior for \nthe system or part of it. \nIn contrast to trace notations, comprehensive models show the complete be-\nhavior of structural elements. Given this type of documentation, it is possible \nto infer all possible paths from initial state to final state. The state machine for-\nmalism represents the behavior of architecture elements because each state is an \nabstraction of all possible histories that could lead to that state. State machine \nlanguages allow you to complement a structural description of the elements of \nthe system with constraints on interactions and timed reactions to both internal \nand environmental stimuli.\nUML state machine diagram notation is based on the statechart graphical \nformalism developed by David Harel for modeling reactive systems; it allows \nyou to trace the behavior of your system, given specific inputs. A UML state ma-\nchine diagram shows states represented as boxes and transitions between states \nrepresented as arrows. The state machine diagrams help to model elements of the \narchitecture and help to illustrate their runtime interactions. Figure 18.6 is a sim-\nple example showing the states of a vehicle cruise control system. \n",
      "content_length": 2953,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "354  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nKey: UML\npress “–”\nto coast\npress “+” \nto accelerate\n press “set” or \n“resume” buttons\npress “cruise \non/off” button\npress “cruise \non/off” button\npress “cruise on/off” button\ntap brake pedal\npush \nthrottle\npedal\noff\non,\ndisengaged\non,\nengaged\nFigure 18.6  UML state machine diagram for the cruise control system of a \nmotor vehicle\nEach transition in a state machine diagram is labeled with the event caus-\ning the transition. For example, in Figure 18.6, the transitions correspond to the \nbuttons the driver can press or driving actions that affect the cruise control sys-\ntem. Optionally, the transition can specify a guard condition, which is enclosed in \nbrackets. When the event corresponding to the transition occurs, the guard condi-\ntion is evaluated and the transition is only enabled if the guard is true at that time. \nTransitions can also have consequences, called actions or effects, indicated by a \nslash. When an action is noted, it indicates that the behavior following the slash \nwill be performed when the transition occurs. The states may also specify entry \nand exit actions.\nOther notations exist for describing comprehensive behavior. For exam-\nple, Architecture Analysis and Design Language (AADL) can be used to reason \nabout runtime behavior. Specification and Description Language (SDL) is used \nin telephony. \n18.8  Architecture Documentation and Quality Attributes \nIf architecture is largely about the achievement of quality attributes and if one of \nthe main uses of architecture documentation is to serve as a basis for analysis (to \nmake sure the architecture will achieve its required quality attributes), where do \nquality attributes show up in the documentation? Short of a full-fledged quality \nview (see page 340), there are five major ways:\n1.\t\nAny major design approach (such as an architecture pattern) will have \nquality attribute properties associated with it. Client-server is good for \nscalability, layering is good for portability, an information-hiding-based \ndecomposition is good for modifiability, services are good for interopera-\nbility, and so forth. Explaining the choice of approach is likely to include \na discussion about the satisfaction of quality attribute requirements and \n",
      "content_length": 2323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "18.9  Documenting Architectures That Change Faster Than You Can Document Them  355\ntradeoffs incurred. Look for the place in the documentation where such an \nexplanation occurs. In our approach, we call that rationale.\n2.\t\nIndividual architectural elements that provide a service often have qual-\nity attribute bounds assigned to them. Consumers of the services need to \nknow how fast, secure, or reliable those services are. These quality attri-\nbute bounds are defined in the interface documentation for the elements, \nsometimes in the form of a service-level agreement. Or they may simply be \nrecorded as properties that the elements exhibit.\n3.\t\nQuality attributes often impart a “language” of things that you would look \nfor. Security involves security levels, authenticated users, audit trails, \nfirewalls, and the like. Performance brings to mind buffer capacities, dead-\nlines, periods, event rates and distributions, clocks and timers, and so on. \nAvailability conjures up mean time between failure, failover mechanisms, \nprimary and secondary functionality, critical and noncritical processes, and \nredundant elements. Someone fluent in the “language” of a quality attribute \ncan search for the kinds of architectural elements (and properties of those \nelements) that were put in place precisely to satisfy that quality attribute \nrequirement.\n4.\t\nArchitecture documentation often contains a mapping to requirements that \nshows how requirements (including quality attribute requirements) are sat-\nisfied. If your requirements document establishes a requirement for avail-\nability, for instance, then you should be able to look it up by name or refer-\nence in your architecture document to see the places where that requirement \nis satisfied.\n5.\t\nEvery quality attribute requirement will have a constituency of stakeholders \nwho want to know that it is going to be satisfied. For these stakeholders, the \narchitect should provide a special place in the documentation’s introduction \nthat either provides what the stakeholder is looking for, or tells the stake-\nholder where in the document to find it. It would say something like this: \n“If you are a performance analyst, you should pay attention to the processes \nand threads and their properties (defined [here]), and their deployment on \nthe underlying hardware platform (defined [here]).” In our documentation \napproach, we put this here’s-what-you’re-looking-for information in a sec-\ntion called the documentation roadmap.\n18.9  \u0007Documenting Architectures That Change \nFaster Than You Can Document Them\nWhen your web browser encounters a file type it’s never seen before, odds are \nthat it will go to the Internet, search for and download the appropriate plug-in to \nhandle the file, install it, and reconfigure itself to use it. Without even needing to \n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "356  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nshut down, let alone go through the code-integrate-test development cycle, the \nbrowser is able to change its own architecture by adding a new component. \nService-oriented systems that utilize dynamic service discovery and binding \nalso exhibit these properties. More challenging systems that are highly dynamic, \nself-organizing, and reflective (meaning self-aware) already exist. In these cases, \nthe identities of the components interacting with each other cannot be pinned \ndown, let alone their interactions, in any static architecture document.\nAnother kind of architectural dynamism, equally challenging from a docu-\nmentation perspective, is found in systems that are rebuilt and redeployed with \ngreat rapidity. Some development shops, such as those responsible for commer-\ncial websites, build and “go live” with their system many times every day. \nWhether an architecture changes at runtime, or as a result of a high-frequency \nrelease-and-deploy cycle, the changes occur much faster than the documentation \ncycle. In either case, nobody is going to hold up things until a new architecture \ndocument is produced, reviewed, and released. \nBut knowing the architecture of these systems is every bit as important, and \narguably more so, than for systems in the world of more traditional life cycles. \nHere’s what you can do if you’re an architect in a highly dynamic environment:\n■\n■Document what is true about all versions of your system. Your web brows-\ner doesn’t go out and grab just any piece of software when it needs a new \nplug-in; a plug-in must have specific properties and a specific interface. \nAnd it doesn’t just plug in anywhere, but in a predetermined location in \nthe architecture. Record those invariants as you would for any architecture. \nThis may make your documented architecture more a description of con-\nstraints or guidelines that any compliant version of the system must follow. \nThat’s fine.\n■\n■Document the ways the architecture is allowed to change. In the previous \nexamples, this will usually mean adding new components and replacing \ncomponents with new implementations. In the Views and Beyond approach, \nthe place to do this is called the variability guide (captured in Section 4 of \nour view template). \n18.10  \u0007Documenting Architecture in an \nAgile Development Project\n“Agile” refers to an approach to software development that emphasizes rapid and \nflexible development and de-emphasizes project and process infrastructure for their \nown sake. In Chapter 15 we discuss the relationships between architecture and Ag-\nile. Here we focus just on how to document architecture in an Agile environment.\n",
      "content_length": 2730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "18.10  Documenting Architecture in an Agile Development Project\n357\nThe Views and Beyond and Agile philosophies agree strongly on a central \npoint: If information isn’t needed, don’t document it. All documentation should \nhave an intended use and audience in mind, and be produced in a way that serves \nboth. One of the fundamental principles of technical documentation is “Write \nfor the reader.” That means understanding who will read the documentation and \nhow they will use it. If there is no audience, there is no need to produce the \ndocumentation.\nArchitecture view selection is an example of applying this principle. The \nViews and Beyond approach prescribes producing a view if and only if it ad-\ndresses the concerns of an explicitly identified stakeholder community.\nAnother central idea to remember is that documentation is not a monolithic \nactivity that holds up all other progress until it is complete. The view selection \nmethod given earlier prescribes producing the documentation in prioritized stages \nto satisfy the needs of the stakeholders who need it now.\nWhen producing Views and Beyond-based architecture documentation us-\ning Agile principles, keep the following in mind:\n■\n■Adopt a template or standard organization to capture your design decisions.\n■\n■Plan to document a view if (but only if) it has a strongly identified stake-\nholder constituency.\n■\n■Fill in the sections of the template for a view, and for information beyond \nviews, when (and in whatever order) the information becomes available. \nBut only do this if writing down this information will make it easier (or \ncheaper or make success more likely) for someone downstream doing their \njob.\n■\n■Don’t worry about creating an architectural design document and then a \nfiner-grained design document. Produce just enough design information to \nallow you to move on to code. Capture the design information in a format \nthat is simple to use and simple to change—a wiki, perhaps. \n■\n■Don’t feel obliged to fill up all sections of the template, and certainly not \nall at once. We still suggest you define and use rich templates because they \nmay be useful in some situations. But you can always write “N/A” for the \nsections for which you don’t need to record the information (perhaps be-\ncause you will convey it orally). \n■\n■Agile teams sometimes make models in brief discussions by the white-\nboard. When documenting a view, the primary presentation may consist of \na digital picture of the whiteboard. Further information about the elements \n(element catalog), rationale discussion (architecture background), variabil-\nity mechanisms being used (variability guide), and all else can be com-\nmunicated verbally to the team—at least for now. Later on, if you find out \nthat it’s useful to record a piece of information about an element, a context \ndiagram, rationale for a certain design decision, or something else, the tem-\nplate will have the right place ready to receive it. \n",
      "content_length": 2967,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "358  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\nThe Software You’re Delivering Isn’t the Only Software That Matters\nAbout ninety-nine percent of the treatment of architecture in this book (and \nothers) is concerned with the software elements that make up the opera-\ntional system that is delivered to its customer. Component-and-connector \nviews show the units of runtime behavior of that system. Module views \nshow the units of implementation that have to be built in order to create \nthat system.\nA colleague of mine is a project manager for a Fortune 500 software \ncompany. On the day I wrote this sidebar, she found out that the develop-\nment platform her project relied on had been infected with a virulent new \nvirus, and the company’s IT department was removing it from service, \nalong with all the backup images, until the virus could be completely re-\nmoved. That was going to take about five days. After that, all of her project’s \nsoftware and tooling would have to be reinstalled and brought back up to \nlatest-version status. Her project was in user final acceptance test, racing \nagainst a delivery deadline, and the IT department’s decision doomed \nher project to join the countless others in our industry that are delivered \nlate. The snarling email she sent to the IT department for (a) allowing the \nplatform to become infected and (b) not providing a backup platform (real \nor virtual) in a timely fashion would melt your screen.\nThe treatment of software architecture we describe in this book is \nperfectly capable of representing and usefully incorporating software other \nthan the software that your customer is paying you to deliver. Allocation \nviews, recall, are about mapping that software to structures in the envi-\nronment. “Uses” views show which software elements rely on the correct \npresence of other software in order to work. Context diagrams are all about \nshowing relations between your system and important elements of its envi-\nronment. It would be the easiest thing in the world to use these constructs \nto represent support software including, in my friend’s case, the develop-\nment platform.\nAn avionics project I worked on years ago included in our decomposition \nview a module called the System Generation Module. This consisted of all \nof the software we needed to construct a loadable image of the product \nwe were building. Not a single byte of code from the System Generation \nModule made it onto the aircraft, but it was as important as any other. \nEven if you don’t build any of your support software but use off-the-shelf \ndevelopment tools from your favorite vendor, someone in your organiza-\ntion is responsible for the care and feeding of that software: its acquisition, \ninstallation, configuration, and upgrade. That constitutes a nontrivial work \nassignment, which suggests that support software also belongs in the work \nassignment view (a kind of allocation view). And of course you always build \nsome of it yourself—test scripts, build scripts, and so forth—so it’s even \nmore deserving of a place in your architecture.\n",
      "content_length": 3118,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "18.11  Summary\n359\nPromoting support and development software to first-class architec-\ntural status makes us ask the right questions about it, especially the most \nimportant one: What quality attributes do we require of it? Will it provide \nus with the right security if (for example) we want to exclude our subcon-\ntracting partners from access to some of our IP during development? Will it \nhave the availability to be up and running at 2 a.m. Sunday morning when \nour project goes into its inevitable final delivery crunch? And if it crashes, \nwill the IT folks have someone standing by to bring it back up? Will it be \nmodifiable or configurable enough to support the way your project intends \nto use it? \nThink about what other software and environmental resources your proj-\nect depends on, and consider using the architectural tools, models, views, \nand concepts at your disposal to help you do what architecture always \nhelps you do: Ask the right questions at the right time to expose risks and \nbegin to mitigate them. These concepts include quality attribute scenarios, \n“uses” views, and deployment and work assignment views that include \nsupport software.\n—PCC\n18.11  Summary\nWriting architectural documentation is much like other types of writing. You \nmust understand the uses to which the writing is to be put and the audience for \nthe writing. Architectural documentation serves as a means for communication \namong various stakeholders, not only up the management chain and down to the \ndevelopers but also across to peers. \nAn architecture is a complicated artifact, best expressed by focusing on par-\nticular perspectives depending on the message to be communicated. These per-\nspectives are called views, and you must choose the views to document, must \nchoose the notation to document these views, and must choose a set of views that \nis both minimal and adequate. This may involve combining various views that \nhave a large overlap. You must document not only the structure of the architecture \nbut also the behavior.\nOnce you have decided on the views, you must decide how to package the \ndocumentation. The packaging will depend on the media used for expressing the \ndocumentation. Print has different characteristics for understanding and group-\ning than various online media. Different online media will also have different \ncharacteristics.\nThe context of the project will also affect the documentation. Some of the \ncontextual factors are the important quality attributes of the system, the rate of \nchange of the system, and the project management strategy.\n",
      "content_length": 2587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "360  Part Three  Architecture in the Life Cycle\t 18—Documenting Software Architectures\n18.12  For Further Reading\nDocumenting Software Architectures (second edition) [Clements 10a] is a com-\nprehensive treatment of the Views and Beyond approach. It describes a multitude \nof different views and notations for them. It also describes how to package the \ndocumentation into a coherent whole.\nISO/IEC/IEEE 42010:2011 (“eye-so-forty-two-ten” for short) is the ISO \n(and IEEE) standard [ISO 11] Systems and software engineering—Architecture \ndescription. The first edition of that standard, IEEE Std. 1471-2000, was devel-\noped by an IEEE working group drawing on experience from industry, academia, \nand other standards bodies between 1995 and 2000. ISO/IEC/IEEE 42010 is cen-\ntered on two key ideas: a conceptual framework for architecture description and a \nstatement of what information must be found in any ISO/IEC/IEEE 42010-com-\npliant architecture description, using multiple viewpoints driven by stakeholders’ \nconcerns.\nUnder ISO/IEC/IEEE 42010, as in the Views and Beyond approach, views \nhave a central role in documenting software architecture. The architecture de-\nscription of a system includes one or more views.\nIf you want to use the Views and Beyond approach to produce an ISO/IEC/\nIEEE 42010-compliant architecture document, you certainly can. The main addi-\ntional obligation is to choose and document a set of viewpoints, identifying the \nstakeholders, their concerns, and the elements catalog for each view, and (to a \nlesser degree) address ISO/IEC/IEEE 42010’s other required information content. \nAADL is an SAE standard. The SAE is an organization for engineering pro-\nfessionals in the aerospace, automotive, and commercial vehicle industries. The \nwebsite for the AADL standard is at www.aadl.info.\nSDL is a notation used in the telecom industry. It is targeted at describing \nthe behavior of reactive and distributed systems in general and telecom systems \nin particular. A real-time version of SDL can be found at www.sdl-rt.org/stan-\ndard/V2.2/pdf/SDL-RT.pdf.\nUML 2.0 added several features specifically to allow architecture to be mod-\neled, such as ports. It is managed by the Object Management Group and can be \nfound at www.omg.org/spec/UML/.\n18.13  Discussion Questions\n1.\t\nGo to the website of your favorite open source system. On the site, look \nfor the architectural documentation for that system. What is there? What \nis missing? How would this affect your ability to contribute code to this \nproject?\n",
      "content_length": 2538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "18.13  Discussion Questions\n361\n2.\t\nBanks are justifiably cautious about security. Sketch the documentation you \nwould need for an automatic teller machine (ATM) in order to reason about \nits security architecture.\n3.\t\nSuppose your company has just purchased another company and that you \nhave been given the task of merging a system in your company with a simi-\nlar system in the other company. What views of the other system’s architec-\nture would you like to see and why? Would you ask for the same views of \nboth systems?\n4.\t\nWhen would you choose to document behavior using trace models or using \ncomprehensive models? What value do you get and what effort is required \nfor each of them?\n5.\t\nHow much of a project’s budget would you devote to software architecture \ndocumentation? Why? How would you measure the cost and the benefit?\n6.\t\nAntony Tang, an architect and one of the reviewers of this book, says that \nhe has used a development view—a kind of quality view—that describes \nhow the software should be developed in relation to the use of tools and \ndevelopment workflows, the use of standard library routines such as for ex-\nception handling, some coding conventions and standards, and some testing \nand deployment conventions. Sketch a definition of a development view.\n",
      "content_length": 1285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "363\n19\nArchitecture, \nImplementation, \nand Testing\nYou don’t make progress by standing on the \nsidelines, whimpering and complaining. You \nmake progress by implementing ideas.\n—Shirley Hufstedler\nAlthough this is a book about software architecture—you’ve noticed that by now, \nno doubt—we need to remind ourselves from time to time that architecture is \nnot a goal unto itself, but only the means to an end. Building systems from the \narchitecture is the end game, systems that have the qualities necessary to meet the \nconcerns of their stakeholders.\nThis chapter covers two critical areas in system-building—implementation \nand testing—from the point of view of architecture. What is the relationship of \narchitecture to implementation (and vice versa)? What is the relationship of ar-\nchitecture to testing (and vice versa)?\n19.1  Architecture and Implementation\nArchitecture is intended to serve as the blueprint for implementation. The sidebar \n“Potayto, Potahto . . .” makes the point that architectures and implementations \nrely on different sets of vocabulary, which results in development tools usually \nserving one community or the other fairly well, but not both. Frequently the \nimplementers are so engrossed in their immediate task at hand that they make \n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "364 \nPart Three\t\n19—Architecture, Implementation, and Testing\nimplementation choices that degrade the modular structure of the architecture, \nfor example.\nThis leads to one of the most frustrating situations for architects. It is very \neasy for code and its intended architecture to drift apart; this is sometimes called \n“architecture erosion.” This section talks about four techniques to help keep the \ncode and the architecture consistent.\nEmbedding the Design in the Code\nA key task for implementers is to faithfully execute the prescriptions of the ar-\nchitecture. George Fairbanks, in Just Enough Architecture, prescribes using an \n“architecturally-evident coding style.” Throughout the code, implementers can \ndocument the architectural concept or guidance that they’re reifying. That is, they \ncan “embed” the architecture in their implementations. They can also try to local-\nize the implementation of each architectural element, as opposed to scattering it \nacross different implementation entities.\nThis practice is made easier if implementers (consistently across a project) \nadopt a set of conventions for how architectural concepts “show up” in code. For \nexample, identifying the layer to which a code unit belongs will make it more \nlikely that implementers and maintainers will respect (and hence not violate) the \nlayering.\nFrameworks\n“Framework” is a terribly overused term, but here we mean a reusable set of \nlibraries or classes for a software system. “Library” and “class” are implementa-\ntion-like terms, but frameworks have broad architectural implications—they are a \nplace where architecture and implementation meet. The classes (in an object-ori-\nented framework) are appropriate to the application domain of the system that is \nbeing constructed. Frameworks can range from small and straightforward (such \nas ones that provide a set of standard and commonly used data types to a system) \nto large and sophisticated. For example, the AUTomotive Open System ARchi-\ntecture (AUTOSAR) is a framework for automotive software, jointly developed \nby automobile manufacturers, suppliers, and tool developers.\nFrameworks that are large and sophisticated often encode architectural in-\nteraction mechanisms, by encoding how the classes (and the objects derived from \nthem) communicate and synchronize with each other. For example, AUTOSAR is \nan architecture and not (just) an architecture framework.\nA framework amounts to a substantial (in some cases, enormous) piece \nof reusable software, and it brings with it all of the advantages of reuse: saving \ntime and cost, avoiding a costly design task, encoding domain knowledge, and \ndecreasing the chance of errors from individual implementers coding the same \nthing differently and erroneously. On the other hand, frameworks are difficult to \n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "19.1  Architecture and Implementation\n365\ndesign and get correct. Adopting a framework means investing in a selection pro-\ncess as well as training, and the framework may not provide all the functionality \nthat you require. The learning curve for a framework is often extremely steep. \nA framework that provides a complete set of functionality for implementing an \napplication in a particular domain is called a “platform.” \nCode Templates\nA template provides a structure within which some architecture-specific func-\ntionality is achieved, in a consistent fashion system-wide. Many code generators, \nsuch as user interface builders, produce a template into which a developer inserts \ncode, although templates can also be provided by the development environment.\nSuppose that an architecture for a high-availability system prescribes that \nevery component that implements a critical responsibility must use a failover \ntechnique that switches control to a backup copy of itself in case a fault is de-\ntected in its operation.\nThe architecture could, and no doubt would, describe the failover protocol. \nIt might go something like this:\nIn the event that a failure is detected in a critical-application component, a \nswitchover occurs as follows:\n1.\t\nA secondary copy, executing in parallel in background on a different pro-\ncessor, is promoted to the new primary.\n2.\t\nThe new primary reconstitutes with the application’s clients by sending \nthem a message that means, essentially: The operational unit that was \nserving you has had a failure. Were you waiting for anything from us at the \ntime? It then proceeds to service any requests received in response.\n3.\t\nA new secondary is started to serve as a backup for the new primary.\n4.\t\nThe newly started secondary announces itself to the new primary, which \nstarts sending it messages as appropriate to keep it up to date while it is \nexecuting in background.\nIf failure is detected within a secondary, a new one is started on some other \nprocessor. It coordinates with its primary and starts receiving state data.\nEven though the primary and secondary copies are never doing the same \nthing at the same time (the primary is performing its duty and sending state up-\ndates to its backups, and the secondaries are waiting to leap into action and ac-\ncepting state updates), both components come from identical copies of the same \nsource code. \nTo accomplish this, the coders of each critical component would be ex-\npected to implement that protocol. However, a cleverer way is to give the coder \na code template that contains the tricky failover part as boilerplate and contains \nfill-in-the-blank sections where coders can fill in the implementation for the func-\ntionality that is unique to each application. This template could be embedded in \n",
      "content_length": 2795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "366 \nPart Three\t\n19—Architecture, Implementation, and Testing\nthe development environment so that when the developer specifies that the mod-\nule being developed is to support a failover protocol, the template appears as the \ninitial code for the module.\nAn example of such a template, taken from an air traffic control system, \nis illustrated in Figure 19.1. The structure is a continuous loop that services in-\ncoming events. If the event is one that causes the application to take a normal \n(non-fault-tolerance-related) action, it carries out the appropriate action, followed \nby an update of its backup counterparts’ data so that the counterpart can take \nover if necessary. Most applications spend most of their time processing normal \nevents. Other events that may be received involve the transfer (transmission and \nreception) of state and data updates. Finally, there is a set of events that involves \nboth the announcement that this unit has become the primary and requests from \nclients for services that the former (now failed) primary did not complete.\nUsing a template has architectural implications: it makes it simple to add \nnew applications to the system with a minimum of concern for the actual work-\nings of the fault-tolerant mechanisms designed into the approach. Coders and \nmaintainers of applications do not need to know about message-handling mecha-\nnisms except abstractly, and they do not need to ensure that their applications are \nfault tolerant—that has been handled architecturally.\nCode templates have implications for reliability: once the template is de-\nbugged, then entire classes of coding errors across the entire system disappear. \nBut in the context of this discussion, templates represent a true common ground \nwhere the architecture and the implementation come together in a consistent and \nuseful fashion.\nKeeping Code and Architecture Consistent\nCode can drift away from architecture in a depressingly large number of ways. \nFirst, there may be no constraints imposed on the coders to follow the archi-\ntecture. This makes no apparent sense, for why would we bother to invest in an \narchitecture if we aren’t going to use it to constrain the code? However, this hap-\npens more often than you might think. Second, some projects use the published \narchitecture to start out, but when problems are encountered (either technical or \nschedule-related), the architecture is abandoned and coders scramble to field the \nsystem as best they can. Third (and perhaps most common), after the system has \nbeen fielded, changes to it are accomplished with code changes only, but these \nchanges affect the architecture. However, the published architecture is not up-\ndated to guide the changes, nor updated afterward to keep up with them. \nOne simple method to remedy the lack of updating the architecture is to \nnot treat the published architecture as an all-or-nothing affair—it’s either all cor-\nrect or all useless. Parts of the architecture may become out of date, but it will \nhelp enormously if those parts are marked as “no longer applicable” or “to be \nrevised.” Conscientiously marking sections as out of date keeps the architecture \n",
      "content_length": 3173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "19.1  Architecture and Implementation\n367\ndocumentation a living document and (paradoxically) sends a stronger message \nabout the remainder: it is still correct and can still be trusted.\nterminate:= false\ninitialize application/application protocols\nask for current state (image request)\nLoop\nGet_event\nCase Event_Type is\n-- “normal” (non-fault-tolerant-related) requests to \n-- perform actions; only happens if this unit is the\n-- current primary address space\nwhen X => Process X\nSend state data updates to other address spaces\nwhen Y => Process Y\nSend state data updates to other address spaces\n...\nwhen Terminate_Directive => clean up resources; terminate  \n          := true\nwhen State_Data_Update => apply to state data\n-- will only happen if this unit is a secondary address\n-- space, receiving the update from the primary after it\n-- has completed a “normal” action sending, receiving \n-- state data\nwhen Image_Request => send current state data to new  \n          address space\nwhen State_Data_Image => Initialize state data\nwhen Switch_Directive => notify service packages of  \n          change in rank\n-- these are requests that come in after a PAS/SAS\n-- switchover; they report services that they had\n-- requested from the old (failed) PAS which this unit \n-- (now the PAS) must complete. A, B, etc. are the names \n-- of the clients.\nwhen Recon_from_A => reconstitute A\nwhen Recon_from_B => reconstitute B\n...\nwhen others => log error\nend case\nexit when terminate\nend loop\nFigure 19.1  A code template for a failover protocol.  “Process X” and \n“Process Y” are placeholders for application-specific code.\n",
      "content_length": 1618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "368 \nPart Three\t\n19—Architecture, Implementation, and Testing\nIn addition, strong management and process discipline will help prevent ero-\nsion. One way is to mandate that changes to the system, no matter when they oc-\ncur, are vetted through the architecture first. The alternatives for achieving code \nalignment with the architecture include the following:\n■\n■Sync at life-cycle milestone. Developers change the code until the end of \nsome phase, such as a release or end of an iteration. At that point, when the \nschedule pressure is less, the architecture is updated.\n■\n■Sync at crisis. This undesirable approach happens when a project has found \nitself in a technical quagmire and needs architectural guidance to get itself \ngoing again. \n■\n■Sync at check-in. Rules for the architecture are codified and used to vet any \ncheck-in. When a change to the code “breaks” the architecture rules, key \nproject stakeholders are informed and then either the code or the architec-\nture rules must be modified. This process is typically automated by tools.\nThese alternatives can work only if the implementation follows the archi-\ntecture mostly, departing from it only here and there and in small ways. That is, \nit works when syncing the architecture involves an update and not a wholesale \noverhaul or do-over.\nPotayto, Potahto, Tomayto, Tomahto— \nLet’s Call the Whole Thing Off!\nOne of the most vexing realities about architecture-based software de-\nvelopment is the gulf between architectural and implementation ontolo-\ngies, the set of concepts and terms inherent in an area. Ask an architect \nwhat concepts they work with all day, and you’re likely to hear things like \nmodules, components, connectors, stakeholders, evaluation, analysis, \ndocumentation, views, modeling, quality attributes, business goals, and \ntechnology roadmaps.\nAsk an implementer the same question, and you likely won’t hear any of \nthose words. Instead you’ll hear about objects, methods, algorithms, data \nstructures, variables, debugging, statements, code comments, compilers, \ngenerics, operator overloading, pointers, and build scripts.\nThis is a gap in language that reflects a gap in concepts. This gap is, in \nturn, reflected in the languages of the tools that each community uses. UML \nstarted out as a way to model object-oriented designs that could be quickly \nconverted to code—that is, UML is conceptually “close” to code. Today it is \na de facto architecture description language, and likely the most popular \none. But it has no built-in concept for the most ubiquitous of architectural \nconcepts, the layer. If you want to represent layers in UML, you have to adopt \nsome convention to do it. Packages stereotyped as <<layer>>, associated \nwith stereotyped <<allowed to use>> dependencies do the trick. But it is a \ntrick, a workaround for a language deficiency. UML has “connectors,” two of \n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "19.1  Architecture and Implementation\n369\nthem in fact. But they are a far cry from what architects think of as connec-\ntors. Architectural connectors can and do have rich functionality. For instance, \nan enterprise service bus (ESB) in a service-oriented architecture handles \nrouting, data and format transformation, technology adaptation, and a host of \nother work. It is most natural to depict the ESB as a connector tying together \nservices that interact with each other through it. But UML connectors are \nimpoverished things, little more than bookkeeping mechanisms that have no \nfunctionality whatsoever. The delegation connector in UML exists merely to \nassociate the ports of a parent component with ports of its nested children, \nto send inputs from the outside into a child’s input port, and outputs from a \nchild to the output port of the parent. And the assembly connector simply ties \ntogether one component’s “requires” interface with another’s “provides” inter-\nface. These are no more than bits of string to tie two components together. To \nrepresent a true architectural connector in UML, you have to adopt a conven-\ntion—another workaround—such as using simple associations tagged with \nexplanatory annotations, or abandon the architectural concept completely \nand capture the functionality in another component. \nPart of the concept gap between architecture and implementation is inevi-\ntable. Architectures, after all, are abstractions of systems and their implemen-\ntations. Back in Chapter 2, we said that was one of the valuable properties \nof architecture: you could build many different systems from one. And that’s \nwhat an abstraction is: a one-to-many mapping. One abstraction, many \ninstances; one architecture, many implementations. That architecture is an \nabstraction of implementation is almost its whole point: architecture lets us \nachieve intellectual control over a system without having to capture, let alone \nmaster, all of the countless and myriad truths about its implementation.\nAnd here comes the gap again: All of those truths about its implementa-\ntion are what coders produce for a living, without which the system remains \nbut an idea. Architects, on the other hand, dismiss all of that reality by \nannouncing that they are not interested in implementation “details.” \nCan’t we all get along?\nWe could. There is nothing inherently impossible about a language that \nembraces architectural as well as coding concepts, and several people have \nproposed some. But UML is beastly difficult to change, and programming \nlanguage purveyors all seem to focus their attention down on the underlying \nmachine and not up to the architecture that is directing the implementation. \nUntil this gap is resolved, until architects and coders (and their tools) \nspeak the same conceptual language, we are likely to continue to deal with \nthe most vexing result of this most vexing reality: writing code (or introducing \na code change) that ignores the architecture is the easiest thing in the world. \nThe good news is that even though architecture and implementation \nspeak different languages, they aren’t languages from different planets. \nConcepts in one ontology usually correspond pretty well to concepts in an-\nother. Frameworks are an area where the languages enjoy a fair amount of \noverlap. So are interfaces. These constructs live on the cusp of the two do-\nmains, and provide hope that we might one day speak the same language.\n—PCC\n",
      "content_length": 3474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "370 \nPart Three\t\n19—Architecture, Implementation, and Testing\n19.2  Architecture and Testing\nWhat is the relationship between architecture and testing? One possible answer is \n“None,” or “Not much.” Testing can be seen as the process of making sure that a \nsoftware system meets its requirements, that it brings the necessary functionality \n(endowed with the necessary quality attributes) to its user community. Testing, \nseen this way, is simply connected to requirements, and hardly connected to ar-\nchitecture at all. As long as the system works as expected, who cares what the ar-\nchitecture is? Yes, the architecture played the leading role in getting the system to \nwork as expected, thank you very much, but once it has played that role it should \nmake a graceful exit off the stage. Testers work with requirements: Thanks, archi-\ntecture, but we’ll take it from here. \nNot surprisingly, we don’t like that answer. This is an impoverished view of \ntesting, and in fact an unrealistic one as well. As we’ll see, architecture cannot \nhelp but play an important role in testing. Beyond that, though, we’ll see that \narchitecture can help make testing less costly and more effective when embraced \nin testing activities. We’ll also see what architects can do to help testers, and what \ntesters can do to take advantage of the architecture.\nLevels of Testing and How Architecture Plays a Role in Each\nThere are “levels” of testing, which range from testing small, individual pieces in \nisolation to an entire system. \n■\n■Unit testing refers to tests run on specific pieces of software. Unit testing is \nusually a part of the job of implementing those pieces. In fact, unit tests are \ntypically written by developers themselves. When the tests are written be-\nfore developing the unit, this practice is known as test-driven development. \nCertifying that a unit has passed its unit tests is a precondition for delivery \nof that unit to integration activities. Unit tests test the software in a standalone \nfashion, often relying on “stubs” to play the role of other units with which the \ntested unit interacts, as those other units may not yet be available. Unit tests \nwon’t usually catch errors dealing with the interaction between elements—\nthat comes later—but unit tests provide confidence that each of the system’s \nbuilding blocks is exhibiting as much correctness as is possible on its own.\nA unit corresponds to an architectural element in one of the architec-\nture’s module views. In object-oriented software, a unit might correspond to \na class. In a layered system, a unit might correspond to a layer, or a part of \na layer. Most often a unit corresponds to an element at the leaf of a module \ndecomposition tree.\nArchitecture plays a strong role in unit testing. First, it defines the units: \nthey are architectural elements in one or more of the module views. Second, \nit defines the responsibilities and requirements assigned to each unit. \n",
      "content_length": 2956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "19.2  Architecture and Testing\n371\nModifiability requirements can also be tested at unit test time. How long \nit will take to make specified changes can be tested, although this is seldom \ndone in practice. If specified changes take too long for the developers to \nmake, imagine how long they will take when a new and separate mainte-\nnance group is in charge without the intimate knowledge of the modules.\nAlthough unit testing goes beyond architecture (tests are based on \nnonarchitectural information such as the unit’s internal data structures, \nalgorithms, and control flows), they cannot begin their work without the \narchitecture.\n■\n■Integration testing tests what happens when separate software units start to \nwork together. Integration testing concentrates on finding problems related \nto the interfaces between elements in a design. Integration testing is inti-\nmately connected to the specific increments or subsets that are planned in a \nsystem’s development. \nThe case where only one increment is planned, meaning that integration \nof the entire system will occur in a single step, is called “big bang integra-\ntion” and has largely been discredited in favor of integrating many incre-\nmentally larger subsets. Incremental integration makes locating errors much \neasier, because any new error that shows up in an integrated subset is likely \nto live in whatever new parts were added this time around. \nAt the end of integration testing, the project has confidence that the \npieces of software work together correctly and provide at least some correct \nsystem-wide functionality (depending on how big a subset of the system is \nbeing integrated). Special cases of integration testing are these:\n■\n■System testing, which is a test of all elements of the system, including \nsoftware and hardware in their intended environment\n■\n■Integration testing that involves third-party software\nOnce again, architecture cannot help but play a strong role in integration \ntesting. First, the increments that will be subject to integration testing must \nbe planned, and this plan will be based on the architecture. The uses view is \nparticularly helpful for this, as it shows what elements must be present for a \nparticular piece of functionality to be fielded. That is, if the project requires \nthat (for example) in the next increment of a social networking system users \nwill be able to manage photographs they’ve allowed other users to post in \ntheir own member spaces, the architect can report that this new functionality \nis part of the user_permissions module, which will use a new part of the \nphoto_sharing module, which in turn will use a new structure in the mas-\nter user_links database, and so forth. Project management will know, then, \nthat all of the software must be ready for integration at the same time.\nSecond, the interfaces between elements are part of the architecture, and \nthose interfaces determine the integration tests that are created and run. \nIntegration testing is where runtime quality attribute requirements can \nbe tested. Performance and reliability testing can be accomplished. A \n",
      "content_length": 3117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "372 \nPart Three\t\n19—Architecture, Implementation, and Testing\nsophisticated test harness is useful for performing these types of tests. How \nlong does an end-to-end synchronization of a local database with a global \ndatabase take? What happens if faults are injected into the system? What \nhappens when a process fails? All of these conditions can be tested at inte-\ngration time.\nIntegration testing is also the time to test what happens when the system \nruns for an extended period. You could monitor resource usage during the \ntesting and look for resources that are consumed but not freed. Does your \npool of free database connections decrease over time? Then maybe data-\nbase connections should be managed more aggressively. Does the thread \npool show signs of degradation over time? Ditto.\n■\n■Acceptance testing is a kind of system testing that is performed by users, \noften in the setting in which the system will run. Two special cases of ac-\nceptance testing are alpha and beta testing. In both of these, users are given \nfree rein to use the system however they like, as opposed to testing that \noccurs under a preplanned regimen of a specific suite of tests. Alpha testing \nusually occurs in-house, whereas beta testing makes the system available to \na select set of end users under a “User beware” proviso. Systems in beta test \nare generally quite reliable—after all, the developing organization is highly \nmotivated to make a good first impression on the user community—but us-\ners are given fair warning that the system might not be bug-free or (if “bug-\nfree” is too lofty a goal) at least not up to its planned quality level.\nArchitecture plays less of a role in acceptance testing than at the other \nlevels, but still an important one. Acceptance testing involves stressing the \nsystem’s quality attribute behavior by running it at extremely heavy loads, \nsubjecting it to security attacks, depriving it of resources at critical times, \nand so forth. A crude analogy is that if you want to bring down a house, you \ncan whale away at random walls with a sledgehammer, but your task will \nbe accomplished much more efficiently if you consult the architecture first \nto find which of the walls is holding up the roof. (The point of testing is, \nafter all, to “bring down the house.”)\nOverlaying all of these types of testing is regression testing, which is testing \nthat occurs after a change has been made to the system. The name comes from \nthe desire to uncover old bugs that might resurface after a change, a sign that the \nsoftware has “regressed” to a less mature state. Regression testing can occur at \nany of the previously mentioned levels, and often consists of rerunning the bank \nof tests and checking for the occurrence of old (or for that matter, new) faults.\nBlack-Box and White-Box Testing\nTesting (at any level) can be “black box” or “white box.” Black-box testing \ntreats the software as an opaque “black box,” not using any knowledge about the \n",
      "content_length": 2979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "19.2  Architecture and Testing\n373\ninternal design, structure, or implementation. The tester’s only source of informa-\ntion about the software is its requirements.\nArchitecture plays a role in black-box testing, because it is often the archi-\ntecture document where the requirements for a piece of the system are described. \nAn element of the architecture is unlikely to correspond one-to-one with a re-\nquirement nicely captured in a requirements document. Rather, when the archi-\ntect creates an architectural element, he or she usually assigns it an amalgamation \nof requirements, or partial requirements, to carry out. In addition, the interface to \nan element also constitutes a set of “requirements” for it—the element must hap-\npily accept the specified parameters and produce the specified effect as a result. \nTesters performing black-box testing on an architectural element (such as a major \nsubsystem) are unlikely to be able to do their jobs using only requirements pub-\nlished in a requirements document. They need the architecture as well, because \nthe architecture will help the tester understand what portions of the requirements \nrelate to the specified subsystem.\nWhite-box testing makes full use of the internal structures, algorithms, and \ncontrol and data flows of a unit of software. Tests that exercise all control paths \nof a unit of software are a primary example of white-box testing. White-box test-\ning is most often associated with unit testing, but it has a role at higher levels as \nwell. In integration testing, for example, white-box testing can be used to con-\nstruct tests that attempt to overload the connection between two components by \nexploiting knowledge about how a component (for example) manages multiple \nsimultaneous interactions. \nGray-box testing lies, as you would expect, between black and white. Tes-\nters get to avail themselves of some, but not all, of the internal structure of a \nsystem. For example, they can test the interactions between components but not \nemploy tests based on knowledge of a component’s internal data structures.\nThere are advantages and disadvantages with each kind of testing. Black-\nbox testing is not biased by a design or implementation, and it concentrates on \nmaking sure that requirements are met. But it can be inefficient by (for example) \nrunning many unit tests that a simple code inspection would reveal to be unnec-\nessary. White-box testing often keys in on critical errors more quickly, but it can \nsuffer from a loss of perspective by concentrating tests to make the implemen-\ntation break, but not concentrating on the software delivering full functionality \nunder all points in its input space.\nRisk-based Testing\nRisk-based testing concentrates effort on areas where risk is perceived to be the \nhighest, perhaps because of immature technologies, requirements uncertainty, de-\nveloper experience gaps, and so forth. Architecture can inform risk-based testing \nby contributing categories of risks to be considered. Architects can identify areas \nwhere architectural decisions (if wrong) would have a widespread impact, where \n",
      "content_length": 3122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "374 \nPart Three\t\n19—Architecture, Implementation, and Testing\narchitectural requirements are uncertain, quality attributes are demanding on the \narchitecture, technology selections risky, or third-party software sources unre-\nliable. Architecturally significant requirements are natural candidates for risk-\nbased test cases. If the architecturally significant requirements are not met, then \nthe system is unacceptable, by definition.\nTest Activities\nTesting, depending on the project, can consume from 30 to 90 percent of a devel-\nopment’s schedule and budget. Any activity that gobbles resources as voraciously \nas that doesn’t just happen, of course, but needs to be planned and carried out \npurposefully and as efficiently as possible. Here are some of the activities associ-\nated with testing:\n■\n■Test planning. Test activities have to be planned so that appropriate resourc-\nes can be allocated. “Resources” includes time in the project schedule, \nlabor to run the tests, and technology with which the testing will be carried \nout. Technology might include test tools, automatic regression testers, test \nscript builders, test beds, test equipment or hardware such as network sniff-\ners, and so forth.\n■\n■Test development. This is an activity in which the test procedures are writ-\nten, test cases are chosen, test datasets are created, and test suites are script-\ned. The tests can be developed either before or after development. Develop-\ning the tests prior to development and then developing a module to satisfy \nthe test is a characteristic of test-first development. \n■\n■Test execution. Here, testers apply the tests to the software and capture and \nrecord errors. \n■\n■Test reporting and defect analysis. Testers report the results of specific tests \nto developers, and they report overall metrics about the test results to the \nproject’s technical management. The analysis might include a judgment \nabout whether the software is ready for release. Defect analysis is done by \nthe development team usually along with the customer, to adjudicate dispo-\nsition of each discovered fault: fix it now, fix it later, don’t worry about it, \nand so on. \n■\n■Test harness creation. One of the architect’s common responsibilities is to \ncreate, along with the architecture, a set of test harnesses through which \nelements of the architecture may be conveniently tested. Such test harness-\nes typically permit setting up the environment for the elements to be tested, \nalong with controlling their state and the data flowing into and out of the \nelements.\nOnce again, architecture plays a role and informs each of these activities; \nthe architect can contribute useful information and suggestions for each. For \ntest planning, the architecture provides the list of software units and incremental \n",
      "content_length": 2798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "19.2  Architecture and Testing\n375\nsubsets. The architect can also provide insight as to the complexity or, if the soft-\nware does not yet exist, the expected complexity of each of the software units. \nThe architect can also suggest useful test technologies that will be compatible \nwith the architecture; for example, Java’s ability to support assertions in the code \ncan dramatically increase software testability, and the architect can provide ar-\nguments for or against adopting that technology. For test development, the ar-\nchitecture can make it easy to swap datasets in and out of the system. Finally, \ntest reporting and defect analysis are usually reported in architectural terms: this \nelement passed all of its tests, but that element still has critical errors showing. \nThis layer passed the delivery test, but that layer didn’t. And so forth.\nThe Architect’s Role\nHere are some of the things an architect can do to facilitate quality testing. First \nand foremost, the architect can design the system so that it is highly testable. \nThat is, the system should be designed with the quality attribute of testability in \nmind. Applying the cardinal rule of architecture (“Know your stakeholders!”), \nthe architect can work with the test team (and, to the extent they have a stake in \ntesting, other stakeholders) to establish what is needed. Together, they can come \nup with a definition of the testability requirements using scenarios, as described \nin Chapter 10. Testability requirements are most likely to be a concern of the de-\nveloping organization and not so much of the customer or users, so don’t expect \nto see many testing requirements in a requirements document. Using those test-\nability requirements, the testability tactics in Chapter 10 can be brought to bear to \nprovide the testability needed. \nIn addition to designing for testability, the architect can also do these other \nthings to help the test effort:\n■\n■Insure that testers have access to the source code, design documents, and \nthe change records.\n■\n■Give testers the ability to control and reset the entire dataset that a program \nstores in a persistent database. Reverting the database to a known state is \nessential for reproducing bugs or running regression tests. Similarly, load-\ning a test bed into the database is helpful. Even products that don’t use da-\ntabases can benefit from routines to automatically preload a set of test data. \nOne way to achieve this is to design a “persistence layer” so that the whole \nprogram is database independent. In this way, the entire database can be \nswapped out for testing, even using an in-memory database if desired.\n■\n■Give testers the ability to install multiple versions of a software product on \na single machine. This helps testers compare versions, isolating when a bug \nwas introduced. In distributed applications, this aids testing deployment \nconfigurations and product scalability. This capability could require con-\nfigurable communication ports and provisions for avoiding collisions over \nresources such as the registry. \n",
      "content_length": 3071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "376 \nPart Three\t\n19—Architecture, Implementation, and Testing\nAs a practical matter, the architect cannot afford to ignore the testing pro-\ncess because if, after delivery, something goes seriously wrong, the architect will \nbe one of the first people brought in to diagnose the problem. In one case we \nheard about, this involved flying to the remote mountains of Peru to diagnose a \nproblem with mining equipment.\n19.3  Summary\nArchitecture plays a key role in both implementation and testing. In the imple-\nmentation phase, letting future readers of the code know what architectural con-\nstructs are being used, using frameworks, and using code templates all make life \neasier both at implementation time and during maintenance.\nDuring testing the architecture determines what is being tested at which \nstage of development. Development quality attributes can be tested during unit \ntest and runtime quality attributes can be tested during integration testing.\nTesting, as with other activities in architecture-based development, is a cost/\nbenefit activity. Do not spend as much time testing for faults whose consequences \nare small and spend the most time testing for faults whose consequences are se-\nrious. Do not neglect testing for faults that show up after the system has been \nexecuting for an extended period.\n19.4  For Further Reading\nGeorge Fairbanks gives an excellent treatment of architecture and implementa-\ntion in Chapter 10 of his book Just Enough Software Architecture, which is enti-\ntled “The Code Model” [Fairbanks 10]. \nMary Shaw long ago recognized the conceptual gap between architecture \nand implementation and wrote about it eloquently in her article “Procedure Calls \nAre the Assembly Language of Software Interconnections: Connectors Deserve \nFirst-Class Status” [Shaw 94]. In it she pointed out the disparity between rich \nconnectors available in architecture and the impoverished subroutine call that is \nthe mainstay of nearly every programming language.\nDetails about the AUTOSAR framework can be found at www.autosar.org.\nArchitecture-based testing is an active field of research. [Bertolino 96b], \n[Muccini 07], [Muccini 03], [Eickelman 96], [Pettichord 02], and [Binder 94] \nspecifically address designing systems so that they are more testable. In fact, the \nthree bullets concerning the architect’s role in Section 19.2 are drawn from Petti-\nchord’s work.\n",
      "content_length": 2397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "19.5  Discussion Questions\n377\nVoas [Voas 95] defines testability, identifies its contributing factors, and de-\nscribes how to measure it.\nBertolino extends Voas’s work and ties testability to dependability [Berto-\nlino 96].\nFinally, Baudry et al. have written an interesting paper that examines the \ntestability of well-known design patterns [Baudry 03].\n19.5  Discussion Questions\n1.\t\nIn a distributed system each computer will have its own clock. It is difficult \nto perfectly synchronize those clocks. How will this complicate making \nperformance measures of distributed systems? How would you go about \ntesting that the performance of a particular system activity is adequate?\n2.\t\nPlan and implement a modification to a module. Ask your colleagues to do \nthe same modification independently. Now compare your results to those of \nyour colleagues. What is the mean and the standard deviation for the time it \ntakes to make that modification?\n3.\t\nList some of the reasons why an architecture and a code base inevitably \ndrift apart. What processes and tools might address this gap? What are their \ncosts and benefits?\n4.\t\nMost user interface frameworks work by capturing events from the user \nand by establishing callbacks or hooks to application-specific functionality. \nWhat limitations do these architectural assumptions impose on the rest of \nthe system?\n5.\t\nConsider building a test harness for a large system. What quality attributes \nshould this harness exhibit? Create scenarios to concretize each of the qual-\nity attributes.\n6.\t\nTesting requires the presence of a test oracle, which determines the success \n(or failure) of a test. For scalability reasons, the oracle must be automatic. \nHow can you ensure that your oracle is correct? How do you ensure that \nits performance will scale appropriately? What process would you use to \nrecord and fix faults in the testing infrastructure?\n7.\t\nIn embedded systems faults often occur “in the field” and it is difficult to \ncapture and replicate the state of the system that led to its failure. What ar-\nchitectural mechanisms might you use to solve this problem?\n8.\t\nIn integration testing it is a bad idea to integrate everything all at once (big \nbang integration). How would you use architecture to help you plan integra-\ntion increments?\n",
      "content_length": 2299,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "379\n20\nArchitecture \nReconstruction and \nConformance\nIt was six men of Indostan / To learning much inclined, \nWho went to see the Elephant / (Though all of them were blind), \nThat each by observation / Might satisfy his mind.\nThe First approach’d the Elephant, / And happening to fall \nAgainst his broad and sturdy side, / At once began to bawl: \n“God bless me! but the Elephant / Is very like a wall!”\nThe Second, feeling of the tusk, / Cried, — “Ho! what have we here \nSo very round and smooth and sharp? / To me ’tis mighty clear \nThis wonder of an Elephant / Is very like a spear!”\nThe Third approached the animal, / And happening to take \nThe squirming trunk within his hands, / Thus boldly up and spake: \n“I see,” quoth he, “the Elephant / Is very like a snake!”\nThe Fourth reached out his eager hand, / And felt about the knee. \n“What most this wondrous beast is like / Is mighty plain,” quoth he, \n“’Tis clear enough the Elephant / Is very like a tree!”\nThe Fifth, who chanced to touch the ear, / Said: “E’en the blindest man \nCan tell what this resembles most; / Deny the fact who can, \nThis marvel of an Elephant / Is very like a fan!”\nThe Sixth no sooner had begun / About the beast to grope, \nThen, seizing on the swinging tail / That fell within his scope, \n“I see,” quoth he, “the Elephant / Is very like a rope!”\nAnd so these men of Indostan / Disputed loud and long, \nEach in his own opinion / Exceeding stiff and strong, \nThough each was partly in the right, / And all were in the wrong!\n—“The Blind Men and the Elephant,” by John Godfrey Saxe\n",
      "content_length": 1561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "380 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nThroughout this book we have treated architecture as something largely under \nyour control and shown how to make architectural decisions to achieve the goals \nand requirements in place for a system under development. But there is another \nside to the picture. Suppose you have been given responsibility for a system that \nalready exists, but you do not know its architecture. Perhaps the architecture \nwas never recorded by the original developers, now long gone. Perhaps it was \nrecorded but the documentation has been lost. Or perhaps it was recorded but \nthe documentation is no longer synchronized with the system after a series of \nchanges. How do you maintain such a system? How do you manage its evolution \nto maintain the quality attributes that its architecture (whatever it may be) has \nprovided for us?\nThis chapter surveys techniques that allow an analyst to build, maintain, and \nunderstand a representation of an existing architecture. This is a process of re-\nverse engineering, typically called architecture reconstruction. Architecture re-\nconstruction is used, by the architect, for two main purposes:\n■\n■To document an architecture where the documentation never existed or \nwhere it has become hopelessly out of date \n■\n■ To ensure conformance between the as-built architecture and the as-de-\nsigned architecture. \nIn architecture reconstruction, the “as-built” architecture of an implemented \nsystem is reverse-engineered from existing system artifacts. \nWhen a system is initially developed, its architectural elements are mapped \nto specific implementation elements: functions, classes, files, objects, and so \nforth. This is forward engineering. When we reconstruct those architectural el-\nements, we need to apply the inverses of the original mappings. But how do we \ngo about determining these mappings? One way is to use automated and semiau-\ntomated extraction tools; the second way is to probe the original design intent of \nthe architect. Typically we use a combination of both techniques in reconstruct-\ning an architecture.\nIn practice, architecture reconstruction is a tool-intensive activity. Tools ex-\ntract information about the system, typically by scouring the source code, but \nthey may also analyze other artifacts as well, such as build scripts or traces from \nrunning systems. But architectures are abstractions—they can not be seen in the \nlow-level implementation details, the programming constructs, of most systems. \nSo we need tools that aid in building and aggregating the abstractions that we \nneed, as architects, on top of the ground facts that we develop, as developers. If \nour tools are usable and accurate, the end result is an architectural representation \nthat aids the architect in reasoning about the system. Of course, if the original \narchitecture and its implementation are “spaghetti,” the reconstruction will faith-\nfully expose this lack of organization. \nArchitecture reconstruction tools are not, however, a panacea. In some cases, \nit may not be possible to generate a useful architectural representation. Further-\nmore, not all aspects of architecture are easy to automatically extract. Consider \n",
      "content_length": 3229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "20.1  Architecture Reconstruction Process \n381\nthis: there is no programming language construct in any major programming lan-\nguage for “layer” or “connector” or other architectural elements; we can’t simply \npick these out of a source code file. Similarly, architectural patterns, if used, are \ntypically not explicitly documented in code. \nArchitecture reconstruction is an interpretive, interactive, and iterative pro-\ncess involving many activities; it is not automatic. It requires the skills and atten-\ntion of both the reverse-engineering expert and, in the best case, the architect (or \nsomeone who has substantial knowledge of the architecture). And whether the \nreconstruction is successful or not, there is a price to pay: the tools come with a \nlearning curve that requires time to climb.\n20.1  Architecture Reconstruction Process \nArchitecture reconstruction requires the skillful application of tools, often with a \nsteep learning curve. No single tool does the entire job. For one reason, there is \noften diversity in the number of implementation languages and dialects in which \na software system is implemented—a mature MRI scanner or a legacy banking \napplication may easily comprise more than ten different programming and script-\ning languages. No tool speaks every language. \nInstead we are inevitably led to a “tool set” approach to support architecture \nreconstruction activities. And so the first step in the reconstruction process is to \nset up the workbench. \nAn architecture reconstruction workbench should be open (making it easy to \nintegrate new tools as required) and provide an integration framework whereby \nnew tools that are added to the tool set do not impact the existing tools or data \nunnecessarily. \nWhether or not an explicit workbench is used, the software architecture re-\nconstruction process comprises the following phases (each elaborated in a subse-\nquent section):\n1.\t\nRaw view extraction. In the raw view extraction phase, raw information \nabout the architecture is obtained from various sources, primarily source \ncode, execution traces, and build scripts. Each of these sets of raw informa-\ntion is called a view.1\n2.\t\nDatabase construction. The database construction phase involves convert-\ning the raw extracted information into a standard form (because the various \nextraction tools may each produce their own form of output). This standard-\nized form of the extracted views is then used to populate a reconstruction \n1.  This use of the term “view” is consistent with our definition in Chapter 18: “a representation of a \nset of system elements and relations among them.”\n",
      "content_length": 2627,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "382 \nPart Three\t\n20—Architecture Reconstruction and Conformance\ndatabase. When the reconstruction process is complete, the database will be \nused to generate authoritative architecture documentation.\n3.\t\nView fusion and manipulation. The view fusion phase combines the var-\nious views of the information stored in the database. Individual views \nmay not contain complete or fully accurate information. View fusion can \nimprove the overall accuracy. For example, a static view extracted from \nsource code might miss dynamically bound information such as calling \nrelationships. This could then be combined with a dynamic view from an \nexecution trace, which will capture all dynamically bound calling infor-\nmation, but which may not provide complete coverage. The combination \nof these views will provide higher quality information than either could \nprovide alone. Furthermore, view creation and fusion is typically associ-\nated with some expert interpretation and manipulation. For example, an \nexpert might decide that a group of elements should be aggregated togeth-\ner to form a layer.\n4.\t\nArchitecture analysis. View fusion will result in a set of hypotheses about \nthe architecture. These hypotheses take the form of architectural elements \n(such as layers) and the constraints and relationships among them. These \nhypotheses need to be tested to see if they are correct, and that is the func-\ntion of the analysis step. Some of these hypotheses might be disproven, \nrequiring additional view extraction, fusion, and manipulation.\nThe four phases of architecture reconstruction are iterative. Figure 20.1 \ndepicts the major tasks of architecture reconstruction and their relationships \nand outputs. Solid lines represent data flow and dashed lines represent human \ninteraction.\nAll of these activities are greatly facilitated by engaging people who are fa-\nmiliar with the system. They can provide insights about what to look for—that is, \nwhat views are amenable to extraction—and provide a guided approach to view \nfusion and analysis. They can also point out or explain exceptions to the design \nrules (which will show up as violations of the hypotheses during the analysis \nphase). If the experts are long gone, reconstruction is still possible, but it may \nwell require more backtracking from incorrect initial guesses.\n20.2  Raw View Extraction\nRaw view extraction involves analyzing a system’s existing design and implementa-\ntion artifacts to construct one or more models of it. The result is a set of information \nthat is used in the view fusion activity to construct more-refined views of the system \nthat directly support the goals of the reconstruction, goals such as these: \n",
      "content_length": 2695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "20.2  Raw View Extraction\n383\nKey:\nView Extraction\nLexical\nParsing\nInstrumentation\n. . .\nDatabase\nConstruction\nArchitecture\nAnalysis\nProcess step\nData flow\nInforms\nView Fusion\nand Manipulation\nDatabase\nArchitecture\nDocumentation\nFigure 20.1  Architecture reconstruction process\n■\n■Extracting and representing a target set of architectural views, to support \nthe overall architecture documentation effort.\n■\n■Answering specific questions about the architecture. For example, “What \ncomponents are potentially affected if I choose to rewrite component X?” \nor “How can I refactor my layering to remove cyclic dependencies?”\nThe raw view extraction process is a blend of the ideal (what information \ndo you want to discover about the architecture that will most help you meet the \ngoals of your reconstruction effort?) and the practical (what information can your \navailable tools actually extract and present?). \nFrom the source artifacts (code, header files, build files, and so on) and other \nartifacts (e.g., execution traces), you can identify and capture the elements of in-\nterest within the system (e.g., files, functions, variables) and their relationships to \nobtain several base system views. Table 20.1 shows a typical list of the elements \nand several relationships among them that might be extracted.\n",
      "content_length": 1312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "384 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nTable 20.1  Examples of Extracted Elements and Relations\nSource \nElement\nRelation\nTarget \nElement\nDescription\nFile\nincludes\nFile\nC preprocessor #include of one \nfile by another\nFile\ncontains\nFunction\nDefinition of a function in a file\nFile\ndefines _ var\nVariable\nDefinition of a variable in a file\nDirectory\ncontains\nDirectory\nDirectory contains a subdirectory\nDirectory\ncontains\nFile\nDirectory contains a file\nFunction\ncalls\nFunction\nStatic function call\nFunction\naccess _ read\nVariable\nRead access on a variable\nFunction\naccess _ write\nVariable\nWrite access on a variable\nEach of the relationships between the elements gives different information \nabout the system:\n■\n■The calls relationship between functions helps us build a call graph. \n■\n■The includes relationship between the files gives us a set of dependen-\ncies between system files. \n■\n■The access_read and access_write relationships between func-\ntions and variables show us how data is used. Certain functions may write \na set of data and others may read it. This information is used to determine \nhow data is passed between various parts of the system. We can determine \nwhether or not a global data store is used or whether most information is \npassed through function calls.\n■\n■Certain elements or subsystems may be stored in particular directories, and \ncapturing relations such as dir_contains_file and dir_contains_\ndir is useful when trying to identify elements later.\n■\n■If the system to be reconstructed is object oriented, classes and methods \nare added to the list of elements to be extracted, and relationships such as \nclass_is_subclass_of_class and class_contains_method \nare extracted and used.\nInformation obtained can be categorized as either static or dynamic. Static \ninformation is obtained by observing only the system artifacts, while dynamic in-\nformation is obtained by observing how the system runs. The goal is to fuse both \nto create more accurate system views. \nIf the architecture of the system changes at runtime, that runtime config-\nuration should be captured and used when carrying out the reconstruction. For \n",
      "content_length": 2171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "20.2  Raw View Extraction\n385\nexample, in some systems a configuration file is read in by the system at startup, \nor a newly started system examines its operating environment, and certain ele-\nments are executed or connections are made as a result.\nAnother reason to capture dynamic information is that some architecturally \nrelevant information may not exist in the source artifacts because of late binding. \nExamples of late binding include the following:\n■\n■Polymorphism\n■\n■Function pointers\n■\n■Runtime parameterization\n■\n■Plug-ins\n■\n■Service interactions mediated by brokers\nFurther, the precise topology of a system may not be determined until \nruntime. For example, in peer-to-peer systems, service-oriented architectures, \nand cloud computing, the topology of the system is established dynamically, \ndepending on the availability, loading, and even dynamic pricing of system re-\nsources. The topology of such systems cannot be directly recovered from their \nsource artifacts and hence cannot be reverse-engineered using static extraction \ntools.\nTherefore, it may be necessary to use tools that can generate dynamic in-\nformation about the system (e.g., profiling tools, instrumentation that generates \nruntime traces, or aspects in an aspect-oriented programming language that can \nmonitor dynamic activity). Of course, this requires that such tools be available \non the platforms on which the system executes. Also, it may be difficult to col-\nlect the results from code instrumentation. For example, embedded systems often \nhave no direct way to output such information.\nTable 20.2 summarizes some of the common categories of tools that might \nbe used to populate the views loaded into the reconstruction database.\nTools to analyze design models, build files, and executables can also be used \nto extract further information as required. For instance, build files include in-\nformation on module or file dependencies that exist within the system, and this \ninformation may not be reflected in the source code, or anywhere else. \nAn additional activity that is often required prior to loading a raw view into \nthe database is to prune irrelevant information. For example, in a C code base \nthere may be several main() routines, but only one of those (and its resulting \ncall graph) will be of concern for analysis. The others may be for test harnesses \nand other utility functions. Similarly if you are building or using libraries that \nare operating-system specific, you may only be interested in a specific OS (e.g., \nLinux) and thus want to discard the libraries for other platforms.\n",
      "content_length": 2596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "386 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nTable 20.2  Tool Categories for Populating Reconstructed Architecture Views\nTool\nStatic or \nDynamic\nDescription\nParsers\nStatic\nParsers analyze the code and generate internal \nrepresentations from it (for the purpose of \ngenerating machine code). It is possible to save \nthis internal representation to obtain a view.\nAbstract Syntax \nTree (AST) \nAnalyzers\nAST analyzers do a similar job to parsers, but \nthey build an explicit tree representation of \nthe parsed information. We can build analysis \ntools that traverse the AST and output selected \npieces of architecturally relevant information in \nan appropriate format.\nLexical Analyzers\nLexical analyzers examine source artifacts \npurely as strings of lexical elements or tokens. \nThe user of a lexical analyzer can specify \na set of code patterns to be matched and \noutput. Similarly, a collection of ad hoc tools \nsuch as grep and Perl can carry out pattern \nmatching and searching within the code to \noutput some required information. All of these \ntools—code-generating parsers, AST-based \nanalyzers, lexical analyzers, and ad hoc pattern \nmatchers—are used to output static information.\nProfilers\nDynamic\nProfiling and code coverage analysis tools can \nbe used to output information about the code as \nit is being executed, and usually do not involve \nadding new code to the system. \nCode \nInstrumentation \nTools\nCode instrumentation, which has wide \napplicability in the field of testing, involves \nadding code to the system to output specific \ninformation while the system is executing. \nAspects, in an aspect-oriented programming \nlanguage, can serve the same purpose \nand have the advantage of keeping the \ninstrumentation code separate from the code \nbeing monitored.\n20.3  Database Construction\nSome of the information extracted from the raw view extraction phase, while \nnecessary for the process of reconstruction, may be too specific to aid in archi-\ntectural understanding. Consider Figure 20.2. In this figure we show a set of facts \nextracted from a code base consisting of classes and methods, and inclusion and \ncalling relations. Each element is plotted on a grid and each relation is drawn as a \n",
      "content_length": 2233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "20.3  Database Construction\n387\nline between the elements. This view, while accurate, provides no insight into the \noverarching abstractions or coarse-grained structures present in the architecture.\nThus we need to manipulate such raw views, to collapse information (for \nexample, hiding methods inside class definitions), and to show abstractions (for \nexample, showing all of the connections between business objects and user inter-\nface objects, or identifying distinct layers).\nIt is helpful to use a database to store the extracted information because the \namount of information being stored is large, and the manipulations of the data \nare tedious and error-prone if done manually. Some reverse-engineering tools, \nsuch as Lattix, SonarJ, and Structure101, fully encapsulate the database, and so \nthe user of the tool need not be concerned with its operation. However, those who \nare using a suite of tools together—a workbench—will need to choose a database \nand decide on internal representations of the views.\nFigure 20.2  A raw extracted view: white noise\n",
      "content_length": 1066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "388 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n20.4  View Fusion\nOnce the raw facts have been extracted and stored in a database, the reconstructor \ncan now perform view fusion. In this phase, the extracted views are manipulated \nto create fused views. Fused views combine information from one or more ex-\ntracted views, each of which may contain specialized information. For example, \na static call view might be fused with a dynamic call view. One might want to \ncombine these two views because a static call view will show all explicit calls \n(where method A calls method B) but will miss calls that are made via late bind-\ning mechanisms. A dynamically extracted call graph will never miss a call that is \nmade during an execution, but it suffers the “testing” problem: it will only report \nresults from those paths through the system that are traversed during its execu-\ntion. So a little-used part of the system—perhaps for initialization or error recov-\nery—might not show up in the dynamic view. Therefore we fuse these two views \nto produce a more complete and more accurate graph of system relationships.\nThe process of creating a fused view is the process of creating a hypothesis \nabout the architecture and a visualization of it to aid in analysis. These hypothe-\nses result in new aggregations that show various abstractions or clusterings of the \nelements (which may be source artifacts or previously identified abstractions). \nBy interpreting these fused views and analyzing them, it is possible to produce \nhypothesized architectural views of the system. These views can be interpreted, \nfurther refined, or rejected. There are no universal completion criteria for this \nprocess; it is complete when the architectural representation is sufficient to sup-\nport the analysis needs of its stakeholders.\nFor example, Figure 20.3 shows the early results of interacting with the tool \nSonarJ. SonarJ first extracts facts from a set of source code files (in this case, \nwritten in Java) and lets you define a set of layers and vertical slices through \nthose layers in a system. SonarJ will then instantiate the user-specified definitions \nof layers and slices and populate them with the extracted software elements. \nFigure 20.3  Hypothesized layers and vertical slices\n",
      "content_length": 2297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "20.5  Architecture Analysis: Finding Violations\n389\nIn the figure there are five layers: Controller, Data, Domain, DSI, and Ser-\nvice. And there are six vertical slices defined that span these layers: Common, \nContact, Customer, Distribution, Request, and User. At this point, however, there \nare no relationships between the layers or vertical slides shown—this is merely \nan enumeration of the important system abstractions. \n20.5  Architecture Analysis: Finding Violations\nConsider the following situation: You have designed an architecture but you have \nsuspicions that the developers are not faithfully implementing what you devel-\noped. They may do this out of ignorance, or because they have differing agendas \nfor the system, or simply because they were rushing to meet a deadline and ig-\nnored any concern not on their critical path. Whatever the root cause, this diver-\ngence of the architecture and the implementation spells problems for you, the \narchitect. So how do you test and ensure conformance to the design?\nThere are two major possibilities for maintaining conformance between \ncode and architecture: \n■\n■Conformance by construction. Ensuring consistency by construction—that \nis, automatically generating a substantial part of the system based on an \narchitectural specification—is highly desirable because tools can guarantee \nconformance. Unfortunately, this approach has limited applicability. It can \nonly be applied in situations where engineers can employ specific architec-\nture-based development tools, languages, and implementation strategies. \nFor systems that are composed of existing parts or that require a style of \narchitecture or implementation outside those supported by generation tools, \nthis approach does not apply. And this is the vast majority of systems.\n■\n■Conformance by analysis. This technique aims to ensure conformance by \nanalyzing (reverse-engineering) system information to flag nonconform-\ning elements, so that they can be fixed: brought into conformance. When \nan implementation is sufficiently constrained so that modularization and \ncoding patterns can be identified with architectural elements, this tech-\nnique can work well. Unfortunately, however, the technique is limited in \nits applicability. There is an inherent mismatch between static, code-based \nstructures such as classes and packages (which are what programmers see) \nand the runtime structures, such as processes, threads, clients, servers, and \ndatabases, that are the essence of most architectural descriptions. Further \ncomplicating this analysis, the actual runtime structures may not be known \nor established until the program executes: clients and servers may come and \ngo dynamically, components not under direct control of the implementers \nmay be dynamically loaded, and so forth.\n",
      "content_length": 2814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "390 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nWe will focus on the second option: conformance by analysis.\nIn the previous step, view fusion gave us a set of hypotheses about the ar-\nchitecture. These hypotheses take the form of architectural elements (sometimes \naggregated, such as layers) and the constraints and relationships among them. \nThese hypotheses need to be tested to see if they are correct—to see if they con-\nform with the architect’s intentions. That is the function of the analysis step.\nFigure 20.4 shows the results of adding relationships and constraints to the \narchitecture initially created in Figure 20.3. These relationship and constraints \nare information added by the architect, to reflect the design intent. In this ex-\nample, the architect has indicated the relationships between the layers of Figure \n20.3. These relationships are indicated by the directed lines drawn between the \nlayers (and vertical slices). Using these relationships and constraints, a tool such \nas SonarJ is able to automatically detect and report violations of the layering in \nthe software.\nWe can now see that the Data layer (row 2 in Figure 20.4) can access, and \nhence depends on, the DSI layer. We can further see that it may not access, and \nhas no dependencies on, Domain, Service, or Controller (rows 1, 3, and 5 in the \nfigure). \nIn addition we can see that the JUnit component in the “External” compo-\nnent is defined to be inaccessible. This is an example of an architectural con-\nstraint that is meant to pervade the entire system: no portion of the application \nshould depend upon JUnit, because this should only be used by test code. \nFigure 20.4  Layers, vertical slices, relationships, and constraints\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "20.5  Architecture Analysis: Finding Violations\n391\nFigure 20.5 shows an example of an architecture violation of the previous \nrestriction. This violation is found by SonarJ by searching through its database, \napplying the user-defined patterns, and finding violations of those patterns. In this \nfigure you can see an arc between the Service layer and JUnit. This arc is high-\nlighted to indicate that this is an illegal dependency and an architectural viola-\ntion. (This figure also shows some additional dependencies, to external modules.)\nArchitecture reconstruction is a means of testing the conformance to such \nconstraints. The preceding example showed how these constraints might be de-\ntected and enforced using static code analysis. But static analysis is primarily \nuseful for understanding module structures. What if one needed to understand \nruntime information, as represented by C&C structures?\nIn the example given in Figure 20.6, an architecture violation was discov-\nered via dynamic analysis, using the research DiscoTect system. In this case an \nanalysis of the runtime architecture of the Duke’s Bank application—a simple \nEnterprise JavaBeans (EJB) banking application created by Sun Microsystems \nas a demonstration of EJB functionality—was performed. The code was “instru-\nmented” using AspectJ; instrumentation aspects were woven into the compiled \nbytecode of the EJB application. These aspects emitted events when methods en-\ntered or exited and when objects were constructed.\nFigure 20.5  Highlighting an architecture violation\n",
      "content_length": 1556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "392 \nPart Three\t\n20—Architecture Reconstruction and Conformance\nFigure 20.6  An architecture violation discovered by dynamic analysis\nFigure 20.6 shows that a “database write” connector was discovered in the \ndynamic analysis of the architecture. Sun’s EJB specification and its documented \narchitecture of Duke’s Bank forbid such connections. All database access is sup-\nposed to be managed by entity beans, and only by entity beans. Such architec-\ntural violations are difficult to find in the source code—often just a single line \nof code is involved—and yet can substantially affect the quality attributes of the \nresulting system. \n20.6  Guidelines\nThe following are a set of guidelines for the reconstruction process:\n■\n■Have a goal and a set of objectives or questions in mind before undertaking \nan architecture reconstruction project. In the absence of these, a lot of effort \ncould be spent on extracting information and generating architecture views \nthat may not be helpful or serve any useful purpose.\n",
      "content_length": 1015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "20.7  Summary\n393\n■\n■Obtain some representation, however coarse, of the system before begin-\nning the detailed reconstruction process. This representation serves several \npurposes, including the following:\n■\n■It identifies what information needs to be extracted from the system.\n■\n■It guides the reconstructor in determining what to look for in the architec-\nture and what views to generate.\nIdentifying layers is a good place to start.\n■\n■In many cases, the existing documentation for a system may not accurate-\nly reflect the system as it is implemented. Therefore it may be necessary \nto disregard the existing documentation and use it only to generate the \nhigh-level views of the system, because it should give an indication of the \nhigh-level concepts.\n■\n■Tools can support the reconstruction effort and shorten the reconstruction \nprocess, but they cannot do an entire reconstruction effort automatically. \nThe work involved in the effort requires the involvement of people (archi-\ntects, maintainers, and developers) who are familiar with the system. It \nis important to get these people involved in the effort at an early stage as \nit helps the reconstructor get a better understanding of the system being \nreconstructed.\n20.7  Summary\nArchitecture reconstruction and architecture conformance are crucial tools in the \narchitect’s toolbox to ensure that a system is built the way it was designed, and \nthat it evolves in a way that is consistent with its creators’ intentions. All nontriv-\nial long-lived systems evolve: the code and the architecture both evolve. This is a \ngood thing. But if the code evolves in an ad hoc manner, the result will be the big \nball of mud, and the system’s quality attributes will inevitably suffer. The only \ndefense against this erosion is consistent attention to architecture quality, which \nimplies the need to maintain architecture conformance.\nThe results of architectural reconstruction can be used in several ways:\n■\n■If no documentation exists or if it is seriously out of date, the recovered \narchitectural representation can be used as a basis for documenting the ar-\nchitecture, as discussed in Chapter 18. \n■\n■It can be used to recover the as-built architecture, or to check conformance \nagainst an “as-designed” architecture. Conformance checking assures us \nthat our developers and maintainers have followed the architectural edicts \nset forth for them and are not eroding the architecture by breaking down ab-\nstractions, bridging layers, compromising information hiding, and so forth. \n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "394 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n■\n■The reconstruction can be used as the basis for analyzing the architec-\nture or as a starting point for reengineering the system to a new desired \narchitecture. \n■\n■Finally, the representation can be used to identify elements for reuse or to \nestablish an architecture-based software product line (see Chapter 25). \nThe software architecture reconstruction process comprises the following \nphases:\n1.\t\nRaw view extraction. In the raw view extraction phase, raw information \nabout the architecture is obtained from various sources, primarily source \ncode, execution traces, and build scripts. Each of these sets of raw informa-\ntion is called a view.\n2.\t\nDatabase construction. The database construction phase involves convert-\ning the extracted information into a standard form (because the various \nextraction tools may each produce their own form of output) and populating \na reconstruction database with this information.\n3.\t\nView fusion. The view fusion phase combines views of the information \nstored in the database.\n4.\t\nArchitecture analysis. View fusion has given us a set of hypotheses about \nthe architecture. These hypotheses take the form of architectural elements \n(sometimes aggregated, such as layers) and the constraints and relationships \namong them. These hypotheses need to be tested to see if they are correct, \nand that is the function of the analysis step.\n20.8  For Further Reading\nThe Software Engineering Institute (SEI) has developed two reconstruction \nworkbenches: Dali and Armin. Dali was our first attempt at creating a workbench \nfor architecture recovery and conformance [Kazman 99]. Armin, a complete re-\nwrite and rethink of Dali, is described in [O’Brien 03].\nBoth Armin and Dali were primarily focused on module structures of an \narchitecture. A later tool, called DiscoTect, was aimed at discovering C&C struc-\ntures. This is described in [Schmerl 06].\nMany other architecture reverse-engineering tools have been created. A few \nof the notable ones created in academia are [van Deursen 04], [Murphy 01], and \n[Storey 97].\nIn addition there are a number of commercial architecture extraction and \nreconstruction tools that have been slowly gaining market acceptance in the past \ndecade. Among these are the following:\n■\n■SonarJ (www.hello2morrow.com)\n■\n■Lattix (www.lattix.com)\n",
      "content_length": 2381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "20.9  Discussion Questions \n395\n■\n■Understand (www.scitools.com)\nCai et al. [Cai 2011] compellingly demonstrate the need for architecture \nconformance testing in an experimental study that they conducted, wherein they \nfound that software engineering students, given UML designs for a variety of rel-\natively simple systems, violate those designs over 70 percent of the time.\nFinally, the set of guidelines presented in this chapter for how to go about \nreconstructing an architecture was excerpted from [Kazman 02].\n20.9  Discussion Questions \n1.\t\nSuppose that for a given system you wanted to extract the architectural \nstructures (as discussed in Chapter 1) listed in the table rows below. For \neach row, fill in each column to appraise each strategy listed in the columns. \n“VH” (very high) means the strategy would be very effective at extracting \nthis structure; “VL” means it would be very ineffective; “H,” M,” and “L” \nhave the obvious in-between values.\nArchitectural Structures\nInterviewing experts on  \nthe system\nReconstruction Strategies\nAnalyzing  \nstructure of  \nsource code files\nStatic  \nanalysis of \nsource code\nDynamic  \nanalysis of system’s \nexecution\nModule structures\nDecomposition\nUses\nLayers\nClass\nData model\nC&C \nstructures\nService (for SOA \nsystems)\nConcurrency\nAllocation structures\nDeployment\nImplementation\nWork assignment\n",
      "content_length": 1353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "396 \nPart Three\t\n20—Architecture Reconstruction and Conformance\n2.\t\nRecall that in layered systems, the relationship among layers is allowed to \nuse. Also recall that it is possible for one piece of software to use another \npiece without actually calling it—for example, by depending on it leaving \nsome shared resource in a usable state. Does this interpretation change your \nanswer above for the “Uses” and “Layers” structures?\n3.\t\nWhat inferences can you make about a system’s module structures from \nexamining a set of behavioral traces gathered dynamically?\n4.\t\nSuppose you believe that the architecture for a system follows a broker \npattern. What information would you want to extract from the source code \nto confirm or refute this hypothesis? What behavioral or interaction pattern \nwould you expect to observe at runtime?\n5.\t\nSuppose you hypothesize that a system makes use of particular tactics to \nachieve a particular quality attribute. Fill in the columns of the table below \nto show how you would go about verifying your hypothesis. (Begin by fill-\ning in column 1 with a particular tactic for the named quality attribute.)\nTactics for…\nReconstruction Strategies\nInterviewing \nexperts on \nthe system\nAnalyzing \nstructure of \nsource code \nfiles\nStatic \nanalysis of \nsource code\nDynamic \nanalysis of \nsystem’s \nexecution\nAvailability\nInteroperability\nModifiability\nPerformance\nSecurity\nTestability\nUsability\n6.\t\nSuppose you want to confirm that developers and maintainers had remained \nfaithful to an architecture over the lifetime of the system. Describe the re-\nconstruction and/or auditing processes you would undertake.\n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "397\n21\nArchitecture Evaluation\nFear cannot be banished, but it can be calm and without \npanic; it can be mitigated by reason and evaluation.\n—Vannevar Bush\nWe discussed analysis techniques in Chapter 14. Analysis lies at the heart of ar-\nchitecture evaluation, which is the process of determining if an architecture is fit \nfor the purpose for which it is intended. Architecture is such an important con-\ntributor to the success of a system and software engineering project that it makes \nsense to pause and make sure that the architecture you’ve designed will be able \nto provide all that’s expected of it. That’s the role of evaluation. Fortunately there \nare mature methods to evaluate architectures that use many of the concepts and \ntechniques you’ve already learned in previous chapters of this book.\n21.1  Evaluation Factors\nEvaluation usually takes one of three forms:\n■\n■Evaluation by the designer within the design process\n■\n■Evaluation by peers within the design process \n■\n■Analysis by outsiders once the architecture has been designed\nEvaluation by the Designer\nEvery time the designer makes a key design decision or completes a design \nmilestone, the chosen and competing alternatives should be evaluated using the \nanalysis techniques of Chapter 14. Evaluation by the designer is the “test” part \nof the “generate-and-test” approach to architecture design that we discussed in \nChapter 17.\n",
      "content_length": 1405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "398 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nHow much analysis? This depends on the importance of the decision. Ob-\nviously, decisions made to achieve one of the driving architectural requirements \nshould be subject to more analysis than others, because these are the ones that \nwill shape critical portions of the architecture. But in all cases, performing anal-\nysis is a matter of cost and benefit. Do not spend more time on a decision than it \nis worth, but also do not spend less time on an important decision than it needs. \nSome specific considerations include these:\n■\n■The importance of the decision. The more important the decision, the more \ncare should be taken in making it and making sure it’s right.\n■\n■The number of potential alternatives. The more alternatives, the more time \ncould be spent in evaluating them. Try to eliminate alternatives quickly so \nthat the number of viable potential alternatives is small.\n■\n■Good enough as opposed to perfect. Many times, two possible alternatives \ndo not differ dramatically in their consequences. In such a case, it is more \nimportant to make a choice and move on with the design process than it is \nto be absolutely certain that the best choice is being made. Again, do not \nspend more time on a decision than it is worth.\nPeer Review\nArchitectural designs can be peer reviewed just as code can be peer reviewed. \nA peer review can be carried out at any point of the design process where a can-\ndidate architecture, or at least a coherent reviewable part of one, exists. There \nshould be a fixed amount of time allocated for the peer review, at least several \nhours and possibly half a day. A peer review has several steps:\n1.\t\nThe reviewers determine a number of quality attribute scenarios to \ndrive the review. Most of the time these scenarios will be architecturally \nsignificant requirements, but they need not be. These scenarios can be \ndeveloped by the review team or by additional stakeholders.\n2.\t\nThe architect presents the portion of the architecture to be evaluated. (At \nthis point, comprehensive documentation for it may not exist.) The review-\ners individually ensure that they understand the architecture. Questions at \nthis point are specifically for understanding. There is no debate about the \ndecisions that were made. These come in the next step.\n3.\t\nFor each scenario, the designer walks through the architecture and explains \nhow the scenario is satisfied. (If the architecture is already documented, \nthen the reviews can use it to assess for themselves how it satisfies the \nscenario.) The reviewers ask questions to determine two different types of \ninformation. First, they want to determine that the scenario is, in fact, sat-\nisfied. Second, they want to determine whether any of the other scenarios \nbeing considered will not be satisfied because of the decisions made in the \nportion of the architecture being reviewed.\n",
      "content_length": 2944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "21.1  Evaluation Factors\n399\n4.\t\nPotential problems are captured. The list of potential problems forms the \nbasis for the follow-up of the review. If the potential problem is a real prob-\nlem, then it either must be fixed or a decision must be explicitly made by \nthe designers and the project manager that they are willing to accept the \nproblem and its probability of occurrence.\nIf the designers are using the ADD process described in Chapter 17, then a \npeer review can be done at the end of step 3 of each ADD iteration.\nAnalysis by Outsiders\nOutside evaluators can cast an objective eye on an architecture. “Outside” is rel-\native; this may mean outside the development project, outside the business unit \nwhere the project resides but within the same company; or outside the company \naltogether. To the degree that evaluators are “outside,” they are less likely to be \nafraid to bring up sensitive problems, or problems that aren’t apparent because of \norganizational culture or because “we’ve always done it that way.” \nOften, outsiders are chosen because they possess specialized knowledge \nor experience, such as knowledge about a quality attribute that’s important to \nthe system being examined, or long experience in successfully evaluating \narchitectures.\nAlso, whether justified or not, managers tend to be more inclined to listen to \nproblems uncovered by an outside team hired at considerable cost. (This can be \nunderstandably frustrating to project staff who may have been complaining about \nthe same problems to no avail for months.)\nIn principle, an outside team may evaluate a completed architecture, an in-\ncomplete architecture, or a portion of an architecture. In practice, because en-\ngaging them is complicated and often expensive, they tend to be used to evaluate \ncomplete architectures. \nContextual Factors\nFor peer reviews or outside analysis, there are a number of contextual factors \nthat must be considered when structuring an evaluation. These include the arti-\nfacts available, whether the results are public or private, the number and skill of \nevaluators, the number and identity of the participating stakeholders, and how the \nbusiness goals are understood by the evaluators.\n■\n■What artifacts are available? To perform an architectural evaluation, there \nmust be an artifact that describes the architecture. This must be located \nand made available. Some evaluations may take place after the system is \noperational. In this case, recovery tools as described in Chapter 20 may be \nused both to assist in discovering the architecture and to test that the as-\nbuilt system conforms to the as-designed system. \n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "400 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n■\n■Who sees the results? Some evaluations are performed with the full knowl-\nedge and participation of all of the stakeholders. Others are performed more \nprivately. The private evaluations may be done for a variety of reasons, \nranging from corporate culture to (in one case we know about) an execu-\ntive wanting to determine which of a collection of competitive systems he \nshould back in an internal dispute about the systems.\n■\n■Who performs the evaluation? Evaluations can be carried out by an individ-\nual or a team. In either case, the evaluator(s) should be highly skilled in the \ndomain and the various quality attributes for which the system is to be eval-\nuated. And for carrying out evaluation methods with extensive stakeholder \ninvolvement, excellent organizational and facilitation skills are a must.\n■\n■Which stakeholders will participate? The evaluation process should provide \na method to elicit the goals and concerns that the important stakeholders \nhave regarding the system. Identifying the individuals who are needed and \nassuring their participation in the evaluation is critical.\n■\n■What are the business goals? The evaluation should answer whether \nthe system will satisfy the business goals. If the business goals are not \nexplicitly captured and prioritized prior to the evaluation, then there should \nbe a portion of the evaluation dedicated to doing so.\n21.2  The Architecture Tradeoff Analysis Method\nThe Architecture Tradeoff Analysis Method (ATAM) has been used for over a de-\ncade to evaluate software architectures in domains ranging from automotive to fi-\nnancial to defense. The ATAM is designed so that evaluators need not be familiar \nwith the architecture or its business goals, the system need not yet be constructed, \nand there may be a large number of stakeholders. \nParticipants in the ATAM \nThe ATAM requires the participation and mutual cooperation of three groups:\n■\n■The evaluation team. This group is external to the project whose \narchitecture is being evaluated. It usually consists of three to five people. \nEach member of the team is assigned a number of specific roles to play \nduring the evaluation. (See Table 21.1 for a description of these roles, along \nwith a set of desirable characteristics for each. A single person may adopt \nseveral roles in an ATAM.) The evaluation team may be a standing unit in \nwhich architecture evaluations are regularly performed, or its members may \nbe chosen from a pool of architecturally savvy individuals for the occasion. \n",
      "content_length": 2593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n401\nThey may work for the same organization as the development team whose \narchitecture is on the table, or they may be outside consultants. In any case, \nthey need to be recognized as competent, unbiased outsiders with no hidden \nagendas or axes to grind. \n■\n■Project decision makers. These people are empowered to speak for the \ndevelopment project or have the authority to mandate changes to it. They \nusually include the project manager, and if there is an identifiable customer \nwho is footing the bill for the development, he or she may be present (or \nrepresented) as well. The architect is always included—a cardinal rule of \narchitecture evaluation is that the architect must willingly participate.\n■\n■Architecture stakeholders. Stakeholders have a vested interest in the \narchitecture performing as advertised. They are the ones whose ability to do \ntheir job hinges on the architecture promoting modifiability, security, high \nreliability, or the like. Stakeholders include developers, testers, integrators, \nmaintainers, performance engineers, users, builders of systems interacting \nwith the one under consideration, and others listed in Chapter 3. Their job \nduring an evaluation is to articulate the specific quality attribute goals that \nthe architecture should meet in order for the system to be considered a \nsuccess. A rule of thumb—and that is all it is—is that you should expect to \nenlist 12 to 15 stakeholders for the evaluation of a large enterprise-critical \narchitecture. Unlike the evaluation team and the project decision makers, \nstakeholders do not participate in the entire exercise.\nTable 21.1  ATAM Evaluation Team Roles\nRole\nResponsibilities\nTeam Leader\nSets up the evaluation; coordinates with client, making sure client’s \nneeds are met; establishes evaluation contract; forms evaluation \nteam; sees that final report is produced and delivered (although the \nwriting may be delegated)\nEvaluation \nLeader\nRuns evaluation; facilitates elicitation of scenarios; administers \nscenario selection/prioritization process; facilitates evaluation of sce-\nnarios against architecture; facilitates on-site analysis\nScenario \nScribe\nWrites scenarios on flipchart or whiteboard during scenario elicitation; \ncaptures agreed-on wording of each scenario, halting discussion until \nexact wording is captured\nProceedings \nScribe\nCaptures proceedings in electronic form on laptop or workstation: \nraw scenarios, issue(s) that motivate each scenario (often lost in the \nwording of the scenario itself), and resolution of each scenario when \napplied to architecture(s); also generates a printed list of adopted \nscenarios for handout to all participants\nQuestioner\nRaises issues of architectural interest, usually related to the quality \nattributes in which he or she has expertise\n",
      "content_length": 2846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "402 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nOutputs of the ATAM\nAs in any testing process, a large benefit derives from preparing for the test. In \npreparation for an ATAM exercise, the project’s decision makers must prepare the \nfollowing:\n1.\t\nA concise presentation of the architecture. One of the requirements of the \nATAM is that the architecture be presented in one hour, which leads to an \narchitectural presentation that is both concise and, usually, understandable.\n2.\t\nArticulation of the business goals. Frequently, the business goals presented in \nthe ATAM are being seen by some of the assembled participants for the first \ntime, and these are captured in the outputs. This description of the business \ngoals survives the evaluation and becomes part of the project’s legacy. \nThe ATAM uses prioritized quality attribute scenarios as the basis for \nevaluating the architecture, and if those scenarios do not already exist (perhaps \nas a result of a prior requirements capture exercise or ADD activity), they are \ngenerated by the participants as part of the ATAM exercise. Many times, ATAM \nparticipants have told us that one of the most valuable outputs of ATAM is this \nnext output:\n3.\t\nPrioritized quality attribute requirements expressed as quality attribute sce-\nnarios. These quality attribute scenarios take the form described in Chap-\nter 4. These also survive past the evaluation and can be used to guide the \narchitecture’s evolution.\nThe primary output of the ATAM is a set of issues of concern about the \narchitecture. We call these risks:\n4.\t\nA set of risks and nonrisks. A risk is defined in the ATAM as an architec-\ntural decision that may lead to undesirable consequences in light of stated \nquality attribute requirements. Similarly, a nonrisk is an architectural deci-\nsion that, upon analysis, is deemed safe. The identified risks form the basis \nfor an architectural risk mitigation plan.\n5.\t\nA set of risk themes. When the analysis is complete, the evaluation team \nexamines the full set of discovered risks to look for overarching themes that \nidentify systemic weaknesses in the architecture or even in the architecture \nprocess and team. If left untreated, these risk themes will threaten the \nproject’s business goals. \nFinally, along the way, other information about the architecture is discovered \nand captured:\n6.\t\nMapping of architectural decisions to quality requirements. Architectural \ndecisions can be interpreted in terms of the qualities that they support or \nhinder. For each quality attribute scenario examined during an ATAM, those \n",
      "content_length": 2614,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n403\narchitectural decisions that help to achieve it are determined and captured. \nThis can serve as a statement of rationale for those decisions.\n7.\t\nA set of identified sensitivity and tradeoff points. These are architectural \ndecisions that have a marked effect on one or more quality attributes. \nThe outputs of the ATAM are used to build a final written report that recaps \nthe method, summarizes the proceedings, captures the scenarios and their \nanalysis, and catalogs the findings.\nThere are intangible results of an ATAM-based evaluation. These include a \npalpable sense of community on the part of the stakeholders, open communica-\ntion channels between the architect and the stakeholders, and a better overall un-\nderstanding on the part of all participants of the architecture and its strengths and \nweaknesses. While these results are hard to measure, they are no less important \nthan the others and often are the longest-lasting.\nPhases of the ATAM\nActivities in an ATAM-based evaluation are spread out over four phases: \n■\n■In phase 0, “Partnership and Preparation,” the evaluation team leadership \nand the key project decision makers informally meet to work out the details \nof the exercise. The project representatives brief the evaluators about the \nproject so that the team can be supplemented by people who possess the \nappropriate expertise. Together, the two groups agree on logistics, such as \nthe time and place of meetings, who brings the flipcharts, and who supplies \nthe donuts and coffee. They also agree on a preliminary list of stakeholders \n(by name, not just role), and they negotiate on when the final report is to \nbe delivered and to whom. They deal with formalities such as a statement \nof work or nondisclosure agreements. The evaluation team examines the \narchitecture documentation to gain an understanding of the architecture and \nthe major design approaches that it comprises. Finally, the evaluation team \nleader explains what information the manager and architect will be expect-\ned to show during phase 1, and helps them construct their presentations if \nnecessary. \n■\n■Phase 1 and phase 2 are the evaluation phases, where everyone gets down \nto the business of analysis. By now the evaluation team will have studied \nthe architecture documentation and will have a good idea of what the \nsystem is about, the overall architectural approaches taken, and the quality \nattributes that are of paramount importance. During phase 1, the evaluation \nteam meets with the project decision makers (for one to two days) to \nbegin information gathering and analysis. For phase 2, the architecture’s \nstakeholders join the proceedings and analysis continues, typically for \ntwo days. Unlike the other phases, phase 1 and phase 2 comprise a set of \nspecific steps; these are detailed in the next section.\n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "404 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nTable 21.2  ATAM Phases and Their Characteristics\nPhase\nActivity\nParticipants\nTypical Duration\n0\nPartnership and \npreparation\nEvaluation team lead-\nership and key project \ndecision makers\nProceeds informally as \nrequired, perhaps over \na few weeks\n1\nEvaluation\nEvaluation team and  \nproject decision makers\n1–2 days followed by a \nhiatus of 1–3 weeks\n2\nEvaluation  \n(continued)\nEvaluation team, project \ndecision makers, and \nstakeholders\n2 days\n3\nFollow-up\nEvaluation team and \nevaluation client\n1 week\nSource: Adapted from [Clements 01b].\n■\n■Phase 3 is follow-up, in which the evaluation team produces and delivers \na written final report. It is first circulated to key stakeholders to make sure \nthat it contains no errors of understanding, and after this review is complete \nit is delivered to the person who commissioned the evaluation.\nTable 21.2 shows the four phases of the ATAM, who participates in each \none, and an approximate timetable. \nSteps of the Evaluation Phases\nThe ATAM analysis phases (phase 1 and phase 2) consist of nine steps. Steps 1 \nthrough 6 are carried out in phase 1 with the evaluation team and the project’s \ndecision makers: typically, the architecture team, project manager, and project \nsponsor. In phase 2, with all stakeholders present, steps 1 through 6 are \nsummarized and steps 7 through 9 are carried out.\nTable 21.3 shows a typical agenda for the first day of phase 1, which covers \nsteps 1 through 5. Step 6 in phase 1 is carried out the next day.\nStep 1: Present the ATAM.  The first step calls for the evaluation leader to \npresent the ATAM to the assembled project representatives. This time is used to \nexplain the process that everyone will be following, to answer questions, and to \nset the context and expectations for the remainder of the activities. Using a stan-\ndard presentation, the leader describes the ATAM steps in brief and the outputs of \nthe evaluation.\nStep 2: Present the Business Drivers.  Everyone involved in the eval-\nuation—the project representatives as well as the evaluation team members—\nneeds to understand the context for the system and the primary business drivers \n",
      "content_length": 2218,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n405\nmotivating its development. In this step, a project decision maker (ideally the \nproject manager or the system’s customer) presents a system overview from a \nbusiness perspective. The presentation should describe the following:\n■\n■The system’s most important functions\n■\n■Any relevant technical, managerial, economic, or political constraints \n■\n■The business goals and context as they relate to the project\n■\n■The major stakeholders\n■\n■The architectural drivers (that is, the architecturally significant \nrequirements)\nStep 3: Present the Architecture.  Here, the lead architect (or architecture \nteam) makes a presentation describing the architecture at an appropriate level \nof detail. The “appropriate level” depends on several factors: how much of the \narchitecture has been designed and documented; how much time is available; and \nthe nature of the behavioral and quality requirements. \nIn this presentation the architect covers technical constraints such as \noperating system, hardware, or middleware prescribed for use, and other systems \nwith which the system must interact. Most important, the architect describes the \narchitectural approaches (or patterns, or tactics, if the architect is fluent in that \nvocabulary) used to meet the requirements.\nTo make the most of limited time, the architect’s presentation should have a \nhigh signal-to-noise ratio. That is, it should convey the essence of the architecture \nand not stray into ancillary areas or delve too deeply into the details of just a few \naspects. Thus, it is extremely helpful to brief the architect beforehand (in phase \n0) about the information the evaluation team requires. A template such as the one \nin the sidebar can help the architect prepare the presentation. Depending on the \narchitect, a dress rehearsal can be included as part of the phase 0 activities.\nTable 21.3  Agenda for Day 1 of the ATAM\nTime\nActivity\n0830 – 1000\nIntroductions; Step 1: Present the ATAM\n1000 – 1100 \nStep 2: Present Business Drivers\n1100 – 1130\nBreak\n1130 – 1230\nStep 3: Present Architecture\n1230 – 1330\nLunch\n1330 – 1430\nStep 4: Identify Architectural Approaches\n1430 – 1530\nStep 5: Generate Utility Tree\n1530 – 1600 \nBreak\n1600 – 1700\nStep 5: Generate Utility Tree (continued)\n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "406 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nArchitecture Presentation (Approximately 20 slides; 60 Minutes)\nDriving architectural requirements, the measurable quantities you \nassociate with these requirements, and any existing standards/models/\napproaches for meeting these (2–3 slides)\nImportant architectural information (4–8 slides):\n■\n■\nContext diagram—the system within the context in which it will exist. \nHumans or other systems with which the system will interact.\n■\n■\nModule or layer view—the modules (which may be subsystems or \nlayers) that describe the system’s decomposition of functionality, along \nwith the objects, procedures, functions that populate these, and the \nrelations among them (e.g., procedure call, method invocation, callback, \ncontainment).\n■\n■\nComponent-and-connector view—processes, threads along with the \nsynchronization, data flow, and events that connect them.\n■\n■\nDeployment view—CPUs, storage, external devices/sensors along with \nthe networks and communication devices that connect them. Also shown \nare the processes that execute on the various processors.\nArchitectural approaches, patterns, or tactics employed, including what \nquality attributes they address and a description of how the approaches \naddress those attributes (3–6 slides):\n■\n■\nUse of commercial off-the-shelf (COTS) products and how they are cho-\nsen/integrated (1–2 slides).\n■\n■\nTrace of 1 to 3 of the most important use case scenarios. If possible, \ninclude the runtime resources consumed for each scenario (1–3 slides).\n■\n■\nTrace of 1 to 3 of the most important change scenarios. If possible, \ndescribe the change impact (estimated size/difficulty of the change) in \nterms of the changed modules or interfaces (1–3 slides).\n■\n■\nArchitectural issues/risks with respect to meeting the driving \narchitectural requirements (2–3 slides).\n■\n■\nGlossary (1 slide).\nSource: Adapted from [Clements 01b].\nAs may be seen in the presentation template, we expect architectural views, \nas described in Chapters 1 and 18, to be the primary vehicle for the architect \nto convey the architecture. Context diagrams, component-and-connector views, \nmodule decomposition or layered views, and the deployment view are useful in \nalmost every evaluation, and the architect should be prepared to show them. Other \nviews can be presented if they contain information relevant to the architecture \nat hand, especially information relevant to achieving important quality attribute \ngoals. \n",
      "content_length": 2506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n407\nAs a rule of thumb, the architect should present the views that he or she \nfound most important during the creation of the architecture and the views that \nhelp to reason about the most important quality attribute concerns of the system.\nDuring the presentation, the evaluation team asks for clarification based on \ntheir phase 0 examination of the architecture documentation and their knowledge \nof the business drivers from the previous step. They also listen for and write down \nany architectural tactics or patterns they see employed.\nStep 4: Identify Architectural Approaches.  The ATAM focuses on \nanalyzing an architecture by understanding its architectural approaches. As we \nsaw in Chapter 13, architectural patterns and tactics are useful for (among other \nreasons) the known ways in which each one affects particular quality attributes. \nA layered pattern tends to bring portability and maintainability to a system, \npossibly at the expense of performance. A publish-subscribe pattern is scalable \nin the number of producers and consumers of data. The active redundancy tactic \npromotes high availability. And so forth.\nBy now, the evaluation team will have a good idea of what patterns and \ntactics the architect used in designing the system. They will have studied the \narchitecture documentation, and they will have heard the architect’s presentation \nin step 3. During that step, the architect is asked to explicitly name the patterns \nand tactics used, but the team should also be adept at spotting ones not mentioned.\nIn this short step, the evaluation team simply catalogs the patterns and \ntactics that have been identified. The list is publicly captured by the scribe for all \nto see and will serve as the basis for later analysis.\nStep 5: Generate Quality Attribute Utility Tree.  In this step, the quality \nattribute goals are articulated in detail via a quality attribute utility tree. Utility \ntrees, which were described in Chapter  16, serve to make the requirements \nconcrete by defining precisely the relevant quality attribute requirements that the \narchitects were working to provide. \nThe important quality attribute goals for the architecture under consideration \nwere named in step 2, when the business drivers were presented, but not to \nany degree of specificity that would permit analysis. Broad goals such as \n“modifiability” or “high throughput” or “ability to be ported to a number of \nplatforms” establish important context and direction, and provide a backdrop \nagainst which subsequent information is presented. However, they are not \nspecific enough to let us tell if the architecture suffices. Modifiable in what way? \nThroughput that is how high? Ported to what platforms and in how much time? \nIn this step, the evaluation team works with the project decision makers to \nidentify, prioritize, and refine the system’s most important quality attribute goals. \nThese are expressed as scenarios, as described in Chapter 4, which populate the \nleaves of the utility tree.\n",
      "content_length": 3065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "408 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nStep 6: Analyze Architectural Approaches.  Here the evaluation team \nexamines the highest-ranked scenarios (as identified in the utility tree) one \nat a time; the architect is asked to explain how the architecture supports each \none. Evaluation team members—especially the questioners—probe for the \narchitectural approaches that the architect used to carry out the scenario. Along \nthe way, the evaluation team documents the relevant architectural decisions and \nidentifies and catalogs their risks, nonrisks, sensitivity points, and tradeoffs. For \nwell-known approaches, the evaluation team asks how the architect overcame \nknown weaknesses in the approach or how the architect gained assurance that \nthe approach sufficed. The goal is for the evaluation team to be convinced that \nthe instantiation of the approach is appropriate for meeting the attribute-specific \nrequirements for which it is intended.\nScenario walkthrough leads to a discussion of possible risks, nonrisks, \nsensitivity points, or tradeoff points. For example:\n■\n■The frequency of heartbeats affects the time in which the system can detect \na failed component. Some assignments will result in unacceptable values of \nthis response—these are risks. \n■\n■The number of simultaneous database clients will affect the number of \ntransactions that a database can process per second. Thus, the assignment \nof clients to the server is a sensitivity point with respect to the response as \nmeasured in transactions per second. \n■\n■The frequency of heartbeats determines the time for detection of a fault. \nHigher frequency leads to improved availability but will also consume \nmore processing time and communication bandwidth (potentially leading to \nreduced performance). This is a tradeoff. \nThese, in turn, may catalyze a deeper analysis, depending on how the architect \nresponds. For example, if the architect cannot characterize the number of clients \nand cannot say how load balancing will be achieved by allocating processes \nto hardware, there is little point in a sophisticated performance analysis. If such \nquestions can be answered, the evaluation team can perform at least a rudimentary, \nor back-of-the-envelope, analysis to determine if these architectural decisions are \nproblematic vis-à-vis the quality attribute requirements they are meant to address. \nThe analysis is not meant to be comprehensive. The key is to elicit sufficient \narchitectural information to establish some link between the architectural \ndecisions that have been made and the quality attribute requirements that need to \nbe satisfied. \nFigure 21.1 shows a template for capturing the analysis of an architectural \napproach for a scenario. As shown, based on the results of this step, the evaluation \nteam can identify and record a set of sensitivity points and tradeoffs, risks, and \nnonrisks. \nAt the end of step 6, the evaluation team should have a clear picture of the \nmost important aspects of the entire architecture, the rationale for key design \ndecisions, and a list of risks, nonrisks, sensitivity points, and tradeoff points. \nAt this point, phase 1 is concluded.\n",
      "content_length": 3203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n409\nScenario #: A12\nScenario: Detect and recover from HW failure\nof main switch.\nAttribute(s)\nAvailability\nEnvironment\nNormal operations\nStimulus\nOne of the CPUs fails\nResponse\n0.999999 availability of switch\nArchitectural decisions\nSensitivity\nTradeoff\nRisk\nNonrisk\nBackup CPU(s)\nS2\nR8\nNo backup data channel\nS3\nT3\nR9\nWatchdog\nS4\nN12\nHeartbeat\nS5\nN13\nFailover routing\nS6\nN14\nReasoning\nEnsures no common mode failure by using different hardware\nand operating system (see Risk 8)\nWorst-case rollover is accomplished in 4 seconds as computing\nstate takes that long at worst\nGuaranteed to detect failure within 2 seconds based on rates of\nheartbeat and watchdog\nWatchdog is simple and has proved reliable\nAvailability requirement might be at risk due to lack of backup\ndata channel ... (see Risk 9)\nArchitecture\ndiagram\nBackup\ny\nSwitch\nCPU\nheartbeat\n(1 sec.)\n(OS1)\nCPU with\nWatchdog\n(OS2)\nPrimar\nima\nimary\nCPU\n(OS1)\nFigure 21.1  Example of architecture approach analysis (adapted from \n[Clements 01b])\nHiatus and Start of Phase 2.  The evaluation team summarizes what it has \nlearned and interacts informally (usually by phone) with the architect during a \nhiatus of a week or two. More scenarios might be analyzed during this period, if \ndesired, or questions of clarification can be resolved. \n",
      "content_length": 1341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "410 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nPhase 2 is attended by an expanded list of participants with additional \nstakeholders attending. To use an analogy from programming: Phase 1 is akin to \nwhen you test your own program, using your own criteria. Phase 2 is when you \ngive your program to an independent quality assurance group, who will likely \nsubject your program to a wider variety of tests and environments.\nIn phase 2, step 1 is repeated so that the stakeholders understand the method \nand the roles they are to play. Then the evaluation leader recaps the results of \nsteps 2 through 6, and shares the current list of risks, nonrisks, sensitivity points, \nand tradeoffs. Now the stakeholders are up to speed with the evaluation results so \nfar, and the remaining three steps can be carried out. \nStep 7: Brainstorm and Prioritize Scenarios.  In this step, the evaluation \nteam asks the stakeholders to brainstorm scenarios that are operationally \nmeaningful with respect to the stakeholders’ individual roles. A maintainer will \nlikely propose a modifiability scenario, while a user will probably come up with \na scenario that expresses useful functionality or ease of operation, and a quality \nassurance person will propose a scenario about testing the system or being able to \nreplicate the state of the system leading up to a fault. \nWhile utility tree generation (step 5) is used primarily to understand how \nthe architect perceived and handled quality attribute architectural drivers, the \npurpose of scenario brainstorming is to take the pulse of the larger stakeholder \ncommunity: to understand what system success means for them. Scenario \nbrainstorming works well in larger groups, creating an atmosphere in which the \nideas and thoughts of one person stimulate others’ ideas. \nOnce the scenarios have been collected, they must be prioritized, for the \nsame reasons that the scenarios in the utility tree needed to be prioritized: the \nevaluation team needs to know where to devote its limited analytical time. First, \nstakeholders are asked to merge scenarios they feel represent the same behavior \nor quality concern. Then they vote for those they feel are most important. Each \nstakeholder is allocated a number of votes equal to 30 percent of the number of \nscenarios,1 rounded up. So, if there were 40 scenarios collected, each stakeholder \nwould be given 12 votes. These votes can be allocated in any way that the \nstakeholder sees fit: all 12 votes for 1 scenario, 1 vote for each of 12 distinct \nscenarios, or anything in between. \nThe list of prioritized scenarios is compared with those from the utility tree \nexercise. If they agree, it indicates good alignment between what the architect \nhad in mind and what the stakeholders actually wanted. If additional driving \nscenarios are discovered—and they usually are—this may itself be a risk, if the \ndiscrepancy is large. This would indicate that there was some disagreement in the \nsystem’s important goals between the stakeholders and the architect. \n1.  This is a common facilitated brainstorming technique.\n",
      "content_length": 3125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n411\nStep 8: Analyze Architectural Approaches.  After the scenarios have \nbeen collected and prioritized in step 7, the evaluation team guides the architect \nin the process of carrying out the highest ranked scenarios. The architect explains \nhow relevant architectural decisions contribute to realizing each one. Ideally this \nactivity will be dominated by the architect’s explanation of scenarios in terms of \npreviously discussed architectural approaches. \nIn this step the evaluation team performs the same activities as in step 6, \nusing the highest-ranked, newly generated scenarios. \nTypically, this step might cover the top five to ten scenarios, as time permits.\nStep 9: Present Results.  In step 9, the evaluation team groups risks into \nrisk themes, based on some common underlying concern or systemic deficiency. \nFor example, a group of risks about inadequate or out-of-date documentation \nmight be grouped into a risk theme stating that documentation is given insufficient \nconsideration. A group of risks about the system’s inability to function in the face \nof various hardware and/or software failures might lead to a risk theme about \ninsufficient attention to backup capability or providing high availability. \nFor each risk theme, the evaluation team identifies which of the business \ndrivers listed in step 2 are affected. Identifying risk themes and then relating them \nto specific drivers brings the evaluation full circle by relating the final results \nto the initial presentation, thus providing a satisfying closure to the exercise. \nAs important, it elevates the risks that were uncovered to the attention of \nmanagement. What might otherwise have seemed to a manager like an esoteric \ntechnical issue is now identified unambiguously as a threat to something the \nmanager is on record as caring about. \nThe collected information from the evaluation is summarized and presented \nto stakeholders. This takes the form of a verbal presentation with slides. The \nevaluation leader recapitulates the steps of the ATAM and all the information \ncollected in the steps of the method, including the business context, driving \nrequirements, constraints, and architecture. Then the following outputs are \npresented:\n■\n■The architectural approaches documented\n■\n■The set of scenarios and their prioritization from the brainstorming\n■\n■The utility tree \n■\n■The risks discovered\n■\n■The nonrisks documented\n■\n■The sensitivity points and tradeoff points found\n■\n■Risk themes and the business drivers threatened by each one\n",
      "content_length": 2580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "412 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n“. . . but it was OK.”\nYears of experience have taught us that no architecture evaluation exer-\ncise ever goes completely by the book. And yet for all the ways that an \nexercise might go terribly wrong, for all the details that can be overlooked, \nfor all the fragile egos that can be bruised, and for all the high stakes that \nare on the table, we have never had an architecture evaluation exercise \nspiral out of control. Every single one has been a success, as measured \nby the feedback we gather from clients.\nWhile they all turned out successfully, there were a few memorable \ncliffhangers. \nMore than once, we began an architecture evaluation only to discover \nthat the development organization had no architecture to be evaluated. \nSometimes there was a stack of class diagrams or vague text descriptions \nmasquerading as an architecture. Once we were promised that the archi-\ntecture would be ready by the time the exercise began, but in spite of good \nintentions, it wasn’t. (We weren’t always so prudent about pre-exercise \npreparation and qualification. Our current diligence was a result of experi-\nences like these.) But it was OK. In cases like these, the evaluation’s main \nresults included the articulated set of quality attributes, a “whiteboard” ar-\nchitecture sketched during the exercise, plus a set of documentation obliga-\ntions on the architect. In all cases, the client felt that the detailed scenarios, \nthe analysis we were able to perform on the elicited architecture, plus the \nrecognition of what needed to be done, more than justified the exercise.\nA couple of times we began an evaluation only to lose the architect in \nthe middle of the exercise. In one case, the architect resigned between \npreparation and execution of the evaluation. This was an organization in \nturmoil and the architect simply got a better offer in a calmer environment \nelsewhere. Normally we don’t proceed without the architect, but it was OK. \nIn this case the architect’s apprentice stepped in. A little additional prework \nto prepare him, and we were all set. The evaluation went off as planned, \nand the preparation that the apprentice did for the exercise helped mightily \nto prepare him to step into the architect’s shoes.\nOnce we discovered halfway through an ATAM exercise that the archi-\ntecture we had prepared to evaluate was being jettisoned in favor of a new \none that nobody had bothered to mention. During step 6 of phase 1, the ar-\nchitect responded to a problem raised by a scenario by casually mentioning \nthat “the new architecture” would not suffer from that deficiency. Everyone \nin the room, stakeholders and evaluators alike, looked at each other in the \npuzzled silence that followed. “What new architecture?” I asked blankly, and \nout it came. The developing organization (a contractor for the U.S. military, \nwhich had commissioned the evaluation), had prepared a new architecture \nfor the system, to handle the more stringent requirements they knew were \ncoming in the future. We called a timeout, conferred with the architect and \nthe client, and decided to continue the exercise using the new architecture \nas the subject instead of the old. We backed up to step 3 (the architecture \npresentation), but everything else on the table—business drivers, utility \n",
      "content_length": 3370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "21.2  The Architecture Tradeoff Analysis Method\n413\ntree, scenarios—still were completely valid. The evaluation proceeded as \nbefore, and at the conclusion of the exercise our military client was ex-\ntremely pleased at the knowledge gained.\nIn perhaps the most bizarre evaluation in our experience, we lost the \narchitect midway through phase 2. The client for this exercise was the \nproject manager in an organization undergoing a massive restructuring. \nThe manager was a pleasant gentleman with a quick sense of humor, but \nthere was an undercurrent about him that said he was not to be crossed. \nThe architect was being reassigned to a different part of the organization \nin the near future; this was tantamount to being fired from the project, and \nthe manager said he wanted to establish the quality of the architecture \nbefore his architect’s awkward departure. (We didn’t find any of this out \nuntil after the evaluation.) When we set up the ATAM exercise, the manager \nsuggested that the junior designers attend. “They might learn something,” \nhe said. We agreed. As the exercise began, our schedule (which was very \ntight to begin with) kept being disrupted. The manager wanted us to meet \nwith his company’s executives. Then he wanted us to have a long lunch \nwith someone who could, he said, give us more architectural insights. The \nexecutives, it turned out, were busy just now, and so could we come back \nand meet with them a bit later? By now, phase 2 was thrown off schedule \nby so much that the architect, to our horror, had to leave to fly back to his \nhome in a distant city. He was none too happy that his architecture was \ngoing to be evaluated without him. The junior designers, he said, would \nnever be able to answer our questions. Before his departure, our team \nhuddled. The exercise seemed to be teetering on the brink of disaster. We \nhad an unhappy departing architect, a blown schedule, and questionable \nexpertise available. We decided to split our evaluation team. One half of the \nteam would continue with phase 2 using the junior designers as our infor-\nmation resource. The second half of the team would continue with phase 2 \nby telephone the next day with the architect. Somehow we would make the \nbest of a bad situation. \nSurprisingly, the project manager seemed completely unperturbed by \nthe turn of events. “It will work out, I’m sure,” he said pleasantly, and then \nretreated to confer with various vice presidents about the reorganization. \nI led the team interviewing the junior designers. We had never gotten \na completely satisfactory architecture presentation from the architect. \nDiscrepancies in the documentation were met with a breezy “Oh, well, \nthat’s not how it really works.” So I decided to start over with ATAM step 3. \nWe asked the half dozen or so designers what their view of the architecture \nwas. “Could you draw it?” I asked them. They looked at each other ner-\nvously, but one said, “I think I can draw part of it.” He took to the whiteboard \nand drew a very reasonable component-and-connector view. Someone else \nvolunteered to draw a process view. A third person drew the architecture for \nan important offline part of the system. Others jumped in to assist.\nAs we looked around the room, everyone was busy transcribing the \nwhiteboard pictures. None of the pictures corresponded to anything we \nhad seen in the documentation so far. “Are these diagrams documented \n",
      "content_length": 3434,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "414 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nanywhere?” I asked. One of the designers looked up from his busy scrib-\nbling for a moment to grin. “They are now,” he said.\nAs we proceeded to step 8, analyzing the architecture using the \nscenarios previously captured, the designers did an astonishingly good \njob of working together to answer our questions. Nobody knew everything, \nbut everybody knew something. Together in a half day, they produced a \nclear and consistent picture of the whole architecture that was much more \ncoherent and understandable than anything the architect had been willing \nto produce in two whole days of pre-exercise discussion. And by the end \nof phase 2, the design team was transformed. This erstwhile group of \ninformation-starved individuals with limited compartmentalized knowledge \nbecame a true architecture team. The members drew out and recognized \neach others’ expertise. This expertise was revealed and validated in front \nof everyone—and most important, in front of their project manager, who \nhad slipped back into the room to observe. There was a look of supreme \nsatisfaction on his face. It began to dawn on me that—you guessed it—it \nwas OK. \nIt turned out that this project manager knew how to manipulate events \nand people in ways that would have impressed Machiavelli. The architect’s \ndeparture was not because of the reorganization, but merely coincident \nwith it. The project manager had orchestrated it. The architect had, the \nmanager felt, become too autocratic and dictatorial, and the manager \nwanted the junior design staff to be given the opportunity to mature and \ncontribute. The architect’s mid-exercise departure was exactly what the \nproject manager had wanted. And the design team’s emergence under \nfire had been the primary purpose of the evaluation exercise all along. \nAlthough we found several important issues related to the architecture, the \nproject manager knew about every one of them before we ever arrived. In \nfact, he made sure we uncovered some of them by a few discreet remarks \nduring breaks or after a day’s session.\nWas this exercise a success? The client could not have been more \npleased. His instincts about the architecture’s strengths and weaknesses \nwere confirmed. We were instrumental in helping his design team, which \nwould guide the system through the stormy seas of the company’s \nreorganization, come together as an effective and cohesive unit at exactly \nthe right time. And the client was so pleased with our final report that he \nmade sure the company’s board of directors saw it.\nThese cliffhangers certainly stand out in our memory. There was no \narchitecture documented. But it was OK. It wasn’t the right architecture. \nBut it was OK. There was no architect. But it was OK. The client really only \nwanted to effect a team reorganization. In every instance we reacted as \nreasonably as we could, and each time it was OK.\nWhy? Why, time after time, does it turn out OK? I think there are three \nreasons.\nFirst, the people who have commissioned the architecture evaluation \nreally want it to succeed. The architect, developers, and stakeholders \n",
      "content_length": 3179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "21.3  Lightweight Architecture Evaluation\n415\nassembled at the client’s behest also want it to succeed. As a group, they \nhelp to keep the exercise marching toward the goal of architectural insight. \nSecond, we are always honest. If we feel that the exercise is derailing, \nwe call a timeout and confer among ourselves, and usually confer with \nthe client. While a small amount of bravado can come in handy during \nan exercise, we never, ever try to bluff our way through an evaluation. \nParticipants can detect that instinctively, and the evaluation team must \nnever lose the respect of the other participants. Third, the methods are \nconstructed to establish and maintain a steady consensus throughout the \nexercise. There are no surprises at the end. The participants lay down \nthe ground rules for what constitutes a suitable architecture, and they \ncontribute to the risks uncovered at every step of the way.\nSo: Do the best job you can. Be honest. Trust the methods. Trust in the \ngoodwill and good intentions of the people you have assembled. And it will \nbe OK. (Adapted from [Clements 01b])\n—PCC\n21.3  Lightweight Architecture Evaluation\nAlthough we attempt to use time in an ATAM exercise as efficiently as possible, \nit remains a substantial undertaking. It requires some 20 to 30 person-days of \neffort from an evaluation team, plus even more for the architect and stakeholders. \nInvesting this amount of time only makes sense on a large and costly project, \nwhere the risks of making a major mistake in the architecture are unacceptable. \nFor this reason, we have developed a Lightweight Architecture Evaluation \nmethod, based on the ATAM, for smaller, less risky projects. A Lightweight \nArchitecture Evaluation exercise may take place in a single day, or even \na half-day meeting. It may be carried out entirely by members internal to the \norganization. Of course this lower level of scrutiny and objectivity may not \nprobe the architecture as deeply, but this is a cost/benefit tradeoff that is entirely \nappropriate for many projects.\nBecause the participants are all internal to the organization and fewer in \nnumber than for the ATAM, giving everyone their say and achieving a shared \nunderstanding takes much less time. Hence the steps and phases of a Lightweight \nArchitecture Evaluation can be carried out more quickly. A suggested schedule \nfor phases 1 and 2 is shown in Table 21.4.\n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "416 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\nTable 21.4  A Typical Agenda for Lightweight Architecture Evaluation\nStep\nTime \nAllotted\nNotes\n1: Present the ATAM\n0 hrs\nThe participants are familiar with the process. \nThis step may be omitted.\n2: Present Business \nDrivers\n0.25 hrs\nThe participants are expected to understand \nthe system and its business goals and their \npriorities. Fifteen minutes is allocated for a brief \nreview to ensure that these are fresh in every-\none’s mind and that there are no surprises.\n3: Present Architecture\n0.5 hrs\nAgain, all participants are expected to be famil-\niar with the system and so a brief overview of \nthe architecture, using at least module and C&C \nviews, is presented and 1 to 2 scenarios are \ntraced through these views.\n4: Identify Architectural \nApproaches\n0.25 hrs\nThe architecture approaches for specific quality \nattribute concerns are identified by the architect. \nThis may be done as a portion of step 3.\n5: Generate Quality \nAttribute Utility Tree\nVariable\n0.5 hrs – \n1.5 hrs\nScenarios might exist: part of previous evals, \npart of design, part of requirements elicitation. \nIf you’ve got ’em, use ’em and make them into a \ntree. Half hour. Otherwise, it will take longer. \nA utility tree should already exist; the team re-\nviews the existing tree and updates it, if needed, \nwith new scenarios, new response goals, or new \nscenario priorities and risk assessments.\n6: Analyze Architectural \nApproaches\n2–3 hrs\nThis step—mapping the highly ranked scenari-\nos onto the architecture—consumes the bulk of \nthe time and can be expanded or contracted as \nneeded.\n7: Brainstorm and \nPrioritize Scenarios\n0 hrs\nThis step can be omitted as the assembled (in-\nternal) stakeholders are expected to contribute \nscenarios expressing their concerns in step 5.\n8: Analyze Architectural \nApproaches\n0 hrs\nThis step is also omitted, since all analysis is \ndone in step 6.\n9: Present Results\n0.5 hrs\nAt the end of an evaluation, the team reviews \nthe existing and newly discovered risks, non-\nrisks, sensitivities, and tradeoffs and discusses \nwhether any new risk themes have arisen. \nTOTAL\n4–6 hrs\nThere is no final report, but (as in the regular ATAM) a scribe is responsible \nfor capturing results, which can then be distributed and serve as the basis for risk \nremediation.\nAn entire Lightweight Architecture Evaluation can be prosecuted in less than \na day—perhaps an afternoon. The results will depend on how well the assembled \nteam understands the goals of the method, the techniques of the method, and the \nsystem itself. The evaluation team, being internal, is typically not objective, and \nthis may compromise the value of its results—one tends to hear fewer new ideas \n",
      "content_length": 2751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "21.5  For Further Reading\n417\nand fewer dissenting opinions. But this version of evaluation is inexpensive, easy \nto convene, and relatively low ceremony, so it can be quickly deployed whenever \na project wants an architecture quality assurance sanity check.\n21.4  Summary\nIf a system is important enough for you to explicitly design its architecture, then \nthat architecture should be evaluated. \nThe number of evaluations and the extent of each evaluation may vary from \nproject to project. A designer should perform an evaluation during the process of \nmaking an important decision. Lightweight evaluations can be performed several \ntimes during a project as a peer review exercise. \nThe ATAM is a comprehensive method for evaluating software architectures. \nIt works by having project decision makers and stakeholders articulate a precise \nlist of quality attribute requirements (in the form of scenarios) and by illuminating \nthe architectural decisions relevant to carrying out each high-priority scenario. \nThe decisions can then be understood in terms of risks or nonrisks to find any \ntrouble spots in the architecture.\nLightweight Architecture Evaluation, based on the ATAM, provides an \ninexpensive, low-ceremony architecture evaluation that can be carried out in an \nafternoon. \n21.5  For Further Reading\nFor a more comprehensive treatment of the ATAM, see [Clements 01b].\nMultiple case studies of applying the ATAM are available. They can be found \nby going to www.sei.cmu.edu/library and searching for “ATAM case study.”\nTo understand the historical roots of the ATAM, and to see a second (simpler) \narchitecture evaluation method, you can read about the software architecture \nanalysis method (SAAM) in [Kazman 94]. \nSeveral lighter weight architecture evaluation methods have been developed. \nThey can be found in [Bouwers 10], [Kanwal 10], and [Bachmann 11].\nMaranzano et al. have published a paper dealing with a long tradition of \narchitecture evaluation at AT&T and its successor companies [Maranzano 05]. \n",
      "content_length": 2027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "418 \nPart Three  Architecture in the Life Cycle\t\n21—Architecture Evaluation\n21.6  Discussion Questions \n1.\t\nThink of a software system that you’re working on. Prepare a 30-minute \npresentation on the business drivers for this system.\n2.\t\nIf you were going to evaluate the architecture for this system, who would \nyou want to participate? What would be the stakeholder roles and who \ncould you get to represent those roles?\n3.\t\nUse the utility tree that you wrote for the ATM in Chapter 16 and the \ndesign that you sketched for the ATM in Chapter 17 to perform the scenario \nanalysis step of the ATAM. Capture any risks and nonrisks that you \ndiscover. Better yet, perform the analysis on the design carried out by a \ncolleague.\n4.\t\nIt is not uncommon for an organization to evaluate two competing \narchitectures. How would you modify the ATAM to produce a quantitative \noutput that facilitates this comparison?\n5.\t\nSuppose you’ve been asked to evaluate the architecture for a system in \nconfidence. The architect isn’t available. You aren’t allowed to discuss the \nevaluation with any of the system’s stakeholders. How would you proceed?\n6.\t\nUnder what circumstances would you want to employ a full-strength ATAM \nand under what circumstances would you want to employ a Lightweight \nArchitecture Evaluation?\n",
      "content_length": 1308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "419\n22\nManagement and \nGovernance\nHow does a project get to be a year behind \nschedule? One day at a time.\n—Fred Brooks\nIn this chapter we deal with those aspects of project management and governance \nthat are important for an architect to know. The project manager is the person \nwith whom you, as the architect, must work most closely, from an organizational \nperspective, and consequently it is important for you to have an understanding of \nthe project manager’s problems and the techniques available to solve those prob-\nlems. We will deal with project management from the perspectives of planning, \norganizing, implementing, and measuring. We will also discuss various gover-\nnance issues associated with architecture.\nIn this chapter, we advocate a middleweight approach to architecture. It has the \nfollowing aspects:\n■\n■Design the software architecture\n■\n■Use the architecture to develop realistic schedules\n■\n■Use incremental development to get to market quickly\nArchitecture is most useful in medium- to large-scale projects—projects that typ-\nically have multiple teams, too much complexity for any individual to fully com-\nprehend, substantial investment, and multiyear duration. For such projects the \nteams need to coordinate, quality attribute problems are not easily corrected, and \nmanagement demands both short time to market and adequate oversight. Light-\nweight management methods do not provide for a framework to guide team coor-\ndination and frequently require extensive restructuring to repair quality attribute \nproblems. Heavyweight management is usually associated with heavy oversight \nand a great emphasis on contractual commitments. In some contexts, this is un-\navoidable, but it has an inherent overhead that slows down development.\n",
      "content_length": 1766,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "420 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.1  Planning\nThe planning for a project proceeds over time. There is an initial plan that is nec-\nessarily top-down to convince upper management to build this system and give \nthem some idea of the cost and schedule. This top-down schedule is inherently \ngoing to be incorrect, possibly by large amounts. It does, however, enable the \nproject manager to educate upper managers as to different elements necessary in \nsoftware development. According to Dan Paulish, based on his experience at Sie-\nmens Corporation, some rules of thumb that can be used to estimate the top-down \nschedule for medium-sized (~150 KSLOC) projects are these:\n■\n■Number of components to be estimated: ~150\n■\n■Paper design time per component: ~4 hours\n■\n■Time between engineering releases: ~8 weeks \n■\n■Overall project development allocation:\n■\n■40 percent design: 5 percent architectural, 35 percent detailed \n■\n■20 percent coding\n■\n■40 percent testing\nOnce the system has been given a go-ahead and a budget, the architecture \nteam is formed and produces an initial architecture design. The budget item de-\nserves some further mention. One case is that the budget is for the whole project \nand includes the schedule as well. We will call this case top-down planning. The \nsecond case is that the budget is just for the architecture design phase. In this case, \nthe overall project budget emerges from the architecture design phase. This pro-\nvides a gate that the team has to pass through and gives the holders of the purse \nstrings a chance to consider whether the value of the project is worth the cost. \nWe now describe a merged process that includes both the top-down budget \nand schedule as well as a bottom-up budget and schedule that emerges from the \narchitecture design phase.\nThe architecture team produces the initial architecture design and the re-\nlease plans for the system: what features will be released and when the releases \nwill occur. Once an initial architecture design has been produced, then leads \nfor the various pieces of the project can be assigned and they can build their \nteams. The definition of the various pieces of the project and their assignment \nto teams is sometimes called the work breakdown structure. At this point, cost \nand schedule estimates from the team leads and an overall project schedule can \nbe produced. This bottom-up schedule is usually much more accurate than the \ntop-down schedule, but there may also be significant differences due to differing \nassumptions. These differences between the top-down and bottom-up schedules \nneed to be discussed and sometimes negotiated. Finally, a software development \nplan can be written that shows the initial (internal) release as the architectural \nskeleton with feature-oriented incremental development after that. The features \nto be in each release are developed in consultation with marketing.\n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "22.1  Planning\n421\nSoftware \nDevelopment Plan\nTop-Down\nSchedule\nBottom-Up\nSchedule\nFirst-Level\nDecomposition\nReconciliation\nFigure 22.1  Overview of planning process \nFigure 22.1 shows a process that includes both a top-down schedule and a \nbottom-up schedule.\nOnce the software development plan has been written, the teams can deter-\nmine times and groups for integration and can define the coordination needs for \nthe various projects. As we will see in the subsection on global development in \nSection 22.2, the coordination needs for distributed teams can be significant.\n",
      "content_length": 576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "422 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.2  Organizing\nSome of the elements of organizing a project are team organization, division of \nresponsibilities between project manager and software architect, and planning for \nglobal or distributed development.\nSoftware Development Team Organization\nOnce the architecture design is in place, it can be used to define the project or-\nganization. Each member of the team that designed the software architecture be-\ncomes the lead for a team whose responsibility is to implement a portion of the \narchitecture. Thus, responsibility for fleshing out and implementing the design is \ndistributed to those who had a role in its definition.\nIn addition, many support functions such as writing user documentation, \nsystem testing, training, integration, quality assurance, and configuration man-\nagement are done in a “matrix” form. That is, individuals who are “matrixed” \nreport to one person—a functional manager—for their tasking and professional \nadvancement and to another individual (or to several individuals)—project man-\nagers—for their project responsibilities. Matrix organizations have the advantage \nof being able to allocate and balance resources as needed rather than assign them \npermanently to a project that may have sporadic needs for individuals with par-\nticular skills. They have the disadvantage that the people in them tend to work on \nseveral projects simultaneously. This can cause problems such as divided loyal-\nties or competition among projects for resources.\nTypical roles within a software development team are the following:\n■\n■Team leader—manages tasks within the team.\n■\n■Developer—designs and implements subsystem code.\n■\n■Configuration manager—performs regular builds and integration tests. This \nrole can frequently be shared among multiple software development teams.\n■\n■System test manager—system test and acceptance testing.\n■\n■Product manager—represents marketing; defines feature sets and how \nsystem being developed integrates with other systems in a product suite.\nDivision of Responsibilities between Project Manager and \nSoftware Architect\nOne of the important relations within a team is between the software architect \nand the project manager. You can view the project manager as responsible for the \nexternal-facing aspects of the project and the software architect as responsible \nfor the internal aspects of the project. This division will only work if the external \nview accurately reflects the internal situation and the internal activities accurately \n",
      "content_length": 2581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "22.2  Organizing\n423\nreflect the expectations of the external stakeholders. That is, the project manager \nshould know, and reflect to management, the progress and the risks within the \nproject, and the software architect should know, and reflect to developers, stake-\nholder concerns. The relationship between the project manager and the software \narchitect will have a large impact on the success of a project. They need to have a \ngood working relation and be mindful of the roles they are filling and the bound-\naries of those roles.\nWe use the knowledge areas for project management taken from the Proj-\nect Management Body of Knowledge (PMBOK) to show the duties of these two \nroles in a variety of categories. Table 22.1 (on the next page) gives the knowledge \narea in the language of the PMBOK, defines the knowledge area in English, what \nthat means in software terms, and how the project manager and the software ar-\nchitect collaborate to satisfy that category.\nObserve in this table that the project manager is responsible for the business \nside of the project—providing resources, creating and managing the budget and \nschedule, negotiating with marketing, ensuring quality—and the software archi-\ntect is responsible for the technical side of the project—achieving quality, deter-\nmining measures to be used, reviewing requirements for feasibility, generating \ndevelop-time requirements, and leading the development team. \nGlobal Development\nMost substantial projects today are developed by distributed teams. In many or-\nganizations these teams are globally distributed. Some reasons for this trend are \nthe following:\n■\n■Cost. Labor costs vary depending on location, and there is a perception that \nmoving some development to a low-cost venue will decrease the overall \ncost of the project. Experience has shown that, at least for software devel-\nopment, savings may only be reaped in the long term. Until the developers \nin the low-cost venue have a sufficient level of domain expertise and until \nthe management practices are adapted to compensate for the difficulties of \ndistributed development, a large amount of rework must be done, thereby \ncutting into and perhaps overwhelming any savings from wages. \n■\n■Skill sets and labor availability. Organizations may not be able to hire \ndevelopers at a single location: relocation costs are high, the size of the de-\nveloper pool may be small, or the skill sets needed are specialized and un-\navailable in a single location. Developing a system in a distributed fashion \nallows for the work to move to where the workers are rather than forcing \nthe workers to move to the work location. \n■\n■Local knowledge of markets. Developers who are developing variants of a \nsystem to be sold in their market have more knowledge about the types of \nfeatures that are assumed and the types of cultural issues that may arise.\n",
      "content_length": 2879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "424 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nTable 22.1  Division of Responsibilities between Project Manager and Architect\nPMBOK Knowledge \nArea\nDescription\nTask\nProject Manager\nSoftware Architect\nProject Integration \nManagement\nEnsuring that the \nvarious elements of \nthe project are properly \ncoordinated\nDeveloping, oversee-\ning, and updating the \nproject plan. Manag-\ning change control \nprocess.\nOrganize project, man-\nage resources, bud-\ngets and schedules. \nDefine metrics and \nmetric collection strat-\negy. Oversee change \ncontrol process.\nCreate, design, and organize \nteam around design. Manage de-\npendencies. Implement the cap-\nture of the metrics. Orchestrate \nrequests for changes. Ensure \nthat appropriate IT infrastructure \nexists.\nProject Scope \nManagement\nEnsuring that the proj-\nect includes all of the \nwork required and only \nthe work required\nRequirements \nNegotiate project \nscope with marketing \nand software architect.\nElicit, negotiate, and review run-\ntime requirements and generate \ndevelopment requirements. \nEstimate cost, schedule, and risk \nof meeting requirements.\nProject Time  \nManagement\nEnsuring that the \nproject completes in a \ntimely fashion\nWork breakdown \nstructure and comple-\ntion tracking. Project \nnetwork diagram with \ndates.\nOversee progress \nagainst schedule. Help \ndefine work breakdown \nstructure. Schedule \ncoarse activities to \nmeet deadlines.\nHelp define work breakdown \nstructure. Define tracking mea-\nsures. Recommend assignment \nof resources to software develop-\nment teams.\nProject Cost  \nManagement\nEnsuring that the proj-\nect is completed within \nthe required budget\nResource planning, \ncost estimation, cost \nbudgeting\nCalculate cost to \ncompletion at various \nstages. Make deci-\nsions regarding build/\nbuy and allocation of \nresources.\nGather costs from individual \nteams. Make recommendations \nregarding build/buy and resource \nallocations.\nProject Quality \nManagement\nEnsuring that the \nproject will satisfy the \nneeds for which it was \nundertaken\nQuality and metrics\nDefine productivity, \nsize, and project-level \nquality measures.\nDesign for quality and track \nsystem against design. Define \ncode-level quality metrics. \n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "22.2  Organizing\n425\nPMBOK Knowledge \nArea\nDescription\nTask\nProject Manager\nSoftware Architect\nProject Human \nResource  \nManagement\nEnsuring that the \nproject makes the most \neffective use of the \npeople involved with the \nproject\nManaging people and \ntheir careers\nMap skill sets of peo-\nple against required \nskill sets. Ensure that \nappropriate training is \nprovided. Monitor and \nmentor career paths of \nindividuals. Authorize \nrecruitment.\nDefine required technical skill \nsets. Mentor developers about ca-\nreer paths. Recommend training. \nInterview candidates.\nProject  \nCommunications \nManagement\nEnsuring timely and \nappropriate generation, \ncollection, dissemi-\nnation, storage, and \ndisposition of project \ninformation\nCommunicating\nManage communi-\ncation between team \nand external entities. \nReport to upper man-\nagement.\nEnsure communication and \ncoordination among developers. \nSolicit feedback as to progress, \nproblems, and risks.\nProject Risk  \nManagement\nIdentifying, analyzing, \nand responding to \nproject risk\nRisk management\nPrioritize risks. Report \nrisks to management. \nTake steps to mitigate \nrisks.\nIdentify and quantify risks. Adjust \narchitecture and processes to \nmitigate risk.\nProject  \nProcurement  \nManagement\nAcquiring goods and \nservices from outside \norganization\nTechnology\nProcure necessary \nresources. Introduce \nnew technology. \nDetermine technology require-\nments. Recommend technology, \ntraining, and tools.\n",
      "content_length": 1450,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "426 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nA consequence of performing global or distributed development is that ar-\nchitecture and allocation of responsibilities to teams is more important than in \nco-located development, where all of the developers are in a single office, or at \nleast in close proximity. For example, assume Module A uses an interface from \nModule B. In time, as circumstances change, this interface may need to be mod-\nified. This means the team responsible for Module B must coordinate with the \nteam responsible for Module A, as indicated in Figure 22.2.\nMethods for coordination include the following:\n■\n■Informal contacts. Informal contacts, such as meeting at the coffee room or \nin the hallway, are only possible if the teams are co-located. \n■\n■Documentation. Documentation, if it is well written, well organized, and \nproperly disseminated, can be used as a means to coordinate the teams, \nwhether co-located or at a distance. \n■\n■Meetings. Teams can hold meetings, either scheduled or ad hoc and either \nface to face or remote, to help bring the team together and raise awareness \nof issues.\n■\n■Electronic communication. Various forms of electronic communication can \nbe used as a coordination mechanism, such as email, news groups, blogs, \nand wikis.\nThe choice of method depends on many factors, including infrastructure \navailable, corporate culture, language skills, time zones involved, and the number \nof teams dependent on a particular module. Until an organization has established \na working method for coordinating among distributed teams, misunderstandings \namong the teams are going to cause delays and, in some cases, serious defects in \na project.\nCoordina\u0013on\t\r  \nModule\t\r  A\t\r  \n\t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \nTeam\t\r  A\t\r  \nDependency\t\r  \nModule\t\r  B\t\r  \n\t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \n\t\r  \t\r  \t\r  \t\r  \nTeam\t\r  B\t\r  \nFigure 22.2  If there is a dependency between Module A and Module B, then \nthe teams must coordinate to develop and modify the interface.\n",
      "content_length": 2041,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "22.3  Implementing\n427\n22.3  Implementing\nDuring the implementation phase of a project, the project manager and architect \nhave a series of decisions to make. In this section we discuss those involving \ntradeoffs, incremental development, and managing risk.\nTradeoffs\nFrom the project manager’s perspective, tradeoffs are between quality, schedule, \nfunctionality, and cost. These are the aspects of the project that are important to \nthe external stakeholders, and the external stakeholders are the project manager’s \nconstituency. Which of these aspects is most important depends on the project \ncontext, and one of the project manager’s major responsibilities is to make this \ndetermination.\nOver time, there is always new functionality that someone wants to have \nadded to the project. Frequently these requests come from the marketing depart-\nment. It is important that the consequences of these new requirements, in terms \nof cost and schedule, be communicated to all concerned stakeholders. This is an \narea where the project manager and the architect must cooperate. What appear \nto be small requirements changes from an outsider’s perspective can, at times, \nrequire major modifications to the architecture and consequently delay a project \nsignificantly.\nThe project manager’s first response to creeping functionality is to resist \nit. Acting as a gatekeeper for the project and shielding it from distractions is a \nportion of the job description. One technique that is frequently used to manage \nchange is a change control board. Bureaucracy can, at times, be your friend. \nChange control boards are committees set up for the purpose of managing change \nwithin a project. The original architecture team members are good candidates to \nsit on the change control board. Before changing an interface, for example, the \nimpact on those modules that depend on the interface needs to be considered. \nAny change to the architecture will incur costs, and it is the architect’s re-\nsponsibility to be the gatekeeper for such changes. A change in the architecture \nimplies changes in code, changes in the architecture documentation, and perhaps \nchanges in build-time tools that enforce architectural conformance. \nDocumentation is especially important in distributed development. Co-lo-\ncated teams have a variety of informal coordination possibilities, such as going \nto the next office or meeting in the coffee room or the hall. Remote teams do not \nhave these informal mechanisms and so must rely on more formal mechanisms \nsuch as documentation; team members must have the initiative to talk to each \nother when doubts arise. One company mounted a webcam on each developer’s \ndesktop to facilitate personal communication.\n",
      "content_length": 2728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "428 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\nIncremental Development\nRecall that the software development plan lays out the overall schedule. Every six \nto eight weeks a new release should be available and the specifics of the next re-\nlease are decided. Forty percent of a typical project’s effort is devoted to testing. \nThis means that testing should begin as soon as possible. Testing for a release can \nbegin once the forward development has begun on the next release. The schedule \nalso has to accommodate repairing the faults uncovered by the testing. This leads \nto a release being in one of three states:\n1.\t\nPlanning. This occurs toward the end of the prior development release. \nEnough of the prior release must be completed to understand what will be \nunfinished in that release and must be carried forward to the next one. At \nthis stage, the software development plan is updated.\n2.\t\nDevelopment. The planned release is coded. We will discuss below how \nthe project manager and architect track progress on the release. Daily \nbuilds and automated testing can give some insight into problems during \ndevelopment.\n3.\t\nTest and repair. The release is tested through exercise of the test plan. In \nChapter 19 we described how the architecture can inform the test plan \nand even obviate the need for certain types of testing. The problems found \nduring test are repaired or are carried forward to the next release.\nTracking Progress\nThe project manager can track progress through personal contact with developers \n(this tends to not scale up well), formal status meetings, metrics, and risk man-\nagement. Metrics will be discussed in the next section. Personal contact involves \nchecking with key personnel individually to determine progress. These are one-\non-one meetings, either scheduled or unscheduled.\nMeetings, in general, are either status or working meetings. The two types \nof meetings should not be mixed. In a status meeting, various teams report on \nprogress. This allows for communication among the teams. Issues raised at status \nmeetings should be resolved outside of these meetings—either by individuals or \nby separately scheduled working meetings. When an issue is raised at a status \nmeeting, a person should be assigned to be responsible for the resolution of that \nissue.\nMeetings are expensive. Holding effective meetings is an important skill for \na manager, whether the project manager or the architect. Meetings should have \nwritten agendas that are circulated before the meetings begin, attendees should be \nexpected to have done some prework for the meeting (such as read-ahead), and \nonly essential individuals should attend.\nOne of the outputs of status meetings is a set of risks. A risk is a potential \nproblem together with the consequences if it occurs. The likelihood of the risk \n",
      "content_length": 2858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "22.4  Measuring\n429\noccurring should also be recorded. Risks are also raised at reviews. We have dis-\ncussed architecture evaluation, and it is important from a project management \nperspective that reviews are included in the schedule. These can be code reviews, \narchitecture reviews, or requirements reviews. Risks are also raised by develop-\ners. They are the ones who have the best perspective on potential problems at the \nimplementation level. Architecture evaluation is also important because it is a \nsource of discovering risks.\nThe project manager must prioritize the risks, frequently with the assistance \nof the architect, and, for the most serious risks, develop a mitigation strategy. \nMitigating risks is also a cost, and so implementing the strategy to reduce a risk \ndepends on its priority, likelihood of occurrence, and cost if it does occur.\n22.4  Measuring\nMetrics are an important tool for project managers. They enable the manager to \nhave an objective basis both for their own decision making and for reporting to \nupper management on the progress of the project.\nMetrics can be global—pertaining to the whole project—or they may de-\npend on a particular phase of the project. Another important class of metrics is \n“cost to complete.” We discuss each of these below.\nGlobal Metrics\nGlobal metrics aid the project manager in obtaining an overall sense of the proj-\nect and tracking its progress over time. All global metrics should be updated from \ntime to time as the project proceeds. \nFirst, the project manager needs a measure of project size. The three most \ncommon measures of project size are lines of code, function points, and size of \nthe test suite. None of these is completely satisfactory as a predictor of cost or \neffort, but these are the most commonly used size metrics in practice. \nSchedule deviation is another global metric. Schedule deviation is measured \nby taking the difference between the planned work time and the actual work time. \nOnce time has passed, it can never be recovered. If a project falls behind in its \nschedule, a tradeoff must be made between the aspects we mentioned before: \nschedule, quality, functionality, and cost. As the project proceeds, schedule de-\nviation indicates a failure of estimation or an unforeseen occurrence. By drilling \ndown in this metric and discovering which teams are slipping, the project man-\nager can decide to reallocate resources, if necessary.\nDeveloper productivity is another metric that the project manager can \ntrack. The project manager should look for anomalies in the productivity of the \n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "430 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\ndevelopers. An anomaly indicates a potential problem that should be investigated: \nperhaps a developer is inadequately trained for a task, or perhaps the task was im-\nproperly estimated. Earned value management is one technique for measuring the \nproductivity of developers.\nFinally, defects should be tracked. Again, anomalies in the number of defects \ndiscovered over time indicate a potential problem that should be investigated. Not \nonly defects but technical debt should be tracked as well (see Chapter 15).\nAll of these measures have both a historical basis and a project basis. The \nhistorical basis is used to make the initial estimates and then the project basis is \nused for ongoing management activities. \nPhase Metrics and Costs to Complete\nOpen issues should be kept for each phase. For example, until a design is com-\nplete, there are always open issues. These should be tracked and additional re-\nsources allocated if they are not resolved in a timely fashion. Risks represent \nopen issues that should be tracked, as does the project backlog.\nUnmitigated risks from reviews are treated in a similar fashion. We have al-\nready discussed risk management, but one item a project manager can report to upper \nmanagement is the number of high-priority risks and the status of their mitigation.\nCosts to complete is a bottom-up measure that derives from the bottom-up \nschedule. Once the various pieces of the architecture have been assigned to teams, \nthen the teams take ownership of their schedule and the cost to complete their pieces.\n22.5  Governance\nUp to this point, we have maintained a  focus in this chapter. We have focused \non the project manager as the embodiment of the project and have not discussed \nthe external forces that act on the project manager. The topic of governance deals \ndirectly with these other forces. The Open Group defines architecture governance \nas “the practice and orientation by which enterprise architectures and other archi-\ntectures are managed and controlled.” Implicit in this definition is the idea that \nthe project—which is the focus of this book—exists in an organizational context. \nThis context will mediate the interactions of the system being constructed with \nthe other systems in the organization. \nThe Open Group goes on to identify four items as responsibilities of a gov-\nernance board:\n■\n■Implementing a system of controls over the creation and monitoring of all \narchitectural components and activities, to ensure the effective introduction, \nimplementation, and evolution of architectures within the organization\n",
      "content_length": 2662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "22.5  Governance\n431\n■\n■Implementing a system to ensure compliance with internal and external \nstandards and regulatory obligations\n■\n■Establishing processes that support effective management of the above pro-\ncesses within agreed parameters\n■\n■Developing practices that ensure accountability to a clearly identified stake-\nholder community, both inside and outside the organization\nNote the emphasis on processes and practices. Maintaining an effective gov-\nernance process without excessive overhead is a line that is difficult to maintain \nfor an organization.\nThe problem comes about because each system that exists in an enterprise has \nits own stakeholders and its own internal governance processes. Creating a system \nthat utilizes a collection of other systems raises the issue of who is in control.\nConsider the following example. Company A has a collection of products \nthat cover different portions of a manufacturing facility. One collection of sys-\ntems manages the manufacturing process, another manages the processes by \nwhich various portions of the end product are integrated, and a third collection \nmanages the enterprise. Each of these collections of systems has its own set of \ncustomers.\nNow suppose that the board of directors wishes to market the collection as \nan end-to-end solution for a manufacturing facility. It further turns out that the \nsystems that manage the manufacturing process have a 6-month release cycle be-\ncause the technology is changing quickly in this area. The systems that manage \nthe integration process have a 9-month release cycle because they are based on \na widely used commercial product with a 9-month release cycle. The enterprise \nsystems have a 12-month release cycle because they are based on an organiza-\ntion’s fiscal year and reflect tax and regulatory changes that are likely to occur \non fiscal year boundaries. Table 22.2 shows these schedules beginning at date 0.\nWhat should the release schedule be for the combined end-to-end solution? \nRecall that each of these sets of products has reasons for their release schedule \nand each has its own set of customers who will not be receptive to changes in the \nrelease schedule. This is typical of the sort of problem that a governance commit-\ntee deals with.\nTable 22.2  Release Schedules of Different Types of Products\nManufacturing Process \nControl\nIntegration Process \nControl\nEnterprise Management\nVersion 1—date 0\nVersion 1—date 0\nVersion 1—date 0\nVersion 2—date 6 months\nVersion 2—date 9 months\nVersion 2—date 12 months\nVersion 3—date 12 months\nVersion 3—date 18 months\nVersion 3—date 24 months\n",
      "content_length": 2612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "432 \nPart Three  Architecture in the Life Cycle\t\n22—Management and Governance\n22.6  Summary\nA project must be planned, organized, implemented, tracked, and governed. \nThe plan for a project is initially developed as a top-down schedule with \nan acknowledgement that it is only an estimate. Once the decomposition of the \nsystem has been done, a bottom-up schedule can be developed. The two must be \nreconciled, and this becomes the basis for the software development plan.\nTeams are created based on the software development plan. The software \narchitect and the project manager must coordinate to oversee the implementation. \nGlobal development creates a need for an explicit coordination strategy that is \nbased on more formal methods than needed for co-located development.\nThe implementation itself causes tradeoffs between schedule, function, and \ncost. Releases are done in an incremental fashion and progress is tracked by both \nformal metrics and informal communication.\nLarger systems require formal governance mechanisms. The issue of who \nhas control over a particular portion of the system may prevent some business \ngoals from being realized.\n22.7  For Further Reading\nDan Paulish has written an excellent book on managing in an architecture-centric \nenvironment— [Paulish 02]—and much of the material in this chapter is adapted \nfrom his book.\nThe Agile community has independently arrived at a medium-weight man-\nagement process that advocates up-front architecture. See [Coplein 10] for an \nexample of the Agile community’s description of architecture.\nThe responsibilities of a governance board can be found in The Open Group \nArchitecture Framework (TOGAF), produced by the Open Group. www.open-\ngroup.org/architecture/togaf8-doc/arch/chap26.html\nBasic concepts of project management are covered in [IEEE 11]. \nUsing concepts of lean manufacturing, Kanban is a method for scheduling \nthe production of a system [Ladas 09].\nEarned value management is discussed in en.wikipedia.org/wiki/Earned_ \nvalue_management\n",
      "content_length": 2029,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "22.8  Discussion Questions\n433\n22.8  Discussion Questions\n1.\t\nWhat are the reasons that top-down and bottom-up schedule estimates dif-\nfer and how would you resolve this conflict in practice?\n2.\t\nGeneric project management practices advocate creating a work breakdown \nstructure as the first artifact produced by a project. What is wrong with this \npractice from an architectural perspective?\n3.\t\nIf you were managing a globally distributed team, what architectural docu-\nmentation artifacts would you want to create first?\n4.\t\nIf you were managing a globally distributed team, what aspects of project \nmanagement would have to change to account for cultural differences?\n5.\t\nEnumerate three different global metrics and discuss the advantages and \ndisadvantages of each?\n6.\t\nHow could you use architectural evaluation as a portion of a governance \nplan?\n7.\t\nIn Chapter 1 we described a work assignment structure for software archi-\ntecture, which can be documented as a work assignment view. (Because it \nmaps software elements—modules—to a nonsoftware environmental struc-\nture—the organizational units that will be responsible for implementing the \nmodules—it is a kind of allocation view.) Discuss how documenting a work \nassignment view for your architecture provides a vehicle for software archi-\ntects and managers to work together to staff a project. Where is the dividing \nline between the part of the work assignment view that the architect should \nprovide and the part that the manager should provide?\n",
      "content_length": 1513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "435\nPart  FO UR\nArchitecture and \nBusiness\nPerhaps the most important job of an architect is to be a fulcrum where business \nand technical decisions meet and interact. The architect is responsible for many \naspects of the business, and must be continually translating business needs and \ngoals into technical realizations, and translating technical opportunities and lim-\nitations into business consequences. \nIn this section of the book, we explore some of the most important business \nconsequences of architecture. This includes treating software architecture deci-\nsions as business investments, dealing with the organizational aspects of architec-\nture (such as organizational learning and knowledge management), and looking \nat architecture as the key enabler of software product lines.\nIn Chapter 23 we discuss the economic implications of architectural de-\ncisions and provide a method—called the CBAM, or Cost Benefit Analysis \nMethod—for making rational, business-driven architectural choices. The CBAM \nbuilds upon other architecture methods and principles that you have already seen \nin this book—scenarios, quality attributes, active stakeholder involvement—but \nadds a new twist to the evaluation of architectural improvements: an explicit con-\nsideration of the utility that an architectural improvement brings. \nIn Chapter 24 we consider the fact that architectures are created by actual \nhuman beings, called architects working in actual organizations. This chapter \nconsiders the questions of how to foster individual competence—that is, how \n",
      "content_length": 1560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "436\nto create competent architects—and how to create architecturally competent \norganizations. \nFinally, in Chapter 25 we look at the concept of software product lines. Not \nsurprisingly, we find that software architectures are the most important compo-\nnent of software-intensive product lines.\n",
      "content_length": 296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "437\n23\nEconomic Analysis of \nArchitectures\nArthur Dent: “I think we have different value systems.” \nFord Prefect: “Well mine’s better.” \n—Douglas Adams, Mostly Harmless\nThus far, we have been primarily investigating the relationships between archi-\ntectural decisions and the quality attributes that the architecture’s stakeholders \nhave deemed important: If I make this architectural decision, what effect will it \nhave on the quality attributes? If I have to achieve that quality attribute require-\nment, what architectural decisions will do the trick?\nAs important as this effort is, this perspective is missing a crucial consider-\nation: What are the economic implications of an architectural decision?\nUsually an economic discussion is focused on costs, primarily the costs of \nbuilding the system in the first place. Other costs, often but not always down-\nplayed, include the long-term costs incurred through cycles of maintenance and \nupgrade. However, as we argue in this chapter, as important as costs are the bene-\nfits that an architectural decision may bring to an organization.\nGiven that the resources for building and maintaining a system are finite, \nthere must be a rational process that helps us choose among architectural options, \nduring both an initial design phase and subsequent upgrade periods. These op-\ntions will have different costs, will consume differing amounts of resources, will \nimplement different features (each of which brings some benefit to the organiza-\ntion), and will have some inherent risk or uncertainty. To capture these aspects, \nwe need economic models of software that take into account costs, benefits, risks, \nand schedule implications.\n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "438 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\n23.1  Decision-Making Context\nAs we saw in Chapter 16, business goals play a key role in requirements for ar-\nchitectures. Because major architectural decisions have technical and economic \nimplications, the business goals behind a software system should be used to \ndirectly guide those decisions. The most immediate economic implication of a \nbusiness goal decision on an architecture is how it affects the cost of implement-\ning the system. The quality attributes achieved by the architecture decisions have \nadditional economic implications because of the benefits (which we call utility) \nthat can be derived from those decisions; for example, making the system faster \nor more secure or easier to maintain and update. It is this interplay between the \ncosts and the benefits of architectural decisions that guides (and torments) the \narchitect. Figure 23.1 show this interplay.\nFor example, using redundant hardware to achieve a desired level of avail-\nability has a cost; checkpointing to a disk file has a different cost. Furthermore, \nboth of these architectural decisions will result in (presumably different) measur-\nable levels of availability that will have some value to the organization develop-\ning the system. Perhaps the organization believes that its stakeholders will pay \nmore for a highly available system (a telephone switch or medical monitoring \nsoftware, for example) or that it will be sued if the system fails (for example, the \nsoftware that controls antilock brakes in an automobile).\nKnowing the costs and benefits associated with particular decisions enables \nreasoned selection from among competing alternatives. The economic analysis \ndoes not make decisions for the stakeholders, just as a financial advisor does not \ntell you how to invest your money. It simply aids in the elicitation and documen-\ntation of value for cost (VFC): a function of the costs, benefits, and uncertainty of \na “portfolio” of architectural investments. It gives the stakeholders a framework \nwithin which they can apply a rational decision-making process that suits their \nneeds and their risk aversion.\nBusiness\nArchitecture\nStrategies\nCost\nBenefit\n•\n•\n•\nPerformance\nSecurity\nModifiability\nUsability\nGoals\nFigure 23.1  Business goals, architectural decisions, costs, and benefits\n",
      "content_length": 2375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "23.2  The Basis for the Economic Analyses\n439\nEconomic analysis isn’t something to apply to every architectural decision, \nbut rather to the most basic ones that put an overarching architectural strategy in \nplace. It can help you assess the viability of that strategy. It can also be the key to \nobjective selection among competing strategies, each of which might have advo-\ncates pushing their own self-interests.\n23.2  The Basis for the Economic Analyses\nWe now describe the key ideas that form the basis for the economic analyses. The \npractical realization of these ideas can be packaged in a variety of ways, as we \ndescribe in Section 23.3. Our goal here is to develop the theory underpinning a \nmeasure of VFC for various architectural strategies in light of scenarios chosen \nby the stakeholders.\nWe begin by considering a collection of scenarios generated as a portion of \nrequirements elicitation, an architectural evaluation, or specifically for the eco-\nnomic analysis. We examine how these scenarios differ in the values of their pro-\njected responses and we then assign utility to those values. The utility is based on \nthe importance of each scenario being considered with respect to its anticipated \nresponse value.\nArmed with our scenarios, we next consider the architectural strategies that \nlead to the various projected responses. Each strategy has a cost, and each im-\npacts multiple quality attributes. That is, an architectural strategy could be imple-\nmented to achieve some projected response, but while achieving that response, \nit also affects some other quality attributes. The utility of these “side effects” \nmust be taken into account when considering a strategy’s overall utility. It is this \noverall utility that we combine with the project cost of an architectural strategy to \ncalculate a final VFC measure.\nUtility-Response Curves\nOur economic analysis uses quality attribute scenarios (from Chapter 4) as the \nway to concretely express and represent specific quality attributes. We vary the \nvalues of the responses, and ask what the utility is of each response. This leads to \nthe concept of a utility-response curve.\nEach scenario’s stimulus-response pair provides some utility (value) to the \nstakeholders, and the utility of different possible values for the response can be \ncompared. This concept of utility has roots that go back to the eighteenth cen-\ntury, and it is a technique to make comparable very different concepts. To help \nus make major architectural decisions, we might wish to compare the value of \nhigh performance against the value of high modifiability against the value of high \nusability, and so forth. The concept of utility lets us do that.\n",
      "content_length": 2706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "440 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nAlthough sometimes it takes a little prodding to get them to do it, stakehold-\ners can express their needs using concrete response measures, such as “99.999 \npercent available.” But that leaves open the question of how much they would \nvalue slightly less demanding quality attributes, such as “99.99 percent avail-\nable.” Would that be almost as good? If so, then the lower cost of achieving that \nlower value might make that the preferred option, especially if achieving the \nhigher value was going to play havoc with another quality attribute like perfor-\nmance. Capturing the utility of alternative responses of a scenario better enables \nthe architect to make tradeoffs involving that quality attribute.\nWe can portray each relationship between a set of utility measures and a cor-\nresponding set of response measures as a graph—a utility-response curve. Some \nexamples of utility-response curves are shown in Figure 23.2. In each, points la-\nbeled a, b, or c represent different response values. The utility-response curve \nthus shows utility as a function of the response value. \nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nUtility\n100\n0\nQuality attribute response\na\nb\nc\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 23.2  Some sample utility-response curves\n",
      "content_length": 1464,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "23.2  The Basis for the Economic Analyses\n441\nThe utility-response curve depicts how the utility derived from a particular \nresponse varies as the response varies. As seen in Figure 23.2, the utility could \nvary nonlinearly, linearly, or even as a step function. For example, graph (c) por-\ntrays a steep rise in utility over a narrow change in a quality attribute response \nlevel. In graph (a), a modest change in the response level results in only a very \nsmall change in utility to the user. \nIn Section 23.3 we illustrate some ways to engage stakeholders to get them \nto construct utility curves.\nWeighting the Scenarios \nDifferent scenarios will have different importance to the stakeholders; in order \nto make a choice of architectural strategies that is best suited to the stakeholders’ \ndesires, we must weight the scenarios. It does no good to spend a great deal of ef-\nfort optimizing a particular scenario in which the stakeholders actually have very \nlittle interest. Section 23.3 presents a technique for applying weights to scenarios.\nSide Effects \nEvery architectural strategy affects not only the quality attributes it was selected \nto achieve, but also other quality attributes as well. As you know by now, these \nside effects on other quality attributes are often negative. If those effects are too \nnegative, we must make sure there is a scenario for the side effect attribute and \ndetermine its utility-response curve so that we can add its utility to the deci-\nsion-making mix. We calculate the benefit of applying an architectural strategy \nby summing its benefits to all relevant quality attributes; for some quality attri-\nbutes the benefit of a strategy might be negative.\nDetermining Benefit and Normalization\nThe overall benefit of an architectural strategy across quality attribute scenarios \nis the sum of the utility associated with each one, weighted by the importance of \nthe scenario. For each architectural strategy i, its benefit Bi over j scenarios (each \nwith weight Wj) is\nBi = ∑j (bi,j × Wj)\nReferring to Figure 23.2, each bi,j is calculated as the change in utility (over \nwhatever architectural strategy is currently in place, or is in competition with the \none being considered) brought about by the architectural strategy with respect to \nthis scenario: \nbi,j = Uexpected – Ucurrent\n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "442 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nThat is, the utility of the expected value of the architectural strategy minus \nthe utility of the current system relative to this scenario. \nCalculating Value for Cost\nThe VFC for each architectural strategy is the ratio of the total benefit, Bi, to the \ncost, Ci, of implementing it:\nVFC = Bi / Ci\nThe cost Ci is estimated using a model appropriate for the system and the \nenvironment being developed, such as a cost model that estimates implementa-\ntion cost by measuring an architecture’s interaction complexity. You can use this \nVFC score to rank-order the architectural strategies under consideration.\nConsider curves (a) and (b) in Figure 23.2. Curve (a) flattens out as the qual-\nity attribute response improves. In this case, it is likely that a point is reached past \nwhich VFC decreases as the quality attribute response improves; spending more \nmoney will not yield a significant increase in utility. On the other hand, curve (b) \nshows that a small improvement in quality attribute response can yield a very sig-\nnificant increase in utility. In that situation, an architectural strategy whose VFC \nis low might rank significantly higher with a modest improvement in its quality \nattribute response.\n23.3  Putting Theory into Practice: The CBAM\nWith the concepts in place we can now describe techniques for putting \nthem into practice, in the form of a method we call the Cost Benefit Analysis \nMethod (CBAM). As we describe the method, remember that, like all of our \nstakeholder-based methods, it could take any of the forms for stakeholder interac-\ntion that we discussed in the introduction to Part III.\nPracticalities of Utility Curve Determination\nTo build the utility-response curve, we first determine the quality attribute levels \nfor the best-case and worst-case situations. The best-case quality attribute level is \nthat above which the stakeholders foresee no further utility. For example, a sys-\ntem response to the user of 0.1 second is perceived as instantaneous, so improv-\ning it further so that it responds in 0.03 second has no additional utility. Simi-\nlarly, the worst-case quality attribute level is a minimum threshold above which a \n",
      "content_length": 2252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": "23.3  Putting Theory into Practice: The CBAM\n443\nsystem must perform; otherwise it is of no use to the stakeholders. These levels—\nbest-case and worst-case—are assigned utility values of 100 and 0, respectively.\nWe then determine the current and desired utility levels for the scenario. The re-\nspective utility values (between 0 and 100) for various alternative strategies are \nelicited from the stakeholders, using the best-case and worst-case values as refer-\nence points. For example, our current design provides utility about half as good \nas we would like, but an alternative strategy being considered would give us 90 \npercent of the maximum utility. Hence, the current utility level is set to 50 and the \ndesired utility level is set to 90.\nIn this manner the utility curves are generated for all of the scenarios.\nShow Business or Accounting?\nAs software architects, what kind of business are we in? One of Irving \nBerlin’s most famous songs is entitled “There’s No Business Like Show \nBusiness.” David Letterman, riffing off this song title, once quipped, \n“There’s no business like show business, but there are several businesses \nlike accounting.”\nHow should we think of ourselves, as architects? Consider two more \nquotations from famous business leaders:\nI never get the accountants in before I start up a business. It’s done on gut \nfeeling.   —Richard Branson\nIt has been my experience that competency in mathematics, both in numeri-\ncal manipulations and in understanding its conceptual foundations, enhances \na person’s ability to handle the more ambiguous and qualitative relationships \nthat dominate our day-to-day financial decision-making.   —Alan Greenspan\nArchitectures are at the fulcrum of a set of business, social, and techni-\ncal decisions. A poor decision in any dimension can be disastrous for an \norganization. A decision in any one dimension is influenced by the other \ndimensions. So in our roles as architects, which are we, Alan Greenspan or \nRichard Branson?\nMy claim is that, as an industry, we in software are more like Richard \nBranson. We make decisions that have enormous economic consequences \non a gut feeling, without ever examining their financial consequences in \na disciplined way. This might be OK if you are an intuitive genius, but it \ndoesn’t work well for most of us. Engineering is about making good deci-\nsions in a rational, predictable way. For this we need methods.\n—RK\n",
      "content_length": 2428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "444 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nPracticalities of Weighting Determination\nOne method of weighting the scenarios is to prioritize them and use their prior-\nity ranking as the weight. So for N scenarios, the highest priority one is given a \nweight of 1, the next highest is given a weight of (N–1)/N, and so on. This turns \nthe problem of weighting the scenarios into one of assigning priorities.\nThe stakeholders can determine the priorities through a variety of voting \nschemes. One simple method is to have each stakeholder prioritize the scenarios \n(from 1 to N) and the total priority of the scenario is the sum of the priorities it \nreceives from all of the stakeholders. This voting can be public or secret.\nOther schemes are possible. Regardless of the scheme used, it must make \nsense to the stakeholders and it must suit their culture. For example, in some cor-\nporate environments, everything is done by consensus. In others there is a strict \nhierarchy, and in still others decisions are made in a democratic fashion. In the \nend it is up to the stakeholders to make sure that the scenario weights agree with \ntheir intuition.\nPracticalities of Cost Determination\nOne of the shortcomings of the field of software architecture is that there are very \nfew cost models for various architectural strategies. There are many software cost \nmodels, but they are based on overall system characteristics such as size or func-\ntion points. These are inadequate to answer the question of how much does it \ncost to, for example, use a publish-subscribe pattern in a particular portion of the \narchitecture. There are cost models that are based on complexity of modules (by \nfunction point analysis according to the requirements assigned to each module) \nand the complexity of module interaction, but these are not widely used in prac-\ntice. More widely used in practice are corporate cost models based on previous \nexperience with the same or similar architectures, or the experience and intuition \nof senior architects.\nLacking cost models whose accuracy can be assured, architects often turn \nto estimation techniques. To proceed, remember that an absolute number for cost \nisn’t necessary to rank candidate architecture strategies. You can often say some-\nthing like “Suppose strategy A costs $x. It looks like strategy B will cost $2x, and \nstrategy C will cost $0.5x.” That’s enormously helpful. A second approach is to \nuse very coarse estimates. Or if you lack confidence for that degree of certainty, \nyou can say something like “Strategy A will cost a lot, strategy B shouldn’t cost \nvery much, and strategy C is probably somewhere in the middle.”\nCBAM\nNow we describe the method we use for economic analysis: the Cost Benefit \nAnalysis Method. CBAM has for the most part been applied when an organization \n",
      "content_length": 2863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "23.3  Putting Theory into Practice: The CBAM\n445\nwas considering a major upgrade to an existing system and they wanted to un-\nderstand the utility and value for cost of making the upgrade, or they wanted to \nchoose between competing architectural strategies for the upgrade. CBAM is also \napplicable for new systems as well, especially for helping to choose among com-\npeting strategies. Its key concepts (quality attribute response curves, cost, and \nutility) do not depend on the setting.\nSteps.  A process flow diagram for the CBAM is given in Figure 23.3. The first \nfour steps are annotated with the relative number of scenarios they consider. That \nnumber steadily decreases, ensuring that the method concentrates the stakehold-\ners’ time on the scenarios believed to be of the greatest potential in terms of VFC.\nThis description of CBAM assumes that a collection of quality attribute sce-\nnarios already exists. This collection might have come from a previous elicitation \nexercise such as an ATAM exercise (see Chapter 21) or quality attribute utility \ntree construction (see Chapter 16).\nThe stakeholders in a CBAM exercise include people who can authorita-\ntively speak to the utility of various quality attribute responses, and probably \ninclude the same people who were the source of the quality attribute scenarios \nbeing used as input. The steps are as follows:\n1.\t\nCollate scenarios. Give the stakeholders the chance to contribute new sce-\nnarios. Ask the stakeholders to prioritize the scenarios based on satisfying \nthe business goals of the system. This can be an informal prioritization \nusing a simple scheme such as “high, medium, low” to rank the scenarios. \nChoose the top one-third for further study.\n2.\t\nRefine scenarios. Refine the scenarios chosen in step 1, focusing on their \nstimulus-response measures. Elicit the worst-case, current, desired, and \nbest-case quality attribute response level for each scenario. For example, \na refined performance scenario might tell us that worst-case performance \nfor our system’s response to user input is 12 seconds, the best case is 0.1 \nseconds, and our desired response is 0.5 seconds. Our current architecture \nprovides a response of 1.5 seconds:\nScenario\nWorst Case\nCurrent\nDesired\nBest Case\nScenario #17: \nResponse to \nuser input\n12 seconds\n1.5 seconds\n0.5 seconds\n0.1 seconds\n…\n3. \nPrioritize scenarios. Prioritize the refined scenarios, based on stakeholder \nvotes. You give 100 votes to each stakeholder and have them distribute the \nvotes among the scenarios, where their voting is based on the desired response \nvalue for each scenario. Total the votes and choose the top 50 percent of \nthe scenarios for further analysis. Assign a weight of 1.0 to the highest-rated \nscenario; assign the other scenarios a weight relative to the highest rated. This \n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "446 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nbecomes the weighting used in the calculation of a strategy’s overall benefit. \nMake a list of the quality attributes that concern the stakeholders.\n4. \nAssign utility. Determine the utility for each quality attribute response level \n(worst-case, current, desired, best-case) for the scenarios from step 3. You can \nconveniently capture these utility curves in a table (one row for each scenario, \none column for each of the four quality attribute response levels). Continuing \nour example from step 2, this step would assign utility values from 1 to 100 \nfor each of the latency values elicited for this scenario in step 2:\nScenario\nWorst Case\nCurrent\nDesired\nBest Case\nScenario #17: \nResponse to \nuser input\n12 seconds\n1.5 seconds\n0.5 seconds\n0.1 seconds\nUtility 5\nUtility 50\nUtility 80\nUtility 85\n5.\t\nMap architectural strategies to scenarios and determine their expected \nquality attribute response levels. For each architectural strategy under con-\nsideration, determine the expected quality attribute response levels that will \nresult for each scenario.\n6.\t\nDetermine the utility of the expected quality attribute response levels by \ninterpolation. Using the elicited utility values (that form a utility curve), \ndetermine the utility of the expected quality attribute response level for the \narchitectural strategy. Do this for each relevant quality attribute enumerated \nin step 3. For example, if we are considering a new architectural strategy \nthat would result in a response time of 0.7 seconds, we would assign this \na utility proportionately between 50 (which it exceeds) and 80 (which it \ndoesn’t exceed).\nThe formula for interpolation between two data points (xa, ya) and (xb, yb) \nis given by:\ny = ya + (yb – ya) (x – xa)\n(xb – xa)\nFor us, the x values are the quality attribute response levels and the y \nvalues are the utility values. So, employing this formula, the utility value of \na 0.7-second response time is 74.\n7.\t\nCalculate the total benefit obtained from an architectural strategy. Subtract \nthe utility value of the “current” level from the expected level and normal-\nize it using the votes elicited in step 3. Sum the benefit due to a particular \narchitectural strategy across all scenarios and across all relevant quality \nattributes.\n8. \nChoose architectural strategies based on VFC subject to cost and schedule \nconstraints. Determine the cost and schedule implications of each archi-\ntectural strategy. Calculate the VFC value for each as a ratio of benefit to \ncost. Rank-order the architectural strategies according to the VFC value and \nchoose the top ones until the budget or schedule is exhausted.\n",
      "content_length": 2719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 468,
      "content": "23.3  Putting Theory into Practice: The CBAM\n447\n9.\t\nConfirm results with intuition. For the chosen architectural strategies, con-\nsider whether these seem to align with the organization’s business goals. If \nnot, consider issues that may have been overlooked while doing this analy-\nsis. If there are significant issues, perform another iteration of these steps.\nStep 5: Map architectural strategies to \nscenarios and determine quality attribute \nresponse levels\nStep 1: Collate scenarios:\nPrioritize to choose top one-third\nN\nscenarios\nStep 2: Refine scenarios: Determine quality\nattribute response levels for best, worst,\ncurrent, and desired cases of the scenario\nN/3\nscenarios\nStep 3: Prioritize scenarios:\nEliminate half of the scenarios\nN/3\nscenarios\nStep 4: Assign utility for the current and the\ndesired levels for each scenario\nN/6\nscenarios\nStep 6: Determine the expected utility value\nof architectural strategy using interpolation\nStep 7: Calculate total benefit obtained\nfrom an architectural strategy\nStep 8: Choose architectural strategies\nbased on ROI subject to cost constraints\nStep 9: Confirm results with intuition\nFigure 23.3  Process flow diagram for the CBAM\n",
      "content_length": 1182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "448 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nComputing Benefit for Architectural Variation Points\nThis chapter is about calculating the cost and benefit of competing archi-\ntectural options. While there are plenty of metrics and methods to mea-\nsure cost, usually as some function of complexity, benefit is more slippery. \nCBAM uses an assemblage of stakeholders to work out jointly what utility \neach architectural option will bring with it. At the end of the day, CBAM’s \nmeasures of utility are subjective, intuitive, and imprecise. That’s all right; \nstakeholders seldom are able to express benefit any better than that, and \nso CBAM takes what stakeholders know and formulates it into a justified \nchoice. That is, CBAM elicits inputs that are imprecise, because nothing \nbetter is available, and does the best that can be done with them.\nAs a counterpoint to CBAM, there is one area of architecture in which \nthe architectural options are of a specific variety: product-line architectures. \nIn Chapter 25, you’ll be introduced to software architectures that serve \nfor an entire family of systems. The architect introduces variation points \ninto the architecture, which are places where it can be quickly tailored \nin preplanned ways so that it can serve each of a variety of different but \nrelated products. In the product-line context, the major architectural option \nis whether to build a particular variation point in the architecture. Doing so \nisn’t free; otherwise, every product-line architecture would have an infin-\nitude of variation points. So the question becomes: When will adding a \nvariation point pay off?\nCBAM would work for this case as well; you could ask the product line’s \nstakeholders what the utility of a new variation point would be, vis-à-vis other \noptions such as including a different variation point instead, or none at all. \nBut in this case, there is a quantitative formula to measure the benefit. John \nMcGregor of Clemson University has long been interested in the econom-\nics of software product lines, and along with others (including me) invented \nSIMPLE, a cost-modeling language for software product lines. SIMPLE is \ngreat at estimating the cost of product-line options, but not so great at estimat-\ning its benefits. Recently, McGregor took a big step toward remedying that.\nHere is his formula for modeling the marginal value of building in an \nadditional variation point to the architecture:\nvi(t,T) = max(0, –E\nci(τ)e–r(τ–t)\nτ=t\nT\n\n+Þi,T E[ \nk\n \nτ=T\nT*\n\nXi,k(τ)e –r(τ–t))])\nmax(0, \nGot that? No? Right, me neither. But we can understand it if we build it \nup from pieces.\nThe equation says (as all value equations do) that value is benefit minus \ncost, and so those are the two terms.\nThe first term measures the expected cost of building variation point i \nover a time period from now until time T (some far-off horizon of interest \nsuch as fielding your fifth system or getting your next round of venture \n",
      "content_length": 2999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "23.3  Putting Theory into Practice: The CBAM\n449\ncapital funding). The function ci(τ) measures the cost of building variation \npoint i at time τ; it’s evaluated using conventional cost-estimating tech-\nniques for software. The e factor is a standard economic term that accounts \nfor the net present value of money; r is the assumed current interest rate. \nThis is summed up over all time between now and T, and made negative to \nreflect that cost is negative value. So here’s the first term decomposed:\n E\nci(τ)e–r(τ–t)\nτ=t\nT\n\nThe second term evaluates the benefit. The function Xi,k(τ) is the key; it \nmeasures the value of the variation point in the kth product of the product \nline. This is equal to the marginal value of the variation point to the kth \nproduct, minus the cost of using (exercising) the variation point in that kth \nproduct. That first part is the hardest part to come by, but your marketing \ndepartment should be able to help by expressing the marginal value in \nterms of the additional products that the variation point would enable you to \nbuild and how much revenue each would bring in. (That’s what marketing \ndepartments are paid to figure out.)\nvalue of variation point i in product k\nat time \nXi,k(τ)e –r(τ–t)\nVMPi,k(τ) – MCi,k(τ)\nτ=\nmarginal value of the i th variation\npoint in the kth product at time τ\nmarginal cost of tailoring variation\npoint i for use in product k\n. . . adjusted by a factor to account \nfor net present value of money\nTo the function X we apply the same factor as before to account for the \nnet present value of money, sum it up over all time periods between now \nand T, and make it nonnegative:\nSYMBOLS FOR TIME \nτ = time variable \nt = time now \nT = target date   \nT* = modeling limit (t=forever) \ni = index over variation points \nr = assumed interest rate \nCost spent to build variation point i at time τ \nExpected cost summed over \nall relevant time intervals \n. . . adjusted by a factor to account \nfor net present value of money\n",
      "content_length": 1987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "450 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nThen we sum that up over all products k in the product line and multiply \nit by the probability that the variation point will in fact be ready by the time \nit’s needed:\nAdd the two terms together and there you have it. It’s easy to put this \nin a spreadsheet that calculates the result given the relevant inputs, and it \nprovides a quantitative measurement to help guide selection of architectural \noptions—in this case, introduction of variation points to support a product line.\n—PCC\n23.4  Case Study: The NASA ECS Project\nWe will now apply the CBAM to a real-world system as an example of the \nmethod in action.\nThe Earth Observing System is a constellation of NASA satellites that gathers \ndata for the U.S. Global Change Research Program and other scientific communities \nworldwide. The Earth Observing System Data Information System (EOSDIS) Core \nSYMBOLS FOR TIME \nτ = time variable \nt = time now \nT = target date   \nT* = modeling limit (t=forever) \ni = index over variation points \nr = assumed interest rate\nk = index over products \nValue cannot \nbe negative\nsummed over all time\nvalue of variation point i in product k\nat time \nXi,k(τ)e\n)\n–r(τ–t)\nmax (0, \nτ=T\nT*\nVMPi,k(τ) – MCi,k(τ)\nτ=\nmarginal value of the i th variation\npoint in the kth product at time τ\nmarginal cost of tailoring variation\npoint i for use in product k\n. . . adjusted by a factor to account \nfor net present value of money\nprobability that variation point i \nwill be ready for use by time T\nvalue of variation point i in product k over \nall time . . . \n      . . . and over all products\n  Þi,T E[ \nk\n \nτ=T\nT*\n\nXi,k(τ)e –r(τ–t))])\nmax(0, \n",
      "content_length": 1703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "23.4  Case Study: The NASA ECS Project\n451\nSystem (ECS) collects data from various satellite downlink stations for further pro-\ncessing. ECS’s mission is to process the data into higher-form information and make \nit available to scientists in searchable form. The goal is to provide both a common \nway to store (and hence process) data and a public mechanism to introduce new data \nformats and processing algorithms, thus making the information widely available.\nThe ECS processes an input stream of hundreds of gigabytes of raw environ-\nment-related data per day. The computation of 250 standard “products” results in \nthousands of gigabytes of information that is archived at eight data centers in the \nUnited States. The system has important performance and availability require-\nments. The long-term nature of the project also makes modifiability important.\nThe ECS project manager had a limited annual budget to maintain and en-\nhance his current system. From a prior analysis—in this case an ATAM exercise—a \nlarge set of desirable changes to the system was elicited from the system stakehold-\ners, resulting in a large set of architectural strategies. The problem was to choose \na (much) smaller subset for implementation, as only 10 to 20 percent of what was \nbeing proposed could actually be funded. The manager used the CBAM to make a \nrational decision based on the economic criterion of return on investment.\nIn the execution of the CBAM described next, we concentrated on analyzing \nthe Data Access Working Group (DAWG) portion of the ECS.\nStep 1: Collate Scenarios\nA subset of the raw scenarios put forward by the DAWG team were as shown in \nTable 23.1. Note that they are not yet well formed and that some of them do not \nhave defined responses. These issues are resolved in step 2, when the number of \nscenarios is reduced.1\nTable 23.1  Collected Scenarios in Priority Order\nScenario Scenario Description\n  1\nReduce data distribution failures that result in hung distribution requests \nrequiring manual intervention.\n  2\nReduce data distribution failures that result in lost distribution requests.\n  3\nReduce the number of orders that fail on the order submission process.\n  4\nReduce order failures that result in hung orders that require manual intervention.\n  5\nReduce order failures that result in lost orders.\n  6\nThere is no good method of tracking ECSGuest failed/canceled orders without \nmuch manual intervention (e.g., spreadsheets).\n  7\nUsers need more information on why their orders for data failed.\n  8\nBecause of limitations, there is a need to artificially limit the size and number of \norders.\n  9\nSmall orders result in too many notifications to users.\n10\nThe system should process a 50-GB user request in one day, and a 1-TB user \nrequest in one week.\n1.  In the presentation of the DAWG case study, we only show the reduced set of scenarios.\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "452 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nStep 2: Refine Scenarios\nThe scenarios were refined, paying particular attention to precisely specifying \ntheir stimulus-response measures. The worst-case, current-case, desired-case, \nand best-case response goals for each scenario were elicited and recorded, as \nshown in Table 23.2.\nTable 23.2  Response Goals for Refined Scenarios\nScenario\nResponse Goals\nWorst\nCurrent\nDesired\nBest\n  1\n10% hung\n5% hung\n1% hung\n0% hung\n  2\n> 5% lost\n< 1% lost\n0% lost\n0% lost\n  3\n10% fail\n5% fail\n1% fail\n0% fail\n  4\n10% hung\n5% hung\n1% hung\n0% hung\n  5\n10% lost\n< 1% lost\n0% lost\n0% lost\n  6\n50% need help\n25% need help\n0% need help\n0% need help\n  7\n10% get \ninformation\n50% get \ninformation\n100% get \ninformation\n100% get \ninformation\n  8\n50% limited\n30% limited\n0% limited\n0% limited\n  9\n1/granule\n1/granule\n1/100 granules\n1/1,000 granules\n10\n< 50% meet goal\n60% meet goal\n80% meet goal\n> 90% meet goal\nStep 3: Prioritize Scenarios\nIn voting on the refined representation of the scenarios, the close-knit team devi-\nated slightly from the method. Rather than vote individually, they chose to dis-\ncuss each scenario and arrived at a determination of its weight via consensus. The \nvotes allocated to the entire set of scenarios were constrained to 100, as shown in \nTable 23.3. Although the stakeholders were not required to make the votes mul-\ntiples of 5, they felt that this was a reasonable resolution and that more precision \nwas neither needed nor justified.\nStep 4: Assign Utility\nIn this step the utility for each scenario was determined by the stakeholders, again \nby consensus. A utility score of 0 represented no utility; a score of 100 represented \nthe most utility possible. The results of this process are given in Table 23.4.\n",
      "content_length": 1811,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "23.4  Case Study: The NASA ECS Project\n453\nTable 23.3  Refined Scenarios with Votes\nScenario\nResponse Goals\nVotes Worst\nCurrent\nDesired\nBest\n  1\n10\n10% hung\n5% hung\n1% hung\n0% hung\n  2\n15\n> 5% lost\n< 1% lost\n0% lost\n0% lost\n  3\n15\n10% fail\n5% fail\n1% fail\n0% fail\n  4\n10\n10% hung\n5% hung\n1% hung\n0% hung\n  5\n15\n10% lost\n< 1% lost\n0% lost\n0% lost\n  6\n10\n50% need help 25% need help 0% need help\n0% need help\n  7\n5\n10% get \ninformation\n50% get \ninformation\n100% get \ninformation\n100% get \ninformation\n  8\n5\n50% limited\n30% limited\n0% limited\n0% limited\n  9\n10\n1/granule\n1/granule\n1/100 granules\n1/1,000 granules\n10\n5\n< 50% meet \ngoal\n60% meet goal 80% meet goal\n> 90% meet goal\nTable 23.4  Scenarios with Votes and Utility Scores\nScenario\nUtility Scores\nVotes\nWorst\nCurrent\nDesired\nBest\n  1\n10\n10\n80\n  95\n100\n  2\n15\n  0\n70\n100\n100\n  3\n15\n25\n70\n100\n100\n  4\n10\n10\n80\n  95\n100\n  5\n15\n  0\n70\n100\n100\n  6\n10\n  0\n80\n100\n100\n  7\n  5\n10\n70\n100\n100\n  8\n  5\n  0\n20\n100\n100\n  9\n10\n50\n50\n80\n  90\n10\n5\n50\n50\n80\n 90\nStep 5: Develop Architectural Strategies for Scenarios and \nDetermine Their Expected Quality Attribute Response Levels\nBased on the requirements implied by the preceding scenarios, a set of 10 archi-\ntectural strategies was developed by the ECS architects. Recall that an architec-\ntural strategy may affect more than one scenario. To account for these complex \nrelationships, the expected quality attribute response level that each strategy is \npredicted to achieve had to be determined with respect to each relevant scenario.\nThe set of architectural strategies, along with the determination of the scenar-\nios they address, is shown in Table 23.5. For each architectural strategy/scenario \npair, the response levels expected to be achieved with respect to that scenario are \nshown (along with the current response, for comparison purposes).\n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "Table 23.5  Architectural Strategies and Scenarios Addressed\nStrategy Name\nDescription\nScenarios \nAffected\nCurrent  \nResponse\nExpected \nResponse\n  1\nOrder persistence on \nsubmission\nStore an order as soon as it arrives in the system.\n3\n5% fail\n2% Fail\n5\n<1% lost\n0% lost\n6\n25% need help\n0% need help\n  2\nOrder chunking\nAllow operators to partition large orders into multiple small orders.\n8\n30% limited\n15% limited\n  3\nOrder bundling\nCombine multiple small orders into one large order.\n9\n1 per granule\n1 per 100\n10\n60% meet goal\n55% meet goal\n  4\nOrder segmentation\nAllow an operator to skip items that cannot be retrieved due to data \nquality or availability issues.\n4\n5% hung\n2% hung\n  5\nOrder reassignment\nAllow an operator to reassign the media type for items in an order.\n1\n5% hung\n2% hung\n  6\nOrder retry\nAllow an operator to retry an order or items in an order that may \nhave failed due to temporary system or data problems.\n4\n5% hung\n3% hung\n  7\nForced order \ncompletion\nAllow an operator to override an item’s unavailability due to data \nquality constraints.\n1\n5% hung\n3% hung\n  8\nFailed order \nnotification\nEnsure that users are notified only when part of their order has truly \nfailed and provide detailed status of each item; user notification \noccurs only if operator okays notification; the operator may edit \nnotification.\n6\n25% need help\n20% need help\n7\n50% get \ninformation\n90% get \ninformation\n  9\nGranule-level order \ntracking\nAn operator and user can determine the status for each item in their \norder.\n6\n25% need help\n10% need help\n7\n50% get \ninformation\n95% get \ninformation\n10\nLinks to user \ninformation\nAn operator can quickly locate a user’s contact information. \nServer will access SDSRV information to determine any data \nrestrictions that might apply and will route orders/order segments to \nappropriate distribution capabilities, including DDIST, PDS, external \nsubsetters and data processing tools, etc.\n7\n50% get \ninformation\n60% get \ninformation\n",
      "content_length": 1978,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "23.4  Case Study: The NASA ECS Project\n455\nStep 6: Determine the Utility of the “Expected” Quality \nAttribute Response Levels by Interpolation\nOnce the expected response level of every architectural strategy has been char-\nacterized with respect to a set of scenarios, their utility can be calculated by con-\nsulting the utility scores for each scenario’s current and desired responses for all \nof the affected attributes. Using these scores, we may calculate, via interpolation, \nthe utility of the expected quality attribute response levels for the architectural \nstrategy/scenario pair applied to the DAWG of ECS.\nTable 23.6  Architectural Strategies and Their Expected Utility\nStrategy \nName\nScenarios \nAffected\nCurrent \nUtility\nExpected \nUtility\n  1\nOrder persistence on submission\n3\n5\n6\n70\n70\n80\n  90\n100\n100\n  2\nOrder chunking\n8\n20\n  60\n  3\nOrder bundling\n9\n10\n50\n70\n  80\n  65\n  4\nOrder segmentation\n4\n80\n  90\n  5\nOrder reassignment\n1\n80\n  92\n  6\nOrder retry\n4\n80\n  85\n  7\nForced order completion\n1\n80\n  87\n  8\nFailed order notification\n6\n7\n80\n70\n  85\n  90\n  9\nGranule-level order tracking\n6\n7\n80\n70\n  90\n  95\n10\nLinks to user information\n7\n70\n  75\nStep 7: Calculate the Total Benefit Obtained \nfrom an Architectural Strategy\nBased on the information collected, as represented in Table 23.6, the total benefit \nof each architectural strategy can now be calculated, following the equation from \nSection 23.2, repeated here:\nBi = ∑j (bi,j × Wj)\nThis equation calculates total benefit as the sum of the benefit that accrues to \neach scenario, normalized by the scenario’s relative weight. Using this formula, \nthe total benefit scores for each architectural strategy are now calculated, and the \nresults are given in Table 23.7.\n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "456 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\nTable 23.7  Total Benefit of Architectural Strategies\nStrategy \nScenario \nAffected\nScenario \nWeight\nRaw \nArchitectural \nStrategy \nBenefit\nNormalized \nArchitectural \nStrategy \nBenefit\nTotal Architectural \nStrategy Benefit\n  1\n  3\n15\n20\n300\n  1\n  5 \n15\n30\n450\n  1\n  6 \n10\n20\n200\n950\n  2\n 8\n 5\n40\n200\n200\n  3\n  9 \n10\n30\n300\n  3\n10\n  5\n–5\n–25\n275\n  4\n  4 \n10\n10\n100\n100\n  5\n  1 \n10\n12\n120\n120\n  6\n  4 \n10\n  5\n  50\n  50\n  7\n  1\n10\n  7\n  70\n  70\n  8\n  6\n10\n  5\n  50\n  8\n  7\n  5\n20\n100\n150\n  9\n  6\n10\n10\n100\n  9\n  7\n  5\n25\n125\n225\n10\n 7\n 5\n 5\n 25\n 25\nStep 8: Choose Architectural Strategies Based \non VFC Subject to Cost Constraints\nTo complete the analysis, the team estimated cost for each architectural strategy. \nThe estimates were based on experience with the system, and a return on invest-\nment for each architectural strategy was calculated. Using the VFC, we were able \nto rank each strategy. This is shown in Table 23.8. Not surprisingly, the ranks \nroughly follow the ordering in which the strategies were proposed: strategy 1 has \nthe highest rank; strategy 3 the second highest. Strategy 9 has the lowest rank; \nstrategy 8, the second lowest. This simply validates stakeholders’ intuition about \nwhich architectural strategies were going to be of the greatest benefit. For the \nECS these were the ones proposed first.\nResults of the CBAM Exercise\nThe most obvious results of the CBAM are shown in Table 23.8: an ordering of \narchitectural strategies based on their predicted VFC. However, just as for the \nATAM method, the benefits of the CBAM extend beyond the qualitative out-\ncomes. There are social and cultural benefits as well.\n",
      "content_length": 1721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "23.5  Summary\n457\nTable 23.8  VFC of Architectural Strategies\nStrategy\nCost\nTotal Strategy Benefit\nStrategy VFC\nStrategy Rank\n  1\n1200\n950\n0.79\n  1\n  2\n  400\n200\n0.5\n  3\n  3\n  400\n275\n0.69\n  2\n  4\n  200\n100\n0.5\n  3\n  5\n  400\n120\n0.3\n  7\n  6\n  200\n  50\n0.25\n  8\n  7\n  200\n  70\n0.35\n  6\n  8\n  300\n150\n0.5\n  3\n  9\n1000\n225\n0.22\n10\n10\n  100\n  25\n0.25\n  8\nJust as important as the ranking of architectural strategies in CBAM is the \ndiscussion that accompanies the information-collecting and decision-making pro-\ncesses. The CBAM process provides a great deal of structure to what is always \nlargely unstructured discussions, where requirements and architectural strategies \nare freely mixed and where stimuli and response goals are not clearly articulated. \nThe CBAM process forces the stakeholders to make their scenarios clear in ad-\nvance, to assign utility levels of specific response goals, and to prioritize these \nscenarios based on the resulting determination of utility. Finally, this process re-\nsults in clarification of both scenarios and requirements, which by itself is a sig-\nnificant benefit.\n23.5  Summary\nArchitecture-based economic analysis is grounded on understanding the utili-\nty-response curve of various scenarios and casting them into a form that makes \nthem comparable. Once they are in this common form—based on the common \ncoin of utility—the VFC for each architecture improvement, with respect to each \nrelevant scenario, can be calculated and compared.\nApplying the theory in practice has a number of practical difficulties, but in \nspite of those difficulties, we believe that the application of economic techniques \nis inherently better than the ad hoc decision-making approaches that projects \n(even quite sophisticated ones) employ today. Our experience with the CBAM \ntells us that giving people the appropriate tools to frame and structure their dis-\ncussions and decision making is an enormous benefit to the disciplined develop-\nment of a complex software system.\n",
      "content_length": 1999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "458 \nPart Four  Architecture and Business\t\n23—Economic Analysis of Architectures\n23.6  For Further Reading\nThe origins of the CBAM can be found in two papers: [Moore 03] and Kazman \n[01].\nA more general background in economic approaches to software engineer-\ning may be found in the now-classic book by Barry Boehm [Boehm 81].\nAnd a more recent, and somewhat broader, perspective on the field can be \nfound in [Biffl 10].\nThe product-line analysis we used in the sidebar on the value of variation \npoints came from a paper in the 2011 International Software Product Line Con-\nference by John McGregor and his colleagues [McGregor 11].\n23.7  Discussion Questions\n1.\t\nThis chapter is about choosing an architectural strategy using rational, eco-\nnomic criteria. See how many other ways you can think of to make a choice \nlike this. Hint: Your candidates need not be “rational.”\n2.\t\nHave two or more different people generate the utility curve for a quality \nattribute scenario for an ATM. What are the difficulties in generating the \ncurve? What are the differences between the two curves? How would you \nreconcile the differences?\n3.\t\nDiscuss the advantages and disadvantages of the method for generating \nscenario priorities used in the CBAM. Can you think of a different way to \nprioritize the scenarios? What are the pluses and minuses of your method?\n4.\t\nUsing the results of your design exercise for the ATM from Chapter 17 as a \nstarting point, develop an architectural strategy for achieving a quality attri-\nbute scenario that your design does not cover.\n5.\t\nGenerate the utility curves for two different systems in the same domain. \nWhat are the differences? Do you believe that there are standard curves de-\npending on the domain? Defend your answer.\n",
      "content_length": 1760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "459\n24\nArchitecture \nCompetence\nThe ideal architect should be a man of letters, a \nskillful draftsman, a mathematician, familiar with \nhistorical studies, a diligent student of philosophy, \nacquainted with music, not ignorant of medicine, \nlearned in the responses of jurisconsults, familiar \nwith astronomy and astronomical calculations.\n—Vitruvius, De Architectura (25 B.C.)\nThe lyf so short, the craft so long to lerne.\n—Geoffrey Chaucer\nIf software architecture is worth “doing,” surely it’s worth doing well. Most of \nthe literature about architecture concentrates on the technical aspects. This is \nnot surprising; it is a deeply technical discipline. There is little information that \nspeaks to the fact that architectures are created by architects working in organiza-\ntions, full of actual human beings. Dealing with these humans is decidedly non-\ntechnical. What can be done to help architects, especially architects-in-training, \nbe better at this important dimension of their job? And what can be done to help \narchitecture organizations do a better job in fostering their architects to produce \ntheir best work?\nAn organization’s ability to routinely produce high-quality architectures that \nare aligned with its business goals well cannot be understood simply through ex-\namination of past architectures and measurement of their deficiencies. The or-\nganizational and human causes of those deficiencies also need to be understood. \nThis chapter is about the competence of individual architects and the orga-\nnizations that wish to produce high-quality architects. We define the architecture \ncompetence of an organization as follows:\nThe architecture competence of an organization is the ability of \nthat organization to grow, use, and sustain the skills and knowledge \n",
      "content_length": 1784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "460 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nnecessary to effectively carry out architecture-centric practices at the \nindividual, team, and organizational levels to produce architectures \nwith acceptable cost that lead to systems aligned with the organization’s \nbusiness goals.\nBecause the architecture competence of an organization depends, in part, on \nthe competence of architects, we begin by asking what it is that architects are ex-\npected to do, know, and be skilled at. Then we’ll look at what organizations can and \nshould do to help their architects produce better architectures. Individual and orga-\nnizational competencies are intertwined. Studying only one or the other won’t do. \n24.1  \u0007Competence of Individuals: Duties, \nSkills, and Knowledge of Architects\nArchitects perform many activities beyond directly producing an architecture. \nThese activities, which we call duties, form the backbone of an individual’s ar-\nchitecture competence. We surveyed a broad body of information aimed at archi-\ntects (such as websites, courses, books, and position descriptions for architects). \nWe also surveyed practicing architects. These surveys tell us that duties are but \none aspect of individual competence. Writers about architects also speak of skills \nand knowledge. For example, the ability to communicate ideas clearly and to ne-\ngotiate effectively are skills often ascribed to competent architects. In addition, \narchitects need to have up-to-date knowledge about patterns, database platforms, \nweb services standards, quality attributes, and a host of other topics. \nDuties, skills, and knowledge1 form a triad upon which architecture compe-\ntence for individuals rests. The relationship among these three is shown in Figure \n24.1—namely, skills and knowledge support the ability to perform the required du-\nties. Infinitely talented architects are of no use if they cannot (for whatever reason) \nperform the duties required of the position; we would not say they were competent. \nTo give examples of these concepts:\n■\n■“Design the architecture” is a duty. \n■\n■“Ability to think abstractly” is a skill. \n■\n■“Patterns and tactics” is a part of the body of knowledge. \nThis example purposely illustrates that skills and knowledge are important \n(only) for supporting the ability to carry out duties effectively. As another exam-\nple, “documenting the architecture” is a duty, “ability to write clearly” is a skill, \nand “ISO Standard 42010” is part of the related body of knowledge. Of course, a \nskill or knowledge area can support more than one duty. \n1.   Some writers speak of the importance of experience. We count experience as a form of \nknowledge. \n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n461\nDuties\nKnowledge\nSkills\nSupport\nFigure 24.1  Skills and knowledge support the execution of duties.\nKnowing the duties, skills, and knowledge of architects (or, more precisely, \nthe duties, skills, and knowledge that are needed of architects in a particular or-\nganizational setting) can help establish measurement and improvement strategies \nfor individual architects. If you want to improve your individual architectural \ncompetence, you should do the following:\n1.\t\nGain experience carrying out the duties. Apprenticeship is a productive \npath to achieving experience. Education alone is not enough, because edu-\ncation without on-the-job application merely enhances knowledge.\n2.\t\nImprove your nontechnical skills. This dimension of improvement involves \ntaking professional development courses, for example, in leadership or time \nmanagement. Some people will never become truly great leaders or com-\nmunicators, but we can all improve on these skills.\n3.\t\nMaster the body of knowledge. One of the most important things a compe-\ntent architect must do is master the body of knowledge and remain up to \ndate on it. To emphasize the importance of remaining up to date, consider \nthe advances in knowledge required for architects that have emerged in \njust the last few years. For example, the cloud and edge computing that we \ndiscuss in Chapters 26 and 27 were not important topics several years ago. \nTaking courses, becoming certified, reading books and journals, visiting \nwebsites and portals, reading blogs, attending architecture-oriented confer-\nences, joining professional societies, and meeting with other architects are \nall useful ways to improve knowledge. \nDuties\nThis section summarizes a wide variety of an architect’s duties. Not every architect \nin every organization will perform every one of these duties on every project. But \na competent architect should not be surprised to find himself or herself engaged in \n",
      "content_length": 2016,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "462 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nany of the activities listed here. We divide the duties into technical duties (Table \n24.1) and nontechnical duties (Table 24.2). One immediate observation you should \nmake is how many nontechnical duties there are. An obvious implication, for those \nof you who wish to be architects, is that you must pay adequate attention to the \nnontechnical aspects of your education and your professional activities.\nTable 24.1  The Technical Duties of a Software Architect\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nArchitecting\nCreating an \narchitecture\nDesign or select an architecture. Create software \narchitecture design plan. Build product line or product \narchitecture. Make design decisions. Expand detail \nand refine design to converge on final design. Identify \nthe patterns and tactics and articulate the principles \nand key mechanisms of the architecture. Partition the \nsystem. Define how the components fit together and \ninteract. \nEvaluating and \nanalyzing an \narchitecture\nEvaluate an architecture (for your current system \nor for other systems) to determine satisfaction of \nuse cases and quality attribute scenarios. Create \nprototypes. Participate in design reviews. Review \nconstruction-level designs. Review the designs \nof the components designed by junior engineers. \nReview designs for compliance with the architecture. \nCompare software architecture evaluation \ntechniques. Apply value-based architecting \ntechniques to evaluate architectural decisions. Model \nalternatives. Perform tradeoff analysis.\nDocumenting an \narchitecture\nPrepare architectural documents and presentations \nuseful to stakeholders. Document software \ninterfaces. Produce documentation standards. \nDocument variability and dynamic behavior. \nWorking with \nand transforming \nexisting \nsystem(s)\nMaintain and evolve existing system and its \narchitecture. Redesign existing architecture(s) for \nmigration to new technology and platforms. \nPerforming other \narchitecting \nduties \nSell the vision, keep the vision alive. Participate in \nproduct design meetings. Give technical advice \non architecture, design, and development. Provide \narchitectural guidelines for software design activities. \nLead architecture improvement activities. Participate \nin software process definition and improvement. \nDefine philosophy and principles for global \narchitecture. Oversee or manage the architecture \ndefinition process. Provide architecture oversight of \nsoftware development projects.\n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n463\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nDuties \nconcerned \nwith life-cycle \nactivities \nother than \narchitecting\nManaging the \nrequirements\nAnalyze functional and quality attribute software \nrequirements. Understand business and customer \nneeds and ensure that the requirements meet \nthese needs. Capture customer, organizational, and \nbusiness requirements on the architecture. Create \nsoftware specifications from business requirements. \nArticulate and refine architectural requirements. \nListen to and understand the scope of the project. \nUnderstand the client’s key design needs and \nexpectations. \nImplementing \nthe product\nProduce code. Conduct code reviews. Develop \nreusable software components. Analyze, select, \nand integrate software components. Set and ensure \nadherence to coding guidelines. Recommend \ndevelopment methodologies and coding standards. \nMonitor, mentor, and review the work of outside \nconsultants and vendors.\nTesting the \nproduct\nEstablish architecture-based testing procedures. \nSupport system testers. Support field testing. Support \nbug fixing and maintenance.\nEvaluating future \ntechnologies\nEvaluate and recommend enterprise’s software \nsolutions. Manage the introduction of new software \nsolutions. Analyze current IT environment and \nrecommend solutions for deficiencies. Work with \nvendors to represent organization’s requirements \nand influence future products. Develop and present \ntechnical white papers. \nSelecting tools \nand technology \nPerform technical feasibility studies of new \ntechnologies and architectures. Evaluate commercial \ntools and software components from an architectural \nperspective. Develop internal technical standards and \ncontribute to the development of external technical \nstandards. \nSkills\nGiven the wide range of duties enumerated in the previous section, what skills \n(beyond mastery of the technical body of knowledge) does an architect need to \npossess? Much has been written about the architect’s special role of leadership in \na project; the ideal architect is an effective communicator, manager, team builder, \nvisionary, and mentor. Some certificate or certification programs emphasize non-\ntechnical skills. Common to these certification programs are nontechnical assess-\nment areas of leadership, organization dynamics, and communication. \nTable 24.3 enumerates the set of nontechnical skills most useful to an \narchitect. \n",
      "content_length": 2501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "464 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nTable 24.2  The Nontechnical Duties of a Software Architect\nGeneral Duty \nArea\nSpecific Duty \nArea\nExample  \nDuties\nManagement\nManaging the \nproject\nHelp with budgeting and planning. Follow budgetary \nconstraints. Manage resources. Perform sizing and \nestimation. Perform migration planning and risk \nassessment. Take care of or oversee configuration \ncontrol. Create development schedules. Measure \nresults using quantitative metrics and improve both \npersonal results and teams’ productivity. Identify and \nschedule architectural releases. \nManaging the \npeople\nBuild “trusted advisor” relationships. Coordinate. \nMotivate. Advocate. Delegate. Act as a supervisor.\nSupporting the \nmanagement\nProvide feedback on appropriateness and difficulty of \nproject. Advise the project manager on the tradeoffs \nbetween software design choices and requirements \nchoices. Provide input to software project manager in \nthe software project planning and estimation process. \nServe as a “bridge” between the technical team and \nthe project manager. \nOrganization and \nbusiness-related \nduties\nSupporting the \norganization \nGrow an architecture evaluation capability in the \norganization. Review and contribute to research and \ndevelopment efforts. Participate in the hiring process \nfor the team. Help with product marketing. Institute \nand oversee cost-effective software architecture \ndesign reviews. Help develop intellectual property. \nSupporting the \nbusiness\nTranslate business strategy into technical vision and \nstrategy. Influence the business strategy. Understand \nand evaluate business processes. Understand \nand communicate the business value of software \narchitecture. Help the organization meet its business \ngoals. Understand customer and market trends. \nIdentify, understand, and resolve business issues. \nAlign architecture with the business goals and \nobjectives.\nLeadership and \nteam building\nProviding \ntechnical \nleadership\nMentor other architects. Produce technology trend \nanalysis or roadmaps. \nBuilding a team Set team context (vision). Build the architecture \nteam and align them with the vision. Mentor junior \narchitects. Educate the team on the use of the \narchitecture. Maintain morale, both within and outside \nthe architecture group. Foster the professional \ndevelopment of team members. Coach teams of \nsoftware design engineers for planning, tracking, and \ncompletion of work within the agreed plan. Mentor \nand coach staff in the use of software technologies. \nWork both as a leader and as an individual \ncontributor.\n",
      "content_length": 2610,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "24.1  Competence of Individuals: Duties, Skills, and Knowledge of Architects\n465\nTable 24.3  The Nontechnical Skills of a Software Architect\nGeneral Skill \nArea\nSpecific Skill \nArea\nExample  \nSkills\nCommunication \nskills\nOutward\nAbility to make oral and written communications \nand presentations. Ability to present and explain \ntechnical information to diverse audiences. Ability \nto transfer knowledge. Ability to persuade. Ability to \nsee from and sell to multiple viewpoints.\nInward\nAbility to listen, interview, consult, and negotiate. \nAbility to understand and express complex topics.\nInterpersonal \nskills\nWithin team\nAbility to be a team player. Ability to work \neffectively with superiors, colleagues, and \ncustomers. Ability to maintain constructive \nworking relationships. Ability to work in a diverse \nteam environment. Ability to inspire creative \ncollaboration. Ability to build consensus. \nWith other \npeople\nAbility to demonstrate interpersonal skills. Ability to \nbe diplomatic and respect others. Ability to mentor \nothers. Ability to handle and resolve conflict. \nWork skills\nLeadership\nAbility to make decisions. Ability to take initiative \nand be innovative. Ability to demonstrate \nindependent judgment, be influential, and \ncommand respect. \nWorkload \nmanagement\nAbility to work well under pressure, plan, manage \ntime, estimate. Ability to support a wide range \nof issues and work on multiple complex projects \nconcurrently. Ability to effectively prioritize and \nexecute tasks in a high-pressure environment. \nSkills to excel \nin corporate \nenvironment\nAbility to think strategically. Ability to work under \ngeneral supervision and under given constraints. \nAbility to organize workflow. Ability to sense where \nthe power is and how it flows in an organization. \nAbility to do what it takes to get the job done. Ability \nto be entrepreneurial, be assertive without being \naggressive, and receive constructive criticism.\nSkills for \nhandling \ninformation\nAbility to be detail oriented while maintaining \noverall vision and focus. Ability to see the big \npicture. Ability to deal with abstraction. \nSkills for \nhandling the \nunexpected\nAbility to tolerate ambiguity. Ability to take and \nmanage risks. Ability to solve problems. \nAbility to be adaptable, flexible, open minded, and \nresilient. Ability to do the juggling necessary to \ndeploy successful software projects.\n",
      "content_length": 2396,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "466 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nKnowledge\nA competent architect has an intimate familiarity with the architectural body of \nknowledge. The software architect should \n■\n■Be comfortable with all branches of software engineering from require-\nments definition to implementation, development, verification and valida-\ntion, and deployment\n■\n■Be familiar with supporting disciplines such as configuration management \nand project management \n■\n■Understand current design and implementation tools and technologies \nKnowledge and experience in one or more application domains is also \nnecessary.\nTable 24.4 is the set of knowledge areas for an architect.\nTable 24.4  The Knowledge Areas of a Software Architect\nGeneral \nKnowledge  \nArea\nSpecific  \nKnowledge  \nArea\nSpecific  \nKnowledge  \nExamples\nComputer \nscience \nknowledge\nKnowledge of \narchitecture \nconcepts\nKnowledge of architecture frameworks, \narchitectural patterns, tactics, viewpoints, \nstandard architectures, relationship \nto system and enterprise architecture, \narchitecture description languages, emerging \ntechnologies, architecture evaluation models \nand methods, and quality attributes.\nKnowledge \nof software \nengineering \nKnowledge of systems engineering. \nKnowledge of software development life \ncycle, software process management, \nand improvement techniques. Knowledge \nof requirements analysis, mathematics, \ndevelopment methods and modeling \ntechniques, elicitation techniques. Knowledge \nof component-based software development, \nreuse methods and techniques, software \nproduct-line techniques, documentation, \ntesting and debugging tools. \nDesign knowledge Knowledge of different tools and design \ntechniques. Knowledge of how to design \ncomplex multi-product systems. Knowledge \nof object-oriented analysis and design, UML \ndiagrams, and UML analysis modeling.\nProgramming \nknowledge\nKnowledge of programming languages and \nprogramming language models. Knowledge \nof specialized programming techniques for \nsecurity, real time, etc. \n",
      "content_length": 2042,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "24.2  Competence of a Software Architecture Organization\n467\nGeneral \nKnowledge  \nArea\nSpecific  \nKnowledge  \nArea\nSpecific  \nKnowledge  \nExamples\nKnowledge of \ntechnologies \nand platforms\nKnowledge \nof specific \ntechnologies and \nplatforms\nKnowledge of hardware/software interfaces, \nweb-based applications, and Internet \ntechnologies. Knowledge of specific software/\noperating systems, such as RDBMS concepts, \ncloud platforms, and SOA implementations.\nGeneral \nknowledge of \ntechnologies and \nplatforms\nKnowledge of IT industry future directions and \nthe ways in which infrastructure impacts an \napplication. \nKnowledge \nabout the \norganization’s \ncontext and \nmanagement\nDomain \nknowledge\nKnowledge of the most relevant domain(s) \nand domain-specific technologies. \nIndustry \nknowledge\nKnowledge of the industry’s best practices and \nindustry standards. Knowledge of how to work \nin onshore/offshore team environment. \nEnterprise \nknowledge\nKnowledge of the company’s business \npractices, and your competition’s products, \nstrategies, and processes. Knowledge of \nbusiness and technical strategy, and business \nreengineering principles and processes. \nKnowledge of strategic planning, financial \nmodels, and budgeting.\nLeadership and \nmanagement \ntechniques \nKnowledge of coaching, mentoring, and \ntraining software developers. Knowledge of \nproject management. Knowledge of project \nengineering.\n24.2  \u0007Competence of a Software \nArchitecture Organization\nOrganizations by their practices and structure can help or hinder architects in per-\nforming their duties. For example, if an organization has a career path for archi-\ntects, that will motivate employees to become architects. If an organization has a \nstanding architecture review board, then the project architect will know how and \nwith whom to schedule a review. Lack of these items will mean that an architect \nhas to fight battles with the organization or determine how to carry out a review \nwithout internal guidance. It makes sense, therefore, to ask whether a particular \norganization is architecturally competent and to develop instruments whose goal \nis measuring the architectural competence of an organization. The architectural \ncompetence of organizations is the topic of this section.\n",
      "content_length": 2261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "468 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nActivities Carried Out by a Competent Organization\nOrganizations have duties, skills, and knowledge for architecture as well. For ex-\nample, adequately funding the architecture effort is an organizational duty, as is \neffectively using the available architecture workforce (by appropriate teaming, \nand so on). These are organizational duties because they are outside the control \nof individual architects. An organization-level skill might be effective knowl-\nedge management or human resource management as applied to architects. An \nexample of organizational knowledge is the composition of an architecture-based \nlife-cycle model that software projects may employ. \nHere are some things—duties—that an organization can perform to help im-\nprove the success of its architecture efforts:\n■\n■Hire talented architects.\n■\n■Establish a career track for architects.\n■\n■Make the position of architect highly regarded through visibility, reward, \nand prestige.\n■\n■Establish a clear statement of responsibilities and authority for architects. \n■\n■Establish a mentoring program for architects.\n■\n■Establish an architecture training and education program. \n■\n■Establish an architect certification program.\n■\n■Have architects receive external architect certifications.\n■\n■Measure architects’ performance.\n■\n■Establish a forum for architects to communicate and share information and \nexperience.\n■\n■Establish a repository of reusable architectures and architecture-based \nartifacts.\n■\n■Develop reusable reference architectures.\n■\n■Establish organization-wide architecture practices.\n■\n■Establish an architecture review board.\n■\n■Measure the quality of architectures produced.\n■\n■Provide a centralized resource to analyze and help with architecture tools.\n■\n■Hold an organization-wide architecture conference.\n■\n■Have architects join professional organizations.\n■\n■Bring in outside expert consultants on architecture.\n■\n■Include architecture milestones in project plans.\n■\n■Have architects provide input into product definition.\n■\n■Have architects advise on the development team structure.\n■\n■Give architects influence throughout the entire project life cycle. \n■\n■Reward or penalize architects based on project success or failure.\n",
      "content_length": 2291,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "24.2  Competence of a Software Architecture Organization\n469\nAssessment Goals\nThe activities enumerated above can be assessed. What are the potential goals \nfrom an assessment of an organization’s architecture competence? There are at \nleast four sets of reasons for assessing organizational architectural competence: \n1.\t\nThere are goals relevant to any business that wishes to improve their archi-\ntectural competence. Businesses regularly assess their own performance in \na variety of means—technical, fiscal, operational (for example, consider the \nwidespread use of multi-criteria techniques such as the Balanced Scorecard \nor Business/IT Alignment in industry)—for a variety of reasons. These \ninclude determining whether they are meeting industry norms and gauging \ntheir progress over time in meeting organizational goals.\n2.\t\nThere are goals relevant to an acquisition organization. For example, an \norganization can use an assessment of architecture competence to assess \na contractor in much the same way that contractors are scrutinized with \nrespect to their CMMI level. Or an organization might use an assessment \nof architecture competence to aid in deciding among competing bids from \ncontractors. All other things being equal, an acquiring organization would \nprefer a contractor with a higher level of architectural competence because \nthis typically means fewer downstream problems and rework. An acquisi-\ntion organization might assess the contractors directly, or hire a third party \nto do the assessment.\n3.\t\nThere are goals relevant to service organizations: such organizations might \nbe motivated to maintain, measure, and advertise their architectural compe-\ntence as a means of attracting and retaining customers. In such a case they \nwould typically rely on outside organizations to assess their level of compe-\ntence in an objective fashion.\n4.\t\nFinally, there are goals that are relevant to product builders: these organi-\nzations would be motivated to assess, monitor (and, over time, increase) \ntheir level of architectural competence as it would (1) aid in advertising the \nquality of their products and (2) aid in their internal productivity and pre-\ndictability. In fact, in these ways their motivations are aligned with those of \nservice organizations. \nAssessing an Organization’s Competence\nIn addition to duties, skills, and knowledge, there are other models of individual \nand organizational competence that are helpful in building an instrument for as-\nsessing an organization’s competence. They are the following:\n■\n■The Human Performance Technology model, which measures the value of \nan individual or department’s output and the cost of producing that output. \n",
      "content_length": 2704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "470 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nIt holds that competent people produce the most value for every organiza-\ntional dollar spent. \n■\n■The Organizational Coordination model, which measures how teams in \nmultiple sites developing a single product or related set of products cooper-\nate to produce a functioning product. An organization that is architecturally \ncompetent will have more effective and efficient coordination mechanisms \nthan an organization that is not architecturally competent. \n■\n■The Organizational Learning model, which measures how well an organi-\nzation’s learning processes transform experience into knowledge, moderat-\ned by context. \nWe have created a framework for organizational architecture competence \nthat forms the foundation for a competence assessment procedure. A small team \nof trained assessors can use the framework to conduct interviews (with architects, \ndevelopers, and technical and organizational managers), examine current prac-\ntices, read documents and evidentiary artifacts (such as organizational standards), \nand investigate architecture-based successes and failures in the recent past. They \ncan use their findings to identify systemic trouble spots and recommend improve-\nment strategies.\nTable 24.5 shows the framework. For convenience, it is divided into prac-\ntice areas that relate to software and system engineering, technical management \n(which is by and large the management of single projects or small numbers of re-\nlated projects), and organizational management (which is management at a scope \nmore broad than that of projects). \nThe framework is populated by questions inspired by the four models of \ncompetence that we previously described. Each question has a set of answers \nthat we might expect to see in a competent organization. For example, a ques-\ntion associated with the practice area “Hire talented architects” deals with how a \ncandidate architect’s experience and capabilities are assessed. Expected answers \nmight include having the architect take a test, requiring that candidates possess \nan architecture certification, or examining previous architectures designed by the \ncandidate. (Our expected answers grow as we visit more and more organizations. \nIt’s a pleasant surprise when we find an organization carrying out a practice area \nin a clever way that we hadn’t thought of.)\nQuestions Based on the Duties, Skills, and Knowledge Model.  Our \nassessment framework contains dozens of questions related to duties, skills, and \nknowledge. The questions are posed in terms of the organization: The questions \nask how the organization ensures that the architectural duties are carried out in \na competent manner, and how the organization measures and nurtures its archi-\ntects’ skills and knowledge. Here is a small set of example questions based on \nthe Duties, Skills, and Knowledge model (chosen from among the dozens that \npopulate our assessment framework) that we use in an architecture competence \nassessment exercise with an organization.\n",
      "content_length": 3057,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "24.2  Competence of a Software Architecture Organization\n471\nTable 24.5  Framework for Organizational Architecture Competence\nSoftware \nEngineering  \nPractice Areas\nQuality Attribute Elicitation Practices\nTools and Technology Selection\nModeling and Prototyping Practices\nArchitecture Design Practices\nArchitecture Description Practices\nArchitecture Evaluation Practices\nSystem Implementation Practices\n■\n■\nSoftware design practices (design conforms to \narchitecture)\n■\n■\nSoftware coding practices (code conforms to design and \narchitecture)\nSoftware Verification Practices\n■\n■\nProving properties of the software\n■\n■\nSoftware testing\nArchitecture Reconstruction Practices\nTechnical \nManagement \nPractice Areas\nBusiness or Mission Goals Practices\n■\n■\nSetting goals\n■\n■\nMeasuring achievement of organization’s goals\n■\n■\nPerformance-based compensation\nProduct or System Definition Practices\n■\n■\nSetting functional requirements\nAllocating Resources\n■\n■\nSetting architect’s workload and schedule\n■\n■\nFunding stakeholder involvement\nProject Management Practices \n■\n■\nProject plan structure aligned with architecture structure\n■\n■\nAdequate time planned for architecture evaluation\nProcess Discipline Practices\n■\n■\nEstablish organization-wide architecture practices\n■\n■\nProcess monitoring and improvement practices\n■\n■\nReuse practices\nCollaboration with Manager Practices\n■\n■\nArchitects advise managers\n■\n■\nArchitects support managers\ncontinues\n",
      "content_length": 1436,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "472 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nTable 24.5  Framework for Organizational Architecture Competence, continued\nOrganizational \nManagement \nPractice Areas\nHire Talented Architects\nEstablish a Career Track for Architects\n■\n■\nLeadership roles for architects\n■\n■\nSuccession planning\nProfessional Development Practices\n■\n■\nOngoing training\n■\n■\nCreating and sustaining an internal community of \narchitects\n■\n■\nSupporting participation in external communities\nOrganizational Planning Practices \nTechnology Planning and Forecasting Practices\nDuty: Creating an Architecture\nQuestion: How do you create an architecture?\n■\n■How do you ensure that the architecture is aligned with the business goals?\n■\n■What is the input into the architecture creation process? What inputs are \nprovided to the architect? \n■\n■How does the architect validate the information provided? What does the \narchitect do in case the input is insufficient or inadequate?\nDuty: Architecture Evaluation and Analysis\nQuestion: How do you evaluate and analyze an architecture?\n■\n■Are evaluations part of the normal software development life cycle or are \nthey done when problems are encountered? \n■\n■Is the evaluation incremental or “big bang”? How is the timing determined?\n■\n■Does the evaluation include an explicit activity relating architecture to busi-\nness goals?\n■\n■What are the inputs to the evaluation? How are they validated?\n■\n■What are the outputs from an evaluation? How are the outputs of the evalu-\nation utilized? Are the outputs differentiated according to impact or impor-\ntance? How are the outputs validated? Who is communicated what outputs?\nKnowledge: Architecture Concepts\nQuestion: How does your organization ensure that its architects have adequate \narchitectural knowledge?\n■\n■How are architects trained in general knowledge of architecture?\n■\n■How do architects learn about architectural frameworks, patterns, tactics, \nstandards, documentation notations, and architecture description languages?\n",
      "content_length": 2016,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "24.2  Competence of a Software Architecture Organization\n473\n■\n■How do architects learn about new or emerging architectural technologies \n(e.g., multi-core processors)? \n■\n■How do architects learn about analysis and evaluation techniques and \nmethods?\n■\n■How do architects learn quality attribute-specific knowledge, such as tech-\nniques for analyzing and managing availability, performance, modifiability, \nand security?\n■\n■How are architects tested to ensure that their level of knowledge is ade-\nquate, and remains adequate, for the tasks that they face?\nQuestions Based on the Organizational Coordination Model.  Ques-\ntions based on the Organizational Coordination model focus on how the organi-\nzation establishes its teams and what support it provides for those teams to coor-\ndinate effectively. Here are a couple of example questions:\nQuestion: How is the architecture designed with distribution of work to \nteams in mind?\n■\n■How available or broadly shared is the architecture to various teams?\n■\n■How do you manage the evolution of architecture during development?\n■\n■Is the work assigned to the teams before or after the architecture is defined, \nand with due consideration of the architectural structure?\nQuestion: Are the aspects of the architecture that will require a lot of inter-\nteam coordination supported by the organization’s coordination/communication \ninfrastructure?\n■\n■Do you co-locate teams with high coordination? Or at least put them in the \nsame time zone? \n■\n■Must all coordination among teams go through the architecture team?\nQuestions Based on the Human Performance Technology Model.  \nThe Human Performance Technology questions deal with the value and cost of \nthe organization’s architectural activities. Here are examples of questions based \non the Human Performance Technology model:\nQuestion: Do you track how much the architecture effort costs, and how it \nimpacts overall project cost and schedule?\n■\n■How do you track the end of architecture activities?\n■\n■How do you track the impact of architecture activities?\nQuestion: Do you track the value or benefits of the architecture? \n■\n■How do you measure stakeholder satisfaction?\n■\n■How do you measure quality? \n",
      "content_length": 2202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "474 \nPart Four  Architecture and Business\t\n24—Architecture Competence\nQuestions Based on the Organizational Learning Model.  Finally, a \nset of example questions, based on the Organizational Learning model, which \ndeal with how the organization systematically internalizes knowledge to its \nadvantage:\nQuestion: How do you capture and share experiences, lessons learned, techno-\nlogical decisions, techniques and methods, and knowledge about available tooling?\n■\n■Do you use any knowledge management tools?\n■\n■Is capture and use of architectural knowledge embedded in your processes?\n■\n■Where is the information about “who knows what” captured and how is this \ninformation maintained?\n■\n■How complete and up to date is your architecture documentation? How \nwidely disseminated is it?\nPerforming an Assessment\nOur organizational competence assessment is carried out using a team of three \nto four assessors. The exercise is set up by establishing the scope of the review: \nAre we assessing the entire company? One of its divisions? Or perhaps a single \nimportant project? \nAfter we establish the scope, we identify the groups we wish to interview. Of \ncourse, we’ll want to interview the architecture team(s) within the scope. From there \nwe identify groups both upstream and downstream of the architects. Upstream are \ngroups that manage the architects or provide organization-wide architecture training. \nDownstream, we interview the “consumers” of the architectures, such as developers, \nintegrators, testers, and maintainers. We interview small groups, making sure that no \nmembers of an interview group have reporting relationships with each other. \nWe try hard to establish an informal atmosphere in the group interviews, to \navoid inhibiting the participants. The tone is conversational, not inquisitional. We \nbegin each interview by reminding the participants of the purpose of the exercise, \nand to assure them that nothing they say will be quoted to anyone outside the \ngroup in any way that could identify them.\nFor each group, we have planned which parts of the framework we wish to \ndiscuss with that group. We won’t ask testers, for example, questions intended for \nmanagers, and vice versa. We use the questions as a guide for the conversation, but \nnot a rigid script. Whenever we pose a question in the assessment instrument, there \nare a number of meta-questions that automatically accompany it. For example:\n■\n■What evidence could you show us to support your answer? Supporting ev-\nidence might include a software development plan that lays out the role of \narchitecture in a project, an organization’s architecture-based training cur-\nriculum, or many other kinds of documentation.\n■\n■How sustainable is your answer over time, over different systems, and \nacross different architects? For example, we might ask how an answer \nmight change if a different architect came on board the project.\n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "24.4  For Further Reading\n475\nThe outcome of an assessment is organized by the practice areas of the \nframework. For each practice, we assign one of three values that correspond to \n“you’re doing this well,” “you could be doing this better (and experiencing more \nbenefit),” and “this is an area of high risk.” Graphically, we show this as green \nlight, yellow light, red light. We have found that this simple metric provides orga-\nnizations with enough granularity to turn their attention to problem areas, which \nis the whole point of the assessment. We do not give an overall rating. Thus, we \nclosely mirror the “continuous representation” option of maturity models such as \nCMMI, in which the result is a vector rather than a scalar. \nWe present the findings in a written report and a slide presentation. In both, \nwe describe and justify each finding, based on what we were told in the inter-\nviews and/or read in provided documents. \n24.3  Summary\nThe vast majority of work on software and systems architecture (including our \nown) has focused on the technical aspects. But an architecture is much more \nthan a technical “blueprint” for a system. This has led us to try to understand, in \na more holistic way, what an architect and an architecture-centric organization \nmust do to succeed. To this end, we have developed a framework that aids us in \nassessing an organization for competence. \nWe use the framework to ask questions about an organization’s practices. \nWe can also ask about recent architecture successes and failures, and investigate \nthe causes of each. The output of this exercise is a formal report that assesses \ncompetence at organization, team, and individual levels. Along with this report \nwe make improvement recommendations based on assessment results; these, too, \nare tied to the underlying competence models. \nYou can do the same sort of evaluation on your own organization. The key \nto the process is in understanding the various models and in creating questions \nbased on these models that aid you in assessing how well you are doing in those \nareas that you care about. Given this knowledge, you can create your own im-\nprovement plan, as an individual architect or for an entire organization.\n24.4  For Further Reading\nThe four models that underlie the assessment framework presented here are de-\nscribed in more detail in the Technical Note “Models for Evaluating and Improv-\ning Architecture Competence” [Bass 08]. These models are the following:\n",
      "content_length": 2488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "476 \nPart Four  Architecture and Business\t\n24—Architecture Competence\n■\n■Duties, Skills, and Knowledge (DSK) model of competence. This model is \npredicated on the belief that architects and architecture-producing organiza-\ntions are useful sources for understanding the tasks necessary to the job of ar-\nchitecting. To assemble a comprehensive set of duties, skills, and knowledge \nfor architects, we surveyed approximately 200 sources of information target-\ned to professional architects—books, websites, blogs, position descriptions, \nand more. The results of this survey can be found in [Clements 07].\n■\n■Human Performance model of competence. This model is based on the hu-\nman performance engineering work of Thomas Gilbert [Gilbert 07]. This \nmodel is predicated on the belief that competent individuals in any pro-\nfession are the ones who produce the most valuable results at a reasonable \ncost. Using this model will involve figuring out how to measure the value \nand cost of the outputs of architecture efforts, finding areas where that ratio \ncan be improved, and crafting improvement strategies based on environ-\nmental and behavioral factors.\n■\n■Organizational Coordination model of competence. The focus of this mod-\nel is on creating an interteam coordination model for teams developing a \nsingle product or a closely related set of products. The architecture for the \nproduct induces a requirement for teams to coordinate during the realiza-\ntion or refinement of architectural decisions. The organizational structure, \npractices, and tool environment of the teams allow for particular types of \ncoordination with a particular interteam communication bandwidth. The \ncoordination model of competence compares the requirements for coor-\ndination that the architecture induces with the bandwidth for coordination \nsupported by the organizational structure, practices, and tool environment \n[Cataldo 07].\n■\n■Organizational Learning model of competence. This model is based on \nthe concept that organizations, and not just individuals, can learn. Organi-\nzational learning is a change in the organization that occurs as a function \nof experience. This change can occur in the organization’s cognitions or \nknowledge (e.g., as presented by Fiol and Lyles [Fiol 85]), its routines or \npractices (e.g., as demonstrated by Levitt and March [Levitt 88]), or its \nperformance (e.g., as presented by Dutton and Thomas [Dutton 84]). Al-\nthough individuals are the medium through which organizational learning \ngenerally occurs, learning by individuals within the organization does not \nnecessarily imply that organizational learning has occurred. For learning to \nbe organizational, it has to have a supra-individual component [Levitt 88]. \nThere are three approaches to measure organizational learning: (1) measure \nknowledge directly through questionnaires, interviews, and verbal proto-\ncols; (2) treat changes in routines and practices as indicators of changes in \nknowledge; or (3) view changes in organizational performance indicators \nassociated with experience as reflecting changes in knowledge [Argote 07].\n",
      "content_length": 3121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "24.5  Discussion Questions\n477\nThe Open Group offers a certification program for qualifying the skills, \nknowledge, and experience of IT, business, and enterprise architects, which is \nrelated to measuring and certifying an individual architect’s competence. Visit \nopengroup.org for details. The International Association of Software Architects \n(IASA) offers a similar certification; see iasahome.org. \nDana Bredemeyer and Ruth Malan have written many articles on the \nrole of the software architect (www.bredemeyer.com/who.htm), including \ntheir duties and skills [Bredemeyer 11] (www.bredemeyer.com/Architect/ \nArchitectSkillsLinks.htm). They have their own competence framework as well \nas a skills development program.\nThe U.K. Chapter of the International Council on Systems Engineering (IN-\nCOSE) maintains a “Core Competencies Framework” for systems engineers that \nincludes a “Basic Skills and Behaviours” section listing “the usual common at-\ntributes required by any professional engineer” [INCOSE 05]. The list includes \ncoaching, communication, negotiation and influencing, and “team working.” \nThe classic work on the Balanced Scorecard was created by Kaplan and \nNorton [Kaplan 92] and the classic work on Business/IT Alignment was origi-\nnally created by Luftman [Luftman 00], although this has been updated to explic-\nitly consider the role of architecture in alignment [Chen 10].\nBoehm, Valerdi, and Honour [Boehm 07] provide one of the few empirical \nstudies of systems engineering and how an investment in “better” engineering \npays off (or doesn’t) in the future. \n24.5  Discussion Questions\n1.\t\nIn which skills and knowledge discussed in this chapter do you think you \nmight be deficient? How would you reduce these deficiencies?\n2.\t\nWhich duties, skills, or knowledge do you think are the most important or \ncost-effective to improve in an individual architect? Justify your answer.\n3.\t\nHow would you measure the specific value of architecture in a project? \nHow would you distinguish the value added by architecture from the val-\nue added by other activities such as quality assurance or configuration \nmanagement?\n4.\t\nHow do you measure items such as “customer satisfaction” or “negotiation \nskills”? How would you validate such measurements?\n5.\t\nHow would you distinguish benefits caused by systematic organizational \nlearning from the benefits due to heroic efforts by individuals within the \norganization?\n",
      "content_length": 2435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "478 \nPart Four  Architecture and Business\t\n24—Architecture Competence\n6.\t\nSection 24.2 listed a number of practices of an architecturally competent \norganization. Prioritize that list based on expected benefit over expected \ncost. \n7.\t\nSuppose you are in charge of hiring an architect for an important system \nin your company. How would you go about it? What would you ask the \ncandidates in an interview? Would you ask them to produce anything? If so, \nwhat? Would you have them take a test of some kind? If so, what? Who in \nyour company would you have interview them? Why?\n",
      "content_length": 576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "479\n25\nArchitecture and \nSoftware Product Lines\nComing together is a beginning. Keeping together \nis progress. Working together is success.\n—Henry Ford\nA software architecture represents a significant investment of time and effort, \nusually by senior talent. So it is natural to want to maximize the return on this \ninvestment by reusing an architecture across multiple systems. \nThere are many ways this happens in practice. The patterns we discussed in \nChapter 13 are a big step in this direction; using a pattern is reusing a package of \narchitectural decisions (albeit not a complete architecture). And strictly speaking, \nevery time you make a change to a system, you are reusing its architecture (or \nwhatever portion of its architecture you don’t have to change). \nThis chapter shows yet another way to reuse a software architecture (and \nmany other assets as well) across a family of related systems, and the benefits \nthat doing so can bring. Many software-producing organizations tend to produce \nsystems or products that resemble each other more than they differ. This is an \nopportunity for reusing the architecture across these similar products. These soft-\nware product lines simplify the creation of new members of a family of similar \nsystems.\nThis kind of reuse has been shown to bring substantial benefits that include \nreduced cost of construction, higher quality, and greatly reduced time to market. \nThis is the lure of the software product line approach to system building.\nThe Software Engineering Institute defines a software product line as “a set \nof software-intensive systems sharing a common, managed set of features that \nsatisfy the specific needs of a particular market segment or mission and that are \ndeveloped from a common set of core assets in a prescribed way.”\nThe vision is of a set of reusable assets (called core assets) based on a com-\nmon architecture and the software elements that populate that architecture. The \n",
      "content_length": 1961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "480 \nPart Four\t\n25—Architecture and Software Product Lines\ncore assets also include designs and their documentation, user manuals, project \nmanagement artifacts such as budgets and schedules, software test plans and test \ncases, and more. \nThe product line approach works because the core assets were built specifi-\ncally to support multiple members of the same family of products. Hence, reusing \nthem is faster and less expensive than reinventing those software assets for each \nnew product or system in the organization’s portfolio. Core assets, including the \narchitecture, are usually designed with built-in variation points—places where \nthey can be quickly tailored in preplanned ways. \nOnce the core assets are in place, system building becomes a matter of\n■\n■Accessing the appropriate assets in the core asset base\n■\n■Exercising the variation points to configure them as required for the system \nbeing built \n■\n■Assembling that system \nIn the ideal case, this can be done automatically. Additional software developed \nfor an individual product, if needed at all, tends to account for a small fraction of \nthe total software. Integration and testing replace design and coding as the pre-\ndominant activities. \nProduct lines are nothing new in manufacturing. Many historians trace the con-\ncept to Eli Whitney’s use of interchangeable parts to build rifles in the early \n1800s, but earlier examples also exist. Today, there are hundreds of examples \nin manufacturing: think of the products of companies like General Motors, Toy-\nota, Boeing, Airbus, Dell, even McDonald’s, and the portfolio of similar products \nthat each one produces. Each company exploits commonality in different ways. \nBoeing, for example, developed the 757 and 767 in tandem, and the parts lists of \nthese two very different aircraft overlap by about 60 percent. \nThe improvements in cost, time to market, and productivity that come with a suc-\ncessful software product line can be breathtaking. Consider:\n■\n■Nokia credits the software product line approach with giving it flexibility to \nbring over a dozen phones to market each year, as opposed to the three or \nso it could manage before, all with an unprecedented variety of features.\n■\n■Cummins, Inc., was able to reduce the time it takes to produce the software \nfor a diesel engine from about a year to about a week.\n■\n■Hewlett-Packard builds products using one-quarter of the staff, in one-third \nof the time, and with one twenty-fifth the number of defects, compared with \nsoftware built before the advent of software product line engineering. \n■\n■Deutsche Bank estimates $4 million in savings per year realized from \nbuilding its global transaction and settlement software as a product line. \n■\n■Philips reports reduced faults during integration in its high-end television \nportfolio by adopting the product line approach. Product diversity used \nto be one of the top three concerns of their architects. Now it doesn’t \neven make the list of concerns at all; the product line approach has taken \n",
      "content_length": 3034,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 502,
      "content": "25—Architecture and Software Product Lines﻿\n481\nsoftware development off the critical path—the software no longer deter-\nmines the delivery date of the product. \n■\n■With a product line of satellite ground control systems it commissioned, the \nU.S. National Reconnaissance Office reported the first product requiring \n10 percent the expected number of developers and having one-tenth the ex-\npected number of defects.\n■\n■In Philips’s medical systems product line, the software product line ap-\nproach has cut both software defects and time to market by more than half.\nCreating a successful product line depends on a coordinated strategy involving \nsoftware engineering, technical management, and organization management. Be-\ncause this is a book on software architecture, we focus on the architectural as-\npects of software product lines, but all aspects must work together in order for an \norganization to successfully create a product line.\nThat Silver Lining Might Have a Cloud\nThe software product line paradigm is a powerful way to leverage an \ninvestment in architecture (and other core assets) into a family of related \nsystems and thus see order-of-magnitude improvements in time to mar-\nket, quality, and productivity. These results are possible and have been \ndemonstrated by companies large and small in many different domains. \nThe effects are real. Further, data from many sources and companies con-\nfirms with astonishing consistency that, to make the investment pay off, \nan organization needs to build only three products. This is the minimum \nnumber we would expect to have in a product line.\nBut other results are possible as well, and a spectacular crash-and-burn \nis not out of the question when trying to adopt this approach. Product line \npractice, like any technology, needs careful thought given to its adoption, \nand a company’s history, situation, and culture must be taken into account. \nFactors that can contribute to product line failure include these:\n■\n■\nLack of a champion in a position of sufficient control and visibility\n■\n■\nFailure of management to provide sustained and unwavering support\n■\n■\nReluctance of middle managers to relinquish autocratic control of projects\n■\n■\nFailure to clearly identify business goals for adopting the product line \napproach\n■\n■\nAbandoning the approach at the first sign of difficulty\n■\n■\nFailure to adequately train staff in the approach and failure to explain or \njustify the change adequately\n■\n■\nLack of discipline in managing the architecture’s variation points\n■\n■\nScoping the product line too broadly or too narrowly\n■\n■\nLack of product line tooling to help manage and exercise the variation \npoints\n",
      "content_length": 2674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "482 \nPart Four\t\n25—Architecture and Software Product Lines\nFortunately, there are strategies for overcoming most of these factors. \nOne good strategy is to launch a small but visible pilot project to demon-\nstrate the quantitative benefits of software product lines. The pilot can \nbe staffed by those most willing to try something new while the skeptics \ngo about their business. It can work out process issues, clarify roles and \nresponsibilities, and in general work out the bugs before the approach is \ntransitioned to a wider setting. \n—PCC\n25.1  An Example of Product Line Variability\nThe following example will help us illustrate the concept of product line vari-\nability. In a product line of software to support U.S. bank loan offices, suppose \nwe have a software module that calculates what a customer owes in the current \nmonth. For 18 of the 21 products in our product line, this module is completely \nadequate. However, our company is about to enter the market in the state of Del-\naware, which has certain laws that affect what a customer can owe. For the three \nproducts we plan to sell in Delaware, we need a module that differs from the \n“standard” module. Analysis shows that the difference will affect about 250 lines \nof source code in our 8,000-line module. \nTo build one of the Delaware products, what do we do? An obvious op-\ntion is to copy the module, change the 250 or so lines, and use the new version \nin the three products. This practice is called “clone-and-own”—the new projects \n“clone” the module, change it, and then “own” the new version. Most companies, \nwhen faced with this situation, resort to clone-and-own. It’s expedient in that it \nprovides a quick start to a new product, but it comes with a substantial cost down \nthe road. \nThe problem with clone-and-own is that it doesn’t scale. Suppose each of \nour 21 products comprises roughly 100 modules. If each module is allowed to \ndiverge for each product, that’s potentially 2,100 modules that the maintenance \nstaff has to deal with, each one spiraling off on its own separate maintenance \ntrajectory based on the needs of the lone project each version is used in. Many \ncompanies’ growth in a market is limited—brought to a halt, in fact—by their in-\nability to staff the maintenance of so many separate versions of so many different \nassets composing the products in their portfolio. An organization fielding several \nversions of several products finds itself dealing with a staggeringly complex code \nbase. The strain begins to show when a systematic change needs to be made to all \nof the products—for example, to add a new feature, or migrate to a new platform, \nor make the user interface work in a different language. Because each version \nof each component used in each product has been allowed to evolve separately, \nnow suddenly making a systematic change becomes prohibitively expensive (and \n",
      "content_length": 2896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "25.2  What Makes a Software Product Line Work?\n483\nonly gets worse each time a new product is added—the labor involved grows as \nthe square of the number of products). It only takes a few such portfolio-wide \nchanges before organizations feel that they’ve hit a wall of complexity and \nexpense.\nSo much for clone-and-own. What else can we do? Instead of allowing up to \n21 versions of each module, we would much rather find a way to take advantage \nof the fact that these nearly identical modules vary only in small, well-defined \nways. To take advantage of their similarities, we introduce a variation mechanism \ninto the module. (Variation mechanisms are often realized as tactics, such as the \n“defer binding” set of tactics described in Chapter 7.) This variation mechanism \nwill let us maintain a single module that can adapt to the range of variations in \nthe applications (in our example, the 21 banking products) that it has to support. \nIf we plan to market our products in states that, like Delaware, have their own \nlaws affecting what a customer owes, we may need to support additional varia-\ntions of the module. So our variation mechanism should be able to accommodate \nthose possibilities as well. \nThe payoff for this up-front planning is that an asset used in any of the prod-\nucts exists as a single version that (through the exercising of built-in variation \nmechanisms) works for all of the products in the product line. And now, mak-\ning a portfolio-wide change merely consists of changing the core assets that are \naffected. Because all future versions of all products use the same core assets, \nchanging the core asset base has the effect of changing all of the products in the \norganization’s portfolio.\n25.2  What Makes a Software Product Line Work?\nWhat makes product lines succeed is that the commonalities shared by the prod-\nucts can be exploited through reuse to achieve production economies. The poten-\ntial for reuse is broad and far-ranging, including the following: \n■\n■Requirements. Most of the requirements are common with those of earlier \nsystems and so can be reused. In fact, many organizations simply maintain \na single set of requirements that apply across the entire family as a core \nasset; the requirements for a particular system are then written as “delta” \ndocuments off the full set. In any case, most of the effort consumed by re-\nquirements analysis is saved from system to system.\n■\n■Architectural design. An architecture for a software system represents a \nlarge investment of time from the organization’s most talented engineers. \nAs we have seen, the quality goals for a system—performance, reliability, \nmodifiability, and so forth—are largely promoted or inhibited once the \narchitecture is in place. If the architecture is wrong, the system cannot be \n",
      "content_length": 2810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 505,
      "content": "484 \nPart Four\t\n25—Architecture and Software Product Lines\nsaved. For a new product, however, this most important design step is al-\nready done and need not be repeated.\n■\n■Software elements. Software elements are applicable across individual \nproducts. Element reuse includes the (often difficult) initial design work. \nDesign successes are captured and reused; design dead ends are avoided, \nnot repeated. This includes design of each element’s interface, its docu-\nmentation, its test plans and procedures, and any models (such as perfor-\nmance models) used to predict or measure its behavior. One reusable set of \nelements is the system’s user interface, which represents an enormous and \nvital set of design decisions. And as a result of this interface reuse, products \nin a product line usually enjoy the same look and feel as each other, an ad-\nvantage in the marketplace.\n■\n■Modeling and analysis. Performance models, schedulability analysis, dis-\ntributed system issues (such as proving the absence of deadlock), allocation \nof processes to processors, fault tolerance schemes, and network load poli-\ncies all carry over from product to product. Companies that build real-time \ndistributed systems report that one of the major headaches associated with \nproduction has all but vanished. When they field a new product in their \nproduct line, they have high confidence that the timing problems have been \nworked out and that the bugs associated with distributed computing—\nsynchronization, network loading, and absence of deadlock—have been \neliminated.\n■\n■Testing. Test plans, test processes, test cases, test data, test harnesses, and \nthe communication paths required to report and fix problems are already in \nplace. \n■\n■Project planning artifacts. Budgeting and scheduling are more predictable \nbecause experience is a high-fidelity indicator of future performance. Work \nbreakdown structures need not be invented each time. Teams, team size, \nand team composition are all easily determined.\nAll of these represent valuable core assets, each of which can be imbued \nwith its own variation points that can be exercised to build a product. We’ll look \nat architectural variation points later in this chapter, but for now imagine that any \nartifact represented by text can consist of text blocks that are exposed or hidden \nfor a particular product. Thus, the artifact that is maintained in the core asset base \nrepresents a superset of any version that will be produced for a product.\nArtifact reuse in turn enables reuse of knowledge:\n■\n■Processes, methods, and tools. Configuration control procedures and fa-\ncilities, documentation plans and approval processes, tool environments, \nsystem generation and distribution procedures, coding standards, and many \nother day-to-day engineering support activities can all be carried over from \nproduct to product. The software development process is in place and has \nbeen used before.\n",
      "content_length": 2940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "25.2  What Makes a Software Product Line Work?\n485\nGiving Software Reuse a New Lease on Life\nSoftware product lines rely on reuse, but reuse has a long but less than \nstellar history in software engineering, with the promise almost always \nexceeding the payoff. One reason for this failure is that until now reuse \nhas been predicated on the idea of “If you build it, they will come.” A reuse \nlibrary is stocked with snippets from previous projects, and developers are \nexpected to check it first before coding new elements. Almost everything \nconspires against this model. If the library is too sparse, the developer will \nnot find anything of use and will stop looking. If the library is too rich, it will \nbe hard to understand and search. If the elements are too small, it is eas-\nier to rewrite them than to find them and carry out whatever modifications \nthey might need. If the elements are too large, it is difficult to determine \nexactly what they do in detail, which in any case is not likely to be exactly \nright for the new application. In most reuse libraries, pedigree is hazy at \nbest. The developer cannot be sure exactly what the element does, how \nreliable it is, or under what conditions it was tested. And there is almost \nnever a match between the quality attributes needed for the new applica-\ntion and those provided by the elements in the library.\nIn any case, it is common that the elements were written for a different \narchitectural model than the one the developer of the new system is using. \nEven if you find something that does the right thing with the right quality \nattributes, it is doubtful that it will be the right kind of architectural element \n(if you need an object, you might find a process), that it will have the right \ninteraction protocol, that it will comply with the new application’s error-han-\ndling or failover policies, and so on.\nThis has led to so many reuse failures that many project managers have \ngiven up on the idea. “Bah!” they exclaim. “We tried reuse before, and it \ndoesn’t work!”\nSoftware product lines make reuse work by establishing a strict context for \nit. The architecture is defined; the functionality is set; the quality attributes are \nknown. Nothing is placed in the reuse library—or “core asset base” in product \nline terms—that was not built to be reused in that product line. Product lines \nwork by relying on strategic or planned, not opportunistic, reuse.\n—PCC\n■\n■People. Because of the commonality of applications, personnel can be \ntransferred among projects as required. Their expertise is applicable across \nthe entire line.\n■\n■Exemplar systems. Deployed products serve as high-quality demonstration \nprototypes or engineering models of performance, security, safety, and \nreliability.\n",
      "content_length": 2769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "486 \nPart Four\t\n25—Architecture and Software Product Lines\n■\n■Defect elimination. Product lines enhance quality because each new system \ntakes advantage of the defect elimination in its forebears. Developer and cus-\ntomer confidence both rise with each new instantiation. The more complicat-\ned the system, the higher the payoff for solving vexing performance, distribu-\ntion, reliability, and other engineering issues once for the entire family.\nAll of this reuse helps products launch more quickly, with higher quality, \nlower cost, and more predictable budget and schedule. This is critical for getting \na product to market in a timely fashion. However, these benefits do not come \nfor free. A product line may require a substantial up-front investment of time \nand effort to set up and manage, as well as to keep the core assets responsive to \nchanging market needs.\n25.3  Product Line Scope\nOne of the most important inputs to an architect building an architecture for a \nsoftware product line is the product line’s scope. A product line’s scope is a state-\nment about what systems an organization is willing to build as part of its line \nand what systems it is not willing to build. Defining a product line’s scope is like \ndrawing a doughnut in the space of all possible systems, as shown in Figure 25.1. \nThe doughnut’s center represents the systems that the organization could eas-\nily build using its base of core assets; these are within its production capability. \nSystems outside the doughnut are out of scope because they are ones the product \nline’s core assets are not well equipped to handle; this would be like asking Toy-\nota to build, say, apple pies on one of its automotive assembly lines. \nFigure 25.1  The space of all possible systems is divided into areas within \nscope (white), areas outside of scope (speckled), and areas that require case-by-\ncase disposition (gray).\n",
      "content_length": 1897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "25.3  Product Line Scope\n487\nSystems on the doughnut itself could be handled, but with some effort. \nThese often represent invitations from the marketplace asking the organization \nto extend its product line. To take advantage of such an opportunity, the orga-\nnization would have to broaden its production capability—that is, make its core \nasset base able to handle the new product. These opportunities require case-by-\ncase disposition as they arise, to see if the potential payoff (such as entry into a \nslightly different area of the market) would outweigh the cost to modify the core \nassets. This would be like asking Toyota to build a riding lawnmower.\nThe scope represents the organization’s best prediction about what products \nit will be asked to build in the foreseeable future. Input to the scoping process \ncomes from the organization’s strategic planners, marketing staff, domain ana-\nlysts who can catalog similar systems (both existing and on the drawing board), \nand technology experts.\nA product line scope is a critical factor in the success of the product line. \nScope too narrowly (the products only vary in a small number of features) and \nan insufficient number of products will be derived to justify the investment in \ndevelopment. Scope too broadly (the products vary in kind as well as in features) \nand the effort required to develop individual products from the core assets is too \ngreat to lead to significant savings. Scope can be refined as a portion of the initial \nestablishment of the product line or opportunistically depending on the product \nline adoption strategy (see the section on adoption strategies in Section 25.8).\nThe problem in defining the scope is not in finding commonality—a cre-\native architect can find points of commonality between any two systems—but \nin finding commonality that can be exploited to substantially reduce the cost of \nconstructing the systems that an organization intends to build. When consider-\ning scope, more than just the systems being built should be considered. Market \nsegmentation and types of customer interactions assumed will help determine \nthe scope of any particular product line. For example, Philips, the Dutch manu-\nfacturer of consumer electronics, has distinct product lines for home video elec-\ntronic systems and digital video communication. Video is the common thread, \nbut one is a mass market, where the customer is assumed to have very little video \nsophistication, and the other is a much smaller market consisting purely of video \nprofessionals. The products being developed reflect these assumptions about the \nsophistication of customers and the amount of care each customer will receive. \nThese differences were sufficient to keep Philips from attempting to develop a \nsingle product line for both markets.\nNarrowly scoped product lines offer opportunities to build specialized tools \nto support the specification of new products. For example, General Motors’ Pow-\nertrain division builds a software product line of automotive software. It makes \nan individual product from its product line core assets based on contracts stored \nin a database. Each element has well-defined interfaces and possible variation \npoints. A tool searches the database based on desired features and assembles the \nproduct. \n",
      "content_length": 3305,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "488 \nPart Four\t\n25—Architecture and Software Product Lines\nThe scope definition is vital to the product line architect because the scope \ndefines what is common across all members of the product line, and the specific \nways in which the products differ from each other. The fixed part of a product \nline architecture reflects what is constant, and the architecture’s variation points \naccommodate the variations among products. \n25.4  The Quality Attribute of Variability\nScoping decisions, which tell the product line architect what kinds of systems are \n“in” and what kinds of systems are “out” of the product line, lead to the introduc-\ntion of variability in the core assets. In fact, the quality attribute of variability is \nmost closely associated with product lines. Some may feature high-performance \nproducts, or high-security products, or high-availability products, but all prod-\nuct lines feature variability aimed at satisfying the commonalities and variations \nidentified by the product line’s scope.\nWe introduced variability in Chapter 12. There we said that variability is a \nspecial form of modifiability, pertaining to the ability of a core asset to adapt to \nusages in the different product contexts that are within the product line scope. \nThe goal of variability in a software product line is to make it easy to build and \nmaintain products in the product line over time. \nTable 25.1 gives the general scenario for variability. The source is some actor in \nthe product line organization who identifies a need for variation; this actor is proba-\nbly someone involved in setting the product line’s scope, such as a marketer.\nIdentifying variation is a constant, iterative process in the life of a software \nproduct line. Because of the many different ways a product can vary, particu-\nlar variants can be identified at virtually any time during the development pro-\ncess. Some variations are identified during product line requirement elicitations; \nothers, during architecture design; and still others, during implementation. Vari-\nations may also be identified during implementation of the second (and subse-\nquent) products as well.\nProduct line architectures feature variability as an important quality attri-\nbute. They achieve this by incorporation of variation mechanisms, which we will \ndiscuss in more detail shortly.\n25.5  The Role of a Product Line Architecture\nOf all of the assets in a core asset repository, the software architecture plays the \nmost central role. There is both a tactical and a strategic reason for this.\n",
      "content_length": 2556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "25.5  The Role of a Product Line Architecture\n489\nTable 25.1  The General Scenario for Variability\nPortion of Scenario\nPossible Values\nSource \nActor requesting variability\nStimulus\nRequests to support variations in the following:\n■\n■\nHardware\n■\n■\nFeature sets\n■\n■\nTechnologies\n■\n■\nUser interfaces\n■\n■\nQuality attributes\n■\n■\n. . . and more\nfor the range of products affected, such as:\n■\n■\nAll\n■\n■\nA specified subset\n■\n■\nThose that include feature set x\n■\n■\nNew products\nEnvironment\nVariants are to be created at:\n■\n■\nRuntime\n■\n■\nBuild time\n■\n■\nDevelopment time\nArtifact\nAsset(s) affected, such as:\n■\n■\nRequirements \n■\n■\nArchitecture\n■\n■\nComponent x\n■\n■\nTest suite y\n■\n■\nProject plan z\n■\n■\n. . . and more\nResponse\nThe requested variants can be created.\nResponse measure\nA specified cost and/or time to create the core assets and \nto create the variants using these core assets\nThe tactical reason is the importance the architecture plays in building prod-\nucts in a product line. The essence of building a successful software product line \nis discriminating between what is expected to remain constant across all family \nmembers and what is expected to vary. Software architecture is ideal for handling \nthis variation, because all architectures are abstractions that admit multiple in-\nstances. By its very nature every architecture is a statement about what we expect \nto remain constant and what we admit may vary. For example, interfaces to com-\nponents are designed to remain stable, with anticipated changes hidden behind \nthose interfaces.\nIn a software product line, the architecture has to encompass both the \nvarying and the nonvarying aspects. A product line architecture must be de-\nsigned to accommodate a set of explicitly allowed variations. Thus, identifying \n",
      "content_length": 1774,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "490 \nPart Four\t\n25—Architecture and Software Product Lines\nthe allowable variations is part of the architect’s responsibility, as is providing \nbuilt-in mechanisms for achieving them. Those variations may be substantial. \nProducts in a software product line exist simultaneously and may vary in terms \nof their behavior, quality attributes, platform, network, physical configuration, \nmiddleware, scale factors, and so forth.\nThe strategic reason has to do with the capability it imparts to an organiza-\ntion outside the realm of an existing product line. As we saw in Chapters 2 and 3, \nan architecture can serve as a technical platform for launching new applications \nand even new business models, and it can serve as a springboard for an organiza-\ntion diving into a new business area. This seems to be especially true for product \nline architectures. There are many cases where an organization has taken advan-\ntage of its production capability—that is, its core asset base crowned by a product \nline architecture—by using that capability to enter new markets. For example, \nCummins took its product line of automotive diesel engines to enter and quickly \ndominate the neighboring market for industrial diesel engines. Industrial diesel \nengines power things like rock crushers and ski lifts, markets of low volume and \nhigh specialization. Systems in that market built uniquely for each application are \nexpensive and don’t yield a high return. But a product line that includes industrial \ndiesel engines in its scope, and whose production capability supports industrial \ndiesel engines, is a recipe for rapid market capture.\nA product line architect needs to consider three things that are unique to \nproduct line architectures:\n■\n■Identifying variation points. This is done by using the scope definition and \nproduct line requirements as input. The product line architect determines \nwhere in the architecture variation points should be made available to sup-\nport the rapid building of products.\n■\n■Supporting variation points. This is done by introducing variation mecha-\nnisms, which will be discussed in the next section.\n■\n■Evaluating the architecture for product line suitability, which will be dis-\ncussed later in this chapter.\n25.6  Variation Mechanisms\nIn a conventional architecture, the mechanism for achieving different instances \noften comes down to modifying the code. But in a software product line, modify-\ning code is undesirable, because this leads to a large number of separately main-\ntained implementations that quickly outstrip an organization’s ability to keep \nthem up to date and consistent. \nThree primary architectural variation mechanisms are these:\n",
      "content_length": 2686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "25.6  Variation Mechanisms\n491\n■\n■Inclusion or omission of elements. This decision can be reflected in the \nbuild procedures for different products, or the implementation of an ele-\nment can be conditionally compiled based on some parameter indicating its \npresence or absence.\n■\n■Inclusion of a different number of replicated elements. For instance, \nhigh-capacity variants might be produced by adding more servers—the ac-\ntual number should be unspecified, as a point of variation, and may be done \ndynamically.\n■\n■Selection of different versions of elements that have the same interface but \ndifferent behavioral or quality attribute characteristics. Selection can occur \nat compile time, build time, or runtime. Selection mechanisms include stat-\nic libraries, which contain external functions linked after compilation time; \ndynamic link libraries, which have the flexibility of static libraries but defer \nthe decision until runtime based on context and execution conditions; and \nadd-ons (e.g., plug-ins, extensions, and themes), which add or modify ap-\nplication functionality at runtime. By changing the libraries, we can change \nthe implementation of functions whose names and signatures are known. \nSome variation mechanisms can be introduced that change aspects of a par-\nticular software element. Modifying the source code each time the element is \nused in a new product—that is, clone-and-own—falls into this category, although \nit is undesirable. More sophisticated techniques include the following:\n■\n■Extension points. These are identified places in the architecture where addi-\ntional behavior or functionality can be safely added.\n■\n■Reflection. This is the ability of a program to manipulate data on itself or its \nexecution environment or state. Reflective programs can adjust their behav-\nior based on their context.\n■\n■Overloading. This is a means of reusing a named functionality to operate \non different types. Overloading promotes code reuse, but at the cost of un-\nderstandability and code complexity.\nOther commonly used variation mechanisms include those in Table 25.2. \nChoosing the right variation mechanism affects numerous costs:\n■\n■The skill set required to implement, or learn and use, the specific variation \nmechanism, such as server or framework programming\n■\n■The one-time costs of building or acquiring the tools (such as compilers or \ngenerators) required to create the variation mechanism \n■\n■The recurring cost and time to exercise the variation mechanism \nThe choice of variation mechanism also affects downstream users and \ndevelopers:\n■\n■The targeted group of users that use the mechanism for product-specific \nadaptation, such as product developer, integrator, system administrator, and \nend user\n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "492 \nPart Four\t\n25—Architecture and Software Product Lines\nFinally, the choice of variation mechanism affects product quality:\n■\n■The impact of the variation mechanism on quality, such as possible perfor-\nmance penalties or memory consumption\n■\n■The impact on the mechanism’s maintainability\nThe architect should document the choice of variation mechanisms. In fact, \nthe documentation of variation mechanisms is the primary way in which the doc-\numentation for a product line architecture differs from that of a conventional ar-\nchitecture. In the documentation template we presented in Chapter 18, the section\nTable 25.2  Common Variation Mechanisms\nVariation \nMechanism\nProperties Relevant to \nBuilding the Core Assets\nProperties Relevant to Exercising  \nthe Variation Mechanism When \nBuilding Products\nInheritance; \nspecializing or \ngeneralizing a \nparticular class\nCost: Medium\nSkills: Object-oriented \nlanguages\nStakeholder: Product developers\nTools: Compiler\nCost: Medium\nComponent \nsubstitution\nCost: Medium\nSkills: Interface definitions\nStakeholder: Product developer, system \nadministrator\nTools: Compiler\nCost: Low\nAdd-ons, plug-\nins\nCost: High\nSkills: Framework \nprogramming\nStakeholder: End user\nTools: None\nCost: Low\nTemplates\nCost: Medium\nSkills: Abstractions\nStakeholder: Product developer, system \nadministrator\nTools: None\nCost: Medium\nParameters \n(including text \npreprocessors)\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: Product developer, system \nadministrator, end user\nTools: None\nCost: Low\nGenerator\nCost: High\nSkills: Generative \nprogramming\nStakeholder: System administrator, end \nuser\nTools: Generator\nCost: Low\nAspects\nCost: Medium\nSkills: Aspect-oriented \nprogramming\nStakeholder: Product developer\nTools: Aspect-oriented language \ncompiler\nCost: Medium\nRuntime \nconditionals\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: None\nTools: None\nCost: No development cost; some \nperformance cost\nConfigurator\nCost: Medium\nSkills: No special skills \nrequired\nStakeholder: Product developer\nTools: Configurator\nCost: Low to medium\n",
      "content_length": 2086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "25.7  Evaluating a Product Line Architecture\n493\ncalled the variability guide is reserved for exactly this purpose. The variability \nguide should describe each variation mechanism, how and when to exercise it, \nand what allowed variations it supports. The architecture documentation should \nalso describe the architecture’s instantiation process—that is, how its variation \npoints are exercised. Also, if certain combinations of variations are disallowed, \nthen the documentation needs to explain valid and invalid variation choices.\n25.7  Evaluating a Product Line Architecture\nLike any other, the architecture for a software product line should be evaluated \nfor fitness of purpose. The architecture should be evaluated for its robustness and \ngenerality, to make sure it can serve as the basis for products in the product line’s \nenvisioned scope. It should also be evaluated to make sure it meets the specific \nbehavioral and quality requirements of the product at hand. We begin by focusing \non the what and how of the evaluation and then turn to when it should take place.\nWhat and How to Evaluate. The evaluation will have to focus on the vari-\nation points to make sure they are appropriate, that they offer sufficient flexibility \nto cover the product line’s intended scope, that they allow products to be built \nquickly, and that they do not impose unacceptable runtime performance costs. If \nyour evaluation is scenario based, expect to elicit scenarios that involve instanti-\nating the architecture to support different products in the family. Also, different \nproducts in the product line may have different quality attribute requirements, \nand the architecture will have to be evaluated for its ability to provide all required \ncombinations. Here again, try to elicit scenarios that capture the quality attributes \nrequired of family members.\nOften, some of the hardware and other performance-affecting factors for \na product line architecture are unknown to begin with. In this case, evaluation \ncan establish bounds on the performance that the architecture is able to achieve, \nassuming bounds on hardware and other variables. The evaluation can identify \npotential contention so that you can put in place the policies and strategies to \nresolve it.\nWhen to Evaluate. An evaluation should be performed on an instance or vari-\nation of the architecture that will be used to build one or more products in the prod-\nuct line. The extent to which this is a separate, dedicated evaluation depends on the \nextent to which the product’s requirements differ from the product line architecture \nenvelope. If it does not differ, the product architecture evaluation can be abbreviated, \nbecause many of the issues normally raised in a single product evaluation will have \nbeen dealt with in the product line evaluation. In fact, just as the product architecture \nis a variation of the product line architecture, the product architecture evaluation is \na variation of the product line architecture evaluation. Therefore, depending on the \nevaluation method used, the evaluation artifacts (scenarios, checklists, and so on) \n",
      "content_length": 3128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "494 \nPart Four\t\n25—Architecture and Software Product Lines\nwill have reuse potential, and you should create them with that in mind. The results \nof evaluation of product architectures often provide useful feedback to the product \nline architects and fuel architectural improvements.\nWhen a new product is proposed that falls outside the scope of the original \nproduct line (for which the architecture was presumably evaluated), the product \nline architecture can be reevaluated to see if it will suffice for it. If it does, the \nproduct line’s scope can be expanded to include the new product, or to spawn a \nnew product line. If it does not, the evaluation can determine how the architecture \nwill have to be modified to accommodate the new product. The product line and \nproduct instance architectures can be evaluated not only to determine architec-\ntural risks but also to understand economic consequences (see Chapter 23), to \ndetermine which products will yield the most return.\n25.8  Key Software Product Line Issues\nIt takes considerable maturity in the developing organization to successfully field \na product line. Technology is not the only barrier to this; organization, process, \nand business issues are equally vital to master to fully reap the benefits of the \nsoftware product line approach.\nArchitecture definition is an important activity for any project, but as we saw \nin the previous section, it needs to emphasize variation points in a software product \nline. Configuration management is also an important activity for any project, but it is \nmore complex for a software product line because each product is the result of bind-\ning a large number of variations. The configuration management problem for prod-\nuct lines is to reproduce any version of any product delivered to any customer, where \n“product” means code and supporting artifacts ranging from requirement specs and \ntest cases to user manuals and installation guides. This involves knowing what ver-\nsion of each core asset was used in a product’s construction, how every asset was \ntailored, and what special-purpose code or documentation was added.\nExamining every facet of launching a product line and institutionalizing a \nproduct line culture is outside the scope of this book, but the next sections will \nexamine a few of the key areas that must be addressed. These are issues that an \norganization will have to face when considering whether to adopt a product line \napproach for software development and, if so, how to go about it.\nAdoption Strategies\nAn organization’s culture and context will dramatically affect how it goes about \nadopting a product line approach. Here are some of the important organizational \nand process factors that we have seen in practice.\n",
      "content_length": 2757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "25.8  Key Software Product Line Issues\n495\nTop-Down vs. Bottom-Up.  Top-down adoption arises when a (typically \nhigh level) manager decrees that the organization will use the approach. The \nproblem is to get employees in the trenches to change the way they work. Bot-\ntom-up adoption happens when designers and developers working at the product \nlevel realize that they are needlessly duplicating each other’s work and begin to \nshare resources and develop generic core assets. The problem is finding a man-\nager willing to sponsor the work and spread the technique to other parts of the \norganization. Both approaches work; both are helped enormously by the presence \nof a strong champion—someone who has thoroughly internalized the product \nline vision and can share that compelling vision with others. (It works better if \nthe champion is in a position of some authority.)\nProactive vs. Reactive.  There are two primary models for how an organiza-\ntion may grow a product line:\n■\n■In a proactive product line, an organization defines the family using a com-\nprehensive definition of scope. They do this not with a crystal ball but by \ntaking advantage of their experience in the application area, their knowledge \nabout the market and technology trends, and their good business sense. \nThe proactive model allows the organization to make the most far-reaching \nstrategic decisions. Explicitly scoping the product line allows you to look \nat areas that are underrepresented by products already in the marketplace, \nmake small extensions to the product line, and move quickly to fill the gap. In \nshort, proactive product line scope allows an organization to take charge of its \nown fate. Sometimes an organization does not have the ability to forecast the \nneeds of the market with the certainty suggested by the proactive model. The \nproactive model also takes some time to define and implement, and in that \ntime the organization needs to continue to construct products.\n■\n■In a reactive product line, an organization builds the next member or mem-\nbers of the product family from earlier products. This is best used when \nthere is uncertainty of requirements. Perhaps the domain is a new one. \nPerhaps the market is in flux. Or perhaps the organization cannot afford to \nbuild a core asset base that will cover the entire scope all at once. In the \nreactive model, with each new product the architecture is extended as need-\ned and the core asset base is built up from what has turned out to be com-\nmon. The reactive model puts much less emphasis on up-front planning and \nstrategic direction setting. Rather, the organization lets itself be taken where \nthe market dictates. This is an example of agile architecting, as described in \nChapter 15.\nIncremental vs. Big Bang.  If you are proactively building a product line, \nyou still need to choose how to populate it: all at once or incrementally over time. \nPopulating the core asset base all at once is a strategy that has worked success-\nfully for some organizations. However, it tends to require all or nearly all of the \n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "496 \nPart Four\t\n25—Architecture and Software Product Lines\norganization’s resources be focused on that task, at the expense of new product \nproduction. A different approach is to populate the core asset base incrementally, \nas circumstances and resources permit. Each product that goes out the door is \nbuilt with whatever core assets are available at the time. That means that early \nproducts will include software not derived from core assets. But those products \nwill still be better off (that is, faster to market, of higher quality, and easier to \nmaintain) than products built entirely from unique code. And it’s entirely pos-\nsible that some of the software unique to those early products can be extracted, \nadapted, and generalized to become core assets themselves, thus helping populate \nthe core asset base in a reactive fashion.\nKnowing the various adoption models can help an organization choose the \none that is right for it. For example, the proactive model requires a heavier initial \ninvestment but less rework than the reactive model. The reactive model relies \nexclusively on rework with little initial investment. Which model should act as a \nguide for a particular organization depends on the business situation.\nCreating Products and Evolving a Product Line\nAn organization that has a product line will have an architecture and a collection \nof elements associated with it. From time to time, the organization will create a \nnew member of the product line that will have features both in common with and \ndifferent from those of other members.\nOne problem associated with a product line is managing its evolution. As \ntime passes, the line—or, more precisely, the set of core assets from which prod-\nucts are built—must evolve. That evolution will be driven by both external and \ninternal sources:\nExternal sources\n■\n■New versions of existing elements within the line will be released by their \nvendors, and future products will need to be constructed from them. \n■\n■New externally created elements may be added to the line. Thus, for ex-\nample, functions that were previously performed by internally developed \nelements may now be performed by elements acquired externally, or vice \nversa. Or future products will need to take advantage of new technology, as \nembodied in externally developed elements.\n■\n■New features may be added to the product line to keep it responsive to user \nneeds or competitive pressures.\nInternal sources\n■\n■Some entity within the organization must determine if new functions add-\ned to a product are within the product line’s scope. If so, they can simply \nbe built from the asset base. If not, a decision must be made: either the \nenhanced product spins off from the product line, following its own evolu-\ntionary path, or the asset base must be expanded to include it. Updating the \n",
      "content_length": 2835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "25.9  Summary\n497\nline may be the wisest choice if the new functionality is likely to be used in \nfuture products, but this capability comes at the cost of the time necessary \nto update the core assets.\n■\n■An organization may wish to replace old products with ones built from the \nmost up-to-date version of the asset base. Keeping products compatible \nwith the product line takes time and effort. But not doing so may make fu-\nture upgrades more time consuming, because either the product will need to \nbe brought into compliance with the latest product line elements or it will \nnot be able to take advantage of improvements in the line.\nOrganizational Structure\nAn asset base on which products depend, but which has its own evolutionary \npath (perhaps driven by technology change), requires an organization to decide \nhow to manage both it and product development. There are two main organiza-\ntional strategies from which to choose, plus a number of minor variations. The \ntwo main structures reflect different answers to the question “Shall we have a \ndedicated group whose sole job is to build and maintain our core asset base?”\n1.\t\nWe’re all in this together. In this scheme, there is no separate core asset \ngroup. The product-building development teams coordinate closely, and \ndivide up the core asset responsibilities among themselves. That is, Product \nTeam 1 might be assigned responsibility for the development and mainte-\nnance of Core Assets 3, 6, 9, 12, and 15; Product Team 2 might take Core \nAssets 1, 4, and 8; and so forth. This works well enough for small organi-\nzations, but as size grows the communication channels become untenable. \nAlso, each team has to resist the temptation to build core assets that are \nespecially appropriate to its needs, but less so to other teams’ needs.\n2.\t\nSeparate core asset unit. In this scheme, a special unit is given responsibility \nfor the development and maintenance of the core asset base. Separate devel-\nopment teams in the organization’s business units build the products. In this \nscheme, the core asset unit (sometimes called a domain engineering unit) \nassumes the responsibility for the overall strategic direction of the product \nline. To the product teams, they appear almost like an external supplier. The \nproduct teams coordinate among themselves to set the core asset team’s de-\nvelopment and test priorities, based on product delivery obligations. \n25.9  Summary\nThis chapter presented an architecture-based development paradigm known \nas software product lines. The product line approach is steadily climbing in \n",
      "content_length": 2591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "498 \nPart Four\t\n25—Architecture and Software Product Lines\npopularity as more organizations see true order-of-magnitude improvements in \ncost, schedule, and quality from using it.\nLike all technologies, however, this one holds some surprises for the un-\naware. Architecturally, the key is identifying and managing commonalities and \nvariations, but nontechnical issues must be addressed as well, including how \nthe organization adopts the model, structures itself, and maintains its external \ninterfaces.\n25.10  For Further Reading\n[Clements 01a] is a comprehensive treatment of software product lines. It in-\ncludes a number of case studies as well as a thorough discussion of product line \n“practice areas,” which are areas of expertise a product line organization should \nhave (or should develop) to help bring about product line success. \n[van der Linden 07] contains a rich set of product line case studies.\n[Anastasopoulos 00] presents a good list of variation mechanisms, as do [Ja-\ncobson 97] and [Svahnberg 00]. [Bachmann 05] provides a list of their own, as \nwell as a treatment of each in terms of cost (it was the source for Table 25.2). \nOrganizational models for software product lines are treated in [Bosch 00]. \nThere is an active software product line community of research and practice. \nThe Software Product Line Conference (SPLC) is the mainstream forum for new \nsoftware product line research and success stories. You can find it at www.splc.\nnet. SPLC maintains a “Software Product Line Hall of Fame,” which showcases \nsuccessful software product lines that can serve as engineering models (and in-\nspiration) to aspiring product line organizations. Each year, new members of the \nHall of Fame are nominated, and in most years a new candidate is inducted. You \ncan see the winners at www.splc.net/fame.html.\nThe SEI’s website contains a wealth of material about software product \nlines, including a collection of “getting started” material: www.sei.cmu.edu/\nproductlines.\n25.11  Discussion Questions\n1.\t\nVariability is achieved by adding variation mechanisms to a system. Vari-\nation mechanisms include inheritance, component substitution, plug-ins, \ntemplates, parameters (including text preprocessors), generators, aspects, \nruntime conditionals, and a configurator tool. Because variability can be \nseen as a kind of modifiability, see if you can map each of these variation \nmechanisms to one or more modifiability tactics given in Chapter 7.\n",
      "content_length": 2469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "25.11  Discussion Questions\n499\n2.\t\nSuppose a company builds two similar systems using a large set of common \nassets, including an architecture. Which of the following would you say \nconstitutes a product line?\n■\n■Sharing only an architecture but no elements. \n■\n■Sharing only a single element. \n■\n■Sharing the same operating system and programming language runtime \nlibraries. \n■\n■Sharing the same team of developers.\nDefend your answer.\n3.\t\nPick a type of system you’re familiar with—for example, an automobile or \na smartphone. Think of three instances of that kind of system. Make a list \nof all of the things the three instances have in common. Now make a list of \nall of the things that distinguish the three instances from each other (that is, \ntheir variation points). If automobiles turn out to be too complex, start with \na simpler kind of “system,” such as an electric light.\n4.\t\nWrite some concrete scenarios to express the variability you identified in \nthe previous question.\n5.\t\nDo the list of variation mechanisms in this chapter constitute tactics for \nvariability? Discuss.\n6.\t\nIn many software product lines, products differ by the quality attributes \nthey exhibit. For instance, a company might sell a cheap, low-security ver-\nsion of its product alongside a more expensive, high-security version of the \nsame product. Which variation mechanisms might you choose to achieve \nthis kind of variability?\n",
      "content_length": 1421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "501\nPart  FIVE\nThe Brave New World\nParts I through IV of this book have dealt with the technical, organizational, and \nbusiness perspectives on software architecture. In this part, we turn our atten-\ntion to emerging technologies. We have often been asked whether principles or \ntechnology is more important, and the answer, of course, is “Yes, they are both \nimportant.” Principles have a long lifetime; technology that affects architects \ntends to change every decade or so. In this part, we provide brief introductions \nto two technologies that we believe will last and have a significant impact on \narchitects—the cloud and the edge. We also discuss one of the continuing prob-\nlems of many architects: How do I get my organization to embrace architectural \nprinciples?\nThe cloud provides you with the option of outsourcing your data center. The \nvision is that computing resources are available to an application as electricity \nis available to a consumer. That is, one plugs in an appliance and electricity is \navailable. In data center terms, you hook your web browser up to an application \nand computation power is available. All of the capacity, management, and opera-\ntional issues of a data center are taken care of by a third party, and all you, as an \narchitect, need to do is to utilize the resources you need. This trend has been ac-\ncompanied by a vast expansion of the amount of data that organizations manage. \nGoogle, Yahoo!, Facebook, and the other web giants all must manage petabytes \nof data. In Chapter 26, we provide a brief introduction to the technologies associ-\nated with the cloud and with managing these vast amounts of data.\nCloud computing is associated with the world of social networks and open \nsource. The term “edge-periphery” is used to describe both the crowdsource and \nthe open source movements. The term refers simultaneously to crowdsourced \n",
      "content_length": 1886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 523,
      "content": "502\nsystems such as Facebook and Wikipedia and open source systems such as the \nApache Web Server and Hadoop. In Chapter 27, we describe this phenomenon \nand explore some of the architectural implications of this aspect of the brave new \nworld.\nWe end by discussing “adoption.” It describes an approach to dealing with \nthe following problem: “OK, you guys have convinced me. Now I need to con-\nvince my organization of the importance of architectural principles. How do I do \nthat?”\n",
      "content_length": 484,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 524,
      "content": "503\n26\nArchitecture in the \nCloud\nThere was a time when every household, town, farm \nor village had its own water well. Today, shared public \nutilities give us access to clean water by simply turning \non the tap; cloud computing works in a similar fashion.\n—Vivek Kundra\nIf you have read anything about the history of computing, you will have read \nabout time-sharing. This was the era, in the late 1960s and the 1970s, sandwiched \nbetween eras when individuals had sole, although limited, access to multimil-\nlion-dollar computers and when individuals had access to their own personal \ncomputers. Time-sharing involved multiple users (maybe as many as several \nhundred) simultaneously accessing a powerful mainframe computer through a \nterminal, potentially remote from the mainframe. The operating system on the \nmainframe made it appear as if each user had sole access to that computer except, \npossibly, for performance considerations. The driving force behind the develop-\nment of time-sharing was economic; it was infeasible to provide every user with \na multimillion-dollar computer, but efficiently sharing this expensive but power-\nful resource was the solution.\nIn some ways, cloud computing is a re-creation of that era. In fact, some \nof the basic techniques—such as virtualization—that are used in the cloud to-\nday date from that period. Any user of an application in the cloud does not need \nto know that the application and the data it uses are situated several time zones \naway, and that thousands of other users are sharing it. Of course, with the advent \nof the Internet, the availability of much more powerful computers today, and the \nrequirement for controlled sharing, designing the architecture for a cloud-based \napplication is much different from designing the architecture for a time-shar-\ning-based application. The driving forces, however, remain much the same. The \n",
      "content_length": 1896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 525,
      "content": "504 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\neconomics of using the cloud as a deployment platform are so compelling that \nfew organizations today can afford to ignore this set of technologies.\nIn this chapter we introduce cloud concepts, and we discuss various ser-\nvice models and deployment options for the cloud, the economic justification \nfor the cloud, the base architectures and mechanisms that make the cloud work, \nand some sample technologies. We will conclude by discussing how an architect \nshould approach building a system in the cloud.\n26.1  Basic Cloud Definitions\nThe essential characteristics of cloud computing (based, in part, on definitions \nprovided by the U.S. National Institute of Standards and Technology, or NIST) \nare the following:\n1.\t\nOn-demand self-service. A resource consumer can unilaterally provision \ncomputing services, such as server time and network storage, as needed \nautomatically without requiring human interaction with each service’s pro-\nvider. This is sometimes called empowerment of end users of computing \nresources. Examples of resources include storage, processing, memory, net-\nwork bandwidth, and virtual machines.\n2.\t\nUbiquitous network access. Cloud services and resources are available over \nthe network and accessed through standard networking mechanisms that \npromote use by a heterogeneous collection of clients. For example, you can \neffectively run large applications on small platforms such as smart phones, \nlaptops, and tablets  by running the resource-intensive portion of those \napplications on the cloud. This capability is independent of location and \ndevice; all you need is a client and the Internet.\n3.\t\nResource pooling. The cloud provider’s computing resources are pooled. \nIn this way they can efficiently serve multiple consumers. The provider can \ndynamically assign physical and virtual resources to consumers, according \nto their instantaneous demands. \n4.\t\nLocation independence. The location independence provided by ubiquitous \nnetwork access is generally a good thing. It does, however, have one poten-\ntial drawback. The consumer generally has less control over, or knowledge \nof, the location of the provided resources than in a traditional implementa-\ntion. This can have drawbacks for data latency. The consumer may be able \nto ameliorate this drawback by specifying abstract location information \n(e.g., country, state, or data center). \n5.\t\nRapid elasticity. Due to resource pooling, it is easy for capabilities to be \nrapidly and elastically provisioned, in some cases automatically, to quickly \nscale out or in. To the consumer, the capabilities available for provisioning \noften appear to be virtually unlimited.\n",
      "content_length": 2722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 526,
      "content": "26.2  Service Models and Deployment Options\n505\n6.\t\nMeasured service. Cloud systems automatically control and optimize re-\nsource use by leveraging a metering capability for the chosen service (e.g., \nstorage, processing, bandwidth, and user accounts). Resource usage can be \nmonitored, controlled, and reported so that consumers of the services are \nbilled only for what they use.\n7.\t\nMulti-tenancy. Multi-tenancy is the use of a single application that is re-\nsponsible for supporting distinct classes or users. Each class or user has its \nown set of data and access rights, and different users or classes of users are \nkept distinct by the application.\n26.2  Service Models and Deployment Options\nIn this section we discuss more terminology and basic concepts. First we discuss \nthe most important models for a consumer using the cloud.\nCloud Service Models\nSoftware as a Service (SaaS).  The consumer in this case is an end user. \nThe consumer uses applications that happen to be running on a cloud. The ap-\nplications can be as varied as email, calendars, video streaming, and real-time \ncollaboration. The consumer does not manage or control the underlying cloud \ninfrastructure, including network, servers, operating systems, storage, or even in-\ndividual application capabilities, with the possible exception of limited user-spe-\ncific application configuration settings.\nPlatform as a Service (PaaS).  The consumer in this case is a developer or \nsystem administrator. The platform provides a variety of services that the con-\nsumer may choose to use. These services can include various database options, \nload-balancing options, availability options, and development environments. The \nconsumer deploys applications onto the cloud infrastructure using programming \nlanguages and tools supported by the provider. The consumer does not manage or \ncontrol the underlying cloud infrastructure, including network, servers, operating \nsystems, or storage, but has control over the deployed applications and possibly \napplication hosting environment configurations. Some levels of quality attributes \n(e.g., uptime, response time, security, fault correction time) may be specified by \nservice-level agreements (SLAs).\nInfrastructure as a Service (IaaS).  The consumer in this case is a de-\nveloper or system administrator. The capability provided to the consumer is to \nprovision processing, storage, networks, and other fundamental computing re-\nsources where the consumer is able to deploy and run arbitrary software, which \n",
      "content_length": 2530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 527,
      "content": "506 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\ncan include operating systems and applications. The consumer can, for example, \nchoose to create an instance of a virtual computer and provision it with some \nspecific version of Linux. The consumer does not manage or control the underly-\ning cloud infrastructure but has control over operating systems, storage, deployed \napplications, and possibly limited control of select networking components (e.g., \nhost firewalls). Again, SLAs are often used to specify key quality attributes.\nDeployment Models\nThe various deployment models for the cloud are differentiated by who owns and \noperates the cloud. It is possible that a cloud is owned by one party and operated \nby a different party, but we will ignore that distinction and assume that the owner \nof the cloud also operates the cloud.\nThere are two basic models and then two additional variants of these. The \ntwo basic models are private cloud and public cloud:\n■\n■Private cloud. The cloud infrastructure is owned solely by a single organi-\nzation and operated solely for applications owned by that organization. The \nprimary purpose of the organization is not the selling of cloud services.\n■\n■Public cloud. The cloud infrastructure is made available to the general pub-\nlic or a large industry group and is owned by an organization selling cloud \nservices.\nThe two variants are community cloud and hybrid cloud:\n■\n■Community cloud. The cloud infrastructure is shared by several organiza-\ntions and supports a specific community that has shared concerns (e.g., mis-\nsion, security requirements, policy, and compliance considerations). \n■\n■Hybrid cloud. The cloud infrastructure is a composition of two or more \nclouds (private, community, or public) that remain unique entities. The con-\nsumer will deploy applications onto some combination of the constituent \ncloud. An example is an organization that utilizes a private cloud except for \nperiods when spikes in load lead to servicing some requests from a public \ncloud. Such a technique is called “cloud bursting.” \n26.3  Economic Justification\nIn this section we discuss three economic distinctions between (cloud) data cen-\nters based on their size and the technology that they use: \n1.\t\nEconomies of scale\n",
      "content_length": 2280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 528,
      "content": "26.3  Economic Justification\n507\n2.\t\nUtilization of equipment\n3.\t\nMulti-tenancy\nThe aggregated savings of the three items we discuss may be as large as 80 \npercent for a 100,000-server data center compared to a 10,000-server data center. \nEconomic considerations have made almost all startups deploy into the cloud. \nMany larger enterprises deploy a portion of their applications into the cloud, and \nalmost every enterprise with substantial computation needs at least considers the \ncloud as a deployment platform. \nEconomies of Scale\nLarge data centers are inherently less expensive to operate per unit measure, such \nas cost per gigabyte, than smaller data centers. Large data centers may have hun-\ndreds of thousands of servers. Smaller data centers have servers numbered in the \nthousands or maybe even the hundreds. The cost of maintaining a data center \ndepends on four factors: \n1.\t\nCost of power. The cost of electricity to operate a data center currently is \n15 to 20 percent of the total cost of operation. The per-server power usage \ntends to be significantly lower in large data centers than in smaller ones \nbecause of the ability to share items such as racks and switches. In addi-\ntion, large power users can negotiate significant discounts (as much as 50 \npercent) compared to the retail rates that operators of small data centers \nmust pay. Some areas of the United States provide power at significantly \nlower rates than the national average, and large data centers can be located \nin those areas. Finally, organizations such as Google are buying or building \ninnovative and cheaper power sources, such as on- and offshore wind farms \nand rooftop solar energy.\n2.\t\nInfrastructure labor costs. Large data centers can afford to automate many \nof the repetitive management tasks that are performed manually in smaller \ndata centers. In a traditional data center, an administrator can service ap-\nproximately 140 servers, whereas in a cloud data center, the same adminis-\ntrator can service thousands of servers.\n3.\t\nSecurity and reliability. Maintaining a given level of security, redundancy, \nand disaster recovery essentially requires a fixed level of investment. Larger \ndata centers can amortize that investment over their larger number of serv-\ners and, consequently, the cost per server will be lower. \n4.\t\nHardware costs. Operators of large data centers can get discounts on hard-\nware purchases of up to 30 percent over smaller buyers. \nThese economies of scale depend only on the size of the data center and \ndo not depend on the deployment model being used. Operators of public clouds \n",
      "content_length": 2612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 529,
      "content": "508 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nhave priced their offerings so that many of the cost savings are passed on to their \nconsumers. \nUtilization of Equipment\nCommon practice in nonvirtualized data centers is to run one application per \nserver. This is caused by the dependency of many enterprise applications on par-\nticular operating systems or even particular versions of these operating systems. \nOne result of the restriction of one application per server is extremely low utili-\nzation of the servers. Figures of 10 to 15 percent utilization for servers are quoted \nby several different vendors. \nUse of virtualization technology, described in Section 26.4, allows for easy \nco-location of distinct applications and their associated operating systems on the \nsame server hardware. The effect of this co-location is to increase the utilization \nof servers. Furthermore, variations in workload can be managed to further in-\ncrease the utilization. We look at five different sources of variation and discuss \nhow they might affect the utilization of servers:\n1.\t\nRandom access. End users may access applications randomly. For example, \nthe checking of email is for some people continuous and for others time-\nboxed into a particular time period. The more users that can be supported \non a single server, the more likely that the randomness of their accesses will \nend up imposing a uniform load on the server.\n2.\t\nTime of day. Those services that are workplace related, unsurprisingly, tend \nto be more heavily used during the work day. Those that are consumer re-\nlated tend to be heavily used during evening hours. Co-locating different \nservices with different time-of-day usage patterns will increase the overall \nutilization of a server. Furthermore, time differences among geographically \ndistinct locations will also affect utilization patterns and can be considered \nwhen planning deployment schedules.\n3.\t\nTime of year. Some applications respond to dates as well as time of day. \nConsumer sites will see increases during the Christmas shopping season, \nand floral sites will see increases around Valentine’s Day and Mother’s Day. \nTax preparation software will see increases around the tax return submis-\nsion due date. Again, these variations in utilization are predictable and can \nbe considered when planning deployment schedules.\n4.\t\nResource usage patterns. Not all applications use resources in the same \nfashion. Search, for example, is heavier in its usage of CPU than email but \nlighter in its use of storage. Co-locating applications with complementary \nresource usage patterns will increase the overall utilization of resources. \n5.\t\nUncertainty. Organizations must maintain sufficient capacity to support \nspikes in usage. Such spikes can be caused by news events if your site is a \nnews provider, by marketing events if your site is consumer-facing, or even \n",
      "content_length": 2909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 530,
      "content": "26.4  Base Mechanisms\n509\nsporting events because viewers of sporting events may turn to their com-\nputers during breaks in the action. Startups can face surges in demand if \ntheir product catches on more quickly than they can build capacity.\nThe first four sources of variation are supported by virtualization without \nreference to the cloud or the cloud deployment model. The last source of varia-\ntion (uncertainty) depends on having a deployment model that can accommodate \nspikes in demand. This is the rationale behind cloud bursting, or keeping applica-\ntions in a private data center and offloading spikes in demand to the public cloud. \nPresumably, a public cloud provider can deploy sufficient capacity to accommo-\ndate any single organization’s spikes in demand.\nMulti-tenancy\nMulti-tenancy applications such as Salesforce.com or Microsoft Office 365 are \narchitected explicitly to have a single application that supports distinct sets of \nusers. The economic benefit of multi-tenancy is based on the reduction in costs \nfor application update and management. Consider what is involved in updating \nan application for which each user has an individual copy on their own desk-\ntop. New versions must be tested by the IT department and then pushed to the \nindividual desktops. Different users may be updated at different times because \nof disconnected operation, user resistance to updates, or scheduling difficulties. \nIncidents result because the new version may have some incompatibilities with \nolder versions, the new version may have a different user interface, or users with \nold versions are unable to share information with users of the new version.\nWith a multi-tenant application, all of these problems are pushed from IT \nto the vendor, and some of them even disappear. Any update is available at the \nsame instant to all of the users, so there are no problems with sharing. Any user \ninterface changes are referred to the vendor’s hotline rather than the IT hotline, \nand the vendor is responsible for avoiding incompatibilities for older versions.\nThe problems of upgrading do not disappear, but they are amortized over all \nof the users of the application rather than being absorbed by the IT department \nof every organization that uses the application. This amortization over more users \nresults in a net reduction in the costs associated with installing an upgraded ver-\nsion of an application.\n26.4  Base Mechanisms\nIn this section we discuss the base mechanisms that clouds use to provide their \nlow-level services. In an IaaS instance, the cloud provides to the consumer a vir-\ntual machine loaded with a machine image. Virtualization is not a new concept; \n",
      "content_length": 2687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 531,
      "content": "510 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nit has been around since the 1960s. But today virtualization is economically en-\nticing. Modern hardware is designed to support virtualization, and the overhead \nit adds has been measured to be just 1 percent per instance running on the bare \nhardware.\nWe will discuss the architecture of an IaaS platform in Section 26.5. In this \nsection, we describe the concepts behind a virtual machine: the hypervisor and \nhow it manages virtual machines, a storage system, and the network.\nHypervisor\nA hypervisor is the operating system used to create and manage virtual machines. \nBecause each virtual machine has its own operating system, a consumer applica-\ntion is actually managed by two layers of operating system: the hypervisor and \nthe virtual machine operating system. The hypervisor manages the virtual ma-\nchine operating system and the virtual machine operating system manages the \nconsumer application. The key services used by the hypervisor to support the \nvirtual machines it manages are a virtual page mapper and a scheduler. A hyper-\nvisor, of course, provides additional services and has a much richer structure than \nwe present here, but these key services are the two that we will discuss.\nPage Mapper\nWe begin by describing how virtual memory works on a bare (nonvirtualized) \nmachine. All modern servers utilize virtual memory. Virtual memory allows an \napplication to assume it has a large amount of memory in which to execute. The \nassumed memory is mapped into a much smaller physical memory through the \nuse of page tables. The consumer application is divided into pages that are either \nin physical memory or temporarily residing on a disk. The page table contains the \nmapping of logical address (consumer application address) to physical address \n(actual machine address) or disk location. Figure 26.1 shows the consumer ap-\nplication executing its next instruction. This causes the CPU to generate a target \naddress from which to fetch the next instruction or data item. The target address \nis used to address into a page table. The page table provides a physical address \nwithin the computer where the actual instruction or data item can be found if it is \ncurrently in main memory. If the physical address is not currently resident in the \nmain memory of the computer, an interrupt is generated that causes a page that \ncontains the target address to be loaded. This is the mechanism that allows a large \n(virtual) address space to be supported on much smaller physical memory.\nTurning the virtual memory mechanism into a virtualization mechanism in-\nvolves adding another level of indirection. Figure 26.2 shows a logical sequence \nthat maps from the consumer application to a physical machine address. Modern \nprocessors contain many optimizations to make this process more efficient. A \nconsumer application generates the next instruction with its target address. This \ntarget address is within the virtual machine in which the consumer application is \n",
      "content_length": 3043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 532,
      "content": "26.4  Base Mechanisms\n511\nTarget address of \nnext instruction\nFetch next\ninstruction\nfrom\nphysical\naddress\nFetch next\ninstruction\nfrom\ninterrupt\nhandler\nPhysical \naddress\ninside \ncurrent\naddress \nspace\nPhysical \naddress\noutside \ncurrent\naddress \nspace\nSoftware\ncomponent\nKey:\nHardware\ncomponent\nCPU\nPage table\nused to\nconvert\ntarget \naddress to\nphysical \naddress\nData flow\nFigure 26.1  Virtual memory page table\nSoftware\ncomponent\nKey:\nHardware\ncomponent\nData flow\nHost Server\nTarget address of \nnext instruction\nPage \nTable\nVMn\nPage \nTable\nnext\ninstruction\nVM1\nHypervisor\nHost page table\npoints to VM\npage table\nCPU\nFigure 26.2  Adding a second level of indirection to determine which virtual \nmachine the address references\n",
      "content_length": 726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 533,
      "content": "512 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nexecuting. The virtual machine page table maps this target address to an address \nwithin the virtual machine based on the target address as before (or indicates that \nthe page is not currently in memory). The address within the virtual machine is \nconverted to a physical address by use of a page table within the hypervisor that \nmanages the current virtual machines.\nScheduler\nThe hypervisor scheduler operates like any operating system scheduler. When-\never the hypervisor gets control, it decides on the virtual machine to which it will \npass control. A simple round-robin scheduling algorithm assigns the processor to \neach virtual machine in turn, but many other possible scheduling algorithms ex-\nist. Choosing the correct scheduling algorithm requires you to make assumptions \nabout the demand characteristics of the different virtual machines hosted within \na single server. One area of research is the application of real-time scheduling \nalgorithms to hypervisors. Real-time schedulers would be appropriate for the use \nof virtualization within embedded systems, but not necessarily within the cloud.\nStorage\nA virtual machine has access to a storage system for persistent data. The storage \nsystem is managed across multiple physical servers and, potentially, across clus-\nters of servers. In this section we describe one such storage system: the Hadoop \nDistributed File System (HDFS).\nWe describe the redundancy mechanism used in HDFS as an example of the \ntypes of mechanisms used in cloud virtual file systems. HDFS is engineered for \nscalability, high performance, and high availability.\nA component-and-connector view of HDFS within a cluster is shown in \nFigure 26.3. There is one NameNode process for the whole cluster, multiple \nDataNodes, and potentially multiple client applications. To explain the function \nof HDFS, we trace through a use case. We describe the successful use case for \n“write.” HDFS also has facilities to handle failure, but we do not describe these. \nSee the “For Further Reading” section for a reference to the HDFS failure-han-\ndling mechanisms. \nFor the “write” use case, we will assume that the file has already been \nopened. HDFS does not use locking to allow for simultaneous writing by differ-\nent processes. Instead, it assumes a single writer that writes until the file is com-\nplete, after which multiple readers can read the file simultaneously. The applica-\ntion process has two portions: the application code and a client library specific to \nHDFS. The application code can write to the client using a standard (but over-\nloaded) Java I/O call. The client buffers the information until a block of 64 MB \nhas been collected. Two of the techniques used by HDFS for enhancing perfor-\nmance are the avoidance of locks and the use of 64-MB blocks as the only block \n",
      "content_length": 2880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 534,
      "content": "26.4  Base Mechanisms\n513\nsize supported. No substructure of the blocks is supported by HDFS. The blocks \nare undifferentiated byte strings. Any substructure and typing of the information \nis managed solely by the application. This is one example of a phenomenon that \nwe will notice in portions of the cloud: moving application-specific functionality \nup the stack as opposed to moving it down the stack to the infrastructure.\nFor reliability purposes each block is replicated a parameterizable number \nof times, with a default of three. For each block to be written, the NameNode al-\nlocates DataNodes to write the replicas. The DataNodes are chosen based on two \ncriteria: (1) their location—replicas are spread across racks to protect against the \npossibility that a rack fails; and (2) the dynamic load on the DataNode. Lightly \nloaded DataNodes are given preference over heavily loaded DataNodes to reduce \nthe possibility of contention for the DataNodes among different files being simul-\ntaneously accessed.\nOnce the client has collected a buffer of 64 MB, it asks the NameNode for \nthe identities of the DataNodes that will contain the actual replicas. The NameNode \nmanages only metadata; it is not involved in the actual transfer or recording of \ndata. These DataNode identities are sent from the NameNode to the client, which \nthen treats them as a pipeline. At this point the client streams the block to the first \nDataNode in the pipeline. The first DataNode then streams the data to the second \nProcess\nKey:\nConnector\nApplication layer\nClient layer\nRPC\nStreaming Protocol\nRPC\nNameNode Process\nDataNode Processes\nFigure 26.3  A component-and-connector view of an HDFS deployment. Each \nprocess exists on a distinct computer.\n",
      "content_length": 1739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 535,
      "content": "514 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nDataNode in the pipeline, and so forth until the pipeline (of three DataNodes, unless \nthe client has specified a different replication value) is completed. Each DataNode \nreports back to the client when it has successfully written the block, and also reports \nto the NameNode that it has successfully written the block.\nNetwork\nIn this section we describe the basic concepts behind Internet Protocol (IP) ad-\ndressing and how a message arrives at your computer. In Section 26.5 we discuss \nhow an IaaS system manages IP addresses.\nAn IP address is assigned to every “device” on a network whether this de-\nvice is a computer, a printer, or a virtual machine. The IP address is used both to \nidentify the device and provide instructions on how to find it with a message. An \nIPv4 address is a constrained 32-bit number that is, typically, represented as four \ngroups for human readability. For example, 192.0.2.235 is a valid IP address. \nThe familiar names that we use for URLs, such as “http://www.pearsonhighered.\ncom/”, go through a translation process, typically through a domain name server \n(DNS), that results in a numeric IP address. A message destined for that IP ad-\ndress goes through a routing process to arrive at the appropriate location. \nEvery IP message consists of a header plus a payload. The header contains \nthe source IP address and the destination IP address. IPv6 replaces the 32-bit \nnumber with a 128-bit number, but the header of an IP message still includes the \nsource and destination IP addresses.\nIt is possible to replace the header of an IP message for various reasons. One \nreason is that an organization uses a gateway to manage traffic between external \ncomputers and computers within the organization. An IP address is either “public,” \nmeaning that it is unique within the Internet, or “private,” meaning that multiple \ncopies of the IP address are used, with each copy owned by a different organization. \nPrivate IP addresses must be accessed through a gateway into the organization that \nowns it. For outgoing messages, the gateway records the address of the internal ma-\nchine and its target and replaces the source address in the TCP header with its own \npublic IP address. On receipt of a return message, the gateway would determine the \ninternal address for the message and overwrite the destination address in the header \nand then send the message onto the internal network. Network address translation \n(NAT) is the name of this process of translation.\n26.5  Sample Technologies\nBuilding on the base mechanisms, we now discuss some of the technologies that \nexist in the cloud. We begin by discussing the design of a generic IaaS platform, \n",
      "content_length": 2749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 536,
      "content": "26.5  Sample Technologies\n515\nthen we move up the stack to a PaaS, and finally we discuss database technology \nin the cloud.\nInfrastructure as a Service\nFundamentally, an IaaS installation provides three services: virtualized computa-\ntion, virtualized networking, and a virtualized file system. In the previous section \non base mechanisms, we described how the operating system for an individual \nserver manages memory to isolate each virtual machine and how TCP/IP mes-\nsages could be manipulated. An IaaS provides a management structure around \nthese base concepts. That is, virtual machines must be allocated and deallocated, \nmessages must be routed to the correct instance, and persistence of storage must \nbe ensured. \nWe now discuss the architecture of a generic IaaS platform. Various provid-\ners will offer somewhat different services within different architectures. Open-\nStack is an open source movement to standardize IaaS services and interfaces, \nbut as of this writing, it is still immature.\nFigure 26.4 shows an allocation view of a generic cloud platform. Each \nserver shown provides a different function to the platform, as we discuss next.\nAn IaaS installation has a variety of clusters. Each cluster may have thousands \nof physical servers. Each cluster has a cluster manager responsible for that cluster’s \nVirtual resource manager\nPersistent object manager\nCluster Manager\nFile System Manager\nNode Manager\nNode Manager\nNode Manager\nCluster Manager\nFile System Manager\nNode Manager\nNode Manager\nNode Manager\nCluster\nCluster\nKey:\nInternet message\nInternal cloud message\nCluster\nServer\nSOAP- or REST-\nbased tools\nWeb Browser\nFigure 26.4  A generic cloud allocation view\n",
      "content_length": 1690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 537,
      "content": "516 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nresources. The persistent object manager supports the manipulation of persistent \nobjects, and the virtual resource managers are in charge of the other virtualized re-\nsources. For requests for new resources, the virtual resource manager is in charge \nof determining which cluster manager will service the request. For requests sent to \nexisting resources, the virtual resource manager is responsible for seeing that the re-\nquests get forwarded to the correct server. The virtual resource manager, in this case, \nacts as a gateway, as described in Section 26.4. \nSome of the services that IaaS providers offer to support applications are \nthese:\n■\n■Automatic reallocation of IP addresses in the case of a failure of the under-\nlying virtual machine instance. This service is useful in case the instance \nhas a public IP address. Unless the provider offers this service, the client \nmust register the IP address of a replacement instance with a domain name \nserver to ensure that messages are sent to the correct location.\n■\n■Automatic scaling. One of the virtues of the cloud is that new instances \ncan be created or deleted relatively quickly in the event of a variation in \ndemand. Detecting the variation in demand, allocating (or deleting) an in-\nstance in the event of a variation, and ensuring that the remaining instances \nare allocated their fair share of messages is another service that could be \nprovided by the IaaS.\nThe persistent object manager is responsible for maintaining files that are in-\ntended to persist past the deletion of a virtual machine instance. It may maintain \nthese files across multiple clusters in a variety of different geographic locations.\nFailure of the underlying hardware is a common occurrence in a large data \ncenter, consequently the virtual resource manager has mechanisms to manage re-\nquests in the event of failure. These mechanisms are typically designed to main-\ntain the availability of the IaaS infrastructure and do not extend to the applica-\ntions deployed with the virtual machines. What this means in practice is that if \nyou make a request for a new resource, it will be honored. If you make a request \nto an existing virtual machine instance, the infrastructure will guarantee that, if \nyour virtual machine instance is active, your request is delivered. If, however, the \nhost on which your virtual machine instance has been allocated has failed, then \nyour virtual machine instance is no longer active and it is your responsibility as \nan application architect to install mechanisms to recognize a failure of your vir-\ntual machine instances and recover from them.\nThe file system manager manages the file system for each cluster. It is sim-\nilar to the Hadoop Distributed File System that we discussed in Section 26.4. It \nalso assumes that failure is a common occurrence and has mechanisms to repli-\ncate the blocks and to manage handoffs in the event of failures.\nThe cluster manager controls the execution of virtual machines running on \nthe nodes within its clusters and manages the virtual networking between virtual \nmachines and between virtual machines and external users.\n",
      "content_length": 3205,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 538,
      "content": "26.5  Sample Technologies\n517\nThe final piece of Figure 26.4 is the node manager; it (through the function-\nality of a hypervisor) controls virtual machine activities, including the execution, \ninspection, and termination of virtual machine instances.\nA client initially requests a virtual machine instance and the virtual resource \nmanager decides on which cluster the virtual machine instance should reside. It \npasses the instance request to the cluster manager, which in turn decides which \nnode should host the virtual machine instance.\nSubsequent requests are routed through the pieces of the generic infrastruc-\nture to the correct instance. The instance can create files using the file system \nmanager. These files will either be deleted when the virtual machine instance is \nfinished or will be persisted past the existence of the virtual machine instance. \nThe choice is the client’s as to how long storage is persisted. If the storage is \npersisted, it can be accessed independently of the creating instance through the \npersistence manager.\nPlatform as a Service\nA Platform as a Service provides a developer with an integrated stack within the \ncloud to develop and deploy applications. IaaS provides virtual machines, and it \nis the responsibility of the developer using IaaS to provision the virtual machines \nwith the software they desire. PaaS is preprovisioned with a collection of inte-\ngrated software. \nConsider a virtual machine provisioned with the LAMP (Linux, Apache, \nMySQL, PHP/Perl/Python) stack. The developer writes code in Python, for ex-\nample, and has available the services provided by the other elements of the stack. \nTake this example and add automatic scaling across virtual machines based on \ncustomer load, automatic failure detection and recovery, backup/restore, security, \noperating system patch installation, and built-in persistence mechanisms. This \nyields a simple example of a PaaS.\nThe vendors offering PaaS and the substance of their offerings are rapidly \nevolving. Google and Microsoft are two of the current vendors.\n1.\t\nThe Google App Engine provides the developer with a development en-\nvironment for Python or Java. Google manages deploying and executing \ndeveloped code. Google provides a database service that is automatically \nreplicated across data centers.\n2.\t\nMicrosoft Azure provides an operating system and development platform \nto access/develop applications on Microsoft data centers. Azure provides \na development environment for applications running on Windows using \n.NET. It also provides for the automatic scaling and replication of instances. \nFor example, if an application instance fails, then the Azure infrastructure \nwill detect the failure and deploy another instance automatically. Azure also \nhas a database facility that automatically keeps replicas of your databases.\n",
      "content_length": 2849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 539,
      "content": "518 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nDatabases\nA number of different forces have converged in the past decade, resulting in the \ncreation of database systems that are substantially different from the relational \ndatabase management systems (RDBMSs) that were prevalent during the 1980s \nand ’90s. \n■\n■Massive amounts of data began to be collected from web systems. A search \nengine must index billions of pages. Facebook, today, has over 800 million \nusers. Much of this data is processed sequentially and, consequently, the so-\nphisticated indexing and query optimizations of RDBMSs are not necessary.\n■\n■Large databases are continually being created during various types of pro-\ncessing of web data. The creation and maintenance of databases using a \ntraditional RDBMS requires a sophisticated data administrator.\n■\n■A theoretical result (the so-called CAP theorem) shows that it is not pos-\nsible to simultaneously achieve consistency, availability, and partitioning. \nOne of these properties must be sacrificed. For many applications, the \nchoice is to sacrifice consistency and provide immediate availability and \n“eventual consistency.” What this means, in practice, is that occasionally a \nuser will access stale data, but updates will be subsequently available. The \nalternative approach, taken by RDBMSs, is to lock values and not allow \naccess until they become consistent.\n■\n■The relational model is not the best model for some applications. The \nrelational model assumes there is one data item for each row-value/col-\numn-name pair. One method for handling web searches, for example, is to \nstore different versions of a single web page indexed by the same row-val-\nue/column-name pair so that the different versions of the web page can be \nquickly accessed and differences easily determined. Using the relational \nmodel requires that the system perform joins to retrieve all of the attributes \nassociated with a particular row value. Joins are expensive from a perfor-\nmance perspective, and consequently, newly emerging database systems \ntend to not support joins and require storing data in a denormalized form.\nThese forces resulted in the creation of new types of databases with different \ndata models and different access mechanisms. These new types of databases go \nunder the name of NoSQL—although as Michael Stonebraker has pointed out, \nthe existence or nonexistence of SQL within the database system is irrelevant to \nthe rationale for their existence.\nWe discuss two open source NoSQL database systems: a key-value one \n(HBase) and a document-centric one (MongoDB) .\nHBase\nHBase is a key-value database system based on the BigTable database system \ndeveloped by Google. Google uses BigTable to store data for many of their ap-\nplications. The number of data items in a HBase database can be in the billions \nor trillions.\n",
      "content_length": 2871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 540,
      "content": "26.5  Sample Technologies\n519\nHBase supports tables, although there is no schema used. One column is \ndesignated as the key. The other columns are treated as field names. A data value \nis indexed by a row value, a column name, and a time stamp. Each row value/\ncolumn name can contain multiple versions of the same data differentiated by \ntime stamps.\nOne use of HBase is for web crawling. In this application, the row value is \nthe URL for the web page. Each column name refers to an attribute of a web page \nthat will support the analysis of the web page. For example, “contents” might be \none column name. In the relational model, each row value/column name would \nretrieve the contents of the web page. Web pages change over time, however, \nand so in the relational model, there would need to be a separate column with \nthe time stamp, and the primary key for the table would be the URL/time stamp. \nIn HBase, the versions of the web page are stored together and retrieved by the \nURL value/“contents”. All of the versions of the web page are retrieved, and it is \nthe responsibility of the application to separate the versions of the web page and \ndetermine which one is desired based on the time stamp.\nMongoDB\nMongoDB uses a document-centric data model. You can think of it as storing ob-\njects rather than tables. An object contains all of the information associated with \na particular concept without regard to whether relations among data items are \nstored in multiple different objects. Two distinct objects may have no field names \nin common, some field names in common, or all of the field names in common.\nYou may store links rather than data items. Links support the concept of \njoining different objects without requiring the maintenance of indices and query \noptimization. It is the responsibility of the application to follow the link.\nDocuments are stored in binary JavaScript Object Notation (JSON) form. \nIndices can be created on fields and can be used for retrieval, but there is no con-\ncept of primary versus secondary keys. A field is either indexed or it is not. Be-\ncause the same field can occur in multiple different documents, a field is indexed \nwherever it occurs.\nWhat Is Left Out of NoSQL Databases\nOne motivation for NoSQL databases is performance when accessing millions or \nbillions of data items. To this end, several standard RDBMS facilities are omit-\nted in NoSQL databases. If an application wishes to have these features, it must \nimplement them itself. Mainly, the features are omitted for performance reasons.\n■\n■Schemas. NoSQL databases typically do not require schemas for their \ndata model and, consequently, there is no checking of field names for \nconsistency.\n■\n■Transactions. NoSQL typically does not support transactions. Transactions \nlock data items, which hinders performance. Applications use techniques \n",
      "content_length": 2863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 541,
      "content": "520 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nsuch as time stamps to determine whether fields have been modified \nthrough simultaneous access.\n■\n■Consistency. NoSQL databases are “eventually consistent.” This means that \nafter some time has passed, different replicas of a data item will have the \nsame value, but in the interim, it is possible to run two successive queries \nthat access the same data item and retrieve two different values.\n■\n■Normalization. NoSQL databases do not support joins. Joins are a require-\nment if you are to normalize your database.\n26.6  Architecting in a Cloud Environment\nNow we take the point of view of an architect who is designing a system to exe-\ncute in the cloud. In some ways, the cloud is a platform, and architecting a system \nto execute in the cloud, especially using IaaS, is no different than architecting for \nany other distributed platform. That is, the architect needs to pay attention to us-\nability, modifiability, interoperability, and testability, just as he or she would for \nany other platform. The quality attributes that have some significant differences \nare security, performance, and availability.\nSecurity\nSecurity, as always, has both technical and nontechnical aspects. The nontechnical \naspects of security are items such as what trust is placed in the cloud provider, what \nphysical security does the cloud provider utilize, how are employees of the cloud \nprovider screened, and so forth. We will focus on the technical aspects of security.\nApplications in the cloud are accessed over the Internet using standard Inter-\nnet protocols. The security and privacy issues deriving from the use of the Inter-\nnet are substantial but no different from the security issues faced by applications \nnot hosted in the cloud. The one significant security element introduced by the \ncloud is multi-tenancy. Multi-tenancy means that your application is utilizing a \nvirtual machine on a physical computer that is hosting multiple virtual machines. \nIf one of the other tenants on your machine is malicious, what damage can they \ndo to you?\nThere are four possible forms of attack utilizing multi-tenancy:\n1.\t\nInadvertent information sharing. Each tenant is given a set of virtual re-\nsources. Each virtual resource is mapped to some physical resource. It is \npossible that information remaining on a physical resource from one tenant \nmay “leak” to another tenant.\n2.\t\nA virtual machine “escape.” A virtual machine is isolated from other vir-\ntual machines through the use of a distinct address space. It is possible, \n",
      "content_length": 2586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 542,
      "content": "26.6  Architecting in a Cloud Environment\n521\nhowever, that an attacker can exploit software errors in the hypervisor to \naccess information they are not entitled to. Thus far, such attacks are ex-\ntremely rare. \n3.\t\nSide-channel attacks. It is possible for a malicious attacker to deduce infor-\nmation about keys and other sensitive information by monitoring the timing \nactivity of the cache. Again, so far, this is primarily an academic exercise.\n4.\t\nDenial-of-service attacks. Other tenants may use sufficient resources on the \nhost computer so that your application is not able to provide service.\nSome providers allow customers to reserve entire machines for their exclu-\nsive use. Although this defeats some of the economic benefits of using the cloud, \nit is a mechanism to prevent multi-tenancy attacks. An organization should con-\nsider possible attacks when deciding which applications to host in the cloud, just \nas they should when considering any hosting option.\nPerformance\nThe instantaneous computational capacity of any virtual machine will vary de-\npending on what else is executing on that machine. Any application will need to \nmonitor itself to determine what resources it is receiving versus what it will need.\nOne virtue of the cloud is that it provides an elastic host. Elasticity means \nthat additional resources can be acquired as needed. An additional virtual ma-\nchine, for example, will provide additional computational capacity. Some \ncloud providers will automatically allocate additional resources as needed, \nwhereas other providers view requesting additional resources as the customer’s \nresponsibility.\nRegardless of whether the provider automatically allocates additional re-\nsources, the application should be self-aware of both its current resource usage \nand its projected resource usage. The best the provider can do is to use general \nalgorithms to determine whether there is a need to allocate or free resources. An \napplication should have a better model of its own behavior and be better equipped \nto do its own allocation or freeing of resources. In the worst case, the application \ncan compare its predictions to those of the provider to gain insight into what will \nhappen. It takes time for the additional resources to be allocated and freed. The \nfreeing of resources may not be instantaneously reflected in the charging algo-\nrithm used by the provider, and that charging algorithm also needs to be consid-\nered when allocating or freeing resources.\nAvailability\nThe cloud is assumed to be always available. But everything can fail. A virtual \nmachine, for example, is hosted on a physical machine that can fail. The virtual \n",
      "content_length": 2676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 543,
      "content": "522 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\nnetwork is less likely to fail, but it too is fallible. It behooves the architect of a \nsystem to plan for failure.\nThe service-level agreement that Amazon provides for its EC2 cloud service \nprovides a 99.95 percent guarantee of service. There are two ways of looking at \nthat number: (1) That is a high number. You as an architect do not need to worry \nabout failure. (2) That number indicates that the service may be unavailable for \n.05 percent of the time. You as an architect need to plan for that .05 percent.\nNetflix is a company that streams videos to home television sets, and its \nreliability is an important business asset. Netflix also hosts much of its operation \non Amazon EC2. On April 21, 2011, Amazon EC2 suffered a four-day sporadic \noutage. Netflix customers, however, were unaware of any problem. \nSome of the things that Netflix did to promote availability that served them \nwell during that period were reported in their tech blog. We discussed their Sim-\nian Army in Chapter 10. Some of the other things they did were applications of \navailability tactics that we discussed in Chapter 5. \n■\n■Stateless services. Netflix services are designed such that any service in-\nstance can serve any request in a timely fashion, so if a server fails, requests \ncan be routed to another service instance. This is an application of the spare \ntactic, because the other service instance acts as a spare.\n■\n■Data stored across zones. Amazon provides what they call “availability \nzones,” which are distinct data centers. Netflix ensured that there were mul-\ntiple redundant hot copies of the data spread across zones. Failures were \nretried in another zone, or a hot standby was invoked. This is an example of \nthe active redundancy tactic.\n■\n■Graceful degradation. The general principles for dealing with failure are \napplications of the degradation or the removal from service tactic:\n■\n■Fail fast: Set aggressive timeouts such that failing components don’t \nmake the entire system crawl to a halt.\n■\n■Fallbacks: Each feature is designed to degrade or fall back to a lower \nquality representation.\n■\n■Feature removal: If a feature is noncritical, then if it is slow it may be re-\nmoved from any given page. \nThe CAP Theorem\nThe CAP theorem—created by Eric Brewer at UC Berkeley—emerged \nover a decade ago. Unlike most theories postulated by academics, \nthis one did not sink into obscurity but rather has grown in renown and \ninfluence since then. The theory states that there are three important \nproperties of a distributed system managing shared data. These are the \nfollowing:\n",
      "content_length": 2654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 544,
      "content": "26.6  Architecting in a Cloud Environment\n523\n■\n■\nConsistency (C): the data will be consistent throughout the distributed \nsystem. \n■\n■\nAvailability (A): the data will be highly available.\n■\n■\nPartitioning (P): the system will tolerate network partitioning.\nAnd the theory further states that no system can achieve all of these \nproperties simultaneously; the best we can hope for is to satisfy two out of \nthree while sacrificing (to some extent) the third property. Brewer explains \nit thus:\nThe easiest way to understand CAP is to think of two nodes on opposite \nsides of a partition. Allowing at least one node to update state will cause the \nnodes to become inconsistent, thus forfeiting C. Likewise, if the choice is to \npreserve consistency, one side of the partition must act as if it is unavailable, \nthus forfeiting A. Only when nodes communicate is it possible to preserve both \nconsistency and availability, thereby forfeiting P. \nIn fact, there is really another important facet to the CAP theorem that \nhas come to dominate the engineering challenge: latency. It wasn’t part of \nthe original acronym (although CLAP is certainly catchy), but a concern for \nlatency now infuses much of the discussion of the tradeoffs in implement-\ning NoSQL databases. \nCreators of large-scale distributed NoSQL databases are constantly \nfaced with tradeoffs. These days no one believes that you simply choose \ntwo of the three properties of CAP; the decisions are far richer and more \nsubtle than that. For example, designers of these systems like to speak of \n“eventual consistency”—partitions are allowed to become inconsistent on a \nregular basis, but with bounds that are carefully engineered and monitored. \nThey might want to specify that no more than x percent of the data should \nbe stale at any given time, and it should not take more than y seconds to \nrestore consistency (on average, or in the worst case). Another common \ntradeoff seen in practice is that availability and latency are typically favored \nover consistency. That is, a Facebook user should get quick response from \nthe system, even if their newsfeed is slightly stale.\nAll of this adds complexity to the system. The designer has to choose \nbetween faster/less consistent and slower/more consistent (as well as a \nhost of other quality issues). And the mechanisms for achieving eventual \nconsistency—caching, replication, message retries, timeouts, and so \nforth—are themselves nontrivial. Consistency, partitioning, latency, and \navailability are four qualities that can be traded off with NoSQL databases. \nIn addition, other quality attributes—interoperability, security, and so forth—\nalso add complexity, and so the tradeoffs involved can get more and more \ncomplicated.\nAlas, this is, increasingly, the world that we live in. Systems with global \nreach and enormous bases of distributed data are not going away anytime \nsoon. So as architects we need to be prepared to deal with tradeoffs and \ncomplexities for the foreseeable future.\n—RK\n",
      "content_length": 3018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 545,
      "content": "524 \nPart Five  Brave New World\t\n26—Architecture in the Cloud\n26.7  Summary\nThe cloud has become a viable alternative for the hosting of data centers primar-\nily for economic reasons. It provides an elastic set of resources through the use of \nvirtual machines, virtual networks, and virtual file systems.\nThe cloud can be used to provide infrastructure, platforms, or services. Each \nof these has its own characteristics.\nNoSQL database systems arose in reaction to the overhead introduced by \nlarge relational database management systems. NoSQL database systems fre-\nquently use a data model based on key-value or documents and do not provide \nsupport for common database services such as transactions.\nArchitecting in the cloud means that the architect should pay attention to \nspecific aspects of quality attributes that are substantially different in cloud envi-\nronments, namely: performance, availability, and security.\n26.8  For Further Reading\nDealers of Lightning: Xerox PARC and the Dawn of the Computer Age [Hiltzik \n00] has a discussion of time-sharing and covers the technologies and the person-\nalities involved in the development of the modern personal workstation.\nThe economics of the cloud are described in [Harms 10].\nThe Computer Measurement Group (CMG) is a not-for-profit worldwide \norganization that provides measurement and forecasting of various quantitative \naspects of computer usage. Their measurements of the overhead due to virtualiza-\ntion can be found at www.cmg.org/measureit/issues/mit39/m_39_1.html.\nIf you want to learn more about TCP/IP and NAT, you can find a discussion \nat www.ipcortex.co.uk/wp/fw.rhtm.\nThe BigTable system is described in [Chang 06].\nNetflix maintains a tech blog that is almost entirely focused on cloud issues. \nIt can be found at techblog.netflix.com.\nThe home page for MongoDB is www.mongodb.org/display/DOCS/Home \nand for HBase is hbase.apache.org.\nMichael Stonebraker is a database expert who has written extensively com-\nparing NoSQL systems with RDBMSs. Some of his writings are [Stonebraker \n09], [Stonebraker 10a], [Stonebraker 11], and [Stonebraker 10b].\nEric Brewer has provided a nice overview of the issues surrounding the CAP \ntheorem for today’s cloud-based systems: [Brewer 12].\n",
      "content_length": 2255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 546,
      "content": "26.9  Discussion Questions\n525\n26.9  Discussion Questions\n1.\t\n“Service-oriented or cloud-based systems cannot meet hard-real-time re-\nquirements because it’s impossible to guarantee how long a service will \ntake to complete.” Do you think this statement is true or false?  In either \ncase, identify the one or two categories of design decisions that are most \nresponsible for the correctness (or incorrectness) of the statement.\n2.\t\n“Using the cloud assumes your application is service oriented.” Do you \nthink this is true or false? Find some examples that would support that \nstatement and, if it is not universally true, find some that would falsify it.\n3.\t\nNetflix discussed their movement from Oracle to SimpleDB on their tech \nblog. They also discussed moving from SimpleDB to Cassandra. Describe \ntheir rationale for these two moves.\n4.\t\nNetflix also describes their Simian Army in their tech blog. Which elements \nof the Simian Army could be offered as a SaaS? What would the design of \nsuch a SaaS look like?\n5.\t\nDevelop the “Hello World” application on an IaaS and on a PaaS.\n",
      "content_length": 1086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 547,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 548,
      "content": "527\n27\nArchitectures for \nthe Edge\nWith Hong-Mei Chen\nHuman nature is not a machine to be built after a \nmodel, and set to do exactly the work prescribed for \nit, but a tree, which requires to grow and develop \nitself on all sides, according to the tendency of \nthe inward forces which make it a living thing.\n—John Stuart Mill\nIn this chapter we discuss the place of architecture in edge-dominant systems and \ndiscuss how an architect should approach building such systems. An edge-dom-\ninant system is one that depends crucially on the inputs of users for its success.\nWhat would Wikipedia be without the encyclopedic entries contributed by \nusers? What would YouTube be without the contributed videos? What would \nFacebook and Twitter be without their user communities? YouTube serves up ap-\nproximately 1 billion videos a day. Twitter boasts that its users tweet 50 million \ntimes per day. Facebook reports that it serves up about 30 billion pieces of con-\ntent each month. Flickr recently announced that users had uploaded more than 6 \nbillion photos. Strong, almost maniacal, user participation has elevated each of \nthese sites from fairly routine repositories to forces that shape society. \nIn each case, the value of these systems comes almost entirely from its us-\ners—who happily contribute their opinions and knowledge, their artistic content, \ntheir software, and their innovations—and not from some centralized organiza-\ntion. This phenomenon is a cornerstone of the so-called “Web 2.0” movement. \nDarcy DiNucci, credited with coining the term, wrote in 1999, “The [new] Web \nwill be understood not as screenfuls of text and graphics but as a transport mech-\nanism, the ether through which interactivity happens.” The “old” web was about \n",
      "content_length": 1753,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 549,
      "content": "528 \nPart Five  Brave New World\t\n27—Architectures for the Edge\ngoing to web pages for static information; the “new” web is about participating in \nthe information creation (“crowdsourcing”) and even becoming part of its organi-\nzation (“folksonomy”). \nMany have written about the social, political, and economic consequences \nof this change, and some see it as nothing short of a revolution along the lines of \nthe industrial revolution. Yochai Benkler’s book The Wealth of Networks—a play \non the title of Adam Smith’s classic book The Wealth of Nations, which heralded \nthe start of the industrial revolution—argues that the “radical transformation” of \nhow we create our information environment is restructuring society, particularly \nour models of production and consumption. Benkler calls this new economic \nmodel commons-based peer production. And it’s big: crowdsourced websites that \nare built on this model have become some of the dominant forces on the web and \nin society in the past few years. Populist revolutions are catalyzed by Twitter and \nFacebook. As of the time this book went to press, five of the top ten websites by \ntraffic are peer produced: Facebook, YouTube, Blogger, Wikipedia, and Twitter. \nAnd the other five are portals or search engines that pore through the content cre-\nated by billions of worldwide users. Websites that actually sell something from a \ncentralized organization are rare in the world’s top sites; Amazon.com is the only \nexample, and it’s no accident that it’s the bookseller that derives much of its pop-\nularity from the value created by customers. \nAlong with this paradigm shift, much of the world’s software is now open \nsource. The two most popular web browsers in the world are open source (Mo-\nzilla Firefox and Google Chrome). Apache is the most popular web server, cur-\nrently powering almost two out of every three websites. Open source databases, \nIDEs, content management systems, and operating systems are all heavy hitters \nin their respective market spaces.\nWhy study this in a book about architecture? First, commons-based \npeer-produced systems are an excellent example of the architecture influence \ncycle. Second, the architecture of such systems has some important differences \nfrom the architectures that you would build for traditional systems. We start by \nexamining how the forces of commons-based peer production change the very \nnature of the system’s development life cycle. \n27.1  The Ecosystem of Edge-Dominant Systems\nAll successful edge-dominant systems, and the organizations that develop and \nuse these systems, share a common ecosystem structure, as shown in Figure 27.1. \nThis is called a “Metropolis” structure, by analogy with a city.\nThe Metropolis structure is not an architecture diagram: it is a representa-\ntion of three communities of stakeholders:\n",
      "content_length": 2843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 550,
      "content": "27.1  The Ecosystem of Edge-Dominant Systems\n529\nCore \n    Edge \nMasses \nOpen  \nSource  \nSoftware \nOpen  \nContent \nSystems \n(Prosumers) \n(Developers) \n(End Users) \n(Customers) \nFigure 27.1  The Metropolis structure of an edge-dominant system \n■\n■Customers and end users, who consume the value produced by the \nMetropolis\n■\n■Developers, who write software and key content for the Metropolis\n■\n■Prosumers, who consume content but also produce it \nThe Metropolis structure also presents three realms of roles and \ninfrastructure: \n■\n■In the outermost ring reside the masses of end users of such systems. They \ncontribute requirements but not content. \n■\n■The middle ring contains developers and prosumers. These are the stake-\nholders at the edge whose actions and whose value creation the organiza-\ntion would like to facilitate. \n■\n■All of this is held together by the core. The core is software; it provides its \nservices through a set of APIs upon which the periphery can build. Linux’s \nkernel, Apache’s core, Wikipedia’s wiki, Facebook’s application platform, \nor the application platforms of the iPhone or Android.\nIn the Metropolis structure, the realms have different “permeability,” which \nthe figure indicates by the dashed and solid lines. \nIn open source systems (such as Linux, MySQL, Apache, Eclipse, and Fire-\nfox), it is possible to move from the role of an end user to a developer to a core \narchitect, by consistently contributing and moving up in the meritocracy. How-\never, in open content systems (such as Wikipedia, Twitter, YouTube, Slashdot, \n",
      "content_length": 1565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 551,
      "content": "530 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nand Facebook), nobody will invite you to become a software developer for the \ncore, no matter how many cool movies or blog entries you contribute.\nThus, a key question for an organization wishing to foster edge-dominant \nsystems: How should we architect the core and what development principles \nshould we embrace for the periphery/edge?\n27.2  Changes to the Software Development Life Cycle\nAll our familiar software development life cycles—waterfall, Agile, iterative, or \nprototyping-based—are broken in an edge-dominant, crowdsourced world. These \nmodels all assume that requirements can be known; software is developed, tested, \nand released in planned increments; projects have dedicated finite resources; and \nmanagement can “manage” these resources. None of these conditions is true in \nthe Metropolis. Let us consider each aspect in turn:\n■\n■Requirements can be known. In edge-dominant systems, requirements \nemerge from its individuals, operating independently. Requirements are \nnever knowable in any global sense and they will inevitably conflict, just \nas the requirements of a city’s inhabitants often conflict (some want better \nhighways, some want more park land).\n■\n■Software is developed, tested, and released in planned increments. All ex-\nisting software development models assume that systems evolve in an or-\nderly way, through planned releases. An edge-dominant system, on the oth-\ner hand, is constantly changing. It doesn’t make sense, for example, to talk \nabout the “latest release” of Wikipedia. Resources are noncentralized and \nso such a system is never “stable.” One cannot conceive of its functionality \nin terms of “releases” any more than a city has a release; parts are being \ncreated, modified, and torn down at all times.\n■\n■Projects have dedicated finite resources. Edge-dominant projects are \n“staffed” by members who are not employed by the project. Such projects \nare subject to the whims of the members who are not required and cannot \nbe compelled to contribute anything. However, successful projects tend to \nattract large numbers of contributors. Unlike traditional projects, which \nhave finite resources, typically limited by budgetary constraints, there is no \nnatural limit to the resources available to an edge-dominant project. And \nthese large numbers tend to ameliorate the (unreliable) actions of any indi-\nvidual contributor.\n■\n■Management can “manage” these resources. In edge-dominant systems, \nthe developers are often volunteers. They participate in decentralized pro-\nduction processes with no identifiable managers. Linus Torvalds, the creator \nof the Linux operating system, has noted that he has no authority to order \nanyone to do anything; he can only attempt to lead and persuade, in the \n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 552,
      "content": "27.3  Implications for Architecture\n531\nhope that others will follow. Teams in this world are often diverse with dif-\nfering, sometimes irreconcilable, views. \nFor edge-dominant systems, the old rules and tools of software development \nwon’t work. Such projects are, to varying degrees, community driven and de-\ncentralized with little overall control, as is the case with the major social net-\nworking communities (e.g., Twitter, Google+, Facebook), and open content sys-\ntems (e.g., Wikipedia, YouTube), especially coupled with open source software \ndevelopment. \n27.3  Implications for Architecture\nThe Metropolis structure presented in the previous section, while not an architec-\nture, has important implications for architecture. The key architectural choice for \nan edge-dominant system is the distinction between core and edge. That is, the \narchitecture of successful edge-dominant systems is, without fail, bifurcated into \n■\n■a core (or kernel) infrastructure and \n■\n■a set of peripheral functions or services that are built on the core. \nThis constitutes an architectural pattern very reminiscent of layering. Linux, \nFirefox, and Apache—to name just a few—are based upon this architectural pat-\ntern. Linux applies this pattern twice: at the outermost level the core is the entire \nLinux kernel, and individual applications, libraries, resources, and auxiliaries act \nas extensions to the kernel’s functionality—the periphery. Digging into the Li-\nnux kernel, we can once again discern a core/periphery pattern. Inside the Linux \nkernel, modules are defined to enable parallel development of different subsys-\ntems. The different functions that one expects to find in an operating system ker-\nnel are all present, but they are designed to be separate modules. For instance, \nthere are modules for processor/cache control, memory management, resource \nmanagement, file system interfacing, networking stacks, device I/O, security, and \nso forth. All of these modules interact, but they are clearly identifiable, separate \nmodules within the kernel.\nWe have said many times in this book that architectures come from business \ngoals, as interpreted through the lens of architecturally significant requirements. \nBut what are the business goals for an edge-dominant system? We said earlier \nthat you cannot “know” the requirements for such a system, in any complete \nsense. Well, this was perhaps a bit hasty. \nRequirements for such systems are typically bifurcated into core require-\nments and periphery requirements:\n■\n■Core requirements deliver little or no end-user value and focus on quality \nattributes and tradeoffs—defining the system’s performance, modularity, \n",
      "content_length": 2679,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 553,
      "content": "532 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nsecurity, and so forth. These requirements are generally slow to change as \nthey define the major capabilities and qualities of the system. \n■\n■Periphery requirements, on the other hand, are unknowable because they \nare contributed by the peer network. These requirements deliver the majori-\nty of the function and end-user value and change relatively rapidly.\nGiven this structure, the majority of implementation (the periphery) is \ncrowdsourced to the world using their own tools, to their own standards, at their \nown pace. The implementers of the core, on the other hand, are generally close \nknit and highly motivated. \nThis has at least three implications for the core, which the architect will \nneed to address:\n■\n■The core needs to be highly modular, and it provides the foundation for the \nachievement of quality attributes. The core in a successful core/periphery \npattern is designed by a small, coherent team. In open source projects, these \npeople are referred to as the “committers.” In Linux, for example, a strong \nemphasis on modularity has been postulated to account for its enormous \ngrowth. This allows for successful contributions of independent enhance-\nments by scores of distributed and unknown-to-each-other programmers. \nThe peripheral services are enabled by and constrained by the kernel, but \nare otherwise unspecified. \n■\n■The core must be highly reliable. Most cores are heavily tested, which \nmeans that testability is important. Heavy testing for the core is tractable \nbecause the core is typically small—often orders of magnitude smaller than \nthe periphery—highly controlled, and relatively slow to change. If the core \ncannot be made small, then its components can be made to be as indepen-\ndent of each other as possible, which eases the testing burden.\n■\n■The core must be highly robust with respect to errors in its environment. \nThe reliability of the periphery software is entirely in the hands of the pe-\nriphery community and the masses (end users and customers). The masses \nare typically recruited as testers (Mozilla claims to have three million), \nalthough this testing is often no more than clicking a button that signals a \nuser’s agreement to have bugs and quality information reported back to the \nproject. Given that the core will undoubtedly be supporting flawed periph-\nery components, robustness of the core is a key requirement; quite simply, \nfailures in the periphery must not cause failures of the core. This means that \na system employing the core/periphery pattern should create monitoring \nmechanisms to determine the current state of the system, and control mech-\nanisms so that bugs in the periphery cannot undermine the core.\nThe core (often called a platform) is usually implemented as a set of ser-\nvices; complex platforms have hundreds. The Amazon EC2 cloud, for exam-\nple, has over 110 different APIs documented, and EC2 is only a portion of the \n",
      "content_length": 2981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 554,
      "content": "27.4  Implications of the Metropolis Model\n533\nAmazon platform. To make these services available to peripheral developers, a \nnumber of conditions must hold: \n■\n■Documentation must be available for each API, it must be well written, \nit must be well organized, and it must be up to date. Because a peripheral \ndeveloper is frequently a volunteer, incomplete, out of date, or unclear doc-\numentation presents a barrier to entry. Even if there is a financial motivation \n(such as from developing an iPhone or Android application), the documen-\ntation still must not present a barrier.\n■\n■There must be a discovery service. Having hundreds of services means \nthat some of them are going to be redundant and others are going to be \nunavailable. A discovery service becomes a necessity to enable navigation \nand flexibility in such a world. A discovery service, in turn, implies a regis-\ntration service. Services must proactively register upon initialization and be \nremoved if they are no longer active.\n■\n■Error detection becomes extremely complicated. If you as a peripheral de-\nveloper encounter a bug in a service, it may be a bug in the service you are \ninvoking, it may be a bug in a service invoked by the service you are invok-\ning, or anywhere in the chain of services. Reporting a problem and getting \nit resolved may end up being extremely time-consuming. Quality assurance \nof services requires constant testing of their availability and correctness. \nThe Netflix Simian Army we discussed in Chapter 10 is an example of how \nquality assurance on a platform might be structured.\n■\n■All of the peer services might be potential denial-of-service attackers. \nThrottling, monitoring, and quotas must be employed to ensure that service \nrequesters receive adequate responses to their requests.\nBuilding a platform to be a core to support peripheral developers is a non-\ntrivial undertaking. Yet having such a core has paid dramatic dividends for com-\npanies comprising the Who’s Who of today’s web.\n27.4  Implications of the Metropolis Model\nThe Metropolis model, as we’ve seen, is paired with the core/periphery pattern \nfor architecture for edge-dominant systems. Adopting this duo brings with it \nchanges to the way that software is developed; in effect, it implies a software de-\nvelopment model, with its implications on tools, processes, activities, roles, and \nexpectations. Many such models have evolved over the years, each with its own \ncharacteristics, strengths, and weaknesses. Clearly, no one model is best for all \nprojects. For instance, Agile methods are best in projects with rapidly evolving \nrequirements and short time-to-market constraints, whereas a waterfall model is \n",
      "content_length": 2696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 555,
      "content": "534 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nbest for large projects with well-understood and stable requirements and complex \norganizational structures. \nThe Metropolis model requires a new perspective on system development, \nresulting in several important changes to how we must create systems: \n1.\t\nIndifference to phases. The Metropolis model uses the metaphor of a bull’s \neye, as opposed to a waterfall, a spiral, a “V,” or other representations that \nprevious models have adopted. The contrast to these previous models is \nsalient: the “phases” of development disappear in the bull’s eye. Instead, we \nmust focus managerial attention on the explicit inclusion of customers (the \nperiphery and the masses) for system development. \n2.\t\nCrowd management. Policies for crowd management must be aligned with \nthe organization’s strategic goals and must be established early. Crowds \nare good for certain tasks, but not for all. This implies that business models \nmust be examined to consider policies and associated system development \ntasks for crowd engagement, performance management monitoring, com-\nmunity protection, and so on. As crowdsourcing is rooted in the “gift” cul-\nture, for-profit organizations must carefully align tasks with the volunteers’ \nvalues and intentions.\n3.\t\nCore versus periphery. The Metropolis model differentiates the core and \nperiphery communities, with different tools, processes, activities, roles, and \nexpectations for each. The core must be small and tightly controlled by a \ngroup who focus on modularity, core services, and core quality attributes; \nthis enables the unbridled and uncoordinated growth at the periphery.\n4.\t\nRequirements process. The requirements for a Metropolis system are pri-\nmarily asserted by the periphery, not elicited from the masses; they emerge \nfrom the collective experiences of the community of the periphery, typically \nthrough their emails, wikis, and discussion forums. So such forums must \nbe made available—typically provided by members of the core—and the \nperiphery should be encouraged to participate in discussions about the re-\nquirements, in effect, to create a community. This changes the fundamental \nnature of requirements engineering, which has traditionally focused on col-\nlecting requirements, making them complete and consistent, and removing \nredundancies wherever possible.\n5.\t\nFocus on architecture. The core architecture is the fabric that holds to-\ngether a Metropolis system. As such, it must be consciously designed to \naccommodate the specific characteristics of open content and open source \nsystems. For this reason, the architecture cannot “emerge,” as it often does \nin traditional life-cycle models, and in Agile models. It must be designed \nup front, built by a small, experienced, motivated team who focus on (1) \nmodularity, to enable the parallel activities of the periphery, and (2) the \ncore quality attributes (security, performance, availability, and so on). There \nshould be a lead architect, or a small team of leads, who can manage proj-\nect coordination and who have the final say in matters affecting the core. \n",
      "content_length": 3148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 556,
      "content": "27.4  Implications of the Metropolis Model\n535\nLinus Torvalds, for example, still exerts “veto” rights on matters affecting \nLinux’s kernel. Virtually every open source project distinguishes between \nthe roles of contributor (who can contribute patches) and committer (who \nchooses which patches make it into any given release). \n6.\t\nDistributed testing. The core/periphery distinction also provides guidance \nfor testing activities. The core must be heavily tested and validated, because \nit is the fabric that holds the system together. Thus, when planning a Me-\ntropolis project, it is important to focus on validation of the core and to put \ntools, guidelines, and processes in place to facilitate this. For example, the \ncore should be kept small; the project should have frequent (perhaps night-\nly) builds and frequent releases; bug reporting should be built in to the sys-\ntem and require little effort on the part of the periphery. The project must \nexplicitly take advantage of the “many eyes” provided by the periphery. \n7.\t\nAutomated delivery. Delivery mechanisms must be created that work in \na distributed, asynchronous manner. These mechanisms must be flexible \nenough to accept incompleteness of the installed base as the norm. Thus, \nany delivery mechanism must be tolerant of older versions, multiple coex-\nisting versions, or even incomplete versions. A Metropolis system should \nalso, as far as possible, be tolerant of incompatibilities both within the \nsystem and between systems. For example, modern web browsers will still \nparse old versions of HTML or interact with old versions of web servers; \nbrowser add-ons and plug-ins coexist at different version levels and no \ncombination of them will “break” the browser. \n8.\t\nManagement of the periphery. One important aspect of the core/periphery \nmodel is that the core exercises very little control over the periphery. Yet \nthis does not mean that the periphery is totally unmanaged. If we examine \nthe extant platforms that are either crowdsourced or peripheral developer \nsourced, we see that there is always a governance policy set by a managing \norganization. The Internet and the World Wide Web have a collection of \ngoverning boards, large open source projects and Wikipedia are managed \nby foundations and meritocracies, and private companies such as Facebook \nor Apple have their own management structures. The governance policies \ncreated by the management organizations are enforced in either a proactive \nor reactive fashion. Some policies are enforced by a combination of both:\n■\n■Proactive enforcement. Proactive enforcement inhibits contributions by \nthe prosumers or the peripheral developers unless they meet certain cri-\nteria. Within the Internet, for example, IP addresses are assigned. One \ncannot make up one’s own IP address. Communication protocols and \nweb standards are defined by groups chartered by one of the Internet or \nweb governing organizations. Apple, as another example, screens appli-\ncations before they are eligible for inclusion in the App Store. And every \nplatform has a collection of APIs that also constrain and govern how a \nperipheral application interacts with it.\n",
      "content_length": 3184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 557,
      "content": "536 \nPart Five  Brave New World\t\n27—Architectures for the Edge\n■\n■Reactive enforcement. Reactive enforcement dictates the response in case \nthere is a violation of the organization’s policy. Wikipedia has a collection \nof editors who are responsible for ensuring the quality of contributions \nafter they have been made. Facebook, YouTube, Flickr and most other \ncrowdsourced sites have procedures to report violations. And if a periph-\neral developer does not adhere to a protocol or a set of APIs, then their \nproduct is flawed in some fashion and the market will likely punish them.\nThe analogy of a city to explain some of the facets of the core/periphery \nmodel can be extended. Zoning is a policy that describes permissible land use \nfor a city or other governmental organization. It specifies, for example, that cer-\ntain pieces of land are for residential use and other pieces are for industrial use. \nZoning policies have both proactive and reactive enforcement. Figure 27.2 shows \nsome of the actors associated with a zoning board. The zoning board is the gov-\nernance organization; it produces a building code that prescribes legitimate uses \nand restrictions on various buildings. The building inspector is a reactive enforcer \nwho is responsible for verifying that the buildings conform to permissible stan-\ndards and usage. As with any analogy, the zoning board is not an exact descrip-\ntion of the core/periphery, but it does identify many of the elements that go into \ncontrolling contributions.\nBuilding \ninspector \nBuilding code \nMember \nserves on \nAppointing authority \nDeveloper \nCitizen or \ncitizen \ngroups \nproduces \nselects \nZoning Board\nconstrains \nRequests \nvariance  \nor zoning \nchange \nEnforces building code \nRequests \nvariance or \nzoning change \nGovernment \nagency\nprovides\nfunds \nApproves \nor rejects \nchange \nrequest \nExpert \nadvises \nFigure 27.2  Zoning board stakeholders\n",
      "content_length": 1904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 558,
      "content": "27.5  Summary\n537\nLife-cycle models are never revolutionary; they arise in reaction to ambient \nconditions in the software development world. The Waterfall model was created \nto focus more attention on removing flaws early in a project’s life cycle, in reac-\ntion to the delays, bugs, and failures of projects of ever-increasing complexity. \nThe spiral model and, later, the Rational Unified Process were created because \nprojects needed to produce working versions of software more quickly and to \nmitigate risks earlier in the software development life cycle. Agile methods grew \nout of the desire for less bureaucracy, more responsiveness to customer needs, \nand shorter times to market.\nSimilarly, the Metropolis model is formally capturing a market response \nthat is already occurring: the rise of commons-based peer production and ser-\nvice-dominant logic. Prior life-cycle models are simply inadequate—mostly \nmute—on the concerns of edge-dominant systems: crowdsourcing, emergent re-\nquirements, and change as a constant. This model offers new ways to think about \nhow a new breed of systems can be developed; its principles help management \nshift to new project management styles and architecture models that take advan-\ntage of the “wisdom of crowds.” \nMetropolis model concepts are not appropriate for all forms of development. \nSmaller systems with limited scope will continue to benefit from the conceptual \nintegrity that accompanies a small, cohesive team. High-security and safety-criti-\ncal systems, and systems that are built around protected intellectual property, will \ncontinue to be built in traditional ways for the foreseeable future. But more and \nmore crowdsourcing, mashups, open source, and other forms of edge-dominant \ndevelopment are being harnessed for value cocreation, and the Metropolis model \ndoes speak to this.\n27.5  Summary\nAn edge-dominant system is one that depends crucially on the inputs of users for \nits success. Users participate in information creation (“crowdsourcing”) and even \nits organization (“folksonomy”). These systems, part of the “Web 2.0” move-\nment, are having profound social, political, and economic consequences.\nAll successful edge-dominant systems, and the organizations that develop \nand use these systems, share a common ecosystem structure known as the “Me-\ntropolis” structure. This structure shows how customers, end users, developers, \nand “prosumers” are related.\nEdge systems bring a new life-cycle model to the fore, in which require-\nments are not completely known, software developed in planned increments is re-\nplaced by software that is constantly changing, and projects are staffed by mem-\nbers outside the purview of the central developing organization.\n",
      "content_length": 2735,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 559,
      "content": "538 \nPart Five  Brave New World\t\n27—Architectures for the Edge\nThe dominant pattern for edge systems is the core/periphery pattern. This \npattern divides the world into a closely controlled core and a loosely controlled \nperiphery. To work, the core needs to be highly modular, highly reliable, and \nhighly robust with respect to external faults. Cores are often designed as a set \nof services with well-documented APIs, discovery and registration, and sophisti-\ncated error detection and reporting.\n27.6  For Further Reading\nAn interesting interview of Linus Torvalds, showing his management style—\nwhat he calls “shepherding”—appeared in BusinessWeek magazine several years \nago [Hamm 04].\nYochai Benkler’s intriguing book The Wealth of Networks [Benkler 07] puts \nforth a powerful premise: that the networked information economy is transform-\ning society. It shows how the modern networked economy transforms methods of \nproduction and consumption, and creates new forms of value that do not depend \non market strategies.\nFor more information and background on the Metropolis model, you can \nread the original paper describing it [Kazman 09].\nMuch of the inspiration for the Metropolis model comes from the Ultra-\nLarge-Scale Systems report [Feiler 06].\nMacCormack and colleagues have written extensively on the architecture \nand properties of what they call “core/periphery” systems. See, for example, \n[MacCormack 10].\n27.7  Discussion Questions\n1.\t\nDraw the architecture influence cycle for Web 2.0 software systems in gen-\neral, and for one of its flagship examples (such as Twitter or Facebook).\n2.\t\nCreate a complete pattern description for the core/periphery pattern, mod-\neled on those in Chapter 13.\n3.\t\nHow might the role of architecture documentation be different for an \nedge-dominant system?\n4.\t\nWhich architectural views would you expect to be the most important to \ndocument for a system built under the Metropolis model?\n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 560,
      "content": "27.7  Discussion Questions\n539\n5.\t\nHow would you establish the architecturally significant requirements for \nthe core? Would you use a Quality Attribute Workshop, or would you use \nsomething less structured and more open? Why?\n6.\t\nMetropolis systems are frequently open source. Some organizations that \nmight want to contribute to or build on top of such a system may balk at \nreleasing all of their code to the public. What architectural means might \nyou employ to address this situation?\n7.\t\nChoose your favorite crowdsourced system. Write a testability scenario for \nthis system, and choose a set of testability tactics that you would use for it.\n8.\t\nConstructing and releasing an application on a platform such as the iPhone \nor the Android requires the developer to adhere to certain specifications and \nto pass through certain hoops. Redraw Figure 27.2 to reflect the Apple iP-\nhone ecosystem and the Android ecosystem.\n9.\t\nFind a study that discusses the motivation of Wikipedia contributors. Find \nanother study that discusses the motivation of open source developers. \nCompare the results of these two studies.\n",
      "content_length": 1120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 561,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 562,
      "content": "541\n28\nEpilogue\nDon’t let it end like this. Tell them I said something.\n—Pancho Villa\nYou’ve arrived at the end of the journey. Nice work.\nWe hope you can find some valuable takeaways from this book. We suggest \nthe list should include the following:\n1.\t\nWhat architecture is, why it’s important, what influences it, and what it \ninfluences. \n2.\t\nThe role that architecture plays in a business, technical, project, and profes-\nsional context. \n3.\t\nThe critically important symbiosis between architecture and quality attri-\nbutes, how to specify quality attribute requirements, and the quick immer-\nsion you’ve had into the dozen or so of the most important QAs. \n4.\t\nHow to capture the architecturally significant requirements for an \narchitecture.\n5.\t\nHow to design an architecture, document it, guide an implementation with \nit, evaluate it to see if it’s a good one for your needs, reverse-engineer it, \nand manage a project around it. \n6.\t\nHow to evaluate an architecture’s cost and benefit, what it means to be \narchitecturally competent, and how to use architecture as the basis for an \nentire software product line. \n7.\t\nArchitectural concepts and patterns for systems on the current technological \nfrontier: edge applications and the cloud.\nFine. Now what?\nIt’s very tempting to end the book with a cheery imperative to go forth and \narchitect great systems. But the truth is, life isn’t that simple. \nThe authors of this book have, collectively, an embarrassingly large num-\nber of years of experience in teaching software architecture. We teach it through \nbooks like this one, in the classroom to students, and in industrial short courses \n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 563,
      "content": "542 \nPart Five  Brave New World\t\n28—Epilogue\nto practicing software architects of all stripes, from “aspiring” to “old hand.” \nAnd often, much too often, we know that after our students conscientiously learn \nwhat we have to offer, they walk into an organization that is not as architecturally \nsavvy as the students now are, and our students have no practical way to put what \nthey have learned into practice. \nMost of you will not be able to dictate an architecture-based philosophy to \nthe projects to which you’ll be assigned, if it’s not already present. You won’t \nbe able to say, “This Agile project needs a lead software architect!” and make it \nstick if the team leaders think that Agile methods won’t permit any overarching \ndesign. You won’t be able to say, “We’re going to include an explicit architecture \nevaluation in our project schedule” and have everyone comply. You won’t be able \nto say, “We’re going to use the Views and Beyond approach and templates for our \narchitecture documentation” and have your directive obeyed.\nLest you feel that all your time absorbing the material in this book was time \nspent for a lost cause, we want to close the book with some advice for carrying \nwhat you’ve learned into your professional setting: \n1.\t\nSpeak the right language. You know by now that architecture is the means \nto an end, and not an end in itself. The decision makers in your organization \ntypically care about the ends, not the means. They care about products, not \nthe architectures of those products. They care about ensuring that the prod-\nucts are competitive in the marketplace. And they care about executing the \norganization’s marketing and business strategy. They may not express their \nconcerns in architecturally significant terms, but rather in market terms that \nyou’ll have to translate. \n2.\t\nSpeak the right language, part 2. Project managers care about reduction \nof technical risk, reliable and realistic scheduling and budgeting, and plan-\nning the production of those products. To the extent that you can justify \na focus on architecture in those terms, you’ll more likely be successful in \ngaining the freedom to carry out some of the practices espoused in this \nbook. And you really can justify this focus: A sound architecture is an \nunparalleled risk reduction strategy, a reliable work estimator, and a good \npredictor of production methods. \n3.\t\nGet involved. One of the best ways to insert architectural concerns into a \nproject is to show its value to stakeholders who don’t often get a chance to \nsee it. Requirements engineers are a case in point. Most often, they meet \nwith customers and users, capture their concerns, write them up, and toss \nthe requirements over the fence (usually a very tall fence) to the designers. \nChallenge this segregation! Try to get involved in the requirements capture \nactivity. Invite yourself to meetings. Say that, as an architect, you want to \nunderstand the problem by hearing the concerns straight from the horse’s \nmouth. This will give you critical exposure to the very stakeholders you’ll \nneed to help with your design, evaluation, and documentation chores. Fur-\nthermore it will give you a chance to add value to the requirements capture \n",
      "content_length": 3233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 564,
      "content": "Epilogue﻿\n543\nprocess. Because you may have a design approach in mind, you may be \nable to offer better quality attribute responses than the customer has in \nmind, and that might make our marketers very happy. Or you may be able \nto spot troublesome requirements early on, and help nudge the customer to \na perfectly adequate but more architecturally palatable QA response. Also, \nyou can take it upon yourself to contact your organization’s marketers. \nThey are often the ones who come up with product concepts. You would \ndo well to learn how they do that, and eventually you could help them by \npointing out useful variations on existing products that could use the same \narchitecture.\n4.\t\nIt’s the economy, stupid. Think in, and couch your arguments in, eco-\nnomic terms. If you think an architectural trade study, or an architecture \ndocument, or an architecture evaluation, or ensuring code compliance with \nthe architecture is a good idea for your project, make a business case for \nit. Pointy-haired bosses in comic strips notwithstanding, managers are re-\nally—trust us here—rational people. But their goals are broad and almost \nalways have to do with economics. You should be able to argue, using \nback-of-the-envelope arithmetic, that (for example) producing an updated \nversion of the architecture document is a worthwhile activity when the \nsystem undergoes a major change. You should be able to argue that activi-\nties undertaken with the new architecture documentation will be much less \nerror-prone (and therefore less expensive) than those same activities under-\ntaken without a guiding architecture. And the effort to keep the documenta-\ntion up to date is much less than the expected savings. You can plug some \nnumbers in a spreadsheet to make this argument. The numbers don’t have \nto be accurate, just reasonable, and they’ll still make your case.\n5.\t\nStart a guerrilla movement. Find like-minded people in your organiza-\ntion and nurture their interest in architecture. Start a brown-bag lunchtime \nreading and discussion group that covers books or book chapters or papers \nor even blogs about architecture. For example, your group could read the \nchapter in this book about architecture competence, and discuss what prac-\ntices you’d like to see your organization adopt, and what it would take to \nadopt them. Or the group could agree on a joint documentation template \nfor architecture, or come up with a set of quality attribute scenarios that \napply across your collective projects. Especially appealing is to come up \nwith a set of patterns that apply to the systems you’re building. Or bring a \nvexing design problem from your individual project, and let the group work \non a written solution to it. Or have the group offer its services as a roving \narchitecture evaluation team to other projects. Your group should meet reg-\nularly and often, and adopt a specific set of tangible goals. The importance \nof an enthusiastic and dedicated leader—you?—who is keen to mature the \norganization’s architecture practices cannot be overstated. Advertise your \n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 565,
      "content": "544 \nPart Five  Brave New World\t\n28—Epilogue\nmeetings, advertise your results, and keep asking more members to join \nyour group.\n6.\t\nRelish small victories. You don’t have to change the world overnight. Ev-\nery improvement you make will put you and your organization in a better \nplace than it would have been otherwise. \nGetting Architecture Reviews into an Organization through \nthe Back Door\nIf you search the web for “code review computer science,” you’ll turn up \nmillions of hits that describe code reviews and the steps that are taken to \nperform them. If you search for “design review computer science,” you’ll \nturn up little that is useful. \nOther disciplines routinely practice and teach design critiques. Search \nfor “design critique” and you will find many hits together with instructions. A \ndesign is a set of decisions of whatever type that attempts to solve a par-\nticular problem, whether an art problem, a user interface design problem, \nor a software problem. Solutions to important design problems should be \nsubject to peer review, just as code should be subject to peer review.\nThere is a wealth of data that points out that the earlier in the life cycle a \nproblem is discovered and fixed, the less the cost of finding and fixing the \nproblem. Design precedes code and so having appropriate design reviews \nseems both intuitively and empirically justified. In addition, the documents \naround the review, both the original design document and also the cri-\ntiques, are valuable learning tools for new developers. In many organiza-\ntions developers switch systems frequently, and so they are constantly \nlearning.\nThis view is not universally shared. A software engineer working in a \nmajor software house tells me that even though the organization aspires to \nwriting and reviewing design documents, it rarely happens. Senior develop-\ners tend to limit their review to a cursory glance. Code reviews, on the other \nhand, are taken quite seriously by the senior developers.\n My software engineer friend offers two possible explanations for this \nstate of affairs:\n1.\t The code review is the last opportunity to affect what is built: \n“review this or live with it.” This explanation assumes that senior \ndevelopers do not believe that the output of design reviews are \nactionable and thus wait to engage until later in the process. \n2.\t The code is more concrete than the design, and is therefore \neasier to assess. This explanation assumes that senior developers \nare incapable of understanding designs. \nI do not find either of these explanations compelling, but I am unable to \ncome up with a better one.\n",
      "content_length": 2629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 566,
      "content": "﻿\n545\nWhat to do?\nWhat this software engineer did is to look for a surrogate process where \na design review could be surreptitiously performed. This individual noticed \nthat when the organization did code reviews, questions such as “Why did \nyou do that?” were frequently asked. The result of such questions was a \ndiscussion of rationale. So the individual would code up a solution to a \nproblem, submit it to a code review, and wait for the question that would \nlead to the rationale discussion.\nA design review is a review where design decisions are presented \ntogether with their rationale. Frequently, design alternatives are explored. \nWhether this is done under the name of code review or design review is not \nnearly as important as getting it done. \nOf course, my friend’s surreptitious approach has drawbacks. It is ineffi-\ncient to code a solution that may have to be thrown away. Also, embedding \ndesign reviews into code reviews means that the designs and reviews end \nup being embedded in the code review tool, making it difficult to search this \ntool for design and design rationale. But these inefficiencies are dwarfed by \nthe inefficiency of pursuing an incorrect solution to a particular problem.\n—LB\nThe practice and discipline of architecture for software systems has come of \nage. You can be proud of joining a profession that has always strived, and is still \nstriving, to be more disciplined, more reliable, more productive, and more effi-\ncient, to produce systems that improve the lives of their stakeholders.\nWith that thought, now it’s time for the cheery book-ending imperative: Go \nforth and architect great systems. Your predecessors have designed systems that \nhave changed the world. It’s your turn.\n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 567,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 568,
      "content": "547\nReferences\n[Abrahamsson 10] P. Abrahamsson, M.A. Babar, and P. Kruchten. “Agility and Ar-\nchitecture: Can They Coexist?” IEEE Software, Vol. 27, No. 2, (March-April \n2010), pp. 16-22.\n[AdvBuilder 10] Java Adventure Builder Reference Application. https://java.net/\nprojects/adventurebuilder/pages/home\n[Anastasopoulos 00] M. Anastasopoulos and C. Gacek. “Implementing Product Line \nVariabilities” (IESE-Report No. 089.00/E, V1.0). Kaiserslautern, Germany: \nFraunhofer Institut Experimentelles Software Engineering, 2000.\n[Anderson 08] Ross Anderson. Security Engineering: A Guide to Building  \nDependable Distributed Systems, Second Edition. Wiley, 2008.\n[Argote 07]. L. Argote and G. Todorova. International Review of Industrial and \nOrganizational Psychology. John Wiley & Sons, Ltd., 2007. \n[Avižienis 04] Algirdas Avižienis, Jean-Claude Laprie, Brian Randell, and Carl \nLandwehr. “Basic Concepts and Taxonomy of Dependable and Secure Comput-\ning,” IEEE Transactions on Dependable and Secure Computing, Vol. 1, No. 1 \n(January 2004), pp. 11-33.\n[Bachmann 05] F. Bachmann and P. Clements. “Variability in Software Product \nLines,” CMU/SEI-2005-TR-012, 2005.\n[Bachmann 07] Felix Bachmann, Len Bass, and Robert Nord. “Modifiability  \nTactics,” CMU/SEI-2007-TR-002, September 2007.\n[Bachmann 11] F. Bachmann. “Give the Stakeholders What They Want: Design Peer \nReviews the ATAM Style,” Crosstalk, November/December 2011, pp. 8-10, \nhttp://www.crosstalkonline.org/storage/issue- \narchives/2011/201111/201111-Bachmann.pdf\n[Barbacci 03] M. Barbacci, R. Ellison, A. Lattanze, J. Stafford, C. Weinstock, and \nW. Wood. “Quality Attribute Workshops (QAWs), Third Edition,” CMU/SEI-\n2003-TR-016, http://www.sei.cmu.edu/reports/03tr016.pdf\n[Bass 03] L. Bass and B.E. John. “Linking Usability to Software Architecture  \nPatterns through General Scenarios,” Journal of Systems and Software 66(3), \npp. 187-197.\n[Bass 08] Len Bass, Paul Clements, Rick Kazman, and Mark Klein. “Models for \nEvaluating and Improving Architecture Competence,” CMU/SEI-2008-TR-006, \nMarch 2008, http://www.sei.cmu.edu/library/abstracts/reports/08tr006.cfm\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 569,
      "content": "548 \nReferences\t\n[Baudry 03] B. Baudry, Yves Le Traon, Gerson Sunyé, and Jean-Marc Jézéquel. \n“Measuring and Improving Design Patterns Testability,” Proceedings of the \nNinth International Software Metrics Symposium (METRICS ’03), 2003.\n[Baudry 05] B. Baudry and Y. Le Traon. “Measuring Design Testability of a UML \nClass Diagram,” Information & Software Technology 47(13)(October 2005), \npp. 859-879.\n[Beck 04] Kent Beck and Cynthia Andres. Extreme Programming Explained: Em-\nbrace Change, Second Edition. Addison-Wesley, 2004.\n[Beizer 90] B. Beizer. Software Testing Techniques, Second Edition. International \nThomson Computer Press, 1990.\n[Bellcore 98] Bell Communications Research. GR-1230-CORE, SONET Bidirec-\ntional Line-Switched Ring Equipment Generic Criteria. 1998.\n[Bellcore 99] Bell Communications Research. GR-1400-CORE, SONET Dual-Fed \nUnidirectional Path Switched Ring (UPSR) Equipment Generic Criteria. 1999.\n[Benkler 07] Y. Benkler. The Wealth of Networks: How Social Production Trans-\nforms Markets and Freedom. Yale University Press, 2007.\n[Bertolino 96a] Antonia Bertolino and Lorenzo Strigini. “On the Use of Testability \nMeasures for Dependability Assessment,” IEEE Transactions on Software \nEngineering, Vol. 22, No. 2 (February 1996), pp. 97-108.\n[Bertolino 96b] A. Bertolino and P. Inverardi. “Architecture-Based Software Test-\ning,” in Proceedings of the Second International Software Architecture \nWorkshop (ISAW-2), L. Vidal, A. Finkelstain, G. Spanoudakis, and A.L. Wolf, \neds. Joint Proceedings of the SIGSOFT ’96 Workshops, San Francisco, October \n1996, ACM Press.\n[Biffl 10] S. Biffl, A. Aurum, B. Boehm, H. Erdogmus, and P. Grunbacher, eds.  \nValue-Based Software Engineering. Springer, 2010.\n[Binder 94] R.V. Binder. “Design for Testability in Object-Oriented Systems,” \nCACM 37(9), pp. 87-101, 1994.\n[Boehm 78] B.W. Boehm, J.R. Brown, J.R. Kaspar, M.L. Lipow, and G. MacCleod. \nCharacteristics of Software Quality. American Elsevier, 1978. \n[Boehm 81] B. Boehm. Software Engineering Economics. Prentice-Hall, 1981.\n[Boehm 91] Barry Boehm. “Software Risk Management: Principles and Practices,” \nIEEE Software, Vol. 8, No. 1, pp. 32-41, January 1991.\n[Boehm 04] B. Boehm and R. Turner. Balancing Agility and Discipline: A Guide \nfor the Perplexed. Addison-Wesley, 2004.\n[Boehm 07] B. Boehm, R. Valerdi, and E. Honour. “The ROI of Systems Engineer-\ning: Some Quantitative Results for Software Intensive Systems,” Systems Engi-\nneering, Vol. 11, No. 3, pp. 221-234. \n[Boehm 10] B. Boehm, J. Lane, S. Koolmanojwong, and R. Turner. “Architected \nAgile Solutions for Software-Reliant Systems,” Technical Report USC-\nCSSE-2010-516, 2010.\n[Booch 11] Grady Booch. “An Architectural Oxymoron,” podcast available at http://\nwww.computer.org/portal/web/computingnow/onarchitecture. Retrieved January \n21, 2011.\n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 570,
      "content": "References\n549\n[Bosch 00] J. Bosch. “Organizing for Software Product Lines,” Proceedings of the \n3rd International Workshop on Software Architectures for Product Families \n(IWSAPF-3), pp. 117-134. Las Palmas de Gran Canaria, Spain, March 15-17, \n2000. Springer, 2000.\n[Bouwers 10] E. Bouwers and A. van Deursen. “A Lightweight Sanity Check for Im-\nplemented Architectures,” IEEE Software 27(4), July/August 2010, pp. 44-50.\n[Bredemeyer 11] D. Bredemeyer and R. Malan. “Architect Competencies: What You \nKnow, What You Do and What You Are,” http://www.bredemeyer.com/ \nArchitect/ArchitectSkillsLinks.htm\n[Brewer 12] E. Brewer. “CAP Twelve Years Later: How the ‘Rules’ Have Changed,” \nIEEE Computer, February 2012, pp. 23-29.\n[Brown 10] N. Brown, R. Nord, and I. Ozkaya. “Enabling Agility Through Architec-\nture,” Crosstalk, November/December 2010, pp. 12-17.\n[Brownsword 96] Lisa Brownsword and Paul Clements. “A Case Study in Successful \nProduct Line Development,” Technical Report CMU/SEI-96-TR-016, October \n1996.\n[Brownsword 04] Lisa Brownsword, David Carney, David Fisher, Grace Lewis, \nCraig Meterys, Edwin Morris, Patrick Place, James Smith, and Lutz Wrage. \n“Current Perspectives on Interoperability,” CMU/SEI-2004-TR-009, http://\nwww.sei.cmu.edu/reports/04tr009.pdf\n[Bruntink 06] Magiel Bruntink and Arie van Deursen. “An Empirical Study into \nClass Testability,” Journal of Systems and Software 79(9)(2006),  \npp. 1219-1232.\n[Buschmann 96] Frank Buschmann, Regine Meunier, Hans Rohnert, Peter Sommer-\nlad, and Michael Stal. Pattern-Oriented Software Architecture Volume 1: A \nSystem of Patterns. Wiley, 1996.\n[Cai 11] Yuanfang Cai, Daniel Iannuzzi, and Sunny Wong. “Leveraging Design \nStructure Matrices in Software Design Education,” Conference on Software \nEngineering Education and Training 2011, pp. 179-188.\n[Cappelli 12] Dawn M. Cappelli, Andrew P. Moore, and Randall F. Trzeciak. The \nCERT Guide to Insider Threats: How to Prevent, Detect, and Respond to \nInformation Technology Crimes (Theft, Sabotage, Fraud). Addison-Wesley, \n2012.\n[Carriere 10] J. Carriere, R. Kazman, and I. Ozkaya. “A Cost-Benefit Framework for \nMaking Architectural Decisions in a Business Context,” Proceedings of 32nd \nInternational Conference on Software Engineering (ICSE 32), Capetown, \nSouth Africa, May 2010.\n[Cataldo 07] M. Cataldo, M. Bass, J. Herbsleb, and L. Bass. “On Coordination \nMechanisms in Global Software Development,” Proceedings Second IEEE \nInternational Conference on Global Software Development, 2007.\n[Chandran 10] S. Chandran, A. Dimov, and S. Punnekkat. “Modeling Uncertainties \nin the Estimation of Software Reliability—A Pragmatic Approach,” Fourth \nIEEE International Conference on Secure Software Integration and  \nReliability Improvement, 2010.\n",
      "content_length": 2767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 571,
      "content": "550 \nReferences\t\n[Chang 06] F. Chang, J. Dean, S. Ghemawat, W. Hsieh, et al. “Bigtable: A Distrib-\nuted Storage System for Structured Data,” Proceedings Operating Systems \nDesign and Implementation, 2006, http://research.google.com/archive/ \nbigtable.html\n[Chen 10] H.-M. Chen, R. Kazman, and O. Perry. “From Software Architecture \nAnalysis to Service Engineering: An Empirical Study of Enterprise SOA Imple-\nmentation,” IEEE Transactions on Services Computing 3(2)(April-June 2010), \npp. 145-160.\n[Chidamber 94] S. Chidamber and C. Kemerer. “A Metrics Suite for Object Oriented \nDesign,” IEEE Transactions on Software Engineering, Vol. 20, No. 6 (June \n1994).\n[Clements 01a] P. Clements and L. Northrop. Software Product Lines.  \nAddison-Wesley, 2001.\n[Clements 01b] P. Clements, R. Kazman, and M. Klein. Evaluating Software  \nArchitectures. Addison-Wesley, 2001.\n[Clements 07] P. Clements, R. Kazman, M. Klein, D. Devesh, S. Reddy, and P. \nVerma. “The Duties, Skills, and Knowledge of Software Architects,” Proceed-\nings of the Working IEEE/IFIP Conference on Software Architecture, 2007.\n[Clements 10a] Paul Clements, Felix Bachmann, Len Bass, David Garlan, James  \nIvers, Reed Little, Paulo Merson, Robert Nord, and Judith Stafford. Document-\ning Software Architectures: Views and Beyond, Second Edition.  \nAddison-Wesley, 2010.\n[Clements 10b] Paul Clements and Len Bass. “Relating Business Goals to Ar-\nchitecturally Significant Requirements for Software Systems,” CMU/SEI-\n2010-TN-018, May 2010.\n[Clements 10c] P. Clements and L. Bass. “The Business Goals Viewpoint,” IEEE \nSoftware 27(6)(November-December 2010), pp. 38-45.\n[Cockburn 04] Alistair Cockburn. Crystal Clear: A Human-Powered Methodology \nfor Small Teams. Addison-Wesley, 2004. \n[Conway 68] Melvin E. Conway. “How Do Committees Invent?” Datamation,  \nVol. 14, No. 4 (1968), pp. 28-31.\n[Coplein 10] J. Coplein and G. Bjornvig. Lean Architecture for Agile Software \nDevelopment. Wiley, 2010.\n[Cunningham 92] W. Cunningham. “The Wycash Portfolio Management System,” \nin Addendum to the Proceedings of Object-Oriented Programming Systems, \nLanguages, and Applications (OOPSLA), pp. 29-30, ACM Press, 1992.\n[CWE 12] The Common Weakness Enumeration. http://cwe.mitre.org/\n[Dean 04] Jeffrey Dean and Sanjay Ghemawat. “MapReduce: Simplified Data  \nProcessing on Large Clusters,” Proceedings Operating System Design and \nImplementation, 1994, http://research.google.com/archive/mapreduce.html\n[Dijkstra 68] E.W. Dijkstra. “The Structure of the ‘THE’-Multiprogramming \nSystem,” Communications of the ACM 11(5), pp. 341-346.\n[Dix 04] Alan Dix, Janet Finlay, Gregory Abowd, and Russell Beale. Human-\nComputer Interaction, Third Edition. Prentice Hall, 2004.\n",
      "content_length": 2715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 572,
      "content": "References\n551\n[Douglass 99] Bruce Douglass. Real-Time Design Patterns: Robust Scalable \nArchitecture for Real-Time Systems. Addison-Wesley, 1999.\n[Dutton 84] J.M. Dutton and A. Thomas. “Treating Progress Functions as a \nManagerial Opportunity,” Academy of Management Review 9 (1984),  \npp. 235-247.\n[Eickelman 96] N. Eickelman and D. Richardson. “What Makes One Software \nArchitecture More Testable Than Another?” in Proceedings of the Second \nInternational Software Architecture Workshop (ISAW-2), L. Vidal, A. \nFinkelstein, G. Spanoudakis, and A.L. Wolf, eds., Joint Proceedings of the \nSIGSOFT ’96 Workshops, San Francisco, October 1996, ACM Press.\n[EOSAN 07] “WP 8.1.4—Define Methodology for Validation within OATA: \nArchitecture Tactics Assessment Process,” http://www.eurocontrol.int/valfor/\ngallery/content/public/OATA-P2-D8.1.4-01%20DMVO%20Architecture%20\nTactics%20Assessment%20Process.pdf\n[FAA 00] “System Safety Handbook,” http://www.faa.gov/library/manuals/aviation/\nrisk_management/ss_handbook/\n[Fairbanks 10] G. Fairbanks. Just Enough Software Architecture. Marshall & \nBrainerd, 2010.\n[Feiler 06] P. Feiler, R.P. Gabriel, J. Goodenough, R. Linger, T. Longstaff, R. \nKazman, M. Klein, L. Northrop, D. Schmidt, K. Sullivan, and K. Wallnau. \nUltra-Large-Scale Systems: The Software Challenge of the Future, http://\nwww.sei.cmu.edu/library/assets/ULS_Book20062.pdf\n[Fiol 85] C.M. Fiol and M.A. Lyles. “Organizational Learning,” Academy of \nManagement Review 10(4)(1985), p. 803.\n[Freeman 09] Steve Freeman and Nat Pryce. Growing Object-Oriented Software, \nGuided by Tests. Addison-Wesley, 2009. \n[Gacek 95] Cristina Gacek, Ahmed Abd-Allah, Bradford Clark, and Barry Boehm. \n“On the Definition of Software System Architecture,” USC/CSE-95-TR-500, \nApril 1995.\n[Gagliardi 09] M. Gagliardi, W. Wood, J. Klein, and J. Morley. “A Uniform \nApproach for System of Systems Architecture Evaluation,” Crosstalk, Vol. 22, \nNo. 3 (March/April 2009), pp. 12-15.\n[Gamma 94] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: \nElements of Reusable Object-Oriented Software. Addison-Wesley, 1994.\n[Garlan 93] D. Garlan and M. Shaw. “An Introduction to Software Architecture,” in \nAmbriola and Tortola, eds., Advances in Software Engineering & Knowledge \nEngineering, Vol. II. World Scientific Pub. Co., 1993, pp. 1-39.\n[Garlan 95] David Garlan, Robert Allen, and John Ockerbloom. “Architectural \nMismatch or Why it’s hard to build systems out of existing parts,” ICSE 1995. \n17th International Conference on Software Engineering, April 1995. \n[Gilbert 07] T. Gilbert. Human Competence: Engineering Worthy Performance. \nPfeiffer, Tribute Edition, 2007.\n[Gokhale 05] S. Gokhale, J. Crigler, W. Farr, and D. Wallace. “System Availability \nAnalysis Considering Hardware/Software Failure Severities,” Proceedings of \nthe 29th Annual IEEE/NASA Software Engineering Workshop (SEW ’05), \nGreenbelt, MD, April 2005, IEEE 2005.\n",
      "content_length": 2929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 573,
      "content": "552 \nReferences\t\n[Gorton 10] Ian Gorton. Essential Software Architecture, Second Edition. \nSpringer, 2010.\n[Graham 07] T.C.N. Graham, R. Kazman, and C. Walmsley. “Agility and Experimen-\ntation: Practical Techniques for Resolving Architectural Tradeoffs,” Proceed-\nings of the 29th International Conference on Software Engineering (ICSE \n29), Minneapolis, MN, May 2007.\n[Gray 93] Jim Gray and Andreas Reuter. Distributed Transaction Processing: Con-\ncepts and Techniques. Morgan Kaufmann, 1993.\n[Grinter 99] Rebecca E. Grinter. “Systems Architecture: Product Designing and \nSocial Engineering,” in Proceedings of the International Joint Conference \non Work Activities Coordination and Collaboration (WACC ’99), Dimitrios \nGeorgakopoulos, Wolfgang Prinz, and Alexander L. Wolf, eds. ACM, 1999,  \npp. 11-18.\n[Hamm 04] “Linus Torvalds’ Benevolent Dictatorship,” BusinessWeek, Au-\ngust 18, 2004, http://www.businessweek.com/technology/content/aug2004/\ntc20040818_1593.htm\n[Hamming 80] R.W. Hamming. Coding and Information Theory. Prentice Hall, \n1980.\n[Hanmer 07] Robert Hanmer. Patterns for Fault Tolerant Software, Wiley, 2007.\n[Harms 10] R. Harms and M. Yamartino. “The Economics of the Cloud,” http:// \neconomics.uchicago.edu/pdf/Harms_110111.pdf\n[Hartman 10] Gregory Hartman. “Attentiveness: Reactivity at Scale,” CMU-\nISR-10-111, 2010.\n[Hiltzik 00] M. Hiltzik. Dealers of Lightning: Xerox PARC and the Dawn of the \nComputer Age. Harper Business, 2000.\n[Hoffman 00] Daniel M. Hoffman and David M. Weiss. Software Fundamentals: \nCollected Papers by David L. Parnas. Addison-Wesley, 2000.\n[Hofmeister 00] Christine Hofmeister, Robert Nord, and Dilip Soni. Applied Soft-\nware Architecture. Addison-Wesley, 2000.\n[Hofmeister 07] Christine Hofmeister, Philippe Kruchten, Robert L. Nord, Henk \nObbink, Alexander Ran, and Pierre America. “A General Model of Software \nArchitecture Design Derived from Five Industrial Approaches,” Journal of Sys-\ntems and Software, Vol. 80, No. 1 (January 2007), pp. 106-126.\n[Howard 04] Michael Howard. “Mitigate Security Risks by Minimizing the Code \nYou Expose to Untrusted Users,” MSDN Magazine, http://msdn.microsoft.com/\nen-us/magazine/cc163882.aspx\n[IEEE 94] “IEEE Standard for Software Safety Plans,” STD-1228-1994, http:// \nstandards.ieee.org/findstds/standard/1228-1994.html\n[IEEE 11] “IEEE Guide—Adoption of the Project Management Institute (PMI) \nStandard: A Guide to the Project Management Body of Knowledge (PMBOK \nGuide), Fourth Edition,” http://www.projectsmart.co.uk/pmbok.html\n[IETF 04] Internet Engineering Task Force. “RFC 3746, Forwarding and Control \nElement Separation (ForCES) Framework,” 2004. \n[IETF 05] Internet Engineering Task Force. “RFC 4090, Fast Reroute Extensions to \nRSVP-TE for LSP Tunnels,” 2005.\n",
      "content_length": 2760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 574,
      "content": "References\n553\n[IETF 06a] Internet Engineering Task Force. “RFC 4443, Internet Control Message \nProtocol (ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,” \n2006.\n[IETF 06b] Internet Engineering Task Force. “RFC 4379, Detecting Multi-Protocol \nLabel Switched (MPLS) Data Plane Failures,” 2006. \n[INCOSE 05] International Council on Systems Engineering. “System Engineering \nCompetency Framework 2010-0205,” http://www.incose.org/ProductsPubs/\nproducts/competenciesframework.aspx\n[ISO 11] International Organization for Standardization. “ISO/IEC 25010: 2011 Sys-\ntems and software engineering—Systems and software Quality Requirements \nand Evaluation (SQuaRE)—System and software quality models.” \n[Jacobson 97] I. Jacobson, M. Griss, and P. Jonsson. Software Reuse: Architecture, \nProcess, and Organization for Business Success. Addison-Wesley, 1997.\n[Kanwal 10] F. Kanwal, K. Junaid, and M.A. Fahiem. “A Hybrid Software Archi-\ntecture Evaluation Method for FDD—An Agile Process Mode,” 2010 Interna-\ntional Conference on Computational Intelligence and Software Engineering \n(CiSE), December 2010, pp. 1-5.\n[Kaplan 92] R. Kaplan and D. Norton. “The Balanced Scorecard: Measures That \nDrive Performance,” Harvard Business Review, January/February 1992,  \npp. 71-79.\n[Karat 94] Claire Marie Karat. “A Business Case Approach to Usability Cost Justi-\nfication,” in Cost-Justifying Usability, R. Bias and D. Mayhew, eds. Academic \nPress, 1994.\n[Kazman 94] Rick Kazman, Len Bass, Mike Webb, and Gregory Abowd. “SAAM: \nA Method for Analyzing the Properties of Software Architectures,” in Proceed-\nings of the 16th International Conference on Software Engineering (ICSE \n’94). Los Alamitos, CA. IEEE Computer Society Press, pp. 81-90. \n[Kazman 99] R. Kazman and S.J. Carriere. “Playing Detective: Reconstructing Soft-\nware Architecture from Available Evidence,” Automated Software Engineering \n6(2)(April 1999), pp. 107-138.\n[Kazman 01] R. Kazman, J. Asundi, and M. Klein. “Quantifying the Costs and \nBenefits of Architectural Decisions,” Proceedings of the 23rd International \nConference on Software Engineering (ICSE 23), Toronto, Canada, May 2001, \npp. 297-306.\n[Kazman 02] R. Kazman, L. O’Brien, and C. Verhoef. “Architecture Reconstruc-\ntion Guidelines, Third Edition,” CMU/SEI Technical Report, CMU/SEI-\n2002-TR-034, 2002.\n[Kazman 04] R. Kazman, P. Kruchten, R. Nord, and J. Tomayko. “Integrating Soft-\nware-Architecture-Centric Methods into the Rational Unified Process,” Techni-\ncal Report CMU/SEI-2004-TR-011, July 2004, http://www.sei.cmu.edu/library/\nabstracts/reports/04tr011.cfm\n[Kazman 05] Rick Kazman and Len Bass. “Categorizing Business Goals for Soft-\nware Architectures,” CMU/SEI-2005-TR-021, December 2005.\n[Kazman 09] R. Kazman and H.-M. Chen. “The Metropolis Model: A New Logic for \nthe Development of Crowdsourced Systems,” Communications of the ACM, \nJuly 2009, pp. 76-84.\n",
      "content_length": 2904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 575,
      "content": "554 \nReferences\t\n[Kircher 03] Michael Kircher and Prashant Jain. Pattern-Oriented Software Archi-\ntecture Volume 3: Patterns for Resource Management. Wiley, 2003.\n[Klein 10] J. Klein and M. Gagliardi. “A Workshop on Analysis and Evaluation of \nEnterprise Architectures,” CMU/SEI-2010-TN-023, http://www.sei.cmu.edu/\nreports/10tn023.pdf\n[Klein 93] M. Klein, T. Ralya, B. Pollak, R. Obenza, and M. Gonzalez Harbour. A \nPractitioner’s Handbook for Real-Time Systems Analysis. Kluwer Academic, \n1993.\n[Koziolet 10] H. Koziolek. “Performance Evaluation of Component-Based Software \nSystems: A Survey,” Performance Evaluation 67(8)(August 2010).\n[Kruchten 95] P.B. Kruchten. “The 4+1 View Model of Architecture,” IEEE Soft-\nware, Vol. 12, No. 6 (November 1995), pp. 42-50.\n[Kruchten 03] Philippe Kruchten. The Rational Unified Process: An Introduction, \nThird Edition. Addison-Wesley, 2003.\n[Kruchten 04] Philippe Kruchten. “An Ontology of Architectural Design Decisions,” \nin Jan Bosch, ed., Proceedings of the 2nd Workshop on Software Variability \nManagement, Groningen, NL, Dec. 3-4, 2004.\n[Kumar 10a] K. Kumar and TV Prabhakar. “Pattern-Oriented Knowledge Model for \nArchitecture Design,” in Pattern Languages of Programs Conference 2010, \nOctober 15-18, 2010, Reno/Tahoe, Nevada.\n[Kumar 10b] Kiran Kumar and TV Prabhakar. “Design Decision Topology Model \nfor Pattern Relationship Analysis,” Asian Conference on Pattern Languages of \nPrograms 2010, March 15-17, 2010, Tokyo, Japan.\n[Ladas 09] Corey Ladas. Scrumban: Essays on Kanban Systems for Lean Soft-\nware Development. Modus Cooperandi Press, 2009.\n[Lattanze 08] Tony Lattanze. Architecting Software Intensive Systems: A Practi-\ntioner’s Guide. Auerbach Publications, 2008.\n[Le Traon 97] Y. Le Traon and C. Robach. “Testability Measurements for Data Flow \nDesigns,” Proceedings of the 4th International Symposium on Software Met-\nrics (METRICS ’97), pp. 91-98. November 1997, Washington, D.C.\n[Leveson 04] Nancy G. Leveson. “The Role of Software in Spacecraft Accidents,” \nJournal of Spacecraft and Rockets 41(4)(July 2004), pp. 564-575.\n[Leveson 11] Nancy G. Leveson. Engineering a Safer World: Systems Thinking \nApplied to Safety. MIT Press, 2011.\n[Levitt 88] B. Levitt and J. March. “Organizational Learning,” Annual Review of \nSociology 14 (1988), pp. 319-340.\n[Liu 00] Jane Liu. Real-Time Systems. Prentice Hall, 2000.\n[Liu 09] Henry Liu. Software Performance and Scalability: A Quantitative  \nApproach. Wiley, 2009. \n[Luftman 00] J. Luftman. “Assessing Business Alignment Maturity,” Communica-\ntions of AIS, Vol. 4, No. 14, 2000.\n[Lyons 62] R. E. Lyons and W. Vanderkulk. “The Use of Triple-Modular Redun-\ndancy to Improve Computer Reliability,” IBM J. Res. Dev. 6(2)(April 1962), \npp. 200-209.\n",
      "content_length": 2754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 576,
      "content": "References\n555\n[MacCormack 06] A. MacCormack, J. Rusnak, and C. Baldwin. “Exploring the \nStructure of Complex Software Designs: An Empirical Study of Open Source \nand Proprietary Code,” Management Science 52(7)(July 2006), pp. 1015-1030.\n[MacCormack 10] A. MacCormack, C. Baldwin, and J. Rusnak. “The Architecture \nof Complex Systems: Do Core-Periphery Structures Dominate?” MIT Sloan \nResearch Paper No. 4770-10, http://www.hbs.edu/research/pdf/10-059.pdf\n[Malan 00] Ruth Malan and Dana Bredemeyer. “Creating an Architectural Vision: \nCollecting Input,” http://www.bredemeyer.com/pdf_files/vision_input.pdf, July \n25, 2000.\n[Maranzano 05] Joseph F. Maranzano, Sandra A. Rozsypal, Gus H. Zimmerman, \nGuy W. Warnken, Patricia E. Wirth, and David M. Weiss. “Architecture Re-\nviews: Practice and Experience,” IEEE Software, March/April 2005, pp. 34-43.\n[Mavis 02] D.G. Mavis. “Soft Error Rate Mitigation Techniques for Modern Mi-\ncrocircuits,” 40th Annual Reliability Physics Symposium Proceedings, April \n2002, Dallas, TX. IEEE, 2002. \n[McCall 77] J.A. McCall, P.K. Richards, and G.F. Walters. Factors in Software \nQuality. Griffiths Air Force Base, N.Y. : Rome Air Development Center Air \nForce Systems Command.\n[McGregor 11] John D. McGregor, J. Yates Monteith, and Jie Zhang. “Quantifying \nValue in Software Product Line Design,” in Proceedings of the 15th Interna-\ntional Software Product Line Conference, Volume 2 (SPLC ’11), Ina Schae-\nfer, Isabel John, and Klaus Schmid, eds.\n[Mettler 91] R. Mettler. “Frederick C. Lindvall,” in Memorial Tributes: National \nAcademy of Engineering, Volume 4, pp. 213-216. National Academy of Engi-\nneering, 1991. \n[Moore 03] M. Moore, R. Kazman, M. Klein, and J. Asundi. “Quantifying the Value \nof Architecture Design Decisions: Lessons from the Field,” Proceedings of the \n25th International Conference on Software Engineering (ICSE 25), Portland, \nOR, May 2003, pp. 557-562.\n[Morelos-Zaragoza 06] R.H. Morelos-Zaragoza. The Art of Error Correcting Cod-\ning, Second Edition. Wiley, 2006.\n[Muccini 03] H. Muccini, A. Bertolino, and P. Inverardi. “Using Software Architec-\nture for Code Testing,” IEEE Transactions on Software Engineering 30(3), \npp. 160-171.\n[Muccini 07] H. Muccini. “What Makes Software Architecture-Based Testing Distin-\nguishable,” in Proc. Sixth Working IEEE/IFIP Conference on Software Archi-\ntecture, WICSA 2007, Mumbai, India, January 2007.\n[Murphy 01] G. Murphy, D. Notkin, and K. Sullivan. “Software Reflexion Models: \nBridging the Gap between Design and Implementation,” IEEE Transactions on \nSoftware Engineering, Vol. 27, pp. 364-380, 2001.\n[Nielsen 08] Jakob Nielsen. “Usability ROI Declining, But Still Strong,” http://www.\nuseit.com/alertbox/roi.html\n[NIST 02] National Institute of Standards and Technology. “Security Requirements \nFor Cryptographic Modules,” FIPS Pub. 140-2, http://csrc.nist.gov/publications/\nfips/fips140-2/fips1402.pdf\n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 577,
      "content": "556 \nReferences\t\n[NIST 04] National Institute of Standards and Technology. “Standards for Security \nCategorization of Federal Information Systems,” FIPS Pub. 199, http://csrc.nist.\ngov/publications/fips/fips199/FIPS-PUB-199-final.pdf\n[NIST 06] National Institute of Standards and Technology. “Minimum Security Re-\nquirements for Federal Information and Information Systems,” FIPS Pub. 200, \nhttp://csrc.nist.gov/publications/fips/fips200/FIPS-200-final-march.pdf\n[NIST 09] National Institute of Standards and Technology. “800-53 v3 Recom-\nmended Security Controls for Federal Information Systems and Organizations,” \nAugust 2009, http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-\nrev3-final.pdf\n[Nord 04] R. Nord, J. Tomayko, and R. Wojcik. “Integrating Software Archi-\ntecture-Centric Methods into Extreme Programming (XP),” CMU/SEI-\n2004-TN-036. Software Engineering Institute, Carnegie Mellon University, \n2004.\n[Nygard 07] Michael T. Nygard. Release It!: Design and Deploy \nProduction-Ready Software. Pragmatic Programmers, 2007.\n[Obbink 02] H. Obbink, P. Kruchten, W. Kozaczynski, H. Postema, A. Ran, L. Dom-\ninic, R. Kazman, R. Hilliard, W. Tracz, and E. Kahane. “Software Architecture \nReview and Assessment (SARA) Report, Version 1.0,” 2002, http://pkruchten.\nwordpress.com/architecture/SARAv1.pdf/\n[O’Brien 03] L. O’Brien and C. Stoermer. “Architecture Reconstruction Case Study,” \nCMU/SEI Technical Note, CMU/SEI-2003-TN-008, 2003.\n[ODUSD 08] Office of the Deputy Under Secretary of Defense for Acquisition and \nTechnology. “Systems Engineering Guide for Systems of Systems, Version 1.0,” \n2008, http://www.acq.osd.mil/se/docs/SE-Guide-for-SoS.pdf\n[Palmer 02] Stephen Palmer and John Felsing. A Practical Guide to \nFeature-Driven Development. Prentice Hall, 2002.\n[Parnas 72] D.L. Parnas. “On the Criteria to Be Used in Decomposing Systems into \nModules,” Communications of the ACM 15(12)(December 1972).\n[Parnas 74] D. Parnas. “On a ‘Buzzword’: Hierarchical Structure,” Proceedings \nIFIP Congress 74, pp. 336-339. North Holland Publishing Company, 1974.\n[Parnas 76] D.L. Parnas. “On the Design and Development of Program Families,” \nIEEE Transactions on Software Engineering, SE-2, 1 (March 1976), pp. 1-9.\n[Parnas 79] D. Parnas. “Designing Software for Ease of Extension and Contraction,” \nIEEE Transactions on Software Engineering, SE-5, 2 (1979), pp. 128-137.\n[Parnas 95] David Parnas and Jan Madey. “Functional Documents for Computer \nSystems,” chapter in Science of Computer Programming. Elsevier, 1995.\n[Paulish 02] Daniel J. Paulish. Architecture-Centric Software Project Manage-\nment: A Practical Guide. Addison-Wesley, 2002.\n[Pena 87] William Pena. Problem Seeking: An Architectural Programming \nPrimer. AIA Press, 1987.\n[Perry 92] Dewayne E. Perry and Alexander L. Wolf. “Foundations for the Study of \nSoftware Architecture,” SIGSOFT Softw. Eng. Notes 17(4)(October 1992),  \npp. 40-52.\n[Pettichord 02] B. Pettichord. “Design for Testability,” Pacific Northwest Software \nQuality Conference, Portland, Oregon, October 2002.\n",
      "content_length": 3059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 578,
      "content": "References\n557\n[Powel Douglass 99] B. Powel Douglass. Doing Hard Time: Developing Real-Time \nSystems with UML, Objects, Frameworks, and Patterns. Addison-Wesley, \n1999.\n[Sangwan 08] Raghvinder Sangwan, Colin Neill, Matthew Bass, and Zakaria El \nHouda. “Integrating a Software Architecture-Centric Method into Object- \nOriented Analysis and Design,” Journal of Systems and Software, Vol. 81, No. \n5 (May 2008), pp. 727-746.\n[Schmerl 06] B. Schmerl, J. Aldrich, D. Garlan, R. Kazman, and H. Yan. “Discov-\nering Architectures from Running Systems,” IEEE Transactions on Software \nEngineering 32(7)(July 2006), pp. 454-466.\n[Schmidt 00] Douglas Schmidt, M. Stal, H. Rohnert, and F. Buschmann.  \nPattern-Oriented Software Architecture: Patterns for Concurrent and Net-\nworked Objects. Wiley, 2000.\n[Schmidt 10] Klaus Schmidt. High Availability and Disaster Recovery: Concepts, \nDesign, Implementation. Springer, 2010.\n[Schneier 96] B. Schneier. Applied Cryptography. Wiley, 1996.\n[Schneier 08] Bruce Schneier. Schneier on Security. Wiley, 2008.\n[Schwaber 04] Ken Schwaber. Agile Project Management with Scrum. Microsoft \nPress, 2004.\n[Scott 09] James Scott and Rick Kazman. “Realizing and Refining Architectural \nTactics: Availability,” Technical Report CMU/SEI-2009-TR-006, August 2009.\n[Seacord 05] Robert Seacord. Secure Coding in C and C++. Addison-Wesley, \n2005.\n[SEI 12] Software Engineering Institute. “A Framework for Software Product Line \nPractice, Version 5.0,” http://www.sei.cmu.edu/productlines/frame_report/ \nPL.essential.act.htm\n[Shaw 94] Mary Shaw. “Procedure Calls Are the Assembly Language of Software \nInterconnections: Connectors Deserve First-Class Status,” Carnegie Mellon \nUniversity Technical Report, 1994, http://repository.cmu.edu/cgi/viewcontent.\ncgi?article=1234&context=sei\n[Shaw 95] Mary Shaw. “Beyond Objects: A Software Design Paradigm Based on \nProcess Control,” ACM Software Engineering Notes, Vol. 20, No. 1 (January \n1995), pp. 27-38.\n[Smith 01] Connie U. Smith and Lloyd G. Williams. Performance Solutions: A \nPractical Guide to Creating Responsive, Scalable Software. Addison-Wesley, \n2001. \n[Soni 95] Dilip Soni, Robert L. Nord, and Christine Hofmeister. “Software Architec-\nture in Industrial Applications,” International Conference on Software Engi-\nneering 1995, April 1995, pp. 196-207.\n[Stonebraker 09] M. Stonebraker. “The ‘NoSQL’ Discussion \nHas Nothing to Do with SQL,” http://cacm.acm.org/blogs/\nblog-cacm/50678-the-nosql-discussion-has-nothing-to-do-with-sql/fulltext \n[Stonebraker 10a] M. Stonebraker. “SQL Databases v. NoSQL Databases,” Commu-\nnications of the ACM 53(4), p. 10.\n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 579,
      "content": "558 \nReferences\t\n[Stonebraker 10b] M. Stonebraker, D. Abadi, D.J. Dewitt, S. Madden, E. Paulson, A. \nPavlo, and A. Rasin. “MapReduce and Parallel DBMSs,” Communications of \nthe ACM 53, p. 6.\n[Stonebraker 11] M. Stonebraker. “Stonebraker on NoSQL and Enterprises,” Com-\nmunications of the ACM 54(8), p. 10.\n[Storey 97] M.-A. Storey, K. Wong, and H. Müller. “Rigi—A Visualization Envi-\nronment for Reverse Engineering (Research Demonstration Summary),” 19th \nInternational Conference on Software Engineering (ICSE 97), May 1997,  \npp. 606-607. IEEE Computer Society Press.\n[Svahnberg 00] M. Svahnberg and J. Bosch. “Issues Concerning Variability in Soft-\nware Product Lines,” in Proceedings of the Third International Workshop on \nSoftware Architectures for Product Families, Las Palmas de Gran Canaria, \nSpain, March 15-17, 2000, pp. 50-60. Springer, 2000.\n[Taylor 09] R. Taylor, N. Medvidovic, and E. Dashofy. Software Architecture: \nFoundations, Theory, and Practice. Wiley, 2009.\n[Telcordia 00] Telcordia. “GR-253-CORE, Synchronous Optical Network (SONET) \nTransport Systems: Common Generic Criteria.” 2000.\n[Urdangarin 08] R. Urdangarin, P. Fernandes, A. Avritzer, and D. Paulish. “Experi-\nences with Agile Practices in the Global Studio Project,” Proceedings of the \nIEEE International Conference on Global Software Engineering, 2008.\n[Utas 05] G. Utas. Robust Communications Software: Extreme Availability, Reli-\nability, and Scalability for Carrier-Grade Systems. Wiley, 2005. \n[van der Linden 07] F. van der Linden, K. Schmid, and E. Rommes. Software \nProduct Lines in Action. Springer, 2007.\n[van Deursen 04] A. van Deursen, C. Hofmeister, R. Koschke, L. Moonen, and C. \nRiva. “Symphony: View-Driven Software Architecture Reconstruction,” Pro-\nceedings of the 4th Working IEEE/IFIP Conference on Software Architec-\nture (WICSA 2004), June 2004, Oslo, Norway. IEEE Computer Society.\n[van Vliet 05] H. van Vliet. “The GRIFFIN project, A GRId For inFormatIoN about \narchitectural knowledge,” http://griffin.cs.vu.nl/, Vrije Universiteit, Amsterdam, \nApril 16, 2005.\n[Verizon 12] “Verizon 2012 Data Breach Investigations Re-\nport,” http://www.verizonbusiness.com/resources/reports/\nrp_data-breach-investigations-report-2012_en_xg.pdf\n[Vesely 81] W.E. Vesely, F.F. Goldberg, N.H. Roberts, and D.F. Haasl. “Fault Tree \nHandbook,” http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr0492/\nsr0492.pdf\n[Vesely 02] William Vesely, Michael Stamatelatos, Joanne Dugan, Joseph Fragola, \nJoseph Minarick III, and Jan Railsback. “Fault Tree Handbook with Aerospace \nApplications,” http://www.hq.nasa.gov/office/codeq/doctree/fthb.pdf\n[Viega 01] John Viega and Gary McGraw. Building Secure Software: How to Avoid \nSecurity Problems the Right Way. Addison-Wesley, 2001.\n[Voas 95] Jeffrey M. Voas and Keith W. Miller. “Software Testability: the New Verifi-\ncation,” IEEE Software 12(3)(May 1995), pp. 17-28. \n",
      "content_length": 2908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 580,
      "content": "References\n559\n[Von Neumann 56] J. Von Neumann. “Probabilistic Logics and the Synthesis of Re-\nliable Organisms from Unreliable Components,” Automata Studies, C.E. Shan-\nnon and J. McCarthy, eds. Princeton University Press, 1956. \n[Wojcik 06] R. Wojcik, F. Bachmann, L. Bass, P. Clements, P. Merson, R. Nord, and \nW. Wood. “Attribute-Driven Design (ADD), Version 2.0,” Technical Report \nCMU/SEI-2006-TR-023, November 2006, http://www.sei.cmu.edu/library/ \nabstracts/reports/06tr023.cfm\n[Wood 07] W. Wood. “A Practical Example of Applying Attribute-Driven Design \n(ADD), Version 2.0,” Technical Report CMU/SEI-2007-TR-005, February 2007, \nhttp://www.sei.cmu.edu/library/abstracts/reports/07tr005.cfm\n[Woods 11] E. Woods and N. Rozanski. Software Systems Architecture: Working \nwith Stakeholders Using Viewpoints and Perspectives, Second Edition.  \nAddison-Wesley, 2011.\n[Wozniak 07] J. Wozniak, V. Baggiolini, D. Garcia Quintas, and J. Wenninger.  \n“Software Interlocks System,” Proceedings ICALEPCS07, http://ics-web4.sns.\nornl.gov/icalepcs07/WPPB03/WPPB03.PDF \n[Wu 06] W. Wu and T. Kelly. “Deriving Safety Requirements as Part of System Ar-\nchitecture Definition,” in Proceedings of 24th International System  \nSafety Conference, published by the System Safety Society, August 2006,  \nAlbuquerque, NM.\n[Yacoub 02] S. Yacoub and H. Ammar. “A Methodology for Architecture-Level  \nReliability Risk Analysis,” IEEE Transactions on Software Engineering, Vol. \n28, No. 6 (June 2002).\n[Yin 94] James Bieman and Hwei Yin. “Designing for Software Testability Using \nAutomated Oracles,” Proceedings International Test Conference, September \n1992, pp. 900-907.\n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 581,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 582,
      "content": "561\nAbout the Authors\nLen Bass is a Senior Principal Researcher at National ICT Australia Ltd. \n(NICTA). He joined NICTA in 2011 after 25 years at the Software Engineer-\ning Institute (SEI) at Carnegie Mellon University. He is the coauthor of two \naward-winning books in software architecture, including Documenting Software \nArchitectures: Views and Beyond, Second Edition (Addison-Wesley, 2011), as \nwell as several other books and numerous papers in computer science and soft-\nware engineering on a wide range of topics. Len has almost 50 years’ experience \nin software development and research in multiple domains, such as scientific \nanalysis systems, embedded systems, and information systems.\nPaul Clements is the Vice President of Customer Success at BigLever Software, \nInc., where he works to spread the adoption of systems and software product \nline engineering. Prior to this position, he was Senior Member of the Technical \nStaff at the SEI, where for 17 years he was leader or co-leader of projects in \nsoftware product line engineering and software architecture documentation and \nanalysis. Other books Paul has coauthored include Documenting Software Archi-\ntectures: Views and Beyond, Second Edition (Addison-Wesley, 2011), Evaluating \nSoftware Architectures: Methods and Case Studies (Addison-Wesley, 2002), and \nSoftware Product Lines: Practices and Patterns (Addison-Wesley, 2002). In ad-\ndition, he has also published dozens of papers in software engineering reflecting \nhis long-standing interest in the design and specification of challenging software \nsystems. Paul was a founding member of the IFIP WG2.10 Working Group on \nSoftware Architecture.\nRick Kazman is a Professor at the University of Hawaii and a Visiting Scientist \n(and former Senior Member of the Technical Staff) at the SEI. He is a coauthor \nof Evaluating Software Architectures: Methods and Case Studies (Addison-Wes-\nley, 2002) and author of more than 100 technical papers. Rick’s primary research \ninterests focus on software architecture, design and analysis, software visualiza-\ntion, and software engineering economics. Rick has created several highly influ-\nential methods and tools for architecture analysis, including the SAAM (Soft-\nware Architecture Analysis Method), the ATAM (Architecture Tradeoff Analysis \nMethod), the CBAM (Cost-Benefit Analysis Method), and the Dali architecture \nreverse-engineering tool.\n",
      "content_length": 2415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 583,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 584,
      "content": "563\nIndex\nAADL (Architecture Analysis and Design \nLanguage), 354\nAbstract common services tactic, 124\nAbstract data sources for testability, 165\nAbstract Syntax Tree (AST) analyzers, 386\nAbstraction, architecture as, 5–6\nAcceptance testing, 372\nAccess\nbasis sets, 261\nnetwork, 504\naccess_read relationship, 384\naccess_write relationship, 384\nACID (atomic, consistent, isolated, and \ndurable) properties, 95\nAcknowledged system of systems, 106\nActive redundancy, 91, 256–259\nActiveMQ product, 224\nActivities\ncompetence, 468\ntest, 374–375\nActivity diagrams for traces, 353\nActors tactic, 152–153\nAdams, Douglas, 437\nADD method. See Attribute-Driven Design \n(ADD) method\nAdd-ons, 491–492\nADLs (architecture description languages), 330\nAdolphus, Gustavus, 42\nAdoption strategies, 494–496\nAdventure Builder system, 224, 226, 237\nAggregation for usability, 180\nAgile projects, 533\narchitecture example, 283–285\narchitecture methods, 281–283\narchitecture overview, 277–281\ndescription, 44–45\ndocumenting, 356–357\nguidelines, 286–287\nintroduction, 275–277\npatterns, 238\nrequirements, 56\nsummary, 287–288\nAIC (Architecture Influence Cycle)\ndescription, 58\nVasa ship, 43\nAir France flight 447, 192\nAir traffic control systems, 366–367\nAllen, Woody, 79\nAllocated to relation\nallocation views, 339–340\ndeployment structure, 14\nmulti-tier pattern, 237\nAllocation of responsibilities category\nASRs, 293\navailability, 96\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 73\nsecurity, 154\ntestability, 169\nusability, 181\nAllocation patterns\nmap-reduce, 232–235\nmiscellaneous, 238\nmulti-tier, 235–237\nAllocation structures, 5, 11, 14\nAllocation views, 339–340\nAllowed-to-use relationship, 206–207\nAlpha testing, 372\nAlternatives, evaluating, 398\nAmazon service-level agreements, 81, 522\nAnalysis\narchitecture, 47–48\nATAM, 408–409, 411\navailability, 255–259\nback-of-the-envelope, 262–264\nconformance by, 389–392\neconomic. See Economic analysis\noutsider, 399\nperformance, 252–255\nAnalysts, 54\nAnalytic model space, 259–260\nAnalytic perspective on up-front work vs. \nagility, 279–281\nAnalytic redundancy tactic, 90\nAND gate symbol, 84\nAnonymizing test data, 171\nAntimissile system, 104\nApache web server, 528, 531\nApproaches\nATAM, 407–409, 411\nCIA, 147–148\nLightweight Architecture Evaluation, 416\n",
      "content_length": 2312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 585,
      "content": "564 \nIndex\t\nArchitects\nbackground and experience, 51–52\ncloud environments, 520–523\ncommunication with, 29\ncompetence, 459–467\ndescription and interests, 54\nduties, 460–464\nknowledge, 466–467\nresponsibilities, 422–423\nskills, 463, 465\ntest role, 375–376\nArchitectural structures\nallocation, 14\ncomponent-and-connector, 13–14\ndocumentation, 17–18\ninsight from, 11–12\nkinds, 10–11\nlimiting, 17\nmodule, 12–13\nrelating to each other, 14, 16–17\nselecting, 17\ntable of, 15\nviews, 9–10\nArchitecturally significant requirements \n(ASRs), 46–47, 291–292\nADD method, 320–321\nfrom business goals, 296–304\ndesigning to, 311–312\ninterviewing stakeholders, 294–296\nfrom requirements documents, 292–293\nutility trees for, 304–307\nArchitecture\nAgile projects. See Agile projects\nanalyzing, 47–48\navailability. See Availability\nbusiness context, 49–51\nchanges, 27–28\ncloud. See Cloud environments\ncompetence. See Competence\nconceptual integrity of, 189\ndesign. See Design and design strategy\ndocumenting. See Documentation\ndrivers in PALM, 305\neconomics. See Economic analysis\nevaluation. See Evaluation\nimplementation. See Implementation\ninfluences, 56–58\nin life cycle, 271–274\nmanagement. See Management and \ngovernance\nmodifiability. See Modifiability\npatterns. See Patterns\nperformance. See Performance\nproduct lines. See Software product lines\nproduct reuse, 483–484\nQAW drivers, 295\nQAW plan presentation, 295\nquality attributes. See Quality attributes\nreconstruction and conformance. See \nReconstruction and conformance\nrequirements. See Architecturally \nsignificant requirements (ASRs); \nRequirements\nsecurity. See Security\nstructures. See Architectural structures\ntactics. See Tactics\ntestability. See Testability\nusability. See Usability\nArchitecture Analysis and Design Language \n(AADL), 354\nArchitecture-centric projects, 279\nArchitecture description languages (ADLs), \n330\nArchitecture Influence Cycle (AIC)\ndescription, 58\nVasa ship, 43\nArchitecture Tradeoff Analysis Method \n(ATAM), 48, 283, 400\napproaches, 407–409, 411\nbusiness drivers, 404–405\nexample exercise, 411–414\noutputs, 402–403\nparticipants, 400–401\nphases, 403–404\npresentation, 403–406\nresults, 411\nscenarios, 408, 410\nsteps, 404–411\nAriane 5 explosion, 192\nAristotle, 185\nArrival pattern for events, 133\nArtifacts\navailability, 85–86\nin evaluation, 399\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nproduct reuse, 484\nquality attributes expressions, 69–70\nsecurity, 148, 150\ntestability, 162–163\nusability, 176\nvariability, 489\nASP.NET framework, 215\nAspects\nfor testability, 167\n",
      "content_length": 2563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 586,
      "content": "Index\n565\nvariation mechanism, 492\nASRs. See Architecturally significant \nrequirements (ASRs)\nAssembly connectors in UML, 369\nAssertions for system state, 166\nAssessment goals, 469\nAssessment of competence, 469–472, \n474–475\nAssign utility\nCBAM, 446\nNASA ECS project, 452\nAST (Abstract Syntax Tree) analyzers, 386\nAsymmetric flow in client-server pattern, 218\nAsynchronous messaging, 223, 225\nATAM. See Architecture Tradeoff Analysis \nMethod (ATAM)\nATM (automatic teller machine) banking \nsystem, 219\nAtomic, consistent, isolated, and durable \n(ACID) properties, 95\nAttachment relation\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector structures, 13\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nshared-data pattern, 231\nAttachments in component-and-connector \nviews, 336–337\nAttribute-Driven Design (ADD) method, 316\nASRs, 320–321\nelement choice, 318–319\nelement design solution, 321\ninputs, 316\noutput, 317–318\nrepeating steps, 324\nverify and refine requirements step, \n321–323\nAttributes. See Quality attributes\nAudiences for documentation, 328–329\nAuditor checklists, 260\nAudits, 153\nAuthenticate actors tactic, 152\nAuthentication in CIA approach, 148\nAuthorization in CIA approach, 148\nAuthorize actors tactic, 152\nAutomated delivery in Metropolis model, 535\nAutomatic reallocation of IP addresses, 516\nAutomatic scaling, 516\nAutomatic teller machine (ATM) banking \nsystem, 219\nAutomation for testability, 171–172\nAUTOSAR framework, 364\nAvailability\nanalytic model space, 259\nanalyzing, 255–259\nbroker pattern, 240\ncalculations, 259\nCAP theorem, 523\nCIA approach, 147\ncloud, 521\ndesign checklist, 96–98\ndetect faults tactic, 87–91\ngeneral scenario, 85–86\nintroduction, 79–81\nplanning for failure, 82–85\nprevent faults tactic, 94–95\nrecover-from-faults tactics, 91–94\nsummary, 98–99\ntactics overview, 87\nAvailability of resources tactic, 136\nAvailability quality attribute, 307\nAvailability zones, 522\nAvižienis, Algirdas, 79\nBack door reviews, 544–545\nBack-of-the-envelope analysis, 262–264\nBackground of architects, 51–52\nBank application, 391–392\nBase mechanisms in cloud, 509–514\nBasis sets for quality attributes, 261\nBDUF (Big Design Up Front) process, 278\nBehavior\ndocumenting, 351–354\nelement, 347\nin software architecture, 6–7\nBenefit in economic analysis, 441–442\nBenkler, Yochai, 528\nBeta testing, 372\nBig bang integration, 371\nBig bang models, 495–496\nBig Design Up Front (BDUF) process, 278\nBigTable database system, 518\nBinder, Robert, 167\nBinding\nlate, 385, 388\nmodifiability, 124–125\nuser interface, 178\nBinding time category\nASRs, 293\navailability, 98\ninteroperability, 115\nmodifiability, 122, 127\nperformance, 144\nquality design, 75–76\nsecurity, 156\n",
      "content_length": 2708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 587,
      "content": "566 \nIndex\t\nBinding time category, continued\ntestability, 170\nusability, 182\nBitTorrent networks, 221\nBlack-box testing, 372–373\n“Blind Men and the Elephant” (Saxe), 379\nBlocked time in performance, 136\nBlogger website, 528\nBoehm, Barry, 279, 281, 286, 288\nBooch, Grady, 286\nBoolean logic diagrams, 83\nBottom-up adoption, 495\nBottom-up analysis mode, 284\nBottom-up schedules, 420–421\nBound execution times tactic, 138\nBound queue sizes tactic, 139\nBoundaries in ADD method, 317\nBox-and-line drawings\nas architectures, 6\ncomponent-and-connector views, 338\nBPEL (Business Process Execution \nLanguage), 108\nBrainstorming\nATAM, 410\nLightweight Architecture Evaluation, 416\nQAW, 295\nBranson, Richard, 443\nBreadth first ADD strategy, 319\nBrewer, Eric, 522\nBroadcast-based publish-subscribe pattern, \n229\nBroker pattern\navailability, 255–259\ndescription, 210–212\nweaknesses, 240–242\nBrooks, Fred, 47, 419\nBuley, Taylor, 147\nBureaucracy in implementation, 427\nBush, Vannevar, 397\nBusiness cases in project life-cycle context, \n46\nBusiness context\narchitecture influence on, 58\narchitectures and business goals, 49–50\nBusiness drivers\nATAM, 404–405\nLightweight Architecture Evaluation, 416\nPALM method, 305\nBusiness goals\nASRs from, 296–304\nassessment, 469\nATAM, 402\nbusiness context, 49–50\ncapturing, 304\ncategorization, 297–299\nevaluation process, 400\nexpressing, 299–301\ngeneral scenario, 301–303\nPALM method, 305\nviews for, 332\nBusiness managers, 54\nBusiness/mission presentation in QAW, 295\nBusiness Process Execution Language \n(BPEL), 108\nBusiness process improvements as business \ngoal, 299\nBusiness-related architect skills, 465\nC&C structures. See Component-and-connector \n(C&C) patterns and structures\nCaching tactic, 139\nCallbacks in Model-View-Controller pattern, \n214\nCalls relationship in view extraction, 384\nCancel command, 179\nCAP theorem, 518, 522–523\nCapture scenarios for quality attributes, \n196–197\nCapturing\nASRs in utility trees, 304–307\nbusiness goals, 304–307\nCatastrophic failures, 82\nCategorization of business goals, 297–299\nCBAM. See Cost Benefit Analysis Method \n(CBAM)\nChange\ndocumenting, 355–356\nmodifiability. See Modifiability\nreasoning and managing, 27–28\nChange control boards, 427\nChange default settings tactic, 153\nChaos Monkey, 160–161\nChaucer, Geoffrey, 459\nCheck-in, syncing at, 368\nChoice of technology category\nASRs, 293\navailability, 98\ninteroperability, 115\nmodifiability, 127\nperformance, 144\nsecurity, 156\ntestability, 170\nusability, 182\nCIA (confidentiality, integrity, and availabil-\nity) approach, 147–148\nCity analogy in Metropolis model, 536\n",
      "content_length": 2587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 588,
      "content": "Index\n567\nclass_contains_method relationship, 384\nclass_is_subclass_of_class relationship, 384\nClass structure, 13\nClasses in testability, 167\nClements, Paul, 66\nClient-server patterns, 19, 217–219\nClient-side proxies, 211\nClients\nbroker pattern, 211\nsimulators, 265\nClone-and-own practice, 482–483\nCloud environments\narchitecting in, 520–523\navailability, 521\nbase mechanisms, 509–514\ndatabase systems, 517–520\ndefinitions, 504–505\ndeployment models, 506\neconomic justification, 506–509\nequipment utilization, 508–509\nIaaS model, 515–517\nintroduction, 503–504\nmulti-tenancy applications, 509\nPaaS model, 517\nperformance, 521\nsecurity, 520–521\nservice models, 505–506\nsummary, 524\nCluster managers, 515\nCMG (Computer Measurement Group), 524\nCo-located teams\nAgile, 277\ncoordination, 427\nCockburn, Alistair, 287\nCOCOMO II (COnstructive COst MOdel II) \nscale factor, 279\nCode\narchitecture consistency, 366–368\ndesign in, 364\nKSLOC, 279–281\nmapping to, 334\nsecurity, 157\ntemplates, 365–367\nCohesion\nin modifiability, 121–123\nin testability, 167\nCold spares, 92, 256–259\nCollaborative system of systems, 106\nCollating scenarios\nCBAM, 445\nNASA ECS project, 451\nCOMBINATION gate symbol, 84\nCombining views, 343–345\nCommercial implementations of map-reduce \npatterns, 234\nCommon Object Request Broker Architecture \n(CORBA), 212\nCommunicates with relation, 237\nCommunication\nAgile software development, 277\narchitect skills, 465\narchitecture, 47\ndocumentation for, 329\nglobal development, 425\nstakeholder, 29–31\nCommunication diagrams for traces, 353\nCommunications views, 341\nCommunity clouds, 506\nCompatibility in component-and-connector \nviews, 336\nCompatibility quality attribute, 193\nCompetence\nactivities, 468\narchitects, 459–467\nassessment, 469–472, 474–475\nassessment goals, 469\nintroduction, 459–460\nmodels, 476\nquestions, 470, 472–474\nsoftware architecture organizations, \n467–475\nsummary, 475\nCompetence center patterns, 19, 238\nCompetence set tactic, 95\nComplexity\nbroker pattern, 211\nquality attributes, 71\nin testability, 167–168\nComponent-and-connector (C&C) patterns \nand structures, 5, 10–11\nbroker, 210–212\nclient-server, 217–219\nModel-View-Controller, 212–215\npeer-to-peer, 220–222\npipe-and-filter, 215–217\npublish-subscribe, 226–229\nservice-oriented architecture, 222–226\nshared-data, 230–231\ntypes, 13–14\nviews, 335–339, 344, 406\nComponents, 5\nindependently developed, 35–36\nreplacing for testability, 167\nsubstituting in variation mechanism, 492\nComprehensive models for behavior docu-\nmentation, 351, 353–354\n",
      "content_length": 2524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 589,
      "content": "568 \nIndex\t\nComputer Measurement Group (CMG), 524\nComputer science knowledge of architects, \n466\nConcepts and terms, 368–369\nConceptual integrity of architecture, 189\nConcrete quality attribute scenarios, 69\nConcurrency\ncomponent-and-connector views, 13–14, \n337\nhandling, 132–133\nCondition monitoring tactic, 89\nConfidence in usability, 175\nConfidentiality, integrity, and availability \n(CIA) approach, 147–148\nConfigurability quality attribute, 307\nConfiguration manager roles, 422\nConfigurators, 492\nConformance, 380–381\nby analysis, 389–392\narchitectural, 48\nby construction, 389\nConformance checkers, 54\nConformity Monkey, 161\nConnectors\ncomponent-and-connector views, \n335–339\nmulti-tier pattern, 236\npeer-to-peer systems, 220\nREST, 225\nUML, 369\nConsistency\nCAP theorem, 523\ncode and architecture, 366–368\ndatabases, 520\nConsolidation in QAW, 295\nConstraints\nADD method, 322–323\nallocation views, 339\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector views, 337\nconformance, 390\ndefining, 32–33\nlayered pattern, 207\nmap-reduce patterns, 235\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 236–237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nrequirements, 64–65\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nConstruction, conformance by, 389\nCOnstructive COst MOdel II (COCOMO II) \nscale factor, 279\nContent-based publish-subscribe pattern, 229\nContention for resources tactic, 136\nContext diagrams\nATAM presentations, 406\nin documentation, 347\nContexts\narchitecture influence, 56–58\nbusiness, 49–51, 58\ndecision-making, 438–439\nprofessional, 51–52\nproject life-cycle, 44–48\nin relationships, 204–205\nstakeholders, 52–55\nsummary, 59\ntechnical, 40–43\nthought experiments, 263\ntypes, 39–40\nContextual factors in evaluation, 399–400\nContinuity as business goal, 298\nControl relation in map-reduce patterns, 235\nControl resource demand tactic, 137–138\nControl tactics for testability, 164–167\nControllers in Model-View-Controller \npattern, 213–214\nConway’s law, 38\nCoordination model category\nASRs, 293\navailability, 96\nglobal development, 426\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 73–74\nsecurity, 155\ntestability, 169\nusability, 181\nCORBA (Common Object Request Broker \nArchitecture), 212\nCore asset units, 497\nCore requirements, 531–532\nCore vs. periphery in Metropolis model, 534\nCorrelation logic for faults, 81\nCost Benefit Analysis Method (CBAM), 442\ncost determination, 444\nresults, 456–457\nsteps, 445–447\nutility curve determination, 442–443\nvariation points, 448–450\nweighting determination, 444\n",
      "content_length": 2663,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 590,
      "content": "Index\n569\nCosts\nCBAM, 444\nof change, 118\nestimates, 34\nglobal development, 423–424\nindependently developed components \nfor, 36\npower, 507\nresources, 244\nthought experiments, 263\nvalue for, 442\nCosts to complete measure, 430\nCoupling\nin modifiability, 121–124\nin testability, 167\nCrashes and availability, 85\nCredit cards, 147, 157, 260, 268\nCrisis, syncing at, 368\nCriteria for ASRs, 306\nCrowd management in Metropolis model, \n534\nCrowdsourcing, 528\nCRUD operations, 109\nCruiseControl tool, 172\nCrystal Clear method, 44, 287\nCummins, Inc., 480, 490\nCunningham, Ward, 286\nCustomers\ncommunication with, 29\nedge-dominant systems, 529\nCustomization of user interface, 180\nDarwin, Charles, 275\nData Access Working Group (DAWG), 451\nData accessors in shared-data pattern, \n230–231\nData latency, utility trees for, 306\nData model category, 13\nASRs, 293\navailability, 96\ninteroperability, 114\nmodifiability, 126\nperformance, 143\nquality design decisions, 74\nsecurity, 155\ntestability, 169\nusability, 182\nData reading and writing in shared-data \npattern, 230–231\nData replication, 139\nData sets\nmap-reduce pattern, 232–233\nfor testability, 170–171\nData stores in shared-data pattern, 230–231\nData transformation systems, 215\nDatabase administrators, 54\nDatabase systems\ncloud, 517–520\nin reconstruction, 386–387\nDataNodes, 512–514\nDAWG (Data Access Working Group), 451\nDeadline monotonic prioritization strategy, \n140\nDeadlines in processing, 134\nDebugging brokers, 211\nDecision makers on ATAM teams, 401\nDecision-making context, 438–439\nDecisions\nevaluating, 398\nmapping to quality requirements, 402–403\nquality design, 72–76\nDecomposition\ndescription, 311–312\nmodule, 5, 12, 16\nviews, 16, 343, 345\nDedicated finite resources, 530\nDefects\nanalysis, 374\neliminating, 486\ntracking, 430\nDefer binding\nmodifiability, 124–125\nuser interface, 178\nDegradation tactic, 93\nDelegation connectors, 369\nDemilitarized zones (DMZs), 152\nDenial-of-service attacks, 79, 521, 533\nDependencies\nbasis set elements, 261\non computations, 136\nintermediary tactic for, 123–124\nuser interface, 178\nDependent events in probability, 257\nDepends-on relation\nlayered pattern, 207\nmodules, 332–333\nDeploy on relation, 235\nDeployability attribute, 129, 187\nDeployers, 54\nDeployment models for cloud, 506\nDeployment structure, 14\nDeployment views\nATAM presentations, 406\ncombining, 345\npurpose, 332\nDepth first ADD strategy, 319\n",
      "content_length": 2390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 591,
      "content": "570 \nIndex\t\nDesign and design strategy, 311\nADD. See Attribute-Driven Design \n(ADD) method\narchitecturally significant requirements, \n311–312\nin code, 364\ndecomposition, 311–312\nearly decisions, 31–32\ngenerate and test process, 313–316\ninitial hypotheses, 314–315\nnext hypotheses, 315\nquality attributes, 197\nsummary, 325\ntest choices, 315\nDesign checklists\navailability, 96–98\ndesign strategy hypotheses, 315\ninteroperability, 114–115\nmodifiability, 125–127\nperformance, 142–144\nquality attributes, 199\nsecurity, 154–156\nsummary, 183\ntestability, 169–170\nusability, 181–182\nDesigners\ndescription and interests, 54\nevaluation by, 397–398\nDetect attacks tactics, 151\nDetect faults tactic, 87–91\nDetect intrusion tactic, 151\nDetect message delay tactic, 151\nDetect service denial tactic, 151\nDeutsche Bank, 480\nDevelopers\nedge-dominant systems, 529\nroles, 422\nDevelopment\nbusiness context, 50–51\nglobal, 423–426\nincremental, 428\nproject life-cycle context, 44–45\ntests, 374\nDevelopment distributability attribute, 186\nDeviation\nfailure from, 80\nmeasuring, 429\nDevices in ADD method, 317\nDiNucci, Darcy, 527\ndir_contains_dir relationship, 384\ndir_contains_file relationship, 384\nDirected system of systems, 106\nDirectories in documentation, 349\nDiscoTect system, 391\nDiscover service tactic, 111\nDiscovery in interoperability, 105\nDiscovery services, 533\nDistributed computing, 221\nDistributed development, 427\nDistributed testing in Metropolis model, 535\nDMZs (demilitarized zones), 152\nDNS (domain name server), 514\nDoctor Monkey, 161\nDocumentation\nAgile development projects, 356–357\narchitect duties, 462\narchitectural structures, 17–18\narchitecture, 47, 347–349\nbehavior, 351–354\nchanging architectures, 355–356\ndistributed development, 427\nglobal development, 426\nintroduction, 327–328\nnotations, 329–331\nonline, 350\npackages, 345–351\npatterns, 350–351\nand quality attributes, 354–355\nservices, 533\nsummary, 359\nuses and audiences for, 328–329\nviews. See Views\nYAGNI, 282\nDocumentation maps, 347–349\nDocuments, control information, 347\nDomain decomposition, 315\nDomain knowledge of architects, 467\nDomain name server (DNS), 514\nDrivers\nATAM, 404–405\nLightweight Architecture Evaluation, 416\nPALM method, 305\nQAW, 295\nDSK (Duties, Skills, and Knowledge) model \nof competence, 476\nDuke’s Bank application, 391–392\nDuties\narchitects, 460–464\ncompetence, 472\nprofessional context, 51\nDuties, Skills, and Knowledge (DSK) model \nof competence, 476\nDynamic allocation views, 340\nDynamic analysis with fault trees, 83\nDynamic priority scheduling strategies, \n140–141\n",
      "content_length": 2562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 592,
      "content": "Index\n571\nDynamic structures, 5\nDynamic system information, 385–386\nEarliest-deadline-first scheduling strategy, 141\nEarly design decisions, 31–32\nEarth Observing System Data Information \nSystem (EOSDIS) Core System \n(ECS). See NASA ECS project\neBay, 234\nEC2 cloud service, 81, 160, 522, 532\nEclipse platform, 228\nEconomic analysis\nbasis, 439–442\nbenefit and normalization, 441–442\ncase study, 451–457\nCBAM. See Cost Benefit Analysis \nMethod (CBAM)\ncost value, 442\ndecision-making context, 438–439\nintroduction, 437\nscenario weighting, 441\nside effects, 441\nsummary, 457\nutility-response curves, 439–441\nEconomics\ncloud, 506–509\nissues, 543\nEconomies of scale in cloud, 507–508\nEcosystems, 528–530\nECS system. See NASA ECS project\nEdge-dominant systems, 528–530\nEdison, Thomas, 203\neDonkey networks, 221\nEducation, documentation as, 328–329\nEffective resource utilization, 187\nEffectiveness category for quality, 189\nEfficiency category for quality, 189–190\nEinstein, Albert, 175\nEJB (Enterprise Java Beans), 212\nElasticity, rapid, 504–505\nElasticity property, 187\nElectric grids, 106\nElectricity, 191, 570\nElectronic communication in global \ndevelopment, 426\nElements\nADD method, 318–319\nallocation views, 339–340\nbroker pattern, 211\ncatalogs, 346–347\nclient-server pattern, 218\ncomponent-and-connector views, 337\ndefined, 5\nlayered pattern, 207\nmap-reduce patterns, 235\nmapping, 75\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\nproduct reuse, 484\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nEmployees\nas goal-object, 302\nresponsibilities to, 299\nEnabling quality attributes, 26–27\nEncapsulation tactic, 123\nEncrypt data tactic, 152\nEnd users in edge-dominant systems, 529\nEnterprise architecture vs. system \narchitecture, 7–8\nEnterprise Java Beans (EJB), 212\nEnterprise resource planning (ERP) systems, \n228\nEnterprise service bus (ESB), 223, 225, 369\nEnvironment\nADD method, 317\nallocation views, 339–340\navailability, 85–86\nbusiness goals, 300\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntechnical context, 41–42\ntestability, 162–163\nusability, 176\nvariability, 489\nEnvironmental change as business goal, 299\nERP (enterprise resource planning) systems, \n228\nErrors, 80\ncore handling of, 532\ndetection by services, 533\nerror-handling views, 341\nin usability, 175\nESB (enterprise service bus), 223, 225, 369\nEscalating restart tactic, 94\nEstimates, cost and schedule, 34\nEvaluation\narchitect duties, 462–463\narchitecture, 47–48\n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 593,
      "content": "572 \nIndex\t\nEvaluation, continued\nATAM. See Architecture Tradeoff \nAnalysis Method (ATAM)\ncontextual factors, 399–400\nby designer, 397–398\nLightweight Architecture Evaluation, \n415–417\noutsider analysis, 399\npeer review, 398–399\nquestions, 472\nsoftware product lines, 493–494\nsummary, 417\nEvaluators, 54\nEvent bus in publish-subscribe pattern, 227\nEvents\nModel-View-Controller pattern, 214\nperformance, 131, 133\nprobability, 257\nEventual consistency model, 168, 523\nEvolutionary prototyping, 33–34\nEvolving software product lines, 496–497\nException detection tactic, 90\nException handling tactic, 92\nException prevention tactic, 95\nException views, 341\nExchanging information via interfaces, \n104–105\nEXCLUSIVE OR gate symbol, 84\nExecutable assertions for system state, 166\nExecution of tests, 374\nExemplar systems, 485\nExercise conclusion in PALM method, 305\nExisting systems in design, 314\nExpected quality attribute response levels, 453\nExperience of architects, 51–52\nExperiments in quality attribute modeling, \n264–265\nExpressing business goals, 299–301\nExtensibility quality attribute, 307\nExtensible programming environments, 228\nExtension points for variation, 491\nExternal sources for product lines, 496\nExternal system representatives, 55\nExternal systems in ADD method, 317\nExternalizing change, 125\nExtract-transform-load functions, 235\nExtraction, raw view, 382–386\nExtreme Programming development \nmethodology, 44\nFacebook, 527–528\nmap-reduce patterns, 234\nusers, 518\nFail fast principle, 522\nFailure Mode, Effects, and Criticality \nAnalysis (FMECA), 83–84\nFailures, 80\navailability. See Availability\nplanning for, 82–85\nprobabilities and effects, 84–85\nFairbanks, George, 279, 364\nFallbacks principle, 522\nFault tree analysis, 82–84\nFaults, 80\ncorrelation logic, 81\ndetection, 87–91\nprevention, 94–95\nrecovery from, 91–94\nFeature removal principle, 522\nFIFO (first-in/first-out) queues, 140\nFile system managers, 516\nFilters in pipe-and-filter pattern, 215–217\nFinancial objectives as business goal, 298\nFinding violations, 389–392\nFire-and-forget information exchange, 223\nFirefox, 531\nFirst-in/first-out (FIFO) queues, 140\nFirst principles from tactics, 72\nFixed-priority scheduling, 140\nFlex software development kit, 215\nFlexibility\ndefer binding tactic, 124\nindependently developed components \nfor, 36\nFlickr service, 527, 536\nFlight control software, 192–193\nFMECA (Failure Mode, Effects, and \nCriticality Analysis), 83–84\nFocus on architecture in Metropolis model, \n534–535\nFollow-up phase in ATAM, 403–404\nFolsonomy, 528\nFord, Henry, 479\nFormal documentation notations, 330\nFrameworks\ndesign strategy hypotheses, 314–315\nimplementation, 364–365\nFrankl, Viktor E., 63\nFreedom from risk category for quality, 189\nFunctional redundancy tactic, 90\nFunctional requirements, 64, 66\nFunctional responsibility in ADD method, \n322–323\nFunctional suitability quality attribute, 193\nFunctionality\ncomponent-and-connector views, 336\n",
      "content_length": 2944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 594,
      "content": "Index\n573\ndescription, 65\nFused views, 388–389\nGamma, E., 212\nGate symbols, 83–84\nGeneral Motors product line, 487\nGeneralization structure, 13\nGenerate and test process, 313–316\nGenerators of variation, 492\nGet method for system state, 165\nGlobal development, 423–426\nGlobal metrics, 429–430\nGnutella networks, 221\nGoal components in business goals, 300\nGoals. See Business goals\nGoldberg, Rube, 102\nGood architecture, 19–21\nGood enough vs. perfect, 398\nGoogle\ndatabase system, 518\nGoogle App Engine, 517\nGoogle Maps, 105–107\ngreenhouse gas from, 190–191\nmap-reduce patterns, 234\npower sources, 507\nGovernance, 430–431\nGovernment, responsibilities to, 299\nGraceful degradation, 522\nGraphical user interfaces in publish-subscribe \npattern, 228\nGray-box testing, 373\nGreen computing, 190–191\nGreenspan, Alan, 443\nGrowth and continuity as business goal, 298\nGuerrilla movements, 543–544\nHadoop Distributed File System (HDFS), 512\nHardware costs for cloud, 507\nHarel, David, 353\nHarnesses for tests, 374\nHazard analysis, 82\nHazardous failures, 82\nHBase database system, 518–519\nHDFS (Hadoop Distributed File System), \n512\nHeartbeat tactic, 89, 256, 408\nHelm, R., 212\nHewlett-Packard, 480\nHiatus stage in ATAM, 409\nHigh availability. See Availability\nHighway systems, 142\nHorizontal scalability, 187\nHot spare tactic, 91\nHTTP (HyperText Transfer Protocol),  \n219\nHudson tool, 172\nHufstedler, Shirley, 363\nHuman body structure, 9\nHuman Performance model of competence, \n476\nHuman Performance Technology model, \n469–473\nHuman resource management in global \ndevelopment, 425\nHybertsson, Henrik, 42–43\nHybrid clouds, 506\nHydroelectric power station catastrophe, 188, \n192\nHypertext for documentation, 350\nHyperText Transfer Protocol (HTTP), 219\nHypervisors, 510–512\nHypotheses\nconformance, 390\ndesign strategy, 314–315\nfused views, 388\nIaaS (Infrastructure as a Service) model, \n505–506, 515–517\nIdentify actors tactic, 152\nIgnore faulty behavior tactic, 93\nImplementation, 363–364, 427\narchitect duties, 463\ncode and architecture consistency, 366–368\ncode templates, 365–367\ndesign in code, 364\nframeworks, 364–365\nincremental development, 428\nmodules, 333–334\nstructure, 14\nsummary, 376\ntesting, 370–376\ntracking progress, 428–429\ntradeoffs, 427\nImplementors, 55\nIn-service software upgrade (ISSU), 92\nIncludes relationship, 384\nInclusion of elements for variation, 491\nIncrease cohesion tactic, 123\nIncrease competence set tactic, 95\nIncrease efficiency tactic, 142\nIncrease resource efficiency tactic, 138\nIncrease resources tactic, 138–139, 142\nIncrease semantic coherence tactic, 123, 239\nIncremental Commitment Model, 286\nIncremental development, 428\nIncremental integration, 371\n",
      "content_length": 2679,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 595,
      "content": "574 \nIndex\t\nIncremental models in adoption strategies, \n495–496\nIndependent events in probability, 257\nIndependently developed components, 35–36\nInflexibility of methods, 277\nInform actors tactic, 153\nInformal contacts in global development, 426\nInformal notations for documentation, 330\nInformation handling skills, 465\nInformation sharing in cloud, 520\nInfrastructure as a Service (IaaS) model, \n505–506, 515–517\nInfrastructure in map-reduce patterns, 235\nInfrastructure labor costs in cloud, 507\nInheritance variation mechanism, 492\nInherits from relation, 13\nINHIBIT gate symbol, 84\nInhibiting quality attributes, 26–27\nInitial hypotheses in design strategy, 314–315\nInputs in ADD method, 316, 321–323\nInstantiate relation, 235\nIntegration management in global \ndevelopment, 424\nIntegration testing, 371–372\nIntegrators, 55\nIntegrity\narchitecture, 189\nCIA approach, 147\nInterchangeable parts, 35–36, 480\nInterfaces\nexchanging information via, 104–105\nseparating, 178\nIntermediary tactic, 123\nIntermediate states in failures, 80\nInternal sources of product lines, 496–497\nInternet Protocol (IP) addresses\nautomatic reallocation, 516\noverview, 514\nInteroperability\nanalytic model space, 259\ndesign checklist, 114–115\ngeneral scenario, 107–110\nintroduction, 103–106\nservice-oriented architecture pattern, 224\nand standards, 112–113\nsummary, 115\ntactics, 110–113\nInterpersonal skills, 465\nInterpolation in CBAM, 446\nInterviewing stakeholders, 294–296\nIntroduce concurrency tactic, 139\nInvokes-services role, 335\nInvolvement, 542–543\nIowability, 195–196\nIP (Internet Protocol) addresses\nautomatic reallocation, 516\noverview, 514\nIs a relation, 332–333\nIs-a-submodule-of relation, 12\nIs an instance of relation, 13\nIs part of relation\nmodules, 332–333\nmulti-tier pattern, 237\nISO 25010 standard, 66, 193–195\nISSU (in-service software upgrade), 92\nIterative approach\ndescription, 44\nreconstruction, 382\nrequirements, 56\nJanitor Monkey, 161\nJavaScript Object Notation (JSON) form, 519\nJitter, 134\nJobs, Steve, 311\nJohnson, R., 212\nJSON (JavaScript Object Notation) form, 519\nJust Enough Architecture (Fairbanks), 279, \n364\nKeys in map-reduce pattern, 232\nKnowledge\narchitects, 460, 466–467\ncompetence, 472–473\nprofessional context, 51\nKroc, Ray, 291\nKruchten, Philippe, 327\nKSLOC (thousands of source lines of code), \n279–281\nKundra, Vivek, 503\nLabor availability in global development, 423\nLabor costs\ncloud, 507\nglobal development, 423\nLanguage, 542\nLarger data sets in map-reduce patterns, 234\nLate binding, 385, 388\nLatency\nCAP theorem, 523\nperformance, 133, 255\nqueuing models for, 198–199\nutility trees for, 306\nLatency Monkey, 161\nLattix tool, 387\nLawrence Livermore National Laboratory, 71\nLayer bridging, 206\n",
      "content_length": 2714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 596,
      "content": "Index\n575\nLayer structures, 13\nLayer views in ATAM presentations, 406\nLayered patterns, 19, 205–210\nLayered views, 331–332\nLeaders on ATAM teams, 401\nLeadership skills, 464–465\nLearning issues in usability, 175\nLeast-slack-first scheduling strategy, 141\nLePatner, Barry, 3\nLetterman, David, 443\nLevels\nfailure, 258\nrestart, 94\ntesting, 370–372\nLeveson, Nancy, 200\nLexical analyzers, 386\nLife cycle\narchitecture in, 271–274\nchanges, 530–531\nMetropolis model, 537\nproject. See Project life-cycle context\nquality attribute analysis, 265–266\nLife-cycle milestones, syncing at, 368\nLightweight Architecture Evaluation method, \n415–417\nLikelihood of change, 117\nLimit access tactic, 152\nLimit complexity tactic, 167\nLimit event response tactic, 137\nLimit exposure tactic, 152\nLimit structural complexity tactic, 167–168\nLinux, 531\nList-based publish-subscribe pattern, 229\nLoad balancers, 139\nLocal changes, 27–28\nLocal knowledge of markets in global devel-\nopment, 423\nLocalize state storage for testability, 165\nLocate tactic, 111\nLocation independence, 504\nLock computer tactic, 153\nLogical threads in concurrency, 13–14\nMacros for testability, 167\nMailing lists in publish-subscribe pattern, \n228\nMaintain multiple copies tactic, 142\nMaintain multiple copies of computations \ntactic, 139\nMaintain multiple copies of data tactic, 139\nMaintain system model tactic, 180\nMaintain task model tactic, 180\nMaintain user model tactic, 180\nMaintainability quality attribute, 195, 307\nMaintainers, 55\nMajor failures, 82\nManage event rate tactic, 142\nManage resources tactic, 137–139\nManage sampling rate tactic\nperformance, 137\nquality attributes, 72\nManagement and governance\narchitect skills, 464\ngovernance, 430–431\nimplementing, 427–429\nintroduction, 419\nmeasuring, 429–430\norganizing, 422–426\nplanning, 420–421\nsummary, 432\nManagement information in modules, 334\nManagers, communication with, 29\nManaging interfaces tactic, 111\nManifesto for Agile software development, \n276\nMap architectural strategies in CBAM, 446\nMap-reduce pattern, 232–235\nMapping\nto requirements, 355, 402–403\nto source code units, 334\nMapping among architectural elements \ncategory\nASRs, 293\navailability, 97\ninteroperability, 114\nmodifiability, 127\nperformance, 144\nquality design decisions, 75\nsecurity, 155\ntestability, 169\nusability, 182\nMaps, documentation, 347–349\nMarket position as business goal, 299\nMarketability category for quality, 190\nMarkov analysis, 83\nMatrixed team members, 422\nMcGregor, John, 448\nMean time between failures (MTBF), 80, \n255–259\nMean time to repair (MTTR), 80, 255–259\nMeasured services, 505\nMeasuring, 429–430\nMeetings\nglobal development, 426\nprogress tracking, 428\nMethods in product reuse, 484\n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 597,
      "content": "576 \nIndex\t\nMetrics, 429–430\nMetropolis structure\nedge-dominant systems, 528–530\nimplications, 533–537\nMicrosoft Azure, 517\nMicrosoft Office 365, 509\nMigrates-to relation, 14\nMill, John Stuart, 527\nMinimal cut sets, 83\nMinor failures, 82\nMissile defense system, 104\nMissile warning system, 192\nMixed initiative in usability, 177\nMobility attribute, 187\nModel driven development, 45\nModel-View-Controller (MVC) pattern\noverview, 212–215\nperformance analysis, 252–254\nuser interface, 178\nModels\nproduct reuse, 484\nquality attributes, 197–198\ntransferable and reusable, 35\nModifiability\nanalytic model space, 259\ncomponent-and-connector views, 337\ndesign checklist, 125–127\ngeneral scenario, 119–120\nintroduction, 117–119\nmanaging, 27\nping/echo, 243\nrestrict dependencies tactic, 246\nscheduling policy tactic, 244–245\nsummary, 128\ntactics, 121–125\nand time-to-market, 284\nunit testing, 371\nin usability, 179\nModularity of core, 532\nModules and module patterns, 10, 205–210\ncoupling, 121\ndecomposition structures, 5\ndescription, 4–5\ntypes, 12–13\nviews, 332–335, 406\nMongoDB database, 519\nMonitor relation in map-reduce patterns, 235\nMonitor tactic, 88–89\nMonitorability attribute, 188\nMoSCoW style, 292\nMSMQ product, 224\nMTBF (mean time between failures), 80, \n255–259\nMTTR (mean time to repair), 80, 255–259\nMulti-tenancy\ncloud, 509, 520\ndescription, 505\nMulti-tier patterns, 19, 235–237\nMultitasking, 132–133\nMusket production, 35–36\nMVC (Model-View-Controller) pattern\noverview, 212–215\nperformance analysis, 252–254\nuser interface, 178\nMythical Man-Month (Brooks), 47\nNameNode process, 512–513\nNames for modules, 333\nNASA ECS project, 451\narchitectural strategies, 452–456\nassign utility, 452\ncollate scenarios, 451\nexpected quality attribute response level, \n453\nprioritizing scenarios, 452\nrefining scenarios, 451–452\nNation as goal-object, 302\nNational Reconnaissance Office, 481\n.NET platform, 212\nNetflix\ncloud, 522\nSimian Army, 160–161\nNetwork administrators, 55\nNetworked services, 36\nNetworks, cloud, 514\nNightingale application, 306–307\nNo effect failures, 82\nNode managers, 516\nNokia, 480\nnon-ASR requirements, 312–313\nNon-stop forwarding (NSF) tactic, 94\nNondeterminism in testability, 168\nNonlocal changes, 27\nNonrepudiation in CIA approach, 148\nNonrisks in ATAM, 402\nNormalization\ndatabases, 520\neconomic analysis, 441–442\nNoSQL database systems, 518–520, 523\nNoSQL movement, 248\nNotations\ncomponent-and-connector views, 338–339\ndocumentation, 329–331\nNotifications\nfailures, 80\nModel-View-Controller pattern, 214\n",
      "content_length": 2527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 598,
      "content": "Index\n577\nNSF (non-stop forwarding) tactic, 94\nNumber of events not processed measure-\nment, 134\nObject-oriented systems\nin testability, 167\nuse cases, 46\nObjects in sequence diagrams, 352\nObservability of failures, 80\nObserve system state tactics, 164–167\nOff-the-shelf components, 36\nOmissions\navailability faults from, 85\nfor variation, 491\nOn-demand self-service, 504\n1+1 redundancy tactic, 91\nOnline documentation, 350\nOntologies, 368–369\nOPC (Order Processing Center) component, \n224, 226\nOpen content systems, 529\nOpen Group\ncertification program, 477\ngovernance responsibilities, 430–431\nOpen source software, 36, 238\nOperation Desert Storm, 104\nOR gate symbol, 84\nOrchestrate tactic, 111\nOrchestration servers, 223, 225\nOrder Processing Center (OPC) component, \n224, 226\nOrganization\nglobal development, 423–426\nproject manager and software architect \nresponsibilities, 422–423\nsoftware development teams, 422\nOrganizational Coordination model, 470, \n473, 476\nOrganizational Learning model, 470, 474, 476\nOrganizations\nactivities for success, 468\narchitect skills, 464\narchitecture influence on, 33\nas goal-object, 302\nsecurity processes, 157\nstructural strategies for products, 497\nOutages. See Availability\nOutputs\nADD method, 317–318\nATAM, 402–403\nOutsider analysis, 399\nOverlay views, 343\nOverloading for variation, 491\nOverview presentations in PALM method, \n305\nP2P (peer-to-peer) pattern, 220–222\nPaaS (Platform as a Service) model, 505, 517\nPage mappers, 510–512\nPALM (Pedigreed Attribute eLicitation \nMethod), 304–305\nParameter fence tactic, 90\nParameter typing tactic, 90\nParameters for variation mechanism, 492\nParser tool, 386\nPartitioning CAP theorem, 523\nPartnership and preparation phase in ATAM, \n403–404\nPassive redundancy, 91–92, 256–259\nPatterns, 18–19\nallocation, 232–237\ncomponent-and-connector. See \nComponent-and-connector (C&C) \npatterns and structures\ndocumenting, 350–351\nintroduction, 203–204\nmodule, 205–210\nrelationships, 204–205\nsummary, 247–248\nand tactics, 238–247, 315\nPaulish, Dan, 420\nPause/resume command, 179\nPayment Card Industry (PCI), 260\nPDF (probability density function), 255\nPDM (platform-definition model), 45\nPedigree and value component of business \ngoals, 301\nPedigreed Attribute eLicitation Method \n(PALM), 304–305\nPeer nodes, 220\nPeer review, 398–399\nPeer-to-peer (P2P) pattern, 220–222\nPenalties in Incremental Commitment Model, \n286\nPeople\nmanaging, 464\nin product reuse, 485\nPerfect vs. good enough, 398\nPerformance\nanalytic model space, 259\nanalyzing, 252–255\nbroker pattern, 241\ncloud, 521\ncomponent-and-connector views, 336\ncontrol resource demand tactics, 137–138\ndesign checklist, 142–144\n",
      "content_length": 2656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 599,
      "content": "578 \nIndex\t\nPerformance, continued\ngeneral scenario, 132–134\nintroduction, 131–132\nmanage resources tactics, 138–139\nmap-reduce pattern, 232\nping/echo, 243\nand quality, 191\nquality attributes tactics, 72\nqueuing models for, 198–199\nresource effects, 244, 246\nsummary, 145\ntactics overview, 135–137\nviews, 341\nPerformance quality attribute, 307\nPerformance efficiency quality attribute, 193\nPeriodic events, 133\nPeriphery\nMetropolis model, 535\nrequirements, 532\nPersistent object managers, 515–516\nPersonal objectives as business goal, 298\nPersonnel availability in ADD method, 320\nPetrov, Stanislav Yevgrafovich, 192\nPhases\nATAM, 403–404\nmetrics, 430\nMetropolis model, 534\nPhilips product lines, 480–481, 487\nPhysical security, 191\nPIM (platform-independent model), 45\nPing/echo tactic, 87–88, 243\nPipe-and-filter pattern, 215–217\nPlanned increments, 530\nPlanning\nfor failure, 82–85\nincremental development, 428\noverview, 420–421\ntests, 374\nPlatform as a Service (PaaS) model, 505, 517\nPlatform-definition model (PDM), 45\nPlatform-independent model (PIM), 45\nPlatforms\narchitect knowledge about, 467\nframeworks in, 365\npatterns, 19, 238\nservices for, 532–533\nPlug-in architectures, 34\nPMBOK (Project Management Body of \nKnowledge), 423–425\nPointers, smart, 95\nPolicies, scheduling, 140\nPooling resources, 504\nPortability quality attributes, 67, 186, 195\nPortfolio as goal-object, 302\nPorts in component-and-connector views, \n335, 337–338\nPotential alternatives, 398\nPotential problems, peer review for, 399\nPotential quality attributes, 305\nPower station catastrophe, 188, 192\nPredicting system qualities, 28\nPredictive model tactic, 95\nPreemptible processes, 141\nPreparation-and-repair tactic, 91–93\nPreprocessor macros, 167\nPresentation\nATAM, 402–406\ndocumentation, 346\nLightweight Architecture Evaluation, 416\nPALM method, 305\nQAW, 295\nPrevent faults tactics, 94–95\nPrimary presentations in documentation, 346\nPrinciples\nAgile, 276–277\ncloud failures, 522\ndesign fragments from, 72\nIncremental Commitment Model, 286\nPrioritize events tactic, 137–138, 142\nPrioritizing\nATAM scenarios, 410\nCBAM scenarios, 445–446\nCBAM weighting, 444\nLightweight Architecture Evaluation \nscenarios, 416\nNASA ECS project scenarios, 452\nQAW, 295–296\nrisk, 429\nschedules, 140–141\nviews, 343\nPRIORITY AND gate symbol, 84\nPrivate clouds, 506\nPrivate IP addresses, 514\nProactive enforcement in Metropolis model, \n535\nProactive product line models, 495\nProbability density function (PDF), 255\nProbability for availability, 256–259\nProblem relationships in patterns, 204–205\nProceedings scribes, 401\nProcesses\ndevelopment, 44–45\nproduct reuse, 484\nrecommendations, 20\nsecurity, 157\nProcessing time in performance, 136\n",
      "content_length": 2694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 600,
      "content": "Index\n579\nProcurement management, 425\nProduct-line managers, 55\nProduct lines. See Software product lines\nProduct manager roles, 422\nProductivity metrics, 429–430\nProfessional context, 51–52, 58\nProfiler tools, 386\nProgramming knowledge of architects, 466\nProject context, 57\nProject life-cycle context\narchitecturally significant requirements, \n46–47\narchitecture analysis and evaluation, 47–48\narchitecture documentation and \ncommunication, 47\narchitecture selection, 47\nbusiness cases, 46\ndevelopment processes, 44–45\nimplementation conformance, 48\nProject Management Body of Knowledge \n(PMBOK), 423–425\nProject managers\ndescription and interests, 55\nresponsibilities, 422–423\nProject planning artifacts in product reuse, 484\nPropagation costs of change, 288\nProsumers in edge-dominant systems, 529\nProtection groups, 91\nPrototypes\nevolutionary, 33–34\nquality attribute modeling and analysis, \n264–265\nfor requirements, 47\nProvides-services role, 335\nProxy servers, 146, 211\nPublic clouds, 506\nPublic IP addresses, 514\nPublicly available apps, 36\nPublish-subscribe connector, 336\nPublish-subscribe pattern, 226–229\nPublisher role, 336\nQAW (Quality Attribute Workshop), 294–296\nQt framework, 215\nQuality attribute modeling and analysis, \n251–252\nanalytic model space, 259–260\navailability analysis, 255–259\nchecklists, 260–262\nexperiments, simulations, and prototypes, \n264–265\nlife cycle stages, 265–266\nperformance analysis, 252–255\nsummary, 266–267\nthought experiments and back-of-the-\nenvelope analysis, 262–264\nQuality Attribute Workshop (QAW), 294–296\nQuality attributes, 185\nADD method, 322–323\nASRs, 294–296\nATAM, 407\ncapture scenarios, 196–197\ncategories, 189–190\nchecklists, 199, 260–262\nconsiderations, 65–67\ndesign approaches, 197\nand documentation, 354–355\ngrand unified theory, 261\nimportant, 185–188\ninhibiting and enabling, 26–27\nintroduction, 63–64\nLightweight Architecture Evaluation, 416\nmodels, 197–198\nNASA ECS project, 453\npeer review, 398\nquality design decisions, 72–76\nrequirements, 64, 68–70\nsoftware and system, 190–193\nstandard lists, 193–196\nsummary, 76–77\ntactics, 70–72, 198–199\ntechnical context, 40–41\nvariability, 488–489\nX-ability, 196–199\nQuality design decisions, 72–73\nallocation of responsibilities, 73\nbinding time, 75–76\ncoordination models, 73–74\ndata models, 74\nelement mapping, 75\nresource management, 74–75\ntechnology choices, 76\nQuality management in global development, \n424\nQuality of products as business goal, 299\nQuality requirements, mapping decisions to, \n402–403\nQuality views, 340–341\nQuestioners on ATAM teams, 401\nQuestions for organizational competence, \n470, 472–474\nQueue sizes tactic, 139\nQueuing models for performance, 198–199, \n252–255\nQuick Test Pro tool, 172\n",
      "content_length": 2727,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 601,
      "content": "580 \nIndex\t\nRace conditions, 133\nRandom access in equipment utilization, 508\nRapid elasticity, 504–505\nRate monotonic prioritization strategy, 140\nRational Unified Process, 44\nRationale in documentation, 347, 349\nRaw view extraction in reconstruction, \n382–386\nRDBMSs (relational database management \nsystems), 518\nReact to attacks tactics, 153\nReactive enforcement in Metropolis model, \n536\nReactive product line models, 495\nReader role in component-and-connector \nviews, 335\nReconfiguration tactic, 93\nReconstruction and conformance, 380–381\ndatabase construction, 386–387\nfinding violations, 389–392\nguidelines, 392–393\nprocess, 381–382\nraw view extraction, 382–386\nsummary, 393–394\nview fusion, 388–389\nRecord/playback method for system state, \n165\nRecover from attacks tactics, 153–154\nRecover-from-faults tactics, 91–94\nReduce computational overhead tactic, 142\nReduce function in map-reduce pattern, \n232–235\nReduce overhead tactic, 138\nRedundancy tactics, 90, 256–259\nRefactor tactic, 124\nRefined scenarios\nNASA ECS project, 451–452\nQAW, 296\nReflection for variation, 491\nReflection pattern, 262\nRegistry of services, 225\nRegression testing, 372\nReintroduction tactics, 91, 93–94\nRejuvenation tactic, 95\nRelational database management systems \n(RDBMSs), 518\nRelations\nallocation views, 339–340\narchitectural structures, 14, 16–17\nbroker pattern, 211\nclient-server pattern, 218\ncomponent-and-connector views, 337\nconformance, 390\nin documentation, 346\nlayered pattern, 207\nmap-reduce patterns, 235\nModel-View-Controller pattern, 213\nmodular views, 333\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nview extraction, 384\nRelease strategy for documentation, 350\nReliability\ncloud, 507\ncomponent-and-connector views, 336\ncore, 532\nindependently developed components \nfor, 36\nvs. safety, 188\nSOAP, 109\nviews, 341\nReliability quality attribute, 195\nRemote procedure call (RPC) model, 109\nRemoval from service tactic, 94–95\nReplicated elements in variation, 491\nReplication tactic, 90\nReport method for system state, 165\nReporting tests, 374\nRepository patterns, 19\nRepresentation of architecture, 6\nRepresentational State Transfer (REST), \n108–110, 223–225\nReputation of products as business goal, 299\nRequest/reply connectors\nclient-server pattern, 218\npeer-to-peer pattern, 222\nRequirements\nASRs. See Architecturally significant \nrequirements (ASRs)\ncategories, 64–65\nfrom goals, 49\nmapping to, 355, 402–403\nMetropolis model, 534\nproduct reuse, 483\nprototypes for, 47\nquality attributes, 68–70\nsoftware development life cycle changes, \n530\nsummary, 308–310\ntying methods together, 308\nRequirements documents\nASRs from, 292–293\n",
      "content_length": 2762,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 602,
      "content": "Index\n581\nWaterfall model, 56\nReset method for system state, 165\nResisting attacks tactics, 152–153\nRESL scale factor, 279\nResource management category\nASRs, 293\navailability, 97\ninteroperability, 115\nmodifiability, 127\nperformance, 144\nquality design decisions, 74–75\nsecurity, 155\nsoftware development life cycle changes, \n530\ntestability, 170\nusability, 182\nResources\ncomponent-and-connector views, 336\nequipment utilization, 508\npooling, 504\nsandboxing, 166\nsoftware development life cycle changes, \n530\nResponse\navailability, 85–86\ninteroperability, 105, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntestability, 162–163\nusability, 176\nvariability, 489\nResponse measure\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 149–150\ntestability, 162–163\nusability, 176\nvariability, 489\nResponsibilities\nas business goal, 299\nmodules, 333\nquality design decisions, 73\nREST (Representational State Transfer), \n108–110, 223–225\nRestart tactic, 94\nRestrict dependencies tactic, 124, 239, \n246–247\nRestrictions on vocabulary, 36\nResults\nATAM, 411\nCBAM, 447, 456–457\nevaluation, 400\nLightweight Architecture Evaluation, 416\nRetry tactic, 93\nReusable models, 35\nReuse of software architecture, 479, 483–486\nReviews\nback door, 544–545\npeer, 398–399\nRevision history of modules, 334\nRevoke access tactic, 153\nRework in agility, 279\nRisk\nADD method, 320\nATAM, 402\nglobal development, 425\nprogress tracking, 429\nRisk-based testing, 373–374\nRobustness of core, 532\nRoles\ncomponent-and-connector views, 335\nproduct line architecture, 488–490\nsoftware development teams, 422\ntesting, 375–376\nRollback tactic, 92\nRound-robin scheduling strategy, 140–141\nRozanski, Nick, 170\nRPC (remote procedure call) model, 109\nRuntime conditionals, 492\nRutan, Burt, 159\nSaaS (Software as a Service) model, 505\nSafety\nchecklists, 260, 268\nuse cases, 46\nSafety attribute, 188\nSafety Integrity Level, 268\nSalesforce.com, 509\nSample technologies in cloud, 514–520\nSampling rate tactic, 137\nSandbox tactic, 165–166\nSanity checking tactic, 89\nSatisfaction in usability, 175\nSaxe, John Godfrey, 379\nScalability\nkinds, 187\npeer-to-peer systems, 220\nWebArrow web-conferencing system, 285\nScalability attribute, 187\nScaling, automatic, 516\nScenario scribes, 401\n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 603,
      "content": "582 \nIndex\t\nScenarios\nATAM, 408, 410\navailability, 85–86\nbusiness goals, 301–303\nCBAM, 445–446\ninteroperability, 107–110\nLightweight Architecture Evaluation, 416\nmodifiability, 119–120\nNASA ECS project, 451–452\nperformance, 132–134\nQAW, 295–296\nquality attributes, 67–70, 196–197\nsecurity, 148–150\nfor structures, 12\ntestability, 162–163\nusability, 176\nweighting, 441, 444\nSchedule resources tactic\nperformance, 139\nquality attributes, 72\nScheduled downtimes, 81\nSchedulers, hypervisor, 512\nSchedules\ndeviation measurements, 429\nestimates, 34\npolicies, 140–141\npolicy tactic, 244–245\ntop-down and bottom-up, 420–421\nSchemas, database, 519\nScope, product line, 486–488\nScope and summary section in \ndocumentation maps, 347\nScrum development methodology, 44\nSDL (Specification and Description \nLanguage), 354\nSecurity\nanalytic model space, 259\nbroker pattern, 242\ncloud, 507, 520–521\ncomponent-and-connector views, 336\ndesign checklist, 154–156\ngeneral scenario, 148–150\nintroduction, 147–148\nping/echo, 243\nquality attributes checklists, 260\nsummary, 156\ntactics, 150–154\nviews, 341\nSecurity Monkey, 161\nSecurity quality attribute, 195, 307\nSEI (Software Engineering Institute), 59\nSelecting\narchitecture, 47\ntools and technology, 463\nSelenium tool, 172\nSelf-organization in Agile, 277\nSelf-test tactic, 91\nSemantic coherence, 178\nSemantic importance, 140\nSemiformal documentation notations, 330\nSensitivity points in ATAM, 403\nSeparate entities tactic, 153\nSeparation of concerns in testability, 167\nSequence diagrams\nthought experiments, 263\nfor traces, 351–352\nServers\nclient-server pattern, 217–219\nproxy, 146, 211\nSAO pattern, 223, 225\nService consumer components, 222, 225\nService discovery in SOAP, 108\nService impact of faults, 81\nService-level agreements (SLAs)\nAmazon, 81, 522\navailability in, 81\nIaaS, 506\nPaaS, 505\nSOA, 222\nService-oriented architecture (SOA) pattern, \n222–226\nService providers, 222–225\nService registry, 223\nService structure, 13\nServices for platforms, 532–533\nSet method for system state, 165\nShadow tactic, 93\nShared-data patterns, 19, 230–231\nShared documents in documentation, 350\nShareholders, responsibilities to, 299\nSiberian hydroelectric plant catastrophe, 188, \n192\nSiddhartha, Gautama, 251\nSide-channel attacks, 521\nSide effects in economic analysis, 439, 441\nSimian Army, 160–161\nSimulations, 264–265\nSize\nmodules, 121\nqueue, 139\nSkeletal systems, 34\nSkeletal view of human body, 9\nSkills\narchitects, 460, 463, 465\nglobal development, 423\nprofessional context, 51\n",
      "content_length": 2507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 604,
      "content": "Index\n583\nSLAs. See Service-level agreements (SLAs)\nSmall victories, 544\nSmart pointers, 95\nSOA (service-oriented architecture) pattern, \n222–226\nSOAP\nvs. REST, 108–110\nSOA pattern, 223–225\nSocial networks in publish-subscribe pattern, \n229\nSocializing in Incremental Commitment \nModel, 286\nSociety\nas goal-object, 302\nservice to, 299\nSoftware architecture importance, 25–26\nchange management, 27–28\nconstraints, 32–33\ncost and schedule estimates, 34\ndesign decisions, 31–32\nevolutionary prototyping, 33–34\nindependently developed components, \n35–36\norganizational structure, 33\nquality attributes, 26–27\nstakeholder communication, 29–31\nsummary, 37\nsystem qualities prediction, 28\ntraining basis, 37\ntransferable, reusable models, 35\nvocabulary restrictions, 36\nSoftware architecture overview, 3–4. See also \nArchitecture\nas abstraction, 5–6\nbehavior in, 6–7\ncompetence, 467–475\ncontexts. See Contexts\ndefinitions, 4\ngood and bad, 19–21\npatterns, 18–19\nselecting, 7\nas set of software structures, 4–5\nstructures and views, 9–18\nsummary, 21–22\nsystem architecture vs. enterprise, 7–8\nSoftware as a Service (SaaS) model, 505\nSoftware Engineering Body of Knowledge \n(SWEBOK), 292\nSoftware Engineering Institute (SEI), 59, 479\nSoftware Product Line Conference (SPLC), \n498\nSoftware Product Line Hall of Fame, 498\nSoftware product lines\nadoption strategies, 494–496\nevaluating, 493–494\nevolving, 496–497\nfailures, 481–482\nintroduction, 479–481\nkey issues, 494–497\norganizational structure, 497\nquality attribute of variability, 488\nreuse potential, 483–486\nrole of, 488–490\nscope, 486–488\nsuccessful, 483–486\nsummary, 497–498\nvariability, 482–483\nvariation mechanisms, 490–493\nSoftware quality attributes, 190–193\nSoftware rejuvenation tactic, 95\nSoftware upgrade tactic, 92–93\nSolutions in relationships, 204–205\nSonarJ tool, 387–391\nSorting in map-reduce pattern, 232\nSoS (system of systems), 106\nSource code\nKSLOC, 279–281\nmapping to, 334\nSource in security scenario, 150\nSource of stimulus\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 148\ntestability, 162–163\nusability, 176\nvariability, 489\nSpare tactics, 91–92, 256–259\nSpecialized interfaces tactic, 165\nSpecification and Description Language \n(SDL), 354\nSpikes in Agile, 284–285\nSPLC (Software Product Line Conference), 498\nSplit module tactic, 123\nSporadic events, 133\nSpring framework, 166\nStaging views, 343\nStakeholders\non ATAM teams, 401\ncommunication among, 29–31, 329\ndocumentation for, 348–349\nevaluation process, 400\ninterests, 52–55\ninterviewing, 294–296\n",
      "content_length": 2608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 605,
      "content": "584 \nIndex\t\nStakeholders, continued\nfor methods, 272\nutility tree reviews, 306\nviews, 342\nStandard lists for quality attributes, 193–196\nStandards and interoperability, 112–113\nState, system, 164–167\nState machine diagrams, 353\nState resynchronization tactic, 93\nStateless services in cloud, 522\nStates, responsibilities to, 299\nStatic allocation views, 340\nStatic scheduling, 141\nStatus meetings, 428\nStein, Gertrude, 142\nSteinberg, Saul, 39\nStimulus\navailability, 85–86\ninteroperability, 107–108\nmodifiability, 119–120\nperformance, 134\nquality attributes expressions, 68–70\nsecurity, 148, 150\nsource. See Source of stimulus\ntestability, 162–163\nusability, 176\nvariability, 489\nStochastic events, 133\nStonebraker, Michael, 518\nStorage\nfor testability, 165\nvirtualization, 512–513\nStrategies in NASA ECS project, 452–456\nStrictly layered patterns, 19\nStructural complexity in testability, 167–168\nStructure101 tool, 387\nStuxnet virus, 80\nSubarchitecture in component-and-connector \nviews, 335\nSubmodules, 333\nSubscriber role, 336\nSubsystems, 9\nSupernodes in peer-to-peer pattern, 220\nSupport and development software, 358–359\nSupport system initiative tactic, 180–181\nSupport user initiative tactic, 179–180\nSWEBOK (Software Engineering Body of \nKnowledge), 292\nSwing classes, 215\nSyncing code and architecture, 368\nSystem analysis and construction, \ndocumentation for, 329\nSystem architecture vs. enterprise \narchitecture, 7–8\nSystem as goal-object, 302\nSystem availability requirements, 81\nSystem efficiency in usability, 175\nSystem engineers, 55\nSystem exceptions tactic, 90\nSystem Generation Module, 358\nSystem initiative in usability, 177\nSystem of systems (SoS), 106\nSystem overview in documentation, 349\nSystem qualities, predicting, 28\nSystem quality attributes, 190–193\nSystem test manager roles, 422\nSystem testing, 371\nTactics\navailability, 87–96\ninteractions, 242–247\ninteroperability, 110–113\nmodifiability, 121–125\npatterns relationships with, 238–242\nperformance, 135–142\nquality attributes, 70–72, 198–199\nsecurity, 150–154\ntestability, 164–168\nusability, 177–181\nTailor interface tactic, 111\nTeam building skills, 463, 465\nTeam leader roles, 422\nTeamCity tool, 172\nTeams\nATAM, 400–401\norganizing, 422\nTechnical contexts\narchitecture influence, 57\nenvironment, 41–42\nquality attributes, 40–41\nVasa ship, 42–43\nTechnical debt, 286\nTechnical processes in security, 157\nTechnology choices, 76\nTechnology knowledge of architects, 467\nTemplates\nATAM, 406\ncode, 365–367\nscenarios. See Scenarios\nvariation mechanism, 492\n10-18 Monkey, 161\nTerminating generate and test process, 316\nTerms and concepts, 368–369\nTest harnesses, 160\n",
      "content_length": 2639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 606,
      "content": "Index\n585\nTestability\nanalytic model space, 259\nautomation, 171–172\nbroker pattern, 241\ndesign checklist, 169–170\ngeneral scenario, 162–163\nintroduction, 159–162\nsummary, 172\ntactics, 164–168\ntest data, 170–171\nTestable requirements, 292\nTestComplete tool, 172\nTesters, 55\nTests and testing\nactivities, 374–375\narchitect role, 375–376, 463\nblack-box and white-box, 372–373\nchoices, 315\nin incremental development, 428\nlevels, 370–372\nmodules, 334\nproduct reuse, 484\nrisk-based, 373–374\nsummary, 376\nTherac-25 fatal overdose, 192\nThought experiments, 262–264\nThousands of source lines of code (KSLOC), \n279–281\nThreads in concurrency, 132–133\nThroughput of systems, 134\nTiers\ncomponent-and-connector views, 337\nmulti-tier pattern, 235–237\nTime and time management\nbasis sets, 261\nglobal development, 424\nperformance, 131\nTime boxing, 264\nTime of day factor in equipment utilization, 508\nTime of year factor in equipment utilization, \n508\nTime-sharing, 503\nTime stamp tactic, 89\nTime to market\nindependently developed components \nfor, 36\nand modifiability, 284\nTimeout tactic, 91\nTiming in availability, 85\nTMR (triple modular redundancy), 89\nTools\nfor product reuse, 484\nselecting, 463\nTop-down adoption, 495\nTop-down analysis mode, 284\nTop-down schedules, 420–421\nTopic-based publish-subscribe patterns, 229\nTopological constraints, 236\nTorvalds, Linus, 530, 535, 538\nTotal benefit in CBAM, 446\nTraces for behavior documentation, 351–353\nTracking progress, 428–429\nTradeoffs\nATAM, 403\nimplementation, 427\nTraffic systems, 142\nTraining, architecture for, 37\nTransactions\navailability, 95\ndatabases, 519–520\nSOAP, 108\nTransferable models, 35\nTransformation systems, 215\nTransforming existing systems, 462\nTransitions in state machine diagrams, 354\nTriple modular redundancy (TMR), 89\nTroeh, Eve, 190\nTurner, R., 279, 281, 288\nTwitter, 528\nTwo-phase commits, 95\nUbiquitous network access, 504\nUDDI (Universal Description, Discovery and \nIntegration) language, 108\nUML\nactivity diagrams, 353\ncommunication diagrams, 353\ncomponent-and-connector views, \n338–339\nconnectors, 369\nsequence diagrams, 351–352\nstate machine diagrams, 353\nUnambiguous requirements, 292\nUncertainty in equipment utilization, \n508–509\nUndo command, 179\nUnified Process, 44\nUnit testing, 370–371\nUnity of purpose in modules, 121\nUniversal Description, Discovery and \nIntegration (UDDI) language, 108\nUp-front planning vs. agility, 278–281\nUsability\nanalytic model space, 259\ndesign checklist, 181–182\ngeneral scenario, 176\n",
      "content_length": 2491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 607,
      "content": "586 \nIndex\t\nUsability, continued\nintroduction, 175\nquality attributes checklists, 260\ntactics, 177–181\nUsability quality attribute, 193, 307\nUsage\nallocation views, 339\ncomponent-and-connector views, 337\nmodular views, 333\nUse an intermediary tactic, 245\nmodifiability, 123\nquality attributes, 72\nUse cases\nATAM presentations, 406\nthought experiments, 263\nfor traces, 351\n“User beware” proviso, 372\nUser initiative in usability, 177\nUser interface\nexchanging information via, 104–105\nseparating, 178\nUser needs in usability, 175\nUser stories in Agile, 278\nUsers\ncommunication with, 29\ndescription and interests, 55\nUses\nfor documentation, 328–329\nviews for, 332\nUses relation in layered patterns, 19\nUses structure in decomposition, 12\nUtility\nassigning, 452\nCBAM, 448\nUtility-response curves, 439–443\nUtility trees\nASRs, 304–307\nATAM, 407, 410\nLightweight Architecture Evaluation, 416\nUtilization of equipment in cloud, 508–509\nValue component\nbusiness goals, 301\nutility trees, 306\nValue for cost (VFC), 438, 442\nVariability\nproduct line, 482–483\nquality attributes, 488–489\nVariability attribute, 186\nVariability guides, 347, 493\nVariation\nbinding time, 75\nsoftware product lines, 490–493\nVariation points\nCBAM, 448–450\nidentifying, 490\nVasa ship, 42–43\nVascular view of human body, 9\nVehicle cruise control systems, 353\nVerify and refine requirements in ADD, \n321–323\nVerify message integrity tactic, 151\nVertical scalability, 187\nVFC (value for cost), 438, 442\nViews, 331–332\nallocation, 339–340\narchitectural structures, 9–10\nchoosing, 341–343\ncombining, 343–345\ncomponent-and-connector, 335–339, 344, \n406\ndocumenting, 345–347\nfused, 388–389\nModel-View-Controller pattern, 213–214\nmodule, 332–335, 406\nquality, 340–341\nViews and Beyond approach, 282, 356–357\nVilla, Pancho, 541\nViolations, finding, 389–392\nVirtual resource managers, 515\nVirtual system of systems, 106\nVirtualization and virtual machines\ncloud, 509–514, 520–521\nlayers as, 13\nin sandboxing, 166\nVisibility of interfaces, 333\nVitruvius, 459\nVlissides, J., 212\nVocabulary\nquality attributes, 67\nrestrictions, 36\nVoting tactic, 89\nVulnerabilities in security views, 341\nWalking skeleton method, 287\nWar ship example, 42–43\nWarm spare tactic, 91–92\nWatchdogs, 89\nWaterfall model\ndescription, 44\nrequirements documents, 56\nWeaknesses\nbroker pattern, 211, 240–242\nclient-server pattern, 218\nlayered pattern, 207\nmap-reduce patterns, 235\n",
      "content_length": 2405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 608,
      "content": "Index\n587\nModel-View-Controller pattern, 213\nmulti-tier pattern, 237\npeer-to-peer pattern, 222\npipe-and-filter pattern, 216\npublish-subscribe pattern, 227\nservice-oriented architecture pattern, 225\nshared-data pattern, 231\nWealth of Networks (Benkler), 528\nWeb 2.0 movement, 527\nWeb-based system events, 131\nWeb-conferencing systems\nAgile example, 283–285\nconsiderations, 265\nWeb Services Description Language \n(WSDL), 110\nWebArrow web-conferencing system, \n284–285\nWebSphere MQ product, 224\nWeighting scenarios, 441, 444\nWells, H. G., 117\nWest, Mae, 131\n“What if” questions in performance \nanalysis, 255\nWhite-box testing, 372–373\nWhitney, Eli, 35–36, 480\nWikipedia, 528\nWikis for documentation, 350\nWisdom of crowds, 537\nWoods, Eoin, 25, 170\nWork assignment structures, 14\nWork-breakdown structures, 33\nWork skills of architect, 465\nWorld Wide Web as client-server pattern, 219\nWrappers, 129\nWriter role in component-and-connector \nviews, 335\nWS*, 108–110\nWSDL (Web Services Description \nLanguage), 110\nX-ability, 196–199\nX-ray view of human body, 9\nYAGNI principle, 282\nYahoo! map-reduce patterns, 234\nYoung, Toby, 39\nYouTube, 528\nZoning policies analogy in Metropolis \nmodel, 536\n",
      "content_length": 1184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 609,
      "content": "588\nSpecial permission to reproduce portions of the following works copyright by \nCarnegie Mellon University is granted by the Software Engineering Institute:\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Software Architecture Documentation \nin Practice: Documenting Architectural Layers,” CMU/SEI-2000-SR-004, March \n2000.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architectures: \nOrganization of Documentation Package,” CMU/SEI-2001-TN-010, August \n2001.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architecture: \nDocumenting Behavior,” CMU/SEI-2002-TN-001, January 2002.\nFelix Bachmann, Len Bass, Paul Clements, David Garlan, James Ivers, Reed Lit-\ntle, Robert Nord, and Judith A. Stafford. “Documenting Software Architecture: \nDocumenting Interfaces,” CMU/SEI-2002-TN-015, June 2002.\nFelix Bachmann and Paul Clements. “Variability in Product Lines,” CMU/SEI-\n2005-TR-012, September 2005. \nFelix Bachmann, Len Bass, and Robert Nord. “Modifiability Tactics,” CMU/SEI-\n2007-TR-002, September 2007.\nMario R. Barbacci, Robert Ellison, Anthony J. Lattanze, Judith A. Stafford, \nCharles B. Weinstock, and William G. Wood. “Quality Attribute Workshops \n(QAWs), Third Edition,” CMU/SEI-2003-TR-016, August 2003.\nLen Bass, Paul Clements, Rick Kazman, and Mark Klein. “Models for Evaluat-\ning and Improving Architecture Competence,” CMU/SEI-2008-TR-006, March \n2008.\nLen Bass, Paul Clements, Rick Kazman, John Klein, Mark Klein, and Jeannine \nSiviy. “A Workshop on Architecture Competence,” CMU/SEI-2009-TN-005, \nApril 2009.\nLisa Brownsword, David Carney, David Fisher, Grace Lewis, Craig Meyers, Ed-\nwin Morris, Patrick Place, James Smith, and Lutz Wrage. “Current Perspectives \non Interoperability,” CMU/SEI-2004-TR-009, March 2004.\nPaul Clements and Len Bass. “Relating Business Goals to Architecturally Signif-\nicant Requirements for Software Systems,” CMU/SEI-2010-TN-018, May 2010.\nRick Kazman and Jeromy Carriere, “Playing Detective: Reconstructing Software \nArchitecture from Available Evidence,” CMU/SEI-97-TR-010, October 1997.\n",
      "content_length": 2304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 610,
      "content": "589\nRick Kazman, Mark Klein, and Paul Clements. “ATAM: Method for Architecture \nEvaluation,” CMU/SEI-2000-TR-004, August 2000.\nRick Kazman, Jai Asundi, and Mark Klein, “Making Architecture Design Deci-\nsions, An Economic Approach,” CMU/SEI-2002-TR-035, September 2002.\nRick Kazman, Liam O’Brien, and Chris Verhoef, “Architecture Reconstruction \nGuidelines, Third Edition,” CMU/SEI-2002-TR-034, November 2003. \nRobert L. Nord, Paul C. Clements, David Emery, and Rich Hilliard. “A Structured \nApproach for Reviewing Architecture Documentation,” CMU/SEI-2009-TN-030, \nDecember 2009.\nJames Scott and Rick Kazman. “Realizing and Refining Architectural Tactics: \nAvailability,” CMU/SEI-2009-TR-006 and ESC-TR-2009-006, August 2009.\n",
      "content_length": 726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 611,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 612,
      "content": "Build Your Credentials with the SEI Software \nArchitecture Professional Certificate\nAs a software architect, you know \nthat complexity is rampant and always \nincreasing. It simply takes more \nskills now to deliver the richly featured, \nhigh-performing products that \ncustomers demand. \nThe SEI Software Architecture \nProfessional Certificate helps you \nkeep pace. From this four-course \nsequence, you’ll gain the ability to \n• apply architecture-centric\n practices throughout the life cycle\n•  produce and understand documen-\ntation for software architecture\n• understand the relationships among\n system qualities such as security \n and performance, architecture, and \n your organization’s business goals \nTake the first step in building credentials \nby registering for the SEI Software \nArchitecture: Principles and Practices \ncourse. Visit www.sei.cmu.edu/go/\nsapptraining/ for details.  \nThe SEI also offers an Architecture \nTradeoff Analysis Method® (ATAM®) \nEvaluator Certificate and an ATAM \nLeader Certification. \n \nFor more information on the Software \nArchitecture Professional Certificate \nand other SEI software architecture \ncredentials, visit www.sei.cmu.edu/go/\narchitecture-credentials/.\n \nArchitecture Tradeoff Analysis Method and ATAM  \nare registered in the U.S. Patent and Trademark Office  \nby Carnegie Mellon University. \n",
      "content_length": 1343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 613,
      "content": "Register the Addison-Wesley, Exam \nCram, Prentice Hall, Que, and \nSams products you own to unlock \ngreat beneﬁ ts. \nTo begin the registration process, \nsimply go to informit.com/register \nto sign in or create an account. \nYou will then be prompted to enter \nthe 10- or 13-digit ISBN that appears \non the back cover of your product.\ninformIT.com \nTHE TRUSTED TECHNOLOGY LEARNING SOURCE\nAddison-Wesley  |  Cisco Press  |  Exam Cram  \nIBM Press   |   Que   |   Prentice Hall   |   Sams \nSAFARI BOOKS ONLINE\nAbout InformIT — THE TRUSTED TECHNOLOGY LEARNING SOURCE\nINFORMIT IS HOME TO THE LEADING TECHNOLOGY PUBLISHING IMPRINTS \nAddison-Wesley Professional, Cisco Press, Exam Cram, IBM Press, Prentice Hall \nProfessional, Que, and Sams. Here you will gain access to quality and trusted content and \nresources from the authors, creators, innovators, and leaders of technology. Whether you’re \nlooking for a book on a new technology, a helpful article, timely newsletters, or access to \nthe Safari Books Online digital library, InformIT has a solution for you.\nRegistering your products can unlock \nthe following beneﬁ ts:\n•  Access to supplemental content, \nincluding bonus chapters, \nsource code, or project ﬁ les. \n•  A coupon to be used on your \nnext purchase.\nRegistration beneﬁ ts vary by product.  \nBeneﬁ ts will be listed on your Account \npage under Registered Products.\ninformit.com/register\nTHIS PRODUCT\n",
      "content_length": 1407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 614,
      "content": " InformIT is a brand of Pearson and the online presence \nfor the world’s leading technology publishers. It’s your source \nfor reliable and qualified content and knowledge, providing \naccess to the top brands, authors, and contributors from \nthe tech community.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nLearnIT at InformIT\nLooking for a book, eBook, or training video on a new technology? Seek-\ning timely and relevant information and tutorials? Looking for expert opin-\nions, advice, and tips?  InformIT has the solution.\n•  Learn about new releases and special promotions by \nsubscribing to a wide variety of newsletters. \nVisit informit.com/newsletters.\n•   Access FREE podcasts from experts at informit.com/podcasts.\n•   Read the latest author articles and sample chapters at \ninformit.com/articles.\n•  Access thousands of books and videos in the Safari Books \nOnline digital library at safari.informit.com.\n• Get tips from expert blogs at informit.com/blogs.\nVisit informit.com/learn to discover all the ways you can access the \nhottest technology content.\ninformIT.com THE TRUSTED TECHNOLOGY LEARNING SOURCE\nAre You Part of the IT Crowd?\nConnect with Pearson authors and editors via RSS feeds, Facebook, \nTwitter, YouTube, and more! Visit informit.com/socialconnect.\n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 615,
      "content": "Sample concrete interoperability scenario\nStimulus:\nResponse:\nEnvironment:\nSystems known\nprior to run-time\nArtifact:\nResponse\nMeasure:\nSource\nof Stimulus:\n3\n2\n1\n4\nOur Vehicle \nInformation \nSystem\nCurrent\nLocation\nSent\nTraffic Monitor \nCombines Current \nLocation with Other \nInformation, \nOverlays on Google \nMaps, and \nBroadcasts\nOur Information \nIncluded Correctly \n99.9% of the Time\nTraffic Monitoring \nSystem\nSample concrete availability scenario\nStimulus:\nServer\nUnresponsive\nResponse:\nInform \nOperator\nContinue\nto Operate\nResponse\nMeasure:\nNo Downtime\nSource:\nHeartbeat\nMonitor\nArtifact:\nProcess\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSample concrete modiﬁability scenario\nStimulus:\nWishes\nto Change\nthe UI\nResponse:\nChange Made \nand Unit Tested \nSource:\nDeveloper\nArtifact:\nCode\nEnvironment:\n \nDesign\nTime\nResponse\nMeasure:\nIn Three\nHours\n3\n2\n1\n4\n",
      "content_length": 852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 616,
      "content": "Sample concrete security scenario \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nNormal\nOperations\n3\n2\n1\n4\nDisgruntled \nEmployee from \nRemote Location\nAttempts to \nModify Pay \nRate\nSystem\nMaintains\nAudit Trail\nCorrect Data Is \nRestored within a\nDay and Source \nof Tampering \nIdentified\nArtifact:\nData within\nthe System\nSample performance scenario\nStimulus:\nInitiate\nTransactions\nResponse:\nTransactions\nAre Processed\nResponse\nMeasure:\nAverage\nLatency\nof Two\nSource:\nUsers\nArtifact:\nSystem\nEnvironment:\nNormal\nOperation\n3\n2\n1\n4\nSeconds\n",
      "content_length": 547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 617,
      "content": "Sample concrete testability scenario\nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nDevelopment\n3\n2\n1\n4\nUnit Tester\nCode Unit \nCompleted\nResults Captured\n85% Path Coverage \nin Three Hours\nArtifact:\nCode Unit\nSample concrete usability scenario \nStimulus:\nResponse:\nResponse\nMeasure:\nSource:\nEnvironment:\nRuntime\n3\n2\n1\n4\nUser\nDownloads \na New \nApplication\nUser Uses\nApplication\nProductively\nWithin Two\nMinutes of\nExperimentation\nArtifact:\nSystem\n",
      "content_length": 456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 618,
      "content": "Interoperability Tactics\nLocate\nManage Interfaces\nDiscover\nService\nOrchestrate\nTailor Interface\nInformation\nExchange\nRequest\nRequest\nCorrectly\nHandled\nAvailability Tactics\nDetect Faults\nPrevent Faults\nPing / Echo\nRemoval from\nService\nMonitor\nTransactions\nPredictive\nModel\nRecover from Faults\nHeartbeat\nPreparation\nand Repair\nReintroduction\nActive\nRedundancy\nPassive\nRedundancy\nSpare\nEscalating\nRestart\nException\nHandling\nShadow\nNon-Stop\nForwarding\nState\nResynchronization\nException\nPrevention\nFault\nFault\nMasked\nor\nRepair\nMade\nTimestamp\nSanity\nChecking\nCondition\nMonitoring\nVoting\nException\nDetection\nSelf-Test\nRollback\nSoftware\nUpgrade\nRetry\nIgnore Faulty\nBehavior\nDegradation\nReconfiguration\nIncrease\nCompetence Set\n",
      "content_length": 718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 619,
      "content": "Modifiability Tactics\nIncrease\nCohesion\nReduce\nCoupling\nSplit Module\nEncapsulate\nUse an\nIntermediary\nChange\nArrives\nChange Made\nwithin Time \nand Budget\nReduce Size\nof a Module\nIncrease\nSemantic\nCoherence\nRestrict\nDependencies\nRefactor\nAbstract Common\nServices\nDefer\nBinding\nPerformance Tactics\nControl Resource Demand\nManage Resources\nManage Sampling Rate\nLimit Event Response\nPrioritize Events\nReduce Overhead\nBound Execution Times\nIncrease Resource\nEfficiency\nEvent\nArrives\nResponse\nGenerated within\nTime Constraints\nIncrease Resources\nIntroduce Concurrency\nMaintain Multiple\nCopies of Computations\nMaintain Multiple\nCopies of Data\nBound Queue Sizes\nSchedule Resources\n",
      "content_length": 671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 620,
      "content": "Usability Tactics\nSupport User\nInitiative\nSupport System\nInitiative\nCancel\nMaintain User\nModel\nMaintain System\nUser\nRequest\nUser Given\nAppropriate\nFeedback and\nAssistance\nUndo\nPause/Resume\nAggregate\nMaintain Task\nModel\nTestability Tactics\nControl and Observe\nSystem State\nLimit Complexity\nSpecialized\nInterfaces\nLimit Structural\nComplexity\nLimit\nNondeterminism\nTests\nExecuted\nFaults\nDetected\nRecord/\nPlayback\nLocalize State\nStorage\nSandbox\nExecutable\nAssertions\nAbstract Data\nSources\nSecurity Tactics\nResist Attacks\nEncrypt Data\nAttack\nSystem Detects,\nResists, Reacts,\nor Recovers\nDetect Attacks\nMaintain\nAudit Trail\nLimit Exposure\nRecover\nfrom Attacks\nReact to\nAttacks\nRevoke\nAccess\nLock\nComputer\nDetect\nIntrustion\nDetect Service\nDenial\nVerify Message\nIntegrity\nDetect Message\nDelay\nChange Default\nSettings\nSeparate\nEntities\nRestore\nSee\nAvailability\nIdentify\nActors\nAuthenticate\nActors\nAuthorize\nActors\nLimit Access\nInform\nActors\n",
      "content_length": 931,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}