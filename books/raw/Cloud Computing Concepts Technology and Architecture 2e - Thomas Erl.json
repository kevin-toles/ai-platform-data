{
  "metadata": {
    "title": "Cloud Computing Concepts Technology and Architecture 2e - Thomas Erl",
    "author": "Thomas Erl, Eric Barcelo",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 1002,
    "conversion_date": "2025-12-19T17:21:44.338676",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Cloud Computing Concepts Technology and Architecture 2e - Thomas Erl.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 4-25)",
      "start_page": 4,
      "end_page": 25,
      "detection_method": "synthetic",
      "content": "Table of Contents\n\nChapter 1 Introduction\n\n1.1 Objectives of This Book\n\n1.2 What This Book Does Not Cover\n\n1.3 Who This Book Is For\n\n1.4 How This Book Is Organized\n\n1.5 Resources\n\nChapter 2 Case Study Background\n\n2.1 Case Study #1: ATN\n\n2.2 Case Study #2: DTGOV\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nChapter 3 Understanding Cloud Computing\n\n3.1 Origins and Influences\n\n3.2 Basic Concepts and Terminology\n\n3.3 Goals and Benefits\n\n3.4 Risks and Challenges\n\nPart I Fundamental Cloud Computing\n\nChapter 4 Fundamental Concepts and Models\n\n4.1 Roles and Boundaries\n\n4.2 Cloud Characteristics\n\n4.3 Cloud Delivery Models\n\n4.4 Cloud Deployment Models\n\nChapter 5 Cloud-Enabling Technology\n\n5.1 Networks and Internet Architecture\n\n5.2 Cloud Data Center Technology\n\n5.3 Modern Virtualization\n\n5.4 Multitenant Technology\n\n5.5 Service Technology and Service APIs\n\n5.6 Case Study Example\n\nChapter 6 Understanding Containerization\n\n6.1 Origins and Influences\n\n6.2 Fundamental Virtualization and Containerization\n\n6.3 Understanding Containers\n\n6.4 Understanding Container Images\n\n6.5 Multi-Container Types\n\n6.6 Case Study Example\n\nChapter 7 Understanding Cloud Security and Cybersecurity\n\n7.1 Basic Security Terminology\n\n7.2 Basic Threat Terminology\n\n7.3 Threat Agents\n\n7.4 Common Threats\n\n7.5 Case Study Example\n\n7.6 Additional Considerations\n\n7.7 Case Study Example\n\nPart II Cloud Computing Mechanisms\n\nChapter 8 Cloud Infrastructure Mechanisms\n\n8.1 Logical Network Perimeter\n\n8.2 Virtual Server\n\n8.3 Hypervisor\n\n8.4 Cloud Storage Device\n\n8.5 Cloud Usage Monitor\n\n8.6 Resource Replication\n\n8.7 Ready-Made Environment\n\n8.8 Container\n\nChapter 9 Specialized Cloud Mechanisms\n\n9.1 Automated Scaling Listener\n\n9.2 Load Balancer\n\n9.3 SLA Monitor\n\n9.4 Pay-Per-Use Monitor\n\n9.5 Audit Monitor\n\n9.6 Failover System\n\n9.7 Resource Cluster\n\n9.8 Multi-Device Broker\n\n9.9 State Management Database\n\nChapter 10 Cloud and Cybersecurity Access-Oriented Mechanisms\n\n10.1 Encryption\n\n10.2 Hashing\n\n10.3 Digital Signature\n\n10.4 Cloud-Based Security Groups\n\n10.5 Public Key Infrastructure (PKI) System\n\n10.6 Single Sign-On (SSO) System\n\n10.7 Hardened Virtual Server Image\n\n10.8 Firewall\n\n10.9 Virtual Private Network (VPN)\n\n10.10 Biometric Scanner\n\n10.11 Multi-Factor Authentication (MFA) System\n\n10.12 Identity and Access Management (IAM) System\n\n10.13 Intrusion Detection System (IDS)\n\n10.14 Penetration Testing Tool\n\n10.15 User Behavior Analytics (UBA) System\n\n10.16 Third-Party Software Update Utility\n\n10.17 Network Intrusion Monitor\n\n10.18 Authentication Log Monitor\n\n10.19 VPN Monitor\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nChapter 11 Cloud and Cyber Security Data-Oriented Mechanisms\n\n11.1 Digital Virus Scanning and Decryption System\n\n11.2 Digital Immune System\n\n11.3 Malicious Code Analysis System\n\n11.4 Data Loss Prevention (DLP) System\n\n11.5 Trusted Platform Module (TPM)\n\n11.6 Data Backup and Recovery System\n\n11.7 Activity Log Monitor\n\n11.8 Traffic Monitor\n\n11.9 Data Loss Protection Monitor\n\nChapter 12 Cloud Management Mechanisms\n\n12.1 Remote Administration System\n\n12.2 Resource Management System\n\n12.3 SLA Management System\n\n12.4 Billing Management System\n\nPart III Cloud Computing Architecture\n\nChapter 13 Fundamental Cloud Architectures\n\n13.1 Workload Distribution Architecture\n\n13.2 Resource Pooling Architecture\n\n13.3 Dynamic Scalability Architecture\n\n13.4 Elastic Resource Capacity Architecture\n\n13.5 Service Load Balancing Architecture\n\n13.6 Cloud Bursting Architecture\n\n13.7 Elastic Disk Provisioning Architecture\n\n13.8 Redundant Storage Architecture\n\n13.9 Multi-Cloud Architecture\n\n13.10 Case Study Example\n\nChapter 14 Advanced Cloud Architectures\n\n14.1 Hypervisor Clustering Architecture\n\n14.2 Virtual Server Clustering Architecture\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\n14.4 Non-Disruptive Service Relocation Architecture\n\n14.5 Zero Downtime Architecture\n\n14.6 Cloud Balancing Architecture\n\n14.7 Resilient Disaster Recovery Architecture\n\n14.8 Distributed Data Sovereignty Architecture\n\n14.9 Resource Reservation Architecture\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\n14.11 Rapid Provisioning Architecture\n\n14.12 Storage Workload Management Architecture\n\n14.13 Virtual Private Cloud Architecture\n\n14.14 Case Study Example\n\nChapter 15 Specialized Cloud Architectures\n\n15.1 Direct I/O Access Architecture\n\n15.2 Direct LUN Access Architecture\n\n15.3 Dynamic Data Normalization Architecture\n\n15.4 Elastic Network Capacity Architecture\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\n15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\n15.7 Load Balanced Virtual Switches Architecture\n\n15.8 Multipath Resource Access Architecture\n\n15.9 Persistent Virtual Network Configuration Architecture\n\n15.10 Redundant Physical Connection for Virtual Servers\n\nArchitecture\n\n15.11 Storage Maintenance Window Architecture\n\n15.12 Edge Computing Architecture\n\n15.13 Fog Computing Architecture\n\nPart IV Working with Clouds\n\nChapter 16 Cloud Delivery Model Considerations\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\n16.3 Case Study Example\n\nChapter 17 Cost Metrics and Pricing Models\n\n17.1 Business Cost Metrics\n\n17.2 Cloud Usage Cost Metrics\n\n17.3 Cost Management Considerations\n\nChapter 18 Service Quality Metrics and SLAs\n\n18.1 Service Quality Metrics\n\n18.2 Case Study Example\n\n18.3 SLA Guidelines\n\n18.4 Case Study Example\n\nPart V Appendices\n\nAppendix A Case Study Conclusions\n\nA.1 ATN\n\nA.2 DTGOV\n\nA.3 Innovartus\n\nAppendix B Common Containerization Technologies\n\nB.1 Docker\n\nB.2 Kubernetes\n\nChapter 1\n\nIntroduction\n\n1.1 Objectives of This Book\n\n1.2 What This Book Does Not Cover\n\n1.3 Who This Book Is For\n\n1.4 How This Book Is Organized\n\n1.5 Resources\n\nCloud computing is, at its essence, a form of service provisioning. As with\n\nany type of service we intend to hire or outsource (IT-related or otherwise),\n\nit is commonly understood that we will be confronted with a marketplace\n\ncomprised of service providers of varying quality and reliability. Some may\n\noffer attractive rates and terms, but may have unproven business histories or\n\nhighly proprietary environments. Others may have a solid business\n\nbackground, but may demand higher rates and less flexible terms. Others\n\nyet, may simply be insincere or temporary business ventures that\n\nunexpectedly disappear or are acquired within a short period of time.\n\nThere is no greater danger to a business than approaching cloud computing\n\nadoption with ignorance. The magnitude of a failed adoption effort not only\n\ncorrespondingly impacts IT departments, but can actually regress a business\n\nto a point where it finds itself steps behind from where it was prior to the\n\nadoption—and, perhaps, even more steps behind competitors that have been\n\nsuccessful at achieving their goals in the meantime.\n\nCloud computing has much to offer but its roadmap is riddled with pitfalls,\n\nambiguities, and mistruths. The best way to navigate this landscape is to\n\nchart each part of the journey by making educated decisions about how and\n\nto what extent your project should proceed. The scope of an adoption is\n\nequally important to its approach, and both of these aspects need to be\n\ndetermined by business requirements. Not by a product vendor, not by a\n\ncloud vendor, and not by self-proclaimed cloud experts. Your organization’s\n\nbusiness goals must be fulfilled in a concrete and measurable manner with\n\neach completed phase of the adoption. This validates your scope, your\n\napproach, and the overall direction of the project. In other words, it keeps\n\nyour project aligned.\n\nGaining a vendor-neutral understanding of cloud computing from an\n\nindustry perspective empowers you with the clarity necessary to determine\n\nwhat is factually cloud-related and what is not, as well as what is relevant to\n\nyour business requirements and what is not. With this information you can\n\nestablish criteria that will allow you to filter out the parts of the cloud\n\ncomputing product and service provider marketplaces to focus on what has\n\nthe most potential to help you and your business to succeed. We developed\n\nthis book to assist you with this goal.\n\n—Thomas Erl\n\n1.1 Objectives of This Book\n\nThis book is the result of much research and analysis of the commercial\n\ncloud computing industry, cloud computing vendor platforms, and further\n\ninnovation and contributions made by cloud computing industry standards\n\norganizations and practitioners. The purpose of this book is to break down\n\nproven and mature cloud computing technologies and practices into a series\n\nof well-defined concepts, models, and technology mechanisms and\n\narchitectures. The resulting chapters establish concrete, academic coverage\n\nof fundamental aspects of cloud computing concepts and technologies. The\n\nrange of topics covered is documented using vendor-neutral terms and\n\ndescriptions, carefully defined to ensure full alignment with the cloud\n\ncomputing industry as a whole.\n\n1.2 What This Book Does Not Cover\n\nDue to the vendor-neutral basis of this book, it does not contain any\n\nsignificant coverage of cloud computing vendor products, services, or\n\ntechnologies. This book is complementary to other titles that provide\n\nproduct-specific coverage and to vendor product literature itself. If you are\n\nnew to the commercial cloud computing landscape, you are encouraged to\n\nuse this book as a starting point before proceeding to books and courses that\n\nare proprietary to vendor product lines.\n\n1.3 Who This Book Is For\n\nThis book is aimed at the following target audience:\n\nIT practitioners and professionals who require vendor-neutral coverage of\n\ncloud computing technologies, concepts, mechanisms, and models\n\nIT managers and decision-makers who seek clarity regarding the business\n\nand technological implications of cloud computing\n\nprofessors and students and educational institutions that require well-\n\nresearched and well-defined academic coverage of fundamental cloud\n\ncomputing topics\n\nbusiness managers who need to assess the potential economic gains and\n\nviability of adopting cloud computing resources\n\ntechnology architects and developers who want to understand the different\n\nmoving parts that comprise contemporary cloud platforms\n\n1.4 How This Book Is Organized\n\nThe book begins with Chapters 1 and 2 providing introductory content and\n\nbackground information for the case studies. All subsequent chapters are\n\norganized into the following parts:\n\nPart I: Fundamental Cloud Computing\n\nPart II: Cloud Computing Mechanisms\n\nPart III: Cloud Computing Architecture\n\nPart IV: Working with Clouds\n\nPart V: Appendices\n\nPart I: Fundamental Cloud Computing\n\nThe five chapters in this part cover introductory topics in preparation for all\n\nsubsequent chapters. Note that Chapters 3 and 4 do not contain case study\n\ncontent.\n\nChapter 3: Understanding Cloud Computing\n\nFollowing a brief history of cloud computing and a discussion of business\n\ndrivers and technology innovations, basic terminology and concepts are\n\nintroduced, along with descriptions of common benefits and challenges of\n\ncloud computing adoption.\n\nChapter 4: Fundamental Concepts and Models\n\nCloud delivery and cloud deployment models are discussed in detail,\n\nfollowing sections that establish common cloud characteristics and roles\n\nand boundaries.\n\nChapter 5: Cloud-Enabling Technology\n\nContemporary technologies that realize modern-day cloud computing\n\nplatforms and innovations are discussed, including data centers,\n\nvirtualization, containerization, and Web-based technologies.\n\nChapter 6: Understanding Containerization\n\nA comparison of virtualization and containerization is provided, along with\n\nin-depth coverage of containerization environments and components.\n\nChapter 7: Understanding Cloud Security and Cybersecurity\n\nCloud security and cybersecurity topics and concepts relevant and distinct\n\nto cloud computing are introduced, including descriptions of common cloud\n\nsecurity threats and attacks.\n\nPart II: Cloud Computing Mechanisms\n\nTechnology mechanisms represent well-defined IT artifacts that are\n\nestablished within an IT industry and commonly distinct to a certain\n\ncomputing model or platform. The technology-centric nature of cloud\n\ncomputing requires the establishment of a formal level of mechanisms to be\n\nable to explore how solutions can be assembled via different combinations\n\nof mechanism implementations.\n\nThis part formally documents over 50 technology mechanisms that are used\n\nwithin cloud environments to enable generic and specialized forms of\n\nfunctionality. Each mechanism description is accompanied by a case study\n\nexample that demonstrates its usage. The utilization of select mechanisms is\n\nfurther explored throughout the technology architectures covered in Part III.\n\nChapter 8: Cloud Infrastructure Mechanisms\n\nTechnology mechanisms foundational to cloud platforms are covered,\n\nincluding Logical Network Perimeter, Virtual Server, Cloud Storage\n\nDevice, Cloud Usage Monitor, Resource Replication, Hypervisor, Ready-\n\nMade Environment and Container.\n\nChapter 9: Specialized Cloud Mechanisms\n\nA range of specialized technology mechanisms is described, including\n\nAutomated Scaling Listener, Load Balancer, SLA Monitor, Pay-Per-Use\n\nMonitor, Audit Monitor, Failover System, Resource Cluster, Multi-Device\n\nBroker, and State Management Database.\n\nChapter 10: Cloud and Cyber Security Access-Oriented Mechanisms\n\nAccess-related security mechanisms that can be used to counter and prevent\n\nsome of the threats described in Chapter 7 are covered, including\n\nEncryption, Hashing, Digital Signature, Cloud-Based Security Groups,\n\nPublic Key Infrastructure (PKI) System, Single Sign-On (SSO) System,\n\nHardened Virtual Server Image, Firewall, Virtual Private Network (VPN),\n\nBiometric Scanner, Multi-Factor Authentication (MFA) System, Identity\n\nand Access Management (IAM) System, Intrusion Detection System (IDS),\n\nPenetration Testing Tool, User Behavior Analytics (UBA) System, Third-\n\nParty Software Update Utility, Network Intrusion Monitor, Authentication\n\nLog Monitor, and VPN Monitor.\n\nChapter 11: Cloud and Cyber Security Data-Oriented Mechanisms\n\nData-related security mechanisms that can be used to counter and prevent\n\nsome of the threats described in Chapter 7 are covered, including Digital\n\nVirus Scanning and Decryption System, Digital Immune System, Malicious\n\nCode Analysis System, Data Loss Prevention (DLP) System, Trusted\n\nPlatform Module (TPM), Data Backup and Recovery System, Activity Log\n\nMonitor, Traffic Monitor, and Data Loss Protection Monitor.\n\nChapter 12: Cloud Management Mechanisms\n\nMechanisms that enable the hands-on administration and management of\n\ncloud-based IT resources are explained, including Remote Administration\n\nSystem, Resource Management System, SLA Management System, and\n\nBilling Management System.\n\nPart III: Cloud Computing Architecture\n\nTechnology architecture within the realm of cloud computing introduces\n\nrequirements and considerations that manifest themselves in broadly scoped\n\narchitectural layers and numerous distinct architectural models.\n\nThis set of chapters builds upon the coverage of cloud computing\n\nmechanisms from Part II by formally documenting over 30 cloud-based\n\ntechnology architectures and scenarios in which different combinations of\n\nthe mechanisms are documented in relation to fundamental, advanced, and\n\nspecialized cloud architectures.\n\nChapter 13: Fundamental Cloud Architectures\n\nFundamental cloud architectural models establish baseline functions and\n\ncapabilities. The architectures covered in this chapter are Workload\n\nDistribution, Resource Pooling, Dynamic Scalability, Elastic Resource\n\nCapacity, Service Load Balancing, Cloud Bursting, Elastic Disk\n\nProvisioning, Redundant Storage and Multi-Cloud.\n\nChapter 14: Advanced Cloud Architectures\n\nAdvanced cloud architectural models establish sophisticated and complex\n\nenvironments, several of which directly build upon fundamental models.\n\nThe architectures covered in this chapter are Hypervisor Clustering, Virtual\n\nServer Clustering, Load Balanced Virtual Server Instances, Non-Disruptive\n\nService Relocation, Zero Downtime, Cloud Balancing, Resilient Disaster\n\nRecovery, Distributed Data Sovereignty, Resource Reservation, Dynamic\n\nFailure Detection and Recovery, Rapid Provisioning, Storage Workload\n\nManagement, and Virtual Private Cloud.\n\nChapter 15: Specialized Cloud Architectures\n\nSpecialized cloud architectural models address distinct functional areas.\n\nThe architectures covered in this chapter are Direct I/O Access, Direct LUN\n\nAccess, Dynamic Data Normalization, Elastic Network Capacity, Cross-\n\nStorage Device Vertical Tiering, Intra-Storage Device Vertical Data Tiering,\n\nLoad-Balanced Virtual Switches, Multipath Resource Access, Persistent\n\nVirtual Network Configuration, Redundant Physical Connection for Virtual\n\nServers, Storage Maintenance Window, Edge Computing, and Fog\n\nComputing.\n\nPart IV: Working with Clouds\n\nCloud computing technologies and environments can be adopted to varying\n\nextents. An organization can migrate select IT resources to a cloud, while\n\nkeeping all other IT resources on-premise—or it can form significant\n\ndependencies on a cloud platform by migrating larger amounts of IT\n\nresources or even using the cloud environment to create them.\n\nFor any organization, it is important to assess a potential adoption from a\n\npractical and business-centric perspective in order to pinpoint the most\n\ncommon factors that pertain to financial investments, business impact, and\n\nvarious legal considerations. This set of chapters explores these and other\n\ntopics related to the real-world considerations of working with cloud-based\n\nenvironments.\n\nChapter 16: Cloud Delivery Model Considerations\n\nCloud environments need to be built and evolved by cloud providers in\n\nresponse to cloud consumer requirements. Cloud consumers can use clouds\n\nto create or migrate IT resources to, subsequent to their assuming\n\nadministrative responsibilities. This chapter provides a technical\n\nunderstanding of cloud delivery models from both the provider and\n\nconsumer perspectives, each of which offers revealing insights into the\n\ninner workings and architectural layers of cloud environments.\n\nChapter 17: Cost Metrics and Pricing Models\n\nCost metrics for network, server, storage, and software usage are described,\n\nalong with various formulas for calculating integration and ownership costs\n\nrelated to cloud environments. The chapter concludes with a discussion of\n\ncost management topics as they relate to common business terms used by\n\ncloud provider vendors.\n\nChapter 18: Service Quality Metrics and SLAs\n\nService level agreements establish the guarantees and usage terms for cloud\n\nservices and are often determined by the business terms agreed upon by\n\ncloud consumers and cloud providers. This chapter provides detailed insight\n\ninto how cloud provider guarantees are expressed and structured via SLAs,\n\nalong with metrics and formulas for calculating common SLA values, such\n\nas availability, reliability, performance, scalability, and resiliency.\n\nPart V: Appendices\n\nAppendix A: Case Study Conclusions\n\nThe individual storylines of the case studies are concluded and the results of\n\neach organization’s cloud computing adoption efforts are summarized.\n\nAppendix B: Common Containerization Technologies\n\nThis appendix acts as a supplement to Chapter 6 by providing a breakdown\n\nof the Docker and Kubernetes environments and relating those\n\nenvironments to the terms and components established in Chapter 6.\n\n1.5 Resources\n\nThese sections provide supplementary information and resources.\n\nPearson Digital Enterprise Book Series\n\nInformation about the books in the Pearson Digital Enterprise Series from\n\nThomas Erl and various supporting resources can be found at:\n\nwww.thomaserl.com/books\n\nThomas Erl on YouTube\n\nSubscribe to the Thomas Erl YouTube channel for animated videos with\n\nstorytelling and podcasts with industry experts. This YouTube channel is\n\ndedicated to digital technology, digital business and digital transformation.\n\nSubscribe at:\n\nwww.youtube.com/@terl\n\nDigital Enterprise Newsletter on LinkedIn\n\nThe Digital Enterprise newsletter on LinkedIn publishes regular articles and\n\nvideos relevant to contemporary digital technology and business topics.\n\nSubscribe at: www.linkedin.com/newsletters/6909573501767028736\n\nCloud Certified Professional (CCP) Program\n\nArcitura Education offers vendor-neutral training and accreditation\n\nprograms with a portfolio of over 100 course modules and 40 certifications.\n\nThis text book is an official part of Arcitura’s Cloud Certified Professional\n\n(CCP) curriculum.\n\nLearn more at:\n\nwww.arcitura.com\n\nChapter 2\n\nCase Study Background\n\n2.1 Case Study #1: ATN\n\n2.2 Case Study #2: DTGOV\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nCase study examples provide scenarios in which organizations assess, use,\n\nand manage cloud computing models and technologies. Three organizations\n\nfrom different industries are presented for analysis in this book, each of\n\nwhich has distinctive business, technological, and architectural objectives\n\nthat are introduced in this chapter.\n\nThe organizations presented for case study are:\n\nAdvanced Telecom Networks (ATN) – a global company that supplies\n\nnetwork equipment to the telecommunications industry\n\nDTGOV – a public organization that specializes in IT infrastructure and\n\ntechnology services for public sector organizations\n\nInnovartus Technologies Inc. – a medium-sized company that develops\n\nvirtual toys and educational entertainment products for children\n\nMost chapters after Part I include one or more Case Study Example\n\nsections. A conclusion to the storylines is provided in Appendix A.\n\n2.1 Case Study #1: ATN\n\nATN is a company that provides network equipment to telecommunications\n\nindustries across the globe. Over the years, ATN has grown considerably\n\nand their product portfolio has expanded to accommodate several\n\nacquisitions, including companies that specialize in infrastructure\n\ncomponents for Internet, GSM, and cellular providers. ATN is now a\n\nleading supplier of a diverse range of telecommunications infrastructure.\n\nIn recent years, market pressure has been increasing. ATN has begun\n\nlooking for ways to increase its competitiveness and efficiency by taking\n\nadvantage of new technologies, especially those that can assist in cost\n\nreduction.\n\nTechnical Infrastructure and Environment\n\nATN’s various acquisitions have resulted in a highly complex and\n\nheterogeneous IT landscape. A cohesive consolidation program was not\n\napplied to the IT environment after each acquisition round, resulting in\n\nsimilar applications running concurrently and an increase in maintenance\n\ncosts. Years ago, ATN merged with a major European telecommunications\n\nsupplier, adding another applications portfolio to its inventory. The IT",
      "page_number": 4
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 26-47)",
      "start_page": 26,
      "end_page": 47,
      "detection_method": "synthetic",
      "content": "complexity snowballed into a serious obstruction and became a source of\n\ncritical concern to ATN’s board of directors.\n\nBusiness Goals and New Strategy\n\nATN management decided to pursue a consolidation initiative and\n\noutsource applications maintenance and operations overseas. This lowered\n\ncosts but unfortunately did not address their overall operational inefficiency.\n\nApplications still had overlapping functions that could not be easily\n\nconsolidated. It eventually became apparent that outsourcing was\n\ninsufficient as consolidation became a possibility only if the architecture of\n\nthe entire IT landscape changed.\n\nAs a result, ATN decided to explore the potential of adopting cloud\n\ncomputing. However, subsequent to their initial inquiries they became\n\noverwhelmed by the plenitude of cloud providers and cloud-based products.\n\nRoadmap and Implementation Strategy\n\nATN is unsure of how to choose the right set of cloud computing\n\ntechnologies and vendors—many solutions appear to still be immature and\n\nnew cloud-based offerings continue to emerge in the market.\n\nA preliminary cloud computing adoption roadmap is discussed to address a\n\nnumber of key points:\n\nIT Strategy – The adoption of cloud computing needs to promote\n\noptimization of the current IT framework, and produce both lower short-\n\nterm investments and consistent long-term cost reduction.\n\nBusiness Benefits – ATN needs to evaluate which of the current applications\n\nand IT infrastructure can leverage cloud computing technology to achieve\n\nthe desired optimization and cost reductions. Additional cloud computing\n\nbenefits such as greater business agility, scalability, and reliability need to\n\nbe realized to promote business value.\n\nTechnology Considerations – Criteria need to be established to help choose\n\nthe most appropriate cloud delivery and deployment models and cloud\n\nvendors and products.\n\nCloud Security – The risks associated with migrating applications and data\n\nto the cloud must be determined.\n\nATN fears that they might lose control over their applications and data if\n\nentrusted to cloud providers, leading to incompliance with internal policies\n\nand telecom market regulations. They also wonder how their existing\n\nlegacy applications would be integrated into the new cloud-based domain.\n\nTo define a succinct plan of action, ATN hires an independent IT consulting\n\ncompany called CloudEnhance, who are well recognized for their\n\ntechnology architecture expertise in the transition and integration of cloud\n\ncomputing IT resources. CloudEnhance consultants begin by suggesting an\n\nappraisal process comprised of five steps:\n\n. A brief evaluation of existing applications to measures factors, such as\n\ncomplexity, business-criticality, usage frequency, and number of active\n\nusers. The identified factors are then placed in a hierarchy of priority to help\n\ndetermine the most suitable candidate applications for migration to a cloud\n\nenvironment.\n\n. A more detailed evaluation of each selected application using a proprietary\n\nassessment tool.\n\n. The development of a target application architecture that exhibits the\n\ninteraction between cloud-based applications, their integration with ATN’s\n\nexisting infrastructure and legacy systems, and their development and\n\ndeployment processes.\n\n. The authoring of a preliminary business case that documents projected cost\n\nsavings based on performance indicators, such as cost of cloud readiness,\n\neffort for application transformation and interaction, ease of migration and\n\nimplementation, and various potential long-term benefits.\n\n. The development of a detailed project plan for a pilot application.\n\nATN proceeds with the process and resultantly builds its first prototype by\n\nfocusing on an application that automates a low-risk business area. During\n\nthis project ATN ports several of the business area’s smaller applications\n\nthat were running on different technologies over to a PaaS platform. Based\n\non positive results and feedback received for the prototype project, ATN\n\ndecides to embark on a strategic initiative to garner similar benefits for\n\nother areas of the company.\n\n2.2 Case Study #2: DTGOV\n\nDTGOV is a public company that was created in the early 1980s by the\n\nMinistry of Social Security. The decentralization of the ministry’s IT\n\noperations to a public company under private law gave DTGOV an\n\nautonomous management structure with significant flexibility to govern and\n\nevolve its IT enterprise.\n\nAt the time of its creation, DTGOV had approximately 1,000 employees,\n\noperational branches in 60 localities nation-wide, and operated two\n\nmainframe-based data centers. Over time, DTGOV has expanded to more\n\nthan 3,000 employees and branch offices in more than 300 localities, with\n\nthree data centers running both mainframe and low-level platform\n\nenvironments. Its main services are related to processing social security\n\nbenefits across the country.\n\nDTGOV has enlarged its customer portfolio in the last two decades. It now\n\nserves other public-sector organizations and provides basic IT infrastructure\n\nand services, such as server hosting and server colocation. Some of its\n\ncustomers have also outsourced the operation, maintenance, and\n\ndevelopment of applications to DTGOV.\n\nDTGOV has sizable customer contracts that encompass various IT\n\nresources and services. However, these contracts, services, and associated\n\nservice levels are not -standardized—negotiated service provisioning\n\nconditions are typically customized for each customer individually.\n\nDTGOV’s operations are resultantly becoming increasingly complex and\n\ndifficult to manage, which has led to inefficiencies and inflated costs.\n\nThe DTGOV board realized, some time ago, that the overall company\n\nstructure could be improved by standardizing its services portfolio, which\n\nimplies the reengineering of both IT operational and management models.\n\nThis process has started with the standardization of the hardware platform\n\nthrough the creation of a clearly defined technological lifecycle, a\n\nconsolidated procurement policy, and the establishment of new acquisition\n\npractices.\n\nTechnical Infrastructure and Environment\n\nDTGOV operates three data centers: one is exclusively dedicated to low-\n\nlevel platform servers while the other two have both mainframe and low-\n\nlevel platforms. The mainframe systems are reserved for the Ministry of\n\nSocial Security and therefore not available for outsourcing.\n\nThe data center infrastructure occupies approximately 20,000 square feet of\n\ncomputer room space and hosts more than 100,000 servers with different\n\nhardware configurations. The total storage capacity is approximately 10,000\n\nterabytes. DTGOV’s network has redundant high-speed data links\n\nconnecting the data centers in a full mesh topology. Their Internet\n\nconnectivity is considered to be provider-independent since their network\n\ninterconnects all of the major national telecom carriers.\n\nServer consolidation and virtualization projects have been in place for five\n\nyears, considerably decreasing the diversity of hardware platforms. As a\n\nresult, systematic tracking of the investments and operational costs related\n\nto the hardware platform has revealed significant improvement. However,\n\nthere is still remarkable diversity in their software platforms and\n\nconfigurations due to customer service customization requirements.\n\nBusiness Goals and New Strategy\n\nA chief strategic objective of the standardization of DTGOV’s service\n\nportfolio is to achieve increased levels of cost effectiveness and operational\n\noptimization. An internal executive-level commission was established to\n\ndefine the directions, goals, and strategic roadmap for this initiative. The\n\ncommission has identified cloud computing as a guidance option and an\n\nopportunity for further diversification and improvement of services and\n\ncustomer portfolios.\n\nThe roadmap addresses the following key points:\n\nBusiness Benefits – Concrete business benefits associated with the\n\nstandardization of service portfolios under the umbrella of cloud computing\n\ndelivery models need to be defined. For example, how can the optimization\n\nof IT infrastructure and operational models result in direct and measurable\n\ncost reductions?\n\nService Portfolio – Which services should become cloud-based, and which\n\ncustomers should they be extended to?\n\nTechnical Challenges – The limitations of the current technology\n\ninfrastructure in relation to the runtime processing requirements of cloud\n\ncomputing models must be understood and documented. Existing\n\ninfrastructure must be leveraged to whatever extent possible to optimize up-\n\nfront costs assumed by the development of the cloud-based service\n\nofferings.\n\nPricing and SLAs – An appropriate contract, pricing, and service quality\n\nstrategy needs to be defined. Suitable pricing and service-level agreements\n\n(SLAs) must be determined to support the initiative.\n\nOne outstanding concern relates to changes to the current format of\n\ncontracts and how they may impact business. Many customers may not\n\nwant to—or may not be prepared to—adopt cloud contracting and service\n\ndelivery models. This becomes even more critical when considering the fact\n\nthat 90% of DTGOV’s current customer portfolio is comprised of public\n\norganizations that typically do not have the autonomy or the agility to\n\nswitch operating methods on such short notice. Therefore, the migration\n\nprocess is expected to be long term, which may become risky if the\n\nroadmap is not properly and clearly defined. A further outstanding issue\n\npertains to IT contract regulations in the public sector—-existing\n\nregulations may become irrelevant or unclear when applied to cloud\n\ntechnologies.\n\nRoadmap and Implementation Strategy\n\nSeveral assessment activities were initiated to address the aforementioned\n\nissues. The first was a survey of existing customers to probe their level of\n\nunderstanding, on-going initiatives, and plans regarding cloud computing.\n\nMost of the respondents were aware of and knowledgeable about cloud\n\ncomputing trends, which was considered a positive finding.\n\nAn investigation of the service portfolio revealed clearly identified\n\ninfrastructure services relating to hosting and colocation. Technical\n\nexpertise and infrastructure were also evaluated, determining that data\n\ncenter operation and management are key areas of expertise of DTGOV IT\n\nstaff.\n\nWith these findings, the commission decided to:\n\n. choose IaaS as the target delivery platform to start the cloud computing\n\nprovisioning initiative\n\n. hire a consulting firm with sufficient cloud provider expertise and\n\nexperience to correctly identify and rectify any business and technical\n\nissues that may afflict the initiative\n\n. deploy new hardware resources with a uniform platform into two different\n\ndata centers, aiming to establish a new, reliable environment to use for the\n\nprovisioning of initial IaaS-hosted services\n\n. identify three customers that plan to acquire cloud-based services in order\n\nto establish pilot projects and define contractual conditions, pricing, and\n\nservice-level policies and models\n\n. evaluate service provisioning of the three chosen customers for the initial\n\nperiod of six months before publicly offering the service to other customers\n\nAs the pilot project proceeds, a new Web-based management environment\n\nis released to allow for the self-provisioning of virtual servers, as well as\n\nSLA and financial tracking functionality in realtime. The pilot projects are\n\nconsidered highly successful, leading to the next step of opening the cloud-\n\nbased services to other customers.\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nThe primary business line of Innovartus Technologies Inc. is the\n\ndevelopment of virtual toys and educational entertainment products for\n\nchildren. These services are provided through a Web portal that employs a\n\nrole-playing model to create customized virtual games for PCs and mobile\n\ndevices. The games allow users to create and manipulate virtual toys (cars,\n\ndolls, pets) that can be outfitted with virtual accessories that are obtained by\n\ncompleting simple educational quests. The main demographic is children\n\nunder 12 years. Innovartus further has a social network environment that\n\nenables users to exchange items and collaborate with others. All of these\n\nactivities can be monitored and tracked by the parents, who can also\n\nparticipate in a game by creating specific quests for their children.\n\nThe most valuable and revolutionary feature of Innovartus’ applications is\n\nan experimental end-user interface that is based on natural interface\n\nconcepts. Users can interact via voice commands, simple gestures that are\n\ncaptured with a Webcam, and directly by touching tablet screens.\n\nThe Innovartus portal has always been cloud-based. It was originally\n\ndeveloped via a PaaS platform and has been hosted by the same cloud\n\nprovider ever since. However, recently this environment has revealed\n\nseveral technical limitations that impact features of Innovartus’ user\n\ninterface programming frameworks.\n\nTechnical Infrastructure and Environment\n\nMany of Innovartus’ other office automation solutions, such as shared file\n\nrepositories and various productivity tools, are also cloud-based. The on-\n\npremise corporate IT environment is relatively small, comprised mainly of\n\nwork area devices, laptops, and graphic design workstations.\n\nBusiness Goals and Strategy\n\nInnovartus has been diversifying the functionality of the IT resources that\n\nare used for their Web-based and mobile applications. The company has\n\nalso increased efforts to internationalize their applications; both the Web\n\nsite and the mobile applications are currently offered in five different\n\nlanguages.\n\nRoadmap and Implementation Strategy\n\nInnovartus intends to continue building upon its cloud-based solutions;\n\nhowever, the current cloud hosting environment has limitations that need to\n\nbe overcome:\n\nscalability needs to be improved to accommodate increased and less\n\npredictable cloud consumer interaction\n\nservice levels need to be improved to avoid outages that are currently more\n\nfrequent than expected\n\ncost effectiveness needs to be improved, as leasing rates are higher with the\n\ncurrent cloud provider when compared to others\n\nThese and other factors have led Innovartus to decide to migrate to a larger,\n\nmore globally established cloud provider.\n\nThe roadmap for this migration project includes:\n\na technical and economic report about the risks and impacts of the planned\n\nmigration\n\na decision tree and a rigorous study initiative focused on the criteria for\n\nselecting the new cloud provider\n\nportability assessments of applications to determine how much of each\n\nexisting cloud service architecture is proprietary to the current cloud\n\nprovider’s environment\n\nInnovartus is further concerned about how and to what extent the current\n\ncloud provider will support and cooperate with the migration process.\n\nPart I\n\nFundamental Cloud Computing\n\nChapter 3: Understanding Cloud Computing\n\nChapter 4: Fundamental Concepts and Models\n\nChapter 5: Cloud-Enabling Technology\n\nChapter 6: Understanding Containerization\n\nChapter 7: Understanding Cloud Security and Cybersecurity\n\nThe upcoming chapters establish concepts and terminology that are\n\nreferenced throughout subsequent chapters and parts in this book. It is\n\nrecommended that Chapters 3 and 4 be reviewed, even for those already\n\nfamiliar with cloud computing fundamentals. Sections in Chapters 5, 6 and\n\n7 can be selectively skipped by those already familiar with the\n\ncorresponding technology and security topics.\n\nChapter 3\n\nUnderstanding Cloud Computing\n\n3.1 Origins and Influences\n\n3.2 Basic Concepts and Terminology\n\n3.3 Goals and Benefits\n\n3.4 Risks and Challenges\n\nThis is the first of two chapters that provide an overview of introductory\n\ncloud computing topics. It begins with a brief history of cloud computing\n\nalong with short descriptions of its business and technology drivers. This is\n\nfollowed by definitions of basic concepts and terminology, in addition to\n\nexplanations of the primary benefits and challenges of cloud computing\n\nadoption.\n\n3.1 Origins and Influences\n\nA Brief History\n\nThe idea of computing in a “cloud” traces back to the origins of utility\n\ncomputing, a concept that computer scientist John McCarthy publicly\n\nproposed in 1961:\n\n“If computers of the kind I have advocated become the computers of the\n\nfuture, then computing may someday be organized as a public utility just as\n\nthe telephone system is a public utility. … The computer utility could\n\nbecome the basis of a new and important industry.”\n\nIn 1969, Leonard Kleinrock, a chief scientist of the Advanced Research\n\nProjects Agency Network or ARPANET project that seeded the Internet,\n\nstated:\n\n“As of now, computer networks are still in their infancy, but as they grow up\n\nand become sophisticated, we will probably see the spread of ‘computer\n\nutilities’ …”.\n\nThe general public has been leveraging forms of Internet-based computer\n\nutilities since the mid-1990s through various incarnations of search engines,\n\nemail services, open publishing platforms, and other types of social media.\n\nThough consumer-centric, these services popularized and validated core\n\nconcepts that form the basis of modern-day cloud computing.\n\nIn 1999, Salesforce.com pioneered the notion of bringing remotely\n\nprovisioned services into the enterprise. In 2006, Amazon.com launched the\n\nAmazon Web Services (AWS) platform, a suite of enterprise-oriented\n\nservices that provide remotely provisioned storage, computing resources,\n\nand business functionality.\n\nA slightly different evocation of the term “Network Cloud” or “Cloud” was\n\nintroduced in the early 1990s throughout the networking industry. It\n\nreferred to an abstraction layer derived in the delivery methods of data\n\nacross heterogeneous public and semi-public networks that were primarily\n\npacket-switched, although cellular networks used the “Cloud” term as well.\n\nThe networking method at this point supported the transmission of data\n\nfrom one end-point (local network) to the “Cloud” (wide area network) and\n\nthen further decomposed to another intended end-point. This is relevant, as\n\nthe networking industry still references the use of this term, and is\n\nconsidered an early adopter of the concepts that underlie utility computing.\n\nIt wasn’t until 2006 that the term “cloud computing” emerged in the\n\ncommercial arena. It was during this time that Amazon launched its Elastic\n\nCompute Cloud (EC2) services that enabled organizations to “lease”\n\ncomputing capacity and processing power to run their enterprise\n\napplications. Google Apps also began providing browser-based enterprise\n\napplications in the same year, and three years later, the Google App Engine\n\nbecame another historic milestone.\n\nDefinitions\n\nA Gartner report listing cloud computing at the top of its strategic\n\ntechnology areas further reaffirmed its prominence as an industry trend by\n\nannouncing its formal definition as:\n\n“…a style of computing in which scalable and elastic IT-enabled\n\ncapabilities are delivered as a service to external customers using Internet\n\ntechnologies.”\n\nThis is a slight revision of Gartner’s original definition from 2008, in which\n\n“massively scalable” was used instead of “scalable and elastic.” This\n\nacknowledges the importance of scalability in relation to the ability to scale\n\nvertically and not just to enormous proportions.\n\nForrester Research provided its own definition of cloud computing as:\n\n“…a standardized IT capability (services, software, or infrastructure)\n\ndelivered via Internet technologies in a pay-per-use, self-service way.”\n\nThe definition that received industry-wide acceptance was composed by the\n\nNational Institute of Standards and Technology (NIST). NIST published its\n\noriginal definition back in 2009, followed by a revised version after further\n\nreview and industry input that was published in September of 2011:\n\n“Cloud computing is a model for enabling ubiquitous, convenient, on-\n\ndemand network access to a shared pool of configurable computing\n\nresources (e.g., networks, servers, storage, applications, and services) that\n\ncan be rapidly provisioned and released with minimal management effort or\n\nservice provider interaction. This cloud model is composed of five essential\n\ncharacteristics, three service models, and four deployment models.”\n\nThis book provides a more concise definition:\n\n“Cloud computing is a specialized form of distributed computing that\n\nintroduces utilization models for remotely provisioning scalable and\n\nmeasured resources.”\n\nThis simplified definition is in line with all of the preceding definition\n\nvariations that were put forth by other organizations within the cloud\n\ncomputing industry. The characteristics, service models, and deployment\n\nmodels referenced in the NIST definition are further covered in Chapter 4.\n\nBusiness Drivers\n\nBefore delving into the layers of technologies that underlie clouds, the\n\nmotivations that led to their creation by industry leaders must first be\n\nunderstood. Several of the primary business drivers that fostered modern\n\ncloud-based technology are presented in this section.\n\nThe origins and inspirations of many of the characteristics, models, and\n\nmechanisms covered throughout subsequent chapters can be traced back to\n\nthe upcoming business drivers. It is important to note that these influences\n\nshaped clouds and the overall cloud computing market from both ends.\n\nThey have motivated organizations to adopt cloud computing in support of\n\ntheir business automation requirements. They have correspondingly\n\nmotivated other organizations to become providers of cloud environments\n\nand cloud technology vendors in order to create and meet the demand to\n\nfulfill consumer needs.\n\nCost Reduction\n\nA direct alignment between IT costs and business performance can be\n\ndifficult to maintain. The growth of IT environments often corresponds to\n\nthe assessment of their maximum usage requirements. This can make the\n\nsupport of new and expanded business automations an ever-increasing\n\ninvestment. Much of this required investment is funneled into infrastructure\n\nexpansion because the usage potential of a given automation solution will\n\nalways be limited by the processing power of its underlying infrastructure.\n\nTwo costs need to be accounted for: the cost of acquiring new\n\ninfrastructure, and the cost of its on-going ownership. Operational overhead\n\nrepresents a considerable share of IT budgets, often exceeding up-front\n\ninvestment costs.\n\nCommon forms of infrastructure-related operating overhead include the\n\nfollowing:\n\ntechnical personnel required to keep the environment operational\n\nupgrades and patches that introduce additional testing and deployment\n\ncycles\n\nutility bills and capital expense investments for power and cooling\n\nsecurity and access control measures that need to be maintained and\n\nenforced to protect infrastructure resources\n\nadministrative and accounts staff that may be required to keep track of\n\nlicenses and support arrangements\n\nThe on-going ownership of internal technology infrastructure can\n\nencompass burdensome responsibilities that impose compound impacts on\n\ncorporate budgets. An IT department can consequently become a significant\n\n—and at times overwhelming—drain on the business, potentially inhibiting\n\nits responsiveness, profitability, and overall evolution.\n\nBusiness Agility\n\nBusinesses need the ability to adapt and evolve to successfully face change\n\ncaused by both internal and external factors. Business agility (or\n\norganizational agility) is the measure of an organization’s responsiveness to\n\nchange.\n\nAn IT enterprise often needs to respond to business change by scaling its IT\n\nresources beyond the scope of what was previously predicted or planned\n\nfor. For example, infrastructure may be subject to limitations that prevent\n\nthe organization from responding to usage fluctuations—even when\n\nanticipated—if previous capacity planning efforts were restricted by\n\ninadequate budgets.\n\nIn other cases, changing business needs and priorities may require IT\n\nresources to be more available and reliable than before. Even if sufficient\n\ninfrastructure is in place for an organization to support anticipated usage\n\nvolumes, the nature of the usage may generate runtime exceptions that\n\nbring down hosting servers. Due to a lack of reliability controls within the\n\ninfrastructure, responsiveness to consumer or customer requirements may\n\nbe reduced to a point whereby a business’ overall continuity is threatened.\n\nOn a broader scale, the up-front investments and infrastructure ownership\n\ncosts that are required to enable new or expanded business automation\n\nsolutions may themselves be prohibitive enough for a business to settle for\n\nIT infrastructure of less-than-ideal quality, thereby decreasing its ability to\n\nmeet real-world requirements.\n\nWorse yet, the business may decide against proceeding with an automation\n\nsolution altogether upon review of its infrastructure budget, because it\n\nsimply cannot afford to. This form of inability to respond can inhibit an\n\norganization from keeping up with market demands, competitive pressures,\n\nand its own strategic business goals.\n\nTechnology Innovations\n\nEstablished technologies are often used as inspiration and, at times, the\n\nactual foundations upon which new technology innovations are derived and\n\nbuilt. This section briefly describes the pre-existing technologies considered\n\nto be the primary influences on cloud computing.\n\nClustering\n\nA cluster is a group of independent IT resources that are interconnected and\n\nwork as a single system. System failure rates are reduced while availability\n\nand reliability are increased, since redundancy and failover features are\n\ninherent to the cluster.\n\nA general prerequisite of hardware clustering is that its component systems\n\nhave reasonably identical hardware and operating systems to provide\n\nsimilar performance levels when one failed component is to be replaced by\n\nanother. Component devices that form a cluster are kept in synchronization\n\nthrough dedicated, high-speed communication links.\n\nThe basic concept of built-in redundancy and failover is core to cloud\n\nplatforms. Clustering technology is explored further in Chapter 9 as part of\n\nthe Resource Cluster mechanism description.\n\nGrid Computing\n\nA computing grid (or “computational grid”) provides a platform in which\n\ncomputing resources are organized into one or more logical pools. These\n\npools are collectively coordinated to provide a high performance distributed\n\ngrid, sometimes referred to as a “super virtual computer.” Grid computing\n\ndiffers from clustering in that grid systems are much more loosely coupled\n\nand distributed. As a result, grid computing systems can involve computing",
      "page_number": 26
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 48-69)",
      "start_page": 48,
      "end_page": 69,
      "detection_method": "synthetic",
      "content": "resources that are heterogeneous and geographically dispersed, which is\n\ngenerally not possible with cluster computing-based systems.\n\nGrid computing has been an on-going research area in computing science\n\nsince the early 1990s. The technological advancements achieved by grid\n\ncomputing projects have influenced various aspects of cloud computing\n\nplatforms and mechanisms, specifically in relation to common feature-sets\n\nsuch as networked access, resource pooling, and scalability and resiliency.\n\nThese types of features can be established by both grid computing and\n\ncloud computing, in their own distinctive approaches.\n\nFor example, grid computing is based on a middleware layer that is\n\ndeployed on computing resources. These IT resources participate in a grid\n\npool that implements a series of workload distribution and coordination\n\nfunctions. This middle tier can contain load balancing logic, failover\n\ncontrols, and autonomic configuration management, each having previously\n\ninspired similar—and several more sophisticated—cloud computing\n\ntechnologies. It is for this reason that some classify cloud computing as a\n\ndescendant of earlier grid computing initiatives.\n\nCapacity Planning\n\nCapacity planning is the process of determining and fulfilling future\n\ndemands of an organization’s IT resources, products, and services. Within\n\nthis context, capacity represents the maximum amount of work that an IT\n\nresource is capable of delivering in a given period of time. A discrepancy\n\nbetween the capacity of an IT resource and its demand can result in a\n\nsystem becoming either inefficient (over-provisioning) or unable to fulfill\n\nuser needs (under-provisioning). Capacity planning is focused on\n\nminimizing this discrepancy to achieve predictable efficiency and\n\nperformance.\n\nDifferent capacity planning strategies exist:\n\nLead Strategy – adding capacity to an IT resource in anticipation of demand\n\nLag Strategy – adding capacity when the IT resource reaches its full\n\ncapacity\n\nMatch Strategy – adding IT resource capacity in small increments, as\n\ndemand increases\n\nPlanning for capacity can be challenging because it requires estimating\n\nusage load fluctuations. There is a constant need to balance peak usage\n\nrequirements without unnecessary over-expenditure on infrastructure. An\n\nexample is outfitting IT infrastructure to accommodate maximum usage\n\nloads which can impose unreasonable financial investments. In such cases,\n\nmoderating investments can result in under-provisioning, leading to\n\ntransaction losses and other usage limitations from lowered usage\n\nthresholds.\n\nVirtualization\n\nVirtualization is the process of converting a physical IT resource into a\n\nvirtual IT resource.\n\nMost types of IT resources can be virtualized, including:\n\nServers – A physical server can be abstracted into a virtual server.\n\nStorage – A physical storage device can be abstracted into a virtual storage\n\ndevice or a virtual disk.\n\nNetwork – Physical routers and switches can be abstracted into logical\n\nnetwork fabrics, such as VLANs.\n\nPower – A physical UPS and power distribution units can be abstracted into\n\nwhat are commonly referred to as virtual UPSs.\n\nNote\n\nThe terms virtual server and virtual machine (VM) are used\n\nsynonymously throughout this book.\n\nA layer of virtualization software allows physical IT resources to provide\n\nmultiple virtual images of themselves so that their underlying processing\n\ncapabilities can be shared by multiple users.\n\nThe first step in creating a new virtual server through virtualization\n\nsoftware is the allocation of physical IT resources, followed by the\n\ninstallation of an operating system. Virtual servers use their own guest\n\noperating systems, which are independent of the operating system in which\n\nthey were created.\n\nBoth the guest operating system and the application software running on the\n\nvirtual server are unaware of the virtualization process, meaning these\n\nvirtualized IT resources are installed and executed as if they were running\n\non a separate physical server. This uniformity of execution that allows\n\nprograms to run on physical systems as they would on virtual systems is a\n\nvital characteristic of virtualization. Guest operating systems typically\n\nrequire seamless usage of software products and applications that do not\n\nneed to be customized, configured, or patched in order to run in a\n\nvirtualized environment.\n\nVirtualization software runs on a physical server called a host or physical\n\nhost, whose underlying hardware is made accessible by the virtualization\n\nsoftware. The virtualization software functionality encompasses system\n\nservices that are specifically related to virtual machine management and not\n\nnormally found on standard operating systems. This is why this software is\n\nsometimes referred to as a virtual machine manager or a virtual machine\n\nmonitor (VMM), but most commonly known as a hypervisor. (The\n\nhypervisor is formally described as a cloud computing mechanism in\n\nChapter 8.)\n\nPrior to the advent of virtualization technologies, software was limited to\n\nresiding on and being coupled with static hardware environments. The\n\nvirtualization process severs this software-hardware dependency, as\n\nhardware requirements can be simulated by emulation software running in\n\nvirtualized environments.\n\nEstablished virtualization technologies can be traced to several cloud\n\ncharacteristics and cloud computing mechanisms, having inspired many of\n\ntheir core features. As cloud computing evolved, a generation of modern\n\nvirtualization technologies emerged to overcome the performance,\n\nreliability, and scalability limitations of traditional virtualization platforms.\n\nModern virtualization technologies are discussed in Chapter 5.\n\nContainerization\n\nContainerization is a form of virtualization technology that allows for the\n\ncreation of virtual hosting environments referred to as “containers” without\n\nthe need to deploy a virtual server for each solution. A container is similar\n\nin concept to a virtual server in that it provides a virtual environment with\n\noperating system resources that can be used to host software programs and\n\nother IT resources.\n\nContainers are briefly introduced in the upcoming Basic Concepts and\n\nTerminology section and containerization technology is covered in detail in\n\nChapter 6.\n\nServerless Environments\n\nA serverless environment is a special operational runtime environment that\n\ndoes not require developers or system administrators to deploy or provision\n\nservers. Instead, it is equipped with technology that allows for the\n\ndeployment of special software packages that already include the required\n\nserver components and configuration information.\n\nUpon deployment, the serverless environment automatically implements\n\nand activates an application deployment together with its packaged server,\n\nwithout the administrator having to do anything further. Programs are\n\ndesigned, coded and deployed alongside the descriptor of the underlying\n\nrequired runtime and any dependencies that may exist. Once deployed, the\n\nserverless environment can run and scale the application and ensure its on-\n\ngoing availability and scalability.\n\nContemporary software architectures deployed in clouds can benefit greatly\n\nfrom serverless environments. More details regarding serverless technology\n\nare provided in Chapter 5.\n\n3.2 Basic Concepts and Terminology\n\nThis section establishes a set of basic terms that represent the fundamental\n\nconcepts and aspects pertaining to the notion of a cloud and its most\n\nprimitive artifacts.\n\nCloud\n\nA cloud refers to a distinct IT environment that is designed for the purpose\n\nof remotely provisioning scalable and measured IT resources. The term\n\noriginated as a metaphor for the Internet which is, in essence, a network of\n\nnetworks providing remote access to a set of decentralized IT resources.\n\nPrior to cloud computing becoming its own formalized IT industry segment,\n\nthe symbol of a cloud was commonly used to represent the Internet in a\n\nvariety of specifications and mainstream documentation of Web-based\n\narchitectures. This same symbol is now used to specifically represent the\n\nboundary of a cloud environment, as shown in Figure 3.1.\n\nFigure 3.1\n\nThe symbol used to denote the boundary of a cloud environment.\n\nIt is important to distinguish the term “cloud” and the cloud symbol from\n\nthe Internet. As a specific environment used to remotely provision IT\n\nresources, a cloud has a finite boundary. There are many individual clouds\n\nthat are accessible via the Internet. Whereas the Internet provides open\n\naccess to many Web-based IT resources, a cloud is typically privately\n\nowned and offers access to IT resources that is metered.\n\nMuch of the Internet is dedicated to the access of content-based IT\n\nresources published via the World Wide Web. IT resources provided by\n\ncloud environments, on the other hand, are dedicated to supplying back-end\n\nprocessing capabilities and user-based access to these capabilities. Another\n\nkey distinction is that it is not necessary for clouds to be Web-based even if\n\nthey are commonly based on Internet protocols and technologies. Protocols\n\nrefer to standards and methods that allow computers to communicate with\n\neach other in a pre-defined and structured manner. A cloud can be based on\n\nthe use of any protocols that allow for the remote access to its IT resources.\n\nNote\n\nDiagrams in this book depict the Internet using the globe\n\nsymbol.\n\nContainer\n\nContainers (Figure 3.2) are commonly used in clouds to provide highly\n\noptimized virtual hosting environments capable of providing only the\n\nresources required for the software programs they host.\n\nFigure 3.2\n\nThe figure on the left is the general symbol used to represent a container.\n\nThe figure on the right (with rounded edges) is used in architectural\n\ndiagrams to represent a container, especially when the contents of the\n\ncontainer need to be shown.\n\nIT Resource\n\nAn IT resource is a physical or virtual IT-related artifact that can be either\n\nsoftware-based, such as a virtual server or a custom software program, or\n\nhardware-based, such as a physical server or a network device (Figure 3.3).\n\nFigure 3.3\n\nExamples of common IT resources and their corresponding symbols.\n\nFigure 3.4 illustrates how the cloud symbol can be used to define a\n\nboundary for a cloud-based environment that hosts and provisions a set of\n\nIT resources. The displayed IT resources are consequently considered to be\n\ncloud-based IT resources.\n\nFigure 3.4\n\nA cloud is hosting eight IT resources: three virtual servers, two cloud\n\nservices, and three storage devices.\n\nTechnology architectures and various interaction scenarios involving IT\n\nresources are illustrated in diagrams like the one shown in Figure 3.4. It is\n\nimportant to note the following points when studying and working with\n\nthese diagrams:\n\nThe IT resources shown within the boundary of a given cloud symbol\n\nusually do not represent all of the available IT resources hosted by that\n\ncloud. Subsets of IT resources are generally highlighted to demonstrate a\n\nparticular topic.\n\nFocusing on the relevant aspects of a topic requires many of these diagrams\n\nto intentionally provide abstracted views of the underlying technology\n\narchitectures. This means that only a portion of the actual technical details\n\nare shown.\n\nFurthermore, some diagrams will display IT resources outside of the cloud\n\nsymbol. This convention is used to indicate IT resources that are not cloud-\n\nbased.\n\nNote\n\nThe virtual server IT resource displayed in Figure 3.3 is\n\nfurther discussed in Chapters 5 and 8. Physical servers are\n\nsometimes referred to as physical hosts (or just hosts) in\n\nreference to the fact that they are responsible for hosting\n\nvirtual servers.\n\nOn-Premise\n\nAs a distinct and remotely accessible environment, a cloud represents an\n\noption for the deployment of IT resources. An IT resource that is hosted in a\n\nconventional IT enterprise within an organizational boundary (that does not\n\nspecifically represent a cloud) is considered to be located on the premises of\n\nthe IT enterprise. In this book the term on-premise is used as another way of\n\nstating “on the premises of a controlled IT environment that is not cloud-\n\nbased.” This term is used to help distinguish an IT resource residing within\n\nthe premises of an IT enterprise from one that is “cloud-based.” In other\n\nwords, an IT resource that is on-premise cannot be cloud-based, and vice-\n\nversa.\n\nNote the following key points:\n\nAn on-premise IT resource can access and interact with a cloud-based IT\n\nresource.\n\nAn on-premise IT resource can be moved to a cloud, thereby changing it to\n\na cloud-based IT resource.\n\nRedundant deployments of an IT resource can exist in both on-premise and\n\ncloud-based environments.\n\nIf the distinction between on-premise and cloud-based IT resources is\n\nconfusing in relation to private clouds (described in the Cloud Deployment\n\nModels section of Chapter 4), then an alternative qualifier can be used.\n\nCloud Consumers and Cloud Providers\n\nThe party that provides cloud-based IT resources is the cloud provider. The\n\nparty that uses cloud-based IT resources is the cloud consumer. These terms\n\nrepresent roles usually assumed by organizations in relation to clouds and\n\ncorresponding cloud provisioning contracts. These roles are formally\n\ndefined in Chapter 4, as part of the Roles and Boundaries section.\n\nScaling\n\nScaling, from an IT resource perspective, represents the ability of the IT\n\nresource to handle increased or decreased usage demands.\n\nThe following are types of scaling:\n\nHorizontal Scaling – scaling out and scaling in\n\nVertical Scaling – scaling up and scaling down\n\nThe next two sections briefly describe each.\n\nHorizontal Scaling\n\nThe allocating or releasing of IT resources that are of the same type is\n\nreferred to as horizontal scaling (Figure 3.5). The horizontal allocation of\n\nresources is referred to as scaling out and the horizontal releasing of\n\nresources is referred to as scaling in. Horizontal scaling is a common form\n\nof scaling within cloud environments.\n\nFigure 3.5\n\nAn IT resource (Virtual Server A) is scaled out by adding more of the same\n\nIT resources (Virtual Servers B and C).\n\nVertical Scaling\n\nWhen an existing IT resource is replaced by another with higher or lower\n\ncapacity, vertical scaling is considered to have occurred (Figure 3.6).\n\nSpecifically, the replacing of an IT resource with another that has a higher\n\ncapacity is referred to as scaling up and the replacing of an IT resource with\n\nanother that has a lower capacity is considered scaling down. Vertical\n\nscaling is less common in cloud environments due to the downtime required\n\nwhile the replacement is taking place.\n\nFigure 3.6\n\nAn IT resource (a virtual server with two CPUs) is scaled up by replacing it\n\nwith a more powerful IT resource with increased capacity for data storage\n\n(a physical server with four CPUs).\n\nTable 3.1 provides a brief overview of common pros and cons associated\n\nwith horizontal and vertical scaling.\n\nTable 3-1\n\nA comparison of horizontal and vertical scaling.\n\nCloud Service\n\nAlthough a cloud is a remotely accessible environment, not all IT resources\n\nresiding within a cloud can be made available for remote access. For\n\nexample, a database or a physical server deployed within a cloud may only\n\nbe accessible by other IT resources that are within the same cloud. A\n\nsoftware program with a published API may be deployed specifically to\n\nenable access by remote clients.\n\nA cloud service is any IT resource that is made remotely accessible via a\n\ncloud. Unlike other IT fields that fall under the service technology umbrella\n\n—such as service-oriented architecture—the term “service” within the\n\ncontext of cloud computing is especially broad. A cloud service can exist as\n\na simple Web-based software program with a technical interface invoked\n\nvia the use of a messaging protocol, or as a remote access point for\n\nadministrative tools or larger environments and other IT resources.\n\nIn Figure 3.7, the yellow circle symbol is used to represent the cloud service\n\nas a simple Web-based software program. A different IT resource symbol\n\nmay be used in the latter case, depending on the nature of the access that is\n\nprovided by the cloud service.\n\nFigure 3.7\n\nA cloud service with a published technical interface is being accessed by a\n\nconsumer outside of the cloud (left). A cloud service that exists as a virtual\n\nserver is also being accessed from outside of the cloud’s boundary (right).\n\nThe cloud service on the left is likely being invoked by a consumer program\n\nthat was designed to access the cloud service’s published technical\n\ninterface. The cloud service on the right may be accessed by a human user\n\nthat has remotely logged on to the virtual server.\n\nThe driving motivation behind cloud computing is to provide IT resources\n\nas services that encapsulate other IT resources, while offering functions for\n\nclients to use and leverage remotely. A multitude of models for generic\n\ntypes of cloud services have emerged, most of which are labeled with the\n\n“as-a-service” suffix.\n\nNote\n\nCloud service usage conditions are typically expressed in a\n\nservice-level agreement (SLA) that is the human-readable\n\npart of a service contract between a cloud provider and\n\ncloud consumer that describes QoS (Quality of Service)\n\nfeatures, behaviors, and limitations of a cloud-based service\n\nor other provisions.\n\nAn SLA provides details of various measurable\n\ncharacteristics related to IT outcomes, such as uptime,\n\nsecurity characteristics, and other specific QoS features,\n\nincluding availability, reliability, and performance. Since the\n\nimplementation of a service is hidden from the cloud\n\nconsumer, an SLA becomes a critical specification. SLAs\n\nare covered in detail in Chapter 18.\n\nCloud Service Consumer\n\nThe cloud service consumer is a temporary runtime role assumed by a\n\nsoftware program when it accesses a cloud service.",
      "page_number": 48
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 70-92)",
      "start_page": 70,
      "end_page": 92,
      "detection_method": "synthetic",
      "content": "As shown in Figure 3.8, common types of cloud service consumers can\n\ninclude software programs and services capable of remotely accessing\n\ncloud services with published service contracts, as well as workstations,\n\nlaptops and mobile devices running software capable of remotely accessing\n\nother IT resources positioned as cloud services.\n\nFigure 3.8\n\nExamples of cloud service consumers. Depending on the nature of a given\n\ndiagram, an artifact labeled as a cloud service consumer may be a software\n\nprogram or a hardware device (in which case it is implied that it is running\n\na software program capable of acting as a cloud service consumer).\n\n3.3 Goals and Benefits\n\nThe common benefits associated with adopting cloud computing are\n\nexplained in this section.\n\nNote\n\nThe following sections make reference to the terms “public\n\ncloud” and “private cloud.” These terms are described in the\n\nCloud Deployment Models section in Chapter 4.\n\nIncreased Responsiveness\n\nCloud computing plays an essential role in enhancing an organization’s\n\nbusiness agility by empowering it to be more responsive to business and\n\nusage scenarios that can be more effectively addressed by leveraging native\n\ncloud capabilities, such as scalability on-demand, data availability, reduced\n\ninfrastructure maintenance, reduced business complexity, automation, and\n\nincreased up-time.\n\nFor example, greater data availability can enable employees to more easily\n\nwork remotely, thereby providing staff with increased flexibility and\n\nproductivity.\n\nUtilizing platforms maintained by cloud providers, frees organizations from\n\nthe responsibilities they would normally have for administering them\n\ninternally. This can also reduce the complexity infrastructure environments,\n\nthereby introducing novel ways for employees to collaborate and work\n\nthrough faster and less complex technology rollouts for new business\n\ninitiatives.\n\nUltimately, cloud computing empowers organizations to become\n\nsignificantly more responsive removing or reducing many organizational\n\nburdens associated with business solution development, deployment and\n\nmaintenance, and by reducing time-to-market timelines.\n\nReduced Investments and Proportional Costs\n\nSimilar to a product wholesaler that purchases goods in bulk for lower price\n\npoints, public cloud providers base their business model on the mass-\n\nacquisition of IT resources that are then made available to cloud consumers\n\nvia attractively priced leasing packages. This opens the door for\n\norganizations to gain access to powerful infrastructure without having to\n\npurchase it themselves.\n\nThe most common economic rationale for investing in cloud-based IT\n\nresources is in the reduction or outright elimination of up-front IT\n\ninvestments, namely hardware and software purchases and ownership costs.\n\nA cloud’s Measured Usage characteristic represents a feature-set that allows\n\nmeasured operational expenditures (directly related to business\n\nperformance) to replace anticipated capital expenditures. This is also\n\nreferred to as proportional costs.\n\nThis elimination or minimization of up-front financial commitments allows\n\nenterprises to start small and accordingly increase IT resource allocation as\n\nrequired. Moreover, the reduction of up-front capital expenses allows for\n\nthe capital to be redirected to the core business investment. In its most basic\n\nform, opportunities to decrease costs are derived from the deployment and\n\noperation of large-scale data centers by major cloud providers. Such data\n\ncenters are commonly located in destinations where real estate, IT\n\nprofessionals, and network bandwidth can be obtained at lower costs,\n\nresulting in both capital and operational savings.\n\nThe same rationale applies to operating systems, middleware or platform\n\nsoftware, and application software. Pooled IT resources are made available\n\nto and shared by multiple cloud consumers, resulting in increased or even\n\nmaximum possible utilization. Operational costs and inefficiencies can be\n\nfurther reduced by applying proven practices and patterns for optimizing\n\ncloud architectures, their management, and their governance.\n\nCommon measurable benefits to cloud consumers include:\n\nOn-demand access to pay-as-you-go computing resources on a short-term\n\nbasis (such as processors by the hour), and the ability to release these\n\ncomputing resources when they are no longer needed.\n\nThe perception of having unlimited computing resources that are available\n\non-demand, thereby reducing the need to prepare for provisioning.\n\nThe ability to add or remove IT resources at a fine-grained level, such as\n\nmodifying available storage disk space by single gigabyte increments.\n\nAbstraction of the infrastructure so applications are not locked into devices\n\nor locations and can be easily moved if needed.\n\nFor example, a company with sizable batch-centric tasks can complete them\n\nas quickly as their application software can scale. Using 100 servers for one\n\nhour costs the same as using one server for 100 hours. This “elasticity” of\n\nIT resources, achieved without requiring steep initial investments to create a\n\nlarge-scale computing infrastructure, can be extremely compelling.\n\nDespite the ease with which many identify the financial benefits of cloud\n\ncomputing, the actual economics can be complex to calculate and assess.\n\nThe decision to proceed with a cloud computing adoption strategy will\n\ninvolve much more than a simple comparison between the cost of leasing\n\nand the cost of purchasing. For example, the financial benefits of dynamic\n\nscaling and the risk transference of both over-provisioning (under-\n\nutilization) and under-provisioning (over-utilization) must also be\n\naccounted for. Chapter 17 explores common criteria and formulas for\n\nperforming detailed financial comparisons and assessments.\n\nNote\n\nAnother area of cost savings offered by clouds is the “as-a-\n\nservice” usage model, whereby technical and operational\n\nimplementation details of IT resource provisioning are\n\nabstracted from cloud consumers and packaged into “ready-\n\nto-use” or “off-the-shelf” solutions. These services-based\n\nproducts can simplify and expedite the development,\n\ndeployment, and administration of IT resources when\n\ncompared to performing equivalent tasks with on-premise\n\nsolutions. The resulting savings in time and required IT\n\nexpertise can be significant and can contribute to the\n\njustification of adopting cloud computing.\n\nIncreased Scalability\n\nBy providing pools of IT resources, along with tools and technologies\n\ndesigned to leverage them collectively, clouds can instantly and\n\ndynamically allocate IT resources to cloud consumers, on-demand or via\n\nthe cloud consumer’s direct configuration. This empowers cloud consumers\n\nto scale their cloud-based IT resources to accommodate processing\n\nfluctuations and peaks automatically or manually. Similarly, cloud-based IT\n\nresources can be released (automatically or manually) as processing\n\ndemands decrease. A simple example of usage demand fluctuations\n\nthroughout a 24 hour period is provided in Figure 3.9.\n\nFigure 3.9\n\nAn example of an organization’s changing demand for an IT resource over\n\nthe course of a day.\n\nThe inherent, built-in feature of clouds to provide flexible levels of\n\nscalability to IT resources is directly related to the aforementioned\n\nproportional costs benefit. Besides the evident financial gain to the\n\nautomated reduction of scaling, the ability of IT resources to always meet\n\nand fulfill unpredictable usage demands avoids potential loss of business\n\nthat can occur when usage thresholds are met.\n\nNote\n\nWhen associating the benefit of Increased Scalability with\n\nthe capacity planning strategies introduced earlier in the\n\nBusiness Drivers section, the Lag and Match Strategies are\n\ngenerally more applicable due to a cloud’s ability to scale IT\n\nresources on-demand.\n\nIncreased Availability and Reliability\n\nThe availability and reliability of IT resources are directly associated with\n\ntangible business benefits. Outages limit the time an IT resource can be\n\n“open for business” for its customers, thereby limiting its usage and\n\nrevenue generating potential. Runtime failures that are not immediately\n\ncorrected can have a more significant impact during high-volume usage\n\nperiods. Not only is the IT resource unable to respond to customer requests,\n\nits unexpected failure can decrease overall customer confidence.\n\nA hallmark of the typical cloud environment is its intrinsic ability to\n\nprovide extensive support for increasing the availability of a cloud-based IT\n\nresource to minimize or even eliminate outages, and for increasing its\n\nreliability so as to minimize the impact of runtime failure conditions.\n\nSpecifically:\n\nAn IT resource with increased availability is accessible for longer periods of\n\ntime (for example, 22 hours out of a 24 hour day). Cloud providers\n\ngenerally offer “resilient” IT resources for which they are able to guarantee\n\nhigh levels of availability.\n\nAn IT resource with increased reliability is able to better avoid and recover\n\nfrom exception conditions. The modular architecture of cloud environments\n\nprovides extensive failover support that increases reliability.\n\nIt is important that organizations carefully examine the SLAs offered by\n\ncloud providers when considering the leasing of cloud-based services and\n\nIT resources. Although many cloud environments are capable of offering\n\nremarkably high levels of availability and reliability, it comes down to the\n\nguarantees made in the SLA that typically represent their actual contractual\n\nobligations.\n\n3.4 Risks and Challenges\n\nSeveral of the most critical cloud computing challenges are presented and\n\nexamined in this section.\n\nIncreased Vulnerability Due to Overlapping Trust Boundaries\n\nMoving business data to the cloud means that the responsibility over data\n\nsecurity is shared with the cloud provider. The remote usage of IT resources\n\nrequires an expansion of trust boundaries by the cloud consumer to include\n\nthe cloud, external to the organization. It can be difficult to establish a\n\nsecurity architecture that spans such a trust boundary without introducing\n\nvulnerabilities unless cloud consumers and cloud providers happen to\n\nsupport the same or compatible security frameworks, which is improbable\n\nwith public clouds.\n\nAnother consequence of overlapping trust boundaries relates to the cloud\n\nprovider’s privileged access to cloud consumer data. The extent to which\n\nthe data is secure is now limited to the security controls and policies applied\n\nby both the cloud consumer and cloud provider. Furthermore, there can be\n\noverlapping trust boundaries from different cloud consumers because cloud-\n\nbased IT resources are commonly shared.\n\nThe overlapping of trust boundaries and the increased exposure of data can\n\nprovide malicious cloud consumers (human and automated) with greater\n\nopportunities to attack IT resources and steal or damage business data.\n\nFigure 3.10 illustrates a scenario whereby two organizations accessing the\n\nsame cloud service are required to extend their respective trust boundaries\n\nto the cloud, resulting in overlapping trust boundaries. Cloud providers are\n\nrequired to offer security mechanisms that accommodate the security\n\nrequirements of both cloud service consumers.\n\nFigure 3.10\n\nThe shaded area with diagonal lines indicates the overlap of two\n\norganizations’ trust boundaries.\n\nOverlapping trust boundaries is a security threat that is discussed in more\n\ndetail in Chapter 7.\n\nIncreased Vulnerability Due to Shared Security Responsibility\n\nInformation security related to on-premise resources is clearly the\n\nresponsibility of the organization that owns those resources. However,\n\ninformation security related to cloud-based resources is not the sole\n\nresponsibility of the cloud provider, even if the cloud-based resources are\n\nowned by the cloud provider. This is because the information stored and\n\nprocessed in them is owned by the cloud consumer.\n\nAs a result, information security in the cloud is a shared responsibility, with\n\nboth the cloud provider and the cloud consumer having a role to play in\n\nsecuring the cloud environment. It is important to be able to understand and\n\nidentify where the responsibility for each role begins and ends, as well as\n\nknowing how to address the security requirements that correspond to the\n\ncloud consumer.\n\nA cloud provider will typically propose a cloud shared responsibility model\n\nas part of the SLA. This model essentially outlines the respective\n\nresponsibilities of cloud provider and cloud consumer when it comes to\n\nsecuring data and applications in the cloud.\n\nIncreased Exposure to Cyber Threats\n\nThe increased adoption of contemporary digital technologies and digital\n\ntransformation practices has led organizations to move more IT resources\n\ntoward and build more solutions within cloud environments. This has\n\nopened the door to cybersecurity threats and risks that may be new to\n\norganizations and for which they need to be prepared (Figure 3.11).\n\nFigure 3.11\n\nAn organization that moves from only consuming content and services from\n\nthe Internet to offering its own content and services through the Internet\n\nincreases its exposure to cyber threats.\n\nAugmented exposure to cybersecurity threats due to increased exposure to\n\nthe Internet requires organizations to take action in the protection of IT\n\nassets, both on-premise and cloud-based. Cloud-based IT resources have the\n\nbenefit of shared responsibility for security and access control, both from\n\nthe cloud provider and from the cloud consumer. However, the ultimate\n\nresponsibility lies with the cloud consumer, who needs to address risk\n\nmanagement and cybersecurity risks responsibly and methodologically.\n\nReduced Operational Governance Control\n\nCloud consumers are usually allotted a level of governance control that is\n\nlower than that over on-premise IT resources. This can introduce risks\n\nassociated with how the cloud provider operates its cloud, as well as the\n\nexternal connections that are required for communication between the cloud\n\nand the cloud consumer.\n\nConsider the following examples:\n\nAn unreliable cloud provider may not maintain the guarantees it makes in\n\nthe SLAs that were published for its cloud services. This can jeopardize the\n\nquality of the cloud consumer solutions that rely on these cloud services.\n\nLonger geographic distances between the cloud consumer and cloud\n\nprovider can require additional network hops that introduce fluctuating\n\nlatency and potential bandwidth constraints.\n\nThe latter scenario is illustrated in Figure 3.12.\n\nFigure 3.12\n\nAn unreliable network connection compromises the quality of\n\ncommunication between cloud consumer and cloud provider environments.\n\nLegal contracts, when combined with SLAs, technology inspections, and\n\nmonitoring, can mitigate governance risks and issues. A cloud governance\n\nsystem is established through SLAs, given the “as-a-service” nature of\n\ncloud computing. A cloud consumer must keep track of the actual service\n\nlevel being offered and the other warranties that are made by the cloud\n\nprovider.\n\nNote that different cloud delivery models offer varying degrees of\n\noperational control granted to cloud consumers, as further explained in\n\nChapter 4.\n\nLimited Portability Between Cloud Providers\n\nDue to a lack of established industry standards within the cloud computing\n\nindustry, public clouds are commonly proprietary to various extents. For\n\ncloud consumers that have custom-built solutions with dependencies on\n\nthese proprietary environments, it can be challenging to move from one\n\ncloud provider to another.\n\nPortability is a measure used to determine the impact of moving cloud\n\nconsumer IT resources and data between clouds (Figure 3.13).\n\nFigure 3.13\n\nA cloud consumer’s application has a decreased level of portability when\n\nassessing a potential migration from Cloud A to Cloud B, because the cloud\n\nprovider of Cloud B does not support the same security technologies as\n\nCloud A.\n\nMulti-Regional Compliance and Legal Issues\n\nThird-party cloud providers will frequently establish data centers in\n\naffordable or convenient geographical locations. Cloud consumers will\n\noften not be aware of the physical location of their IT resources and data\n\nwhen hosted by public clouds. For some organizations, this can pose serious\n\nlegal concerns pertaining to industry or government regulations that specify\n\ndata privacy and storage policies. For example, some UK laws require\n\npersonal data belonging to UK citizens to be kept within the United\n\nKingdom.\n\nAnother potential legal issue pertains to the accessibility and disclosure of\n\ndata. Countries have laws that require some types of data to be disclosed to\n\ncertain government agencies or to the subject of the data. For example, a\n\nEuropean cloud consumer’s data that is located in the U.S. can be more\n\neasily accessed by government agencies (due to the U.S. Patriot Act) when\n\ncompared to data located in many European Union countries.\n\nMost regulatory frameworks recognize that cloud consumer organizations\n\nare ultimately responsible for the security, integrity, and storage of their\n\nown data, even when it is held by an external cloud provider.\n\nCost Overruns\n\nCreating a business case for cloud computing can be a difficult undertaking\n\ndue to the number of requirements, considerations and stakeholders that\n\nneed to be accommodated. Many organizations proceed with cloud\n\nmigration initiatives without a proper business case at all. This has been one\n\nof the root causes of cost overruns in cloud projects and has led to poor\n\nplanning, poor or absent governance and costly cloud risk mitigation\n\npolicies.\n\nTraditionally, a business case process is triggered by the need to justify\n\nlarge capital investments. However, with cloud environments that allow\n\nusers to quickly obtain desired capabilities, organizations can incorrectly\n\nassume that they will not require additional capital investments. As cloud\n\nadoption then grows, there is eventually a recognition that this operating\n\nmodel requires capital investment beyond the migration itself, but there\n\nmay not be a method of estimating the amount of the investment necessary\n\nprior to adoption or migration.\n\nChapter 4\n\nFundamental Concepts and Models\n\n4.1 Roles and Boundaries\n\n4.2 Cloud Characteristics\n\n4.3 Cloud Delivery Models\n\n4.4 Cloud Deployment Models\n\nThe upcoming sections cover introductory topic areas pertaining to the\n\nfundamental models used to categorize and define clouds and their most\n\ncommon service offerings, along with definitions of organizational roles\n\nand the specific set of characteristics that collectively distinguish a cloud.\n\n4.1 Roles and Boundaries\n\nOrganizations and humans can assume different types of pre-defined roles\n\ndepending on how they relate to and/or interact with a cloud and its hosted\n\nIT resources. Each of the upcoming roles participates in and carries out\n\nresponsibilities in relation to cloud-based activity. The following sections\n\ndefine these roles and identify their main interactions.\n\nCloud Provider\n\nThe organization that provides cloud-based IT resources is the cloud\n\nprovider. When assuming the role of cloud provider, an organization is\n\nresponsible for making cloud services available to cloud consumers, as per\n\nagreed upon SLA guarantees. The cloud provider is further tasked with any\n\nrequired management and administrative duties to ensure the on-going\n\noperation of the overall cloud infrastructure.\n\nCloud providers normally own the IT resources that are made available for\n\nlease by cloud consumers; however, some cloud providers also “resell” IT\n\nresources leased from other cloud providers.\n\nCloud Consumer\n\nA cloud consumer is an organization (or a human) that has a formal contract\n\nor arrangement with a cloud provider to use IT resources made available by\n\nthe cloud provider. Specifically, the cloud consumer uses a cloud service\n\nconsumer to access a cloud service (Figure 4.1).\n\nFigure 4.1\n\nA cloud consumer (Organization A) interacts with a cloud service from a\n\ncloud provider (that owns Cloud A). Within Organization A, the cloud\n\nservice consumer is being used to access the cloud service.\n\nThe figures in this book do not always explicitly label symbols as “cloud\n\nconsumers.” Instead, it is generally implied that organizations or humans\n\nshown remotely accessing cloud-based IT resources are considered cloud\n\nconsumers.\n\nNote\n\nWhen depicting interaction scenarios between cloud-based\n\nIT resources and consumer organizations, there are no strict\n\nrules as to how the terms “cloud service consumer” and\n\n“cloud consumer” are used in this book. The former is\n\nusually used to label software programs or applications that\n\nprogrammatically interface with a cloud service’s technical\n\ncontract or API. The latter term is broader in that it can be\n\nused to label an organization, an individual accessing a user-\n\ninterface, or a software program that assumes the role of\n\ncloud consumer when interacting with a cloud, a cloud-\n\nbased IT resource, or a cloud provider. The broad\n\napplicability of the “cloud consumer” term is intentional as\n\nit allows it to be used in figures that explore different types\n\nof consumer-provider relationships within different technical\n\nand business contexts.\n\nCloud Broker\n\nA third-party organization that assumes the responsibility of negotiating,\n\nmanaging and operating cloud services on behalf of a cloud consumer is\n\nassuming the role of cloud broker. Cloud brokers can provide mediation\n\nservices between cloud consumers and cloud providers, including\n\nintermediation, aggregation, arbitrage, and others.",
      "page_number": 70
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 93-113)",
      "start_page": 93,
      "end_page": 113,
      "detection_method": "synthetic",
      "content": "A cloud broker commonly provides these services for multiple cloud\n\nconsumers facing multiple cloud providers alternatively or simultaneously,\n\nacting as an integrator of cloud services and an aggregator of cloud\n\nconsumers, as shown in Figure 4.2.\n\nFigure 4.2\n\nA cloud broker offers the cloud-based services and IT resources from three\n\ndifferent cloud providers to its customers, Cloud Consumers A, B, and C.\n\nCloud Service Owner\n\nThe person or organization that legally owns a cloud service is called a\n\ncloud service owner. The cloud service owner can be the cloud consumer,\n\nor the cloud provider that owns the cloud within which the cloud service\n\nresides.\n\nFor example, either the cloud consumer of Cloud X or the cloud provider of\n\nCloud X could own Cloud Service A (Figures 4.3 and 4.4).\n\nFigure 4.3\n\nA cloud consumer can be a cloud service owner when it deploys its own\n\nservice in a cloud.\n\nFigure 4.4\n\nA cloud provider becomes a cloud service owner if it deploys its own cloud\n\nservice, typically for other cloud consumers to use.\n\nNote that a cloud consumer that owns a cloud service hosted by a third-\n\nparty cloud does not necessarily need to be the user (or consumer) of the\n\ncloud service. Several cloud consumer organizations develop and deploy\n\ncloud services in clouds owned by other parties for the purpose of making\n\nthe cloud services available to the general public.\n\nThe reason a cloud service owner is not called a cloud resource owner is\n\nbecause the cloud service owner role only applies to cloud services (which,\n\nas explained in Chapter 3, are externally accessible IT resources that reside\n\nin a cloud).\n\nCloud Resource Administrator\n\nA cloud resource administrator is the person or organization responsible for\n\nadministering a cloud-based IT resource (including cloud services). The\n\ncloud resource administrator can be (or belong to) the cloud consumer or\n\ncloud provider of the cloud within which the cloud service resides.\n\nAlternatively, it can be (or belong to) a third-party organization contracted\n\nto administer the cloud-based IT resource.\n\nFor example, a cloud service owner can contract a cloud resource\n\nadministrator to administer a cloud service (Figures 4.5 and 4.6).\n\nFigure 4.5\n\nA cloud resource administrator can be with a cloud consumer organization\n\nand administer remotely accessible IT resources that belong to the cloud\n\nconsumer.\n\nFigure 4.6\n\nA cloud resource administrator can be with a cloud provider organization\n\nfor which it can administer the cloud provider’s internally and externally\n\navailable IT resources.\n\nThe reason a cloud resource administrator is not referred to as a “cloud\n\nservice administrator” is because this role may be responsible for\n\nadministering cloud-based IT resources that don’t exist as cloud services.\n\nFor example, if the cloud resource administrator belongs to (or is contracted\n\nby) the cloud provider, IT resources not made remotely accessible may be\n\nadministered by this role (and these types of IT resources are not classified\n\nas cloud services).\n\nAdditional Roles\n\nThe NIST Cloud Computing Reference Architecture defines the following\n\nsupplementary roles:\n\nCloud Auditor – A third-party (often accredited) that conducts independent\n\nassessments of cloud environments assumes the role of the cloud auditor.\n\nThe typical responsibilities associated with this role include the evaluation\n\nof security controls, privacy impacts, and performance. The main purpose\n\nof the cloud auditor role is to provide an unbiased assessment (and possible\n\nendorsement) of a cloud environment to help strengthen the trust\n\nrelationship between cloud consumers and cloud providers.\n\nCloud Carrier – The party responsible for providing the wire-level\n\nconnectivity between cloud consumers and cloud providers assumes the\n\nrole of the cloud carrier. This role is often assumed by network and\n\ntelecommunication providers.\n\nWhile each is legitimate, most architectural scenarios covered in this book\n\ndo not include these roles.\n\nOrganizational Boundary\n\nAn organizational boundary represents the physical perimeter that\n\nsurrounds a set of IT resources that are owned and governed by an\n\norganization. The organizational boundary does not represent the boundary\n\nof an actual organization, only an organizational set of IT assets and IT\n\nresources. Similarly, clouds have an organizational boundary (Figure 4.7).\n\nFigure 4.7\n\nOrganizational boundaries of a cloud consumer (left), and a cloud provider\n\n(right), represented by a broken line notation.\n\nTrust Boundary\n\nWhen an organization assumes the role of cloud consumer to access cloud-\n\nbased IT resources, it needs to extend its trust beyond the physical boundary\n\nof the organization to include parts of the cloud environment.\n\nA trust boundary is a logical perimeter that typically spans beyond physical\n\nboundaries to represent the extent to which IT resources are trusted (Figure\n\n4.8). When analyzing cloud environments, the trust boundary is most\n\nfrequently associated with the trust issued by the organization acting as the\n\ncloud consumer.\n\nFigure 4.8\n\nAn extended trust boundary encompasses the organizational boundaries of\n\nthe cloud provider and the cloud consumer.\n\nNote\n\nAnother type of boundary relevant to cloud environments is\n\nthe logical network perimeter. This type of boundary is\n\nclassified as a cloud computing mechanism and is covered in\n\nChapter 8.\n\n4.2 Cloud Characteristics\n\nAn IT environment requires a specific set of characteristics to enable the\n\nremote provisioning of scalable and measured IT resources in an effective\n\nmanner. These characteristics need to exist to a meaningful extent for the IT\n\nenvironment to be considered an effective cloud.\n\nThe following characteristics are common to the majority of cloud\n\nenvironments:\n\non-demand usage\n\nubiquitous access\n\nmultitenancy (and resource pooling)\n\nelasticity\n\nmeasured usage\n\nresiliency\n\nCloud providers and cloud consumers can assess these characteristics\n\nindividually and collectively to measure the value offering of a given cloud\n\nplatform. Although cloud-based services and IT resources will inherit and\n\nexhibit individual characteristics to varying extents, usually the greater the\n\ndegree to which they are supported and utilized, the greater the resulting\n\nvalue proposition.\n\nOn-Demand Usage\n\nA cloud consumer can unilaterally access cloud-based IT resources giving\n\nthe cloud consumer the freedom to self-provision these IT resources. Once\n\nconfigured, usage of the self-provisioned IT resources can be automated,\n\nrequiring no further human involvement by the cloud consumer or cloud\n\nprovider. This results in an on-demand usage environment. Also known as\n\n“on-demand self-service usage,” this characteristic enables the service-\n\nbased and usage-driven features found in mainstream clouds.\n\nUbiquitous Access\n\nUbiquitous access represents the ability for a cloud service to be widely\n\naccessible. Establishing ubiquitous access for a cloud service can require\n\nsupport for a range of devices, transport protocols, interfaces, and security\n\ntechnologies. To enable this level of access generally requires that the cloud\n\nservice architecture be tailored to the particular needs of different cloud\n\nservice consumers.\n\nMultitenancy (and Resource Pooling)\n\nThe characteristic of a software program that enables an instance of the\n\nprogram to serve different consumers (tenants) whereby each is isolated\n\nfrom the other, is referred to as multitenancy. A cloud provider pools its IT\n\nresources to serve multiple cloud service consumers by using multitenancy\n\nmodels that frequently rely on the use of virtualization technologies.\n\nThrough the use of multitenancy technology, IT resources can be\n\ndynamically assigned and reassigned, according to cloud service consumer\n\ndemands.\n\nResource pooling allows cloud providers to pool large-scale IT resources to\n\nserve multiple cloud consumers. Different physical and virtual IT resources\n\nare dynamically assigned and reassigned according to cloud consumer\n\ndemand, typically followed by execution through statistical multiplexing.\n\nResource pooling is commonly achieved through multitenancy technology,\n\nand therefore encompassed by this multitenancy characteristic. See the\n\nResource Pooling Architecture section in Chapter 13 for a more detailed\n\nexplanation.\n\nFigures 4.9 and 4.10 illustrate the difference between single-tenant and\n\nmultitenant environments.\n\nFigure 4.9\n\nIn a single-tenant environment, each cloud consumer has a separate IT\n\nresource instance.\n\nFigure 4.10\n\nIn a multitenant environment, a single instance of an IT resource, such as a\n\ncloud storage device, serves multiple consumers.\n\nAs illustrated in Figure 4.10, multitenancy allows several cloud consumers\n\nto use the same IT resource or its instance while each remains unaware that\n\nit may be used by others.\n\nElasticity\n\nElasticity is the automated ability of a cloud to transparently scale IT\n\nresources, as required in response to runtime conditions or as pre-\n\ndetermined by the cloud consumer or cloud provider. Elasticity is often\n\nconsidered a core justification for the adoption of cloud computing,\n\nprimarily due to the fact that it is closely associated with the Reduced\n\nInvestment and Proportional Costs benefit. Cloud providers with vast IT\n\nresources can offer the greatest range of elasticity.\n\nMeasured Usage\n\nThe measured usage characteristic represents the ability of a cloud platform\n\nto keep track of the usage of its IT resources, primarily by cloud consumers.\n\nBased on what is measured, the cloud provider can charge a cloud\n\nconsumer only for the IT resources actually used and/or for the timeframe\n\nduring which access to the IT resources was granted. In this context,\n\nmeasured usage is closely related to the on-demand characteristic.\n\nMeasured usage is not limited to tracking statistics for billing purposes. It\n\nalso encompasses the general monitoring of IT resources and related usage\n\nreporting (for both cloud provider and cloud consumers). Therefore,\n\nmeasured usage is also relevant to clouds that do not charge for usage\n\n(which may be applicable to the private cloud deployment model described\n\nin the upcoming Cloud Deployment Models section).\n\nResiliency\n\nResilient computing is a form of failover that distributes redundant\n\nimplementations of IT resources across physical locations. IT resources can\n\nbe pre-configured so that if one becomes deficient, processing is\n\nautomatically handed over to another redundant implementation. Within\n\ncloud computing, the characteristic of resiliency can refer to redundant IT\n\nresources within the same cloud (but in different physical locations) or\n\nacross multiple clouds. Cloud consumers can increase both the reliability\n\nand availability of their applications by leveraging the resiliency of cloud-\n\nbased IT resources (Figure 4.11).\n\nFigure 4.11\n\nA resilient system in which Cloud B hosts a redundant implementation of\n\nCloud Service A to provide failover in case Cloud Service A on Cloud A\n\nbecomes unavailable.\n\n4.3 Cloud Delivery Models\n\nA cloud delivery model represents a specific, pre-packaged combination of\n\nIT resources offered by a cloud provider. Three common cloud delivery\n\nmodels have become widely established and formalized:\n\nInfrastructure-as-a-Service (IaaS)\n\nPlatform-as-a-Service (PaaS)\n\nSoftware-as-a-Service (SaaS)\n\nThese three models are interrelated in how the scope of one can encompass\n\nthat of another, as explored in the Combining Cloud Delivery Models\n\nsection later in this chapter.\n\nNote\n\nNote that a cloud delivery model can be referred to as a\n\ncloud service delivery model because each model is\n\nclassified as a different type of cloud service offering.\n\nInfrastructure-as-a-Service (IaaS)\n\nThe IaaS delivery model represents a self-contained IT environment\n\ncomprised of infrastructure-centric IT resources that can be accessed and\n\nmanaged via cloud service-based interfaces and tools. This environment can\n\ninclude hardware, network, connectivity, operating systems, and other\n\n“raw” IT resources. In contrast to traditional hosting or outsourcing\n\nenvironments, with IaaS, IT resources are typically virtualized and\n\npackaged into bundles that simplify up-front runtime scaling and\n\ncustomization of the infrastructure.\n\nThe general purpose of an IaaS environment is to provide cloud consumers\n\nwith a high level of control and responsibility over its configuration and\n\nutilization. The IT resources provided by IaaS are generally not pre-\n\nconfigured, placing the administrative responsibility directly upon the cloud\n\nconsumer. This model is therefore used by cloud consumers that require a\n\nhigh level of control over the cloud-based environment they intend to\n\ncreate.\n\nSometimes cloud providers will contract IaaS offerings from other cloud\n\nproviders to scale their own cloud environments. The types and brands of\n\nthe IT resources provided by IaaS products offered by different cloud\n\nproviders can vary. IT resources available through IaaS environments are\n\ngenerally offered as freshly initialized virtual instances. A central and\n\nprimary IT resource within a typical IaaS environment is the virtual server.\n\nVirtual servers are leased by specifying server hardware requirements, such\n\nas processor capacity, memory, and local storage space, as shown in Figure\n\n4.12.\n\nFigure 4.12\n\nA cloud consumer is using a virtual server within an IaaS environment.\n\nCloud consumers are provided with a range of contractual guarantees by\n\nthe cloud provider, pertaining to characteristics such as capacity,\n\nperformance, and availability.\n\nPlatform-as-a-Service (PaaS)\n\nThe PaaS delivery model represents a pre-defined “ready-to-use”\n\nenvironment typically comprised of already deployed and configured IT\n\nresources. Specifically, PaaS relies on (and is primarily defined by) the\n\nusage of a ready-made environment that establishes a set of pre-packaged\n\nproducts and tools used to support the entire delivery lifecycle of custom\n\napplications.\n\nCommon reasons a cloud consumer would use and invest in a PaaS\n\nenvironment include:\n\nThe cloud consumer wants to extend on-premise environments into the\n\ncloud for scalability and economic purposes.\n\nThe cloud consumer uses the ready-made environment to entirely substitute\n\nan on-premise environment.\n\nThe cloud consumer wants to become a cloud provider and deploys its own\n\ncloud services to be made available to other external cloud consumers.\n\nBy working within a ready-made platform, the cloud consumer is spared the\n\nadministrative burden of setting up and maintaining the bare infrastructure\n\nIT resources provided via the IaaS model. Conversely, the cloud consumer\n\nis granted a lower level of control over the underlying IT resources that host\n\nand provision the platform (Figure 4.13).\n\nFigure 4.13\n\nA cloud consumer is accessing a ready-made PaaS environment. The\n\nquestion mark indicates that the cloud consumer is intentionally shielded",
      "page_number": 93
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 114-137)",
      "start_page": 114,
      "end_page": 137,
      "detection_method": "synthetic",
      "content": "from the implementation details of the platform.\n\nPaaS products are available with different development stacks. For\n\nexample, Google App Engine offers a Java and Python-based environment.\n\nThe ready-made environment is further described as a cloud computing\n\nmechanism in Chapter 8.\n\nSoftware-as-a-Service (SaaS)\n\nA software program positioned as a shared cloud service and made\n\navailable as a “product” or generic utility represents the typical profile of a\n\nSaaS offering. The SaaS delivery model is typically used to make a reusable\n\ncloud service widely available (often commercially) to a range of cloud\n\nconsumers. An entire marketplace exists around SaaS products that can be\n\nleased and used for different purposes and via different terms (Figure 4.14).\n\nFigure 4.14\n\nThe cloud service consumer is given access to the cloud service contract,\n\nbut not to any underlying IT resources or implementation details.\n\nA cloud consumer is generally granted very limited administrative control\n\nover a SaaS implementation. It is most often provisioned by the cloud\n\nprovider, but it can be legally owned by whichever entity assumes the cloud\n\nservice owner role. For example, an organization acting as a cloud\n\nconsumer while using and working with a PaaS environment can build a\n\ncloud service that it decides to deploy in that same environment as a SaaS\n\noffering. The same organization then effectively assumes the cloud provider\n\nrole as the SaaS-based cloud service is made available to other\n\norganizations that act as cloud consumers when using that cloud service.\n\nComparing Cloud Delivery Models\n\nProvided in this section are two tables that compare different aspects of\n\ncloud delivery model usage and implementation. Table 4.1 contrasts control\n\nlevels and Table 4.2 compares typical responsibilities and usage.\n\nTable 4-1\n\nA comparison of typical cloud delivery model control levels.\n\nTable 4-2\n\nTypical activities carried out by cloud consumers and cloud providers\n\nin relation to the cloud delivery models.\n\nCombining Cloud Delivery Models\n\nThe three base cloud delivery models comprise a natural provisioning\n\nhierarchy, allowing for opportunities for the combined application of the\n\nmodels to be explored. The upcoming sections briefly highlight\n\nconsiderations pertaining to two common combinations.\n\nIaaS + PaaS\n\nA PaaS environment will be built upon an underlying infrastructure\n\ncomparable to the physical and virtual servers and other IT resources\n\nprovided in an IaaS environment. Figure 4.15 shows how these two models\n\ncan conceptually be combined into a simple layered architecture.\n\nFigure 4.15\n\nA PaaS environment based on the IT resources provided by an underlying\n\nIaaS environment.\n\nA cloud provider would not normally need to provision an IaaS\n\nenvironment from its own cloud in order to make a PaaS environment\n\navailable to cloud consumers. So how would the architectural view\n\nprovided by Figure 4.16 be useful or applicable? Let’s say that the cloud\n\nprovider offering the PaaS environment chose to lease an IaaS environment\n\nfrom a different cloud provider.\n\nThe motivation for such an arrangement may be influenced by economics\n\nor maybe because the first cloud provider is close to exceeding its existing\n\ncapacity by serving other cloud consumers. Or, perhaps a particular cloud\n\nconsumer imposes a legal requirement for data to be physically stored in a\n\nspecific region (different from where the first cloud provider’s cloud\n\nresides), as illustrated in Figure 4.16.\n\nFigure 4.16\n\nAn example of a contract between Cloud Providers X and Y, in which\n\nservices offered by Cloud Provider X are physically hosted on virtual\n\nservers belonging to Cloud Provider Y. Sensitive data that is legally\n\nrequired to stay in a specific region is physically kept in Cloud B, which is\n\nphysically located in that region.\n\nIaaS + PaaS + SaaS\n\nAll three cloud delivery models can be combined to establish layers of IT\n\nresources that build upon each other. For example, by adding on to the\n\npreceding layered architecture shown in Figure 4.16, the ready-made\n\nenvironment provided by the PaaS environment can be used by the cloud\n\nconsumer organization to develop and deploy its own SaaS cloud services\n\nthat it can then make available as commercial products (Figure 4.17).\n\nFigure 4.17\n\nA simple layered view of an architecture comprised of IaaS and PaaS\n\nenvironments hosting three SaaS cloud service implementations.\n\nCloud Delivery Sub Models\n\nMany specialized variations of the cloud delivery models exist, each\n\ncomprised of a distinct combination of IT resources. These cloud delivery\n\nsub models are also typically named using the “as-a-Service” convention\n\nand each can be mapped to one of the three basic cloud delivery models.\n\nFor example, the Database-as-a-Service sub model (Figure 4.18) belongs to\n\nthe PaaS model, since a database system is commonly a component of the\n\nready-made environment that is part of a PaaS platform.\n\nFigure 4.18\n\nThe Database-as-a-Service cloud delivery sub model is represented by a\n\ncloud provider to provide access to databases.\n\nSimilarly, Security-as-a-Service is a sub model of SaaS and is used to\n\nprovide access to features that can be used to secure cloud consumer IT\n\nassets.\n\nAnother example is the Storage-as-a-Service sub model (Figure 4.19) of\n\nIaaS that a cloud provider can use to delivers cloud storage-related services\n\nto cloud consumers.\n\nFigure 4.19\n\nA Storage-as-a-Service offering can provide different storage-related\n\nservices, such as structured and unstructured data storage, file storage,\n\nobject storage, and long-term archive storage.\n\nAlso considered a sub model of SaaS is the cloud-native delivery sub model\n\nthat allows for cloud-native applications to be built and deployed as\n\ncollections of self-contained services packaged in lightweight containers.\n\nCloud-native applications (Figure 4.20) have no preference for any\n\nparticular operating system or computer and work at a higher degree of\n\nabstraction. These types of applications run on infrastructure that is\n\nvirtualized, shared, and elastic. They may align with the underlying\n\ninfrastructure to dynamically grow and shrink in response to varied load.\n\nFigure 4.20\n\nA cloud-native application deployed using multiple containers.\n\nOther examples of common cloud delivery sub models include (but are not\n\nlimited to) the following:\n\nCommunication-as-a-Service (a sub model of SaaS)\n\nIntegration-as-a-Service (a sub model of PaaS)\n\nTesting-as-a-Service (a sub model of SaaS)\n\nProcess-as-a-Service (a sub model of SaaS)\n\nDesktop-as-a-Service (a sub model of IaaS)\n\n4.4 Cloud Deployment Models\n\nA cloud deployment model represents a specific type of cloud environment,\n\nprimarily distinguished by ownership, size, and access.\n\nThere are four common cloud deployment models:\n\nPublic cloud\n\nPrivate cloud\n\nMulti-cloud\n\nHybrid cloud\n\nThe following sections describe each model.\n\nPublic Clouds\n\nA public cloud is a publicly accessible cloud environment owned by a third-\n\nparty cloud provider. The IT resources on public clouds are usually\n\nprovisioned via the previously described cloud delivery models and are\n\ngenerally offered to cloud consumers at a cost or are commercialized via\n\nother avenues (such as advertisement).\n\nThe cloud provider is responsible for the creation and on-going\n\nmaintenance of the public cloud and its IT resources. Many of the scenarios\n\nand architectures explored in upcoming chapters involve public clouds and\n\nthe relationship between the providers and consumers of IT resources via\n\npublic clouds.\n\nFigure 4.21 shows a partial view of the public cloud landscape, highlighting\n\nsome of the primary vendors in the marketplace.\n\nFigure 4.21\n\nOrganizations act as cloud consumers when accessing cloud services and\n\nIT resources made available by different cloud providers.\n\nPrivate Clouds\n\nA private cloud is owned by a single organization. Private clouds enable an\n\norganization to use cloud computing technology as a means of centralizing\n\naccess to IT resources by different parts, locations, or departments of the\n\norganization. When a private cloud exists as a controlled environment, the\n\nproblems described in the Risks and Challenges section from Chapter 3 do\n\nnot tend to apply.\n\nThe use of a private cloud can change how organizational and trust\n\nboundaries are defined and applied. The actual administration of a private\n\ncloud environment may be carried out by internal or outsourced staff.\n\nFigure 4.22\n\nA cloud service consumer in the organization’s on-premise environment\n\naccesses a cloud service hosted on the same organization’s private cloud\n\nvia a virtual private network.\n\nWith a private cloud, the same organization is technically both the cloud\n\nconsumer and cloud provider (Figure 4.22). In order to differentiate these\n\nroles:\n\na separate organizational department typically assumes the responsibility\n\nfor provisioning the cloud (and therefore assumes the cloud provider role)\n\ndepartments requiring access to the private cloud assume the cloud\n\nconsumer role\n\nIt is important to use the terms “on-premise” and “cloud-based” correctly\n\nwithin the context of a private cloud. Even though the private cloud may\n\nphysically reside on the organization’s premises, IT resources it hosts are\n\nstill considered “cloud-based” as long as they are made remotely accessible\n\nto cloud consumers. IT resources hosted outside of the private cloud by the\n\ndepartments acting as cloud consumers are therefore considered “on-\n\npremise” in relation to the private cloud-based IT resources.\n\nMulti-Clouds\n\nWith a multi-cloud deployment model, a cloud consumer organization can\n\nuse cloud services and IT resources from different public clouds provided\n\nby multiple cloud providers, as shown in Figure 4.23.\n\nFigure 4.23\n\nAn organization uses the multi-cloud model to utilize cloud-based IT\n\nresources from different cloud providers.\n\nFor example, this deployment model can be used to improve redundancy\n\nand system backups, to improve mobility by reducing vendor lock-in, or to\n\nleverage best-of-breed cloud services from different cloud vendors.\n\nHybrid Clouds\n\nA hybrid cloud is a cloud environment comprised of two or more different\n\ncloud deployment models. For example, a cloud consumer may choose to\n\ndeploy cloud services processing sensitive data to a private cloud and other,\n\nless sensitive cloud services to a public cloud. The result of this\n\ncombination is a hybrid deployment model (Figure 4.24).\n\nFigure 4.24\n\nAn organization using a hybrid cloud architecture that utilizes both a\n\nprivate and public cloud.\n\nHybrid deployment architectures can be complex and challenging to create\n\nand maintain due to the potential disparity in cloud environments and the\n\nfact that management responsibilities are typically split between the private\n\ncloud provider organization and the public cloud provider.\n\nChapter 5\n\nCloud-Enabling Technology\n\n5.1 Networks and Internet Architecture\n\n5.2 Cloud Data Center Technology\n\n5.3 Modern Virtualization\n\n5.4 Multitenant Technology\n\n5.5 Service Technology and Service APIs\n\n5.6 Case Study Example\n\nModern-day clouds are underpinned by a set of primary technology\n\ncomponents that collectively enable key features and characteristics\n\nassociated with contemporary cloud computing.\n\nMost existed and matured prior to the advent of cloud computing, although\n\ncloud computing advancements helped further evolve select areas of these\n\ncloud-enabling technologies.\n\n5.1 Networks and Internet Architecture\n\nAll clouds must be connected to a network. This inevitable requirement\n\nforms an inherent dependency on internetworking.",
      "page_number": 114
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 138-164)",
      "start_page": 138,
      "end_page": 164,
      "detection_method": "synthetic",
      "content": "Internetworks, or the Internet, allow for the remote provisioning of IT\n\nresources and are directly supportive of ubiquitous network access. Cloud\n\nconsumers have the option of accessing the cloud using only private and\n\ndedicated network links in LANs, although most clouds are Internet-\n\nenabled. The potential of cloud platforms therefore generally grows in\n\nparallel with advancements in Internet connectivity and service quality.\n\nInternet Service Providers (ISPs)\n\nEstablished and deployed by ISPs, the Internet’s largest backbone networks\n\nare strategically interconnected by core routers that connect the world’s\n\nmultinational networks. As shown in Figure 5.1, an ISP network\n\ninterconnects to other ISP networks and various organizations.\n\nFigure 5.1\n\nMessages travel over dynamic network routes in this ISP internetworking\n\nconfiguration.\n\nThe concept of the Internet was based on a decentralized provisioning and\n\nmanagement model. ISPs can freely deploy, operate, and manage their\n\nnetworks in addition to selecting partner ISPs for interconnection. No\n\ncentralized entity comprehensively governs the Internet, although bodies\n\nlike the Internet Corporation for Assigned Names and Numbers (ICANN)\n\nsupervise and coordinate Internet communications.\n\nGovernmental and regulatory laws dictate the service provisioning\n\nconditions for organizations and ISPs both within and outside of national\n\nborders. Certain realms of the Internet still require the demarcation of\n\nnational jurisdiction and legal boundaries.\n\nThe Internet’s topology has become a dynamic and complex aggregate of\n\nISPs that are highly interconnected via its core protocols. Smaller branches\n\nextend from these major nodes of interconnection, branching outwards\n\nthrough smaller networks until eventually reaching every Internet-enabled\n\nelectronic device.\n\nWorldwide connectivity is enabled through a hierarchical topology\n\ncomposed of Tiers 1, 2, and 3 (Figure 5.2). The core Tier 1 is made of large-\n\nscale, international cloud providers that oversee massive interconnected\n\nglobal networks, which are connected to Tier 2’s large regional providers.\n\nThe interconnected ISPs of Tier 2 connect with Tier 1 providers, as well as\n\nthe local ISPs of Tier 3. Cloud consumers and cloud providers can connect\n\ndirectly using a Tier 1 provider, since any operational ISP can enable\n\nInternet connection.\n\nFigure 5.2\n\nAn abstraction of the internetworking structure of the Internet.\n\nThe communication links and routers of the Internet and ISP networks are\n\nIT resources that are distributed among countless traffic generation paths.\n\nTwo fundamental components used to construct the internetworking\n\narchitecture are connectionless packet switching (datagram networks) and\n\nrouter-based interconnectivity.\n\nConnectionless Packet Switching (Datagram Networks)\n\nEnd-to-end (sender-receiver pair) data flows are divided into packets of a\n\nlimited size that are received and processed through network switches and\n\nrouters, then queued and forwarded from one intermediary node to the next.\n\nEach packet carries the necessary location information, such as the Internet\n\nProtocol (IP) or Media Access Control (MAC) address, to be processed and\n\nrouted at every source, intermediary, and destination node.\n\nRouter-Based Interconnectivity\n\nA router is a device that is connected to multiple networks through which it\n\nforwards packets. Even when successive packets are part of the same data\n\nflow, routers process and forward each packet individually while\n\nmaintaining the network topology information that locates the next node on\n\nthe communication path between the source and destination nodes. Routers\n\nmanage network traffic and gauge the most efficient hop for packet\n\ndelivery, since they are privy to both the packet source and packet\n\ndestination.\n\nThe basic mechanics of internetworking are illustrated in Figure 5.3, in\n\nwhich a message is coalesced from an incoming group of disordered\n\npackets. The depicted router receives and forwards packets from multiple\n\ndata flows.\n\nFigure 5.3\n\nPackets traveling through the Internet are directed by a router that\n\narranges them into a message.\n\nThe communication path that connects a cloud consumer with its cloud\n\nprovider may involve multiple ISP networks. The Internet’s mesh structure\n\nconnects Internet hosts (endpoint systems) using multiple alternative\n\nnetwork routes that are determined at runtime. Communication can\n\ntherefore be sustained even during simultaneous network failures, although\n\nusing multiple network paths can cause routing fluctuations and latency.\n\nThis applies to ISPs that implement the Internet’s internetworking layer and\n\ninteract with other network technologies, as follows:\n\nPhysical Network\n\nIP packets are transmitted through underlying physical networks that\n\nconnect adjacent nodes, such as Ethernet, ATM network, and the 3G mobile\n\nHSDPA. Physical networks comprise a data link layer that controls data\n\ntransfer between neighboring nodes, and a physical layer that transmits data\n\nbits through both wired and wireless media.\n\nTransport Layer Protocol\n\nTransport layer protocols, such as the Transmission Control Protocol (TCP)\n\nand User Datagram Protocol (UDP), use the IP to provide standardized,\n\nend-to-end communication support that facilitates the navigation of data\n\npackets across the Internet.\n\nApplication Layer Protocol\n\nProtocols such as HTTP, SMTP for e-mail, BitTorrent for P2P, and SIP for\n\nIP telephony use transport layer protocols to standardize and enable specific\n\ndata packet transferring methods over the Internet. Many other protocols\n\nalso fulfill application-centric requirements and use either TCP/IP or UDP\n\nas their primary method of data transferring across the Internet and LANs.\n\nFigure 5.4 presents the Internet Reference Model and the protocol stack.\n\nFigure 5.4\n\nA generic view of the Internet reference model and protocol stack.\n\nTechnical and Business Considerations\n\nConnectivity Issues\n\nIn traditional, on-premise deployment models, enterprise applications and\n\nvarious IT solutions are commonly hosted on centralized servers and\n\nstorage devices residing in the organization’s own data center. End-user\n\ndevices, such as smartphones and laptops, access the data center through the\n\ncorporate network, which provides uninterrupted Internet connectivity.\n\nTCP/IP facilitates both Internet access and on-premise data exchange over\n\nLANs (Figure 5.5). Although not commonly referred to as a cloud model,\n\nthis configuration has been implemented numerous times for medium and\n\nlarge on-premise networks.\n\nFigure 5.5\n\nThe internetworking architecture of a private cloud. The physical IT\n\nresources that constitute the cloud are located and managed within the\n\norganization.\n\nOrganizations using this deployment model can directly access the network\n\ntraffic to and from the Internet and usually have complete control over and\n\ncan safeguard their corporate networks using firewalls and monitoring\n\nsoftware. These organizations also assume the responsibility of deploying,\n\noperating, and maintaining their IT resources and Internet connectivity.\n\nEnd-user devices that are connected to the network through the Internet can\n\nbe granted continuous access to centralized servers and applications in the\n\ncloud (Figure 5.6).\n\nFigure 5.6\n\nThe internetworking architecture of an Internet-based cloud deployment\n\nmodel. The Internet is the connecting agent between non-proximate cloud\n\nconsumers, roaming end-users, and the cloud provider’s own network.\n\nA salient cloud feature that applies to end-user functionality is how\n\ncentralized IT resources can be accessed using the same network protocols\n\nregardless of whether they reside inside or outside of a corporate network.\n\nWhether IT resources are on-premise or Internet-based dictates how internal\n\nversus external end-users access services, even if the end-users themselves\n\nare not concerned with the physical location of cloud-based IT resources\n\n(Table 5.1).\n\nTable 5-1\n\nA comparison of on-premise and cloud-based internetworking.\n\nCloud providers can easily configure cloud-based IT resources to be\n\naccessible for both external and internal users through an Internet\n\nconnection (as previously shown in Figure 5.6). This internetworking\n\narchitecture benefits internal users that require ubiquitous access to\n\ncorporate IT solutions, as well as cloud consumers that need to provide\n\nInternet-based services to external users. Major cloud providers offer\n\nInternet connectivity that is superior to the connectivity of individual\n\norganizations, resulting in additional network usage charges as part of their\n\npricing model.\n\nNetwork Bandwidth and Latency Issues\n\nIn addition to being affected by the bandwidth of the data link that connects\n\nnetworks to ISPs, end-to-end bandwidth is determined by the transmission\n\ncapacity of the shared data links that connect intermediary nodes. ISPs need\n\nto use broadband network technology to implement the core network\n\nrequired to guarantee end-to-end connectivity. This type of bandwidth is\n\nconstantly increasing, as Web acceleration technologies, such as dynamic\n\ncaching, compression, and pre-fetching, continue to improve end-user\n\nconnectivity.\n\nAlso referred to as time delay, latency is the amount of time it takes a\n\npacket to travel from one data node to another. Latency increases with every\n\nintermediary node on the data packet’s path. Transmission queues in the\n\nnetwork infrastructure can result in heavy load conditions that also increase\n\nnetwork latency. Networks are dependent on traffic conditions in shared\n\nnodes, making Internet latency highly variable and often unpredictable.\n\nPacket networks with “best effort” quality-of-service (QoS) typically\n\ntransmit packets on a first-come/first-serve basis. Data flows that use\n\ncongested network paths suffer service-level degradation in the form of\n\nbandwidth reduction, latency increase, or packet loss when traffic is not\n\nprioritized.\n\nThe nature of packet switching allows data packets to choose routes\n\ndynamically as they travel through the Internet’s network infrastructure.\n\nEnd-to-end QoS can be impacted as a result of this dynamic selecting, since\n\nthe travel speed of data packets is susceptible to conditions like network\n\ncongestion and is therefore non-uniform.\n\nIT solutions need to be assessed against business requirements that are\n\naffected by network bandwidth and latency, which are inherent to cloud\n\ninterconnection. Bandwidth is critical for applications that require\n\nsubstantial amounts of data to be transferred to and from the cloud, while\n\nlatency is critical for applications with a business requirement of swift\n\nresponse times.\n\nWireless and Cellular\n\nCloud-based solutions that need to be accessible anywhere from any device,\n\nespecially those that are targeted towards mobile clients and consumers,\n\nneed to be accessible via wireless and cellular communication links. For\n\nexample, mobile edge computing (MEC), an enabling technology for the\n\nInternet of Vehicles (IoV), offers prospective solutions for sharing\n\nprocessing capabilities across vehicles as well as other readily available\n\nresources.\n\nThe autonomous vehicular edge (AVE) is a distributed vehicular edge\n\ncomputing technology which enables sharing of nearby cars' available\n\nresources via vehicle-to-vehicle (V2V) communications. AVE is a principle\n\nthat can be applied to a broader online solution known as the hybrid\n\nvehicular edge cloud (HVC), which enables effective sharing of all\n\nobtainable computing resources, including roadside units (RSUs) and the\n\ncloud via multiaccess networks.\n\nThese are all examples of how wireless and cellular networks can be\n\nadapted or evolved to constitute valid internetworking components of\n\ncloud-based solutions by overcoming many of the natural bandwidth and\n\nlatency restrictions of wireless and cellular technologies.\n\nCloud Carrier and Cloud Provider Selection\n\nThe service levels of Internet connections between cloud consumers and\n\ncloud providers are determined by their ISPs, which are usually different\n\nand therefore include multiple ISP networks in their paths. QoS\n\nmanagement across multiple ISPs is difficult to achieve in practice,\n\nrequiring collaboration of the cloud carriers on both sides to ensure that\n\ntheir end-to-end service levels are sufficient for business requirements.\n\nCloud consumers and cloud providers may need to use multiple cloud\n\ncarriers in order to achieve the necessary level of connectivity and\n\nreliability for their cloud applications, resulting in additional costs. Cloud\n\nadoption can therefore be easier for applications with more relaxed latency\n\nand bandwidth requirements.\n\n5.2 Cloud Data Center Technology\n\nGrouping IT resources in close proximity with one another, rather than\n\nhaving them geographically dispersed, allows for power sharing, higher\n\nefficiency in shared IT resource usage, and improved accessibility for IT\n\npersonnel. These are the advantages that naturally popularized the data\n\ncenter concept. Modern data centers exist as specialized IT infrastructure\n\nused to house centralized IT resources, such as servers, databases,\n\nnetworking and telecommunication devices, and software systems. Data\n\ncenters for cloud providers often require additional technologies.\n\nData centers are typically comprised of the following technologies and\n\ncomponents:\n\nVirtualization\n\nData centers consist of both physical and virtualized IT resources. The\n\nphysical IT resource layer refers to the facility infrastructure that houses\n\ncomputing/networking systems and equipment, together with hardware\n\nsystems and their operating systems (Figure 5.7). The resource abstraction\n\nand control of the virtualization layer is comprised of operational and\n\nmanagement tools that are often based on virtualization platforms that\n\nabstract the physical computing and networking IT resources as virtualized\n\ncomponents that are easier to allocate, operate, release, monitor, and\n\ncontrol.\n\nFigure 5.7\n\nThe common components of a data center working together to provide\n\nvirtualized IT resources supported by physical IT resources.\n\nVirtualization components are discussed separately in the upcoming\n\nModern Virtualization section.\n\nStandardization and Modularity\n\nData centers are built upon standardized commodity hardware and designed\n\nwith modular architectures, aggregating multiple identical building blocks\n\nof facility infrastructure and equipment to support scalability, growth, and\n\nspeedy hardware replacements. Modularity and standardization are key\n\nrequirements for reducing investment and operational costs as they enable\n\neconomies of scale for the procurement, acquisition, deployment, operation,\n\nand maintenance processes.\n\nCommon virtualization strategies and the constantly improving capacity\n\nand performance of physical devices both favor IT resource consolidation,\n\nsince fewer physical components are needed to support complex\n\nconfigurations. Consolidated IT resources can serve different systems and\n\nbe shared among different cloud consumers.\n\nAutonomic Computing\n\nAutonomic computing is the ability of a system to be self-managing, which\n\nmeans that it is expected to react to external input without the need for\n\nhuman intervention. Using autonomic computing, clouds can be equipped\n\nto manage certain tasks themselves, without human involvement.\n\nThe common features of self-management can include:\n\nSelf-configuration, by which cloud services can configure themselves\n\nautomatically in response to established policies, avoiding manual\n\nintervention from cloud resource administrators. It also involves the\n\nautomatic configuration of new cloud resources when provisioned.\n\nSelf-optimization, by which cloud resources continuously strive to improve\n\ntheir performance indicators by modifying their configuration parameters at\n\nruntime, like scaling up or out dynamically.\n\nSelf-healing, by which cloud services can recover from hardware or\n\nsoftware failure, having detected and diagnosed issues automatically\n\nbeforehand.\n\nSelf-protecting, by which cloud computing platforms are able to defend\n\nthemselves from malicious attacks or cascading failure conditions. This is\n\npossible due to their ability to predict potential problem situations based on\n\nthe analysis of logs and diagnostics, in which data science technologies are\n\ntypically involved.\n\nRemote Operation and Management\n\nMost of the operational and administrative tasks of IT resources in data\n\ncenters are commanded through the network’s remote consoles and\n\nmanagement systems. Technical personnel are not required, and many\n\ntimes, not allowed, to visit the dedicated rooms that house servers, except to\n\nperform highly specific tasks, such as equipment handling and cabling or\n\nhardware-level installation and maintenance.\n\nHigh Availability\n\nSince any form of data center outage significantly impacts business\n\ncontinuity for the organizations that use their services, data centers are\n\ndesigned to operate with increasingly higher levels of redundancy to sustain\n\navailability. Data centers usually have redundant, uninterruptable power\n\nsupplies, cabling, and environmental control subsystems in anticipation of\n\nsystem failure, along with communication links and clustered hardware for\n\nload balancing.\n\nSecurity-Aware Design, Operation, and Management\n\nRequirements for security, such as physical and logical access controls and\n\ndata recovery strategies, need to be thorough and comprehensive for data\n\ncenters, since they are centralized structures that store and process business\n\ndata.\n\nDue to the sometimes-prohibitive nature of building and operating on-\n\npremise data centers, outsourcing data center-based IT resources has been a\n\ncommon industry practice for decades. However, the outsourcing models\n\noften required long-term consumer commitment and usually could not\n\nprovide elasticity, issues that a typical cloud can address via inherent\n\nfeatures, such as ubiquitous access, on-demand provisioning, rapid\n\nelasticity, and pay-per-use.\n\nFacilities\n\nData center facilities are custom-designed locations that are outfitted with\n\nspecialized computing, storage, and network equipment. These facilities\n\nhave several functional layout areas, as well as various power supplies,\n\ncabling, and environmental control stations that regulate heating,\n\nventilation, air conditioning, fire protection, and other related subsystems.\n\nThe site and layout of a given data center facility are typically demarcated\n\ninto segregated spaces. Appendix D provides a breakdown of the common\n\nrooms and utilities found in data centers.\n\nComputing Hardware\n\nMuch of the heavy processing in data centers is often executed by\n\nstandardized commodity servers that have substantial computing power and\n\nstorage capacity. Several computing hardware technologies are integrated\n\ninto these modular servers, such as:\n\nrackmount form factor server design composed of standardized racks with\n\ninterconnects for power, network, and internal cooling\n\nsupport for different hardware processing architectures, such as x86-32bits,\n\nx86-64, and RISC\n\na power-efficient multi-core CPU architecture that houses hundreds of\n\nprocessing cores in a space as small as a single unit of standardized racks\n\nredundant and hot-swappable components, such as hard disks, power\n\nsupplies, network interfaces, and storage controller cards\n\nComputing architectures such as blade server technologies use rack-\n\nembedded physical interconnections (blade enclosures), fabrics (switches),\n\nand shared power supply units and cooling fans. The interconnections\n\nenhance inter-component networking and management while optimizing\n\nphysical space and power. These systems typically support individual server\n\nhot-swapping, scaling, replacement, and maintenance, which benefits the\n\ndeployment of fault-tolerant systems that are based on computer clusters.\n\nContemporary computing hardware platforms generally support industry-\n\nstandard and proprietary operational and management software systems that\n\nconfigure, monitor, and control hardware IT resources from remote\n\nmanagement consoles. With a properly established management console, a\n\nsingle operator can oversee hundreds to thousands of physical servers,\n\nvirtual servers, and other IT resources.\n\nStorage Hardware\n\nData centers have specialized storage systems that maintain enormous\n\namounts of digital information in order to fulfill considerable storage\n\ncapacity needs. These storage systems are containers housing numerous\n\nhard disks that are organized into arrays.\n\nStorage systems usually involve the following technologies:\n\nHard Disk Arrays – These arrays inherently divide and replicate data among\n\nmultiple physical drives, and increase performance and redundancy by\n\nincluding spare disks. This technology is often implemented using\n\nredundant arrays of independent disks (RAID) schemes, which are typically\n\nrealized through hardware disk array controllers.\n\nI/O Caching – This is generally performed through hard disk array\n\ncontrollers, which enhance disk access times and performance by data\n\ncaching.\n\nHot-Swappable Hard Disks – These can be safely removed from arrays\n\nwithout requiring prior powering down.\n\nStorage Virtualization – This is realized through the use of virtualized hard\n\ndisks and storage sharing.\n\nFast Data Replication Mechanisms – These include snapshotting, which is\n\nsaving a virtual machine’s memory into a hypervisor-readable file for future\n\nreloading, and volume cloning, which is copying virtual or physical hard\n\ndisk volumes and partitions.\n\nStorage systems encompass tertiary redundancies, such as robotized tape\n\nlibraries, which are used as backup and recovery systems that typically rely\n\non removable media. This type of system can exist as a networked IT\n\nresource or direct-attached storage (DAS), in which a storage system is\n\ndirectly connected to the computing IT resource using a host bus adapter\n\n(HBA). In the former case, the storage system is connected to one or more\n\nIT resources through a network.\n\nNetworked storage devices usually fall into one of the following categories:\n\nStorage Area Network (SAN) – Physical data storage media are connected\n\nthrough a dedicated network and provide block-level data storage access\n\nusing industry standard protocols, such as the Small Computer System\n\nInterface (SCSI).\n\nNetwork-Attached Storage (NAS) – Hard drive arrays are contained and\n\nmanaged by this dedicated device, which connects through a network and\n\nfacilitates access to data using file-centric data access protocols like the\n\nNetwork File System (NFS) or Server Message Block (SMB).\n\nNAS, SAN, and other more advanced storage system options provide fault\n\ntolerance in many components through controller redundancy, cooling\n\nredundancy, and hard disk arrays that use RAID storage technology.\n\nNetwork Hardware\n\nData centers require extensive network hardware in order to enable multiple\n\nlevels of connectivity. For a simplified version of networking infrastructure,\n\nthe data center is broken down into five network subsystems, followed by a\n\nsummary of the most common elements used for their implementation.\n\nCarrier and External Networks Interconnection\n\nA subsystem related to the internetworking infrastructure, this\n\ninterconnection is usually comprised of backbone routers that provide\n\nrouting between external WAN connections and the data center’s LAN, as\n\nwell as perimeter network security devices such as firewalls and VPN\n\ngateways.\n\nWeb-Tier Load Balancing and Acceleration\n\nThis subsystem comprises Web acceleration devices, such as XML pre-\n\nprocessors, encryption/decryption appliances, and layer 7 switching devices\n\nthat perform content-aware routing.\n\nLAN Fabric\n\nThe LAN fabric constitutes the internal LAN and provides high-\n\nperformance and redundant connectivity for all of the data center’s\n\nnetwork-enabled IT resources. It is often implemented with multiple\n\nnetwork switches that facilitate network communications and operate at\n\nspeeds of up to ten gigabits per second. These advanced network switches\n\ncan also perform several virtualization functions, such as LAN segregation\n\ninto VLANs, link aggregation, controlled routing between networks, load\n\nbalancing, and failover.\n\nSAN Fabric\n\nRelated to the implementation of storage area networks (SANs) that provide\n\nconnectivity between servers and storage systems, the SAN fabric is usually\n\nimplemented with Fibre Channel (FC), Fibre Channel over Ethernet\n\n(FCoE), and InfiniBand network switches.\n\nNAS Gateways\n\nThis subsystem supplies attachment points for NAS-based storage devices\n\nand implements protocol conversion hardware that facilitates data\n\ntransmission between SAN and NAS devices.\n\nData center network technologies have operational requirements for\n\nscalability and high availability that are fulfilled by employing redundant\n\nand/or fault-tolerant configurations. These five network subsystems\n\nimprove data center redundancy and reliability to ensure that they have\n\nenough IT resources to maintain a certain level of service even in the face\n\nof multiple failures.\n\nUltra-high-speed network optical links can be used to aggregate individual\n\ngigabit-per-second channels into single optical fibers using multiplexing\n\ntechnologies like dense wavelength-division multiplexing (DWDM).\n\nSpread over multiple locations and used to interconnect server farms,\n\nstorage systems, and replicated data centers, optical links improve transfer\n\nspeeds and resiliency.\n\nServerless Environments\n\nA serverless environment consists of technologies that automatically\n\nprovide runtime resources for applications that can be deployed without the\n\nneed to set up the underlying resources required for them to run. The\n\ndeployed logic still runs on servers, whether physical, virtual, containerized,",
      "page_number": 138
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 165-183)",
      "start_page": 165,
      "end_page": 183,
      "detection_method": "synthetic",
      "content": "or otherwise, but service administrators do not need be concerned with\n\ncapacity planning, management, resiliency, or elasticity configurations,\n\nwhich are aspects taken care of by serverless environment.\n\nServerless technologies include automation, virtualization, infrastructure\n\nand software deployment and management, Infrastructure-as-Code, and\n\nContinuous Deployment, all encompassed in a highly customized cloud\n\nservice that is allows developers to simply upload their code and an\n\naccompanying description of its runtime requirements in a language\n\nspecific to each cloud provider. The serverless environment then takes over\n\nfrom there.\n\nA serverless environment is most commonly provided and operated by a\n\npublic cloud provider that relies either on container engines or virtual\n\nmachines to isolate the runtime of one application from another. The\n\nruntime details are hidden from the cloud consumer, and the cloud provider\n\nis responsible for managing the low-level infrastructure, including operating\n\nsystems, virtual machines, and containers.\n\nResources required by programs deployed using these serverless\n\ntechnologies are billed by cloud providers only for the time that the\n\nprograms run. When the program is not running, it does not generate any\n\ncost. This can be considered one of the most important advantages of\n\nserverless technologies, along with the ease of use provided to development\n\nteams by the automation of the entire deployment process all the way into\n\nproduction.\n\nNoSQL Clustering\n\nNoSQL (short for Not only SQL) refers to technologies used to develop\n\nnext-generation non-relational databases that are highly scalable and fault\n\ntolerant. These technologies achieve high levels of scalability and fault\n\ntolerance because they are designed as clusters of servers that act as a single\n\ndatabase or storage entity. This is known as NoSQL clustering.\n\nA cluster is a centrally managed group of nodes connected together via a\n\nnetwork to process tasks in parallel, where each node is responsible for a\n\nsub-task of a larger problem (Figure 5.8). Clusters enable distributed data\n\nprocessing. Ideally, a cluster comprises low-cost commodity nodes that\n\ncollectively provide increased processing capacity with inherent\n\nredundancy and fault tolerance, as it consists of physically separate nodes.\n\nFigure 5.8\n\nA cluster can be used as a deployment environment for various types of\n\ncloud-based solutions, including NoSQL databases.\n\nClusters are highly scalable, supporting horizontal scaling with linear\n\nperformance gains. They provide an ideal deployment environment for a\n\nprocessing engine as large datasets can be divided into smaller datasets and\n\nthen processed in parallel in a distributed manner.\n\nClusters are a fundamental resource provided by cloud computing\n\nplatforms. Clustering technology is used to provide big data platform-\n\nrelated services, advanced container management environments,\n\ndevelopment of applications that scale automatically, and deployment\n\nenvironments (such as PaaS), among others.\n\nNoSQL clustering provides storage devices that are scalable, available,\n\nfault-tolerant, and very fast for read/write operations. However, they do not\n\nprovide the same transaction and consistency support as exhibited by\n\nrelational database management systems.\n\nBelow are some of the principal features of NoSQL storage devices:\n\nSchemaless Data Model – Data can exist in its raw form.\n\nScale Out Rather Than Scale Up – More nodes are added as required rather\n\nthan replacing an existing one with a better, higher performance node.\n\nHighly Available – Built on cluster-based technologies providing fault\n\ntolerance out of box.\n\nLower Operational Costs – Built on open-source platforms with no\n\nlicensing costs and can be deployed on commodity hardware.\n\nEventual Consistency – Reads across multiple nodes may not be consistent\n\nimmediately after a write. However, all nodes will eventually be in a\n\nconsistent state.\n\nBASE, not ACID – BASE compliance requires a database to maintain high\n\navailability in the event of network or node failure, while not requiring the\n\ndatabase to be in a consistent state whenever an update occurs. The\n\ndatabase can be in a soft or inconsistent state until it eventually attains\n\nconsistency.\n\nAPI-Driven Data Access – Data access is generally supported via API-based\n\nqueries, including RESTful APIs, whereas some implementations may also\n\nprovide SQL-like query capability.\n\nAuto Sharding and Replication – To support horizontal scaling and provide\n\nhigh availability, a NoSQL storage device automatically employs sharding\n\nand replication techniques where the dataset is partitioned horizontally and\n\nthen copied over to multiple nodes.\n\nIntegrated Caching – Removes the need for a third-party distributed\n\ncaching layer, such as Memcached.\n\nDistributed Query Support – NoSQL storage devices maintain consistent\n\nquery behavior across multiple shards.\n\nPolyglot Persistence – The use of NoSQL device storage does not mandate\n\nretiring traditional RDBMSs. Both types of storage can be used at the same\n\ntime, thereby supporting polyglot persistence, which is an approach of\n\npersisting data using different types of storage technologies. This is good\n\nfor developing systems requiring structured as well as semi-structured or\n\nunstructured data.\n\nAggregate-Focused – Unlike relational databases that are most effective\n\nwith fully normalized data, NoSQL storage devices store de-normalized\n\naggregated data (an entity containing merged, often nested, data for an\n\nobject), thereby eliminating the need for joins and mapping between\n\napplication objects and the data stored in the database.\n\nOther Considerations\n\nIT hardware is subject to rapid technological obsolescence, with lifecycles\n\nthat typically last between five to seven years. The on-going need to replace\n\nequipment frequently results in a mix of hardware whose heterogeneity can\n\ncomplicate the entire data center’s operations and management, although\n\nthis can be partially mitigated through virtualization.\n\nSecurity is another major issue when considering the role of the data center\n\nand the vast quantities of data contained within its doors. Even with\n\nextensive security precautions in place, housing data exclusively at one data\n\ncenter facility means much more can be compromised by a successful\n\nsecurity incursion than if data was distributed across individual unlinked\n\ncomponents.\n\n5.3 Modern Virtualization\n\nModern virtualization technology is a foundation of contemporary cloud\n\nplatforms. It provides a variety of virtualization types and technology layers\n\nthat are introduced in this section.\n\nHardware Independence\n\nThe installation of an operating system’s configuration and application\n\nsoftware in a unique IT hardware platform results in many software-\n\nhardware dependencies. In a non-virtualized environment, the operating\n\nsystem is configured for specific hardware models and requires\n\nreconfiguration if these IT resources need to be modified.\n\nVirtualization is a conversion process that translates unique IT hardware\n\ninto emulated and standardized software-based copies. Through hardware\n\nindependence, virtual servers can easily be moved to another virtualization\n\nhost, automatically resolving multiple hardware-software incompatibility\n\nissues. As a result, cloning and manipulating virtual IT resources is much\n\neasier than duplicating physical hardware. The architectural models\n\nexplored in Part III of this book provide numerous examples of this.\n\nServer Consolidation\n\nThe coordination function that is provided by the virtualization software\n\nallows multiple virtual servers to be simultaneously created in the same\n\nvirtualization host. Virtualization technology enables different virtual\n\nservers to share one physical server. This process is called server\n\nconsolidation and is commonly used to increase hardware utilization, load\n\nbalancing, and optimization of available IT resources. The resulting\n\nflexibility is such that different virtual servers can run different guest\n\noperating systems on the same host.\n\nThis fundamental capability directly supports common cloud features, such\n\nas on-demand usage, resource pooling, elasticity, scalability, and resiliency.\n\nResource Replication\n\nVirtual servers are created as virtual disk images that contain binary file\n\ncopies of hard disk content. These virtual disk images are accessible to the\n\nhost’s operating system, meaning simple file operations, such as copy,\n\nmove, and paste, can be used to replicate, migrate, and back up the virtual\n\nserver. This ease of manipulation and replication is one of the most salient\n\nfeatures of virtualization technology as it enables:\n\nThe creation of standardized virtual machine images commonly configured\n\nto include virtual hardware capabilities, guest operating systems, and\n\nadditional application software, for pre-packaging in virtual disk images in\n\nsupport of instantaneous deployment.\n\nIncreased agility in the migration and deployment of a virtual machine’s\n\nnew instances by being able to rapidly scale out and up.\n\nThe ability to roll back, which is the instantaneous creation of VM\n\nsnapshots by saving the state of the virtual server’s memory and hard disk\n\nimage to a host-based file. (Operators can easily revert to these snapshots\n\nand restore the virtual machine to its prior state.)\n\nThe support of business continuity with efficient backup and restoration\n\nprocedures, as well as the creation of multiple instances of critical IT\n\nresources and applications.\n\nOperating System-Based Virtualization\n\nOperating system-based virtualization is the installation of virtualization\n\nsoftware in a pre-existing operating system, which is called the host\n\noperating system (Figure 5.9). For example, a user whose workstation is\n\ninstalled with a specific version of Windows wants to generate virtual\n\nservers and installs virtualization software into the host operating system\n\nlike any other program. This user needs to use this application to generate\n\nand operate one or more virtual servers. The user needs to use virtualization\n\nsoftware to enable direct access to any of the generated virtual servers.\n\nSince the host operating system can provide hardware devices with the\n\nnecessary support, operating system virtualization can rectify hardware\n\ncompatibility issues even if the hardware driver is not available to the\n\nvirtualization software.\n\nFigure 5.9\n\nThe different logical layers of operating system-based virtualization, in\n\nwhich the VM is first installed into a full host operating system and\n\nsubsequently used to generate virtual machines.\n\nHardware independence that is enabled by virtualization allows hardware\n\nIT resources to be more flexibly used. For example, consider a scenario in\n\nwhich the host operating system has the software necessary for controlling\n\nfive network adapters that are available to the physical computer. The\n\nvirtualization software can make the five network adapters available to the\n\nvirtual server, even if the virtualized operating system is incapable of\n\nphysically housing five network adapters.\n\nVirtualization software translates hardware IT resources that require unique\n\nsoftware for operation into virtualized IT resources that are compatible with\n\na range of operating systems. Since the host operating system is a complete\n\noperating system in itself, many operating system-based services that are\n\navailable as administration tools can be used to manage the physical host.\n\nExamples of such services include:\n\nBackup and Recovery\n\nIntegration to Directory Services\n\nSecurity Management\n\nOperating system-based virtualization can introduce demands and issues\n\nrelated to performance overhead such as:\n\nThe host operating system consumes CPU, memory, and other hardware IT\n\nresources.\n\nHardware-related calls from guest operating systems need to traverse\n\nseveral layers to and from the hardware, which decreases overall\n\nperformance.\n\nLicenses are usually required for host operating systems, in addition to\n\nindividual licenses for each of their guest operating systems.\n\nA concern with operating system-based virtualization is the processing\n\noverhead required to run the virtualization software and host operating\n\nsystems. Implementing a virtualization layer will negatively affect overall\n\nsystem performance. Estimating, monitoring, and managing the resulting\n\nimpact can be challenging because it requires expertise in system\n\nworkloads, software and hardware environments, and sophisticated\n\nmonitoring tools.\n\nHardware-Based Virtualization\n\nThis option represents the installation of virtualization software directly on\n\nthe physical host hardware so as to bypass the host operating system, which\n\nis presumably engaged with operating system-based virtualization (Figure\n\n5.10). Allowing the virtual servers to interact with hardware without\n\nrequiring intermediary action from the host operating system generally\n\nmakes hardware-based virtualization more efficient.\n\nFigure 5.10\n\nThe different logical layers of hardware-based virtualization, which does\n\nnot require another host operating system.\n\nVirtualization software is typically referred to as a hypervisor for this type\n\nof processing. A hypervisor has a simple user-interface that requires a\n\nnegligible amount of storage space. It exists as a thin layer of software that\n\nhandles hardware management functions to establish a virtualization\n\nmanagement layer. Device drivers and system services are optimized for the\n\nprovisioning of virtual servers, although many standard operating system\n\nfunctions are not implemented. This type of virtualization system is\n\nessentially used to optimize performance overhead inherent to the\n\ncoordination that enables multiple virtual servers to interact with the same\n\nhardware platform.\n\nOne of the main issues of hardware-based virtualization concerns\n\ncompatibility with hardware devices. The virtualization layer is designed to\n\ncommunicate directly with the host hardware, meaning all of the associated\n\ndevice drivers and support software need to be compatible with the\n\nhypervisor. Hardware device drivers may not be as available to hypervisor\n\nplatforms as they are to operating systems. Host management and\n\nadministration features may further not include the range of advanced\n\nfunctions that are common to operating systems.\n\nContainers and Application-Based Virtualization\n\nApplication virtualization is a method of creating and using applications\n\nwithout operating system dependency. For many types of applications and\n\nservices, containers provide a portable, compatible, and highly manageable\n\ndeployment environment that allows independent and autonomous software\n\nprograms and systems to run on almost any platform, in accordance with\n\nthe definition of application virtualization.\n\nSoftware running in containers can be deployed almost anywhere, always\n\nproviding the same functionality independently of the runtime environment\n\nin which it is deployed. Containers are suitable for application-based\n\nvirtualization because applications running in containers can run on any\n\nplatform, regardless of the underlying operating system or hardware\n\narchitecture, as long as a compatible containerization engine is running on\n\nthat platform, as depicted in Figure 5.11.\n\nFigure 5.11\n\nA virtualized application running in a container can be deployed anywhere\n\nits corresponding containerization engine is installed, regardless of\n\nunderlying hardware or operating system architectures.\n\nContainerization has become a fundamental infrastructure technology in\n\ncontemporary cloud environments and is covered in detail in Chapter 6.\n\nVirtualization Management\n\nMany administrative tasks can be performed more easily using virtual\n\nservers as opposed to using their physical counterparts. Modern\n\nvirtualization software provides several advanced management functions\n\nthat can automate administration tasks and reduce the overall operational\n\nburden on virtualized IT resources.\n\nVirtualized IT resource management is often supported by virtualization\n\ninfrastructure management (VIM) tools that collectively manage virtual IT\n\nresources and rely on a centralized management module, otherwise known\n\nas a controller, that runs on a dedicated computer. VIMs are commonly\n\nencompassed by the resource management system mechanism described in\n\nChapter 12.\n\nOther Considerations\n\nPerformance Overhead – Virtualization may not be ideal for complex\n\nsystems that have high workloads with little use for resource sharing and\n\nreplication. A poorly formulated virtualization plan can result in excessive\n\nperformance overhead. A common strategy used to rectify the overhead\n\nissue is a technique called para-virtualization, which presents a software\n\ninterface to the virtual machines that is not identical to that of the\n\nunderlying hardware. The software interface has instead been modified to\n\nreduce the guest operating system’s processing overhead, which is more\n\ndifficult to manage. A major drawback of this approach is the need to adapt\n\nthe guest operating system to the para-virtualization API, which can impair\n\nthe use of standard guest operating systems while decreasing solution\n\nportability.\n\nSpecial Hardware Compatibility – Many hardware vendors that distribute\n\nspecialized hardware may not have device driver versions that are\n\ncompatible with virtualization software. Conversely, the software itself may\n\nbe incompatible with recently released hardware versions. These types of\n\nincompatibility issues can be resolved using established commodity\n\nhardware platforms and mature virtualization software products. Container\n\nengines are not affected by this consideration because they run on top of the\n\nhost operating system, which abstracts any potential hardware\n\ncompatibility, making containerization a highly portable type of\n\nvirtualization technology.\n\nPortability – The programmatic and management interfaces that establish\n\nadministration environments for a virtualization program to operate with\n\nvarious virtualization solutions can introduce portability gaps due to\n\nincompatibilities. Initiatives such as the Open Virtualization Format (OVF)\n\nfor the standardization of virtual disk image formats are dedicated to\n\nalleviating this concern. Furthermore, containerization provides an\n\nalternative type of virtualization technology with very high levels of\n\nportability.\n\n5.4 Multitenant Technology\n\nThe multitenant application design was created to enable multiple users\n\n(tenants) to access the same application logic simultaneously. Each tenant\n\nhas its own view of the application that it uses, administers, and customizes\n\nas a dedicated instance of the software while remaining unaware of other\n\ntenants that are using the same application.\n\nMultitenant applications ensure that tenants do not have access to data and\n\nconfiguration information that is not their own. Tenants can individually\n\ncustomize features of the application, such as:\n\nUser Interface – Tenants can define a specialized “look and feel” for their\n\napplication interface.\n\nBusiness Process – Tenants can customize the rules, logic, and workflows\n\nof the business processes that are implemented in the application.\n\nData Model – Tenants can extend the data schema of the application to\n\ninclude, exclude, or rename fields in the application data structures.\n\nAccess Control – Tenants can independently control the access rights for\n\nusers and groups.\n\nMultitenant application architecture is often significantly more complex\n\nthan that of single-tenant applications. Multitenant applications need to\n\nsupport the sharing of various artifacts by multiple users (including portals,\n\ndata schemas, middleware, and databases), while maintaining security\n\nlevels that segregate individual tenant operational environments.\n\nCommon characteristics of multitenant applications include:",
      "page_number": 165
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 184-201)",
      "start_page": 184,
      "end_page": 201,
      "detection_method": "synthetic",
      "content": "Usage Isolation – The usage behavior of one tenant does not affect the\n\napplication availability and performance of other tenants.\n\nData Security – Tenants cannot access data that belongs to other tenants.\n\nRecovery – Backup and restore procedures are separately executed for the\n\ndata of each tenant.\n\nApplication Upgrades – Tenants are not negatively affected by the\n\nsynchronous upgrading of shared software artifacts.\n\nScalability – The application can scale to accommodate increases in usage\n\nby existing tenants and/or increases in the number of tenants.\n\nMetered Usage – Tenants are charged only for the application processing\n\nand features that are actually consumed.\n\nData Tier Isolation – Tenants can have individual databases, tables, and/or\n\nschemas isolated from other tenants. Alternatively, databases, tables, and/or\n\nschemas can be designed to be intentionally shared by tenants.\n\nA multitenant application that is being concurrently used by two different\n\ntenants is illustrated in Figure 5.12. This type of application is typical with\n\nSaaS implementations.\n\nFigure 5.12\n\nA multitenant application that is serving multiple cloud service consumers\n\nsimultaneously.\n\nMultitenancy vs. Virtualization\n\nMultitenancy is sometimes mistaken for virtualization\n\nbecause the concept of multiple tenants is similar to the\n\nconcept of virtualized instances.\n\nThe differences lie in what is multiplied within a physical\n\nserver acting as a host:\n\nWith virtualization: Multiple virtual copies of the server\n\nenvironment can be hosted by a single physical server. Each\n\ncopy can be provided to different users, can be configured\n\nindependently, and can contain its own operating systems\n\nand applications.\n\nWith multitenancy: A physical or virtual server hosting an\n\napplication is designed to allow usage by multiple different\n\nusers. Each user feels as though they have exclusive usage\n\nof the application.\n\n5.5 Service Technology and Service APIs\n\nThe field of service technology is a keystone foundation of cloud\n\ncomputing that formed the basis of the “as-a-service” cloud delivery\n\nmodels. Several prominent service technologies that are used to realize and\n\nbuild upon cloud-based environments are described in this section.\n\nAbout Web-based Services\n\nReliant on the use of standardized protocols, Web-based\n\nservices are self-contained units of logic that support\n\ninteroperable machine-to-machine interaction over a\n\nnetwork. These services are generally designed to\n\ncommunicate via non-proprietary technologies in\n\naccordance with industry standards and conventions.\n\nBecause their sole function is to process data between\n\ncomputers, these services expose APIs and do not have user\n\ninterfaces. Web services and REST services represent two\n\ncommon forms of Web-based services.\n\nREST Services\n\nREST services are designed according to a set of constraints that shape the\n\nservice architecture to emulate the properties of the World Wide Web,\n\nresulting in service implementations that rely on the use of core Web\n\ntechnologies.\n\nREST services do not have individual technical interfaces but instead share\n\na common technical interface that is known as the uniform contract, which\n\nis typically established via the use of HTTP methods.\n\nThe six REST design constraints are:\n\nClient-Server\n\nStateless\n\nCache\n\nInterface/Uniform Contract\n\nLayered System\n\nCode-On-Demand\n\nNote\n\nTo learn more about REST services read SOA with REST:\n\nPrinciples, Patterns & Constraints for Building Enterprise\n\nSolutions with REST from the Pearson Digital Enterprise\n\nSeries from Thomas Erl.\n\nWeb Services\n\nAlso commonly prefixed with “SOAP-based,” Web services represent an\n\nestablished and common medium for sophisticated, Web-based service\n\nlogic. Along with XML, the core technologies behind Web services are\n\nrepresented by the following industry standards:\n\nWeb Service Description Language (WSDL) – This markup language is\n\nused to create a WSDL definition that defines the application programming\n\ninterface (API) of a Web service, including its individual operations\n\n(functions) and each operation’s input and output messages.\n\nXML Schema Definition Language (XML Schema) – Messages exchanged\n\nby Web services must be expressed using XML. XML schemas are created\n\nto define the data structure of the XML-based input and output messages\n\nexchanged by Web services. XML schemas can be directly linked to or\n\nembedded within WSDL definitions.\n\nSOAP – Formerly known as the Simple Object Access Protocol, this\n\nstandard defines a common messaging format used for request and response\n\nmessages exchanged by Web services. SOAP messages are comprised of\n\nbody and header sections. The former houses the main message content and\n\nthe latter is used to contain metadata that can be processed at runtime.\n\nUniversal Description, Discovery, and Integration (UDDI) – This standard\n\nregulates service registries in which WSDL definitions can be published as\n\npart of a service catalog for discovery purposes.\n\nThese four technologies collectively form the first generation of Web\n\nservice technologies (Figure 5.13). A comprehensive set of second-\n\ngeneration Web service technologies (commonly referred to as WS-*) has\n\nbeen developed to address various additional functional areas, such as\n\nsecurity, reliability, transactions, routing, and business process automation.\n\nNote\n\nTo learn more about Web service technologies, read Web\n\nService Contract Design & Versioning for SOA from the\n\nPearson Digital Enterprise Series from Thomas Erl. This\n\ntitle covers first and second-generation Web service\n\nstandards in technical detail.\n\nFigure 5.13\n\nAn overview of how first-generation Web service technologies commonly\n\nrelate to each other.\n\nService Agents\n\nService agents are event-driven programs designed to intercept messages at\n\nruntime. There are active and passive service agents, both of which are\n\ncommon in cloud environments. Active service agents perform an action\n\nupon intercepting and reading the contents of a message. The action\n\ntypically requires making changes to the message contents (most commonly\n\nmessage header data and less commonly the body content) or changes to the\n\nmessage path itself. Passive service agents, on the other hand, do not\n\nchange message contents. Instead, they read the message and may then\n\ncapture certain parts of its contents, usually for monitoring, logging, or\n\nreporting purposes.\n\nCloud-based environments rely heavily on the use of system-level and\n\ncustom service agents to perform much of the runtime monitoring and\n\nmeasuring required to ensure that features, such as elastic scaling and pay-\n\nfor-use billing, can be carried out instantaneously.\n\nSeveral of the mechanisms described in Part II of this book exist as, or rely\n\non the use of, service agents.\n\nService Middleware\n\nFalling under the umbrella of service technology is the large market of\n\nmiddleware platforms that evolved from messaging-oriented middleware\n\n(MOM) platforms used primarily to facilitate integration, to sophisticated\n\nservice middleware platforms designed to accommodate complex service\n\ncompositions.\n\nThe two most common types of middleware platforms relevant to services\n\ncomputing are the enterprise service bus (ESB) and the orchestration\n\nplatform. The ESB encompasses a range of intermediary processing\n\nfeatures, including service brokerage, routing, and message queuing.\n\nOrchestration environments are designed to host and execute workflow\n\nlogic that drives the runtime composition of services.\n\nBoth forms of service middleware can be deployed and operated within\n\ncloud-based environments.\n\nWeb-based RPC\n\nCloud providers commonly deliver access to the resources that they offer\n\nvia RESTful services. The interaction between RESTful services and their\n\nservice consumers requires considerable amounts of bandwidth in\n\nconversations that require multiple messages to be exchanged through the\n\nnetwork.\n\nTraditional RPC frameworks can overcome some of the performance\n\nchallenges presented by RESTful architectures, but they are bound to\n\ncommunication via TCP/IP, which is a limitation incompatible with Web-\n\nbased application requirements. To address both sets of limitations, a set of\n\ncontemporary protocols was developed that leverage the performance\n\nbenefits of RPC, while supporting Web-based communication. These\n\ninclude:\n\ngRPC (originally developed by Google)\n\nGraphQL (originally developed by Facebook)\n\nFalcor (originally developed by Netflix)\n\nEach of these protocols was developed by an organization in response to a\n\nneed to overcome limitations with more established protocols.\n\n5.6 Case Study Example\n\nDTGOV has assembled cloud-aware infrastructures in\n\neach of its data centers, which are comprised of the\n\nfollowing components:\n\nTier-3 facility infrastructure, which provides redundant\n\nconfigurations for all of the central subsystems in the\n\ndata center facility layer.\n\nRedundant connections with utility service providers that\n\nhave installed local capacity for power generation and\n\nwater supply that activates in the event of general\n\nfailure.\n\nAn internetwork that supplies an ultra-high bandwidth\n\ninterconnection between the three data centers through\n\ndedicated links.\n\nRedundant Internet connections in each data center to\n\nmultiple ISPs and the .GOV extranet, which\n\ninterconnects DTGOV with its main government clients.\n\nStandardized hardware of higher aggregated capacity\n\nthat is abstracted by a cloud-aware virtualization\n\nplatform.\n\nPhysical servers are organized on server racks, each of\n\nwhich has two redundant top-of-rack router switches\n\n(layer 3) that are connected to each physical server.\n\nThese router switches are interconnected to LAN core-\n\nswitches that have been configured as a cluster. The\n\ncore-switches connect to routers that supply\n\ninternetworking capabilities and firewalls that provide\n\nnetwork access control capabilities. Figure 5.14\n\nillustrates the physical layout of the server network\n\nconnections inside of the data center.\n\nFigure 5.14\n\nA view of the server network connections inside the\n\nDTGOV data center.\n\nA separate network that connects the storage systems\n\nand servers is installed with clustered storage area\n\nnetwork (SAN) switches and similar redundant\n\nconnections to various devices (Figure 5.15).\n\nFigure 5.15\n\nA view of the storage system network connections inside\n\nthe DTGOV data center.\n\nFigure 5.16 illustrates an internetworking architecture\n\nthat is established between every data center pair within\n\nthe DTGOV corporate infrastructure.\n\nAs shown in Figures 5.15 and 5.16, combining\n\ninterconnected physical IT resources with virtualized IT\n\nresources on the physical layer enables the dynamic and\n\nwell-managed configuration and allocation of virtual IT\n\nresources.\n\nFigure 5.16\n\nThe internetworking setup between two data centers that is similarly\n\nimplemented between every pair of DTGOV data centers. The DTGOV\n\ninternetwork is designed to be an autonomous system (AS) on the Internet,\n\nmeaning the links interconnecting the data centers with the LANs define the\n\nintra-AS routing domain. The interconnections to external ISPs are\n\ncontrolled through inter-AS routing technology, which shapes Internet\n\ntraffic and enables flexible configurations for load-balancing and failover.\n\nChapter 6\n\nUnderstanding Containerization\n\n6.1 Origins and Influences\n\n6.2 Fundamental Virtualization and Containerization\n\n6.3 Understanding Containers\n\n6.4 Understanding Container Images\n\n6.5 Multi-Container Types\n\n6.6 Case Study Example\n\nContainerization is a virtualization technology used to deploy and run\n\napplications and services without the need to deploy a virtual server for\n\neach solution. This chapter covers fundamental topics pertaining to\n\nvirtualization and then takes a close look at containerization technology and\n\nthe utilization of containers.\n\nNote\n\nThis chapter is supplemented with coverage of the Docker\n\nand Kubernetes containerization technologies provided in\n\nAppendix B.\n\n6.1 Origins and Influences\n\nA Brief History\n\nThe concept of containers has been present since the 1970s, when it\n\noriginally referred to a capability used in Unix systems to better segregate\n\napplication code. Early containers provided an isolated environment in\n\nwhich services and applications could operate without interfering with other\n\nprocesses, resulting in an environment similar to a sandbox for testing apps,\n\nservices and other processes.\n\nContainers gained widespread usage decades later due to a slew of Linux\n\ndistributions that released new deployment and management tools.\n\nContainers running on Linux systems were turned into an operating system-\n\nlevel virtualization technique specially designed to enable several isolated\n\nLinux environments to run on a single Linux host. However, while running\n\ncontainers on a Linux platform broadened their usefulness, there remained\n\nkey obstacles to solve, including unified administration, true portability,\n\ncompatibility and control of scale.\n\nThe introduction of Apache Mesos, Google Borg and Facebook\n\nTupperware, all of which provided varied degrees of container orchestration\n\nand cluster management capabilities, marked a significant advancement in\n\nthe use of containers on Linux systems. These systems enabled the instant\n\ncreation of hundreds of containers, as well as automatic failover and other\n\nmission-critical functionality necessary for container management at scale.\n\nAfter Docker containers were introduced, containerization began becoming\n\npart of the IT mainstream. Docker's prominence led to the innovation of\n\nsophisticated containerization platforms, including Marathon, Kubernetes\n\nand Docker Swarm.\n\nContainerization and Cloud Computing\n\nCloud computing helped popularize virtualization technology, and further\n\nadvances in cloud computing technology helped realize contemporary\n\ncontainerization technology. Containerization is now a fundamental part of\n\ncloud computing infrastructure.\n\nThe use of containers can help support the primary business drivers behind\n\ncloud computing.\n\nThe simplified and flexible deployment architecture established by\n\ncontainerization can directly support the primary Cost Reduction and\n\nBusiness Agility business drivers behind cloud computing (as introduced in\n\nChapter 3) and can further enable cloud-based solutions to be made better\n\nrespond to fluctuating usage requirements.",
      "page_number": 184
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 202-224)",
      "start_page": 202,
      "end_page": 224,
      "detection_method": "synthetic",
      "content": "6.2 Fundamental Virtualization and Containerization\n\nThis section covers the fundamental terms and concepts associated with\n\noperating systems and virtualization technology. It then proceeds to explain\n\nthe basic components of containerization and concludes with a comparison\n\nof virtualization and containerization.\n\nOperating System Basics\n\nAn operating system is software installed on a computer that provides a\n\nrange of programs, tools, libraries and other resources used to manage a\n\ncomputer, as well as programs used to host and support the on-going\n\noperations of applications installed on the operating system. The installation\n\nof an operating system can also include various consumer applications.\n\nThe operating system programs used to support the execution and active\n\noperation of applications are collectively referred to as the runtime (Figure\n\n6.1). Applications themselves may introduce their own runtime software\n\nthat operates on top of an operating system runtime environment.\n\nFigure 6.1\n\nThe symbol used to represent a runtime.\n\nVirtualization Basics\n\nTo best understand containerization, it is important to first establish some\n\nbasics about virtualization. As has already been explained in Chapter 3,\n\nvirtualization is the technology that enables a physical IT resource to\n\nprovide multiple virtual images of itself so that its underlying processing\n\ncapabilities can be shared by multiple solutions.\n\nPhysical Servers\n\nThe physical IT resource most commonly virtualized is the physical server\n\n(Figure 6.2). A physical server provides an operating system environment\n\nthat can host applications, services and other software programs.\n\nFigure 6.2\n\nThe symbol used to represent a physical server.\n\nVirtual Servers\n\nWhen utilizing virtualization technology, the operating system hosting\n\nenvironment provided by the physical server can be abstracted into one or\n\nmore virtual servers (Figure 6.3).\n\nFigure 6.3\n\nThe symbol used to represent a virtual server.\n\nEach virtual server can then provide a fresh and dedicated copy (or image)\n\nof the operating system hosting environment, which can be further referred\n\nto as a guest operating system. Each virtual server can provide its\n\nvirtualized operating system environment to a different set of consumer\n\napplications or services that do not require any knowledge of how the\n\nunderlying physical server exists or operates (Figure 6.4). As consumer\n\nusage demands fluctuate, the physical server can be scaled accordingly.\n\nFigure 6.4\n\nThree virtual servers that exist on two physical servers.\n\nThe administrator responsible for the physical server can retain\n\nadministrative control of the physical server hardware and its operating\n\nsystem. The administrators responsible for the individual virtual servers are\n\nnot given (nor require) access to the underlying physical server, but they\n\ncan independently control their respective virtual operating system\n\nenvironments.\n\nHypervisors\n\nThe component responsible for creating and running multiple virtual servers\n\nfrom a physical server is the hypervisor (Figures 6.5 and 6.6).\n\nFigure 6.5\n\nThe symbol used to represent the hypervisor.\n\nFigure 6.6\n\nThree virtual servers created and run by a hypervisor that exist on two\n\nphysical servers.\n\nVirtual servers perceive the emulated hardware presented to them by the\n\nhypervisor as real hardware. Each virtual server has its own operating\n\nsystem (also known as a guest operating system) that needs to be deployed\n\ninside the virtual server and managed and maintained as if it were deployed\n\non a physical server.\n\nVirtualization Types\n\nThere are two types of virtualization environments that are primarily\n\ndistinguished by whether the physical server has an operating system\n\ninstalled.\n\nIn a Type 1 virtualization environment, the physical server does not have an\n\noperating system installed. Instead, only the hypervisor is installed on the\n\nphysical server and it is responsible for creating the virtual servers and\n\nproviding them with virtualized operating system environments (Figure\n\n6.7).\n\nFigure 6.7\n\nThe physical server hosts only the hypervisor that creates virtual servers,\n\neach with its own operating system.\n\nIn a Type 2 virtualization environment, the physical server has an operating\n\nsystem installed and may also have a hypervisor installed. In this case, the\n\nphysical server is accessible via its operating system and the hypervisor\n\nremains responsible for creating the virtual servers and providing them with\n\ntheir virtualized operating system environments (Figure 6.8).\n\nFigure 6.8\n\nThe physical server hosts its own operating system as well as a hypervisor\n\nthat creates virtual servers with their own operating system environments.\n\nContainerization Basics\n\nContainers\n\nA container (Figure 6.9) is a virtualized hosting environment that can be\n\noptimized to provide only the resources required for the software programs\n\nit hosts.\n\nFigure 6.9\n\nThe symbol on the left is the container icon. The symbol on the right is also\n\nused to represent a container and to show its contents.\n\nContainers have various features and characteristics that are explored in\n\nmore detail in the upcoming Understanding Containers section.\n\nContainer Images\n\nA container image (Figure 6.10) is similar to a pre-defined template that is\n\nused to create deployed containers.\n\nFigure 6.10\n\nThe symbol used to represent a container image.\n\nThe definition and usage of container images is integral to how\n\ncontainerization platforms operate. Further details are provided in the\n\nupcoming Understanding Container Images section.\n\nContainer Engines\n\nThe container engine (Figure 6.11), also referred to as the containerization\n\nengine, is responsible for creating containers based on pre-defined container\n\nimages. The container engine is deployed in a physical or virtual server’s\n\noperating system from where it can abstract the resources required for a\n\ngiven container.\n\nFigure 6.11\n\nThe symbol used to represent the container engine.\n\nThe container engine is a core part of a containerization platform and is\n\nresponsible for many of its primary processing tasks. Its implementation is\n\norganized into two “planes”, as follows:\n\nManagement Plane — the GUI and command-line tools made available to\n\nenable human administrators to configure and maintain the container engine\n\nenvironment\n\nControl Plane — all remaining container engine functions and features that\n\nthe container engine carries out automatically and in response to settings\n\nand commands issued via the management plane\n\nA given container engine can create multiple containers (Figure 6.12).\n\nFigure 6.12\n\nA container engine creating two different containers.\n\nPods\n\nA pod, also known as a logical pod container, is a special type of system\n\ncontainer that can be used to host a single container or a group of containers\n\n(Figure 6.13) that have shared storage and/or network resources, and also\n\nshare the same configuration that determines how the containers are to be\n\nrun.\n\nFigure 6.13\n\nA pod is depicted as a perforated outline showing the containers it is\n\nhosting.\n\nHow pods relate to container deployment is further explored in the\n\nContainers and Pods sub-section of the upcoming Understanding\n\nContainers section.\n\nHosts\n\nA host is the environment in which a container is deployed. A host can be\n\nreferred to as a server or a node. The host provides the operating system\n\nfrom which the container abstracts the resources it needs to support the\n\nprograms it is hosting. Multiple containers can be deployed and run on a\n\nsingle host (Figure 6.14).\n\nFigure 6.14\n\nThree containers in a single pod reside on Host A, which is a physical\n\nserver.\n\nDifferent combinations of containers and pods can be deployed on different\n\nhosts (Figure 6.15). However, a single pod cannot span more than one host.\n\nFigure 6.15\n\nHost A has three active containers in a pod, whereas there are six\n\ncontainers in a pod operating on Host B.\n\nContainers also operate on a host without a pod when the container engine\n\ndeployed does not support pods (Figure 6.16).\n\nFigure 6.16\n\nThree containers are deployed on Host A without the involvement of a pod.\n\nHosts commonly exist as physical servers, but a host can also be a virtual\n\nserver. When a container is deployed on a virtual server, it is considered a\n\nform of nested virtualization because one virtualized system is deployed on\n\nanother.\n\nHost Clusters\n\nHost servers can be combined into “clusters” that can collectively establish\n\na pool of readily available processing resources with increased computing\n\ncapacity. Both virtual and physical hosts can be clustered (Figures 6.17 and\n\n6.18). Within clustering environments, host servers are commonly referred\n\nto as nodes.\n\nFigure 6.17\n\nThe symbol used to represent a physical host cluster.",
      "page_number": 202
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 225-248)",
      "start_page": 225,
      "end_page": 248,
      "detection_method": "synthetic",
      "content": "Figure 6.18\n\nThe symbol used to represent a virtual host cluster.\n\nCommon types of host clusters include:\n\nLoad Balanced Cluster — This type of host cluster specializes in\n\ndistributing workloads among hosts to increase resource capacity while\n\npreserving the centralization of resource management. It usually\n\nimplements a load balancer that is embedded within a cluster management\n\nplatform or set up as a separate resource.\n\nHigh Availability (HA) Cluster — This type of cluster maintains system\n\navailability in the event of multiple host failures. It typically provides\n\nredundant implementations of most or all of the clustered resources and\n\nimplements a failover system that monitors failure conditions and\n\nautomatically redirects workloads away from failed host environments.\n\nScaling Cluster — This type of cluster is used to support both vertical and\n\nhorizontal scaling.\n\nContainerization platforms utilize all of the aforementioned types of host\n\ncluster models in support of high-performance and resiliency requirements,\n\nas well as in relation to optimized deployment capabilities.\n\nHost Networks and Overlay Networks\n\nEach host has its own container engine that is responsible for generating\n\ncontainer images and deploying and running containers on that host.\n\nRelated containers within a host can communicate with each other using a\n\nlocal host network. Related containers and container engines on different\n\nhosts can communicate with each other via an overlay network. Both of\n\nthese types of networks are considered container networks (Figure 6.19).\n\nFigure 6.19\n\nThe symbol used to represent a container network.\n\nContainer networks can be configured by administrators to support various\n\nscalability and resiliency capabilities and to control which hosted programs\n\ncan access resources outside of the container network, as further explored in\n\nthe Container Networks sub-section in the upcoming Understanding\n\nContainers section.\n\nVirtualization and Containerization\n\nThe primary distinction between a virtual server and a container is that a\n\nvirtual server provides a virtual version of a physical server’s entire\n\noperating system, whereas a container only provides the subset of the\n\noperating system resources actually required by the software program (or\n\nprograms) it is hosting. As a result, a container consumes less space and\n\nperforms more efficiently than a virtual server.\n\nContainerization on Physical Servers\n\nWhen deploying containers on a physical server, the containerization\n\nplatform requires no virtualization environment since virtual servers are not\n\nrequired. The underlying physical server has an operating system installed\n\nand the containerization platform can create containers that each only\n\nabstract the subset of the operating system relevant to the software\n\nprograms it hosts (Figure 6.20).\n\nFigure 6.20\n\nA physical server with an operating system hosts a containerization\n\nplatform that creates containers, each with an environment that has only a\n\nsubset of the underlying operating system.\n\nContainerization on Virtual Servers\n\nWhen deploying containers on one or more virtual servers, the\n\ncontainerization platform can be implemented on a Type 1 virtualization\n\nenvironment (Figure 6.21) or a Type 2 virtualization environment with a\n\nhypervisor (Figure 6.22). Both types of virtualization environments allow\n\nfor the creation of virtual servers that can host containerization engines.\n\nFigure 6.21\n\nA physical server with no operating system hosts a hypervisor that creates\n\nvirtual servers with operating systems, each of which hosts a\n\ncontainerization platform that can create containers that only have an\n\noperating system subset.\n\nFigure 6.22\n\nA physical server with an operating system hosts a hypervisor that creates\n\nvirtual server environments with their own operating systems. Each virtual\n\nserver hosts a containerization platform that creates containers that host a\n\nsubset of the operating system.\n\nThe motivation behind deploying containers on virtual servers is often\n\nrelated to security vulnerabilities that exist when the physical server has an\n\noperating system installed. As a result, the Type 1 virtualization\n\nenvironment is more common in most production environments. Type 2\n\nvirtualization is typically used in development environments when\n\ncontainerized solutions are being built and tested.\n\nType 2 virtualization can also be used for smaller solutions or for smaller\n\norganizations when the underlying physical server needs an operating\n\nsystem in order to host additional programs and systems alongside the\n\ncontainerization platform.\n\nThe next two sections highlight key benefits and challenges of utilizing\n\ncontainerization technology with an emphasis of how containers compare to\n\nvirtual servers.\n\nContainerization Benefits\n\nThe following section highlights the key benefits of utilizing\n\ncontainerization technology. Many of these benefits are described in\n\nrelation to how containers compare to virtual servers.\n\nSolution Optimization — Being able to customize an isolated environment\n\nfor a solution that minimizes its footprint allows the solution to perform\n\nmore optimally while demanding only the infrastructure resources it\n\nactually requires.\n\nEnhanced Scalability — The reduced CPU, memory and storage usage\n\nfootprint of containers allows them to be more effectively and rapidly\n\nscaled in response to usage demands.\n\nEnhanced Resiliency — Using special features of container environments,\n\nresiliency can be natively provided to ensure that new solution instances are\n\nautomatically generated in response to failure conditions.\n\nEnhanced Deployment Speed — Containers can be created and deployed\n\nfaster than virtual servers, which supports rapid deployment and facilitates\n\nDevOps approaches, such as continuous integration (CI).\n\nVersion Support — Containers allow versions of software code and its\n\ndependencies to be tracked. Some platforms allow developers to maintain\n\nand track versions of a solution, inspect differences between different\n\nversions, and roll back to previous versions, when required.\n\nEnhanced Portability — A containerized solution can be more easily moved\n\nacross server hosting environments, without the need to change the solution\n\nsoftware within the container.\n\nContainerization Risks and Challenges\n\nThe following are common risks and challenges of using containerization:\n\nLack of Isolation from Host Operating System — When multiple containers\n\nare deployed on the same physical server, they end up sharing the same host\n\noperating system. This means that if the underlying physical server fails or\n\nis compromised, all containers running on the server will likely be\n\nimpacted.\n\nContainer Attack Threat — Whereas the administrator of a virtual server\n\ncannot access or modify the operating system of the underlying physical\n\nserver, the administrator of a container can, because the operating system’s\n\nkernel is shared among all containers running on the same physical server.\n\nThis introduces a significant security vulnerability when containerization\n\nplatforms are deployed without the involvement of virtual servers.\n\nIncreased Complexity — The addition of containerization technology adds\n\nnew layers and design considerations that can increase the complexity of\n\nthe underlying solution infrastructure. This can introduce effort and risk, as\n\nwell as an increased learning curve for those responsible for building the\n\nsolution and its underlying infrastructure environment.\n\nIncreased Administrative Overhead — Because a given container provides\n\na given version of a solution with only the operating system resources it\n\nrequires, on-going administrative effort may be needed to maintain the\n\ncreation of subsequent container versions that may be needed to\n\naccommodate the changing needs of future solution versions. In a virtual\n\nserver environment, this is less of a concern because the entire operating\n\nsystem is always provided to a solution and its subsequent versions.\n\n6.3 Understanding Containers\n\nWhile a container can contain any type of software program, it is most\n\ncommonly used to host applications or services that comprise or are part of\n\na greater automation solution (Figure 6.23).\n\nFigure 6.23\n\nThe symbol on the left is used to represent a software program that is an\n\napplication or an application component. The symbol on the right\n\nrepresents a software program that is designed as a service.\n\nContainer Hosting\n\nA single container can host a single software program and multiple\n\ncontainers can co-exist, side-by-side, in the same environment (Figure\n\n6.24). When multiple containers reside in the same underlying environment,\n\nthey are securely isolated from each other so that each container can operate\n\nindependently.\n\nFigure 6.24\n\nThree different containers host three different software programs.\n\nA single container can also be used to host multiple related or different\n\nsoftware programs (Figure 6.25).\n\nFigure 6.25\n\nEach of the three containers hosts one or more different software programs.\n\nContainers are dynamically generated based on pre-defined container\n\nimages, as explained shortly.\n\nContainers and Pods\n\nGrouping individual containers in a pod allows related software programs\n\nto be kept together, such as when they are part of the same overall\n\ndistributed solution (or namespace) and when they need to run under a\n\nsingle IP address (as explained later in the Container Network Addresses\n\nsection) (Figure 6.26). Containers inside a pod can find and discover each\n\nother via the host that the pod is deployed on and can communicate with\n\neach other using standard inter-process communication methods, such as\n\nshared memory. As explained shortly, containers in a pod can also share a\n\nfile system, dataset or data storage device.\n\nFigure 6.26\n\nA single pod deployed on a virtual server allows the hosted services to\n\nshare the same IP address. The pod can also be deployed directly on a\n\nphysical server.\n\nThe pod establishes this environment, while ensuring that hosted programs\n\nare still isolated from each other. Pods further provide special\n\ncontainerization capabilities associated with container chains, orchestration\n\nand scaling. The usage of pods is therefore often required by the\n\ncontainerization platform, which is why single pods are frequently used to\n\nhost single containers.\n\nAn administrator creates and configures a pod, and the containers are then\n\nadded (Figures 6.27 and 6.28).\n\nFigure 6.27\n\nAn empty Pod A is created by Administrator A.\n\nFigure 6.28\n\nAdministrator A instructs the container engine to add Containers A, B and\n\nC to Pod A.\n\nA common feature of pods is the ability for the pod to provide common\n\nstorage to the containers residing in it. The storage usually exists as a file\n\nsystem that is referred to as a volume. This form of common storage can be\n\nattractive as it offers high-speed access to stored content. Types of files\n\nstored in a volume can include log files, media files and configuration files.\n\nThe administrator can configure a pod to enable access to resident\n\ncontainers (Figure 6.29).\n\nFigure 6.29\n\nThe administrator allocates file system storage that is made available to the\n\ncontainers deployed inside the pod.\n\nWhen deploying a pod hosted on a virtual server, the additional\n\nvirtualization layer may add runtime processing latency. Depending on the\n\nhosted application or service requirements, this can cause performance\n\nissues. In some deployment scenarios, the performance may be impacted by\n\nother virtual servers hosted on the same host. If the applications or services\n\ndeployed in the pod are latency sensitive, they can be especially negatively\n\nimpacted if the pod resides on a virtual server. Performance and latency can\n\nbe measured before it is determined where to best deploy the pod.\n\nContainer Instances and Clusters\n\nMultiple instances of the same container with the same software program\n\ncan be generated (Figure 6.30). This is usually required when concurrent\n\nusage of the hosted software program by multiple consumer programs is\n\nnecessary. Instances of containers are commonly referred to as replicas.\n\nFigure 6.30\n\nThree instances of Container A and its hosted Service A are generated. This\n\nallows each instance of Service A to interact with a different consumer\n\nprogram.\n\nContainer clusters (Figure 6.31) are pools of container instances that are\n\ninstantiated in advance of their actual usage. Container clusters can be\n\nmanually created or automatically generated. They are loaded into memory\n\nwhere they sit idle, waiting to be invoked. They can be scheduled so that\n\nthey only reside in memory during pre-determined time periods, such as\n\nanticipated peak usage times.\n\nFigure 6.31\n\nThe symbol used to represent a container cluster.\n\nContainer clusters are primarily created in support of high-performance\n\nrequirements, often for service-based solutions, to ensure that the\n\ncontainerized service instances can be rapidly provisioned in response to\n\nusage demands. Container cluster enviornments can provide auto-scaling\n\ncapabilities, enabling them to dynamically adjust the size of a cluster based\n\non demand.",
      "page_number": 225
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 249-271)",
      "start_page": 249,
      "end_page": 271,
      "detection_method": "synthetic",
      "content": "Container Package Management\n\nContainer package management refers to the process of managing software\n\npackages and dependencies within containerized applications. It enables an\n\napplication and its dependencies to be grouped into a single portable unit\n\ncalled a package, which can be deployed on any system that supports the\n\ncontainerization technology.\n\nA container package manager is a tool that makes containerized application\n\npackaging and distribution easier. It allows container images and their\n\ndependencies to be grouped into a single, distributable package that can be\n\ndeployed and managed across multiple container orchestrators (a\n\nmechanism described in the following section).\n\nContainer package managers typically include a set of command-line tools\n\nfor creating, tagging, and submitting container images to a container\n\nregistry, as well as for creating and managing container images and their\n\ndependencies. They frequently allow the use of templates or configuration\n\nfiles to define the contents of the package and its dependencies, as well as a\n\nmethod to version and manage the package over time.\n\nA container package manager is used to coordinate the initial deployment of\n\ncontainers based on pre-defined workflow logic. The deployment workflow\n\nlogic is defined in a package (also known as a container deployment file)\n\n(Figure 6.32). Typically, host clusters are required to provide a pool of hosts\n\nin support of the deployment requirements.\n\nFigure 6.32\n\nThe symbol used to represent a package.\n\nThe container deployment file is retrieved from a package repository\n\n(Figure 6.33).\n\nFigure 6.33\n\nThe symbol used to represent a package repository.\n\nThe container deployment file is then provided to the container package\n\nmanager (Figure 6.34).\n\nFigure 6.34\n\nThe symbol used to represent a container package manager.\n\nBefore the container package manager carries out the deployment\n\nworkflow, a special deployment optimizer program (Figure 6.35) studies the\n\ncontents of the package and then assesses available hosts in the cluster to\n\ndetermine the optimal destination for the containers to be deployed.\n\nBesides the processing capacity of a candidate host, some of the other\n\nfactors that the deployment optimizer may consider include:\n\nhardware and software policy limitations\n\naffinity and anti-affinity specifications\n\ndata locality\n\ninter-workload interference\n\nOnce it has chosen a suitable destination host, the deployment optimizer\n\ninstructs the container package manager as to where the containers should\n\ngo. A deployment optimizer can further monitor already deployed\n\ncontainers to ensure their current hosts remain suitable.\n\nFigure 6.35\n\nThe symbol used to represent a deployment optimizer.\n\nNote\n\nWithin the context of containerization, deployment\n\noptimization is often referred to as “scheduling”.\n\nFurthermore, container package manager and deployment\n\noptimizer programs are often limited to deploying containers\n\nresiding in pods.\n\nTypically, a package represents the containers that comprise an entire\n\nsolution. Container package managers are therefore created for a set of\n\nrelated containers. In this sense, the package repository can provide a means\n\nof application version management.\n\nExamples of what is defined in a package include:\n\nwhich host a given container will be deployed on\n\nwhich pod a given container will be deployed in\n\nwhat sequence a set of containers are deployed in\n\nThe administrator authors a package, stores it in the package repository and\n\nthen assigns it to the container package manager when it is time for the\n\ncontainers to be deployed (Figure 6.36).\n\nFigure 6.36\n\nThe container package manager coordinates the deployment of containers,\n\nas per the deployment workflow logic provided in the package and the host\n\ndeployment instructions it receives from the deployment optimizer.\n\nAfter the deployment, packages are still usually kept in the package\n\nrepository as they are often reusable. For example, if a set of containers\n\nshould need to be ported to a new host, the same container deployment file\n\ncould be revised with the new host information and then reused.\n\nDocker Compose and Helm are some popular container package managers.\n\nThese tools make it easier for developers to deploy and manage\n\ncontainerized applications on a variety of container orchestrators (described\n\nin the following section) by simplifying the packaging and distribution of\n\ncontainerized applications.\n\nContainer Orchestration\n\nThe process of automating the deployment, scaling, and management of\n\ncontainerized applications in a distributed computing environment is known\n\nas container orchestration. It entails the use of a container orchestrator,\n\nalso referred to as a container orchestration tool or container orchestration\n\nplatform.\n\nA container orchestrator performs a wide range of operations in a\n\ndistributed computing environment. These are some of the key operations\n\nperformed by a container orchestrator:\n\nContainer Deployment – A container orchestrator deploys containers across\n\nmultiple nodes in a cluster, ensuring that the containers are properly\n\nconfigured and networked.\n\nLoad Balancing – The orchestrator distributes traffic across multiple\n\ncontainers running the same application, helping to ensure high availability\n\nand scalability.\n\nScaling – The orchestrator automatically scales up, down, in or out the\n\nnumber of containers running an application based on demand, helping to\n\nensure optimal resource utilization and cost efficiency.\n\nHealth Monitoring – The orchestrator monitors the health of containers and\n\ncan automatically restart failed containers or replace them with healthy\n\nones.\n\nService Discovery – The orchestrator maintains a service registry, allowing\n\napplications to discover and communicate with each other across the\n\nnetwork.\n\nStorage Orchestration – The orchestrator manages the persistent storage\n\nneeds of containers, ensuring that data is stored and retrieved correctly.\n\nNetwork Orchestration – The orchestrator manages the networking needs of\n\ncontainers, providing each container with a unique IP address and routing\n\nnetwork traffic between containers.\n\nConfiguration Management – The orchestrator manages the configuration\n\nof containers and can automatically apply changes to running containers.\n\nA container orchestrator typically consists of several components that work\n\ntogether. Some of the key components of a container orchestrator are:\n\nContainer Runtime – Responsible for running and managing containers on\n\neach node in the cluster.\n\nAPI Server – Provides a central interface for interacting with the\n\norchestrator. It accepts API requests from clients and communicates with\n\nthe other components of the orchestrator to perform the requested actions.\n\nScheduler – Responsible for deciding which node in the cluster to deploy a\n\nnew container to, based on factors such as resource availability and\n\nworkload balancing.\n\nController Manager – Responsible for managing various controllers that\n\nautomate different aspects of the containerized application lifecycle, such as\n\nscaling, replication, and health monitoring.\n\nDistributed Key-Value Store – Used by the orchestrator to store\n\nconfiguration data, service discovery information, and other metadata.\n\nNetworking – A component that provides the necessary network\n\ninfrastructure to allow containers to communicate with each other across\n\nthe cluster, including routing and load balancing.\n\nStorage – A component that manages the persistent storage needs of\n\ncontainers, including providing access to shared storage resources and\n\nensuring data integrity.\n\nThe basic steps involved in container orchestration are:\n\nCreate a container image – Developers create a container image that\n\nincludes their application code and all its dependencies.\n\nPush the image to a container registry – The container image is pushed to a\n\ncontainer registry, which is a central remote repository of container images.\n\nDefine the application deployment – Using a container orchestrator,\n\ndevelopers define how the containerized application should be deployed,\n\nincluding the number of replicas, the network configuration, and any\n\nstorage requirements.\n\nDeploy the application – The container orchestrator deploys the application\n\nacross multiple nodes in a cluster, ensuring that the desired number of\n\nreplicas are running and that the application is accessible to users.\n\nMonitor and manage the application – The container orchestrator monitors\n\nthe health of the application, automatically scaling it up or down as needed,\n\nand rolling out updates and patches without causing downtime. It also\n\nprovides logging and monitoring capabilities to identify and troubleshoot\n\nany issues that arise.\n\nManage multiple applications – The container orchestrator can manage\n\nmultiple containerized applications simultaneously, ensuring that they are\n\ndeployed, scaled, and managed according to their individual requirements.\n\nContainer Package Manager vs. Container Orchestrator\n\nA container package manager and a container orchestrator serve different\n\nfunctions. The following are the key differences:\n\nFunction – A container package manager is responsible for managing\n\ncontainer images and their dependencies, while a container orchestrator is\n\nresponsible for automating the deployment, scaling, and management of\n\ncontainerized applications in a distributed computing environment.\n\nScope – Container package managers focus specifically on managing\n\ncontainer images and their dependencies, while container orchestrators\n\nmanage the entire containerized application, from deployment to scaling to\n\nmanagement.\n\nLevel of Abstraction – Container package managers operate at a lower level\n\nof abstraction than container orchestrators. Package managers deal with\n\nindividual container images and their dependencies, while orchestrators\n\nprovide a high-level view of the entire containerized application.\n\nToolset – Container package managers typically provide a more limited set\n\nof tools focused on managing container images and their dependencies.\n\nContainer orchestrators, on the other hand, provide a range of tools and\n\nAPIs for managing containers, networks, storage, and other infrastructure\n\nresources.\n\nContainer Networks\n\nContainerization platforms generally provide virtual container networks in\n\norder to enable communication among containers that need to connect with\n\neach other. A container network is required to enable various\n\ncontainerization platform and system capabilities in support of providing:\n\ncontainer availability\n\ncontainer scalability\n\ncontainer resiliency\n\nThe container network typically exists as a virtual network (Figure 6.37)\n\nthat can be independently managed, configured and encrypted.\n\nFigure 6.37\n\nA container network allows containers to communicate with each other\n\nindependently from the communication among the software programs they\n\nhost.\n\nAs previously described in the Fundamental Virtualization and\n\nContainerization section, there are two primary types of container\n\nnetworks:\n\nHost Network\n\nOverlay Network\n\nWhereas the host network is managed by a single container engine to\n\nsupport communication among containers on the same host, the overlay\n\nnetwork enables container engines deployed on different servers to enable\n\ncommunication between containers on different hosts.\n\nFor example, if a distributed solution encompasses two services, each in its\n\nown container, then a container network is established for the two\n\ncontainers that are part of that solution. If the containers are on the same\n\nhost, then a host network is created (Figure 6.38). If one container is on one\n\nhost and the other is on a different host, then an overlay network is created\n\n(Figure 6.39).\n\nFigure 6.38\n\nContainers A and B reside in separate pods on the same host and can\n\ncommunicate with each other via Host Network A.\n\nFigure 6.39\n\nContainers A and B reside on different hosts and can communicate with\n\neach other via Overlay Network A.\n\nContainer Network Scope\n\nThe scope of a container network is usually equal to the scope of a given\n\nsolution. This is because the scope of a solution will encompass only those\n\ncontainers hosting software programs that are part of that solution.\n\nTherefore, when multiple solutions are hosted, multiple container networks\n\nwill be required.\n\nSome solutions share software programs, such as reusable utility services or\n\na shared database. If the reusable software program is in a container, then\n\nthat container can participate in multiple container networks (Figure 6.40).\n\nFigure 6.40\n\nContainers A and B reside on the same host and can communicate via Host\n\nNetwork A. Container B is further part of Overlay Network B through which\n\nit can communicate with other containers.\n\nNote\n\nThe container network(s) that a given container will join can\n\nbe specified by the administrator in the container image’s\n\nbuild file and also in the container deployment package. If\n\nno network is specified, the container engine may\n\nautomatically assign a container to a “default” host network.\n\nBuild files are covered later in this chapter in the Container\n\nBuild Files sub-section of the upcoming Understanding\n\nContainer Images section.\n\nUsually, container networks by default limit communication of the\n\ncontainerized solution software programs so that they can only\n\ncommunicate with each other. However, a solution may need to be able to\n\naccess software programs or IT resources that are not containerized and\n\ntherefore reside outside of the container network. In this case, the container\n\nnetwork needs to be configured to allow the solution to communicate\n\noutside of the container network boundary (Figure 6.41).\n\nFigure 6.41\n\nContainers A and B communicate with each other via Host Network A.\n\nContainerized Applications A and B need to communicate with each other,\n\nas well as with Database A, which resides outside of Host Network A. The\n\nadministrator enables this by explicitly configuring Host Network A to\n\nallow for the required external access.\n\nContainer Network Addresses\n\nEach deployed container receives a network address that enables it to\n\nparticipate in a container network. A network address usually exists as an IP\n\naddress. If a container needs to participate in multiple container networks, it\n\nwill require a separate network address for each container network.\n\nFor example, if a container hosting a software program is being reused\n\nacross two container networks (such as in the example shown in Figure\n\n6.40), then that container will need two network addresses.\n\nNetwork addresses are usually assigned by the container engine subsequent\n\nto container deployment. They can also be manually assigned by the\n\nadministrator in a deployment package. Containers residing in the same pod\n\nshare the same network address and are individually identified through\n\ndifferent network ports.\n\nRich Containers\n\nDifferent types of containerization platforms can vary in the range of\n\nfeatures supported by a given container. Containers that are more feature-\n\nrich are referred to as rich containers (Figure 6.42).\n\nFigure 6.42\n\nService A is deployed inside a rich container that provides extra features,\n\nincluding monitoring capabilities that can provide on-going status and\n\nhealth information about the service.\n\nThe extent to which a container can be feature-rich is determined by the\n\ncapabilities of the underlying container engine responsible for creating the\n\ncontainer.\n\nExamples of features provided by more advanced container engines\n\ninclude:\n\nContainer resources can be limited to control and govern the maximum\n\nnumber of resources that a container can consume.\n\nUsage logs can be collected for auditing and regulatory purposes.\n\nContainer restart criteria can be specified. For example, the container can be\n\nconfigured to automatically restart if a certain event or error occurs, but not\n\nif other types of events or errors occur.\n\nContainer storage management features, such as enabling an isolated file\n\nsystem to be shared by multiple containerized services.\n\nSharing storage between the host and the containers running on the host.\n\nThis may be required for regulatory and auditing purposes or to ensure that\n\nif the container is turned off, access to the data is still available.\n\nSupport for the execution of service composition logic for services\n\ndeployed inside a container hosted together on the same host.",
      "page_number": 249
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 273-295)",
      "start_page": 273,
      "end_page": 295,
      "detection_method": "synthetic",
      "content": "Container Image Types and Roles\n\nHow container images are used, stored and processed depends on what type\n\nthey are or what role they assume.\n\nThere are two primary types of container images:\n\nBase Container Images — These container images act as templates for\n\ncustomized container images. In this book, this type of container image is\n\nqualified as a “base” container image. Base container images are also\n\nreferred to as partial container images.\n\nCustomized Container Images — These container images are created by the\n\ncontainer engine, which then uses them to create actual, deployed\n\ncontainers. In this book, this type of container image may or may not be\n\nqualified as a “customized” container image. When a symbol is only labeled\n\nas a container image, it is implied that it has been customized.\n\nThe reason that these types of container images can be considered roles is\n\nthat a customized container image created from a base container image can,\n\nitself, become a base container image to be used as a template for future,\n\ndifferent customized container images.\n\nAs illustrated in Figure 6.43, a container image classified as a base\n\ncontainer image is published to the image registry from where it can then be\n\naccessed by the container engine to form the basis of customized container\n\nimages (as explained in further detail later in this section).\n\nFigure 6.43\n\nThe container engine supports four different runtimes that can be deployed\n\nin containers. Application A requires the capabilities of Runtime A. Base\n\nContainer Image A is used by the container engine to create the customized\n\nContainer Image A that is then used to create and deploy the actual\n\nContainer A for Application A.\n\nContainer Image Immutability\n\nA key characteristic of container images is that, once created, they are\n\nimmutable. This means that they cannot be altered (neither patched, nor\n\nupdated, nor any other type of alteration). If a change to a container image\n\nis required, then a new or revised build file needs to be created and a new\n\nversion of the container image needs to be generated, further resulting in the\n\nneed for a new version of the deployed container.\n\nThe scope of a container’s immutability relates to the contents of the build\n\nfile. Administrative tools allow for settings on a container to be changed\n\nthat do not relate to a build file and can therefore be made without the need\n\nto create a new version of the container.\n\nThe container engine assigns each individual container image a unique\n\nauto-generated image key that is further kept where the container image is\n\nstored, either in the image registry or in the container engine’s internal\n\nstorage.\n\nContainer Image Abstraction\n\nA base container image will typically provide a subset of the functions\n\noffered by the underlying host operating system. This is referred to as\n\noperating system abstraction or, simply, abstraction. However, not all parts\n\nof the operating system are abstracted by the container image, as further\n\nexplained in the next two sub-sections.\n\nOperating System Kernel Abstraction\n\nEach operating system has a kernel, which exists as a set of the most\n\nessential operating system functions. Kernels in different operating systems\n\nare comprised of very similar functions. For example, the kernel of a\n\nWindows operating system has similar functions to the kernel of a Linux\n\noperating system.\n\nCommon functions provided by a kernel can include:\n\naccess to CPU resources\n\naccess to processing\n\naccess to host memory\n\naccess to input/output devices in the host\n\naccess to hardware storage\n\naccess to device drivers\n\naccess to server file systems\n\naccess to power management\n\nThe kernel is not abstracted by the container image. Instead, it is\n\nencompassed by the container engine. As a result, container images do not\n\nneed to copy the kernel, which helps to further reduce their footprint.\n\nThe container engine acts as somewhat of a liaison or intermediary,\n\nenabling containers to have full access to the entire set of kernel functions\n\nat runtime. Container engines are generally able to interact with kernels\n\nfrom different operating systems, which helps enable the portability of\n\ncontainers and their platforms across different hosting environments.\n\nOperating System Abstraction Beyond the Kernel\n\nThe parts of an operating system outside of the kernel can be abstracted and\n\nincluded in a container image.\n\nCommon operating system functions and resources that exist outside of the\n\nkernel can include:\n\nprogramming language libraries and compilers\n\nvarious system libraries\n\nencryption platforms\n\nsystem monitors and monitoring functions\n\nconfiguration files and editors\n\nadministrative functions and platforms\n\nadministrative tools (for use by human administrators)\n\nlocalization programs\n\nThis form of abstraction represents the subset of the operating system that a\n\ngiven container image can capture to provide a customized and optimized\n\nhosting environment for the software programs that will be deployed in the\n\ncontainer generated from that container image.\n\nWhen abstracting these types of functions and resources, the container\n\nremains portable because the abstracted functions and resources are copied\n\nto and ported with the container.\n\nContainer Build Files\n\nA container build file (or just the build file) (Figure 6.44) is a human-\n\neditable, machine-processable configuration file that specifies what belongs\n\nin (or what is abstracted by) a customized container image.\n\nFigure 6.44\n\nThe symbol used to represent a container build file.\n\nSpecifically, the build file can identify:\n\nthe base container image that will be used to form the basis of the\n\ncustomized container image\n\nthe additional operating system resources to be added to (or abstracted by)\n\nthe customized container image\n\nthe container network(s) that the deployed customized container will need\n\nto participate in\n\nThe syntax and format in a given build file can vary, depending on the type\n\nof container engine being used.\n\nContainer Image Layers\n\nA container image organizes its content into layers. Each layer corresponds\n\nto a container build file statement or instruction.\n\nExamples of content in container image layers include:\n\ndata files and folders\n\nconfiguration files\n\ndatabases and repositories\n\nexecutable files\n\noperating system program files and runtimes\n\nExcept for the very final layer, all the layers are read-only. The\n\ncontainerization platform uses a union file system as the basis of container\n\nimage layering. The use of the union file system and layering is what makes\n\nthe reusability of base container images possible.\n\nA base container image is comprised of a number of layers that represent\n\nwhat it abstracts (Figure 6.45).\n\nFigure 6.45\n\nThe base container image has its own set of layers.\n\nThe customized container image that is derived from the base container\n\nimage will add layers to what is provided by the base container image. In\n\nthe customized container image, the entire base container image represents\n\nthe bottom layer (Figure 6.46).\n\nFigure 6.46\n\nThe bottom layer of the customized container image is comprised of the\n\ncontents of the base container image.\n\nA software program that will be hosted by the deployed container that will\n\nbe generated from the customized container image can, itself, reside in a\n\nlayer of the customized container image (Figure 6.47).\n\nFigure 6.47\n\nA layer within the customized container image is comprised of the software\n\nprogram that the deployed container will be responsible for hosting.\n\nBecause container images are immutable, if a layer within an image needs\n\nto be removed or added to, a new container image version needs to be\n\ncreated.\n\nHow Customized Container Images are Created\n\nThe container engine uses the build file together with the base container\n\nimage to generate the customized container image (Figure 6.48).\n\nFigure 6.48\n\nThe administrator authors a build file for Container A (1). The\n\nadministrator provides the build file to the container engine (2). The\n\ncontainer engine retrieves the required base container image from the\n\nimage registry (3). The container engine then uses the base container image\n\nand the information from the build file to create a new customized\n\nContainer Image A from which it then generates and deploys Container A\n\n(4).\n\nOnce the actual container implementation is created from the customized\n\ncontainer image and deployed, there may no longer be a need to keep the\n\nbuild file because the container engine now has the customized container\n\nimage that it can use to create more instances of the actual container in the\n\nfuture.\n\nNote that the customized container image is not typically stored in the\n\nimage registry. Instead, it is stored in the container engine’s internal storage\n\nso that the engine can retain immediate access to it in support of efficiently\n\nand rapidly creating new container instances for scalability and resiliency\n\npurposes.\n\nA customized container image can also be published to the image registry if\n\nthe administrator determines that it may be useful as a base container image\n\nto be used for new types of customized container images.\n\n6.5 Multi-Container Types\n\nSo far, the majority of containers shown have been hosting applications and\n\nservices that, presumably, are responsible for processing primary business\n\nlogic. However, for applications to function in distributed environments,\n\nadditional types of secondary (or utility) processing are also required.\n\nThis section introduces the following set of basic multi-container types,\n\neach of which adds a container with a secondary component that abstracts\n\nutility-related processing:\n\nSidecar Container\n\nAdapter Container\n\nAmbassador Container\n\nSidecar Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to process generic utility logic, the ability for the application to\n\nprocess its business logic reliably and effectively can be compromised\n\n(Figure 6.49).\n\nFigure 6.49\n\nApplication A is burdened with carrying out business logic and utility logic.\n\nA secondary containerized application component (referred to as a sidecar\n\ncomponent) is added to abstract the utility logic-related processing (Figure\n\n6.50). The sidecar component is deployed in a separate container, usually\n\nwithin the same pod as the application. Depending on the nature of the\n\nutility processing, the application may or may not need to communicate\n\nwith the sidecar component.\n\nFigure 6.50\n\nThe utility logic is placed in Sidecar Component A that resides in the\n\nseparate Container B, in the same pod. This enables Application A to focus\n\nexclusively on carrying out its business logic.\n\nAdapter Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to carry out data conversion logic to accommodate external\n\nconsumer applications, the ability for the application to process its business\n\nlogic reliably and effectively can be compromised (Figure 6.51).\n\nFurthermore, by embedding this conversion logic into the application, it\n\nmay become coupled to multiple different external consumer programs,\n\nwhich can become burdensome when those consumer programs change\n\nover time.\n\nFigure 6.51\n\nApplication A is burdened with carrying out business logic and specific\n\nconversion logic required by Application B.\n\nA secondary containerized application component (referred to as an adapter\n\ncomponent) is added to abstract any necessary data conversion processing\n\nlogic (Figure 6.52). The adapter component is deployed in a separate\n\ncontainer, usually within the same pod as the application. A separate adapter\n\ncomponent can be deployed for each consumer application that requires a\n\ndifferent representation of the output data.\n\nFigure 6.52\n\nThe conversion logic is placed in Adapter Component A that resides in the\n\nseparate Container B, in the same pod. This enables Application A to focus\n\nexclusively on carrying out its business logic.\n\nAmbassador Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to carry out external communication processing logic to connect\n\nwith external consumer applications, the ability for the application to\n\nprocess its business logic reliably and effectively can be compromised\n\n(Figure 6.53). Also, by embedding this specific communication logic (such\n\nas logic related to protocols, messaging and security) into the application, it\n\nmay become coupled to multiple different external programs, which can\n\nbecome burdensome when the APIs of those programs change over time.\n\nFigure 6.53\n\nApplication A is burdened with carrying out business logic and specific\n\ncommunications processing logic required to connect with Application B.\n\nA secondary containerized application component (referred to as an\n\nambassador component) is added to abstract any necessary communication\n\nprocessing logic (Figure 6.54). The ambassador component is deployed in a\n\nseparate container, usually within the same pod as the application. A\n\nseparate ambassador component can be deployed for each application that\n\nhas a set of different communication requirements.\n\nFigure 6.54",
      "page_number": 273
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 296-321)",
      "start_page": 296,
      "end_page": 321,
      "detection_method": "synthetic",
      "content": "The communications logic is placed in Ambassador Component A that\n\nresides in the separate Container B, in the same pod. This enables\n\nApplication A to focus exclusively on carrying out its business logic.\n\nUsing Multi-Containers Together\n\nThe three types of multi-containers can be used individually or together, as\n\nrequired. For example, depending on the nature of this business logic, an\n\napplication may require some or all of the secondary containers to be\n\ndeployed with it (Figure 6.55).\n\nFigure 6.55\n\nApplication A is supported by three secondary containers.\n\n6.6 Case Study Example\n\nInnovartus Technologies Inc. has identified multiple benefits from using\n\ncontainerization technology in support of its technology and business\n\nstrategies, including the following:\n\nScalability can be greatly improved to accommodate increased and less\n\npredictable cloud consumer interaction.\n\nService levels can also be improved to avoid the frequent outages that are\n\ncurrently occurring more frequently than usual.\n\nCost effectiveness can be improved by reducing the number of virtual\n\nservers required for the delivery of its virtual products, as these products\n\ncan now be deployed in containers instead of virtual servers.\n\nThe virtual toys and educational entertainment products for children offered\n\nby Innovartus Technologies were designed as applications comprised of\n\nmultiple independent services that work together to provide the necessary\n\nfunctionality. This allows for every individual service to be deployed in its\n\nindividual container and scaled out dynamically in accordance with its\n\nperformance and total capacity requirements.\n\nDue to certain security-related requirements, three services that provide\n\naccess to parents for the configuration of their child’s virtual toys need to\n\nshare the same IP address. Deploying them in separate containers within a\n\nsingle logical pod provides the ideal deployment solution for this\n\nrequirement.\n\nMonitoring the usage, performance and security of the virtual toys and\n\nentertainment products is fundamental to its business strategy. However, to\n\nallow each service running in its own container to focus on the functionality\n\nit must deliver as part of a virtual toy or other entertainment product,\n\nsidecar containers can be used to separate utility-resalted functionality, like\n\nwriting logs or reporting data to performance and security monitoring logic,\n\nin components running in the sidecar container.\n\nTwo of the monitoring systems that the services need to send telemetry data\n\nto are deployed remotely. In this case, ambassador containers are used to\n\nallow services to delegate the communication with the remote system to the\n\nambassador and focus on the core functionality they were designed to\n\ndeliver.\n\nFinally, adapter containers are used throughout the Innovartus architecture\n\nto allow the use of their products by users via different devices (such as\n\nsmart phones, tablets and computers) with the adapter containers running\n\nthe logic necessary for every device to be accessed separately by both\n\nparents and children.\n\nChapter 7\n\nUnderstanding Cloud Security and Cybersecurity\n\n7.1 Basic Security Terminology\n\n7.2 Basic Threat Terminology\n\n7.3 Threat Agents\n\n7.4 Common Threats\n\n7.5 Case Study Example\n\n7.6 Additional Considerations\n\n7.7 Case Study Example\n\nThis chapter introduces terms and concepts that address basic information\n\nsecurity within clouds, and then concludes by defining a set of threats and\n\nattacks common to public cloud environments. The cloud security and\n\ncybersecurity mechanisms covered in Chapters 10 and 11 establish the\n\nsecurity controls used to counter these threats.\n\n7.1 Basic Security Terminology\n\nInformation security is a complex ensemble of techniques, technologies,\n\nregulations, and behaviors that collaboratively protect the integrity of and\n\naccess to computer systems and data. IT security measures aim to defend\n\nagainst threats and interference that arise from both malicious intent and\n\nunintentional user error.\n\nThe upcoming sections define fundamental security terms relevant to cloud\n\ncomputing and describe associated concepts.\n\nConfidentiality\n\nConfidentiality is the characteristic of something being made accessible\n\nonly to authorized parties (Figure 7.1). Within cloud environments,\n\nconfidentiality primarily pertains to restricting access to data in transit and\n\nstorage.\n\nFigure 7.1\n\nThe message issued by the cloud consumer to the cloud service is\n\nconsidered confidential only if it is not accessed or read by an unauthorized\n\nparty.\n\nIntegrity\n\nIntegrity is the characteristic of not having been altered by an unauthorized\n\nparty (Figure 7.2). An important issue that concerns data integrity in the\n\ncloud is whether a cloud consumer can be guaranteed that the data it\n\ntransmits to a cloud service matches the data received by that cloud service.\n\nIntegrity can extend to how data is stored, processed, and retrieved by cloud\n\nservices and cloud-based IT resources.\n\nFigure 7.2\n\nThe message issued by the cloud consumer to the cloud service is\n\nconsidered to have integrity if it has not been altered.\n\nAvailability\n\nAvailability is the characteristic of being accessible and usable during a\n\nspecified time period. In typical cloud environments, the availability of\n\ncloud services can be a responsibility that is shared by the cloud provider\n\nand the cloud carrier. The availability of a cloud-based solution that extends\n\nto cloud service consumers is further shared by the cloud consumer.\n\nFigure 7.3 depicts a scenario that demonstrates how a collection of security\n\ntechnologies helps ensure the confidentiality and integrity of data exchange\n\nover the Internet, as well as the availability of a central database containing\n\nprivate data.\n\nFigure 7.3\n\nA hospital contributes confidential medical data to a database in a cloud\n\n(1) shared by a research institution that retrieves the data (2). Supporting\n\ncybersecurity technologies provide confidentiality via encryption, integrity\n\nvia runtime scanning and availability by ensuring the on-going safety of the\n\nshared cloud-based database.\n\nAuthenticity\n\nAuthenticity is the characteristic of something having been provided by an\n\nauthorized source. This concept encompasses non-repudiation, which is the\n\ninability of a party to deny or challenge the authentication of an interaction.\n\nAuthentication in non-repudiable interactions provides proof that these\n\ninteractions are uniquely linked to an authorized source. For example, a\n\nuser may not be able to access a non-repudiable file after its receipt without\n\nalso generating a record of this access.\n\nSecurity Controls\n\nSecurity controls are countermeasures used to prevent or respond to\n\nsecurity threats and to reduce or avoid risk. Details on how to use security\n\ncountermeasures are typically outlined in the security policy, which\n\ncontains a set of rules and practices specifying how to implement a system,\n\nservice, or security plan for maximum protection of sensitive and critical IT\n\nresources.\n\nSecurity Mechanisms\n\nCountermeasures are typically described in terms of security mechanisms,\n\nwhich are components comprising a defensive framework that protects IT\n\nresources, information, and services. Chapters 10 and 11 describe a series of\n\ncloud security and cybersecurity mechanisms.\n\nSecurity Policies\n\nA security policy establishes a set of security rules and regulations. Often,\n\nsecurity policies will further define how these rules and regulations are\n\nimplemented and enforced. For example, the positioning and usage of\n\nsecurity controls and mechanisms can be determined by security policies.\n\n7.2 Basic Threat Terminology\n\nThis section covers some fundamental topics that help establish the primary\n\npurpose and scope of cybersecurity practices and technologies, as well as\n\nsome essential vocabulary.\n\nThe following section provides descriptions of fundamental terms\n\nassociated with cybersecurity threats.\n\nRisk\n\nRisk is the potential unwanted and unexpected loss that may result from a\n\ngiven action. Risks can pertain to various aspects of cybersecurity,\n\nincluding external threats, internal vulnerabilities, responses carried out\n\nagainst threats, as well as risks associated with possible human error,\n\ntechnology malfunctions and the overall quality of a cybersecurity\n\nenvironment.\n\nVulnerability\n\nA vulnerability, within the context of cybersecurity, is a flaw, gap or\n\nweakness in an IT environment or associated policies or processes that\n\nleaves an organization open to potentially successful security breaches.\n\nVulnerabilities can be physical or digital. Attackers attempt to exploit\n\nvulnerabilities, while organizations attempt to eliminate or mitigate them.\n\nExploit\n\nAn exploit occurs when an attacker is able to take advantage of a\n\nvulnerability.\n\nZero-Day Vulnerability\n\nA zero-day vulnerability is a vulnerability that an organization is either\n\nunaware of or for which it has not been able to yet provide a patch or fix.\n\nAs a result, an attacker may be able to more easily exploit this vulnerability\n\nuntil the organization is able to address it.\n\nSecurity Breach\n\nA security breach is any incident that may result in unauthorized access to\n\ninformation or systems. It typically occurs when an attacker is able to\n\nbypass security mechanisms and controls.\n\nData Breach\n\nA data breach is a type of security breach whereby an attacker is able to\n\nsteal confidential information.\n\nData Leak\n\nA data leak occurs when sensitive information is shared with unauthorized\n\nparties without an attack taking place. Data leaks can occur accidentally or\n\nintentionally and are usually carried out by humans.\n\nThreat (or Cyber Threat)\n\nA threat or a cyber threat is a known, potential attack that poses danger and\n\nrisk to an organization. The collection of threats relevant to a given\n\norganization is known as the threat landscape or cyber threat landscape.\n\nAttack (or Cyber Attack)\n\nWhen a threat is carried out by an attacker, it becomes an attack or a cyber\n\nattack.\n\nAttacker and Intruder\n\nWithin the context of cloud security and cybersecurity, an attacker is an\n\nindividual or organization that carries out cyber attacks.\n\nThere are different types of attackers:\n\nCyber Criminals — Attackers that attempt to steal private information for\n\nthe purpose of profit or other types of illegal activity.\n\nMalicious Users — An authorized user, such as a rogue employee, who\n\nabuses trusted privileges to access a system with the intent to cause harm or\n\ncarry out unauthorized actions.\n\nCyber Activists — Attackers that carry out malicious activity to promote a\n\npolitical agenda, religious belief or social ideology.\n\nState-Sponsored Attackers — Attackers who are hired by a government\n\nagency.\n\nAny attacker that has successfully gained unauthorized access within an\n\norganizational boundary is known as an intruder.\n\nAttack Vector and Surface\n\nAn attack vector is the path that an attacker takes to exploit vulnerabilities.\n\nExamples of attack vectors are email attachments, pop-up windows, chat\n\nrooms and instant messages. Human error or ignorance are commonly\n\nplanned for when creating a given attack vector. An attack surface is a\n\ncollection of attack vectors from where an attacker can access a system or\n\nextract information.\n\n7.3 Threat Agents\n\nA threat agent is an entity that poses a threat because it is capable of\n\ncarrying out an attack. Cloud security threats can originate either internally\n\nor externally, from humans or software programs. Corresponding threat\n\nagents are described in the upcoming sections. Figure 7.4 illustrates the role\n\na threat agent assumes in relation to vulnerabilities, threats, and risks, and\n\nthe safeguards established by security policies and security mechanisms.\n\nFigure 7.4\n\nHow security policies and security mechanisms are used to counter threats,\n\nvulnerabilities, and risks caused by threat agents.\n\nAnonymous Attacker\n\nAn anonymous attacker is a non-trusted cloud service consumer without\n\npermissions in the cloud (Figure 7.5). It typically exists as an external\n\nsoftware program that launches network-level attacks through public\n\nnetworks. When anonymous attackers have limited information on security\n\npolicies and defenses, it can inhibit their ability to formulate effective\n\nattacks. Therefore, anonymous attackers often resort to committing acts like\n\nbypassing user accounts or stealing user credentials, while using methods\n\nthat either ensure anonymity or require substantial resources for\n\nprosecution.\n\nFigure 7.5\n\nThe notation used for an anonymous attacker.\n\nMalicious Service Agent\n\nA malicious service agent is able to intercept and forward the network\n\ntraffic that flows within a cloud (Figure 7.6). It typically exists as a service\n\nagent (or a program pretending to be a service agent) with compromised or\n\nmalicious logic. It may also exist as an external program able to remotely\n\nintercept and potentially corrupt message contents.\n\nFigure 7.6\n\nThe notation used for a malicious service agent.\n\nTrusted Attacker\n\nA trusted attacker shares IT resources in the same cloud environment as the\n\ncloud consumer and attempts to exploit legitimate credentials to target\n\ncloud providers and the cloud tenants with whom they share IT resources\n\n(Figure 7.7). Unlike anonymous attackers (which are non-trusted), trusted\n\nattackers usually launch their attacks from within a cloud’s trust boundaries\n\nby abusing legitimate credentials or via the appropriation of sensitive and\n\nconfidential information.\n\nFigure 7.7\n\nThe notation that is used for a trusted attacker.\n\nTrusted attackers (also known as malicious tenants) can use cloud-based IT\n\nresources for a wide range of exploitations, including the hacking of weak\n\nauthentication processes, the breaking of encryption, the spamming of e-\n\nmail accounts, or to launch common attacks, such as denial of service\n\ncampaigns.\n\nMalicious Insider\n\nMalicious insiders are human threat agents acting on behalf of or in relation\n\nto the cloud provider. They are typically current or former employees or\n\nthird parties with access to the cloud provider’s premises. This type of\n\nthreat agent carries tremendous damage potential, as the malicious insider\n\nmay have administrative privileges for accessing cloud consumer IT\n\nresources.\n\nNote\n\nA notation used to represent a general form of human-driven\n\nattack is the workstation combined with a lightning bolt\n\n(Figure 7.8). This generic symbol does not imply a specific\n\nthreat agent, only that an attack was initiated via a\n\nworkstation.\n\nFigure 7.8\n\nThe notation used for an attack originating from a workstation. The human\n\nsymbol is optional.\n\n7.4 Common Threats\n\nThis section introduces several common threats and vulnerabilities in cloud-\n\nbased environments and describes the roles of the aforementioned threat\n\nagents.\n\nTraffic Eavesdropping\n\nTraffic eavesdropping occurs when data being transferred to or within a\n\ncloud (usually from the cloud consumer to the cloud provider) is passively\n\nintercepted by a malicious service agent for illegitimate information\n\ngathering purposes (Figure 7.9). The aim of this attack is to directly\n\ncompromise the confidentiality of the data and, possibly, the confidentiality\n\nof the relationship between the cloud consumer and cloud provider. Because\n\nof the passive nature of the attack, it can more easily go undetected for\n\nextended periods of time.\n\nFigure 7.9\n\nAn externally positioned malicious service agent carries out a traffic\n\neavesdropping attack by intercepting a message sent by the cloud service\n\nconsumer to the cloud service. The service agent makes an unauthorized\n\ncopy of the message before it is sent along its original path to the cloud\n\nservice.\n\nMalicious Intermediary\n\nThe malicious intermediary threat arises when messages are intercepted and\n\naltered by a malicious service agent, thereby potentially compromising the\n\nmessage’s confidentiality and/or integrity. It may also insert harmful data\n\ninto the message before forwarding it to its destination. Figure 7.10\n\nillustrates a common example of the malicious intermediary attack.\n\nFigure 7.10\n\nThe malicious service agent intercepts and modifies a message sent by a\n\ncloud service consumer to a cloud service (not shown) being hosted on a\n\nvirtual server. Because harmful data is packaged into the message, the\n\nvirtual server is compromised.\n\nNote\n\nWhile not as common, the malicious intermediary attack can\n\nalso be carried out by a malicious cloud service consumer\n\nprogram.\n\nDenial of Service\n\nThe objective of the denial of service (DoS) attack is to overload IT\n\nresources to the point where they cannot function properly. This form of\n\nattack is commonly launched in one of the following ways:\n\nThe workload on cloud services is artificially increased with imitation\n\nmessages or repeated communication requests.\n\nThe network is overloaded with traffic to reduce its responsiveness and\n\ncripple its performance.\n\nMultiple cloud service requests are sent, each of which is designed to\n\nconsume excessive memory and processing resources.\n\nSuccessful DoS attacks produce server degradation and/or failure, as\n\nillustrated in Figure 7.11.\n\nFigure 7.11\n\nCloud Service Consumer A sends multiple messages to a cloud service (not\n\nshown) hosted on Virtual Server A. This overloads the capacity of the\n\nunderlying physical server, which causes outages with Virtual Servers A and\n\nB. As a result, legitimate cloud service consumers, such as Cloud Service\n\nConsumer B, become unable to communicate with any cloud services\n\nhosted on Virtual Servers A and B.",
      "page_number": 296
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 322-345)",
      "start_page": 322,
      "end_page": 345,
      "detection_method": "synthetic",
      "content": "Note\n\nA common variation of the DoS attack is the DDoS\n\n(distributed denial of service) attack, in which multiple\n\ncompromised systems are used to flood a targeted website or\n\nnetwork with traffic, in an attempt to make it\n\nunavailable.Insufficient Authorization\n\nThe insufficient authorization attack occurs when access is granted to an\n\nattacker erroneously or too broadly, resulting in the attacker getting access\n\nto IT resources that are normally protected. This is often a result of the\n\nattacker gaining direct access to IT resources that were implemented under\n\nthe assumption that they would only be accessed by trusted consumer\n\nprograms (Figure 7.12).\n\nFigure 7.12\n\nCloud Service Consumer A gains access to a database that was\n\nimplemented under the assumption that it would only be accessed through a\n\nWeb service with a published service contract (as per Cloud Service\n\nConsumer B).\n\nA variation of this attack, known as weak authentication, can result when\n\nweak passwords or shared accounts are used to protect IT resources. Within\n\ncloud environments, these types of attacks can lead to significant impacts\n\ndepending on the range of IT resources and the range of access to those IT\n\nresources the attacker gains (Figure 7.13).\n\nFigure 7.13\n\nAn attacker has cracked a weak password used by Cloud Service Consumer\n\nA. As a result, a malicious cloud service consumer (owned by the attacker)\n\nis designed to pose as Cloud Service Consumer A in order to gain access to\n\nthe cloud-based virtual server.\n\nVirtualization Attack\n\nVirtualization provides multiple cloud consumers with access to IT\n\nresources that share underlying hardware but are logically isolated from\n\neach other. Because cloud providers grant cloud consumers administrative\n\naccess to virtualized IT resources (such as virtual servers), there is an\n\ninherent risk that cloud consumers could abuse this access to attack the\n\nunderlying physical IT resources.\n\nA virtualization attack exploits vulnerabilities in the virtualization platform\n\nto jeopardize its confidentiality, integrity, and/or availability. This threat is\n\nillustrated in Figure 7.14, where a trusted attacker successfully accesses a\n\nvirtual server to compromise its underlying physical server. With public\n\nclouds, where a single physical IT resource may be providing virtualized IT\n\nresources to multiple cloud consumers, such an attack can have significant\n\nrepercussions.\n\nFigure 7.14\n\nAn authorized cloud service consumer carries out a virtualization attack by\n\nabusing its administrative access to a virtual server to exploit the\n\nunderlying hardware.\n\nOverlapping Trust Boundaries\n\nIf physical IT resources within a cloud are shared by different cloud service\n\nconsumers, these cloud service consumers have overlapping trust\n\nboundaries. Malicious cloud service consumers can target shared IT\n\nresources with the intention of compromising cloud consumers or other IT\n\nresources that share the same trust boundary. The consequence is that some\n\nor all of the other cloud service consumers could be impacted by the attack\n\nand/or the attacker could use virtual IT resources against others that happen\n\nto also share the same trust boundary.\n\nFigure 7.15 illustrates an example in which two cloud service consumers\n\nshare virtual servers hosted by the same physical server and, resultantly,\n\ntheir respective trust boundaries overlap.\n\nFigure 7.15\n\nCloud Service Consumer A is trusted by the cloud and therefore gains\n\naccess to a virtual server, which it then attacks with the intention of\n\nattacking the underlying physical server and the virtual server used by\n\nCloud Service Consumer B.\n\nContainerization Attack\n\nThe use of containerization introduces a lack of isolation from the host\n\noperating system level. Since containers deployed on the same machine\n\nshare the same host operating system, security threats can increase because\n\naccess to the entire system can be gained. If the underlying host is\n\ncompromised, all containers running on the host may be impacted.\n\nContainers can be created from within an operating system running on a\n\nvirtual server. This can help ensure that if a security breach occurs that\n\nimpacts the operating system a container is running on, the attacker can\n\nonly gain access to and alter the virtual server’s operating system or the\n\ncontainers running on a single virtual server, while other virtual servers (or\n\nphysical servers) remain intact.\n\nAnother option is a one-service per physical server deployment model\n\nwhere all container images deployed on the same host are the same. This\n\ncan reduce risk without the need to virtualize the IT resources. In this case,\n\na security breach to one cloud service instance would only allow access to\n\nother instances, and the residual risk could be considered as acceptable.\n\nHowever, this approach may not be optimal for deploying many different\n\ncloud services because it can significantly increase the total number of\n\nphysical IT resources that need to be deployed and managed while further\n\nincreasing cost and operational complexity.\n\nMalware\n\nMalware, also referred to as malicious software, is a type of software\n\nprogram designed to cause harm to a computer system or network.\n\nMalware can be used to perform a variety of malicious activities, including:\n\nstealing protected data\n\ndeleting confidential documents\n\nlistening to private communication\n\ncollecting information about confidential activity\n\nThe fundamental basis of a malware attack is the installation of\n\nunauthorized software on the victim’s computer (Figure 7.16).\n\nFigure 7.16\n\nAn attacker making a server available to a user (via a website, for example)\n\nwho inadvertently downloads malware to a local workstation.\n\nThe following are common types of malware-based cyber attacks:\n\nVirus — Malware that can spread by infecting systems and files with code\n\nthat enables the virus to replicate and perform additional actions on the\n\ninfected system.\n\nTrojan — A piece of malicious software that appears to be a legitimate\n\napplication or service. A trojan can engage in malicious behavior, often as\n\npart of background processes, to carry out activities such as installing\n\nbackdoor code and injecting code into other running processes. A trojan\n\nmay or may not contain a virus.\n\nSpyware — A type of malware that collects information about users or\n\norganizations without their knowledge.\n\nAdware — Software designed to display unwanted advertisements or pop-\n\nups. Adware can be considered a security threat because it can collect\n\nsensitive information and may slow down systems and make them\n\nvulnerable to other types of malware.\n\nRansomware — Malware that restricts or prevents data usage or access with\n\nthe purpose of demanding payment of a fee to decrypt or release the data.\n\nAn on-going ransomware attack can be carried out using remote code\n\nexecution (as covered later in this section).\n\nBot — Malware capable of remotely receiving commands and reporting\n\ninformation to a remote destination. A bot is usually designed to work\n\ntogether with other bots (as per the Botnet threat covered later in this\n\nsection).\n\nRogue Antivirus — An application claiming to be an antivirus program that,\n\nonce installed, falsely reports security issues to mislead victims into\n\npurchasing a “full” version of the program.\n\nCrypto Jacking — The practice of using browser-based programs that run\n\nscripts embedded in web content to mine cryptocurrency without the user’s\n\nknowledge or consent.\n\nWorm — A self-replicating, self-propagating, self-contained program that\n\nuses network mechanisms to spread itself. Worms do not usually cause\n\nmuch harm beyond using up computing resources and are generally not\n\ncommon.\n\nData science technologies can be used to support malware attacks by\n\nanalyzing systems in order to discover and identify new vulnerabilities that\n\ncan be exploited by malware programs. These technologies can further\n\nenable the development of reactive malicious code that can, itself, look for\n\nnew vulnerabilities.\n\nInsider Threat\n\nAn insider threat is associated with potential damage that can be inflicted\n\nby an organization’s staff and others that may have access to the\n\norganization’s premises or systems.\n\nCommon types of insider threats include the following (Figure 7.17):\n\nmalicious — attempts by an insider (such as a disgruntled employee) to\n\naccess and potentially harm an organization's data, systems or IT\n\ninfrastructure\n\naccidental — accidental damage caused by insiders making mistakes out of\n\nignorance or due to human error, such as accidentally deleting an important\n\nfile or inadvertently sharing confidential data with an unauthorized party\n\nnegligent — accidental damage caused by insiders due to carelessness or an\n\nunwillingness to following established cybersecurity standards and policies\n\nFigure 7.17\n\nExamples of malicious (left), negligent (middle) and accidental (right)\n\ninsiders posing threats to an organization.\n\nInsider threats can put organizational assets in danger, including physical\n\nhardware, physical product inventory, corporate websites, social media\n\ncommunication and information assets.\n\nSocial Engineering and Phishing\n\nSocial engineering is a form of attack where individuals are tricked into\n\nrevealing sensitive information or performing potentially damaging actions,\n\nsuch as granting access to unauthorized parties (Figure 7.18). Social\n\nengineering tactics are popular because it can be easier to exploit people\n\nthan it is to exploit technology.\n\nFigure 7.18\n\nAn example of an attacker attempting to carry out a social engineering\n\nattack by extracting sensitive information from an employee who may be\n\nworking for a cloud consumer or cloud provider organization.\n\nPhishing is a form of social engineering that uses electronic\n\ncommunication, such as sending fraudulent emails that appear to come from\n\nvalid sources, in an attempt to coerce users into releasing sensitive\n\ninformation, triggering a security breach or performing other damaging\n\nactions.\n\nBotnet\n\nAs described earlier in the Malware section, a bot is a form of malware\n\ncapable of receiving and acting upon instructions issued by a remote\n\nattacker. A botnet attack utilizes multiple bots that are distributed across\n\ndifferent hosts in order to carry out an attack via a coordinated network of\n\nbots (a bot-net).\n\nA common technique for carrying out a botnet attack is to start with an\n\ninitial malware infection to create “zombie” hosts. A zombie host is a\n\ncomputer that belongs to an unsuspecting, legitimate organization that an\n\nattacker has taken control of. The attacker then typically uses the zombie\n\nhost to carry out attacks against another party (Figure 7.19).\n\nFigure 7.19\n\nAn attacker has turned a regular server at Organization A into a zombie\n\nserver that is controlled by the attacker to transmit malware to a user’s\n\ncomputer at Organization B.\n\nA botnet can be comprised of bots located on host servers that belong to the\n\nattacker, as well as zombie servers. Once installed, a bot seeks to connect\n\nwith other bots on other infected hosts and devices to form a network that\n\nthe attacker can use to perform malicious actions (Figure 7.20), such as\n\ncarrying out large-scale DDoS attacks, crypto jacking attacks, sending mass\n\nemails with harmful content, stealing data or even recruiting new bots.\n\nBotnets can be purchased on the dark web and can be even rented for short\n\nperiods.\n\nFigure 7.20\n\nAn attacker has turned regular servers at Organizations A and B into\n\nzombie servers. The attacker uses these servers together with some local\n\nservers to carry out an attack on Organization C.\n\nNote that botnet attacks often encompass other attacks and techniques, such\n\nas remote code execution, privilege escalation, social engineering and\n\ninsider threats.\n\nPrivilege Escalation\n\nA privilege escalation attack occurs when an attacker attempts to gain\n\nadministrator permissions after compromising a user account with limited\n\naccess privileges (Figure 7.21). This can be done by exploiting\n\nvulnerabilities that unintentionally allow a user account’s access levels to be\n\nincreased.\n\nFigure 7.21\n\nAn attacker is able to infiltrate an employee’s user account and then exploit\n\na vulnerability to upgrade access privileges.\n\nData science technologies can be used to support privilege escalation\n\nattacks by developing models that can be used to continuously search and\n\nanalyze potential victim user accounts and systems for exploitable\n\nvulnerabilities. For example, another attack could be employed to collect\n\ndata about how current systems in a network are with regards to third-party\n\nsoftware patches. The data science system may be able to process this\n\ninformation, along with additional data about the third-party software\n\nprograms and how they are configured in the target environment in order to\n\nproduce a set of recommended target areas.\n\nBrute Force\n\nIn a brute force attack, an attacker attempts a broad range of possible\n\nusername and password combinations to try to determine which one\n\ncombination is correct and enables the attacker to gain unauthorized access\n\nto a system (Figure 7.22).\n\nFigure 7.22\n\nAn attacker issues a brute force attack by bombarding a website with a\n\nseries of username and password combinations.\n\nAs such, password-only systems are most vulnerable to brute force attacks\n\nand user accounts with weak passwords are those most easily accessed.\n\nThe simplest type of brute force attack is a dictionary attack in which the\n\nattacker reads from a dictionary of possible passwords and essentially tries\n\nthem all. Credential recycling is another variation whereby usernames and\n\npasswords from prior data breaches are reused to try to break into other\n\nsystems.\n\nRemote Code Execution\n\nRemote code execution is a cyber attack in which an attacker remotely\n\nexecutes commands on a third party’s computing device.\n\nExamples of how this attack can succeed include:\n\nmalicious software (malware) being downloaded by the host (Figure 7.23)\n\nusing tunneling to gain remote access to run host server or database\n\ncommands or to control system and OS services\n\nThis attack can also be enabled by the attacker obtaining login credentials\n\nto the host computer via a brute force or Wi-Fi deauthentication attack, or\n\nvia social engineering and insider threats. A remote code execution attack is\n\nusually prefaced by an information gathering process in which the attacker\n\nuses an automated scanning tool to identify vulnerabilities.\n\nFigure 7.23\n\nUsing an already installed malware program, an attacker is able to issue it\n\ncommands to carry out damaging actions on an organization’s server.\n\nThe remote code execution technique can be utilized by other cyber attacks,\n\nsuch as botnet attacks and malware attacks that utilize ransomware or\n\ntrojans.\n\nSQL Injection\n\nSQL injection is a technique used to attack applications in which malicious\n\ncode in the form of SQL statements is inserted into an entry field on a web\n\napplication user interface, causing the web application server to execute the\n\nmalicious code (Figure 7.24).\n\nFigure 7.24\n\nAn attacker inserts harmful SQL code into a web application’s user\n\ninterface.\n\nWhen successful, access to the server can be compromised, resulting in\n\nmalware being written into the server’s database. Attackers often use search\n\nengines to identify vulnerable sites that can be altered using SQL injection.\n\nNote\n\nSQL (or the Structured Query Language) is a syntax used to\n\nissue commands to a database, such as queries and updates.\n\nData science technologies can be used to support SQL injection attacks by\n\nanalyzing historical SQL commands that have been issued against a given\n\nweb application to better determine which are more or less effective. The\n\ndata science system itself can help generate different combinations of SQL\n\ncode for automated attacks, learning from and improving over time, based\n\non the successful or failed outcome of each code submission.\n\nTunneling\n\nTunneling is a technique whereby data is embedded in an authorized\n\nprotocol packet to bypass firewall controls, allowing sensitive data to exit\n\nthe network and unauthorized or malicious data to enter without ever\n\ntriggering an alert or log entry (Figure 7.25). Tunneling can be difficult to\n\ndetect and block because tunneling packets are designed to adhere to the\n\nrules of firewalls.",
      "page_number": 322
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 346-365)",
      "start_page": 346,
      "end_page": 365,
      "detection_method": "synthetic",
      "content": "Figure 7.25\n\nAn attacker is able to get a malicious packet through an organization’s\n\nfirewall, thereby enabling the attacker to set up a tunnel to an internal\n\nserver.\n\nIn order to “tunnel” the data, the attacker uses a software program that can\n\npretend to talk to the protocol but, in reality, transfers data for some other\n\npurpose. For example, an established tunnel can be used to place malware\n\non the victim’s computer, such as spyware that remains on a host for a\n\nprolonged period to collect confidential information. It can also be used\n\nwith remote code execution in support of a botnet attack by enabling the\n\nattacker to place bots on hosts for the purpose of turning them into zombie\n\nservers.\n\nNote\n\nCommonly used protocols for attacking systems using\n\ntunneling techniques include HTTP, SSH, DNS, and ICMP.\n\nAdvanced Persistent Threat (APT)\n\nAn advanced persistent threat (APT) is a method whereby an attacker uses\n\nmultiple attacks to breach security. Often, the attacks are coordinated to\n\ntake place over a longer period of time (Figure 7.26). APTs require\n\nsophisticated technology and long-term preparation and planning by the\n\nattacker, and are therefore more common with attackers that target high-\n\nvalue organizations.\n\nFigure 7.26\n\nA set of coordinated attacks is carried out against an organization over a\n\nperiod of time. The different attacks are carried out in a specific sequence\n\nand in support of a common objective.\n\nAn objective behind APTs can be to position resources within an\n\norganization’s environment subsequent to gaining access via a security\n\nbreach. For example, an APT attack may succeed in gaining access to a\n\nnetwork after which the attacker tries to establish a foothold by implanting\n\nmalware that creates backdoors and tunnels that are then used to continue to\n\npersistently attack systems over a longer period of time.\n\nThe attacker may then attempt to deepen access by using techniques, such\n\nas brute force attacks, to gain administrator rights that then enable the\n\nattacker to take control of system resources and perhaps even lock others\n\nout.\n\nBecause successful APT attacks are carried out as a larger campaign over\n\nlonger periods of time, they enable attackers to observe and learn about an\n\nenvironment to discover further ways to harvest information or value (or do\n\nmore damage) than the attacker originally planned.\n\nA critical success factor with APT attacks is often human involvement.\n\nMany APT attacks succeed as a result of an insider threat, which can be a\n\nhuman who (possibly inadvertently) has compromised security via social\n\nengineering or phishing techniques.\n\nNote\n\nGroups of attackers that collectively carry out APT attacks\n\nare known as APT groups. An APT group can include access\n\nbrokers that only obtain and sell access information to\n\nattackers. For example, the use of access brokers is common\n\namong ransomware attackers.\n\n7.5 Case Study Example\n\nDTGOV, acting as a third-party provider to so many\n\ndifferent government organizations, undergoes a review\n\nto identify which threats it will likely be most vulnerable\n\nto.\n\nThe results indicate the following primary concerns:\n\nvirtualization attacks, because it is a completely new\n\ntype of attack it had not been prepared for before using\n\ncloud services on behalf of its customers,\n\noverlapping trust boundaries, given that all of its\n\ncustomers will now be sharing resources from a cloud\n\nprovider, they will be subject to this new threat,\n\nsocial engineering and phishing, due to the fact that, as a\n\nservice provider, it has no control over the behavior of\n\nthe end users of the systems it runs and manages.\n\nDTGOV plans to mitigate all of these threats by revising\n\nits customer agreements and by utilizing a number of the\n\nsecurity mechanisms covered in Chapters 10 and 11.\n\n7.6 Additional Considerations\n\nThis section provides a diverse checklist of issues and guidelines that relate\n\nto cloud security. The listed considerations are in no particular order.\n\nFlawed Implementations\n\nThe substandard design, implementation, or configuration of cloud service\n\ndeployments can have undesirable consequences, beyond runtime\n\nexceptions and failures. If the cloud provider’s software and/or hardware\n\nhave inherent security flaws or operational weaknesses, attackers can\n\nexploit these vulnerabilities to impair the integrity, confidentiality, and/or\n\navailability of cloud provider IT resources and cloud consumer IT resources\n\nhosted by the cloud provider.\n\nFigure 7.27 depicts a poorly implemented cloud service that results in a\n\nserver shutdown. Although in this scenario the flaw is exposed accidentally\n\nby a legitimate cloud service consumer, it could have easily been\n\ndiscovered and exploited by an attacker.\n\nFigure 7.27\n\nCloud Service Consumer A’s message triggers a configuration flaw in\n\nCloud Service A, which in turn causes the virtual server that is also hosting\n\nCloud Services B and C to crash.\n\nSecurity Policy Disparity\n\nWhen a cloud consumer places IT resources with a public cloud provider, it\n\nmay need to accept that its traditional information security approach may\n\nnot be identical or even similar to that of the cloud provider. This\n\nincompatibility needs to be assessed to ensure that any data or other IT\n\nassets being relocated to a public cloud are adequately protected. Even\n\nwhen leasing raw infrastructure-based IT resources, the cloud consumer\n\nmay not be granted sufficient administrative control or influence over\n\nsecurity policies that apply to the IT resources leased from the cloud\n\nprovider. This is primarily because those IT resources are still legally\n\nowned by the cloud provider and continue to fall under its responsibility.\n\nFurthermore, with some public clouds, additional third parties, such as\n\nsecurity brokers and certificate authorities, may introduce their own distinct\n\nset of security policies and practices, further complicating any attempts to\n\nstandardize the protection of cloud consumer assets.\n\nContracts\n\nCloud consumers need to carefully examine contracts and SLAs put forth\n\nby cloud providers to ensure that security policies, and other relevant\n\nguarantees, are satisfactory when it comes to asset security. There needs to\n\nbe clear language that indicates the amount of liability assumed by the\n\ncloud provider and/or the level of indemnity the cloud provider may ask for.\n\nThe greater the assumed liability by the cloud provider, the lower the risk to\n\nthe cloud consumer.\n\nAnother aspect to contractual obligations is where the lines are drawn\n\nbetween cloud consumer and cloud provider assets. A cloud consumer that\n\ndeploys its own solution upon infrastructure supplied by the cloud provider\n\nwill produce a technology architecture comprised of artifacts owned by\n\nboth the cloud consumer and cloud provider. If a security breach (or other\n\ntype of runtime failure) occurs, how is blame determined? Furthermore, if\n\nthe cloud consumer can apply its own security policies to its solution, but\n\nthe cloud provider insists that its supporting infrastructure be governed by\n\ndifferent (and perhaps incompatible) security policies, how can the resulting\n\ndisparity be overcome?\n\nSometimes the best solution is to look for a different cloud provider with\n\nmore compatible contractual terms.\n\nRisk Management\n\nWhen assessing the potential impacts and challenges pertaining to cloud\n\nadoption, cloud consumers are encouraged to perform a formal risk\n\nassessment as part of a risk management strategy. A cyclically executed\n\nprocess used to enhance strategic and tactical security, risk management is\n\ncomprised of a set of coordinated activities for overseeing and controlling\n\nrisks. The main activities are generally defined as risk assessment, risk\n\ntreatment, and risk control (Figure 7.28).\n\nRisk Assessment — In the risk assessment stage, the cloud environment is\n\nanalyzed to identify potential vulnerabilities and shortcomings that threats\n\ncan exploit. The cloud provider can be asked to produce statistics and other\n\ninformation about past attacks (successful and unsuccessful) carried out in\n\nits cloud. The identified risks are quantified and qualified according to the\n\nprobability of occurrence and the degree of impact in relation to how the\n\ncloud consumer plans to utilize cloud-based IT resources.\n\nFigure 7.28\n\nThe on-going risk management process, which can be initiated from any of\n\nthe three stages.\n\nRisk Treatment — Mitigation policies and plans are designed during the risk\n\ntreatment stage with the intent of successfully treating the risks that were\n\ndiscovered during risk assessment. Some risks can be eliminated, others can\n\nbe mitigated, while others can be dealt with via outsourcing or even\n\nincorporated into the insurance and/or operating loss budgets. The cloud\n\nprovider itself may agree to assume responsibility as part of its contractual\n\nobligations.\n\nRisk Control — The risk control stage is related to risk monitoring, a three-\n\nstep process that is comprised of surveying related events, reviewing these\n\nevents to determine the effectiveness of previous assessments and\n\ntreatments, and identifying any policy adjustment needs. Depending on the\n\nnature of the monitoring required, this stage may be carried out or shared by\n\nthe cloud provider.\n\nThe threat agents and cloud security threats covered in this chapter (as well\n\nas others that may surface) can be identified and documented as part of the\n\nrisk assessment stage. The cloud security and cybersecurity mechanisms\n\ncovered in Chapters 10 and 11 can be documented and referenced as part of\n\nthe corresponding risk treatment.\n\n7.7 Case Study Example\n\nBased on an assessment of its internal applications, ATN\n\nanalysts identify a set of risks. One such risk is\n\nassociated with the myTrendek application that was\n\nadopted from OTC, a company ATN recently acquired.\n\nThis application includes a feature that analyzes\n\ntelephone and Internet usage, and enables a multi-user\n\nmode that grants varying access rights. Administrators,\n\nsupervisors, auditors, and regular users can therefore be\n\nassigned different privileges. The application’s user-base\n\nencompasses internal users and external users, such as\n\nbusiness partners and contractors.\n\nThe myTrendek application poses a number of security\n\nchallenges pertaining to usage by internal staff:\n\nauthentication does not require or enforce complex\n\npasswords\n\ncommunication with the application is not encrypted\n\nEuropean regulations (ETelReg) require that certain\n\ntypes of data collected by the application be deleted after\n\nsix months\n\nATN is planning to migrate this application to a cloud\n\nvia a PaaS environment, but the weak authentication\n\nthreat and the lack of confidentiality supported by the\n\napplication make them reconsider. A subsequent risk\n\nassessment further reveals that if the application is\n\nmigrated to a PaaS environment hosted by a cloud that\n\nresides outside of Europe, local regulations may be in\n\nconflict with ETelReg. Given that the cloud provider is\n\nnot concerned with ETelReg compliance, this could\n\neasily result in monetary penalties being assessed to\n\nATN. Based on the results of the risk assessment, ATN\n\ndecides not to proceed with its cloud migration plan.\n\nPart II\n\nCloud Computing Mechanisms\n\nChapter 8: Cloud Infrastructure Mechanisms\n\nChapter 9: Specialized Cloud Mechanisms\n\nChapter 10: Cloud and Cyber Security Access-Oriented Mechanisms\n\nChapter 11: Cloud and Cyber Security Data-Oriented Mechanisms\n\nChapter 12: Cloud Management Mechanisms\n\nTechnology mechanisms represent well-defined IT artifacts that are\n\nestablished within the IT industry and commonly distinct to a certain\n\ncomputing model or platform. The technology-centric nature of cloud\n\ncomputing requires the establishment of a formal set of mechanisms that act\n\nas building blocks for cloud technology architectures.\n\nThe chapters in this part of the book define over 50 common cloud\n\ncomputing mechanisms that can be combined in different and alternative\n\nvariations.\n\nSelect mechanisms are further referenced in the architectural models\n\ncovered in Part III: Cloud Computing Architecture.\n\nChapter 8\n\nCloud Infrastructure Mechanisms\n\n8.1 Logical Network Perimeter\n\n8.2 Virtual Server\n\n8.3 Hypervisor\n\n8.4 Cloud Storage Device\n\n8.5 Cloud Usage Monitor\n\n8.6 Resource Replication\n\n8.7 Ready-Made Environment\n\n8.8 Container\n\nCloud infrastructure mechanisms are foundational building blocks of cloud\n\nenvironments that establish primary artifacts to form the basis of\n\nfundamental cloud technology architecture.\n\nThe following cloud infrastructure mechanisms are described in this\n\nchapter:\n\nLogical Network Perimeter\n\nVirtual Server\n\nHypervisor\n\nCloud Storage Device\n\nCloud Usage Monitor\n\nResource Replication\n\nReady-Made Environment\n\nContainer\n\nNot all of these mechanisms are necessarily broad-reaching, nor does each\n\nestablish an individual architectural layer. Instead, they should be viewed as\n\ncore components that are common to cloud platforms.\n\n8.1 Logical Network Perimeter\n\nDefined as the isolation of a network environment from the rest of a\n\ncommunications network, the logical network perimeter establishes a\n\nvirtual network boundary that can encompass and isolate a group of related\n\ncloud-based IT resources that may be physically distributed (Figure 8.1).\n\nFigure 8.1\n\nThe dashed line notation used to indicate the boundary of a logical network\n\nperimeter.\n\nThis mechanism can be implemented to:\n\nisolate IT resources in a cloud from non-authorized users\n\nisolate IT resources in a cloud from non-users\n\nisolate IT resources in a cloud from cloud consumers\n\ncontrol the bandwidth that is available to isolated IT resources\n\nLogical network perimeters are typically established via network devices\n\nthat supply and control the connectivity of a data center and are commonly\n\ndeployed as virtualized IT environments that include:\n\nVirtual Firewall – An IT resource that actively filters network traffic to and\n\nfrom the isolated network while controlling its interactions with the\n\nInternet.\n\nVirtual Network – Usually acquired through VLANs, this IT resource\n\nisolates the network environment within the data center infrastructure.\n\nFigure 8.2 introduces the notation used to denote these two IT resources.\n\nFigure 8.3 depicts a scenario in which one logical network perimeter\n\ncontains a cloud consumer’s on-premise environment, while another\n\ncontains a cloud provider’s cloud-based environment. These perimeters are\n\nconnected through a VPN that protects communications, since the VPN is\n\ntypically implemented by point-to-point encryption of the data packets sent\n\nbetween the communicating endpoints.\n\nFigure 8.2\n\nThe symbols used to represent a virtual firewall (top) and a virtual network\n\n(bottom).\n\nFigure 8.3\n\nTwo logical network perimeters surround the cloud consumer and cloud\n\nprovider environments.\n\nCase Study Example\n\nDTGOV has virtualized its network infrastructure to\n\nproduce a logical network layout favoring network\n\nsegmentation and isolation. Figure 8.4 depicts the logical\n\nnetwork perimeter implemented at each DTGOV data\n\ncenter, as follows:\n\nThe routers that connect to the Internet and extranet are\n\nnetworked to external firewalls, which provide network\n\ncontrol and protection to the furthest external network\n\nboundaries using virtual networks that logically abstract\n\nthe external network and extranet perimeters. Devices\n\nconnected to these network perimeters are loosely\n\nisolated and protected from external users. No cloud\n\nconsumer IT resources are available within these\n\nperimeters.\n\nA logical network perimeter classified as a demilitarized\n\nzone (DMZ) is established between the external\n\nfirewalls and its own firewalls. The DMZ is abstracted\n\nas a virtual network hosting the proxy servers (not\n\nshown in Figure 8.3) that intermediate access to\n\ncommonly used network services (DNS, e-mail, Web",
      "page_number": 346
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 366-384)",
      "start_page": 366,
      "end_page": 384,
      "detection_method": "synthetic",
      "content": "portal), as well as Web servers with external\n\nmanagement functions.\n\nThe network traffic leaving the proxy servers passes\n\nthrough a set of management firewalls that isolate the\n\nmanagement network perimeter, which hosts the servers\n\nproviding the bulk of the management services that\n\ncloud consumers can externally access. These services\n\nare provided in direct support of self-service and on-\n\ndemand allocation of cloud-based IT resources.\n\nAll of the traffic to cloud-based IT resources flows\n\nthrough the DMZ to the cloud service firewalls that\n\nisolate every cloud consumer’s perimeter network,\n\nwhich is abstracted by a virtual network that is also\n\nisolated from other networks.\n\nBoth the management perimeter and isolated virtual\n\nnetworks are connected to the intra-data center firewalls,\n\nwhich regulate the network traffic to and from the other\n\nDTGOV data centers that are also connected to intra-\n\ndata center routers at the intra-data center network\n\nperimeter.\n\nThe virtual firewalls are allocated to and controlled by a\n\nsingle cloud consumer in order to regulate its virtual IT\n\nresource traffic. These IT resources are connected\n\nthrough a virtual network that is isolated from other\n\ncloud consumers. The virtual firewall and the isolated\n\nvirtual network jointly form the cloud consumer’s\n\nlogical network perimeter.\n\nFigure 8.4\n\nA logical network layout is established through a set of logical network\n\nperimeters using various firewalls and virtual networks.\n\n8.2 Virtual Server\n\nA virtual server is a form of virtualization software that emulates a physical\n\nserver. Virtual servers are used by cloud providers to share the same\n\nphysical server with multiple cloud consumers by providing cloud\n\nconsumers with individual virtual server instances. Figure 8.5 shows three\n\nvirtual servers being hosted by two physical servers. The number of\n\ninstances a given physical server can share is limited by its capacity.\n\nFigure 8.5\n\nThe first physical server hosts two virtual servers, while the second physical\n\nserver hosts one virtual server.\n\nNote\n\nThe terms virtual server and virtual machine (VM) are used\n\nsynonymously throughout this book.\n\nThe virtual infrastructure manager (VIM) referenced in this\n\nchapter is described in Chapter 12 as part of the Resource\n\nManagement System section.\n\nAs a commodity mechanism, the virtual server represents the most\n\nfoundational building block of cloud environments. Each virtual server can\n\nhost numerous IT resources, cloud-based solutions, and various other cloud\n\ncomputing mechanisms. The instantiation of virtual servers from image\n\nfiles is a resource allocation process that can be completed rapidly and on-\n\ndemand.\n\nCloud consumers that install or lease virtual servers can customize their\n\nenvironments independently from other cloud consumers that may be using\n\nvirtual servers hosted by the same underlying physical server. Figure 8.6\n\ndepicts a virtual server that hosts a cloud service being accessed by Cloud\n\nService Consumer B, while Cloud Service Consumer A accesses the virtual\n\nserver directly to perform an administration task.\n\nFigure 8.6\n\nA virtual server hosts an active cloud service and is further accessed by a\n\ncloud consumer for administrative purposes.\n\nCase Study Example\n\nDTGOV’s IaaS environment contains hosted virtual\n\nservers that were instantiated on physical servers\n\nrunning the same hypervisor software that controls the\n\nvirtual servers. Their VIM is used to coordinate the\n\nphysical servers in relation to the creation of virtual\n\nserver instances. This approach is used at each data\n\ncenter to apply a uniform implementation of the\n\nvirtualization layer.\n\nFigure 8.7 depicts several virtual servers running over\n\nphysical servers, all of which are jointly controlled by a\n\ncentral VIM.\n\nFigure 8.7\n\nVirtual servers are created via the physical servers’\n\nhypervisors and a central VIM.\n\nIn order to enable the on-demand creation of virtual\n\nservers, DTGOV provides cloud consumers with a set of\n\ntemplate virtual servers that are made available through\n\npre-made VM images.\n\nThese VM images are files that represent the virtual disk\n\nimages used by the hypervisor to boot the virtual server.\n\nDTGOV enables the template virtual servers to have\n\nvarious initial configuration options that differ, based on\n\noperating system, drivers, and management tools being\n\nused. Some template virtual servers also have additional,\n\npre-installed application server software.\n\nThe following virtual server packages are offered to\n\nDTGOV’s cloud consumers. Each package has different\n\npre-defined performance configurations and limitations:\n\nSmall Virtual Server Instance – 1 virtual processor core,\n\n4 GB of virtual RAM, 20 GB of storage space in the root\n\nfile system\n\nMedium Virtual Server Instance – 2 virtual processor\n\ncores, 8 GB of virtual RAM, 20 GB of storage space in\n\nthe root file system\n\nLarge Virtual Server Instance – 8 virtual processor\n\ncores, 16 GB of virtual RAM, 20 GB of storage space in\n\nthe root file system\n\nMemory Large Virtual Server Instance – 8 virtual\n\nprocessor cores, 64 GB of virtual RAM, 20 GB of\n\nstorage space in the root file system\n\nProcessor Large Virtual Server Instance – 32 virtual\n\nprocessor cores, 16 GB of virtual RAM, 20 GB of\n\nstorage space in the root file system\n\nUltra-Large Virtual Server Instance – 128 virtual\n\nprocessor cores, 512 GB of virtual RAM, 40 GB of\n\nstorage space in the root file system\n\nAdditional storage capacity can be added to a virtual\n\nserver by attaching a virtual disk from a cloud storage\n\ndevice. All of the template virtual machine images are\n\nstored on a common cloud storage device that is\n\naccessible only through the cloud consumers’\n\nmanagement tools that are used to control the deployed\n\nIT resources. Once a new virtual server needs to be\n\ninstantiated, the cloud consumer can choose the most\n\nsuitable virtual server template from the list of available\n\nconfigurations. A copy of the virtual machine image is\n\nmade and allocated to the cloud consumer, who can then\n\nassume the administrative responsibilities.\n\nThe allocated VM image is updated whenever the cloud\n\nconsumer customizes the virtual server. After the cloud\n\nconsumer initiates the virtual server, the allocated VM\n\nimage and its associated performance profile is passed to\n\nthe VIM, which creates the virtual server instance from\n\nthe appropriate physical server.\n\nDTGOV uses the process described in Figure 8.8 to\n\nsupport the creation and management of virtual servers\n\nthat have different initial software configurations and\n\nperformance characteristics.\n\nFigure 8.8\n\nThe cloud consumer uses the self-service portal to select\n\na template virtual server for creation (1). A copy of the\n\ncorresponding VM image is created in a cloud\n\nconsumer-controlled cloud storage device (2). The cloud\n\nconsumer initiates the virtual server using the usage and\n\nadministration portal (3), which interacts with the VIM\n\nto create the virtual server instance via the underlying\n\nhardware (4). The cloud consumer is able to use and\n\ncustomize the virtual server via other features on the\n\nusage and administration portal (5). (Note that the self-\n\nservice portal and usage and administration portal are\n\nexplained in Chapter 12.)\n\n8.3 Hypervisor\n\nThe hypervisor mechanism is a fundamental part of virtualization\n\ninfrastructure that is primarily used to generate virtual server instances of a\n\nphysical server. A hypervisor is generally limited to one physical server and\n\ncan therefore only create virtual images of that server (Figure 8.9).\n\nSimilarly, a hypervisor can only assign virtual servers it generates to\n\nresource pools that reside on the same underlying physical server. A\n\nhypervisor has limited virtual server management features, such as\n\nincreasing the virtual server’s capacity or shutting it down. The VIM\n\nprovides a range of features for administering multiple hypervisors across\n\nphysical servers.\n\nFigure 8.9\n\nVirtual servers are created via individual hypervisor on individual physical\n\nservers. All three hypervisors are jointly controlled by the same VIM.\n\nHypervisor software can be installed directly in bare-metal servers and\n\nprovides features for controlling, sharing and scheduling the usage of\n\nhardware resources, such as processor power, memory, and I/O. These can\n\nappear to each virtual server’s operating system as dedicated resources.\n\nCase Study Example\n\nDTGOV has established a virtualization platform in\n\nwhich the same hypervisor software product is running\n\non all physical servers. The VIM coordinates the\n\nhardware resources in each data center so that virtual\n\nserver instances can be created from the most expedient\n\nunderlying physical server.\n\nAs a result, cloud consumers are able to lease virtual\n\nservers with auto-scaling features. In order to offer\n\nflexible configurations, the DTGOV virtualization\n\nplatform provides live VM migration of virtual servers\n\namong physical servers inside the same data center. This\n\nis illustrated in Figures 8.10 and 8.11, where a virtual\n\nserver live-migrates from one busy physical server to\n\nanother that is idle, allowing it to scale up in response to\n\nan increase in its workload.\n\nFigure 8.10\n\nA virtual server capable of auto-scaling experiences an\n\nincrease in its workload (1). The VIM decides that the\n\nvirtual server cannot scale up because its underlying\n\nphysical server host is being used by other virtual\n\nservers (2).\n\nFigure 8.11\n\nThe VIM commands the hypervisor on the busy physical\n\nserver to suspend execution of the virtual server (3). The\n\nVIM then commands the instantiation of the virtual\n\nserver on the idle physical server. State information\n\n(such as dirty memory pages and processor registers) is\n\nsynchronized via a shared cloud storage device (4). The\n\nVIM commands the hypervisor at the new physical\n\nserver to resume the virtual server processing (5).\n\n8.4 Cloud Storage Device\n\nThe cloud storage device mechanism represents storage devices that are\n\ndesigned specifically for cloud-based provisioning. Instances of these\n\ndevices can be virtualized, similar to how physical servers can spawn\n\nvirtual server images. Cloud storage devices are commonly able to provide\n\nfixed-increment capacity allocation in support of the pay-per-use\n\nmechanism. Cloud storage devices can be exposed for remote access via\n\ncloud storage services.\n\nNote\n\nThis is a parent mechanism that represents cloud storage\n\ndevices in general. There are numerous specialized cloud\n\nstorage devices, several of which are described in the\n\narchitectural models covered in Part III of this book.\n\nA primary concern related to cloud storage is the security, integrity, and\n\nconfidentiality of data, which becomes more prone to being compromised\n\nwhen entrusted to external cloud providers and other third parties. There\n\ncan also be legal and regulatory implications that result from relocating data\n\nacross geographical or national boundaries. Another issue applies\n\nspecifically to the performance of large databases. LANs provide locally\n\nstored data with network reliability and latency levels that are superior to\n\nthose of WANs.\n\nCloud Storage Levels\n\nCloud storage device mechanisms provide common logical units of data\n\nstorage, such as:\n\nFiles – Collections of data are grouped into files that are located in folders.\n\nBlocks – The lowest level of storage and the closest to the hardware, a block\n\nis the smallest unit of data that is still individually accessible.\n\nDatasets – Sets of data are organized into a table-based, delimited, or record\n\nformat.\n\nObjects – Data and its associated metadata are organized as Web-based\n\nresources.\n\nEach of these data storage levels is commonly associated with a certain type\n\nof technical interface which corresponds to a particular type of cloud\n\nstorage device and cloud storage service used to expose its API (Figure\n\n8.12).",
      "page_number": 366
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 385-409)",
      "start_page": 385,
      "end_page": 409,
      "detection_method": "synthetic",
      "content": "Figure 8.12\n\nDifferent cloud service consumers utilize different technologies to interface\n\nwith virtualized cloud storage devices. (Adapted from the CDMI Cloud\n\nStorage Reference Model.)\n\nNetwork Storage Interfaces\n\nLegacy network storage most commonly falls under the category of\n\nnetwork storage interfaces. It includes storage devices in compliance with\n\nindustry standard protocols, such as SCSI for storage blocks and the server\n\nmessage block (SMB), common Internet file system (CIFS), and network\n\nfile system (NFS) for file and network storage. File storage entails storing\n\nindividual data in separate files that can be different sizes and formats and\n\norganized into folders and subfolders. Original files are often replaced by\n\nthe new files that are created when data has been modified.\n\nWhen a cloud storage device mechanism is based on this type of interface,\n\nits data searching and extraction performance will tend to be suboptimal.\n\nStorage processing levels and thresholds for file allocation are usually\n\ndetermined by the file system itself. Block storage requires data to be in a\n\nfixed format (known as a data block), which is the smallest unit that can be\n\nstored and accessed and the storage format closest to hardware. Using either\n\nthe logical unit number (LUN) or virtual volume block-level storage will\n\ntypically have better performance than file-level storage.\n\nObject Storage Interfaces\n\nVarious types of data can be referenced and stored as Web resources. This is\n\nreferred to as object storage, which is based on technologies that can\n\nsupport a range of data and media types. Cloud Storage Device mechanisms\n\nthat implement this interface can typically be accessed via REST or Web\n\nservice-based cloud services using HTTP as the prime protocol. The\n\nStorage Networking Industry Association’s Cloud Data Management\n\nInterface (SNIA’s CDMI) supports the use of object storage interfaces.\n\nDatabase Storage Interfaces\n\nCloud storage device mechanisms based on database storage interfaces\n\ntypically support a query language in addition to basic storage operations.\n\nStorage management is carried out using a standard API or an\n\nadministrative user-interface.\n\nThis classification of storage interface is divided into two main categories\n\naccording to storage structure, as follows.\n\nRelational Data Storage\n\nTraditionally, many on-premise IT environments store data using relational\n\ndatabases or relational database management systems (RDBMSs).\n\nRelational databases (or relational storage devices) rely on tables to\n\norganize similar data into rows and columns. Tables can have relationships\n\nwith each other to give the data increased structure, to protect data integrity,\n\nand to avoid data redundancy (which is referred to as data normalization).\n\nWorking with relational storage commonly involves the use of the industry\n\nstandard Structured Query Language (SQL).\n\nA cloud storage device mechanism implemented using relational data\n\nstorage could be based on any number of commercially available database\n\nproducts, such as IBM DB2, Oracle Database, Microsoft SQL Server, and\n\nMySQL.\n\nChallenges with cloud-based relational databases commonly pertain to\n\nscaling and performance. Scaling a relational cloud storage device\n\nvertically can be more complex and cost-ineffective than horizontal scaling.\n\nDatabases with complex relationships and/or containing large volumes of\n\ndata can be afflicted with higher processing overhead and latency,\n\nespecially when accessed remotely via cloud services.\n\nNon-Relational Data Storage\n\nNon-relational storage (also commonly referred to as NoSQL storage)\n\nmoves away from the traditional relational database model in that it\n\nestablishes a “looser” structure for stored data with less emphasis on\n\ndefining relationships and realizing data normalization. The primary\n\nmotivation for using non-relational storage is to avoid the potential\n\ncomplexity and processing overhead that can be imposed by relational\n\ndatabases. Also, non-relational storage can be more horizontally scalable\n\nthan relational storage.\n\nThe trade-off with non-relational storage is that the data loses much of the\n\nnative form and validation due to limited or primitive schemas or data\n\nmodels. Furthermore, non-relational repositories don’t tend to support\n\nrelational database functions, such as transactions or joins.\n\nNormalized data exported into a non-relational storage repository will\n\nusually become denormalized, meaning that the size of the data will\n\ntypically grow. An extent of normalization can be preserved, but usually not\n\nfor complex relationships. Cloud providers often offer non-relational\n\nstorage that provides scalability and availability of stored data over multiple\n\nserver environments. However, many non-relational storage mechanisms\n\nare proprietary and therefore can severely limit data portability.\n\nCase Study Example\n\nDTGOV provides cloud consumers access to a cloud\n\nstorage device based on an object storage interface. The\n\ncloud service that exposes this API offers basic functions\n\non stored objects, such as search, create, delete, and\n\nupdate. The search function uses a hierarchical object\n\narrangement that resembles a file system. DTGOV\n\nfurther offers a cloud service that is used exclusively\n\nwith virtual servers and enables the creation of cloud\n\nstorage devices via a block storage network interface.\n\nBoth cloud services use APIs that are compliant with\n\nSNIA’s CDMI v1.0.\n\nThe object-based cloud storage device has an underlying\n\nstorage system with variable storage capacity, which is\n\ndirectly controlled by a software component that also\n\nexposes the interface. This software enables the creation\n\nof isolated cloud storage devices that are allocated to\n\ncloud consumers. The storage system uses a security\n\ncredential management system to administer user-based\n\naccess control to the device’s data objects (Figure 8.13).\n\nFigure 8.13\n\nThe cloud consumer interacts with the usage and\n\nadministration portal to create a cloud storage device\n\nand define access control policies (1). The usage and\n\nadministration portal interact with the cloud storage\n\nsoftware to create the cloud storage device instance and\n\napply the required access policy to its data objects (2).\n\nEach data object is assigned to a cloud storage device\n\nand all of the data objects are stored in the same virtual\n\nstorage volume. The cloud consumer uses the\n\nproprietary cloud storage device UI to interact directly\n\nwith the data objects (3). (Note that the usage and\n\nadministration portal is explained in Chapter 12.)\n\nAccess control is granted on a per-object basis and uses\n\nseparate access policies for creating, reading from, and\n\nwriting to each data object. Public access permissions\n\nare allowed, although they are read-only. Access groups\n\nare formed by nominated users that must be previously\n\nregistered via the credential management system. Data\n\nobjects can be accessed from both Web applications and\n\nWeb service interfaces, which are implemented by the\n\ncloud storage software.\n\nThe creation of the cloud consumers’ block-based cloud\n\nstorage devices is managed by the virtualization\n\nplatform, which instantiates the LUN’s implementation\n\nof the virtual storage (Figure 8.14). The cloud storage\n\ndevice (or the LUN) must be assigned by the VIM to an\n\nexisting virtual server before it can be used. The capacity\n\nof block-based cloud storage devices is expressed by one\n\nGB increments. It can be created as fixed storage that\n\ncloud consumers can modify administratively or as\n\nvariable size storage that has an initial 5 GB capacity\n\nthat automatically increases and decreases by 5 GB\n\nincrements according to usage demands.\n\nFigure 8.14\n\nThe cloud consumer uses the usage and administration\n\nportal to create and assign a cloud storage device to an\n\nexisting virtual server (1). The usage and administration\n\nportal interacts with the VIM software (2a), which\n\ncreates and configures the appropriate LUN (2b). Each\n\ncloud storage device uses a separate LUN controlled by\n\nthe virtualization platform. The cloud consumer remotely\n\nlogs into the virtual server directly (3a) to access the\n\ncloud storage device (3b).\n\n8.5 Cloud Usage Monitor\n\nThe cloud usage monitor mechanism is a lightweight and autonomous\n\nsoftware program responsible for collecting and processing IT resource\n\nusage data.\n\nNote\n\nThis is a parent mechanism that represents a broad range of\n\ncloud usage monitors, several of which are established as\n\nspecialized mechanisms in Chapter 9 and several more of\n\nwhich are described in the cloud architectural models\n\ncovered in Part III of this book.\n\nDepending on the type of usage metrics they are designed to collect and the\n\nmanner in which usage data needs to be collected, cloud usage monitors can\n\nexist in different formats. The upcoming sections describe three common\n\nagent-based implementation formats. Each can be designed to forward\n\ncollected usage data to a log database for post-processing and reporting\n\npurposes.\n\nMonitoring Agent\n\nA monitoring agent is an intermediary, event-driven program that exists as a\n\nservice agent and resides along existing communication paths to\n\ntransparently monitor and analyze dataflows (Figure 8.15). This type of\n\ncloud usage monitor is commonly used to measure network traffic and\n\nmessage metrics.\n\nFigure 8.15\n\nA cloud service consumer sends a request message to a cloud service (1).\n\nThe monitoring agent intercepts the message to collect relevant usage data\n\n(2) before allowing it to continue to the cloud service (3a). The monitoring\n\nagent stores the collected usage data in a log database (3b). The cloud\n\nservice replies with a response message (4) that is sent back to the cloud\n\nservice consumer without being intercepted by the monitoring agent (5).\n\nResource Agent\n\nA resource agent is a processing module that collects usage data by having\n\nevent-driven interactions with specialized resource software (Figure 8.16).\n\nThis module is used to monitor usage metrics based on pre-defined,\n\nobservable events at the resource software level, such as initiating,\n\nsuspending, resuming, and vertical scaling.\n\nFigure 8.16\n\nThe resource agent is actively monitoring a virtual server and detects an\n\nincrease in usage (1). The resource agent receives a notification from the\n\nunderlying resource management program that the virtual server is being\n\nscaled up and stores the collected usage data in a log database, as per its\n\nmonitoring metrics (2).\n\nPolling Agent\n\nA polling agent is a processing module that collects cloud service usage\n\ndata by polling IT resources. This type of cloud service monitor is\n\ncommonly used to periodically monitor IT resource status, such as uptime\n\nand downtime (Figure 8.17).\n\nFigure 8.17\n\nA polling agent monitors the status of a cloud service hosted by a virtual\n\nserver by sending periodic polling request messages and receiving polling\n\nresponse messages that report usage status “A” after a number of polling\n\ncycles, until it receives a usage status of “B” (1), upon which the polling\n\nagent records the new usage status in the log database (2).\n\nCase Study Example\n\nOne of the challenges encountered during DTGOV’s\n\ncloud adoption initiative has been ensuring that their\n\ncollected usage data is accurate. The resource allocation\n\nmethods of previous IT outsourcing models had resulted\n\nin their clients being billed chargeback fees based on the\n\nnumber of physical servers that was listed in annual\n\nleasing contracts, regardless of actual usage.\n\nDTGOV now needs to define a model that allows virtual\n\nservers of varying performance levels to be leased and\n\nbilled hourly. Usage data needs to be at an extremely\n\ngranular level in order to achieve the necessary degree of\n\naccuracy. DTGOV implements a resource agent that\n\nrelies on the resource usage events generated by the VIM\n\nplatform to calculate the virtual server usage data.\n\nThe resource agent is designed with logic and metrics\n\nthat are based on the following rules:\n\n. Each resource usage event that is generated by the VIM\n\nsoftware can contain the following data:\n\nEvent Type (EV_TYPE) – Generated by the VIM\n\nplatform, there are five types of events:\n\nVM Starting (creation at the hypervisor)\n\nVM Started (completion of the boot procedure)\n\nVM Stopping (shutting down)\n\nVM Stopped (termination at the hypervisor)\n\nVM Scaled (change of performance parameters)\n\nVM Type (VM_TYPE) – This represents a type of\n\nvirtual server, as dictated by its performance parameters.\n\nA predefined list of possible virtual server configurations\n\nprovides the parameters that are described by the\n\nmetadata whenever a VM starts or scales.\n\nUnique VM Identifier (VM_ID) – This identifier is\n\nprovided by the VIM platform.\n\nUnique Cloud Consumer Identifier (CS_ID) – Another\n\nidentifier provided by the VIM platform to represent the\n\ncloud consumer.\n\nEvent Timestamp (EV_T) – An identification of an event\n\noccurrence that is expressed in date-time format, with\n\nthe time zone of the data center and referenced to UTC\n\nas defined in RFC 3339 (as per the ISO 8601 profile).\n\n. Usage measurements are recorded for every virtual\n\nserver that a cloud consumer creates.\n\n. Usage measurements are recorded for a measurement\n\nperiod whose length is defined by two timestamps called\n\nt\n\nstart\n\nand t\n\nend\n\n. The start of the measurement period\n\ndefaults to the beginning of the calendar month (t\n\nstart\n\n=\n\n2012-12-01T00:00:00-08:00) and finishes at the end of\n\nthe calendar month (t\n\nend\n\n= 2012-12-31T23:59:59-08:00).\n\nCustomized measurement periods are also supported.\n\n. Usage measurements are recorded at each minute of\n\nusage. The virtual server usage measurement period\n\nstarts when the virtual server is created at the hypervisor\n\nand stops at its termination.\n\n. Virtual servers can be started, scaled, and stopped\n\nmultiple times during the measurement period. The time\n\ninterval between each occurrence i (i = 1, 2, 3, …) of\n\nthese pairs of successive events that are declared for a\n\nvirtual server is called a usage cycle that is known as\n\nT\n\ncycle_i\n\n:\n\nVM_Starting, VM_Stopping – VM size is unchanged at\n\nthe end of the cycle\n\nVM_Starting, VM_Scaled – VM size has changed at the\n\nend of the cycle\n\nVM_Scaled, VM_Scaled – VM size has changed while\n\nscaling, at the end of the cycle\n\nVM_Scaled, VM_Stopping – VM size has changed at\n\nthe end of the cycle\n\n. The total usage, U\n\ntotal\n\n, for each virtual server during the\n\nmeasurement period is calculated using the following\n\nresource usage event log database equations:\n\nFor each VM_TYPE and VM_ID in the log database:\n\nAs per the total usage time that is measured for each\n\nVM_TYPE, the vector of usage for each VM_ID is\n\nU\n\ntotal\n\n: U\n\ntotal\n\n= {type 1, U\n\ntotal_VM_type_1\n\n, type 2,\n\nU\n\ntotal_VM_type_2\n\n, …}\n\nFigure 8.18 depicts the resource agent interacting with\n\nthe VIM’s event-driven API.\n\nFigure 8.18\n\nThe cloud consumer (CS_ID = CS1) requests the\n\ncreation of a virtual server (VM_ID = VM1) of\n\nconfiguration size type 1 (VM_TYPE = type1) (1). The\n\nVIM creates the virtual server (2a). The VIM’s event-\n\ndriven API generates a resource usage event with\n\ntimestamp = t1, which the cloud usage monitor software\n\nagent captures and records in the resource usage event\n\nlog database (2b). Virtual server usage increases and\n\nreaches the auto-scaling threshold (3). The VIM scales\n\nup Virtual Server VM1 (4a) from configuration type 1 to\n\ntype 2 (VM_TYPE = type2). The VIM’s event-driven API\n\ngenerates a resource usage event with timestamp = t2,\n\nwhich is captured and recorded at the resource usage\n\nevent log database by the cloud usage monitor software\n\nagent (4b). The cloud consumer shuts down the virtual\n\nserver (5). The VIM stops Virtual Server VM1 (6a) and\n\nits event-driven API generates a resource usage event\n\nwith timestamp = t3, which the cloud usage monitor\n\nsoftware agent captures and records at the log database\n\n(6b). The usage and administration portal accesses the\n\nlog database and calculates the total usage (Utotal) for\n\nVirtual Server Utotal VM1 (7).\n\n8.6 Resource Replication\n\nDefined as the creation of multiple instances of the same IT resource,\n\nreplication is typically performed when an IT resource’s availability and\n\nperformance need to be enhanced. Virtualization technology is used to\n\nimplement the resource replication mechanism to replicate cloud-based IT\n\nresources (Figure 8.19).\n\nFigure 8.19\n\nThe hypervisor replicates several instances of a virtual server, using a\n\nstored virtual server image.\n\nNote\n\nThis is a parent mechanism that represents different types of\n\nsoftware programs capable of replicating IT resources. The\n\nmost common example is the hypervisor mechanism\n\ndescribed in this chapter. For example, the virtualization\n\nplatform’s hypervisor can access a virtual server image to\n\ncreate several instances, or to deploy and replicate ready-\n\nmade environments and entire applications. Other common\n\ntypes of replicated IT resources include cloud service\n\nimplementations and various forms of data and cloud\n\nstorage device replication.\n\nCase Study Example\n\nDTGOV establishes a set of high-availability virtual\n\nservers that can be automatically relocated to physical\n\nservers running in different data centers in response to\n\nsevere failure conditions. This is illustrated in the\n\nscenario depicted in Figures 8.20 to 8.22, where a virtual\n\nserver that resides on a physical server running at one\n\ndata center experiences a failure condition. VIMs from\n\ndifferent data centers coordinate to overcome the\n\nunavailability by reallocating the virtual server to a\n\ndifferent physical server running in another data center.\n\nFigure 8.20\n\nA high-availability virtual server is running in Data\n\nCenter A. VIM instances in Data Centers A and B are\n\nexecuting a coordination function that allows detection\n\nof failure conditions. Stored VM images are replicated\n\nbetween data centers as a result of the high-availability\n\narchitecture.\n\nFigure 8.21\n\nThe virtual server becomes unavailable in Data Center\n\nA. The VIM in Data Center B detects the failure\n\ncondition and starts to reallocate the high-availability\n\nserver from Data Center A to Data Center B.\n\nFigure 8.22\n\nA new instance of the virtual server is created and made\n\navailable in Data Center B.\n\n8.7 Ready-Made Environment\n\nThe ready-made environment mechanism (Figure 8.23) is a defining\n\ncomponent of the PaaS cloud delivery model that represents a pre-defined,\n\ncloud-based platform comprised of a set of already installed IT resources,\n\nready to be used and customized by a cloud consumer. These environments\n\nare utilized by cloud consumers to remotely develop and deploy their own\n\nservices and applications within a cloud. Typical ready-made environments\n\ninclude pre-installed IT resources, such as databases, middleware,\n\ndevelopment tools, and governance tools.\n\nFigure 8.23\n\nA cloud consumer accesses a ready-made environment hosted on a virtual\n\nserver.\n\nA ready-made environment is generally equipped with a complete software\n\ndevelopment kit (SDK) that provides cloud consumers with programmatic\n\naccess to the development technologies that comprise their preferred\n\nprogramming stacks.\n\nMiddleware is available for multitenant platforms to support the\n\ndevelopment and deployment of Web applications. Some cloud providers",
      "page_number": 385
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 411-431)",
      "start_page": 411,
      "end_page": 431,
      "detection_method": "synthetic",
      "content": "back-end part contains the logic required to render the\n\ncomplete catalog and correlate similar components and\n\nlegacy part numbers.\n\nFigure 8.24 illustrates the development and deployment\n\nenvironment for ATN’s Part Number Catalog\n\napplication. Note how the cloud consumer assumes both\n\nthe developer and end-user roles.\n\nFigure 8.24\n\nThe developer uses the provided SDK to develop the\n\nPart Number Catalog Web application (1). The\n\napplication software is deployed on a Web platform that\n\nwas established by two ready-made environments called\n\nthe front-end instance (2a) and the back-end instance\n\n(2b). The application is made available for usage and\n\none end-user accesses its front-end instance (3). The\n\nsoftware running in the front-end instance invokes a\n\nlong-running task at the back-end instance that\n\ncorresponds to the processing required by the end-user\n\n(4). The application software deployed at both the front-\n\nend and back-end instances is backed by a cloud storage\n\ndevice that provides persistent storage of the application\n\ndata (5).\n\n8.8 Container\n\nContainers can provide an effective means of deploying and delivering\n\ncloud services. A container is represented by a symbol that similar to the\n\norganizational boundary symbol introduced in Roles and Boundaries\n\nsection in Chapter 4, except that it has rounded corners instead of sharp\n\ncorners Containerization technology is explained in Chapter 6.\n\nThe symbol used to represent a container.\n\nChapter 9\n\nSpecialized Cloud Mechanisms\n\n9.1 Automated Scaling Listener\n\n9.2 Load Balancer\n\n9.3 SLA Monitor\n\n9.4 Pay-Per-Use Monitor\n\n9.5 Audit Monitor\n\n9.6 Failover System\n\n9.7 Resource Cluster\n\n9.8 Multi-Device Broker\n\n9.9 State Management Database\n\nA typical cloud technology architecture contains numerous moving parts to\n\naddress distinct usage requirements of IT resources and solutions. Each\n\nmechanism covered in this chapter fulfills a specific runtime function in\n\nsupport of one or more cloud characteristics.\n\nThe following specialized cloud mechanisms are described in this chapter:\n\nAutomated Scaling Listener\n\nLoad Balancer\n\nSLA Monitor\n\nPay-Per-Use Monitor\n\nAudit Monitor\n\nFailover System\n\nResource Cluster\n\nMulti-Device Broker\n\nState Management Database\n\nAll of these mechanisms can be considered extensions to cloud\n\ninfrastructure, and can be combined in numerous ways as part of distinct\n\nand custom technology architectures, many examples of which are provided\n\nin Part III of this book.\n\n9.1 Automated Scaling Listener\n\nThe automated scaling listener mechanism is a service agent that monitors\n\nand tracks communications between cloud service consumers and cloud\n\nservices for dynamic scaling purposes. Automated scaling listeners are\n\ndeployed within the cloud, typically near the firewall, from where they\n\nautomatically track workload status information. Workloads can be\n\ndetermined by the volume of cloud consumer-generated requests or via\n\nback-end processing demands triggered by certain types of requests. For\n\nexample, a small amount of incoming data can result in a large amount of\n\nprocessing.\n\nAutomated scaling listeners can provide different types of responses to\n\nworkload fluctuation conditions, such as:\n\nAutomatically scaling IT resources out or in based on parameters previously\n\ndefined by the cloud consumer (commonly referred to as auto-scaling).\n\nAutomatic notification of the cloud consumer when workloads exceed\n\ncurrent thresholds or fall below allocated resources (Figure 9.1). This way,\n\nthe cloud consumer can choose to adjust its current IT resource allocation.\n\nDifferent cloud provider vendors have different names for service agents\n\nthat act as automated scaling listeners.\n\nFigure 9.1\n\nThree cloud service consumers attempt to access one cloud service\n\nsimultaneously (1). The automated scaling listener scales out and initiates\n\nthe creation of three redundant instances of the service (2). A fourth cloud\n\nservice consumer attempts to use the cloud service (3). Programmed to\n\nallow up to only three instances of the cloud service, the automated scaling\n\nlistener rejects the fourth attempt and notifies the cloud consumer that the\n\nrequested workload limit has been exceeded (4). The cloud consumer’s\n\ncloud resource administrator accesses the remote administration\n\nenvironment to adjust the provisioning setup and increase the redundant\n\ninstance limit (5).\n\nCase Study Example\n\nNote\n\nThis case study example makes reference to the live VM\n\nmigration component, which is introduced in the\n\nHypervisor Clustering Architecture section in Chapter\n\n14, and further described and demonstrated in\n\nsubsequent architecture scenarios.\n\nDTGOV’s physical servers vertically scale virtual server\n\ninstances, starting with the smallest virtual machine\n\nconfiguration (1 virtual processor core, 4 GB of virtual\n\nRAM) to the largest (128 virtual processor cores, 512\n\nGB of virtual RAM). The virtualization platform is\n\nconfigured to automatically scale a virtual server at\n\nruntime, as follows:\n\nScaling-Down – The virtual server continues residing on\n\nthe same physical host server while being scaled down\n\nto a lower performance configuration.\n\nScaling-Up – The virtual server’s capacity is doubled on\n\nits original physical host server. The VIM may also live\n\nmigrate the virtual server to another physical server if\n\nthe original host server is overcommitted. Migration is\n\nautomatically performed at runtime and does not require\n\nthe virtual server to shut down.\n\nAuto-scaling settings controlled by cloud consumers\n\ndetermine the runtime behavior of automated scaling\n\nlistener agents, which run on the hypervisor that\n\nmonitors the resource usage of the virtual servers. For\n\nexample, one cloud consumer has it set up so that\n\nwhenever resource usage exceeds 80% of a virtual\n\nserver’s capacity for 60 consecutive seconds, the\n\nautomated scaling listener triggers the scaling-up process\n\nby sending the VIM platform a scale-up command.\n\nConversely, the automated scaling listener also\n\ncommands the VIM to scale down whenever resource\n\nusage dips 15% below capacity for 60 consecutive\n\nseconds (Figure 9.2).\n\nFigure 9.2\n\nA cloud consumer creates and starts a virtual server\n\nwith 8 virtual processor cores and 16 GB of virtual RAM\n\n(1). The VIM creates the virtual server at the cloud\n\nservice consumer’s request and allocates it to Physical\n\nServer 1 to join 3 other active virtual servers (2). Cloud\n\nconsumer demand causes the virtual server usage to\n\nincrease by over 80% of the CPU capacity for 60\n\nconsecutive seconds (3). The automated scaling listener\n\nrunning at the hypervisor detects the need to scale up\n\nand commands the VIM accordingly (4).\n\nFigure 9.3 illustrates the live migration of a virtual\n\nmachine, as performed by the VIM.\n\nFigure 9.3\n\nThe VIM determines that scaling up the virtual server on\n\nPhysical Server 1 is not possible and proceeds to live\n\nmigrate it to Physical Server 2.\n\nThe scaling down of the virtual server by the VIM is\n\ndepicted in Figure 9.4.\n\nFigure 9.4\n\nThe virtual server’s CPU/RAM usage remains below\n\n15% capacity for 60 consecutive seconds (6). The\n\nautomated scaling listener detects the need to scale\n\ndown and commands the VIM (7), which scales down the\n\nvirtual server (8) while it remains active on Physical\n\nServer 2.\n\n9.2 Load Balancer\n\nA common approach to horizontal scaling is to balance a workload across\n\ntwo or more IT resources to increase performance and capacity beyond\n\nwhat a single IT resource can provide. The load balancer mechanism is a\n\nruntime agent with logic fundamentally based on this premise.\n\nBeyond simple division of labor algorithms (Figure 9.5), load balancers can\n\nperform a range of specialized runtime workload distribution functions that\n\ninclude:\n\nAsymmetric Distribution – larger workloads are issued to IT resources with\n\nhigher processing capacities\n\nWorkload Prioritization – workloads are scheduled, queued, discarded, and\n\ndistributed workloads according to their priority levels\n\nContent-Aware Distribution – requests are distributed to different IT\n\nresources as dictated by the request content\n\nFigure 9.5\n\nA load balancer implemented as a service agent transparently distributes\n\nincoming workload request messages across two redundant cloud service\n\nimplementations, which in turn maximizes performance for the cloud\n\nservice consumers.\n\nA load balancer is programmed or configured with a set of performance and\n\nQoS rules and parameters with the general objectives of optimizing IT\n\nresource usage, avoiding overloads, and maximizing throughput.\n\nThe load balancer mechanisms can exist as a:\n\nmulti-layer network switch\n\ndedicated hardware appliance\n\ndedicated software-based system (common in server operating systems)\n\nservice agent (usually controlled by cloud management software)\n\nThe load balancer is typically located on the communication path between\n\nthe IT resources generating the workload and the IT resources performing\n\nthe workload processing. This mechanism can be designed as a transparent\n\nagent that remains hidden from the cloud service consumers, or as a proxy\n\ncomponent that abstracts the IT resources performing their workload.\n\nCase Study Example\n\nThe ATN Part Number Catalog cloud service does not\n\nmanipulate transaction data even though it is used by\n\nmultiple factories in different regions. It has peak usage\n\nperiods during the first few days of every month that\n\ncoincide with the preparatory processing of heavy stock\n\ncontrol routines at the factories. ATN followed their\n\ncloud provider’s recommendations and upgraded the\n\ncloud service to be highly scalable in order to support\n\nthe anticipated workload fluctuations.\n\nAfter developing the necessary upgrades, ATN decides\n\nto test the scalability by using a robot automation testing\n\ntool that simulates heavy workloads. The tests need to\n\ndetermine whether the application can seamlessly scale\n\nto serve peak workloads that are 1,000 times greater than\n\ntheir average workloads. The robots proceed to simulate\n\nworkloads that last 10 minutes.\n\nThe application’s resulting auto-scaling functionality is\n\ndemonstrated in Figure 9.6.\n\nFigure 9.6\n\nNew instances of the cloud services are automatically\n\ncreated to meet increasing usage requests. The load\n\nbalancer uses round-robin scheduling to ensure that the\n\ntraffic is distributed evenly among the active cloud\n\nservices.\n\n9.3 SLA Monitor\n\nThe SLA monitor mechanism is used to specifically observe the runtime\n\nperformance of cloud services to ensure that they are fulfilling the\n\ncontractual QoS requirements that are published in SLAs (Figure 9.7). The\n\ndata collected by the SLA monitor is processed by an SLA management\n\nsystem to be aggregated into SLA reporting metrics. The system can\n\nproactively repair or failover cloud services when exception conditions\n\noccur, such as when the SLA monitor reports a cloud service as “down.”\n\nFigure 9.7\n\nThe SLA monitor polls the cloud service by sending over polling request\n\nmessages (M\n\nREQ1\n\nto M\n\nREQN\n\n). The monitor receives polling response\n\nmessages (M\n\nREP1\n\nto M\n\nREPN\n\n) that report that the service was “up” at each\n\npolling cycle (1a). The SLA monitor stores the “up” time—time period of\n\nall polling cycles 1 to N—in the log database (1b).\n\nThe SLA monitor polls the cloud service that sends polling request\n\nmessages (M\n\nREQN+1\n\nto M\n\nREQN+M\n\n). Polling response messages are not\n\nreceived (2a). The response messages continue to time out, so the SLA\n\nmonitor stores the “down” time—time period of all polling cycles N+1 to\n\nN+M—in the log database (2b).\n\nThe SLA monitor sends a polling request message (M\n\nREQN+M+1\n\n) and\n\nreceives the polling response message (M\n\nREPN+M+1\n\n) (3a). The SLA monitor\n\nstores the “up” time in the log database (3b).\n\nThe SLA management system mechanism is discussed in Chapter 12.\n\nCase Study Example\n\nThe standard SLA for virtual servers in DTGOV’s\n\nleasing agreements defines a minimum IT resource\n\navailability of 99.95%, which is tracked using two SLA\n\nmonitors: one based on a polling agent and the other\n\nbased on a regular monitoring agent implementation.\n\nSLA Monitor Polling Agent\n\nDTGOV’s polling SLA monitor runs in the external\n\nperimeter network to detect physical server timeouts. It\n\nis able to identify data center network, hardware, and\n\nsoftware failures (with minute-granularity) that result in\n\nphysical server non-responsiveness. Three consecutive\n\ntimeouts of 20-second polling periods are required to\n\ndeclare IT resource unavailability.\n\nThree types of events are generated:\n\nPS_Timeout – the physical server polling has timed out\n\nPS_Unreachable – the physical server polling has\n\nconsecutively timed out three times\n\nPS_Reachable – the previously unavailable physical\n\nserver becomes responsive to polling again\n\nSLA Monitoring Agent\n\nThe VIM’s event-driven API implements the SLA\n\nmonitor as a monitoring agent to generate the following\n\nthree events:\n\nVM_Unreachable – the VIM cannot reach the VM\n\nVM Failure – the VM has failed and is unavailable\n\nVM_Reachable – the VM is reachable\n\nThe events generated by the polling agent have\n\ntimestamps that are logged into an SLA event log\n\ndatabase and used by the SLA management system to\n\ncalculate IT resource availability. Complex rules are\n\nused to correlate events from different polling SLA\n\nmonitors and the affected virtual servers, and to discard\n\nany false positives for periods of unavailability.\n\nFigures 9.8 and 9.9 show the steps taken by SLA\n\nmonitors during a data center network failure and\n\nrecovery.",
      "page_number": 411
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 432-456)",
      "start_page": 432,
      "end_page": 456,
      "detection_method": "synthetic",
      "content": "Figure 9.8\n\nAt timestamp = t1, a firewall cluster has failed and all of\n\nthe IT resources in the data center become unavailable\n\n(1). The SLA monitor polling agent stops receiving\n\nresponses from physical servers and starts to issue\n\nPS_timeout events (2). The SLA monitor polling agent\n\nstarts issuing PS_unreachable events after three\n\nsuccessive PS_timeout events. The timestamp is now t2\n\n(3).\n\nFigure 9.9\n\nThe IT resource becomes operational at timestamp = t3\n\n(4). The SLA monitor polling agent receives responses\n\nfrom the physical servers and issues PS_reachable\n\nevents. The timestamp is now t4 (5). The SLA monitoring\n\nagent did not detect any unavailability since the\n\ncommunication between the VIM platform and physical\n\nservers was not affected by the failure (6).\n\nThe SLA management system uses the information\n\nstored in the log database to calculate the period of\n\nunavailability as t4 – t3, which affected all of the virtual\n\nservers in the data center.\n\nFigures 9.10 and 9.11 illustrate the steps that are taken\n\nby the SLA monitors during the failure and subsequent\n\nrecovery of a physical server that is hosting three virtual\n\nservers (VM1, VM2, VM3).\n\nFigure 9.10\n\nAt timestamp = t1, the physical host server has failed\n\nand becomes unavailable (1). The SLA monitoring agent\n\ncaptures a VM_unreachable event that is generated for\n\neach virtual server in the failed host server (2a). The\n\nSLA monitor polling agent stops receiving responses\n\nfrom the host server and issues PS_timeout events (2b).\n\nAt timestamp = t2, the SLA monitoring agent captures a\n\nVM_failure event that is generated for each of the failed\n\nhost server’s three virtual servers (3a). The SLA monitor\n\npolling agent starts to issue PS_unavailable events after\n\nthree successive PS_timeout events at timestamp = t3\n\n(3b).\n\nFigure 9.11\n\nThe host server becomes operational at timestamp = t4\n\n(4). The SLA monitor polling agent receives responses\n\nfrom the physical server and issues PS_reachable events\n\nat timestamp = t5 (5a). At timestamp = t6, the SLA\n\nmonitoring agent captures a VM_reachable event that is\n\ngenerated for each virtual server (5b). The SLA\n\nmanagement system calculates the unavailability period\n\nthat affected all of the virtual servers as t6 – t2.\n\n9.4 Pay-Per-Use Monitor\n\nThe pay-per-use monitor mechanism measures cloud-based IT resource\n\nusage in accordance with predefined pricing parameters and generates\n\nusage logs for fee calculations and billing purposes.\n\nSome typical monitoring variables are:\n\nrequest/response message quantity\n\ntransmitted data volume\n\nbandwidth consumption\n\nThe data collected by the pay-per-use monitor is processed by a billing\n\nmanagement system that calculates the payment fees. The billing\n\nmanagement system mechanism is covered in Chapter 12.\n\nFigure 9.12 shows a pay-per-use monitor implemented as a resource agent\n\nused to determine the usage period of a virtual server.\n\nFigure 9.12\n\nA cloud consumer requests the creation of a new instance of a cloud service\n\n(1). The IT resource is instantiated and the pay-per-use monitor receives a\n\n“start” event notification from the resource software (2). The pay-per-use\n\nmonitor stores the value timestamp in the log database (3). The cloud\n\nconsumer later requests that the cloud service instance be stopped (4). The\n\npay-per-use monitor receives a “stop” event notification from the resource\n\nsoftware (5) and stores the value timestamp in the log database (6).\n\nFigure 9.13 illustrates a pay-per-use monitor designed as a monitoring agent\n\nthat transparently intercepts and analyzes runtime communication with a\n\ncloud service.\n\nFigure 9.13\n\nA cloud service consumer sends a request message to the cloud service (1).\n\nThe pay-per-use monitor intercepts the message (2), forwards it to the cloud\n\nservice (3a), and stores the usage information in accordance with its\n\nmonitoring metrics (3b). The cloud service forwards the response messages\n\nback to the cloud service consumer to provide the requested service (4).\n\nCase Study Example\n\nDTGOV decides to invest in a commercial system\n\ncapable of generating invoices based on events pre-\n\ndefined as “billable” and customizable pricing models.\n\nThe installation of the system results in two proprietary\n\ndatabases: the billing event database and the pricing\n\nscheme database.\n\nRuntime events are collected via cloud usage monitors\n\nthat are implemented as extensions to the VIM platform\n\nusing the VIM’s API. The pay-per-use monitor polling\n\nagent periodically supplies the billing system with\n\nbillable events information. A separate monitoring agent\n\nprovides further supplemental billing-related data, such\n\nas:\n\nCloud Consumer Subscription Type – This information is\n\nused to identify the type of pricing model for usage fee\n\ncalculations, including pre-paid subscription with usage\n\nquota, post-paid subscription with maximum usage\n\nquota, and post-paid subscription with unlimited usage.\n\nResource Usage Category – The billing management\n\nsystem uses this information to identify the range of\n\nusage fees that are applicable to each usage event.\n\nExamples include normal usage, reserved IT resource\n\nusage, and premium (managed) service usage.\n\nResource Usage Quota Consumption – When usage\n\ncontracts define IT resource usage quotas, usage event\n\nconditions are typically supplemented with quota\n\nconsumption and updated quota limits.\n\nFigure 9.14 illustrates the steps that are taken by\n\nDTGOV’s pay-per-use monitor during a typical usage\n\nevent.\n\nFigure 9.14\n\nThe cloud consumer (CS_ID = CS1) creates and starts a\n\nvirtual server (VM_ID = VM1) of configuration size type\n\n1 (VM_TYPE = type1) (1). The VIM creates the virtual\n\nserver instance as requested (2a). The VIM’s event-\n\ndriven API generates a resource usage event with\n\ntimestamp = t1, which is captured and forwarded to the\n\npay-per-use monitor by the cloud usage monitor (2b).\n\nThe pay-per-use monitor interacts with the pricing\n\nscheme database to identify the chargeback and usage\n\nmetrics that apply to the resource usage. A “started\n\nusage” billable event is generated and stored in the\n\nbillable event log database (3). The virtual server’s\n\nusage increases and reaches the auto-scaling threshold\n\n(4). The VIM scales up Virtual Server VM1 (5a) from\n\nconfiguration type 1 to type 2 (VM_TYPE = type2). The\n\nVIM’s event-driven API generates a resource usage\n\nevent with timestamp = t2, which is captured and\n\nforwarded to the pay-per-use monitor by the cloud usage\n\nmonitor (5b). The pay-per-use monitor interacts with the\n\npricing scheme database to identify the chargeback and\n\nusage metrics that apply to the updated IT resource\n\nusage. A “changed usage” billable event is generated\n\nand stored in the billable event log database (6). The\n\ncloud consumer shuts down the virtual server (7) and the\n\nVIM stops Virtual Server VM1 (8a). The VIM’s event-\n\ndriven API generates a resource usage event with\n\ntimestamp = t3, which is captured and forwarded to the\n\npay-per-use monitor by the cloud usage monitor (8b).\n\nThe pay-per-use monitor interacts with the pricing\n\nscheme database to identify the chargeback and usage\n\nmetrics that apply to the updated IT resource usage. A\n\n“finished usage” billable event is generated and stored\n\nin the billable event log database (9). The billing system\n\ntool can now be used by the cloud provider to access the\n\nlog database and calculate the total usage fee for the\n\nvirtual server as (Fee(VM1)) (10).\n\n9.5 Audit Monitor\n\nThe audit monitor mechanism is used to collect audit tracking data for\n\nnetworks and IT resources in support of (or dictated by) regulatory and\n\ncontractual obligations. Figure 9.15 depicts an audit monitor implemented\n\nas a monitoring agent that intercepts “login” requests and stores the\n\nrequestor’s security credentials, as well as both failed and successful login\n\nattempts, in a log database for future audit reporting purposes.\n\nFigure 9.15\n\nA cloud service consumer requests access to a cloud service by sending a\n\nlogin request message with security credentials (1). The audit monitor\n\nintercepts the message (2) and forwards it to the authentication service (3).\n\nThe authentication service processes the security credentials. A response\n\nmessage is generated for the cloud service consumer, in addition to the\n\nresults from the login attempt (4). The audit monitor intercepts the response\n\nmessage and stores the entire collected login event details in the log\n\ndatabase, as per the organization’s audit policy requirements (5). Access\n\nhas been granted, and a response is sent back to the cloud service consumer\n\n(6).\n\nCase Study Example\n\nA key feature of Innovartus’ role-playing solution is its\n\nunique user-interface. However, the advanced\n\ntechnologies used for its design have imposed licensing\n\nrestrictions that legally prevent Innovartus from charging\n\nusers in certain geographical regions for usage of the\n\nsolution. Innovartus’ legal department is working on\n\ngetting these issues resolved. But in the meantime, it has\n\nprovided the IT department with a list of countries in\n\nwhich the application can either not be accessed by users\n\nor in which user access needs to be free of charge.\n\nIn order to collect information about the origin of clients\n\naccessing the application, Innovartus asks its cloud\n\nprovider to establish an audit monitoring system. The\n\ncloud provider deploys an audit monitoring agent to\n\nintercept each inbound message, analyze its\n\ncorresponding HTTP header, and collect details about\n\nthe origin of the end-user. As per Innovartus’ request, the\n\ncloud provider further adds a log database to collect the\n\nregional data of each end-user request for future\n\nreporting purposes. Innovartus further upgrades its\n\napplication so that end-users from select countries are\n\nable to access the application at no charge (Figure 9.16).\n\nFigure 9.16\n\nAn end-user attempts access to the Role Player cloud\n\nservice (1). An audit monitor transparently intercepts the\n\nHTTP request message and analyzes the message header\n\nto determine the geographical origin of the end-user (2).\n\nThe audit monitoring agent determines that the end-user\n\nis from a region that Innovartus is not authorized to\n\ncharge a fee for access to the application. The agent\n\nforwards the message to the cloud service (3a) and\n\ngenerates the audit track information for storage in the\n\nlog database (3b). The cloud service receives the HTTP\n\nmessage and grants the end-user access at no charge\n\n(4).\n\n9.6 Failover System\n\nThe failover system mechanism is used to increase the reliability and\n\navailability of IT resources by using established clustering technology to\n\nprovide redundant implementations. A failover system is configured to\n\nautomatically switch over to a redundant or standby IT resource instance\n\nwhenever the currently active IT resource becomes unavailable.\n\nFailover systems are commonly used for mission-critical programs and\n\nreusable services that can introduce a single point of failure for multiple\n\napplications. A failover system can span more than one geographical region\n\nso that each location hosts one or more redundant implementations of the\n\nsame IT resource.\n\nThe resource replication mechanism is sometimes utilized by the failover\n\nsystem to provide redundant IT resource instances, which are actively\n\nmonitored for the detection of errors and unavailability conditions.\n\nFailover systems come in two basic configurations:\n\nActive-Active\n\nIn an active-active configuration, redundant implementations of the IT\n\nresource actively serve the workload synchronously (Figure 9.17). Load\n\nbalancing among active instances is required. When a failure is detected,\n\nthe failed instance is removed from the load balancing scheduler (Figure\n\n9.18). Whichever IT resource remains operational when a failure is detected\n\ntakes over the processing (Figure 9.19).\n\nFigure 9.17\n\nThe failover system monitors the operational status of Cloud Service A.\n\nFigure 9.18\n\nWhen a failure is detected in one Cloud Service A implementation, the\n\nfailover system commands the load balancer to switch over the workload to\n\nthe redundant Cloud Service A implementation.\n\nFigure 9.19\n\nThe failed Cloud Service A implementation is recovered or replicated into\n\nan operational cloud service. The failover system now commands the load\n\nbalancer to distribute the workload again.\n\nActive-Passive\n\nIn an active-passive configuration, a standby or inactive implementation is\n\nactivated to take over the processing from the IT resource that becomes\n\nunavailable, and the corresponding workload is redirected to the instance\n\ntaking over the operation (Figures 9.20 to 9.22).\n\nFigure 9.20\n\nThe failover system monitors the operational status of Cloud Service A. The\n\nCloud Service A implementation acting as the active instance is receiving\n\ncloud service consumer requests.\n\nFigure 9.21\n\nThe Cloud Service A implementation acting as the active instance\n\nencounters a failure that is detected by the failover system, which\n\nsubsequently activates the inactive Cloud Service A implementation and\n\nredirects the workload toward it. The newly invoked Cloud Service A\n\nimplementation now assumes the role of active instance.\n\nFigure 9.22\n\nThe failed Cloud Service A implementation is recovered or replicated an\n\noperational cloud service, and is now positioned as the standby instance,\n\nwhile the previously invoked Cloud Service A continues to serve as the\n\nactive instance.\n\nSome failover systems are designed to redirect workloads to active IT\n\nresources that rely on specialized load balancers that detect failure\n\nconditions and exclude failed IT resource instances from the workload\n\ndistribution. This type of failover system is suitable for IT resources that do\n\nnot require execution state management and provide stateless processing\n\ncapabilities. In technology architectures that are typically based on\n\nclustering and virtualization technologies, the redundant or standby IT\n\nresource implementations are also required to share their state and\n\nexecution context. A complex task that was executed on a failed IT resource\n\ncan remain operational in one of its redundant implementations.\n\nCase Study Example\n\nDTGOV creates a resilient virtual server to support the\n\nallocation of virtual server instances that are hosting\n\ncritical applications, which are being replicated in\n\nmultiple data centers. The replicated resilient virtual\n\nserver has an associated active-passive failover system.\n\nIts network traffic flow can be switched between the IT\n\nresource instances that are residing at different data\n\ncenters, if the active instance were to fail (Figure 9.23).\n\nFigure 9.23\n\nA resilient virtual server is established by replicating the\n\nvirtual server instance across two different data centers,\n\nas performed by the VIM that is running at both data\n\ncenters. The active instance receives the network traffic\n\nand is vertically scaling in response, while the standby\n\ninstance has no workload and runs at the minimum\n\nconfiguration.\n\nFigure 9.24 illustrates SLA monitors detecting failure in\n\nan active instance of a virtual server.\n\nFigure 9.24\n\nSLA monitors detect when the active virtual server\n\ninstance becomes unavailable.\n\nFigure 9.25 shows traffic being switched over to the\n\nstandby instance, which has now become active.",
      "page_number": 432
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 457-480)",
      "start_page": 457,
      "end_page": 480,
      "detection_method": "synthetic",
      "content": "Figure 9.25\n\nThe failover system is implemented as an event-driven\n\nsoftware agent that intercepts the message notifications\n\nthe SLA monitors send regarding server unavailability.\n\nIn response, the failover system interacts with the VIM\n\nand network management tools to redirect all of the\n\nnetwork traffic to the now-active standby instance.\n\nIn Figure 9.26, the failed virtual server becomes\n\noperational and turns into the standby instance.\n\nFigure 9.26\n\nThe failed virtual server instance is revived and scaled\n\ndown to the minimum standby instance configuration\n\nafter it resumes normal operation.\n\n9.7 Resource Cluster\n\nCloud-based IT resources that are geographically diverse can be logically\n\ncombined into groups to improve their allocation and use. The resource\n\ncluster mechanism (Figure 9.27) is used to group multiple IT resource\n\ninstances so that they can be operated as a single IT resource. This increases\n\nthe combined computing capacity, load balancing, and availability of the\n\nclustered IT resources.\n\nFigure 9.27\n\nThe curved dashed lines are used to indicate that IT resources are clustered.\n\nResource cluster architectures rely on high-speed dedicated network\n\nconnections, or cluster nodes, between IT resource instances to\n\ncommunicate about workload distribution, task scheduling, data sharing,\n\nand system synchronization. A cluster management platform that is running\n\nas distributed middleware in all of the cluster nodes is usually responsible\n\nfor these activities. This platform implements a coordination function that\n\nallows distributed IT resources to appear as one IT resource, and also\n\nexecutes IT resources inside the cluster.\n\nCommon resource cluster types include:\n\nServer Cluster – Physical or virtual servers are clustered to increase\n\nperformance and availability. Hypervisors running on different physical\n\nservers can be configured to share virtual server execution state (such as\n\nmemory pages and processor register state) in order to establish clustered\n\nvirtual servers. In such configurations, which usually require physical\n\nservers to have access to shared storage, virtual servers are able to live-\n\nmigrate from one to another. In this process, the virtualization platform\n\nsuspends the execution of a given virtual server at one physical server and\n\nresumes it on another physical server. The process is transparent to the\n\nvirtual server operating system and can be used to increase scalability by\n\nlive-migrating a virtual server that is running at an overloaded physical\n\nserver to another physical server that has suitable capacity.\n\nDatabase Cluster – Designed to improve data availability, this high-\n\navailability resource cluster has a synchronization feature that maintains the\n\nconsistency of data being stored at different storage devices used in the\n\ncluster. The redundant capacity is usually based on an active-active or\n\nactive-passive failover system committed to maintaining the\n\nsynchronization conditions.\n\nLarge Dataset Cluster – Data partitioning and distribution is implemented\n\nso that the target datasets can be efficiently partitioned without\n\ncompromising data integrity or computing accuracy. Each cluster node\n\nprocesses workloads without communicating with other nodes as much as\n\nin other cluster types.\n\nMany resource clusters require cluster nodes to have almost identical\n\ncomputing capacity and characteristics in order to simplify the design of\n\nand maintain consistency within the resource cluster architecture. The\n\ncluster nodes in high-availability cluster architectures need to access and\n\nshare common storage IT resources. This can require two layers of\n\ncommunication between the nodes—one for accessing the storage device\n\nand another to execute IT resource orchestration (Figure 9.28). Some\n\nresource clusters are designed with more loosely coupled IT resources that\n\nonly require the network layer (Figure 9.29).\n\nFigure 9.28\n\nLoad balancing and resource replication are implemented through a\n\ncluster-enabled hypervisor. A dedicated storage area network is used to\n\nconnect the clustered storage and the clustered servers, which are able to\n\nshare common cloud storage devices. This simplifies the storage replication\n\nprocess, which is independently carried out at the storage cluster. (See the\n\nHypervisor Clustering Architecture section in Chapter 14 for a more\n\ndetailed description.)\n\nFigure 9.29\n\nA loosely coupled server cluster that incorporates a load balancer. There is\n\nno shared storage. Resource replication is used to replicate cloud storage\n\ndevices through the network by the cluster software.\n\nThere are two basic types of resource clusters:\n\nLoad Balanced Cluster – This resource cluster specializes in distributing\n\nworkloads among cluster nodes to increase IT resource capacity while\n\npreserving the centralization of IT resource management. It usually\n\nimplements a load balancer mechanism that is either embedded within the\n\ncluster management platform or set up as a separate IT resource.\n\nHA Cluster – A high-availability cluster maintains system availability in the\n\nevent of multiple node failures, and has redundant implementations of most\n\nor all of the clustered IT resources. It implements a failover system\n\nmechanism that monitors failure conditions and automatically redirects the\n\nworkload away from any failed nodes.\n\nThe provisioning of clustered IT resources can be considerably more\n\nexpensive than the provisioning of individual IT resources that have an\n\nequivalent computing capacity.\n\nCase Study Example\n\nDTGOV is considering introducing a clustered virtual\n\nserver to run in a high--availability cluster as part of the\n\nvirtualization platform (Figure 9.30). The virtual servers\n\ncan live migrate among the physical servers, which are\n\npooled in a high-availability hardware cluster that is\n\ncontrolled by coordinated cluster-enabled hypervisors.\n\nThe coordination function keeps replicated snapshots of\n\nthe running virtual servers to facilitate migration to other\n\nphysical servers in the event of a failure.\n\nFigure 9.30\n\nAn HA virtualization cluster of physical servers is\n\ndeployed using a cluster-enabled hypervisor, which\n\nguarantees that the physical servers are constantly in\n\nsync. Every virtual server that is instantiated in the\n\ncluster is automatically replicated in at least two\n\nphysical servers.\n\nFigure 9.31 identifies the virtual servers that are\n\nmigrated from their failed physical host server to other\n\navailable physical servers.\n\nFigure 9.31\n\nAll of the virtual servers that are hosted on a physical\n\nserver experiencing failure are automatically migrated\n\nto other physical servers.\n\n9.8 Multi-Device Broker\n\nAn individual cloud service may need to be accessed by a range of cloud\n\nservice consumers differentiated by their hosting hardware devices and/or\n\ncommunication requirements. To overcome incompatibilities between a\n\ncloud service and a disparate cloud service consumer, mapping logic needs\n\nto be created to transform (or convert) information that is exchanged at\n\nruntime.\n\nThe multi-device broker mechanism is used to facilitate runtime data\n\ntransformation so as to make a cloud service accessible to a wider range of\n\ncloud service consumer programs and devices (Figure 9.32).\n\nFigure 9.32\n\nA multi-device broker contains the mapping logic necessary to transform\n\ndata exchanges between a cloud service and different types of cloud service\n\nconsumer devices. This scenario depicts the multi-device broker as a cloud\n\nservice with its own API. This mechanism can also be implemented as a\n\nservice agent that intercepts messages at runtime to perform necessary\n\ntransformations.\n\nMulti-device brokers commonly exist as gateways or incorporate gateway\n\ncomponents, such as:\n\nXML Gateway – transmits and validates XML data\n\nCloud Storage Gateway – transforms cloud storage protocols and encodes\n\nstorage devices to facilitate data transfer and storage\n\nMobile Device Gateway – transforms the communication protocols used by\n\nmobile devices into protocols that are compatible with a cloud service\n\nThe levels at which transformation logic can be created include:\n\ntransport protocols\n\nmessaging protocols\n\nstorage device protocols\n\ndata schemas/data models\n\nFor example, a multi-device broker may contain mapping logic that coverts\n\nboth transport and messaging protocols for a cloud service consumer\n\naccessing a cloud service with a mobile device.\n\nCase Study Example\n\nInnovartus has decided to make its role-playing\n\napplication available to various mobile and smartphone\n\ndevices. A complication that hindered Innovartus’\n\ndevelopment team during the mobile enhancement\n\ndesign stage was the difficulty in reproducing identical\n\nuser experiences across different mobile platforms. To\n\nresolve this issue, Innovartus implements a multi-device\n\nbroker to intercept incoming messages from devices,\n\nidentify the software platform, and convert the message\n\nformat into the native, server-side application format\n\n(Figure 9.33).\n\nFigure 9.33\n\nThe multi-device broker intercepts incoming messages\n\nand detects the platform (Web browser, iOS, Android) of\n\nthe source device (1). The multi-device broker\n\ntransforms the message into the standard format\n\nrequired by the Innovartus cloud service (2). The cloud\n\nservice processes the request and responds using the\n\nsame standard format (3). The multi-device broker\n\ntransforms the response message into the format\n\nrequired by the source device and delivers the message\n\n(4).\n\n9.9 State Management Database\n\nA state management database is a storage device that is used to temporarily\n\npersist state data for software programs. As an alternative to caching state\n\ndata in memory, software programs can off-load state data to the database in\n\norder to reduce the amount of runtime memory they consume (Figures 9.34\n\nand 9.35). By doing so, the software programs and the surrounding\n\ninfrastructure are more scalable. State management databases are\n\ncommonly used by cloud services, especially those involved in long-\n\nrunning runtime activities.\n\nFigure 9.34\n\nDuring the lifespan of a cloud service instance it may be required to remain\n\nstateful and keep state data cached in memory, even when idle.\n\nFigure 9.35\n\nBy deferring state data to a state data repository, the cloud service is able\n\nto transition to a stateless condition (or a partially stateless condition),\n\nthereby temporarily freeing system resources.\n\nCase Study Example\n\nATN is expanding its ready-made environment\n\narchitecture to allow for the deferral of state information\n\nfor extended periods by utilizing the state management\n\ndatabase mechanism. Figure 9.36 demonstrates how a\n\ncloud service consumer working with a ready-made\n\nenvironment pauses activity, causing the environment to\n\noff-load cached state data.\n\nFigure 9.36\n\nThe cloud consumer accesses the ready-made\n\nenvironment and requires three virtual servers to\n\nperform all activities (1). The cloud consumer pauses\n\nactivity. All of the state data needs to be preserved for\n\nfuture access to the ready-made environment (2). The\n\nunderlying infrastructure is automatically scaled in by\n\nreducing the number of virtual servers. State data is\n\nsaved in the state management database and one virtual\n\nserver remains active to allow for future logins by the\n\ncloud consumer (3). At a later point, the cloud consumer\n\nlogs in and accesses the ready-made environment to\n\ncontinue activity (4). The underlying infrastructure is\n\nautomatically scaled out by increasing the number of\n\nvirtual servers and by retrieving the state data from the\n\nstate management database (5).\n\nChapter 10\n\nCloud and Cybersecurity Access-Oriented Mechanisms\n\n10.1 Encryption\n\n10.2 Hashing\n\n10.3 Digital Signature\n\n10.4 Cloud-Based Security Groups\n\n10.5 Public Key Infrastructure (PKI) System\n\n10.6 Single Sign-On (SSO) System\n\n10.7 Hardened Virtual Server Image\n\n10.8 Firewall\n\n10.9 Virtual Private Network (VPN)\n\n10.10 Biometric Scanner\n\n10.11 Multi-Factor Authentication (MFA) System\n\n10.12 Identity and Access Management (IAM) System\n\n10.13 Intrusion Detection System (IDS)\n\n10.14 Penetration Testing Tool\n\n10.15 User Behavior Analytics (UBA) System\n\n10.16 Third-Party Software Update Utility\n\n10.17 Network Intrusion Monitor\n\n10.18 Authentication Log Monitor\n\n10.19 VPN Monitor\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nThis section describes the following mechanisms that are focused on\n\nestablishing cloud access controls that cloud access monitoring functions.\n\nEncryption\n\nHashing\n\nDigital Signature\n\nCloud-Based Security Groups\n\nPublic Key Infrastructure (PKI) System\n\nSingle Sign-On (SSO) System\n\nHardened Virtual Server Image\n\nFirewall\n\nVirtual Private Network (VPN)\n\nBiometric Scanner\n\nMulti-Factor Authentication (MFA) System\n\nIdentity and Access Management (IAM) System\n\nIntrusion Detection System (IDS)\n\nPenetration Testing Tool\n\nUser Behavior Analytics (UBA) System\n\nThird-Party Software Update Utility\n\nNetwork Intrusion Monitor\n\nAuthentication Log Monitor\n\nVPN Monitor\n\n10.1 Encryption\n\nData, by default, is coded in a readable format known as plaintext. When\n\ntransmitted over a network, plaintext is vulnerable to unauthorized and\n\npotentially malicious access. The encryption mechanism is a digital coding\n\nsystem dedicated to preserving the confidentiality and integrity of data. It is\n\nused for encoding plaintext data into a protected and unreadable format.\n\nEncryption technology commonly relies on a standardized algorithm called\n\na cipher to transform original plaintext data into encrypted data, referred to\n\nas ciphertext. Access to ciphertext does not divulge the original plaintext\n\ndata, apart from some forms of metadata, such as message length and\n\ncreation date. When encryption is applied to plaintext data, the data is\n\npaired with a string of characters called an encryption key, a secret message\n\nthat is established by and shared among authorized parties. The encryption\n\nkey is used to decrypt the ciphertext back into its original plaintext format.\n\nThe encryption mechanism can help counter the traffic eavesdropping,\n\nmalicious intermediary, insufficient authorization, and overlapping trust\n\nboundaries security threats. For example, malicious service agents that\n\nattempt traffic eavesdropping are unable to decrypt messages in transit if\n\nthey do not have the encryption key (Figure 10.1).\n\nFigure 10.1\n\nA malicious service agent is unable to retrieve data from an encrypted\n\nmessage. The retrieval attempt may furthermore be revealed to the cloud\n\nservice consumer. (Note the use of the lock symbol to indicate that a\n\nsecurity mechanism has been applied to the message contents.)\n\nThere are two common forms of encryption known as symmetric encryption\n\nand asymmetric encryption.\n\nSymmetric Encryption\n\nSymmetric encryption uses the same key for both encryption and\n\ndecryption, both of which are performed by authorized parties that use the\n\none shared key. Also known as secret key cryptography, messages that are\n\nencrypted with a specific key can be decrypted by only that same key.\n\nParties that rightfully decrypt the data are provided with evidence that the\n\noriginal encryption was performed by parties that rightfully possess the key.\n\nA basic authentication check is always performed, because only authorized\n\nparties that own the key can create messages. This maintains and verifies\n\ndata confidentiality.\n\nNote that symmetrical encryption does not have the characteristic of non-\n\nrepudiation, since determining exactly which party performed the message\n\nencryption or decryption is not possible if more than one party is in\n\npossession of the key.\n\nAsymmetric Encryption\n\nAsymmetric encryption relies on the use of two different keys, namely a\n\nprivate key and a public key. With asymmetric encryption (which is also\n\nreferred to as public key cryptography), the private key is known only to its\n\nowner while the public key is commonly available. A document that was\n\nencrypted with a private key can only be correctly decrypted with the\n\ncorresponding public key. Conversely, a document that was encrypted with\n\na public key can be decrypted only using its private key counterpart. As a\n\nresult of two different keys being used instead of just the one, asymmetric\n\nencryption is almost always computationally slower than symmetric\n\nencryption.\n\nThe level of security that is achieved is dictated by whether a private key or\n\npublic key was used to encrypt the plaintext data. As every asymmetrically\n\nencrypted message has its own private-public key pair, messages that were\n\nencrypted with a private key can be correctly decrypted by any party with",
      "page_number": 457
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 481-498)",
      "start_page": 481,
      "end_page": 498,
      "detection_method": "synthetic",
      "content": "the corresponding public key. This method of encryption does not offer any\n\nconfidentiality protection, even though successful decryption proves that\n\nthe text was encrypted by the rightful private key owner. Private key\n\nencryption therefore offers integrity protection in addition to authenticity\n\nand non-repudiation. A message that was encrypted with a public key can\n\nonly be decrypted by the rightful private key owner, which provides\n\nconfidentiality protection. However, any party that has the public key can\n\ngenerate the ciphertext, meaning this method provides neither message\n\nintegrity nor authenticity protection due to the communal nature of the\n\npublic key.\n\nNote\n\nThe encryption mechanism, when used to secure web-based\n\ndata transmissions, is most commonly applied via HTTPS,\n\nwhich refers to the use of SSL/TLS as an underlying\n\nencryption protocol for HTTP. TLS (transport layer security)\n\nis the successor to the SSL (secure sockets layer)\n\ntechnology. Because asymmetric encryption is usually more\n\ntime-consuming than symmetric encryption, TLS uses the\n\nformer only for its key exchange method. TLS systems then\n\nswitch to symmetric encryption once the keys have been\n\nexchanged.\n\nMost TLS implementations primarily support RSA as the\n\nchief asymmetrical encryption cipher, while ciphers such as\n\nRC4, Triple-DES, and AES are supported for symmetrical\n\nencryption.\n\nCase Study Example\n\nInnovartus has recently learned that users who access\n\ntheir User Registration Portal via public Wi-Fi hot zones\n\nand unsecured LANs may be transmitting personal user\n\nprofile details via plaintext. Innovartus immediately\n\nremedies this vulnerability by applying the encryption\n\nmechanism to its web portal via the use of HTTPS\n\n(Figure 10.2).\n\nFigure 10.2\n\nThe encryption mechanism is added to the\n\ncommunication channel between outside users and\n\nInnovartus’s User Registration Portal. This safeguards\n\nmessage confidentiality via the use of HTTPS.\n\n10.2 Hashing\n\nThe hashing mechanism is used when a one-way, non-reversible form of\n\ndata protection is required. Once hashing has been applied to a message, it\n\nis locked and no key is provided for the message to be unlocked. A common\n\napplication of this mechanism is the storage of passwords.\n\nHashing technology can be used to derive a hashing code or message digest\n\nfrom a message, which is often of a fixed length and smaller than the\n\noriginal message. The message sender can then utilize the hashing\n\nmechanism to attach the message digest to the message. The recipient\n\napplies the same hash function to the message to verify that the produced\n\nmessage digest is identical to the one that accompanied the message. Any\n\nalteration to the original data results in an entirely different message digest\n\nand clearly indicates that tampering has occurred.\n\nIn addition to its utilization for protecting stored data, the cloud threats that\n\ncan be mitigated by the hashing mechanism include malicious intermediary\n\nand insufficient authorization. An example of the former is illustrated in\n\nFigure 10.3.\n\nFigure 10.3\n\nA hashing function is applied to protect the integrity of a message that is\n\nintercepted and altered by a malicious service agent, before it is forwarded.\n\nThe firewall can be configured to determine that the message has been\n\naltered, thereby enabling it to reject the message before it can proceed to\n\nthe cloud service.\n\nCase Study Example\n\nA subset of the applications that have been selected to be\n\nported to ATN’s PaaS platform allows users to access\n\nand alter highly sensitive corporate data. This\n\ninformation is being hosted on a cloud to enable access\n\nby trusted partners who may use it for critical calculation\n\nand assessment purposes. Concerned that the data could\n\nbe tampered with, ATN decides to apply the hashing\n\nmechanism as a means of protecting and preserving the\n\ndata’s integrity.\n\nATN cloud resource administrators work with the cloud\n\nprovider to incorporate a digest-generating procedure\n\nwith each application version that is deployed in the\n\ncloud. Current values are logged to a secure database on-\n\npremises and the procedure is regularly repeated with the\n\nresults analyzed. Figure 10.4 illustrates how ATN\n\nimplements hashing to determine whether any non-\n\nauthorized actions have been performed against the\n\nported applications.\n\nFigure 10.4\n\nA hashing procedure is invoked when the PaaS\n\nenvironment is accessed (1). The applications that were\n\nported to this environment are checked (2) and their\n\nmessage digests are calculated (3). The message digests\n\nare stored in a secure database on-premises (4), and a\n\nnotification is issued if any of their values are not\n\nidentical to the ones in storage.\n\n10.3 Digital Signature\n\nThe digital signature mechanism is a means of providing data authenticity\n\nand integrity through authentication and non-repudiation. A message is\n\nassigned a digital signature prior to transmission, which is then rendered\n\ninvalid if the message experiences any subsequent, unauthorized\n\nmodifications. A digital signature provides evidence that the message\n\nreceived is the same as the one created by its rightful sender.\n\nBoth hashing and asymmetrical encryption are involved in the creation of a\n\ndigital signature, which essentially exists as a message digest that was\n\nencrypted by a private key and appended to the original message. The\n\nrecipient verifies the signature validity and uses the corresponding public\n\nkey to decrypt the digital signature, which produces the message digest. The\n\nhashing mechanism can also be applied to the original message to produce\n\nthis message digest. Identical results from the two different processes\n\nindicate that the message maintained its integrity.\n\nThe digital signature mechanism helps mitigate the malicious intermediary,\n\ninsufficient authorization, and overlapping trust boundaries security threats\n\n(Figure 10.5).\n\nFigure 10.5\n\nCloud Service Consumer B sends a message that was digitally signed but\n\nwas altered by trusted attacker Cloud Service Consumer A. Virtual Server B\n\nis configured to verify digital signatures before processing incoming\n\nmessages even if they are within its trust boundary. The message is revealed\n\nas illegitimate due to its invalid digital signature, and is therefore rejected\n\nby Virtual Server B.\n\nCase Study Example\n\nWith DTGOV’s client portfolio expanding to include\n\npublic-sector organizations, many of its cloud computing\n\npolicies have become unsuitable and require\n\nmodification. Considering that public-sector\n\norganizations frequently handle strategic information,\n\nsecurity safeguards need to be established to protect data\n\nmanipulation and to establish a means of auditing\n\nactivities that may impact government operations.\n\nDTGOV proceeds to implement the digital signature\n\nmechanism specifically to protect its web-based\n\nmanagement environment (Figure 10.6). Virtual server\n\nself-provisioning inside the IaaS environment and the\n\ntracking functionality of realtime SLA and billing are all\n\nperformed via web portals. As a result, user error or\n\nmalicious actions could result in legal and financial\n\nconsequences.\n\nFigure 10.6\n\nWhenever a cloud consumer performs a management\n\naction that is related to IT resources provisioned by\n\nDTGOV, the cloud service consumer program must\n\ninclude a digital signature in the message request to\n\nprove the legitimacy of its user.\n\nDigital signatures provide DTGOV with the guarantee\n\nthat every action performed is linked to its legitimate\n\noriginator. Unauthorized access is expected to become\n\nhighly improbable, since digital signatures are only\n\naccepted if the encryption key is identical to the secret\n\nkey held by the rightful owner. Users will not have\n\ngrounds to deny attempts at message adulteration\n\nbecause the digital signatures will confirm message\n\nintegrity.\n\n10.4 Cloud-Based Security Groups\n\nSimilar to constructing dykes and levees that separate land from water, data\n\nprotection is increased by placing barriers between IT resources. Cloud\n\nresource segmentation is a process by which separate physical and virtual\n\nIT environments are created for different users and groups. For example, an\n\norganization’s WAN can be partitioned according to individual network\n\nsecurity requirements. One network can be established with a resilient\n\nfirewall for external internet access, while a second is deployed without a\n\nfirewall because its users are internal and unable to access the internet.\n\nResource segmentation is used to enable virtualization by allocating a\n\nvariety of physical IT resources to virtual machines. It needs to be\n\noptimized for public cloud environments, since organizational trust\n\nboundaries from different cloud consumers overlap when sharing the same\n\nunderlying physical IT resources.\n\nThe cloud-based resource segmentation process creates cloud-based\n\nsecurity group mechanisms that are determined through security policies.\n\nNetworks are segmented into logical cloud-based security groups that form\n\nlogical network perimeters. Each cloud-based IT resource is assigned to at\n\nleast one logical cloud-based security group. Each logical cloud-based\n\nsecurity group is assigned specific rules that govern the communication\n\nbetween the security groups.\n\nMultiple virtual servers running on the same physical server can become\n\nmembers of different logical cloud-based security groups (Figure 10.7).\n\nVirtual servers can further be separated into public-private groups,\n\ndevelopment-production groups, or any other designation configured by the\n\ncloud resource administrator.\n\nFigure 10.7\n\nCloud-Based Security Group A encompasses Virtual Servers A and D and is\n\nassigned to Cloud Consumer A. Cloud-Based Security Group B is\n\ncomprised of Virtual Servers B, C, and E and is assigned to Cloud\n\nConsumer B. If Cloud Service Consumer A’s credentials are compromised,\n\nthe attacker would only be able to access and damage the virtual servers in\n\nCloud-Based Security Group A, thereby protecting Virtual Servers B, C, and\n\nE.\n\nCloud-based security groups delineate areas where different security\n\nmeasures can be applied. Properly implemented cloud-based security\n\ngroups help limit unauthorized access to IT resources in the event of a\n\nsecurity breach. This mechanism can be used to help counter the denial of\n\nservice, insufficient authorization, overlapping trust boundaries,\n\nvirtualization attack and container attack threats, and is closely related to\n\nthe logical network perimeter mechanism.\n\nCase Study Example\n\nNow that DTGOV has itself become a cloud provider,\n\nsecurity concerns are raised pertaining to its hosting of\n\npublic-sector client data. A team of cloud security\n\nspecialists is brought in to define cloud-based security\n\ngroups together with the digital signature and PKI\n\nmechanisms.\n\nSecurity policies are classified into levels of resource\n\nsegmentation before being integrated into DTGOV’s\n\nweb portal management environment. Consistent with\n\nthe security requirements guaranteed by its SLAs,\n\nDTGOV maps IT resource allocation to the appropriate\n\nlogical cloud-based security group (Figure 10.8), which\n\nhas its own security policy that clearly stipulates its IT\n\nresource isolation and control levels.\n\nFigure 10.8\n\nWhen an external cloud resource administrator accesses\n\nthe web portal to allocate a virtual server, the requested\n\nsecurity credentials are assessed and mapped to an\n\ninternal security policy that assigns a corresponding\n\ncloud-based security group to the new virtual server.\n\nDTGOV informs its clients about the availability of\n\nthese new security policies. Cloud consumers can\n\noptionally choose to utilize them and doing so results in\n\nincreased fees.\n\n10.5 Public Key Infrastructure (PKI) System\n\nA common approach for managing the issuance of asymmetric keys is\n\nbased on the public key infrastructure (PKI) system mechanism, which\n\nexists as a system of protocols, data formats, rules, and practices that enable\n\nlarge-scale systems to securely use public key cryptography. This system is\n\nused to associate public keys with their corresponding key owners (known\n\nas public key identification) while enabling the verification of key validity.\n\nPKI systems rely on the use of digital certificates, which are digitally signed\n\ndata structures that bind public keys to certificate owner identities, as well\n\nas to related information, such as validity periods. Digital certificates are\n\nusually digitally signed by a third-party certificate authority (CA), as\n\nillustrated in Figure 10.9.\n\nOther methods of generating digital signatures can be employed, even\n\nthough the majority of digital certificates are issued by only a handful of\n\ntrusted CAs like VeriSign and Comodo. Larger organizations, such as\n\nMicrosoft, can act as their own CA and issue certificates to their clients and\n\nthe public, since even individual users can generate certificates as long as\n\nthey have the appropriate software tools.\n\nBuilding up an acceptable level of trust for a CA is time-intensive but\n\nnecessary. Rigorous security measures, substantial infrastructure\n\ninvestments, and stringent operational processes all contribute to\n\nestablishing the credibility of a CA. The higher its level of trust and\n\nreliability, the more esteemed and reputable its certificates. The PKI system\n\nis a dependable method for implementing asymmetric encryption, managing\n\ncloud consumer and cloud provider identity information, and helping to\n\ndefend against the malicious intermediary and insufficient authorization\n\nthreats.\n\nThe PKI system mechanism is primarily used to counter the insufficient\n\nauthorization threat.\n\nFigure 10.9\n\nThe common steps involved during the generation of certificates by a\n\ncertificate authority.\n\nCase Study Example\n\nDTGOV requires that its clients use digital signatures to\n\naccess its web-based management environment. These\n\nare to be generated from public keys that have been\n\ncertified by a recognized certificate authority (Figure\n\n10.10).",
      "page_number": 481
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 499-519)",
      "start_page": 499,
      "end_page": 519,
      "detection_method": "synthetic",
      "content": "Figure 10.10\n\nAn external cloud resource administrator uses a digital\n\ncertificate to access the web-based management\n\nenvironment. DTGOV’s digital certificate is used in the\n\nHTTPS connection and then signed by a trusted CA.\n\n10.6 Single Sign-On (SSO) System\n\nPropagating the authentication and authorization information for a cloud\n\nservice consumer across multiple cloud services can be a challenge,\n\nespecially if numerous cloud services or cloud-based IT resources need to\n\nbe invoked as part of the same overall runtime activity. The single sign-on\n\n(SSO) system mechanism enables one cloud service consumer to be\n\nauthenticated by a security broker, which establishes a security context that\n\nis persisted while the cloud service consumer accesses other cloud services\n\nor cloud-based IT resources. Otherwise, the cloud service consumer would\n\nneed to re-authenticate itself with every subsequent request.\n\nThe SSO system mechanism essentially enables mutually independent\n\ncloud services and IT resources to generate and circulate runtime\n\nauthentication and authorization credentials. The credentials initially\n\nprovided by the cloud service consumer remain valid for the duration of a\n\nsession, while its security context information is shared (Figure 10.11). The\n\nSSO system mechanism’s security broker is especially useful when a cloud\n\nservice consumer needs to access cloud services residing on different clouds\n\n(Figure 10.12).\n\nFigure 10.11\n\nA cloud service consumer provides the security broker with login\n\ncredentials (1). The security broker responds with an authentication token\n\n(message with small lock symbol) upon successful authentication, which\n\ncontains cloud service consumer identity information (2) that is used to\n\nautomatically authenticate the cloud service consumer across Cloud\n\nServices A, B, and C (3).\n\nThis mechanism does not directly counter any of the cloud security threats\n\nlisted in Chapter 7. It primarily enhances the usability of cloud-based\n\nenvironments for access and management of distributed IT resources and\n\nsolutions.\n\nCase Study Example\n\nThe migration of applications to ATN’s new PaaS\n\nplatform was successful, but also raised a number of new\n\nconcerns pertaining to the responsiveness and\n\navailability of PaaS-hosted IT resources. ATN intends to\n\nmove more applications to a PaaS platform, but decides\n\nto do so by establishing a second PaaS environment with\n\na different cloud provider. This will allow them to\n\ncompare cloud providers during a three-month\n\nassessment period.\n\nTo accommodate this distributed cloud architecture, the\n\nSSO system mechanism is used to establish a security\n\nbroker capable of propagating login credentials across\n\nboth clouds (Figure 10.12). This enables a single cloud\n\nresource administrator to access IT resources on both\n\nPaaS environments without having to log in separately to\n\neach one.\n\nFigure 10.12\n\nThe credentials received by the security broker are\n\npropagated to ready-made environments across two\n\ndifferent clouds. The security broker is responsible for\n\nselecting the appropriate security procedure with which\n\nto contact each cloud.\n\n10.7 Hardened Virtual Server Image\n\nAs previously discussed, a virtual server is created from a template\n\nconfiguration called a virtual server image (or virtual machine image).\n\nHardening is the process of stripping unnecessary software from a system to\n\nlimit potential vulnerabilities that can be exploited by attackers. Removing\n\nredundant programs, closing unnecessary server ports, and disabling unused\n\nservices, internal root accounts, and guest access are all examples of\n\nhardening.\n\nA hardened virtual server image is a template for virtual service instance\n\ncreation that has been subjected to a hardening process (Figure 10.13). This\n\ngenerally results in a virtual server template that is significantly more\n\nsecure than the original standard image.\n\nFigure 10.13\n\nA cloud provider applies its security policies to harden its standard virtual\n\nserver images. The hardened image template is saved in the VM images\n\nrepository as part of a resource management system.\n\nHardened virtual server images help counter the denial of service,\n\ninsufficient authorization, and overlapping trust boundaries threats.\n\nCase Study Example\n\nOne of the security features made available to cloud\n\nconsumers as part of DTGOV adoption of cloud-based\n\nsecurity groups is an option to have some or all virtual\n\nservers within a given group hardened (Figure 10.14).\n\nEach hardened virtual server image results in an extra\n\nfee but spares cloud consumers from having to carry out\n\nthe hardening process themselves.\n\nFigure 10.14\n\nThe cloud resource administrator chooses the hardened\n\nvirtual server image option for the virtual servers\n\nprovisioned for Cloud-Based Security Group B.\n\n10.8 Firewall\n\nA firewall is a network gateway that limits access between networks in\n\naccordance with an established security policy. It acts as the interface of a\n\nnetwork to one or more external networks and regulates the network traffic\n\npassing through it by accepting or rejecting packets in accordance with a set\n\nof criteria. Both physical and virtual firewalls exist (Figure 10.15).\n\nFigure 10.15\n\nThe icons used to represent physical and virtual firewalls.\n\nFirewalls are used to protect the attack surface of an organization by\n\nintercepting all traffic that goes in and out and identifying whether any of\n\nthat traffic matches predefined rules that can be preconfigured to control the\n\ntraffic flow.\n\nA physical firewall protects physical connections to network devices.\n\nHowever, physical firewalls are not capable of filtering out traffic that\n\nbelongs to virtual networks which exist only within virtualized hosts in\n\nnetworking environments that are not represented by physical connections\n\nbetween physical devices. In such an environment, a virtual firewall can be\n\ndeployed to provide the same kind of protection for the virtual network\n\n(Figure 10.16). It is very common for virtual and physical firewalls to be\n\nintegrated so as to provide coordinated protection that encompasses\n\nphysical and virtual networks.\n\nFigure 10.16\n\nA physical firewall filters traffic for a physical network, whereas a virtual\n\nfirewall filters traffic for a virtual network.\n\nSome firewall implementations will also rely on firewall agents, which are\n\nprograms deployed to run on individual software programs. These agents\n\ncan provide a more individualized level of protection. A firewall with\n\nagents can be referred to as a distributed firewall because the overall\n\nfirewall capabilities are provided collectively by the central firewall and its\n\nagents.\n\nNote\n\nContemporary firewall products can encompass the\n\ncapabilities of other mechanisms, such as features from the\n\nintrusion detection system (IDS) and digital virus scanning\n\nand decryption system. Some firewall products utilize data\n\nscience technologies, such as machine learning and artificial\n\nintelligence (AI), to enable the firewall to evolve in its\n\nability to protect network traffic.\n\nCase Study Example\n\nAs part of DTGOV’s cloud migration strategy, deploying\n\nvirtual firewalls as part of every individual client\n\nnetwork is fundamental to ensuring that all of them are\n\nprotected against unauthorized access not only from the\n\ninternet but also from each other. A virtual firewall for\n\neach client will allow DTGOV to customize network\n\naccess individually as required by each different\n\ngovernment organization.\n\n10.9 Virtual Private Network (VPN)\n\nA VPN (Figure 10.17) is a mechanism that exists as an encrypted\n\nconnection that allows remote users to access devices on a firewall-\n\nprotected network. The VPN provides a secure communications tunnel for\n\ndata to be transmitted between networks. It is commonly used to establish\n\nan encrypted extension of a private network across an untrusted network,\n\nsuch as the internet. VPNs are implemented as virtual networks.\n\nFigure 10.17\n\nThe icon used to represent the virtual private network (VPN) mechanism.\n\nVPNs protect access to internal information assets by allowing only\n\nauthorized parties with the required security clearance to remotely access\n\ndata, while blocking other parties. VPNs are commonly built using\n\ncryptographic technologies to authenticate, authorize and cypher all of the\n\ntraffic that passes through the VPN connection.\n\nThere are two types of VPNs:\n\nSecure VPN – This type of VPN sends and receives traffic in an encrypted\n\nand authenticated manner. Both the server and the client agree on security\n\nproperties and no one outside of the VPN can modify these agreed\n\nproperties.\n\nTrusted VPN – This type of VPN may not use encryption, but instead, users\n\ntrust the VPN provider to ensure that no one else is using the same IP\n\naddress in the pathway of that VPN. In a trusted VPN, only the provider can\n\nchange, inject or delete data in the VPN’s communication channel.\n\nHybrid VPNs exist that combine the encryption property of a secure VPN\n\nand the dedicated connection property of a trusted VPN.\n\nNote\n\nCommon VPN protocols include Open VPN, L2TP/IPSec,\n\nSSTP, IKEv2, PPTP and Wireguard. Each offers different\n\nlevels of speed, security and ease of setup.\n\nCase Study Example\n\nDTGOV has identified certain client government\n\norganizations that need to be able to access protected\n\ndata stored in cloud-based storage servers from remote\n\nlocations in a secure manner. Dedicated physical\n\nconnections are not available everywhere, requiring\n\nmany of their clients to use the internet to access such\n\ndata. By implementing VPN connections, it can\n\nguarantee secure access to the protected data via the\n\ninternet.\n\n10.10 Biometric Scanner\n\nBiometrics is a technology used to determine a person’s identity based on\n\ntheir physiological or behavioral characteristics. Since biometric data is\n\ndirectly derived from these types of unique user characteristics, it cannot be\n\nlost or forgotten by the user, nor can it be easily forged by attackers. This\n\novercomes some of the problems users may have with passwords and\n\ntokens, which can be lost, forgotten, stolen or otherwise compromised by\n\nattackers.\n\nA biometric scanner (Figure 10.18) is a mechanism capable of validating a\n\nhuman’s identity by scanning or capturing a physiological or behavioral\n\ncharacteristic, such as handwriting, signatures, fingerprints, eyes, voice or\n\nfacial recognition.\n\nFigure 10.18\n\nThe icon used to represent the biometric scanner mechanism.\n\nThere are two primary types of identifiers that can be validated using\n\nbiometric scanners:\n\nPhysiological Identifiers – These can be either biological or morphological.\n\nBiological identifiers include DNA, blood, saliva and urine tests, which are\n\ncommonly used by medical teams and police forensics and do not really\n\napply to cybersecurity protection mechanisms. Morphological identifiers\n\ninclude fingerprints, hand shape or vein pattern, eyes (including iris and\n\nretina) and face shape.\n\nBehavioral Identifiers – These include voice recognition, signature\n\ndynamics (including the speed of movement of the pen, accelerations,\n\npressure exerted and inclination), keystroke dynamics, the way we use\n\ncertain objects, gait (the sound of a person’s steps when they walk) and\n\nother types of gestures.\n\nDifferent types of identifiers and measurements do not always have the\n\nsame level of reliability. Physiological measurements usually offer the\n\nbenefit of staying stable throughout the lifetime of a person and are not\n\nsubjected to stress, whereas behavioral measurements can change with\n\ndifferent life stages and stress levels (Figure 10.19).\n\nFigure 10.19\n\nOver time, a person’s fingerprint won’t normally change, making it a\n\nreliable physiological identifier. However, a person’s voice may change,\n\nmaking it a less reliable behavioral identifier.\n\nSome biometric scanner mechanisms combine different types of biometric\n\nscanners in order to increase the range of security validations and the\n\naccuracy of identifications. These types of systems are referred to as\n\nmultimodal biometric scanners and they require at least two biometric\n\ncredentials to perform identification. For example, a multimodal biometric\n\nscanner system may require both facial and fingerprint recognition in order\n\nto validate a user.\n\nBiometric scanners that are limited to verifying one identifier can also be\n\nreferred to as unimodal biometric scanners.\n\nCase Study Example\n\nRecognizing how critical it is to ensure that only\n\nauthorized guardians of children using their products\n\nshould be allowed to access a cloud-based account,\n\nInnovartus decides to support the option for parents to\n\nrequire that access is only allowed via the use of a thumb\n\nscan. To enable this, Innovartus uses the biometric\n\nscanner mechanism and makes it available for users of\n\nmobile devices. This can help guarantee that only",
      "page_number": 499
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 520-543)",
      "start_page": 520,
      "end_page": 543,
      "detection_method": "synthetic",
      "content": "parents and other authorized guardians gain access to the\n\ncloud account that stores private data.\n\n10.11 Multi-Factor Authentication (MFA) System\n\nA multi-factor authentication system (Figure 10.20) uses two or more\n\nfactors (verifiers) to achieve authentication. It works by requesting one\n\nform of verification from a user during a sign-in process, and then\n\nrequesting a second form of verification in order to complete the sign-in.\n\nThe types of authentication methods are kept independent of each other,\n\nthereby making it difficult for malicious users to gain unauthorized access.\n\nFigure 10.20\n\nThe icon used to represent the multi-factor authentication (MFA) system\n\nmechanism.\n\nFactors used in MFA systems typically include:\n\nsomething a user knows, such as a password or PIN (Figure 10.21)\n\nsomething a user has, such as a digital signature or token\n\nsome part of a user, such as a biometric identifier or measurement\n\nFigure 10.21\n\nAn MFA system is used to require multi-factor authentication steps by a\n\nuser after it is determined that the user is attempting access from a new\n\ngeographical location.\n\nMFA systems may also support:\n\nLocation-Based Authentication – A more advanced type of MFA that\n\nperforms verification based on a user’s IP address and geolocation.\n\nRisk-Based Authentication – A type of verification based on an analysis of\n\ncontext or behavior when a user is trying to access an account, such as\n\nwhen or from where the user is trying to sign in, whether sign-in is being\n\nperformed by a known or new device, how many failed sign-in attempts\n\nhave occurred, etc. This is also known as adaptive authentication.\n\nMFA systems are commonly used together with VPNs within organizations\n\nto enable employees to access corporate servers remotely.\n\nCase Study Example\n\nSome of the parents of children using Innovartus\n\nTechnologies products have requested specific persons\n\nto access their children’s accounts in the cloud on their\n\nbehalf. To ensure that the alternate “guardian” is truly\n\nthe one requesting access, Innovartus provides the option\n\nfor cloud accounts to be only accessible via multi-factor\n\nauthentication, such as a one-time password (OTP) sent\n\nvia text to the authorized party’s mobile device.\n\n10.12 Identity and Access Management (IAM) System\n\nThe identity and access management (IAM) system mechanism\n\nencompasses the components and policies necessary to control and track\n\nuser identities and access privileges for IT resources, environments, and\n\nsystems.\n\nSpecifically, IAM system mechanisms exist as systems comprised of four\n\nmain components:\n\nAuthentication – Username and password combinations remain the most\n\ncommon forms of user authentication credentials managed by the IAM\n\nsystem, which also can support digital signatures, digital certificates,\n\nbiometric hardware (fingerprint readers), specialized software (such as\n\nvoice analysis programs), and locking user accounts to registered IP or\n\nMAC addresses.\n\nAuthorization – The authorization component defines the correct granularity\n\nfor access controls and oversees the relationships between identities, access\n\ncontrol rights, and IT resource availability.\n\nUser Management – Related to the administrative capabilities of the\n\nsystem, the user management program is responsible for creating new user\n\nidentities and access groups, resetting passwords, defining password\n\npolicies, and managing privileges.\n\nCredential Management – The credential management system establishes\n\nidentities and access control rules for defined user accounts, which\n\nmitigates the threat of insufficient authorization.\n\nAlthough its objectives are similar to those of the PKI system mechanism,\n\nthe IAM system mechanism’s scope of implementation is distinct because\n\nits structure encompasses access controls and policies in addition to\n\nassigning specific levels of user privileges.\n\nThe IAM system mechanism is primarily used to counter the insufficient\n\nauthorization, denial of service, overlapping trust boundaries threats,\n\nvirtualization attack and containerization attack threats.\n\nAn IAM system (Figure 10.22) is an established mechanism used to\n\nidentify, authenticate and authorize users based on predefined user roles and\n\naccess privileges.\n\nFigure 10.22\n\nThe icon used to represent the identity and access management (IAM)\n\nsystem mechanism.\n\nAn IAM system can:\n\nverify users\n\nassign roles to users\n\nassign levels of access to users or groups of users (Figure 10.23)\n\nFigure 10.23\n\nThe IAM system authenticates User A and identifies the user as belonging\n\nto Role X. Based on the user’s role, the IAM system authorizes the user to\n\naccess two specific folders on a physical file server.\n\nAn IAM system can carry out the identification, authentication and\n\nauthorization of a user by utilizing:\n\nUnique Passwords – Traditionally, the most common type of digital\n\nauthentication an IAM system uses.\n\nPre-Shared Key (PSK) – A type of digital authentication whereby the\n\npassword is shared among users authorized to access the same IT resources.\n\nIt provides convenience but is less secure than the use of individual\n\npasswords.\n\nBehavioral Authentication – For access to sensitive information or critical\n\nsystems, the IAM can encompass or be used together with a biometric\n\nscanner to provide behavioral authentication. For example, it may analyze\n\nkeystroke dynamics or mouse-use characteristics to instantly determine\n\nwhether a user’s sign-in behavior falls outside of the norm.\n\nOther Biometrics – IAM systems can use other biometric identifiers for\n\nmore precise authentication.\n\nContemporary IAM systems can include AI technology to help assess user\n\npatterns and behaviors. The system may collect historical user access data\n\nthat the AI system can use to learn about users and as a reference when\n\ncomparing recent user behavior to historically recorded user behavior.\n\nCase Study Example\n\nAs a result of several past corporate acquisitions, ATN’s\n\nlegacy landscape has become complex and\n\nheterogeneous. Maintenance costs have increased due to\n\nredundant and similar applications and databases\n\nrunning concurrently. Legacy repositories of user\n\ncredentials are just as assorted.\n\nNow that ATN has ported several applications to a PaaS\n\nenvironment, new identities are created and configured\n\nin order to grant users access. The CloudEnhance\n\nconsultants suggest that ATN capitalize on this\n\nopportunity by starting a pilot IAM system initiative,\n\nespecially since a new group of cloud-based identities is\n\nneeded.\n\nATN agrees, and a specialized IAM system is designed\n\nspecifically to regulate the security boundaries within\n\ntheir new PaaS environment. With this system, the\n\nidentities assigned to cloud-based IT resources differ\n\nfrom corresponding on-premises identities, which were\n\noriginally defined according to ATN’s internal security\n\npolicies.\n\n10.13 Intrusion Detection System (IDS)\n\nThe intrusion detection system (IDS) mechanism (Figure 10.24) detects\n\nunauthorized or intrusion activity. It is the first line of defense for many\n\nnetworks. Intrusion detection systems (IDSs) reference a database of known\n\nattack data to help recognize suspicious activity. Contemporary systems\n\nutilize machine learning and AI technology to help recognize activity\n\nassociated with new attacks or being carried out by new attackers.\n\nFigure 10.24\n\nThe icon used to represent the intrusion detection system (IDS) mechanism.\n\nBased on the type of machine learning or AI technology used, different\n\nforms of intrusion detection can be carried out.\n\nFor example, an anomaly-based detection system works by creating a\n\nbaseline for each information asset that represents a profile of “normal\n\nbehavior”. This profile considers usage bandwidth and other metrics for\n\neach device in the organization’s attack surface, generating an alert for any\n\nactivity that deviates from this baseline. Since each information asset is\n\nunique, these customized profiles can be created to make it more difficult\n\nfor an attacker to know which specific activity can be carried out without\n\nsetting off an alarm.\n\nFeatures like this can help detect zero-day attacks, due to the fact that such\n\na system does not depend on an established database of prior known\n\nintrusions, but instead focuses on the deviations from the established\n\nbaselines.\n\nThere are two primary types of IDS mechanisms:\n\nPassive – The previously described scenario is an example of a passive\n\nIDS, as its main responsibilities are to detect intrusions and raise alerts.\n\nDynamic – A dynamic IDS (also known as an intrusion detection and\n\nprevention system) is additionally designed to take action when a suspected\n\nintrusion has been detected.\n\nIn general, an intrusion detection and prevention system is a combination of\n\na passive IDS and access control devices that the system executes to block\n\nan intruder.\n\nCase Study Example\n\nGiven that secret or confidential data is not allowed\n\noutside of the secure perimeter of each client\n\norganization, some law-enforcement clients of DTGOV\n\nhave been attacked with the purpose of acquiring this\n\ntype of data, such as data about open cases. Therefore,\n\nDTGOV decides to install an intrusion detection system\n\nthat will help enable it to take immediate action\n\nwhenever it is detected that attackers are attempting to\n\npenetrate a secure perimeter established by DTGOV for\n\none of those clients.\n\n10.14 Penetration Testing Tool\n\nThe penetration testing tool (Figure 10.25) is used to carry out penetration\n\ntesting, also known as pentesting, which is the practice of testing a network\n\nor system to expose security vulnerabilities. It helps organizations\n\nunderstand the current capabilities of their cybersecurity environments,\n\nprovides insight into which attacks can more successfully occur and allows\n\nfor security professionals to carry out simulations of actual attacks.\n\nFigure 10.25\n\nThe icon used to represent the penetration testing tool mechanism.\n\nContemporary attack vectors need to be examined thoroughly for potential\n\nvulnerabilities using modern and improved penetration testing techniques,\n\nwhich include:\n\nautomated pentesting\n\ncloud-based pentesting\n\nsocial engineering pentesting (to evaluate how humans may react to threats,\n\nsuch as phishing)\n\nFigure 10.26 illustrates several pentesting scenarios.\n\nFigure 10.26\n\nA security professional uses the penetration testing tool to verify that an\n\nintrusion detection system (IDS) (1) is working correctly. The penetration\n\ntesting tool then exposes a vulnerability in a virtual firewall (2). Finally, it\n\ntries (unsuccessfully) to trick a human worker into opening a phishing\n\nemail (3).\n\nPenetration testing can be performed in a fully automated manner that\n\nallows for more frequent tests to be performed on the organization’s\n\nsecurity infrastructure. This can help establish a continuous assessment of a\n\ncybersecurity environment’s effectiveness.\n\nCase Study Example\n\nDTGOV has implemented many security measures and\n\ncontrols to protect its clients’ data and IT resources.\n\nHowever, the effectiveness of all those mechanisms,\n\nindividually or as a group, has never been assessed.\n\nDTGOVE employs the use of a penetration testing tool\n\nto periodically test its security controls to ensure their\n\neffectiveness.\n\nSpecifically, the penetration testing tool is used in\n\nspecial exercises designed and scheduled to test and\n\nverify how well certain cloud security mechanisms are\n\nperforming. This helps DTGOV make further\n\nadjustments and improvements in its security\n\narchitecture.\n\n10.15 User Behavior Analytics (UBA) System\n\nA UBA system (Figure 10.27) monitors the behavior of users in realtime in\n\norder to establish a baseline for “normal user activity” with the purpose of\n\nidentifying abnormal user behavior that could indicate malicious activity.\n\nMonitored behaviors can include attempts to open, view, delete and modify\n\nfiles, modifying critical system settings and initiating network\n\ncommunications. A UBA system can block suspicious behaviors in realtime\n\nand/or terminate offending software. Some advanced UBA solutions focus\n\non network and perimeter system activity such as logins and application and\n\nsystem-level events. Others may focus on more granular metadata in the\n\nsystem itself, such as user activity on files and emails.\n\nFigure 10.27\n\nThe icon used to represent the user behavior analytics (UBA) system\n\nmechanism.\n\nA UBA system utilizes data science practices and technologies. The system\n\nneeds to be trained to identify normal behavior by processing activity logs,\n\nfile access, logins, network activity and other types of historical activity.\n\nThrough a variety of machine learning analysis techniques and the use of AI\n\nand neural networks, the system can establish a baseline from which it will\n\nbe possible to predict what is and is not normal (Figure 10.28).\n\nFigure 10.28\n\nA UBA system detects suspicious activity when a user demonstrates\n\nabnormal behavior.\n\nOther features of UBA systems include:\n\nProcessing High User Activity Volume – File systems can be enormous and\n\nsensitive data can be spread out scarcely. In order to be able to identify\n\nattackers, a UBA system needs be able to search through and analyze key\n\nmetadata and activity of many users across potentially massive amounts\n\ndata.\n\nRealtime Alerts – A UBA system’s attacker detection algorithms must be\n\nable to raise alerts in near realtime since the time window for when\n\nattackers access and copy sensitive data can be very short.\n\nCase Study Example\n\nDTGOV recognizes that its users cannot be effectively\n\ntrained in cloud security awareness as quickly and\n\neffectively as it would want. It utilizes a UBA system\n\nthat allows it to monitor and recognize user behavior to\n\nidentify when a given user might actually be an attacker\n\nor intruder.\n\nThe UBA system analyzes the behavior of all of\n\nDTGOV’s clients’ end users and learns their common\n\nbehavior in preparation for any unauthorized user that\n\nmay attempt to use an account that belongs to a\n\nlegitimate user.\n\n10.16 Third-Party Software Update Utility\n\nCybersecurity-related software vulnerabilities commonly show up after a\n\nnew version of third-party software has been released. When this happens,\n\ndevelopers try to patch the vulnerability as fast as possible by releasing an\n\nupgrade or a patch that needs to be applied on all implementations of that\n\nsoftware to remediate the encountered vulnerability. The longer it takes for\n\nthe systems administrator to apply the update or patch, the higher the\n\nprobability of being attacked through said vulnerability. The third-party\n\nsoftware update utility (Figure 10.29) can help administrators automate the\n\nprocess of patching or updating their third-party software programs.\n\nFigure 10.29\n\nThe icon used to represent the third-party software update utility\n\nmechanism.\n\nThis mechanism typically works as follows:\n\nThe administrator defines a baseline that determines the level of update and\n\npatching that is required.\n\nAll related third-party software programs are reviewed against this baseline\n\nand the required updating and patching for each are identified.\n\nPatches and updates are downloaded from a central repository, usually\n\nthrough a secure channel to ensure that the software has not been tampered\n\nwith. They are stored locally for further remediation purposes.\n\nThe remediation process (the updating, upgrading or patching activity that\n\nis performed automatically by the tool) is scheduled and/or carried out\n\nwhen required (Figure 10.30).",
      "page_number": 520
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 545-565)",
      "start_page": 545,
      "end_page": 565,
      "detection_method": "synthetic",
      "content": "sheer amount of cloud-based virtual servers that have to\n\nbe updated periodically.\n\nDTGOV enlists the use of a third-party software update\n\nutility for each different operating system installed on\n\nthe virtual servers it manages. This allows it to ensure\n\nthat the operating systems are updated with all necessary\n\nsecurity vulnerability patches and fixes as soon as they\n\nbecome available.\n\n10.17 Network Intrusion Monitor\n\nThe network intrusion monitor (Figure 10.31) is dedicated to monitoring\n\nnetwork packets across different sub-networks in order to find any\n\nsuspicious activity. It reports back its findings to a centralized network\n\nintrusion detection system (IDS) that coordinates its activity.\n\nFigure 10.31\n\nThe icon used to represent the network intrusion monitor mechanism.\n\nThis mechanism can be signature-based or anomaly detection-based. The\n\nformer is reactive while the latter is proactive, autonomous and can be\n\nconfigured to respond automatically to identified threats.\n\nCase Study Example\n\nAs a provider of networking equipment itself for the\n\ntelecommunications industry, ATN is concerned about\n\nthe security of the virtual networks that connect all its\n\ncloud-based resources. ATN knows how networks can be\n\nbreached and wants to ensure that if that happened to its\n\nown cloud-based network, ATN can take immediate\n\naction to deal with the intruder appropriately.\n\nATN recognizes in the network intrusion monitor a\n\nmechanism that will notify the interested parties within\n\nthe organization when a network has been breached. It\n\nwill even provide sufficient information about the breach\n\nto allow IT security specialists from ATN to respond in\n\ntime and avert all potential damages to the organization.\n\n10.18 Authentication Log Monitor\n\nThe authentication log monitor (Figure 10.32) scans historical logs that\n\ninclude information about authentication events that occurred when users\n\nattempted to access protected network resources. This information may be\n\nused to solve access difficulties and to change authentication policy rules.\n\nFigure 10.32\n\nThe icon used to represent the authentication log monitor mechanism.\n\nAmong the data collected by this monitor is also authentication rule data,\n\nsuch as timeouts that represent the period of time during which a user can\n\naccess a resource after first being authenticated for its access.\n\nCase Study Example\n\nDTGOV needs to manage access for a very large number\n\nof users. This is a burdensome responsibility that has\n\nbeen performed manually by the administrators of the\n\ndifferent cloud-based resources that DTGOV manages\n\non behalf of its clients. Manual data entry has been\n\nprone to human error, which has resulted in complaints\n\nabout possible unauthorized access to users’ accounts.\n\nDTGOV decided to use an authentication log monitor to\n\nregularly analyze access-related information for users\n\nthat complain about their access privileges being used\n\nimproperly. This helps them determine when actual\n\nintrusion events may have occurred. With this\n\ninformation, DTGOV can proceed to review the access\n\nprivileges given to affected users to see if access was\n\ncarried out in compliance with the originally requested\n\nprivileges.\n\n10.19 VPN Monitor\n\nA VPN monitor (Figure 10.33) tracks and collects information about VPN\n\nconnections, such as which users have connected (or are currently\n\nconnected), the kinds of connections used and the volume of data\n\nexchanged over a certain period. In case of failed connection attempts, it\n\nrecords connection issues and sends notifications to administrators. This\n\nmechanism helps identify network anomalies.\n\nFigure 10.33\n\nThe icon used to represent the VPN monitor mechanism.\n\nCase Study Example\n\nSeveral of DTGOV’s clients that allow remote access to\n\ntheir cloud-based data and systems via a VPN have\n\ncomplained that their data may have been accessed by\n\nunauthorized parties. To be able to verify this behavior,\n\nDTGOV uses a VPN monitor. DTGOV analyzes the\n\ninformation collected by the VPN monitor to identify\n\npotential unauthorized access through the VPN.\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nThe following is a list of further third-party cloud security access-oriented\n\npractices and technologies:\n\nCloud Access Security Brokers (CASB) – Security solutions designed to\n\nprotect cloud-based applications and services. These solutions are typically\n\ndeployed between cloud service consumers and providers, allowing\n\norganizations to enforce security policies and gain visibility into cloud\n\nusage.\n\nSecure Access Service Edge (SASE) – A network architecture that combines\n\nnetwork security and wide-area networking capabilities to deliver secure\n\naccess to cloud-based applications and resources.\n\nCloud Security Posture Management (CSPM) – A cloud security solution\n\nthat provides continuous monitoring and management of an organization's\n\ncloud infrastructure to ensure compliance with security policies and\n\nregulations.\n\nCloud Workflow Protection Platforms (CWPP) – A type of cloud security\n\ntool that is designed to protect and secure the various workflows and\n\nprocesses that occur within cloud-based environments. These platforms\n\nhelp to ensure that these workflows are safe from unauthorized access, data\n\nbreaches, and other security threats.\n\nCloud Infrastructure Entitlement Management (CIEM) – A type of security\n\nsolution designed to manage and monitor access to cloud resources, such as\n\nservers, databases, and applications. CIEM helps organizations ensure that\n\nonly authorized personnel have access to their cloud infrastructure,\n\nreducing the risk of data breaches and unauthorized modifications. The\n\nsolution provides visibility into user access permissions and activity,\n\nallowing organizations to detect and respond to suspicious behavior in real-\n\ntime.\n\nChapter 11\n\nCloud and Cyber Security Data-Oriented Mechanisms\n\n11.1 Digital Virus Scanning and Decryption System\n\n11.2 Digital Immune System\n\n11.3 Malicious Code Analysis System\n\n11.4 Data Loss Prevention (DLP) System\n\n11.5 Trusted Platform Module (TPM)\n\n11.6 Data Backup and Recovery System\n\n11.7 Activity Log Monitor\n\n11.8 Traffic Monitor\n\n11.9 Data Loss Protection Monitor\n\nThis section describes the following mechanisms that are focused on\n\nestablishing data access controls that cloud data monitoring functions.\n\nDigital Virus Scanning and Decryption System\n\nDigital Immune System\n\nMalicious Code Analysis System\n\nData Loss Prevention (DLP) System\n\nTrusted Platform Module (TPM)\n\nData Backup and Recovery System\n\nActivity Log Monitor\n\nTraffic Monitor\n\nData Loss Protection Monitor\n\n11.1 Digital Virus Scanning and Decryption System\n\nThe digital virus scanning and decryption system (Figure 11.1) is\n\nessentially an advanced anti-virus system comprised of client-side and\n\nserver-side components. The client-side component detects viruses by\n\nscanning files using detection methods that include specific pattern matches\n\nwithin executable files or heuristic methods to detect viral activity. It\n\nattempts to clean an identified virus infection by removing the virus’s code\n\nand restoring the original file’s contents.\n\nFigure 11.1\n\nThe icon used to represent the digital virus scanning and decryption system\n\nmechanism.\n\nThe server-side component is responsible for maintaining a database of\n\ncollected virus information and using data science technologies to analyze\n\nand learn from the available information to help identify and counter new\n\npotential viruses or virus variants.\n\nThe client-side component periodically receives updated intelligence from\n\nthe server-side component to improve its ability to detect and remove\n\nviruses.\n\nThe digital virus scanning and decryption system commonly also provides\n\nthe following features:\n\nGeneric Decryption\n\nThis feature enables the system to detect highly complex viruses while\n\nmaintaining fast scanning speeds. Executable files are run through a generic\n\ndecryption scanner, which consists of three fundamental elements:\n\nCPU Emulator — A software-based virtual computer where an executable\n\nfile is run rather than executing it on the underlying processor.\n\nVirus Signature Scanner — Software that scans the executable file looking\n\nfor known virus signatures.\n\nEmulation Control Module — Software that controls the execution of the\n\nexecutable file.\n\nCase Study Example\n\nSince the numerous users working for DTGOV’s clients\n\nconnect to systems and IT resources that DTGOV\n\ndeploys and manages in the cloud on their behalf,\n\nseveral viruses have successfully attacked parts of that\n\ninfrastructure since the migration to the cloud first\n\nbegan.\n\nDTGOV employs a digital virus scanning and decryption\n\nsystem as part of their new cloud security defense\n\nstrategy. This system significantly reduces the spread of\n\nviruses throughout its cloud-based resources.\n\n11.2 Digital Immune System\n\nThis mechanism enables the system to capture a virus, strip it of\n\nconfidential information and then automatically submit it to a central virus\n\nanalysis center, where the virus is examined and a virus signature is created.\n\nThe virus signature is then tested against the original sample, and if\n\nsuccessful, it is sent back to the server to be deployed on the client-side\n\nmechanism component (Figure 11.2).\n\nFigure 11.2\n\nThe client-side component of the digital virus scanning and decryption\n\nsystem detects a virus on a workstation (1). The server-side component logs\n\nthe virus information in a central database (2) and further forwards it to a\n\ncentral virus analysis center (3), where it is assigned a virus signature. The\n\nvirus signature is returned (4), logged by the server-side component (5) and\n\ndistributed to all workstations under the system’s protection (6).\n\nCase Study Example\n\nDTGOV needs to contend with a variety of end users\n\nthat have poor knowledge about how to recognize\n\nmalicious email messages and potentially infected media\n\nin removable devices. This has resulted in virus and\n\nmalware programs being able to infect some servers.\n\nSince several of its client government organizations take\n\na long time to train their personnel, DTGOV makes use\n\nof a digital immune system to obtain maximum\n\nprotection from potential virus and other malware\n\nattacks.\n\n11.3 Malicious Code Analysis System\n\nThe malicious code analysis system (Figure 11.3) is a mechanism that is\n\nable to perform analysis of massive volumes of malicious code to quickly\n\nproduce a report that a human analyst can use to determine what actions the\n\nmalicious code took. Contemporary malicious code analysis systems rely\n\non machine learning technology to carry out and constantly improve\n\nmalware detection capabilities.\n\nFigure 11.3\n\nThe icon used to represent the malicious code analysis system mechanism.\n\nThe large processing capabilities of these systems enable them to accelerate\n\nsecurity investigations with detailed data about workload-related events,\n\napplication logs, infrastructure metrics, audits and other sources of\n\nmalicious code behavior information. The malicious code analysis system is\n\nfurther able to issue alerts about malicious or anomalous patterns (Figure\n\n11.4).\n\nFigure 11.4\n\nAn automated malicious code analysis system detects malicious code on a\n\nworkstation (1) and analyzes the code in realtime (2) to alert and provide a\n\nreport to a security professional to review (3).\n\nThe use of this mechanism can help an organization defend against zero-\n\nday attacks since the intelligence gathered is not necessarily based on\n\nhistorical intrusion detection, but rather on the results of data analysis\n\nprovided by models capable of identifying new malware in realtime.\n\nThere are two primary types of malicious code analysis systems:\n\nStatic — This type of system is capable of executing malicious code in a\n\nsafe and isolated environment called a sandbox, a controlled environment\n\nthat enables security professionals to watch the malware in action without\n\nrisking its potential effects on the organization’s business environment.\n\nDynamic — This type of system can provide deep insight into the\n\ncapabilities of malicious code. It utilizes automated sandboxing that\n\neliminates the time that it would take to reverse engineer a file after\n\nmalicious code has performed actions on it.\n\nSome attackers prepare their malicious programs to remain dormant while\n\nrunning in a sandbox environment. Therefore, a hybrid combination of\n\nstatic and dynamic malicious code analysis systems can be used to provide\n\na reliable means of detecting more sophisticated malicious code by hiding\n\nthe presence of a sandbox.\n\nCase Study Example\n\nInnovartus Technologies has been the target of multiple\n\ndifferent virus attacks and has therefore decided to\n\nimplement a malicious code analysis system to prevent\n\nsuch attacks in the future.\n\nThis system has especially helped Innovartus identify\n\nthe more sophisticated attacks that require specialized\n\nand profound code analysis to identify.\n\n11.4 Data Loss Prevention (DLP) System\n\nA DLP system (Figure 11.5) is a tool that enables security professionals to\n\nmanage the security of and configure access to distributed information\n\nassets, which becomes more difficult the more remote the workforce. It is\n\ncommonly used to avoid the unauthorized or accidental sharing of\n\nconfidential data by internal staff.\n\nFigure 11.5\n\nThe icon used to represent the data loss prevention (DLP) system\n\nmechanism.\n\nThe DLP mechanism’s capabilities can include:\n\nDevice Control — This allows the administrator to control which devices\n\nusers can store or copy data on. For example, it can be used to block users\n\nfrom storing potentially confidential data on USB drives or SD cards.\n\nContent Aware Protection — This allows the administrator to monitor and\n\ncontrol files, emails and other kinds of artifacts that can hold data primarily\n\nto ensure that no confidential information can be extracted from them.\n\nData Scanning — This function can scan files, emails and digital\n\ndocuments across different devices in order to mark those that can be\n\nconsidered confidential. Information assets can be labeled as confidential\n\nfor future reference by other mechanisms.\n\nForced Encryption — This is used to ensure that any content that is allowed\n\nto leave the organization is encrypted to ensure that it will be accessed only\n\nby authorized parties.\n\nFigure 11.6 demonstrates some of these capabilities.\n\nFigure 11.6\n\nA security professional with a DLP system blocks a user from storing\n\ncompany data on a USB (1), scans a corporate server with files in folders to\n\nidentify the ones with confidential data (2) and forces an email going\n\noutside of the organization boundary to be encrypted (3).\n\nData loss prevention (DLP) systems can exist as cloud-based services used\n\nto monitor cloud-based file sharing applications and sites.\n\nCase Study Example\n\nSome of DTGOV’s clients are law-enforcement\n\ngovernment organizations that need to keep certain data\n\nconfidential and secret. To prevent classified data from\n\nbeing shared in an unauthorized manner, a cloud-based\n\ndata loss prevention system is established specifically\n\nfor those clients.\n\nThis system ensures that any data copied or moved is\n\nchecked to see if it is confidential or secret. If that is the\n\ncase, it will not be allowed outside of a specified\n\nperimeter withing that client’s cloud environment.\n\n11.5 Trusted Platform Module (TPM)",
      "page_number": 545
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 566-583)",
      "start_page": 566,
      "end_page": 583,
      "detection_method": "synthetic",
      "content": "A TPM (Figure 11.7) is a mechanism that stores artifacts that are used to\n\nauthenticate devices (“platforms”), such as a PC, laptop, mobile phone or\n\ntablet. A TPM can exist as a chip that has a unique and secret key burned in\n\nduring production.\n\nFigure 11.7\n\nThe icon used to represent a trusted platform module (TPM) mechanism.\n\nThe TPM chip performs certain measurements each time the device starts.\n\nThese measurements include taking hashes of the BIOS code, BIOS\n\nsettings, TPM settings bootloader and OS kernel so that alternative versions\n\nof the measured modules cannot be easily produced, and so that the hashes\n\nlead to identical measurements. These measurements are used to validate\n\nagainst known good values.\n\nDuring boot-up, the mechanism verifies the characteristics of the hardware\n\ncomponents connected to the processor against the device information\n\nstored in the TPM. If the outcomes differ, then it is confirmed that the\n\nhardware has been compromised (Figure 11.8).\n\nFigure 11.8\n\nOn Day 1, an administrator starts up a physical server. The TPM\n\nmechanism verifies that the hardware is okay. On Day 2, the administrator\n\nstarts up the same server, only this time the TPM mechanism indicates that\n\nthe hardware confirmation does not match its measurements. The\n\nadministrator is made aware that the server could have been tampered with.\n\nCase Study Example\n\nThe security of children while using the virtual toys\n\nprovided by Innovartus Technologies is of critical\n\nimportance to their parents. It is therefore important for\n\nInnovartus to ensure that no malicious code can run on\n\nany of its cloud-based virtual servers.\n\nTo achieve this, it installs a TPM on each of its physical\n\nservers that host its cloud-based virtual servers. It uses\n\nthe TPM to verify that the hypervisor and every\n\noperating system instance running on those servers is\n\nverified for authenticity before it is loaded into memory.\n\nThis guarantees that no tampering with the physical\n\nhardware firmware or any other logic that runs on those\n\nservers can occur. This, in turn, helps eliminates the\n\npossibility of malware running together with their virtual\n\ntoy products.\n\n11.6 Data Backup and Recovery System\n\nThe data backup and recovery system (Figure 11.9) is a mechanism that is\n\nused to provide fast data recovery in the event of data loss or corruption\n\nresulting from cyber-attacks, cyber theft, physical theft and hardware and\n\nsoftware failure.\n\nFigure 11.9\n\nThe icon used to represent a data backup and recovery system mechanism.\n\nThe data backup and recovery system essentially copies important data to\n\nseparate storage repositories to provide a constant fallback for an\n\norganization to recover data (Figure 11.10).\n\nFigure 11.10\n\nA common technique for using the data backup and recovery system\n\nmechanism is known as the “3-2-1 approach,” which requires that data be\n\nkept in three separate locations, utilizing two different storage formats and\n\nwith one extra copy kept elsewhere, in a different geographical region.\n\nMany variations of this mechanism rely on placing backup data in clouds.\n\nCloud providers often provide backup-as-a-service (BaaS) offerings, which\n\ncan simplify data backup and recovery because they do not require the need\n\nto install and configure a storage device and additional software, such as an\n\noperating system.\n\nCase Study Example\n\nThe sheer volume of data that DTGOV is storing and\n\nprocessing in the cloud on behalf of its many clients\n\nplaces a significant responsibility on DTGOV to ensure\n\nthat it is always made available to its clients, regardless\n\nof any failure conditions or disruptions that may occur in\n\nthe cloud environment.\n\nA data backup and recovery system helps DTGOV\n\nensure that the most critical data it stores and processes\n\non behalf of its clients is copied in a safe and available\n\nmedium, in a location that is not subject to the same\n\nenvironmental or operational hazards that the original\n\ndata may be exposed to. This way, in case the original\n\ndata becomes unavailable, the copy can be used to\n\nrestore that data.\n\n11.7 Activity Log Monitor\n\nThe activity log monitor (Figure 11.11) scans historical log files or\n\ndatabases to attempt to find patterns of activity on networks which may\n\nprovide indicators of possible security breaches. Activity log data can come\n\nfrom event logs, device configuration logs, operating system logs, etc.\n\nFigure 11.11\n\nThe icon used to represent the activity log monitor mechanism.\n\nCase Study Example\n\nWhen parents complain to Innovartus Technologies\n\nabout possible unauthorized access to their cloud-based\n\naccounts, Innovartus needs to be able to verify such\n\nclaims.\n\nFor this purpose, they use an activity log monitor to\n\nsearch through all recorded access attempts, whether\n\nsuccessful or not, to the account in question.\n\nThis monitor provides information about any activity\n\npatterns that may indicate malicious behavior, which\n\nInnovartus can study to verify the legitimacy of each\n\ncomplaint.\n\n11.8 Traffic Monitor\n\nThe traffic monitor mechanism (Figure 11.12) is responsible for monitoring\n\nnetwork traffic in order to review and analyze traffic activity in search of\n\nabnormalities that may be adversely affecting network performance,\n\navailability and/or security. This mechanism provides network\n\nadministrators with realtime data and long-term usage trends for network\n\ndevices.\n\nFigure 11.12\n\nThe icon used to represent the traffic monitor mechanism.\n\nCase Study Example\n\nMultiple types of security incidents trigger specific\n\nnetwork-related events within the virtual networks that\n\ninterconnect cloud-based resources. Therefore, as a\n\ncomplement to the network intrusion monitor, ATN\n\ninstalls a traffic monitor mechanism to gather data about\n\nthe behavior of the network, which can be correlated\n\nwith information from the network intrusion monitor to\n\nidentify more specifically the type of intrusion or\n\nnetwork breach that happened, allowing ATN to take the\n\nmost effective action against the intrusion.\n\n11.9 Data Loss Protection Monitor\n\nA data loss protection monitor (Figure 11.13) is designed to safeguard vital\n\ndata by utilizing capture technology that acts as a digital recorder and\n\nreplays after-the-fact data loss incidents. These recordings can be used for\n\nsubsequent investigations. This mechanism can streamline remediation by\n\nalerting senders, recipients, content owners and system administrators.\n\nFigure 11.13\n\nThe icon used to represent the data loss protection monitor mechanism.\n\nThe data loss protection monitor is commonly used for an organization’s\n\nmost important information assets (such as source code, internal memos,\n\npatent applications, etc.). It detects many different content types traversing\n\nany port or protocol to uncover unknown threats. The monitor can find and\n\nanalyze sensitive information traveling across the network and apply rules\n\nto prevent future risks. This mechanism can further provide input for reports\n\nthat explain who sent data, where it went and how it was sent.\n\nNote\n\nA data loss protection monitor can help an organization meet\n\ndata loss monitoring regulatory requirements, such as PCI,\n\nGLBA, HIPAA and SOX.\n\nCase Study Example\n\nIn support of the strict data requirements of its law-\n\nenforcement clients, DTGOV relies on a data loss\n\nprotection monitor to keep it informed when any activity,\n\nsuch as copying or moving of data, occurs that is not in\n\ncompliance with their regulations and policies.\n\nChapter 12\n\nCloud Management Mechanisms\n\n12.1 Remote Administration System\n\n12.2 Resource Management System\n\n12.3 SLA Management System\n\n12.4 Billing Management System\n\nCloud-based IT resources need to be set up, configured, maintained, and\n\nmonitored. The systems covered in this chapter are mechanisms that\n\nencompass and enable these types of management tasks. They form key\n\nparts of cloud technology architectures by facilitating the control and\n\nevolution of the IT resources that form cloud platforms and solutions.\n\nThe following management-related mechanisms are described in this\n\nchapter:\n\nRemote Administration System\n\nResource Management System\n\nSLA Management System\n\nBilling Management System\n\nThese systems typically provide integrated APIs and can be offered as\n\nindividual products, custom applications, or combined into various product\n\nsuites or multi-function applications.\n\n12.1 Remote Administration System\n\nThe remote administration system mechanism (Figure 12.1) provides tools\n\nand user—interfaces for external cloud resource administrators to configure\n\nand administer cloud-based IT resources.\n\nFigure 12.1\n\nThe symbol used in this book for the remote administration system. The\n\ndisplayed user-interface will typically be labeled to indicate a specific type\n\nof portal.\n\nA remote administration system can establish a portal for access to\n\nadministration and management features of various underlying systems,\n\nincluding the resource management, SLA management, and billing\n\nmanagement systems described in this chapter (Figure 12.2).\n\nFigure 12.2\n\nThe remote administration system abstracts underlying management\n\nsystems to expose and centralize administration controls to external cloud\n\nresource administrators. The system provides a customizable user console,\n\nwhile programmatically interfacing with underlying management systems\n\nvia their APIs.\n\nThe tools and APIs provided by a remote administration system are\n\ngenerally used by the cloud provider to develop and customize online\n\nportals that provide cloud consumers with a variety of administrative\n\ncontrols.\n\nThe following are the two primary types of portals that are created with the\n\nremote administration system:\n\nUsage and Administration Portal — A general purpose portal that\n\ncentralizes management controls to different cloud-based IT resources and\n\ncan further provide IT resource usage reports. This portal is part of\n\nnumerous cloud technology architectures covered in Chapters 13 to 15.\n\nSelf-Service Portal — This is essentially a shopping portal that allows cloud\n\nconsumers to search an up-to-date list of cloud services and IT resources\n\nthat are available from a cloud provider (usually for lease). The cloud\n\nconsumer submits its chosen items to the cloud provider for provisioning.\n\nThis portal is primarily associated with the rapid provisioning architecture\n\ndescribed in Chapter 14.\n\nFigure 12.3 illustrates a scenario involving a remote administration system\n\nand both usage and administration and self-service portals.\n\nFigure 12.3",
      "page_number": 566
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 584-607)",
      "start_page": 584,
      "end_page": 607,
      "detection_method": "synthetic",
      "content": "A cloud resource administrator uses the usage and administration portal to\n\nconfigure an already leased virtual server (not shown) to prepare it for\n\nhosting (1). The cloud resource administrator then uses the self-service\n\nportal to select and request the provisioning of a new cloud service (2). The\n\ncloud resource administrator then accesses the usage and administration\n\nportal again to configure the newly provisioned cloud service that is hosted\n\non the virtual server (3). Throughout these steps, the remote administration\n\nsystem interacts with the necessary management systems to perform the\n\nrequested actions (4).\n\nDepending on:\n\nthe type of cloud product or cloud delivery model the cloud consumer is\n\nleasing or using from the cloud provider,\n\nthe level of access control granted by the cloud provider to the cloud\n\nconsumer, and\n\nwhich underlying management systems the remote administration system\n\ninterfaces with,\n\n…the following tasks can commonly be performed by cloud consumers via\n\na remote administration console:\n\nconfiguring and setting up cloud services\n\nprovisioning and releasing IT resource for on-demand cloud services\n\nmonitoring cloud service status, usage, and performance\n\nmonitoring QoS and SLA fulfillment\n\nmanaging leasing costs and usage fees\n\nmanaging user accounts, security credentials, authorization, and access\n\ncontrol\n\ntracking internal and external access to leased services\n\nplanning and assessing IT resource provisioning\n\ncapacity planning\n\nWhile the user-interface provided by the remote administration system will\n\ntend to be proprietary to the cloud provider, there is a preference among\n\ncloud consumers to work with remote administration systems that offer\n\nstandardized APIs. This allows a cloud consumer to invest in the creation of\n\nits own front-end with the fore-knowledge that it can reuse this console if it\n\ndecides to move to another cloud provider that supports the same\n\nstandardized API. Additionally, the cloud consumer would be able to further\n\nleverage standardized APIs if it is interested in leasing and centrally\n\nadministering IT resources from multiple cloud providers and/or IT\n\nresources residing in cloud and on-premise environments (Figure 12.4).\n\nFigure 12.4\n\nStandardized APIs published by remote administration systems from\n\ndifferent clouds enable a cloud consumer to develop a custom portal that\n\ncentralizes a single IT resource management portal for both cloud-based\n\nand on-premise IT resources.\n\nCase Study Example\n\nDTGOV has been offering its cloud consumers a user-\n\nfriendly remote administration system for some time,\n\nand recently determined that upgrades are required in\n\norder to accommodate the growing number of cloud\n\nconsumers and increasing diversity of requests. DTGOV\n\nis planning a development project to extend the remote\n\nadministration system to fulfill the following\n\nrequirements:\n\nCloud consumers need to be able to self-provision\n\nvirtual servers and virtual storage devices. The system\n\nspecifically needs to interoperate with the cloud-enabled\n\nVIM platform’s proprietary API to enable self-\n\nprovisioning capabilities.\n\nA single sign-on mechanism (described in Chapter 10)\n\nneeds to be incorporated to centrally authorize and\n\ncontrol cloud consumer access.\n\nAn API that supports the provisioning, starting, stopping,\n\nreleasing, up-down scaling, and replicating of commands\n\nfor virtual servers and cloud storage devices needs to be\n\nexposed.\n\nIn support of these features, a self-service portal is\n\ndeveloped and the feature-set of DTGOV’s existing\n\nusage and administration portal is extended.\n\n12.2 Resource Management System\n\nThe resource management system mechanism helps coordinate IT resources\n\nin response to management actions performed by both cloud consumers and\n\ncloud providers (Figure 12.5). Core to this system is the virtual\n\ninfrastructure manager (VIM) that coordinates the server hardware so that\n\nvirtual server instances can be created from the most expedient underlying\n\nphysical server. A VIM is a commercial product that can be used to manage\n\na range of virtual IT resources across multiple physical servers. For\n\nexample, a VIM can create and manage multiple instances of a hypervisor\n\nacross different physical servers or allocate a virtual server on one physical\n\nserver to another (or to a resource pool).\n\nFigure 12.5\n\nA resource management system encompassing a VIM platform and a virtual\n\nmachine image repository. The VIM may have additional repositories,\n\nincluding one dedicated to storing operational data.\n\nTasks that are typically automated and implemented through the resource\n\nmanagement system include:\n\nmanaging virtual IT resource templates that are used to create pre-built\n\ninstances, such as virtual server images\n\nallocating and releasing virtual IT resources into the available physical\n\ninfrastructure in response to the starting, pausing, resuming, and\n\ntermination of virtual IT resource instances\n\ncoordinating IT resources in relation to the involvement of other\n\nmechanisms, such as resource replication, load balancer, and failover\n\nsystem\n\nenforcing usage and security policies throughout the lifecycle of cloud\n\nservice instances\n\nmonitoring operational conditions of IT resources\n\nResource management system functions can be accessed by cloud resource\n\nadministrators employed by the cloud provider or cloud consumer. Those\n\nworking on behalf of a cloud provider will often be able to directly access\n\nthe resource management system’s native console.\n\nResource management systems typically expose APIs that allow cloud\n\nproviders to build remote administration system portals that can be\n\ncustomized to selectively offer resource management controls to external\n\ncloud resource administrators acting on behalf of cloud consumer\n\norganizations via usage and administration portals.\n\nBoth forms of access are depicted in Figure 12.6.\n\nFigure 12.6\n\nThe cloud consumer’s cloud resource administrator accesses a usage and\n\nadministration portal externally to administer a leased IT resource (1). The\n\ncloud provider’s cloud resource administrator uses the native user-interface\n\nprovided by the VIM to perform internal resource management tasks (2).\n\nCase Study Example\n\nThe DTGOV resource management system is an\n\nextension of a new VIM product it purchased, and\n\nprovides the following primary features:\n\nmanagement of virtual IT resources with a flexible\n\nallocation of pooled IT resources across different data\n\ncenters\n\nmanagement of cloud consumer databases\n\nisolation of virtual IT resources at logical perimeter\n\nnetworks\n\nmanagement of a template virtual server image\n\ninventory available for immediate instantiation\n\nautomated replication (“snapshotting”) of virtual server\n\nimages for virtual server creation\n\nautomated up-down scaling of virtual servers according\n\nto usage thresholds to enable live VM migration among\n\nphysical servers\n\nan API for the creation and management of virtual\n\nservers and virtual storage devices\n\nan API for the creation of network access control rules\n\nan API for the up-down scaling of virtual IT resources\n\nan API for the migration and replication of virtual IT\n\nresources across multiple data centers\n\ninteroperation with a single sign-on mechanism through\n\nan LDAP interface\n\nCustom-designed SNMP command scripts are further\n\nimplemented to interoperate with the network\n\nmanagement tools to establish isolated virtual networks\n\nacross multiple data centers.\n\n12.3 SLA Management System\n\nThe SLA management system mechanism represents a range of\n\ncommercially available cloud management products that provide features\n\npertaining to the administration, collection, storage, reporting, and runtime\n\nnotification of SLA data (Figure 12.7).\n\nFigure 12.7\n\nAn SLA management system encompassing an SLA manager and QoS\n\nmeasurements repository.\n\nAn SLA management system deployment will generally include a\n\nrepository used to store and retrieve collected SLA data based on pre-\n\ndefined metrics and reporting parameters. It will further rely on one or more\n\nSLA monitor mechanisms to collect the SLA data that can then be made\n\navailable in near-real time to usage and administration portals to provide\n\non-going feedback regarding active cloud services (Figure 12.8). The\n\nmetrics monitored for individual cloud services are aligned with the SLA\n\nguarantees in corresponding cloud provisioning contracts.\n\nFigure 12.8\n\nA cloud service consumer interacts with a cloud service (1). An SLA\n\nmonitor intercepts the exchanged messages, evaluates the interaction, and\n\ncollects relevant runtime data in relation to quality-of-service guarantees\n\ndefined in the cloud service’s SLA (2A). The data collected is stored in a\n\nrepository (2B) that is part of the SLA management system (3). Queries can\n\nbe issued and reports can be generated for an external cloud resource\n\nadministrator via a usage and administration portal (4) or for an internal\n\ncloud resource administrator via the SLA management system’s native user-\n\ninterface (5).\n\nCase Study Example\n\nDTGOV implements an SLA management system that\n\ninteroperates with its existing VIM. This integration\n\nallows DTGOV cloud resource administrators to monitor\n\nthe availability of a range of hosted IT resources via\n\nSLA monitors.\n\nDTGOV works with the SLA management system’s\n\nreport design features to create the following pre-defined\n\nreports that are made available via custom dashboards:\n\nPer-Data Center Availability Dashboard — Publicly\n\naccessible through DTGOV’s corporate cloud portal, this\n\ndashboard shows the overall operational conditions of\n\neach group of IT resources at each data center, in\n\nrealtime.\n\nPer-Cloud Consumer Availability Dashboard — This\n\ndashboard displays realtime operational conditions of\n\nindividual IT resources. Information about each IT\n\nresource can only be accessed by the cloud provider and\n\nthe cloud consumer leasing or owning the IT resource.\n\nPer-Cloud Consumer SLA Report — This report\n\nconsolidates and summarizes SLA statistics for cloud\n\nconsumer IT resources, including downtimes and other\n\ntime-stamped SLA events.\n\nThe SLA events generated by the SLA monitors\n\nrepresent the status and performance of physical and\n\nvirtual IT resources that are controlled by the\n\nvirtualization platform. The SLA management system\n\ninteroperates with the network management tools\n\nthrough a custom-designed SNMP software agent that\n\nreceives the SLA event notifications.\n\nThe SLA management system also interacts with the\n\nVIM through its proprietary API to associate each\n\nnetwork SLA event to the affected virtual IT resource.\n\nThe system includes a proprietary database used to store\n\nSLA events (such as virtual server and network\n\ndowntimes).\n\nThe SLA management system exposes a REST API that\n\nDTGOV uses to interface with its central remote\n\nadministration system. The proprietary API has a\n\ncomponent service implementation that can be used for\n\nbatch-processing with the billing management system.\n\nDTGOV utilizes this to periodically provide downtime\n\ndata that translates into credit applied to cloud consumer\n\nusage fees.\n\n12.4 Billing Management System\n\nThe billing management system mechanism is dedicated to the collection\n\nand processing of usage data as it pertains to cloud provider accounting and\n\ncloud consumer billing. Specifically, the billing management system relies\n\non pay-per-use monitors to gather runtime usage data that is stored in a\n\nrepository that the system components then draw from for billing, reporting,\n\nand invoicing purposes (Figures 12.9 and 12.10).\n\nFigure 12.9\n\nA billing management system comprised of a pricing and contract manager\n\nand a pay-per-use measurements repository.\n\nFigure 12.10\n\nA cloud service consumer exchanges messages with a cloud service (1). A\n\npay-per-use monitor keeps track of the usage and collects data relevant to\n\nbilling (2A), which is forwarded to a repository that is part of the billing\n\nmanagement system (2B). The system periodically calculates the\n\nconsolidated cloud service usage fees and generates an invoice for the\n\ncloud consumer (3). The invoice may be provided to the cloud consumer\n\nthrough the usage and administration portal (4).\n\nThe billing management system allows for the definition of different pricing\n\npolicies, as well as custom pricing models on a per cloud consumer and/or\n\nper IT resource basis. Pricing models can vary from the traditional pay-per-\n\nuse models, to flat-rate or pay-per-allocation modes, or combinations\n\nthereof.\n\nBilling arrangements be based on pre-usage and post-usage payments. The\n\nlatter type can include pre-defined limits or it can be set up (with the mutual\n\nagreement of the cloud consumer) to allow for unlimited usage (and,\n\nconsequently, no limit on subsequent billing). When limits are established,\n\nthey are usually in the form of usage quotas. When quotas are exceeded, the\n\nbilling management system can block further usage requests by cloud\n\nconsumers.\n\nCase Study Example\n\nDTGOV decides to establish a billing management\n\nsystem that enables them to create invoices for custom-\n\ndefined billable events, such as subscriptions and IT\n\nresource volume usage. The billing management system\n\nis customized with the necessary events and pricing\n\nscheme metadata.\n\nIt includes the following two corresponding proprietary\n\ndatabases:\n\nbillable event repository\n\npricing scheme repository\n\nUsage events are collected from pay-per-use monitors\n\nthat are implemented as extensions to the VIM platform.\n\nThin-granularity usage events, such as virtual server\n\nstarting, stopping, up-down scaling, and\n\ndecommissioning, are stored in a repository managed by\n\nthe VIM platform.\n\nThe pay-per-use monitors further regularly supply the\n\nbilling management system with the appropriate billable\n\nevents. A standard pricing model is applied to most\n\ncloud consumer contracts, although it can be customized\n\nwhen special terms are negotiated.\n\nPart III\n\nCloud Computing Architecture\n\nChapter 13: Fundamental Cloud Architectures\n\nChapter 14: Advanced Cloud Architectures\n\nChapter 15: Specialized Cloud Architectures\n\nCloud technology architectures formalize functional domains within cloud\n\nenvironments by establishing well-defined solutions comprised of\n\ninteractions, behaviors, and distinct combinations of cloud computing\n\nmechanisms and other specialized cloud technology components.\n\nThe fundamental cloud architectural models covered in Chapter 13 establish\n\nfoundational layers of technology architecture common to most clouds.\n\nMany of the advanced and specialized models described in Chapters 14 and\n\n15 build upon these foundations to add complex and narrower-focused\n\nsolution architectures.\n\nNote\n\nMost of the cloud architectures described over the next three\n\nchapters are documented in greater detail in the book Cloud\n\nComputing Design Patterns (by Thomas Erl and Amin\n\nNaserpour), also part of the Pearson Digital Enterprise\n\nSeries from Thomas Erl. Visit www.thomaserl.com/books\n\nfor more information.\n\nChapter 13\n\nFundamental Cloud Architectures\n\n13.1 Workload Distribution Architecture\n\n13.2 Resource Pooling Architecture\n\n13.3 Dynamic Scalability Architecture\n\n13.4 Elastic Resource Capacity Architecture\n\n13.5 Service Load Balancing Architecture\n\n13.6 Cloud Bursting Architecture\n\n13.7 Elastic Disk Provisioning Architecture\n\n13.8 Redundant Storage Architecture\n\n13.9 Multi-Cloud Architecture\n\n13.10 Case Study Example\n\nThis chapter describes the following foundational cloud architectural\n\nmodels:\n\nWorkload Distribution\n\nResource Pooling\n\nDynamic Scalability\n\nElastic Resource Capacity\n\nService Load Balancing\n\nCloud Bursting\n\nElastic Disk Provisioning\n\nRedundant Storage\n\nMulti-Cloud\n\nFor each architecture, the typical involvement of cloud computing\n\nmechanisms (previously covered in Part II) is documented.\n\n13.1 Workload Distribution Architecture\n\nIT resources can be horizontally scaled via the addition of one or more\n\nidentical IT resources, and a load balancer that provides runtime logic\n\ncapable of evenly distributing the workload among the available IT\n\nresources (Figure 13.1). The resulting workload distribution architecture\n\nreduces both IT resource over-utilization and under-utilization to an extent",
      "page_number": 584
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 608-630)",
      "start_page": 608,
      "end_page": 630,
      "detection_method": "synthetic",
      "content": "dependent upon the sophistication of the load balancing algorithms and\n\nruntime logic.\n\nFigure 13.1\n\nA redundant copy of Cloud Service A is implemented on Virtual Server B.\n\nThe load balancer intercepts cloud service consumer requests and directs\n\nthem to both Virtual Servers A and B to ensure even workload distribution.\n\nThis fundamental architectural model can be applied to any IT resource,\n\nwith workload distribution commonly carried out in support of distributed\n\nvirtual servers, cloud storage devices, and cloud services. Load balancing\n\nsystems applied to specific IT resources usually produce specialized\n\nvariations of this architecture that incorporate aspects of load balancing,\n\nsuch as:\n\nthe service load balancing architecture explained later in this chapter\n\nthe load balanced virtual server architecture covered in Chapter 14\n\nthe load balanced virtual switches architecture described in Chapter 15\n\nIn addition to the base load balancer mechanism, and the virtual server and\n\ncloud storage device mechanisms to which load balancing can be applied,\n\nthe following mechanisms can also be part of this cloud architecture:\n\nAudit Monitor — When distributing runtime workloads, the type and\n\ngeographical location of the IT resources that process the data can\n\ndetermine whether monitoring is necessary to fulfill legal and regulatory\n\nrequirements.\n\nCloud Usage Monitor — Various monitors can be involved to carry out\n\nruntime workload tracking and data processing.\n\nHypervisor — Workloads between hypervisors and the virtual servers that\n\nthey host may require distribution.\n\nLogical Network Perimeter — The logical network perimeter isolates cloud\n\nconsumer network boundaries in relation to how and where workloads are\n\ndistributed.\n\nResource Cluster — Clustered IT resources in active/active mode are\n\ncommonly used to support workload balancing between different cluster\n\nnodes.\n\nResource Replication — This mechanism can generate new instances of\n\nvirtualized IT resources in response to runtime workload distribution\n\ndemands.\n\n13.2 Resource Pooling Architecture\n\nA resource pooling architecture is based on the use of one or more resource\n\npools, in which identical IT resources are grouped and maintained by a\n\nsystem that automatically ensures that they remain synchronized.\n\nProvided here are common examples of resource pools:\n\nPhysical server pools are composed of networked servers that have been\n\ninstalled with operating systems and other necessary programs and/or\n\napplications and are ready for immediate use.\n\nVirtual server pools are usually configured using one of several available\n\ntemplates chosen by the cloud consumer during provisioning. For example,\n\na cloud consumer can set up a pool of mid-tier Windows servers with 4 GB\n\nof RAM or a pool of low-tier Ubuntu servers with 2 GB of RAM.\n\nStorage pools, or cloud storage device pools, consist of file-based or block-\n\nbased storage structures that contain empty and/or filled cloud storage\n\ndevices.\n\nNetwork pools (or interconnect pools) are composed of different\n\npreconfigured network connectivity devices. For example, a pool of virtual\n\nfirewall devices or physical network switches can be created for redundant\n\nconnectivity, load balancing, or link aggregation.\n\nCPU pools are ready to be allocated to virtual servers, and are typically\n\nbroken down into individual processing cores.\n\nPools of physical RAM can be used in newly provisioned physical servers\n\nor to vertically scale physical servers.\n\nDedicated pools can be created for each type of IT resource and individual\n\npools can be grouped into a larger pool, in which case each individual pool\n\nbecomes a sub-pool (Figure 13.2).\n\nFigure 13.2\n\nA sample resource pool that is comprised of four sub-pools of CPUs,\n\nmemory, cloud storage devices, and virtual network devices.\n\nResource pools can become highly complex, with multiple pools created for\n\nspecific cloud consumers or applications. A hierarchical structure can be\n\nestablished to form parent, sibling, and nested pools in order to facilitate the\n\norganization of diverse resource pooling requirements (Figure 13.3).\n\nFigure 13.3\n\nPools B and C are sibling pools that are taken from the larger Pool A,\n\nwhich has been allocated to a cloud consumer. This is an alternative to\n\ntaking the IT resources for Pool B and Pool C from a general reserve of IT\n\nresources that is shared throughout the cloud.\n\nSibling resource pools are usually drawn from physically grouped IT\n\nresources, as opposed to IT resources that are spread out over different data\n\ncenters. Sibling pools are isolated from one another so that each cloud\n\nconsumer is only provided access to its respective pool.\n\nIn the nested pool model, larger pools are divided into smaller pools that\n\nindividually group the same type of IT resources together (Figure 13.4).\n\nNested pools can be used to assign resource pools to different departments\n\nor groups in the same cloud consumer organization.\n\nFigure 13.4\n\nNested Pools A.1 and Pool A.2 are comprised of the same IT resources as\n\nPool A, but in different quantities. Nested pools are typically used to\n\nprovision cloud services that need to be rapidly instantiated using the same\n\ntype of IT resources with the same configuration settings.\n\nAfter resources pools have been defined, multiple instances of IT resources\n\nfrom each pool can be created to provide an in-memory pool of “live” IT\n\nresources.\n\nIn addition to cloud storage devices and virtual servers, which are\n\ncommonly pooled mechanisms, the following mechanisms can also be part\n\nof this cloud architecture:\n\nAudit Monitor — This mechanism monitors resource pool usage to ensure\n\ncompliance with privacy and regulation requirements, especially when\n\npools contain cloud storage devices or data loaded into memory.\n\nCloud Usage Monitor — Various cloud usage monitors are involved in the\n\nruntime tracking and synchronization that are required by the pooled IT\n\nresources and any underlying management systems.\n\nHypervisor — The hypervisor mechanism is responsible for providing\n\nvirtual servers with access to resource pools, in addition to hosting the\n\nvirtual servers and sometimes the resource pools themselves.\n\nLogical Network Perimeter — The logical network perimeter is used to\n\nlogically organize and isolate resource pools.\n\nPay-Per-Use Monitor — The pay-per-use monitor collects usage and billing\n\ninformation on how individual cloud consumers are allocated and use IT\n\nresources from various pools.\n\nRemote Administration System — This mechanism is commonly used to\n\ninterface with backend systems and programs in order to provide resource\n\npool administration features via a front-end portal.\n\nResource Management System — The resource management system\n\nmechanism supplies cloud consumers with the tools and permission\n\nmanagement options for administering resource pools.\n\nResource Replication — This mechanism is used to generate new instances\n\nof IT resources for resource pools.\n\n13.3 Dynamic Scalability Architecture\n\nThe dynamic scalability architecture is an architectural model based on a\n\nsystem of predefined scaling conditions that trigger the dynamic allocation\n\nof IT resources from resource pools. Dynamic allocation enables variable\n\nutilization as dictated by usage demand fluctuations, since unnecessary IT\n\nresources are efficiently reclaimed without requiring manual interaction.\n\nThe automated scaling listener is configured with workload thresholds that\n\ndictate when new IT resources need to be added to the workload processing.\n\nThis mechanism can be provided with logic that determines how many\n\nadditional IT resources can be dynamically provided, based on the terms of\n\na given cloud consumer’s provisioning contract.\n\nThe following types of dynamic scaling are commonly used:\n\nDynamic Horizontal Scaling — IT resource instances are scaled out and in\n\nto handle fluctuating workloads. The automatic scaling listener monitors\n\nrequests and signals resource replication to initiate IT resource duplication,\n\nas per requirements and permissions.\n\nDynamic Vertical Scaling — IT resource instances are scaled up and down\n\nwhen there is a need to adjust the processing capacity of a single IT\n\nresource. For example, a virtual server that is being overloaded can have its\n\nmemory dynamically increased or it may have a processing core added.\n\nDynamic Relocation — The IT resource is relocated to a host with more\n\ncapacity. For example, a database may need to be moved from a tape-based\n\nSAN storage device with 4 GB per second I/O capacity to another disk-\n\nbased SAN storage device with 8 GB per second I/O capacity.\n\nFigures 13.5 to 13.7 illustrate the process of dynamic horizontal scaling.\n\nFigure 13.5\n\nCloud service consumers are sending requests to a cloud service (1). The\n\nautomated scaling listener monitors the cloud service to determine if\n\npredefined capacity thresholds are being exceeded (2).\n\nFigure 13.6\n\nThe number of requests coming from cloud service consumers increases (3).\n\nThe workload exceeds the performance thresholds. The automated scaling\n\nlistener determines the next course of action based on a predefined scaling\n\npolicy (4). If the cloud service implementation is deemed eligible for\n\nadditional scaling, the automated scaling listener initiates the scaling\n\nprocess (5).\n\nFigure 13.7\n\nThe automated scaling listener sends a signal to the resource replication\n\nmechanism (6), which creates more instances of the cloud service (7). Now\n\nthat the increased workload has been accommodated, the automated\n\nscaling listener resumes monitoring and detracting and adding IT\n\nresources, as required (8).\n\nThe dynamic scalability architecture can be applied to a range of IT\n\nresources, including virtual servers and cloud storage devices. Besides the\n\ncore automated scaling listener and resource replication mechanisms, the\n\nfollowing mechanisms can also be used in this form of cloud architecture:\n\nCloud Usage Monitor — Specialized cloud usage monitors can track\n\nruntime usage in response to dynamic fluctuations caused by this\n\narchitecture.\n\nHypervisor — The hypervisor is invoked by a dynamic scalability system to\n\ncreate or remove virtual server instances, or to be scaled itself.\n\nPay-Per-Use Monitor — The pay-per-use monitor is engaged to collect\n\nusage cost information in response to the scaling of IT resources.\n\n13.4 Elastic Resource Capacity Architecture\n\nThe elastic resource capacity architecture is primarily related to the\n\ndynamic provisioning of virtual servers, using a system that allocates and\n\nreclaims CPUs and RAM in immediate response to the fluctuating\n\nprocessing requirements of hosted IT resources (Figures 13.8 and 13.9).\n\nFigure 13.8\n\nCloud service consumers are actively sending requests to a cloud service\n\n(1), which are monitored by an automated scaling listener (2). An\n\nintelligent automation engine script is deployed with workflow logic (3) that\n\nis capable of notifying the resource pool using allocation requests (4).\n\nFigure 13.9\n\nCloud service consumer requests increase (5), causing the automated\n\nscaling listener to signal the intelligent automation engine to execute the\n\nscript (6). The script runs the workflow logic that signals the hypervisor to\n\nallocate more IT resources from the resource pools (7). The hypervisor\n\nallocates additional CPU and RAM to the virtual server, enabling the\n\nincreased workload to be handled (8).\n\nResource pools are used by scaling technology that interacts with the\n\nhypervisor and/or VIM to retrieve and return CPU and RAM resources at\n\nruntime. The runtime processing of the virtual server is monitored so that\n\nadditional processing power can be leveraged from the resource pool via\n\ndynamic allocation, before capacity thresholds are met. The virtual server\n\nand its hosted applications and IT resources are vertically scaled in\n\nresponse.\n\nThis type of cloud architecture can be designed so that the intelligent\n\nautomation engine script sends its scaling request via the VIM instead of to\n\nthe hypervisor directly. Virtual servers that participate in elastic resource\n\nallocation systems may require rebooting in order for the dynamic resource\n\nallocation to take effect.\n\nIntelligent\n\nAutomation Engine\n\nThe intelligent automation engine automates\n\nadministration tasks by executing scripts that contain\n\nworkflow logic.\n\nSome additional mechanisms that can be included in this cloud architecture\n\nare the following:\n\nCloud Usage Monitor — Specialized cloud usage monitors collect resource\n\nusage information on IT resources before, during, and after scaling, to help\n\ndefine the future processing capacity thresholds of the virtual servers.\n\nPay-Per-Use Monitor — The pay-per-use monitor is responsible for\n\ncollecting resource usage cost information as it fluctuates with the elastic\n\nprovisioning.\n\nResource Replication — Resource replication is used by this architectural\n\nmodel to generate new instances of the scaled IT resources.\n\n13.5 Service Load Balancing Architecture\n\nThe service load balancing architecture can be considered a specialized\n\nvariation of the workload distribution architecture that is geared specifically\n\nfor scaling cloud service implementations. Redundant deployments of cloud\n\nservices are created, with a load balancing system added to dynamically\n\ndistribute workloads.\n\nThe duplicate cloud service implementations are organized into a resource\n\npool, while the load balancer is positioned as either an external or built-in\n\ncomponent to allow the host servers to balance the workloads themselves.\n\nDepending on the anticipated workload and processing capacity of host\n\nserver environments, multiple instances of each cloud service\n\nimplementation can be generated as part of a resource pool that responds to\n\nfluctuating request volumes more efficiently.\n\nThe load balancer can be positioned either independent of the cloud\n\nservices and their host servers (Figure 13.10), or built-in as part of the\n\napplication or server’s environment. In the latter case, a primary server with\n\nthe load balancing logic can communicate with neighboring servers to\n\nbalance the workload (Figure 13.11).\n\nFigure 13.10\n\nThe load balancer intercepts messages sent by cloud service consumers (1)\n\nand forwards them to the virtual servers so that the workload processing is\n\nhorizontally scaled (2).\n\nFigure 13.11\n\nCloud service consumer requests are sent to Cloud Service A on Virtual\n\nServer A (1). The cloud service implementation includes built-in load\n\nbalancing logic that is capable of distributing requests to the neighboring\n\nCloud Service A implementations on Virtual Servers B and C (2).\n\nThe service load balancing architecture can involve the following\n\nmechanisms in addition to the load balancer:\n\nCloud Usage Monitor — Cloud usage monitors may be involved with\n\nmonitoring cloud service instances and their respective IT resource\n\nconsumption levels, as well as various runtime monitoring and usage data\n\ncollection tasks.\n\nResource Cluster — Active-active cluster groups are incorporated in this\n\narchitecture to help balance workloads across different members of the\n\ncluster.\n\nResource Replication — The resource replication mechanism is utilized to\n\ngenerate cloud service implementations in support of load balancing\n\nrequirements.\n\n13.6 Cloud Bursting Architecture\n\nThe cloud bursting architecture establishes a form of dynamic scaling that\n\nscales or “bursts out” on-premise IT resources into a cloud whenever",
      "page_number": 608
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 631-650)",
      "start_page": 631,
      "end_page": 650,
      "detection_method": "synthetic",
      "content": "predefined capacity thresholds have been reached. The corresponding\n\ncloud-based IT resources are redundantly pre-deployed but remain inactive\n\nuntil cloud bursting occurs. After they are no longer required, the cloud-\n\nbased IT resources are released and the architecture “bursts in” back to the\n\non-premise environment.\n\nCloud bursting is a flexible scaling architecture that provides cloud\n\nconsumers with the option of using cloud-based IT resources only to meet\n\nhigher usage demands. The foundation of this architectural model is based\n\non the automated scaling listener and resource replication mechanisms.\n\nThe automated scaling listener determines when to redirect requests to\n\ncloud-based IT resources, and resource replication is used to maintain\n\nsynchronicity between on-premise and cloud-based IT resources in relation\n\nto state information (Figure 13.12).\n\nFigure 13.12\n\nAn automated scaling listener monitors the usage of on-premise Service A,\n\nand redirects Service Consumer C’s request to Service A’s redundant\n\nimplementation in the cloud (Cloud Service A) once Service A’s usage\n\nthreshold has been exceeded (1). A resource replication system is used to\n\nkeep state management databases synchronized (2).\n\nIn addition to the automated scaling listener and resource replication,\n\nnumerous other mechanisms can be used to automate the burst in and out\n\ndynamics for this architecture, depending primarily on the type of IT\n\nresource being scaled.\n\n13.7 Elastic Disk Provisioning Architecture\n\nCloud consumers are commonly charged for cloud-based storage space\n\nbased on fixed-disk storage allocation, meaning the charges are\n\npredetermined by disk capacity and not aligned with actual data storage\n\nconsumption. Figure 13.13 demonstrates this by illustrating a scenario in\n\nwhich a cloud consumer provisions a virtual server with the Windows\n\nServer operating system and three 150 GB hard drives. The cloud consumer\n\nis billed for using 450 GB of storage space after installing the operating\n\nsystem, even though it has not yet installed any software.\n\nFigure 13.13\n\nThe cloud consumer requests a virtual server with three hard disks, each\n\nwith a capacity of 150 GB (1). The virtual server is provisioned according\n\nto the elastic disk provisioning architecture, with a total of 450 GB of disk\n\nspace (2). The 450 GB is allocated to the virtual server by the cloud\n\nprovider (3). The cloud consumer has not installed any software yet,\n\nmeaning the actual used space is currently 0 GB (4). Because the 450 GB\n\nare already allocated and reserved for the cloud consumer, it will be\n\ncharged for 450 GB of disk usage as of the point of allocation (5).\n\nThe elastic disk provisioning architecture establishes a dynamic storage\n\nprovisioning system that ensures that the cloud consumer is granularly\n\nbilled for the exact amount of storage that it actually uses. This system uses\n\nthin-provisioning technology for the dynamic allocation of storage space,\n\nand is further supported by runtime usage monitoring to collect accurate\n\nusage data for billing purposes (Figure 13.14).\n\nFigure 13.14\n\nThe cloud consumer requests a virtual server with three hard disks, each\n\nwith a capacity of 150 GB (1). The virtual server is provisioned by this\n\narchitecture with a total of 450 GB of disk space (2). The 450 GB are set as\n\nthe maximum disk usage that is allowed for this virtual server, although no\n\nphysical disk space has been reserved or allocated yet (3). The cloud\n\nconsumer has not installed any software, meaning the actual used space is\n\ncurrently at 0 GB (4). Because the allocated disk space is equal to the\n\nactual used space (which is currently at zero), the cloud consumer is not\n\ncharged for any disk space usage (5).\n\nThin-provisioning software is installed on virtual servers that process\n\ndynamic storage allocation via the hypervisor, while the pay-per-use\n\nmonitor tracks and reports granular billing-related disk usage data (Figure\n\n13.15).\n\nFigure 13.15\n\nA request is received from a cloud consumer, and the provisioning of a new\n\nvirtual server instance begins (1). As part of the provisioning process, the\n\nhard disks are chosen as dynamic or thin-provisioned disks (2). The\n\nhypervisor calls a dynamic disk allocation component to create thin disks\n\nfor the virtual server (3). Virtual server disks are created via the thin-\n\nprovisioning program and saved in a folder of near-zero size. The size of\n\nthis folder and its files grow as operating applications are installed and\n\nadditional files are copied onto the virtual server (4). The pay-per-use\n\nmonitor tracks the actual dynamically allocated storage for billing\n\npurposes (5).\n\nThe following mechanisms can be included in this architecture in addition\n\nto the cloud storage device, virtual server, hypervisor, and pay-per-use\n\nmonitor:\n\nCloud Usage Monitor — Specialized cloud usage monitors can be used to\n\ntrack and log storage usage fluctuations.\n\nResource Replication — Resource replication is part of an elastic disk\n\nprovisioning system when conversion of dynamic thin-disk storage into\n\nstatic thick-disk storage is required.\n\n13.8 Redundant Storage Architecture\n\nCloud storage devices are occasionally subject to failure and disruptions\n\nthat are caused by network connectivity issues, controller or general\n\nhardware failure, or security breaches. A compromised cloud storage\n\ndevice’s reliability can have a ripple effect and cause impact failure across\n\nall of the services, applications, and infrastructure components in the cloud\n\nthat are reliant on its availability.\n\nLUN\n\nA logical unit number (LUN) is a logical drive that\n\nrepresents a partition of a physical drive.\n\nStorage service gateway\n\nThe storage service gateway is a component that acts as\n\nthe external interface to cloud storage services, and is\n\ncapable of automatically redirecting cloud consumer\n\nrequests whenever the location of the requested data has\n\nchanged.\n\nThe redundant storage architecture introduces a secondary duplicate cloud\n\nstorage device as part of a failover system that synchronizes its data with\n\nthe data in the primary cloud storage device. A storage service gateway\n\ndiverts cloud consumer requests to the secondary device whenever the\n\nprimary device fails (Figures 13.16 and 13.17).\n\nFigure 13.16\n\nThe primary cloud storage device is routinely replicated to the secondary\n\ncloud storage device (1).\n\nFigure 13.17\n\nThe primary storage becomes unavailable and the storage service gateway\n\nforwards the cloud consumer requests to the secondary storage device (2).\n\nThe secondary storage device forwards the requests to the LUNs, allowing\n\ncloud consumers to continue to access their data (3).\n\nThis cloud architecture primarily relies on a storage replication system that\n\nkeeps the primary cloud storage device synchronized with its duplicate\n\nsecondary cloud storage devices (Figure 13.18).\n\nFigure 13.18\n\nStorage replication is used to keep the redundant storage device\n\nsynchronized with the primary storage device.\n\nStorage replication\n\nStorage replication is a variation of the resource\n\nreplication mechanisms used to synchronously or\n\nasynchronously replicate data from a primary storage\n\ndevice to a secondary storage device. It can be used to\n\nreplicate partial and entire LUNs.\n\nCloud providers may locate secondary cloud storage devices in a different\n\ngeographical region than the primary cloud storage device, usually for\n\neconomic reasons. However, this can introduce legal concerns for some\n\ntypes of data. The location of the secondary cloud storage devices can\n\ndictate the protocol and method used for synchronization, as some\n\nreplication transport protocols have distance restrictions.\n\nSome cloud providers use storage devices with dual array and storage\n\ncontrollers to improve device redundancy, and place secondary storage\n\ndevices in a different physical location for cloud balancing and disaster\n\nrecovery purposes. In this case, cloud providers may need to lease a\n\nnetwork connection via a third-party cloud provider in order to establish the\n\nreplication between the two devices.\n\n13.9 Multi-Cloud Architecture\n\nA cloud architecture that combines two or more public clouds is referred to\n\nas a multi-cloud architecture (Figure 13.19). The different clouds combined\n\nin this type of architecture may offer their resources through any of the\n\ncloud delivery models, namely IaaS, PaaS, or SaaS. One of the fundamental\n\nreasons to utilize a multi-cloud architecture is avoiding vendor lock-in that\n\nis caused by forming dependencies on only a single cloud provider.\n\nFigure 13.19\n\nAn organization uses different types of resources from different clouds,\n\ntaking advantage of those resources that each cloud is better at and\n\navoiding vendor lock-in.\n\nWhen using multi-cloud architectures, cloud consumers commonly select\n\nproviders for specific resources or services, based on advantages or benefits\n\nthey might have over others.\n\nReasons for selecting one cloud provider over another can be any of the\n\nfollowing:\n\ngeographical – when the physical geographical location of resources\n\nrequires cloud consumers to use local cloud providers for regulatory\n\npurposes\n\neconomical – prices or billing models\n\noperational – seeking higher capacity, more resiliency, or better peformance\n\nfunctional – looking for more features, specific capabilities required by the\n\ncloud consumer, or better quality in general\n\nFor cloud consumers to be able to make use of IT resources distributed\n\nacross different clouds, the cloud resource administrator uses a centralized\n\nremote administration system mechanism that connects to the management\n\nsystems of each individual cloud provider via their respective APIs (Figure\n\n13.20). This allows the cloud consumer to manage all cloud-based IT\n\nresources from a central location and then use and access them as easily as\n\nif if they were coming from a single cloud.\n\nFigure 13.20\n\nA cloud resource administrator utilizes a remote administration system\n\nmechanism to connect to the individual management systems of each\n\ndifferent cloud provider in order to manage their resources from a central\n\nmanagement location.\n\nThe ultimate business benefits resulting from the use of a multi-cloud\n\narchitecture can be very different for each individual cloud consumer.\n\nWhether the goal of an organizations is to maximize agility, minimize\n\ntechnology costs, or optimize its profit margins, a multi-cloud architecture\n\nwill allow the organization to mix-and-match and choose best-of-breed\n\ncloud-based resources and services from multiple, competing cloud\n\nproviders.\n\n13.10 Case Study Example\n\nAn in-house solution that ATN did not migrate to the\n\ncloud is the Remote Upload Module, a program that is\n\nused by their clients to upload accounting and legal\n\ndocuments to a central archive on a daily basis. Usage\n\npeaks occur without warning, since the quantity of\n\ndocuments received on a day-by-day basis is\n\nunpredictable.\n\nThe Remote Upload Module currently rejects upload\n\nattempts when it is operating at capacity, which is\n\nproblematic for users that need to archive certain\n\ndocuments before the end of a business day or prior to a\n\ndeadline.",
      "page_number": 631
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 651-672)",
      "start_page": 651,
      "end_page": 672,
      "detection_method": "synthetic",
      "content": "ATN decides to take advantage of its cloud-based\n\nenvironment by creating a cloud-bursting architecture\n\naround the on-premise Remote Upload Module service\n\nimplementation. This enables it to burst out into the\n\ncloud whenever on-premise processing thresholds are\n\nexceeded (Figures 13.21 and 13.22).\n\nFigure 13.21\n\nA cloud-based version of the on-premise Remote Upload\n\nModule service is deployed on ATN’s leased ready-made\n\nenvironment (1). The automated scaling listener\n\nmonitors service consumer requests (2).\n\nFigure 13.22\n\nThe automated scaling listener detects that service\n\nconsumer usage has exceeded the local Remote Upload\n\nModule service’s usage threshold, and begins diverting\n\nexcess requests to the cloud-based Remote Upload\n\nModule implementation (3). The cloud provider’s pay-\n\nper-use monitor tracks the requests received from the on-\n\npremise automated scaling listener to collect billing\n\ndata, and Remote Upload Module cloud service\n\ninstances are created on-demand via resource\n\nreplication (4).\n\nA “burst in” system is invoked after the service usage\n\nhas decreased enough so that service consumer requests\n\ncan be processed by the on-premise Remote Upload\n\nModule implementation again. Instances of the cloud\n\nservices are released, and no additional cloud-related\n\nusage fees are incurred.\n\nChapter 14\n\nAdvanced Cloud Architectures\n\n14.1 Hypervisor Clustering Architecture\n\n14.2 Virtual Server Clustering Architecture\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\n14.4 Non-Disruptive Service Relocation Architecture\n\n14.5 Zero Downtime Architecture\n\n14.6 Cloud Balancing Architecture\n\n14.7 Resilient Disaster Recovery Architecture\n\n14.8 Distributed Data Sovereignty Architecture\n\n14.9 Resource Reservation Architecture\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\n14.11 Rapid Provisioning Architecture\n\n14.12 Storage Workload Management Architecture\n\n14.13 Virtual Private Cloud Architecture\n\n14.14 Case Study Example\n\nThe following cloud technology architectures are explored in this chapter:\n\nHypervisor Clustering\n\nVirtual Server Clustering\n\nLoad Balanced Virtual Server Instances\n\nNon-Disruptive Service Relocation\n\nZero Downtime\n\nCloud Balancing\n\nResilient Disaster Recovery\n\nDistributed Data Sovereignty\n\nResource Reservation\n\nDynamic Failure Detection and Recovery\n\nRapid Provisioning\n\nStorage Workload Management\n\nVirtual Private Cloud\n\nThese models represent distinct and sophisticated architectural layers,\n\nseveral of which can be built upon the more foundational environments\n\nestablished by the architectures covered in Chapter 13. For each\n\narchitecture, the associated mechanisms are also documented.\n\n14.1 Hypervisor Clustering Architecture\n\nHypervisors can be responsible for creating and hosting multiple virtual\n\nservers. Because of this dependency, any failure conditions that affect a\n\nhypervisor can cascade to its virtual servers (Figure 14.1).\n\nFigure 14.1\n\nPhysical Server A is hosting a hypervisor that hosts Virtual Servers A and B\n\n(1). When Physical Server A fails, the hypervisor and two virtual servers\n\nconsequently fail as well (2).\n\nHeartbeats\n\nHeartbeats are -system-level messages exchanged\n\nbetween hypervisors, between hypervisors and -virtual\n\nservers, and between hypervisors and VIMs.\n\nThe hypervisor clustering architecture establishes a high-availability cluster\n\nof hypervisors across multiple physical servers. If a given hypervisor or its\n\nunderlying physical server becomes unavailable, the hosted virtual servers\n\ncan be moved to another physical server or hypervisor to maintain runtime\n\noperations (Figure 14.2).\n\nFigure 14.2\n\nPhysical Server A becomes unavailable and causes its hypervisor to fail.\n\nVirtual Server A is migrated to Physical Server B, which has another\n\nhypervisor that is part of the cluster to which Physical Server A belongs.\n\nThe hypervisor cluster is controlled via a central VIM, which sends regular\n\nheartbeat messages to the hypervisors to confirm that they are up and\n\nrunning. Unacknowledged heartbeat messages cause the VIM to initiate the\n\nlive VM migration program, in order to dynamically move the affected\n\nvirtual servers to a new host.\n\nLive VM Migration\n\nLive VM migration is a system that is capable of\n\nrelocating virtual servers or virtual server instances at\n\nruntime.\n\nThe hypervisor cluster uses a shared cloud storage device to live-migrate\n\nvirtual servers, as illustrated in Figures 14.3 to 14.6.\n\nFigure 14.3\n\nHypervisors are installed on Physical Servers A, B, and C (1). Virtual\n\nservers are created by the hypervisors (2). A shared cloud storage device\n\ncontaining virtual server configuration files is positioned in a shared cloud\n\nstorage device for access by all hypervisors (3). The hypervisor cluster is\n\nenabled on the three physical server hosts via a central VIM (4).\n\nFigure 14.4\n\nThe physical servers exchange heartbeat messages with one another and\n\nthe VIM according to a pre-defined schedule (5).\n\nFigure 14.5\n\nPhysical Server B fails and becomes unavailable, jeopardizing Virtual\n\nServer C (6). The other physical servers and the VIM stop receiving\n\nheartbeat messages from Physical Server B (7).\n\nFigure 14.6\n\nThe VIM chooses Physical Server C as the new host to take ownership of\n\nVirtual Server C after assessing the available capacity of other hypervisors\n\nin the cluster (8). Virtual Server C is live-migrated to the hypervisor\n\nrunning on Physical Server C, where restarting may be necessary before\n\nnormal operations can be resumed (9).\n\nIn addition to the hypervisor and resource cluster mechanisms that form the\n\ncore of this architectural model and the virtual servers that are protected by\n\nthe clustered environment, the following mechanisms can be incorporated:\n\nLogical Network Perimeter — The logical boundaries created by this\n\nmechanism ensure that none of the hypervisors of other cloud consumers\n\nare accidentally included in a given cluster.\n\nResource Replication — Hypervisors in the same cluster inform one\n\nanother about their status and availability. Updates on any changes that\n\noccur in the cluster, such as the creation or deletion of a virtual switch, need\n\nto be replicated to all of the hypervisors via the VIM.\n\n14.2 Virtual Server Clustering Architecture\n\nA virtual server clustering architecture represents the deployment of one or\n\nmore clusters of virtual servers on physical hosts running hypervisors. This\n\narchitecture is focused on leveraging the efficiency, resiliency, and\n\nscalability that a cloud can provide for clusters of servers through the use of\n\nvirtualization.\n\nIndividual virtual servers are instantiated on top of separate physical hosts\n\nrunning hypervisors (Figure 14.7). This provides the virtual infrastructure\n\non which virtual server clusters can be configured for different purposes,\n\nsuch as big data analytics, service-oriented architectures, distributed\n\nNoSQL databases, and advanced container management platforms.\n\nFigure 14.7\n\nPhysical Servers A, B and C are running hypervisors that allow multiple\n\nvirtual servers to be hosted on each. (These virtual servers are then\n\nconfigured by a resource cluster mechanism into clusters of virtual servers.)\n\nThe following mechanisms can be included in this architecture, in addition\n\nto the hypervisor, resource cluster, and virtual server:\n\nLogical Network Perimeter – A logical network perimeter ensures that the\n\nvirtual server cluster is enclosed in a connected environment that allows all\n\nof its nodes to communicate securely with each other.\n\nResource Replication – Virtual servers in the same cluster inform one\n\nanother about their status and availability. Updates on any changes that\n\noccur in the cluster, such as the creation or deletion of a virtual switch, need\n\nto be replicated to all virtual servers.\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\nKeeping cross-server workloads evenly balanced between physical servers\n\nwhose operation and management are isolated can be challenging. A\n\nphysical server can easily end up hosting more virtual servers or receive\n\nlarger workloads than its neighboring physical servers (Figure 14.8). Both\n\nphysical server over and under-utilization can increase dramatically over\n\ntime, leading to on-going performance challenges (for over-utilized servers)\n\nand constant waste (for the lost processing potential of under-utilized\n\nservers).\n\nFigure 14.8\n\nThree physical servers have to host different quantities of virtual server\n\ninstances, leading to both over-utilized and under-utilized servers.\n\nThe load balanced virtual server instances architecture establishes a\n\ncapacity watchdog system that dynamically calculates virtual server\n\ninstances and associated workloads, before distributing the processing\n\nacross available physical server hosts (Figure 14.9).\n\nFigure 14.9\n\nThe virtual server instances are more evenly distributed across the physical\n\nserver hosts.\n\nThe capacity watchdog system is comprised of a capacity watchdog cloud\n\nusage monitor, the live VM migration program, and a capacity planner. The\n\ncapacity watchdog monitor tracks physical and virtual server usage and\n\nreports any significant fluctuations to the capacity planner, which is\n\nresponsible for dynamically calculating physical server computing\n\ncapacities against virtual server capacity requirements. If the capacity\n\nplanner decides to move a virtual server to another host to distribute the\n\nworkload, the live VM migration program is signaled to move the virtual\n\nserver (Figures 14.10 to 14.12).",
      "page_number": 651
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 673-695)",
      "start_page": 673,
      "end_page": 695,
      "detection_method": "synthetic",
      "content": "Figure 14.10\n\nThe hypervisor cluster architecture provides the foundation upon which the\n\nload-balanced virtual server architecture is built (1). Policies and\n\nthresholds are defined for the capacity watchdog monitor (2), which\n\ncompares physical server capacities with virtual server processing (3). The\n\ncapacity watchdog monitor reports an over-utilization to the VIM (4).\n\nFigure 14.11\n\nThe VIM signals the load balancer to redistribute the workload based on\n\npre-defined thresholds (5). The load balancer initiates the live VM\n\nmigration program to move the virtual servers (6). Live VM migration\n\nmoves the selected virtual servers from one physical host to another (7).\n\nFigure 14.12\n\nThe workload is balanced across the physical servers in the cluster (8). The\n\ncapacity watchdog continues to monitor the workload and resource\n\nconsumption (9).\n\nThe following mechanisms can be included in this architecture, in addition\n\nto the hypervisor, resource clustering, virtual server, and (capacity\n\nwatchdog) cloud usage monitor:\n\nAutomated Scaling Listener — The automated scaling listener may be used\n\nto initiate the process of load balancing and to dynamically monitor\n\nworkload coming to the virtual servers via the hypervisors.\n\nLoad Balancer — The load balancer mechanism is responsible for\n\ndistributing the workload of the virtual servers between the hypervisors.\n\nLogical Network Perimeter — A logical network perimeter ensures that the\n\ndestination of a given relocated virtual server is in compliance with SLA\n\nand privacy regulations.\n\nResource Replication — The replication of virtual server instances may be\n\nrequired as part of the load balancing functionality.\n\n14.4 Non-Disruptive Service Relocation Architecture\n\nA cloud service can become unavailable for a number of reasons, such as:\n\nruntime usage demands that exceed its processing capacity\n\na maintenance update that mandates a temporary outage\n\npermanent migration to a new physical server host\n\nCloud service consumer requests are usually rejected if a cloud service\n\nbecomes unavailable, which can potentially result in exception conditions.\n\nRendering the cloud service temporarily unavailable to cloud consumers is\n\nnot preferred even if the outage is planned.\n\nThe non-disruptive service relocation architecture establishes a system by\n\nwhich a pre-defined event triggers the duplication or migration of a cloud\n\nservice implementation at runtime, thereby avoiding any disruption. Instead\n\nof scaling cloud services in or out with redundant implementations, cloud\n\nservice activity can be temporarily diverted to another hosting environment\n\nat runtime by adding a duplicate implementation onto a new host. Similarly,\n\ncloud service consumer requests can be temporarily redirected to a\n\nduplicate implementation when the original implementation needs to\n\nundergo a maintenance outage. The relocation of the cloud service\n\nimplementation and any cloud service activity can also be permanent to\n\naccommodate cloud service migrations to new physical server hosts.\n\nA key aspect of the underlying architecture is that the new cloud service\n\nimplementation is guaranteed to be successfully receiving and responding\n\nto cloud service consumer requests before the original cloud service\n\nimplementation is deactivated or removed. A common approach is for live\n\nVM migration to move the entire virtual server instance that is hosting the\n\ncloud service. The automated scaling listener and/or load balancer\n\nmechanisms can be used to trigger a temporary redirection of cloud service\n\nconsumer requests, in response to scaling and workload distribution\n\nrequirements. Either mechanism can contact the VIM to initiate the live VM\n\nmigration process, as shown in Figures 14.13 to 14.15.\n\nFigure 14.13\n\nThe automated scaling listener monitors the workload for a cloud service\n\n(1). The cloud service’s pre-defined threshold is reached as the workload\n\nincreases (2), causing the automated scaling listener to signal the VIM to\n\ninitiate relocation (3). The VIM uses the live VM migration program to\n\ninstruct both the origin and destination hypervisors to carry out runtime\n\nrelocation (4).\n\nFigure 14.14\n\nA second copy of the virtual server and its hosted cloud service are created\n\nvia the destination hypervisor on Physical Server B (5).\n\nFigure 14.15\n\nThe state of both virtual server instances is synchronized (6). The first\n\nvirtual server instance is removed from Physical Server A after cloud\n\nservice consumer requests are confirmed to be successfully exchanged with\n\nthe cloud service on Physical Server B (7). Cloud service consumer\n\nrequests are now only sent to the cloud service on Physical Server B (8).\n\nVirtual server migration can occur in one of the following two ways,\n\ndepending on the location of the virtual server’s disks and configuration:\n\nA copy of the virtual server disks is created on the destination host, if the\n\nvirtual server disks are stored on a local storage device or non-shared\n\nremote storage devices attached to the source host. After the copy has been\n\ncreated, both virtual server instances are synchronized and virtual server\n\nfiles are removed from the origin host.\n\nCopying the virtual server disks is unnecessary if the virtual server’s files\n\nare stored on a remote storage device that is shared between origin and\n\ndestination hosts. Ownership of the virtual server is simply transferred from\n\nthe origin to the destination physical server host, and the virtual server’s\n\nstate is automatically synchronized.\n\nThis architecture can be supported by the persistent virtual network\n\nconfigurations architecture, so that the defined network configurations of\n\nmigrated virtual servers are preserved to retain connection with the cloud\n\nservice consumers.\n\nBesides the automated scaling listener, load balancer, cloud storage device,\n\nhypervisor, and virtual server, other mechanisms that can be part of this\n\narchitecture include the following:\n\nCloud Usage Monitor — Different types of cloud usage monitors can be\n\nused to continuously track IT resource usage and system activity.\n\nPay-Per-Use Monitor — The pay-per-use monitor is used to collect data for\n\nservice usage cost calculations for IT resources at both source and\n\ndestination locations.\n\nResource Replication — The resource replication mechanism is used to\n\ninstantiate the shadow copy of the cloud service at its destination.\n\nSLA Management System — This management system is responsible for\n\nprocessing SLA data provided by the SLA monitor to obtain cloud service\n\navailability assurances, both during and after cloud service duplication or\n\nrelocation.\n\nSLA Monitor — This monitoring mechanism collects the SLA information\n\nrequired by the SLA management system, which may be relevant if\n\navailability guarantees rely on this architecture.\n\nNote\n\nThe non-disruptive service relocation technology\n\narchitecture conflicts and cannot be applied together with\n\nthe direct I/O access architecture covered in Chapter 15. A\n\nvirtual server with direct I/O access is locked into its\n\nphysical server host and cannot be moved to other hosts in\n\nthis fashion.\n\n14.5 Zero Downtime Architecture\n\nA physical server naturally acts as a single point of failure for the virtual\n\nservers it hosts. As a result, when the physical server fails or is\n\ncompromised, the availability of any (or all) hosted virtual servers can be\n\naffected. This makes the issuance of zero downtime guarantees by a cloud\n\nprovider to cloud consumers challenging.\n\nThe zero downtime architecture establishes a sophisticated failover system\n\nthat allows virtual servers to be dynamically moved to different physical\n\nserver hosts, in the event that their original physical server host fails (Figure\n\n14.16).\n\nFigure 14.16\n\nPhysical Server A fails triggering the live VM migration program to\n\ndynamically move Virtual Server A to Physical Server B.\n\nMultiple physical servers are assembled into a group that is controlled by a\n\nfault tolerance system capable of switching activity from one physical\n\nserver to another, without interruption. The live VM migration component\n\nis typically a core part of this form of high availability cloud architecture.\n\nThe resulting fault tolerance assures that, in case of physical server failure,\n\nhosted virtual servers will be migrated to a secondary physical server. All\n\nvirtual servers are stored on a shared volume (as per the persistent virtual\n\nnetwork configuration architecture) so that other physical server hosts in the\n\nsame group can access their files.\n\nBesides the failover system, cloud storage device, and virtual server\n\nmechanisms, the following mechanisms can be part of this architecture:\n\nAudit Monitor — This mechanism may be required to check whether the\n\nrelocation of virtual servers also relocates hosted data to prohibited\n\nlocations.\n\nCloud Usage Monitor — Incarnations of this mechanism are used to\n\nmonitor the actual IT resource usage of cloud consumers to help ensure that\n\nvirtual server capacities are not exceeded.\n\nHypervisor — The hypervisor of each affected physical server hosts the\n\naffected virtual servers.\n\nLogical Network Perimeter — Logical network perimeters provide and\n\nmaintain the isolation that is required to ensure that each cloud consumer\n\nremains within its own logical boundary subsequent to virtual server\n\nrelocation.\n\nResource Cluster — The resource cluster mechanism is applied to create\n\ndifferent types of active-active cluster groups that collaboratively improve\n\nthe availability of virtual server-hosted IT resources.\n\nResource Replication — This mechanism can create the new virtual server\n\nand cloud service instances upon primary virtual server failure.\n\n14.6 Cloud Balancing Architecture\n\nThe cloud balancing architecture establishes a specialized architectural\n\nmodel in which IT resources can be load-balanced across multiple clouds.\n\nThe cross-cloud balancing of cloud service consumer requests can help:\n\nimprove the performance and scalability of IT resources\n\nincrease the availability and reliability of IT resources\n\nimprove load-balancing and IT resource optimization\n\nCloud balancing functionality is primarily based on the combination of the\n\nautomated scaling listener and failover system mechanisms (Figure 14.17).\n\nMany more components (and possibly other mechanisms) can be part of a\n\ncomplete cloud balancing architecture.\n\nFigure 14.17\n\nAn automated scaling listener controls the cloud balancing process by\n\nrouting cloud service consumer requests to redundant implementations of\n\nCloud Service A distributed across multiple clouds (1). The failover system\n\ninstills resiliency within this architecture by providing cross-cloud failover\n\n(2).\n\nAs a starting point, the two mechanisms are utilized as follows:\n\nThe automated scaling listener redirects cloud service consumer requests to\n\none of several redundant IT resource implementations, based on current\n\nscaling and performance requirements.\n\nThe failover system ensures that redundant IT resources are capable of\n\ncross-cloud failover in the event of a failure within an IT resource or its\n\nunderlying hosting environment. IT resource failures are announced so that\n\nthe automated scaling listener can avoid inadvertently routing cloud service\n\nconsumer requests to unavailable or unstable IT resources.\n\nFor a cloud balancing architecture to function effectively, the automated\n\nscaling listener needs to be aware of all redundant IT resource\n\nimplementations within the scope of the cloud balanced architecture.\n\nNote that if the manual synchronization of cross-cloud IT resource\n\nimplementations is not possible, the resource replication mechanism may\n\nneed to be incorporated to automate the synchronization.\n\n14.7 Resilient Disaster Recovery Architecture\n\nNatural or man-made disasters can occur anytime and without warning. IT\n\nenterprises can establish disaster recovery strategies to ensure that, in case\n\nan event destroys or limits the functionality of important IT systems, a\n\nsecondary remote location is available with redundant implementations of\n\nthose systems that can then take over. This is the purpose of the resilient\n\ndisaster recovery architecture.\n\nCloud providers offer cloud-based IT resources with high levels of\n\navailability, which makes cloud environments ideal secondary sites for the\n\nprotection of on-premise IT resources from disasters. The ubiquitous access\n\nand resiliency cloud characteristics support this architecture when deployed\n\nin a public cloud, since resources located therein are available anytime,\n\nanywhere and accessible by many means.\n\nA resilient disaster recovery architecture uses resource replication\n\nmechanisms to create redundant copies of all the critical resources in an\n\nenterprise technology architecture. These copies are then placed in a remote\n\nlocation where they are expected to stay synchronized with their original\n\ncopies, ready to replace the originals in case a major catastrophe happens in\n\nthe original location (Figure 14.18).\n\nFigure 14.18\n\nAn organization uses a resource replication mechanism to create duplicate\n\nvirtual instances of its physical infrastructure in a pubic cloud. The storage\n\nreplication mechanism synchronizes on-premise data sources with their\n\nduplicates in the cloud.\n\nThe resource replication mechanism keeps the replicated IT resources in the\n\ncloud-based section of the architecture in constant synchronization with its\n\noriginal copies. Other mechanisms that can be part of this architecture\n\ninclude the following:\n\nHypervisor – The hypervisor mechanism allows physical hosts in the cloud\n\nenvironment selected for redundancy to host virtual servers that are replicas\n\nof the physical or virtual servers on-premise.\n\nVirtual Server – The virtual server mechanism is used to keep synchronized\n\nreplicas of original on-premise physical of virtual servers in the redundant\n\ncloud architecture.\n\nCloud Storage Device – The cloud storage device mechanism stores\n\nredundant copies of data from the original on-premise site in the replicated\n\ncloud-based site.\n\n14.8 Distributed Data Sovereignty Architecture\n\nRegulations regarding the appropriate governance of data, particularly\n\npersonal data, can vary across different countries and regions. Typically,\n\nsuch regulations require data holders to ensure that the data protected by the\n\nregulation is physically located within a particular geographical boundary.\n\nUsually cloud consumers are considered the official data holders of cloud-\n\nbased data, while cloud providers are not commonly required to comply\n\nwith these types of regulations.\n\nCloud providers commonly use sophisticated data replication systems to\n\nachieve redundancy levels that allow them to provide high availability for\n\nthe cloud storage services they offer. The replicas are often geographically\n\ndistributed in order to guarantee the highest level of availability possible\n\nbecause this distribution provides a higher degree of isolation from\n\npotential failures.\n\nHowever, geographical distribution might result in a cloud provider keeping\n\ncopies of protected data in locations that may be in violation of data\n\nprotection regulations that its cloud consumers are required to comply to.\n\nThe distributed data sovereignty architecture is a model that can used to\n\navoid this situation by ensuring that distributed data is in compliance with\n\nregulations.\n\nA distributed data sovereignty architecture is designed to guarantee that\n\nprotected data is stored in one or more specific physical locations.\n\nAn important design consideration for this architecture is making sure that\n\nthe data replication mechanisms used by the cloud provider can be\n\nconfigured for regulatory compliance. The distributed data sovereignty",
      "page_number": 673
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 696-718)",
      "start_page": 696,
      "end_page": 718,
      "detection_method": "synthetic",
      "content": "architecture further relies on a data governance management mechanism to\n\ncoordinate the appropriate storage of protected data within the region\n\nnecessary to comply with different local or regional regulations (Figure\n\n14.19).\n\nFigure 14.19\n\nAn organization uses a data governance manager mechanism to ensure that\n\nits cloud-based data is located in the region in which it must reside\n\naccording to regional data protection regulations.\n\nAdditionally, the following mechanisms are also part of this architecture:\n\nCloud Storage Device – The cloud storage device mechanism stores the\n\nprotected data in the location that allows the organization to comply with\n\nregional regulations.\n\nAudit Monitor – This mechanism may be required to check whether the\n\nlocal data has been replicated to prohibited locations.\n\nStorage Replication – The storage replication mechanism keeps copies of\n\ndata made for resiliency purposes in storage devices that are geographically\n\nlocated in accordance to data protection regulations.\n\nNote\n\nAn alternative approach is for cloud consumers to identify\n\nlocal cloud providers in every different region for which\n\nregulatory compliance is necessary, thereby establishing a\n\nmulti-cloud architecture (as described in Chapter 13) in\n\nwhich each cloud belongs to a different cloud provider.\n\n14.9 Resource Reservation Architecture\n\nDepending on how IT resources are designed for shared usage and\n\ndepending on their available levels of capacity, concurrent access can lead\n\nto a runtime exception condition called resource constraint. A resource\n\nconstraint is a condition that occurs when two or more cloud consumers\n\nhave been allocated to share an IT resource that does not have the capacity\n\nto accommodate the total processing requirements of the cloud consumers.\n\nAs a result, one or more of the cloud consumers encounter degraded\n\nperformance or may be rejected altogether. The cloud service itself may go\n\ndown, resulting in all cloud consumers being rejected.\n\nOther types of runtime conflicts can occur when an IT resource (especially\n\none not specifically designed to accommodate sharing) is concurrently\n\naccessed by different cloud service consumers. For example, nested and\n\nsibling resource pools introduce the notion of resource borrowing, whereby\n\none pool can temporarily borrow IT resources from other pools. A runtime\n\nconflict can be triggered when the borrowed IT resource is not returned due\n\nto prolonged usage by the cloud service consumer that is borrowing it. This\n\ncan inevitably lead back to the occurrence of resource constraints.\n\nThe resource reservation architecture establishes a system whereby one of\n\nthe following is set aside exclusively for a given cloud consumer (Figures\n\n14.20 to 14.22):\n\nsingle IT resource\n\nportion of an IT resource\n\nmultiple IT resources\n\nThis protects cloud consumers from each other by avoiding the\n\naforementioned resource constraint and resource borrowing conditions.\n\nFigure 14.20\n\nA physical resource group is created (1), from which a parent resource pool\n\nis created as per the resource pooling architecture (2). Two smaller child\n\npools are created from the parent resource pool, and resource limits are\n\ndefined using the resource management system (3). Cloud consumers are\n\nprovided with access to their own exclusive resource pools (4).\n\nFigure 14.21\n\nAn increase in requests from Cloud Consumer A results in more IT\n\nresources being allocated to that cloud consumer (5), meaning some IT\n\nresources need to be borrowed from Pool 2. The amount of borrowed IT\n\nresources is confined by the resource limit that was defined in Step 3, to\n\nensure that Cloud Consumer B will not face any resource constraints (6).\n\nFigure 14.22\n\nCloud Consumer B now imposes more requests and usage demands and\n\nmay soon need to utilize all available IT resources in the pool (6). The\n\nresource management system forces Pool 1 to release the IT resources and\n\nmove them back to Pool 2 to become available for Cloud Consumer B (7).\n\nThe creation of an IT resource reservation system can require involving the\n\nresource management system mechanism, which is used to define the usage\n\nthresholds for individual IT resources and resource pools. Reservations lock\n\nthe amount of IT resources that each pool needs to keep, with the balance of\n\nthe pool’s IT resources still available for sharing and borrowing. The remote\n\nadministration system mechanism is also used to enable front-end\n\ncustomization, so that cloud consumers have administration controls for the\n\nmanagement of their reserved IT resource allocations.\n\nThe types of mechanisms that are commonly reserved within this\n\narchitecture are cloud storage devices and virtual servers. Other\n\nmechanisms that may be part of the architecture can include:\n\nAudit Monitor — The audit monitor is used to check whether the resource\n\nreservation system is complying with cloud consumer auditing, privacy, and\n\nother regulatory requirements. For example, it may track the geographical\n\nlocation of reserved IT resources.\n\nCloud Usage Monitor — A cloud usage monitor may oversee the thresholds\n\nthat trigger the allocation of reserved IT resources.\n\nHypervisor — The hypervisor mechanism may apply reservations for\n\ndifferent cloud consumers to ensure that they are correctly allocated to their\n\nguaranteed IT resources.\n\nLogical Network Perimeter — This mechanism establishes the boundaries\n\nnecessary to ensure that reserved IT resources are made exclusively\n\navailable to cloud consumers.\n\nResource Replication — This component needs to stay informed about each\n\ncloud consumer’s limits for IT resource consumption, in order to replicate\n\nand provision new IT resource instances expediently.\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\nCloud-based environments can be comprised of vast quantities of IT\n\nresources that are simultaneously accessed by numerous cloud consumers.\n\nAny of those IT resources can experience failure conditions that require\n\nmore than manual intervention to resolve. Manually administering and\n\nsolving IT resource failures is generally inefficient and impractical.\n\nThe dynamic failure detection and recovery architecture establishes a\n\nresilient watchdog system to monitor and respond to a wide range of pre-\n\ndefined failure scenarios (Figures 14.23 and 14.24). This system notifies\n\nand escalates the failure conditions that it cannot automatically resolve\n\nitself. It relies on a specialized cloud usage monitor called the intelligent\n\nwatchdog monitor to actively track IT resources and take pre-defined\n\nactions in response to pre-defined events.\n\nFigure 14.23\n\nThe intelligent watchdog monitor keeps track of cloud consumer requests\n\n(1) and detects that a cloud service has failed (2).\n\nFigure 14.24\n\nThe intelligent watchdog monitor notifies the watchdog system (3), which\n\nrestores the cloud service based on pre-defined policies. The cloud service\n\nresumes its runtime operation (4).\n\nThe resilient watchdog system performs the following five core functions:\n\nwatching\n\ndeciding upon an event\n\nacting upon an event\n\nreporting\n\nescalating\n\nSequential recovery policies can be defined for each IT resource to\n\ndetermine the steps that the intelligent watchdog monitor needs to take\n\nwhen a failure condition occurs. For example, a recovery policy can state\n\nthat one recovery attempt needs to be automatically carried out before\n\nissuing a notification (Figure 14.25).\n\nFigure 14.25\n\nIn the event of a failure, the intelligent watchdog monitor refers to its pre-\n\ndefined policies to recover the cloud service step-by-step, escalating the\n\nprocess when a problem proves to be deeper than expected.\n\nSome of the actions the intelligent watchdog monitor commonly takes to\n\nescalate an issue include:\n\nrunning a batch file\n\nsending a console message\n\nsending a text message\n\nsending an email message\n\nsending an SNMP trap\n\nlogging a ticket\n\nThere are varieties of programs and products that can act as intelligent\n\nwatchdog monitors. Most can be integrated with standard ticketing and\n\nevent management systems.\n\nThis architectural model can further incorporate the following mechanisms:\n\nAudit Monitor — This mechanism is used to track whether data recovery is\n\ncarried out in compliance with legal or policy requirements.\n\nFailover System — The failover system mechanism is usually used during\n\nthe initial attempts to recover failed IT resources.\n\nSLA Management System and SLA Monitor — Since the functionality\n\nachieved by applying this architecture is closely associated with SLA\n\nguarantees, the system commonly relies on the information that is managed\n\nand processed by these mechanisms.\n\n14.11 Rapid Provisioning Architecture\n\nA conventional provisioning process can involve a number of tasks that are\n\ntraditionally completed manually by administrators and technology experts\n\nthat prepare the requested IT resources as per pre-packaged specifications\n\nor custom client requests. In cloud environments, where higher volumes of\n\ncustomers are serviced and where the average customer requests higher\n\nvolumes of IT resources, manual provisioning processes are inadequate and\n\ncan even lead to unreasonable risk due to human error and inefficient\n\nresponse times.\n\nFor example, a cloud consumer that requests the installation, configuration,\n\nand updating of twenty-five Windows servers with several applications\n\nrequires that half of the applications be identical installations, while the\n\nother half be customized. Each operating system deployment can take up to\n\n30 minutes, followed by additional time for security patches and operating\n\nsystem updates that require server rebooting. The applications finally need\n\nto be deployed and configured. Using a manual or semi-automated\n\napproach requires excessive amounts of time, and introduces a probability\n\nof human error that increases with each installation.\n\nThe rapid provisioning architecture establishes a system that automates the\n\nprovisioning of a wide range of IT resources, either individually or as a\n\ncollective. The underlying technology architecture for rapid IT resource\n\nprovisioning can be sophisticated and complex, and relies on a system\n\ncomprised of an automated provisioning program, rapid provisioning\n\nengine, and scripts and templates for on-demand provisioning.\n\nBeyond the components displayed in Figure 14.26, many additional\n\narchitectural artifacts are available to coordinate and automate the different\n\naspects of IT resource provisioning, such as:\n\nServer Templates — Templates of virtual image files that are used to\n\nautomate the instantiation of new virtual servers.\n\nFigure 14.26\n\nA cloud resource administrator requests a new cloud service through the\n\nself-service portal (1). The self-service portal passes the request to the\n\nautomated service provisioning program installed on the virtual server (2),\n\nwhich passes the necessary tasks to be performed to the rapid provisioning\n\nengine (3). The rapid provisioning engine announces when the new cloud\n\nservice is ready (4). The automated service provisioning program finalizes\n\nand publishes the cloud service on the usage and administration portal for\n\ncloud consumer access (5).\n\nServer Images — These images are similar to virtual server templates, but\n\nare used to provision physical servers.\n\nApplication Packages — Collections of applications and other software that\n\nare packaged for automated deployment.\n\nApplication Packager — The software used to create application packages.\n\nCustom Scripts — Scripts that automate administrative tasks, as part of an\n\nintelligent automation engine.\n\nSequence Manager — A program that organizes sequences of automated\n\nprovisioning tasks.\n\nSequence Logger — A component that logs the execution of automated\n\nprovisioning task sequences.\n\nOperating System Baseline — A configuration template that is applied after\n\nthe operating system is installed, to quickly prepare it for usage.\n\nApplication Configuration Baseline — A configuration template with the\n\nsettings and environmental parameters that are needed to prepare new\n\napplications for use.\n\nDeployment Data Store — The repository that stores virtual images,\n\ntemplates, scripts, baseline configurations, and other related data.\n\nThe following step-by-step description helps provide some insight into the\n\ninner workings of a rapid provisioning engine, involving a number of the\n\npreviously listed system components:\n\n. A cloud consumer requests a new server through the self-service portal.\n\n. The sequence manager forwards the request to the deployment engine for\n\nthe preparation of an operating system.\n\n. The deployment engine uses the virtual server templates for provisioning if\n\nthe request is for a virtual server. Otherwise, the deployment engine sends\n\nthe request to provision a physical server.\n\n. The pre-defined image for the requested type of operating system is used\n\nfor the provisioning of the operating system, if available. Otherwise, the\n\nregular deployment process is executed to install the operating system.\n\n. The deployment engine informs the sequence manager when the operating\n\nsystem is ready.\n\n. The sequence manager updates and sends the logs to the sequence logger\n\nfor storage.\n\n. The sequence manager requests that the deployment engine apply the\n\noperating system baseline to the provisioned operating system.\n\n. The deployment engine applies the requested operating system baseline.\n\n. The deployment engine informs the sequence manager that the operating\n\nsystem baseline has been applied.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.\n\nThe sequence manager requests that the deployment engine install the\n\napplications.\n\n. The deployment engine deploys the applications on the provisioned server.\n\n. The deployment engine informs the sequence manager that the applications\n\nhave been installed.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.\n\n. The sequence manager requests that the deployment engine apply the\n\napplication’s configuration baseline.\n\n. The deployment engine applies the configuration baseline.\n\n. The deployment engine informs the sequence manager that the\n\nconfiguration baseline has been applied.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.\n\nThe cloud storage device mechanism is used to provide storage for\n\napplication baseline information, templates, and scripts, while the\n\nhypervisor rapidly creates, deploys, and hosts the virtual servers that are\n\neither provisioned themselves, or host other provisioned IT resources. The\n\nresource replication mechanism is usually used to generate replicated\n\ninstances of IT resources in response to rapid provisioning requirements.\n\n14.12 Storage Workload Management Architecture\n\nOver-utilized cloud storage devices increase the workload on the storage\n\ncontroller and can cause a range of performance challenges. Conversely,\n\ncloud storage devices that are under-utilized are wasteful due to lost\n\nprocessing and storage capacity potential (Figure 14.27).\n\nFigure 14.27\n\nAn unbalanced cloud storage architecture has six storage LUNs in Storage\n\n1 for cloud consumers to use, while Storage 2 is hosting one LUN and\n\nStorage 3 is hosting two. The majority of the workload ends up with Storage\n\n1, since it is hosting the most LUNs.\n\nLUN Migration\n\nLUN migration is a specialized storage program that is\n\nused to move LUNs from one storage device to another\n\nwithout interruption, while remaining transparent to\n\ncloud consumers.\n\nThe storage workload management architecture enables LUNs to be evenly\n\ndistributed across available cloud storage devices, while a storage capacity\n\nsystem is established to ensure that runtime workloads are evenly\n\ndistributed across the LUNs (Figure 14.28).",
      "page_number": 696
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 719-740)",
      "start_page": 719,
      "end_page": 740,
      "detection_method": "synthetic",
      "content": "Figure 14.28\n\nLUNs are dynamically distributed across cloud storage devices, resulting in\n\nmore even distribution of associated types of workloads.\n\nCombining cloud storage devices into a group allows LUN data to be\n\ndistributed between available storage hosts equally. A storage management\n\nsystem is configured and an automated scaling listener is positioned to\n\nmonitor and equalize runtime workloads among the grouped cloud storage\n\ndevices, as illustrated in Figures 14.29 to 14.31.\n\nFigure 14.29\n\nThe storage capacity system and storage capacity monitor are configured to\n\nsurvey three storage devices in realtime, whose workload and capacity\n\nthresholds are pre-defined (1). The storage capacity monitor determines\n\nthat the workload on Storage 1 is reaching its threshold (2).\n\nFigure 14.30\n\nThe storage capacity monitor informs the storage capacity system that\n\nStorage 1 is over-utilized (3). The storage capacity system identifies the\n\nLUNs to be moved from Storage 1 (4).\n\nFigure 14.31\n\nThe storage capacity system calls for LUN migration to move some of the\n\nLUNs from Storage 1 to the other two storage devices (5). LUN migration\n\ntransitions LUNs to Storage 2 and 3 to balance the workload (6).\n\nThe storage capacity system can keep the hosting storage device in power-\n\nsaving mode for the periods when the LUNs are being accessed less\n\nfrequently or only at specific times.\n\nSome other mechanisms that can be included in the storage workload\n\nmanagement architecture to accompany the cloud storage device are as\n\nfollows:\n\nAudit Monitor — This monitoring mechanism is used to check for\n\ncompliance with regulatory, privacy, and security requirements, since the\n\nsystem established by this architecture can physically relocate data.\n\nAutomated Scaling Listener — The automated scaling listener is used to\n\nwatch and respond to workload fluctuations.\n\nCloud Usage Monitor — In addition to the capacity workload monitor,\n\nspecialized cloud usage monitors are used to track LUN movements and\n\ncollect workload distribution statistics.\n\nLoad Balancer — This mechanism can be added to horizontally balance\n\nworkloads across available cloud storage devices.\n\nLogical Network Perimeter — Logical network perimeters provide levels of\n\nisolation so that cloud consumer data that undergoes relocation remains\n\ninaccessible to unauthorized parties.\n\n14.13 Virtual Private Cloud Architecture\n\nThe virtual private cloud architecture establishes a private cloud with\n\nunderlying infrastructure that belongs to a public cloud provider but that is\n\nexclusively dedicated to one specific cloud consumer for whom the private\n\ncloud is delivered. This can be useful for an organization who wants to have\n\na private cloud but does not have the necessary infrastructure to support it\n\non-premise.\n\nTo the cloud consumer with exclusive access, this is a private cloud.\n\nHowever, from the cloud provider’s point of view, it is part of its\n\ninfrastructure, which is why it is referred to as a “virtual” private cloud. The\n\nunderlying physical resources, which are typically virtualized for more\n\nefficient utilization, are not shared with other cloud consumers. Instead,\n\nthey are solely dedicated to the “owner” (the cloud consumer) of the virtual\n\nprivate cloud.\n\nThe physical resources used to build this architecture need special isolation\n\nfrom the rest of the cloud provider’s infrastructure, including a separate\n\nphysical network to which the cloud consumer connects via a secure virtual\n\nprivate network (VPN), as shown in Figure 14.32. Sometimes this VPN can\n\nbe replaced by a dedicated physical link from the cloud provider to the\n\ncloud consumer (although that could result in a much more expensive\n\narchitecture).\n\nFigure 14.32\n\nA virtual private cloud architecture utilizes physical resources from a public\n\ncloud provider dedicated for exclusive use by a specific cloud consumer,\n\naccessible via a secure connection, such as can be provided by a VPN.\n\nThe mechanisms involved in this architecture are the same mechanisms\n\nrequired to build any other private cloud, with the exception of the VPN,\n\nwhich is normally not required when a private cloud is deployed on\n\ninfrastructure that resides within the physical boundary of an organization.\n\nThese mechanisms include:\n\nHypervisor – The hypervisor mechanism provides an efficient way for\n\nutilizing physical servers by allowing for the deployment of virtual servers\n\non physical servers.\n\nVirtual Server – The virtual server mechanism provides the most common\n\ntype of resource used in cloud environments to host workloads of all types.\n\nCloud Storage Device – The cloud storage device mechanism provides\n\nstorage capabilities within the virtual private cloud.\n\nVirtual Switch – The virtual switch mechanism provides connectivity\n\nbetween virtual servers and the rest of the resources in the virtual private\n\ncloud.\n\n14.14 Case Study Example\n\nInnovartus is leasing two cloud-based environments\n\nfrom two different cloud providers, and intends to take\n\nadvantage of this opportunity to establish a pilot cloud-\n\nbalancing architecture for its Role Player cloud service.\n\nAfter assessing its requirements against the respective\n\nclouds, Innovartus’ cloud architects produce a design\n\nspecification that is based on each cloud having multiple\n\nimplementations of the cloud service. This architecture\n\nincorporates separate automated scaling listener and\n\nfailover system implementations, together with a central\n\nload balancer mechanism (Figure 14.33).\n\nFigure 14.33\n\nA load-balancing service agent routes cloud service\n\nconsumer requests according to a pre-defined algorithm\n\n(1). Requests are received by the local or external\n\nautomated scaling listener (2A, 2B), which forward each\n\nrequest to a cloud service implementation (3). Failover\n\nsystem monitors are used to detect and respond to cloud\n\nservice failure (4).\n\nThe load balancer distributes cloud service consumer\n\nrequests across clouds using a workload distribution\n\nalgorithm, while each cloud’s automated scaling listener\n\nroutes requests to local cloud service implementations.\n\nThe failover systems can failover to the redundant cloud\n\nservice implementations that are both within and across\n\nclouds. Inter-cloud failover is carried out primarily when\n\nlocal cloud service implementations are nearing their\n\nprocessing thresholds, or if a cloud is encountering a\n\nsevere platform failure.\n\nChapter 15\n\nSpecialized Cloud Architectures\n\n15.1 Direct I/O Access Architecture\n\n15.2 Direct LUN Access Architecture\n\n15.3 Dynamic Data Normalization Architecture\n\n15.4 Elastic Network Capacity Architecture\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\n15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\n15.7 Load Balanced Virtual Switches Architecture\n\n15.8 Multipath Resource Access Architecture\n\n15.9 Persistent Virtual Network Configuration Architecture\n\n15.10 Redundant Physical Connection for Virtual Servers Architecture\n\n15.11 Storage Maintenance Window Architecture\n\n15.12 Edge Computing Architecture\n\n15.13 Fog Computing Architecture\n\nThe architectural models that are covered in this chapter span a broad range\n\nof functional areas and topics to offer creative combinations of mechanisms\n\nand specialized components.\n\nThe following architectures are covered:\n\nDirect I/O Access\n\nDirect LUN Access\n\nDynamic Data Normalization\n\nElastic Network Capacity\n\nCross-Storage Device Vertical Tiering\n\nIntra-Storage Device Vertical Data Tiering\n\nLoad Balanced Virtual Switches\n\nMultipath Resource Access\n\nPersistent Virtual Network Configuration\n\nRedundant Physical Connection for Virtual Servers\n\nStorage Maintenance Window\n\nEdge Computing\n\nFog Computing\n\nWhere applicable, the involvement of related cloud mechanisms is\n\ndescribed.\n\n15.1 Direct I/O Access Architecture\n\nAccess to the physical I/O cards that are installed on a physical server is\n\nusually provided to hosted virtual servers via a hypervisor-based layer of\n\nprocessing called I/O virtualization. However, virtual servers sometimes\n\nneed to connect to and use I/O cards without any hypervisor interaction or\n\nemulation.\n\nWith the direct I/O access architecture, virtual servers are allowed to\n\ncircumvent the hypervisor and directly access the physical server’s I/O card\n\nas an alternative to emulating a connection via the hypervisor (Figures 15.1\n\nto 15.3).\n\nFigure 15.1\n\nCloud service consumers access a virtual server, which accesses a database\n\non a SAN storage LUN (1). Connectivity from the virtual server to the\n\ndatabase occurs via a virtual switch.\n\nFigure 15.2\n\nThere is an increase in the amount of cloud service consumer requests (2),\n\ncausing the bandwidth and performance of the virtual switch to become\n\ninadequate (3).\n\nFigure 15.3\n\nThe virtual server bypasses the hypervisor to connect to the database server\n\nvia a direct physical link to the physical server (4). The increased workload\n\ncan now be properly handled.\n\nTo achieve this solution and access the physical I/O card without hypervisor\n\ninteraction, the host CPU needs to support this type of access with the\n\nappropriate drivers installed on the virtual server. The virtual server can\n\nthen recognize the I/O card as a hardware device after the drivers are\n\ninstalled.\n\nOther mechanisms that can be involved in this architecture in addition to\n\nthe virtual server and hypervisor include:\n\nCloud Usage Monitor — The cloud service usage data that is collected by\n\nruntime monitors can include and separately classify direct I/O access.\n\nLogical Network Perimeter — The logical network perimeter ensures that\n\nthe allocated physical I/O card does not allow cloud consumers to access\n\nother cloud consumers’ IT resources.\n\nPay-Per-Use Monitor — This monitor collects usage cost information for\n\nthe allocated physical I/O card.\n\nResource Replication — Replication technology is used to replace virtual\n\nI/O cards with physical I/O cards.\n\n15.2 Direct LUN Access Architecture\n\nStorage LUNs are typically mapped via a host bus adapter (HBA) on the\n\nhypervisor, with the storage space emulated as file-based storage to virtual\n\nservers (Figure 15.4). However, virtual servers sometimes need direct\n\naccess to RAW block-based storage. For example, access via an emulated\n\nadapter is insufficient when a cluster is implemented and a LUN is used as\n\nthe shared cluster storage device between two virtual servers.\n\nFigure 15.4\n\nThe cloud storage device is installed and configured (1). The LUN mapping\n\nis defined so that each hypervisor has access to its own LUN and can also\n\nsee all of the mapped LUNs (2). The hypervisor shows the mapped LUNs to\n\nthe virtual servers as normal file-based storage to be used as such (3).\n\nThe direct LUN access architecture provides virtual servers with LUN\n\naccess via a physical HBA card, which is effective because virtual servers\n\nin the same cluster can use the LUN as a shared volume for clustered\n\ndatabases. After implementing this solution, the virtual servers’ physical\n\nconnectivity to the LUN and cloud storage device is enabled by the physical\n\nhosts.\n\nThe LUNs are created and configured on the cloud storage device for LUN\n\npresentation to the hypervisors. The cloud storage device needs to be\n\nconfigured using raw device mapping to make the LUNs visible to the\n\nvirtual servers as a block-based RAW SAN LUN, which is unformatted, un-\n\npartitioned storage. The LUN needs to be represented with a unique LUN\n\nID to be used by all of the virtual servers as shared storage. Figures 15.5\n\nand 15.6 illustrate how virtual servers are given direct access to block-based\n\nstorage LUNs.\n\nFigure 15.5\n\nThe cloud storage device is installed and configured (1). The required\n\nLUNs are created and presented to the hypervisors (2), which map the\n\npresented LUNs directly to the virtual servers (3). The virtual servers can\n\nsee the LUNs as RAW block-based storage and can access them directly (4).",
      "page_number": 719
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 741-759)",
      "start_page": 741,
      "end_page": 759,
      "detection_method": "synthetic",
      "content": "Figure 15.6\n\nThe virtual servers’ storage commands are received by the hypervisors (5),\n\nwhich process and forward the requests to the storage processor (6).\n\nBesides the virtual server, hypervisor, and cloud storage device, the\n\nfollowing mechanisms can be incorporated into this architecture:\n\nCloud Usage Monitor — This monitor tracks and collects storage usage\n\ninformation that pertains to the direct usage of LUNs.\n\nPay-Per-Use Monitor — The pay-per-use monitor collects and separately\n\nclassifies usage cost information for direct LUN access.\n\nResource Replication — This mechanism relates to how virtual servers\n\ndirectly access block-based storage in replacement of file-based storage.\n\n15.3 Dynamic Data Normalization Architecture\n\nRedundant data can cause a range of issues in cloud-based environments,\n\nsuch as:\n\nincreased time required to store and catalog files\n\nincreased required storage and backup space\n\nincreased costs due to increased data volume\n\nincreased time required for replication to secondary storage\n\nincreased time required to backup data\n\nFor example, if a cloud consumer copies 100 MB of files onto a cloud\n\nstorage device and the data is redundantly copied ten times, the\n\nconsequences can be considerable:\n\nThe cloud consumer will be charged for using 10 x 100 MB of storage\n\nspace, even though only 100 MB of unique data was actually stored.\n\nThe cloud provider needs to provide an unnecessary 900 MB of space in the\n\nonline cloud storage device and any backup storage systems.\n\nSignificantly more time is required to store and catalog data.\n\nData replication duration and performance are unnecessarily taxed\n\nwhenever the cloud provider performs a site recovery, since 1,000 MB need\n\nto be replicated instead of 100 MB.\n\nThese impacts can be significantly amplified in multitenant public clouds.\n\nThe dynamic data normalization architecture establishes a de-duplication\n\nsystem, which prevents cloud consumers from inadvertently saving\n\nredundant copies of data by detecting and eliminating redundant data on\n\ncloud storage devices. This system can be applied to both block and file-\n\nbased storage devices, although it is most effective on the former. This de-\n\nduplication system checks each received block to determine whether it is\n\nredundant with a block that has already been received. Redundant blocks\n\nare replaced with pointers to the equivalent blocks that are already in\n\nstorage (Figure 15.7).\n\nFigure 15.7\n\nData sets containing redundant data are unnecessarily bloating storage\n\n(left). The data de-duplication system normalizes the data, so that only\n\nunique data is stored (right).\n\nThe de-duplication system examines received data prior to passing it to\n\nstorage controllers. As part of the examination process, a hash code is\n\nassigned to every piece of data that has been processed and stored. An index\n\nof hashes and pieces is also maintained. As a result, the generated hash of a\n\nnewly received block of data is compared with the hashes in storage to\n\ndetermine whether it is a new or duplicate data block. New blocks are\n\nsaved, while duplicate data is eliminated and a pointer to the original data\n\nblock is created and saved instead.\n\nThis architectural model can be used for both disk storage and backup tape\n\ndrives. One cloud provider can decide to prevent redundant data only on\n\nbackup cloud storage devices, while another can more aggressively\n\nimplement the data de-duplication system on all of its cloud storage\n\ndevices. There are different methods and algorithms for comparing blocks\n\nof data to confirm their duplicity with other blocks.\n\n15.4 Elastic Network Capacity Architecture\n\nEven if IT resources are scaled on-demand by a cloud platform,\n\nperformance and scalability can still be inhibited when remote access to the\n\nIT resources is impacted by network bandwidth limitations (Figure 15.8).\n\nFigure 15.8\n\nA lack of available bandwidth causes performance issues for cloud\n\nconsumer requests.\n\nThe elastic network capacity architecture establishes a system in which\n\nadditional bandwidth is allocated dynamically to the network to avoid\n\nruntime bottlenecks. This system ensures that each cloud consumer is using\n\na different set of network ports to isolate individual cloud consumer traffic\n\nflows.\n\nThe automated scaling listener and intelligent automation engine scripts are\n\nused to detect when traffic reaches the bandwidth threshold, and to\n\ndynamically allocate additional bandwidth and/or network ports when\n\nrequired.\n\nThe cloud architecture can be equipped with a network resource pool\n\ncontaining network ports that are made available for shared usage. The\n\nautomated scaling listener monitors workload and network traffic, and\n\nsignals the intelligent automation engine to modify the number of allocated\n\nnetwork ports and/or bandwidth in response to usage fluctuations.\n\nNote that when this architectural model is implemented at the virtual switch\n\nlevel, the intelligent automation engine may need to run a separate script\n\nthat adds physical uplinks to the virtual switch specifically. Alternatively,\n\nthe direct I/O access architecture can also be incorporated to increase\n\nnetwork bandwidth that is allocated to the virtual server.\n\nIn addition to the automated scaling listener, the following mechanisms can\n\nbe part of this architecture:\n\nCloud Usage Monitor — These monitors are responsible for tracking elastic\n\nnetwork capacity before, during, and after scaling.\n\nHypervisor — The hypervisor provides virtual servers with access to the\n\nphysical network, via virtual switches and physical uplinks.\n\nLogical Network Perimeter — This mechanism establishes the boundaries\n\nthat are needed to provide individual cloud consumers with their allocated\n\nnetwork capacity.\n\nPay-Per-Use Monitor — This monitor keeps track of any billing-related\n\ndata that pertains to dynamic network bandwidth consumption.\n\nResource Replication — Resource replication is used to add network ports\n\nto physical and virtual servers, in response to workload demands.\n\nVirtual Server — Virtual servers host the IT resources and cloud services to\n\nwhich network resources are allocated and are themselves affected by the\n\nscaling of network capacity.\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\nCloud storage devices are sometimes unable to accommodate the\n\nperformance requirements of cloud consumers, and have more data\n\nprocessing power or bandwidth added to increase IOPS. These conventional\n\nmethods of vertical scaling are usually inefficient and time-consuming to\n\nimplement, and can become wasteful when the increased capacity is no\n\nlonger required.\n\nThe scenario in Figures 15.9 and 15.10 depicts an approach in which a\n\nnumber of requests for access to a LUN has increased, requiring its manual\n\ntransfer to a high-performance cloud storage device.\n\nFigure 15.9\n\nA cloud provider installs and configures a cloud storage device (1) and\n\ncreates LUNs that are made available to the cloud service consumers for\n\nusage (2). The cloud service consumers initiate data access requests to the\n\ncloud storage device (3), which forwards the requests to one of the LUNs\n\n(4).\n\nFigure 15.10\n\nThe number of requests increases, resulting in high storage bandwidth and\n\nperformance demands (5). Some of the requests are rejected, or time out\n\ndue to performance capacity limitations within the cloud storage device (6).\n\nThe cross-storage device vertical tiering architecture establishes a system\n\nthat survives bandwidth and data processing power constraints by vertically\n\nscaling between storage devices that have different capacities. LUNs can\n\nautomatically scale up and down across multiple devices in this system so\n\nthat requests can use the appropriate storage device level to perform cloud\n\nconsumer tasks.\n\nNew cloud storage devices with increased capacity can also be made\n\navailable, even if the automated tiering technology can move data to cloud\n\nstorage devices with the same storage processing capacity. For example,\n\nsolid-state drives (SSDs) can be suitable devices for data processing power\n\nupgrades.\n\nThe automated scaling listener monitors the requests that are sent to specific\n\nLUNs, and signals the storage management program to move the LUN to a\n\nhigher capacity device once it identifies a predefined threshold has been\n\nreached. Service interruption is prevented because there is never a\n\ndisconnection during the transfer. The original device remains up and\n\nrunning, while the LUN data scales up to another device. Cloud consumer\n\nrequests are automatically redirected to the new cloud storage device as\n\nsoon as the scaling is completed (Figures 15.11 to 15.13).\n\nFigure 15.11\n\nThe lower capacity primary cloud storage device is responding to cloud\n\nservice consumer storage requests (1). A secondary cloud storage device\n\nwith higher capacity and performance is installed (2). The LUN migration\n\n(3) is configured via the storage management program that is configured to\n\ncategorize the storage based on device performance (4). Thresholds are\n\ndefined in the automated scaling listener that is monitoring the requests (5).\n\nCloud service consumer requests are received by the storage service\n\ngateway and sent to the primary cloud storage device (6).\n\nFigure 15.12\n\nThe number of cloud service consumer requests reaches the predefined\n\nthreshold (7), and the automated scaling listener notifies the storage\n\nmanagement program that scaling is required (8). The storage management\n\nprogram calls LUN migration to move the cloud consumer’s LUN to the\n\nsecondary, higher capacity storage device (9) and the LUN migration\n\nperforms this move (10).\n\nFigure 15.13\n\nThe storage service gateway forwards the cloud service consumer requests\n\nfrom the LUN to the new cloud storage device (11). The original LUN is\n\ndeleted from the lower capacity device via the storage management\n\nprogram and LUN migration (12). The automated scaling listener monitors\n\ncloud service consumer requests to ensure that the request volume\n\ncontinues to require access to the higher capacity secondary storage for the\n\nmigrated LUN (13).\n\nIn addition to the automated scaling listener and cloud storage device, the\n\nmechanisms that can be incorporated in this technology architecture\n\ninclude:\n\nAudit Monitor — The auditing performed by this monitor checks whether\n\nthe relocation of cloud consumer data does not conflict with any legal or\n\ndata privacy regulations or policies.\n\nCloud Usage Monitor — This infrastructure mechanism represents various\n\nruntime monitoring requirements for tracking and recording data transfer\n\nand usage, at both source and destination storage locations.\n\nPay-Per-Use Monitor — Within the context of this architecture, the pay-\n\nper-use monitor collects storage usage information on both source and\n\ndestination locations, as well as IT resource usage information for carrying\n\nout cross-storage tiering functionality.\n\n15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\nSome cloud consumers may have distinct data storage requirements that\n\nrestrict the data’s physical location to a single cloud storage device.\n\nDistribution across other cloud storage devices may be disallowed due to\n\nsecurity, privacy, or various legal reasons. This type of limitation can\n\nimpose severe scalability limitations upon the device’s storage and\n\nperformance capacity. These limitations can further cascade to any cloud\n\nservices or applications that are dependent upon the use of the cloud storage\n\ndevice.\n\nThe intra-storage device vertical data tiering architecture establishes a\n\nsystem to support vertical scaling within a single cloud storage device. This\n\nintra-device scaling system optimizes the availability of different disk types\n\nwith different capacities (Figure 15.14).\n\nFigure 15.14\n\nThe cloud intra-storage device system vertically scales through disk types\n\ngraded into different tiers (1). Each LUN is moved to a tier that",
      "page_number": 741
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 760-781)",
      "start_page": 760,
      "end_page": 781,
      "detection_method": "synthetic",
      "content": "corresponds to its processing and storage requirements (2).\n\nThis cloud storage architecture requires the use of a complex storage device\n\nthat supports different types of hard disks, especially high-performance\n\ndisks like SATAs, SASs, and SSDs. The disk types are organized into\n\ngraded tiers so that LUN migration can vertically scale the device based on\n\nthe allocation of disk types, which align with the processing and capacity\n\nrequirements.\n\nData load conditions and definitions are set after disk categorization so that\n\nthe LUNs can move to either a higher or lower grade, depending on which\n\npredefined conditions are met. These thresholds and conditions are used by\n\nthe automated scaling listener when monitoring runtime data processing\n\ntraffic (Figures 15.15 to 15.17).\n\nFigure 15.15\n\nDifferent types of hard disks are installed in the enclosures of a cloud\n\nstorage device (1). Similar disk types are grouped into tiers to create\n\ndifferent grades of disk groups based on I/O performance (2).\n\nFigure 15.16\n\nTwo LUNs have been created on Disk Group 1 (3). The automated scaling\n\nlistener monitors the requests in relation to pre-defined thresholds (4). The\n\npay-per-use monitor tracks the actual amount of disk usage, based on free\n\nspace and disk group performance (5). The automated scaling listener\n\ndetermines that the number of requests is reaching a threshold, and informs\n\nthe storage management program that the LUN needs to be moved to a\n\nhigher performance disk group (6). The storage management program\n\nsignals the LUN migration program to perform the required move (7). The\n\nLUN migration program works with the storage controller to move the LUN\n\nto the higher capacity Disk Group 2 (8).\n\nFigure 15.17\n\nThe usage price of the migrated LUN in Disk Group 2 is now higher than\n\nbefore, because a higher performance disk group is being used (9).\n\n15.7 Load Balanced Virtual Switches Architecture\n\nVirtual servers are connected to the outside world via virtual switches,\n\nwhich send and receive traffic with the same uplink. Bandwidth bottlenecks\n\nform when the network traffic on the uplink’s port increases to a point that\n\nit causes transmission delays, performance issues, packet loss, and lag time\n\n(Figures 15.18 and 15.19).\n\nFigure 15.18\n\nA virtual switch is interconnecting virtual servers (1). A physical network\n\nadapter has been attached to the virtual switch to be used as an uplink to\n\nthe physical (external) network, connecting the virtual servers to cloud\n\nconsumers (2). Cloud service consumers send requests via the physical\n\nuplink (3).\n\nFigure 15.19\n\nThe amount of traffic passing through the physical uplink grows in parallel\n\nwith the increasing number of requests. The number of packets that need to\n\nbe processed and forwarded by the physical network adapter also increases\n\n(4). The physical adapter cannot handle the workload, now that the network\n\ntraffic has exceeded its capacity (5). The network forms a bottleneck that\n\nresults in performance degradation and the loss of delay-sensitive data\n\npackets (6).\n\nThe load balanced virtual switches architecture establishes a load balancing\n\nsystem where multiple uplinks are provided to balance network traffic\n\nworkloads across multiple uplinks or redundant paths, which can help avoid\n\nslow transfers and data loss (Figure 15.20). Link aggregation can be\n\nexecuted to balance the traffic, which allows the workload to be distributed\n\nacross multiple uplinks at the same time so that none of the network cards\n\nare overloaded.\n\nFigure 15.20\n\nAdditional physical uplinks are added to distribute and balance network\n\ntraffic.\n\nThe virtual switch needs to be configured to support multiple physical\n\nuplinks, which are usually configured as an NIC team that has defined\n\ntraffic-shaping policies.\n\nThe following mechanisms can be incorporated into this architecture:\n\nCloud Usage Monitor — Cloud usage monitors are used to monitor\n\nnetwork traffic and bandwidth usage.\n\nHypervisor — This mechanism hosts and provides the virtual servers with\n\naccess to both the virtual switches and external network.\n\nLoad Balancer — The load balancer distributes the network workload\n\nacross the different uplinks.\n\nLogical Network Perimeter — The logical network perimeter creates\n\nboundaries that protect and limit the bandwidth usage for each cloud\n\nconsumer.\n\nResource Replication — This mechanism is used to generate additional\n\nuplinks to the virtual switch.\n\nVirtual Server — Virtual servers host the IT resources that benefit from the\n\nadditional uplinks and bandwidth via virtual switches.\n\n15.8 Multipath Resource Access Architecture\n\nCertain IT resources can only be accessed using an assigned path (or\n\nhyperlink) that leads to their exact location. This path can be lost or\n\nincorrectly defined by the cloud consumer or changed by the cloud\n\nprovider. An IT resource whose hyperlink is no longer in the possession of\n\nthe cloud consumer becomes inaccessible and unavailable (Figure 15.21).\n\nException conditions that result from IT resource unavailability can\n\ncompromise the stability of larger cloud solutions that depend on the IT\n\nresource.\n\nFigure 15.21\n\nPhysical Server A is connected to LUN A via a single fiber channel, and\n\nuses the LUN to store different types of data. The fiber channel connection\n\nbecomes unavailable due to a HBA card failure and invalidates the path\n\nused by Physical Server A, which has now lost access to LUN A and all of\n\nits stored data.\n\nThe multipath resource access architecture establishes a multipathing\n\nsystem with alternative paths to IT resources, so that cloud consumers have\n\nthe means to programmatically or manually overcome path failures (Figure\n\n15.22).\n\nFigure 15.22\n\nA multipathing system is providing alternative paths to a cloud storage\n\ndevice.\n\nThis technology architecture requires the use of a multipathing system and\n\nthe creation of alternative physical or virtual hyperlinks that are assigned to\n\nspecific IT resources. The multipathing system resides on the server or\n\nhypervisor, and ensures that each IT resource can be seen via each\n\nalternative path identically (Figure 15.23).\n\nFigure 15.23\n\nPhysical Server A is connected to the LUN A cloud storage device via two\n\ndifferent paths (1). LUN A is seen as different LUNs from each of the two\n\npaths (2). The multipathing system is configured (3). LUN A is seen as one\n\nidentical LUN from both paths (4), and Physical Server A has access to\n\nLUN A from two different paths (5). A link failure occurs and one of the\n\npaths becomes unavailable (6). Physical Server A can still use LUN A\n\nbecause the other link remains active (7).\n\nThis architecture can involve the following mechanisms:\n\nCloud Storage Device — The cloud storage device is a common IT resource\n\nthat requires the creation of alternative paths in order to remain accessible\n\nto solutions that rely on data access.\n\nHypervisor — Alternative paths to a hypervisor are required in order to\n\nhave redundant links to the hosted virtual servers.\n\nLogical Network Perimeter — This mechanism guarantees the maintenance\n\nof cloud consumer privacy, even when multiple paths to the same IT\n\nresource are created.\n\nResource Replication — The resource replication mechanism is required\n\nwhen a new instance of an IT resource needs to be created to generate the\n\nalternative path.\n\nVirtual Server — These servers host the IT resources that have multipath\n\naccess via different links or virtual switches. Hypervisors can provide\n\nmultipath access to the virtual servers.\n\n15.9 Persistent Virtual Network Configuration Architecture\n\nNetwork configurations and port assignments for virtual servers are\n\ngenerated during the creation of the virtual switch on the host physical\n\nserver and the hypervisor hosting the virtual server. These configurations\n\nand assignments reside in the virtual server’s immediate hosting\n\nenvironment, meaning a virtual server that is moved or migrated to another\n\nhost will lose network connectivity because destination hosting\n\nenvironments do not have the required port assignments and network\n\nconfiguration information (Figure 15.24).\n\nFigure 15.24\n\nPart A shows Virtual Server A connected to the network through Virtual\n\nSwitch A, which was created on Physical Server A. In Part B, Virtual Server\n\nA is connected to Virtual Switch B after being moved to Physical Server B.\n\nThe virtual server cannot connect to the network because its configuration\n\nsettings are missing.\n\nIn the persistent virtual network configuration architecture, network\n\nconfiguration information is stored in a centralized location and replicated\n\nto physical server hosts. This allows the destination host to access the\n\nconfiguration information when a virtual server is moved from one host to\n\nanother.\n\nThe system established with this architecture includes a centralized virtual\n\nswitch, VIM, and configuration replication technology. The centralized\n\nvirtual switch is shared by physical servers and configured via the VIM,\n\nwhich initiates replication of the configuration settings to the physical\n\nservers (Figure 15.25).\n\nFigure 15.25\n\nA virtual switch’s configuration settings are maintained by the VIM, which\n\nensures that these settings are replicated to other physical servers. The\n\ncentralized virtual switch is published, and each host physical server is\n\nassigned some of its ports. Virtual Server A is moved to Physical Server B\n\nwhen Physical Server A fails. The virtual server’s network settings are\n\nretrievable, since they are stored on a centralized virtual switch that is\n\nshared by both physical servers. Virtual Server A maintains network\n\nconnectivity on its new host, Physical Server B.\n\nIn addition to the virtual server mechanism for which this architecture\n\nprovides a migration system, the following mechanisms can be included:\n\nHypervisor — The hypervisor hosts the virtual servers that require the\n\nconfiguration settings to be replicated across the physical hosts.\n\nLogical Network Perimeter — The logical network perimeter helps ensure\n\nthat access to the virtual server and its IT resources is isolated to the rightful\n\ncloud consumer, before and after a virtual server is migrated.\n\nResource Replication — The resource replication mechanism is used to\n\nreplicate the virtual switch configurations and network capacity allocations\n\nacross the hyper-visors, via the centralized virtual switch.\n\n15.10 Redundant Physical Connection for Virtual Servers\n\nArchitecture\n\nA virtual server is connected to an external network via a virtual switch\n\nuplink port, meaning the virtual server will become isolated and\n\ndisconnected from the external network if the uplink fails (Figure 15.26).\n\nFigure 15.26\n\nA physical network adapter installed on the host physical server is\n\nconnected to the physical switch on the network (1). A virtual switch is\n\ncreated for use by two virtual servers. The physical network adapter is\n\nattached to the virtual switch to act as an uplink, since it requires access to\n\nthe physical (external) network (2). The virtual servers communicate with\n\nthe external network via the attached physical uplink network card (3). A\n\nconnection failure occurs, either because of a physical link connectivity\n\nissue between the physical adapter and the physical switch (4.1), or\n\nbecause of a physical network card failure (4.2). The virtual servers lose\n\naccess to the physical external network and are no longer accessible to\n\ntheir cloud consumers (5).\n\nThe redundant physical connection for virtual servers architecture\n\nestablishes one or more redundant uplink connections and positions them in\n\nstandby mode. This architecture ensures that a redundant uplink connection\n\nis available to connect the active uplink, whenever the primary uplink\n\nconnection becomes unavailable (Figure 15.27).\n\nFigure 15.27\n\nRedundant uplinks are installed on a physical server that is hosting several\n\nvirtual servers. When an uplink fails, another uplink takes over to maintain\n\nthe virtual servers’ active network connections.\n\nIn a process that is transparent to both virtual servers and their users, a\n\nstandby uplink automatically becomes the active uplink as soon as the main",
      "page_number": 760
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 782-804)",
      "start_page": 782,
      "end_page": 804,
      "detection_method": "synthetic",
      "content": "uplink fails, and the virtual servers use the newly active uplink to sends\n\npackets externally.\n\nThe second NIC does not forward any traffic while the primary uplink is\n\nalive, even though it receives the virtual server’s packets. However, the\n\nsecondary uplink will start forwarding packets immediately if the primary\n\nuplink were to fail (Figures 15.28 to 15.30). The failed uplink becomes the\n\nprimary uplink again after it returns to operation, while the second NIC\n\nreturns to standby mode.\n\nFigure 15.28\n\nA new network adapter is added to support a redundant uplink (1). Both\n\nnetwork cards are connected to the physical external switch (2), and both\n\nphysical network adapters are configured to be used as uplink adapters for\n\nthe virtual switch (3).\n\nFigure 15.29\n\nOne physical network adapter is designated as the primary adapter (4),\n\nwhile the other is designated as the secondary adapter providing the\n\nstandby uplink. The secondary adapter does not forward any packets.\n\nFigure 15.30\n\nThe primary uplink becomes unavailable (5). The secondary standby uplink\n\nautomatically takes over and uses the virtual switch to forward the virtual\n\nservers’ packets to the external network (6). The virtual servers do not\n\nexperience interruptions and remain connected to the external network (7).\n\nThe following mechanisms are commonly part of this architecture, in\n\naddition to the virtual server:\n\nFailover System — The failover system performs the transition of\n\nunavailable uplinks to standby uplinks.\n\nHypervisor — This mechanism hosts virtual servers and some virtual\n\nswitches, and provides virtual networks and virtual switches with access to\n\nthe virtual servers.\n\nLogical Network Perimeter — Logical network perimeters ensure that the\n\nvirtual switches that are allocated or defined for each cloud consumer\n\nremain isolated.\n\nResource Replication — Resource replication is used to replicate the\n\ncurrent status of active uplinks to standby uplinks so as to maintain the\n\nnetwork connection.\n\n15.11 Storage Maintenance Window Architecture\n\nCloud storage devices that are subject to maintenance and administrative\n\ntasks sometimes need to be temporarily shut down, meaning cloud service\n\nconsumers and IT resources consequently lose access to these devices and\n\ntheir stored data (Figure 15.31).\n\nFigure 15.31\n\nA pre-scheduled maintenance task carried out by a cloud resource\n\nadministrator causes an outage for the cloud storage device, which\n\nbecomes unavailable to cloud service consumers. Because cloud consumers\n\nwere previously notified of the outage, cloud consumers do not attempt any\n\ndata access.\n\nLive Storage Migration\n\nThe live storage migration program is a sophisticated\n\nsystem that utilizes the LUN migration component to\n\nreliably move LUNs by enabling the original copy to\n\nremain active until after the destination copy has been\n\nverified as being fully functional.\n\nThe data of a cloud storage device that is about to undergo a maintenance\n\noutage can be temporarily moved to a secondary duplicate cloud storage\n\ndevice. The storage maintenance window architecture enables cloud service\n\nconsumers to be automatically and transparently redirected to the secondary\n\ncloud storage device, without becoming aware that their primary storage\n\ndevice has been taken offline.\n\nThis architecture uses a live storage migration program, as demonstrated in\n\nFigures 15.32 to 15.37.\n\nFigure 15.32\n\nThe cloud storage device is scheduled to undergo a maintenance outage,\n\nbut unlike the scenario depicted in Figure 15.31, the cloud service\n\nconsumers were not notified of the outage and continue to access the cloud\n\nstorage device.\n\nFigure 15.33\n\nLive storage migration moves the LUNs from the primary storage device to\n\na secondary storage device.\n\nFigure 15.34\n\nRequests for the data are forwarded to the duplicate LUNs on the secondary\n\nstorage device, once the LUN’s data has been migrated.\n\nFigure 15.35\n\nThe primary storage is powered off for maintenance.\n\nFigure 15.36\n\nThe primary storage is brought back online, after the maintenance task is\n\nfinished. Live storage migration restores the LUN data from the secondary\n\nstorage device to the primary storage device.\n\nFigure 15.37\n\nThe live storage migration process is completed and all of the data access\n\nrequests are forwarded back to the primary cloud storage device.\n\nIn addition to the cloud storage device mechanism that is principal to this\n\narchitecture, the resource replication mechanism is used to keep the primary\n\nand secondary storage devices synchronized. Both manually and\n\nautomatically initiated failover can also be incorporated into this cloud\n\narchitecture via the failover system mechanism, even though the migration\n\nis often pre-scheduled.\n\nNote\n\nEdge and fog computing architectures establish\n\nenvironments outside of clouds but are covered here because\n\nthese environments still relate to clouds and are primarily\n\ncreated in support of alleviating clouds from processing\n\nresponsibilities so as to improve the performance,\n\nresponsiveness, and scalability of consumer organization\n\nsolutions.\n\nEdge and fog computing architectures offer data processing\n\nand storage capacity closer to end user devices in order to\n\nstreamline the processing and storage of data that will\n\neventually be processed and stored in the cloud.\n\nEdge and fog architectures are commonly used for IoT\n\nsolutions in support of geographically distributed IoT\n\ndevices. However, both architectures can be utilized to\n\nimprove the effectiveness of standard business automation\n\nsolutions for organizations, especially those with end users\n\nin multiple physical locations.\n\n15.12 Edge Computing Architecture\n\nAn edge computing architecture introduces an intermediate processing layer\n\nthat is physically positioned between the cloud and the cloud consumer. The\n\nedge environment is intentionally designed and located to be more\n\naccessible and performant for the consumer organization.\n\nPortions of the cloud-based solution are moved to the edge environment\n\nwhere they can be supported with dedicated infrastructure that enables them\n\nto perform faster, more responsively, and with greater scalability. Typically,\n\nthe heaver processing responsibilities will remain with the cloud, while the\n\nparts of a solution with lower-end processing responsibilities are moved to\n\nthe edge layer.\n\nEdge architectures are typically utilized by consumer organizations with\n\nmultiple, distributed physical locations. For each such location a separate\n\nedge environment can be established (Figure 15.38). Edge computing\n\nenvironments can be implemented in suitable third-party locations that have\n\nthe necessary resources, such as internet service providers and\n\ntelecommunication providers.\n\nFigure 15.38\n\nAn edge computing architecture with a set of edge environments, each of\n\nwhich accommodates users or devices in a separate physical location.\n\nEdge computing can benefit application architectures by reducing\n\nbandwidth requirements, optimizing resource utilization, improving\n\nsecurity (by encrypting data closer to its origin), and even reducing power\n\nconsumption.\n\n15.13 Fog Computing Architecture\n\nA fog computing architecture adds an additional processing layer in\n\nbetween edge environments and a cloud (Figure 15.39). This allows\n\nintermediate-level processing responsibilities to be moved from the cloud to\n\nfog environments, each of which can support and facilitate multiple edge\n\nenvironments.\n\nFigure 15.39\n\nThe use of the fog computing architecture inserts an intermediary\n\nprocessing layer between the cloud and the edge environments.\n\nFog computing pushes data processing capacity from the cloud to the fog\n\nlayer where gateways may exist to effectively relay data back and forth\n\nbetween the edge environments and the cloud. When edge environments\n\nneed to send massive volumes of data to the cloud, the fog environment can\n\nfirst determine what data carries more value in order to optimize the data\n\ntransfers. The gateways in the fog then first send critical data to the cloud to\n\nbe stored and processed while the remaining data relayed by edge\n\ncomputers may need to then be locally processed by resources in the fog\n\nenvironment.\n\nAs with edge computing, fog computing is also commonly used to support\n\nIoT solutions. The use of fog computing for a business automation solution\n\nis generally warranted when the solution needs to support many users\n\nacross highly distributed user bases.\n\nPart IV\n\nWorking with Clouds\n\nChapter 16: Cloud Delivery Model Considerations\n\nChapter 17: Cost Metrics and Pricing Models\n\nChapter 18: Service Quality Metrics and SLAs\n\nEach of the chapters in this part of the book addresses a different topic area\n\nthat pertains to planning or using cloud environments and cloud-based\n\ntechnologies. The numerous considerations, strategies, and metrics provided\n\nin these chapters help associate topics covered in preceding chapters with\n\nreal-world requirements and constraints.",
      "page_number": 782
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 805-825)",
      "start_page": 805,
      "end_page": 825,
      "detection_method": "synthetic",
      "content": "Chapter 16\n\nCloud Delivery Model Considerations\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\n16.3 Case Study Example\n\nMost of the preceding chapters have been focused on technologies and\n\nmodels used to define and implement infrastructure and architecture layers\n\nwithin cloud environments. This chapter revisits the cloud delivery models\n\nthat were introduced in Chapter 4 in order to address a number of real world\n\nconsiderations within the context of IaaS, PaaS, and SaaS-based\n\nenvironments.\n\nThe chapter is organized into two primary sections that explore cloud\n\ndelivery model issues pertaining to cloud providers and cloud consumers\n\nrespectively.\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\nThis section explores the architecture and administration of IaaS, PaaS, and\n\nSaaS cloud delivery models from the point of view of the cloud provider.\n\nThe integration and management of these cloud-based environments as part\n\nof greater environments and how they can relate to different technologies\n\nand cloud mechanism combinations are examined.\n\nBuilding IaaS Environments\n\nThe virtual server and cloud storage device mechanisms represent the two\n\nmost fundamental IT resources that are delivered as part of a standard rapid\n\nprovisioning architecture within IaaS environments. They are offered in\n\nvarious standardized configurations that are defined by the following\n\nproperties:\n\noperating system\n\nprimary memory capacity\n\nprocessing capacity\n\nvirtualized storage capacity\n\nMemory and virtualized storage capacity is usually allocated with\n\nincrements of 1 GB to simplify the provisioning of underlying physical IT\n\nresources. When limiting cloud consumer access to virtualized\n\nenvironments, IaaS offerings are preemptively assembled by cloud\n\nproviders via virtual server images that capture the pre-defined\n\nconfigurations. Some cloud providers may offer cloud consumers direct\n\nadministrative access to physical IT resources, in which case the bare-metal\n\nprovisioning architecture may come into play.\n\nSnapshots can be taken of a virtual server to record its current state,\n\nmemory, and configuration of a virtualized IaaS environment for backup\n\nand replication purposes, in support of horizontal and vertical scaling\n\nrequirements. For example, a virtual server can use its snapshot to become\n\nreinitialized in another hosting environment after its capacity has been\n\nincreased to allow for vertical scaling. The snapshot can alternatively be\n\nused to duplicate a virtual server. The management of custom virtual server\n\nimages is a vital feature that is provided via the remote administration\n\nsystem mechanism. Most cloud providers also support importing and\n\nexporting options for custom-built virtual server images in both proprietary\n\nand standard formats.\n\nData Centers\n\nCloud providers can offer IaaS-based IT resources from multiple\n\ngeographically diverse data centers, which provides the following primary\n\nbenefits:\n\nMultiple data centers can be linked together for increased resiliency. Each\n\ndata center is placed in a different location to lower the chances of a single\n\nfailure forcing all of the data centers to go offline simultaneously.\n\nConnected through high-speed communications networks with low latency,\n\ndata centers can perform load balancing, IT resource backup and\n\nreplication, and increase storage capacity, while improving availability and\n\nreliability. Having multiple data centers spread over a greater area further\n\nreduces network latency.\n\nData centers that are deployed in different countries make access to IT\n\nresources more convenient for cloud consumers that are constricted by legal\n\nand regulatory requirements.\n\nFigure 16.1 provides an example of a cloud provider that is managing four\n\ndata centers that are split between two different geographic regions.\n\nFigure 16.1\n\nA cloud provider is provisioning and managing an IaaS environment with\n\nIT resources from different data centers in the United States and the United\n\nKingdom.\n\nWhen an IaaS environment is used to provide cloud consumers with\n\nvirtualized network environments, each cloud consumer is segregated into a\n\ntenant environment that isolates IT resources from the rest of the cloud\n\nthrough the Internet. VLANs and network access control software\n\ncollaboratively realize the corresponding logical network perimeters.\n\nScalability and Reliability\n\nWithin IaaS environments, cloud providers can automatically provision\n\nvirtual servers via the dynamic vertical scaling type of the dynamic\n\nscalability architecture. This can be performed through the VIM, as long as\n\nthe host physical servers have sufficient capacity. The VIM can scale virtual\n\nservers out using resource replication as part of a resource pool architecture,\n\nif a given physical server has insufficient capacity to support vertical\n\nscaling. The load balancer mechanism, as part of a workload distribution\n\narchitecture, can be used to distribute the workload among IT resources in a\n\npool to complete the horizontal scaling process.\n\nManual scalability requires the cloud consumer to interact with a usage and\n\nadministration program to explicitly request IT resource scaling. In contrast,\n\nautomatic scalability requires the automated scaling listener to monitor the\n\nworkload and reactively scale the resource capacity. This mechanism\n\ntypically acts as a monitoring agent that tracks IT resource usage in order to\n\nnotify the resource management system when capacity has been exceeded.\n\nReplicated IT resources can be arranged in high-availability configuration\n\nthat forms a failover system for implementation via standard VIM features.\n\nAlternatively, a high-availability/high-performance resource cluster can be\n\ncreated at the physical or virtual server level, or both simultaneously. The\n\nmultipath resource access architecture is commonly employed to enhance\n\nreliability via the use of redundant access paths, and some cloud providers\n\nfurther offer the provisioning of dedicated IT resources via the resource\n\nreservation architecture.\n\nMonitoring\n\nCloud usage monitors in an IaaS environment can be implemented using the\n\nVIM or specialized monitoring tools that directly comprise and/or interface\n\nwith the virtualization platform. Several common capabilities of the IaaS\n\nplatform involve monitoring:\n\nVirtual Server Lifecycles – Recording and tracking uptime periods and the\n\nallocation of IT resources, for pay-per-use monitors and time-based billing\n\npurposes.\n\nData Storage – Tracking and assigning the allocation of storage capacity to\n\ncloud storage devices on virtual servers, for pay-per-use monitors that\n\nrecord storage usage for billing purposes.\n\nNetwork Traffic – For pay-per-use monitors that measure inbound and\n\noutbound network usage and SLA monitors that track QoS metrics, such as\n\nresponse times and network losses.\n\nFailure Conditions – For SLA monitors that track IT resource and QoS\n\nmetrics to provide warning in times of failure.\n\nEvent Triggers – For audit monitors that appraise and evaluate the\n\nregulatory compliance of select IT resources.\n\nMonitoring architectures within IaaS environments typically involve service\n\nagents that communicate directly with backend management systems.\n\nSecurity\n\nCloud security mechanisms that are relevant for securing IaaS environments\n\ninclude:\n\nencryption, hashing, digital signature, and PKI mechanisms for overall\n\nprotection of data transmission\n\nIAM and SSO mechanisms for accessing services and interfaces in security\n\nsystems that rely on user identification, authentication, and authorization\n\ncapabilities\n\ncloud-based security groups for isolating virtual environments through\n\nhypervisors and network segments via network management software\n\nhardened virtual server images for internal and externally available virtual\n\nserver environments\n\nvarious cloud usage monitors to track provisioned virtual IT resources to\n\ndetect abnormal usage patterns\n\nEquipping PaaS Environments\n\nPaaS environments typically need to be outfitted with a selection of\n\napplication development and deployment platforms in order to\n\naccommodate different programming models, languages, and frameworks.\n\nA separate ready-made environment is usually created for each\n\nprogramming stack that contains the necessary software to run applications\n\nspecifically developed for the platform.\n\nEach platform is accompanied by a matching SDK and IDE, which can be\n\ncustom-built or enabled by IDE plugins supplied by the cloud provider. IDE\n\ntoolkits can simulate the cloud runtime locally within the PaaS environment\n\nand usually include executable application servers. The security restrictions\n\nthat are inherent to the runtime are also simulated in the development\n\nenvironment, including checks for unauthorized attempts to access system\n\nIT resources.\n\nCloud providers often offer a resource management system mechanism that\n\nis customized for the PaaS platform so that cloud consumers can create and\n\ncontrol customized virtual server images with ready-made environments.\n\nThis mechanism also provides features specific to the PaaS platform, such\n\nas managing deployed applications and configuring multitenancy. Cloud\n\nproviders further rely on a variation of the rapid provisioning architecture\n\nknown as platform provisioning, which is designed specifically to provision\n\nready-made environments.\n\nScalability and Reliability\n\nThe scalability requirements of cloud services and applications that are\n\ndeployed within PaaS environments are generally addressed via dynamic\n\nscalability and workload distribution architectures that rely on the use of\n\nnative automated scaling listeners and load balancers. The resource pooling\n\narchitecture is further utilized to provision IT resources from resource pools\n\nmade available to multiple cloud consumers.\n\nCloud providers can evaluate network traffic and server-side connection\n\nusage against the instance’s workload, when determining how to scale an\n\noverloaded application as per parameters and cost limitations provided by\n\nthe cloud consumer. Alternatively, cloud consumers can configure the\n\napplication designs to customize the incorporation of available mechanisms\n\nthemselves.\n\nThe reliability of ready-made environments and hosted cloud services and\n\napplications can be supported with standard failover system mechanisms\n\n(Figure 16.2), as well as the non-disruptive service relocation architecture,\n\nso as to shield cloud consumers from failover conditions. The resource\n\nreservation architecture may also be in place to offer exclusive access to\n\nPaaS-based IT resources. As with other IT resources, ready-made\n\nenvironments can also span multiple data centers and geographical regions\n\nto further increase availability and resiliency.\n\nFigure 16.2\n\nLoad balancers are used to distribute ready-made environment instances\n\nthat are part of a failover system, while automated scaling listeners are\n\nused to monitor the network and instance workloads (1). The ready-made\n\nenvironments are scaled out in response to an increase in workload (2), and\n\nthe failover system detects a failure condition and stops replicating a failed\n\nready-made environment (3).\n\nMonitoring\n\nSpecialized cloud usage monitors in PaaS environments are used to monitor\n\nthe following:\n\nReady-Made Environment Instances – The applications of these instances\n\nare recorded by pay-per-use monitors for the calculation of time-based\n\nusage fees.\n\nData Persistence – This statistic is provided by pay-per-use monitors that\n\nrecord the number of objects, individual occupied storage sizes, and\n\ndatabase transactions per billing period.\n\nNetwork Usage – Inbound and outbound network usage is tracked for pay-\n\nper-use monitors and SLA monitors that track network-related QoS metrics.\n\nFailure Conditions – SLA monitors that track the QoS metrics of IT\n\nresources need to capture failure statistics.\n\nEvent Triggers – This metric is primarily used by audit monitors that need\n\nto respond to certain types of events.\n\nSecurity\n\nThe PaaS environment, by default, does not usually introduce the need for\n\nnew cloud security mechanisms beyond those that are already provisioned\n\nfor IaaS environments.\n\nOptimizing SaaS Environments\n\nIn SaaS implementations, cloud service architectures are generally based on\n\nmultitenant environments that enable and regulate concurrent cloud\n\nconsumer access (Figure 16.3). SaaS IT resource segregation does not\n\ntypically occur at the infrastructure level in SaaS environments, as it does in\n\nIaaS and PaaS environments.\n\nFigure 16.3\n\nThe SaaS-based cloud service is hosted by a multitenant environment\n\ndeployed in a high-performance virtual server cluster. A usage and\n\nadministration portal is used by the cloud consumer to access and\n\nconfigure the cloud service.\n\nSaaS implementations rely heavily on the features provided by the native\n\ndynamic scalability and workload distribution architectures, as well as non-\n\ndisruptive service relocation to ensure that failover conditions do not impact\n\nthe availability of SaaS-based cloud services.\n\nHowever, it is vital to acknowledge that, unlike the relatively vanilla\n\ndesigns of IaaS and PaaS products, each SaaS deployment will bring with it\n\nunique architectural, functional, and runtime requirements. These\n\nrequirements are specific to the nature of the business logic the SaaS-based\n\ncloud service is programmed with, as well as the distinct usage patterns it is\n\nsubjected to by its cloud service consumers.\n\nFor example, consider the diversity in functionality and usage of the\n\nfollowing recognized online SaaS offerings:\n\ncollaborative authoring and information-sharing (Wikipedia, Blogger)\n\ncollaborative management (Zimbra, Google Apps)\n\nconferencing services for instant messaging, audio/video communications\n\n(Zoom, Skype, Google Meet)\n\nenterprise management systems (ERP, CRM, CM)\n\nfile-sharing and content distribution (YouTube, Dropbox)\n\nindustry-specific software (engineering, bioinformatics)\n\nmessaging systems (e-mail, voicemail)\n\nmobile application marketplaces (Google Play Store, Apple App Store)\n\noffice productivity software suites (Microsoft Office, Adobe Creative\n\nCloud)\n\nsearch engines (Google, Yahoo)\n\nsocial networking media (Twitter, LinkedIn)\n\nNow consider that many of the previously listed cloud services are offered\n\nin one or more of the following implementation mediums:\n\nmobile application\n\nREST service\n\nWeb service\n\nEach of these SaaS implementation mediums provide Web-based APIs for\n\ninterfacing by cloud consumers. Examples of online SaaS-based cloud\n\nservices with Web-based APIs include:\n\nelectronic payment services (PayPal)\n\nmapping and routing services (Google Maps)\n\npublishing tools (WordPress)\n\nMobile-enabled SaaS implementations are commonly supported by the\n\nmulti-device broker mechanism, unless the cloud service is intended\n\nexclusively for access by specific mobile devices.\n\nThe potentially diverse nature of SaaS functionality, the variation in\n\nimplementation technology, and the tendency to offer a SaaS-based cloud\n\nservice redundantly with multiple different implementation mediums makes\n\nthe design of SaaS environments highly specialized. Though not essential to\n\na SaaS implementation, specialized processing requirements can prompt the\n\nneed to incorporate architectural models, such as:\n\nService Load Balancing – for workload distribution across redundant SaaS-\n\nbased cloud service implementations\n\nDynamic Failure Detection and Recovery – to establish a system that can\n\nautomatically resolve some failure conditions without disruption in service\n\nto the SaaS implementation\n\nStorage Maintenance Window – to allow for planned maintenance outages\n\nthat do not impact SaaS implementation availability\n\nElastic Resource Capacity/Elastic Network Capacity – to establish inherent\n\nelasticity within the SaaS-based cloud service architecture that enables it to\n\nautomatically accommodate a range of runtime scalability requirements\n\nCloud Balancing – to instill broad resiliency within the SaaS\n\nimplementation, which can be especially important for cloud services\n\nsubjected to extreme concurrent usage volumes\n\nSpecialized cloud usage monitors can be used in SaaS environments to\n\ntrack the following types of metrics:\n\nTenant Subscription Period – This metric is used by pay-per-use monitors\n\nto record and track application usage for time-based billing. This type of\n\nmonitoring usually incorporates application licensing and regular\n\nassessments of leasing periods that extend beyond the hourly periods of\n\nIaaS and PaaS environments.\n\nApplication Usage – This metric, based on user or security groups, is used\n\nwith pay-per-use monitors to record and track application usage for billing\n\npurposes.\n\nTenant Application Functional Module – This metric is used by pay-per-use\n\nmonitors for function-based billing. Cloud services can have different\n\nfunctionality tiers according to whether the cloud consumer is free-tier or a\n\npaid subscriber.\n\nSimilar to the cloud usage monitoring that is performed in IaaS and PaaS\n\nimplementations, SaaS environments are also commonly monitored for data\n\nstorage, network traffic, failure conditions, and event triggers.\n\nSecurity\n\nSaaS implementations generally rely on a foundation of security controls\n\ninherent to their deployment environment. Distinct business processing\n\nlogic will then add layers of additional cloud security mechanisms or\n\nspecialized security technologies.\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\nThis section raises various considerations concerning the different ways in\n\nwhich cloud delivery models are administered and utilized by cloud\n\nconsumers.\n\nWorking with IaaS Environments\n\nVirtual servers are accessed at the operating system level through the use of\n\nremote terminal applications. Accordingly, the type of client software used\n\ndirectly depends on the type of operating system that is running at the\n\nvirtual server, of which two common options are:\n\nRemote Desktop (or Remote Desktop Connection) Client – for Windows-\n\nbased environments and presents a Windows GUI desktop\n\nSSH Client – for Mac and Linux-based environments to allow for secure\n\nchannel connections to text-based shell accounts running on the server OS\n\nFigure 16.4 illustrates a typical usage scenario for virtual servers that are\n\nbeing offered as IaaS services after having been created with management\n\ninterfaces.\n\nFigure 16.4\n\nA cloud resource administrator uses the Windows-based Remote Desktop\n\nclient to administer a Windows-based virtual server and the SSH client for\n\nthe Linux-based virtual server.\n\nA cloud storage device can be attached directly to the virtual servers and\n\naccessed through the virtual servers’ functional interface for management\n\nby the operating system. Alternatively, a cloud storage device can be\n\nattached to an IT resource that is being hosted outside of the cloud, such as\n\nan on-premise device over a WAN or VPN. In these cases, the following\n\nformats for the manipulation and transmission of cloud storage data are\n\ncommonly used:\n\nNetworked File System – System-based storage access, whose rendering of\n\nfiles is similar to how folders are organized in operating systems (NFS,\n\nCIFS)\n\nStorage Area Network Devices – Block-based storage access collates and\n\nformats geographically diverse data into cohesive files for optimal network\n\ntransmission (iSCSI, Fibre Channel)\n\nWeb-Based Resources – Object-based storage access by which an interface\n\nthat is not integrated into the operating system logically represents files,\n\nwhich can be accessed through a Web-based interface (Amazon S3)\n\nIT Resource Provisioning Considerations\n\nCloud consumers have a high degree of control over how and to what extent\n\nIT resources are provisioned as part of their IaaS environments.\n\nFor example:\n\ncontrolling scalability features (automated scaling, load balancing)\n\ncontrolling the lifecycle of virtual IT resources (shutting down, restarting,\n\npowering up of virtual devices)\n\ncontrolling the virtual network environment and network access rules\n\n(firewalls, logical network perimeters)\n\nestablishing and displaying service provisioning agreements (account\n\nconditions, usage terms)\n\nmanaging the attachment of cloud storage devices\n\nmanaging the pre-allocation of cloud-based IT resources (resource\n\nreservation)\n\nmanaging credentials and passwords for cloud resource administrators\n\nmanaging credentials for cloud-based security groups that access virtualized\n\nIT resources through an IAM\n\nmanaging security-related configurations\n\nmanaging customized virtual server image storage (importing, exporting,\n\nbackup)\n\nselecting high-availability options (failover, IT resource clustering)\n\nselecting and monitoring SLA metrics\n\nselecting basic software configurations (operating system, pre-installed\n\nsoftware for new virtual servers)",
      "page_number": 805
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 827-848)",
      "start_page": 827,
      "end_page": 848,
      "detection_method": "synthetic",
      "content": "using the IDE to emulate the cloud deployment environment. Compiled or\n\ncompleted applications are then bundled and uploaded to the cloud, and\n\ndeployed via the ready-made environments. This deployment process can\n\nalso be controlled through the IDE.\n\nPaaS also allows for applications to use cloud storage devices as\n\nindependent data storing systems for holding development-specific data (for\n\nexample in a repository that is available outside of the cloud environment).\n\nBoth SQL and NoSQL database structures are generally supported.\n\nIT Resource Provisioning Considerations\n\nPaaS environments provide less administrative control than IaaS\n\nenvironments, but still offer a significant range of management features.\n\nFor example:\n\nestablishing and displaying service provisioning agreements, such as\n\naccount conditions and usage terms\n\nselecting software platform and development frameworks for ready-made\n\nenvironments\n\nselecting instance types, which are most commonly frontend or backend\n\ninstances\n\nselecting cloud storage devices for use in ready-made environments\n\ncontrolling the lifecycle of PaaS-developed applications (deployment,\n\nstarting, shutdown, restarting, and release)\n\ncontrolling the versioning of deployed applications and modules\n\nconfiguring availability and reliability-related mechanisms\n\nmanaging credentials for developers and cloud resource administrators\n\nusing IAM\n\nmanaging general security settings, such as accessible network ports\n\nselecting and monitoring PaaS-related SLA metrics\n\nmanaging and monitoring usage and IT resource costs\n\ncontrolling scalability features such as usage quotas, active instance\n\nthresholds, and the configuration and deployment of the automated scaling\n\nlistener and load balancer mechanisms\n\nThe usage and administration portal that is used to access PaaS\n\nmanagement features can provide the feature of pre-emptively selecting the\n\ntimes at which an IT resource is started and stopped. For example, a cloud\n\nresource administrator can set a cloud storage device to turn itself on at\n\n9:00AM then turn off twelve hours later. Building on this system can enable\n\nthe option of having the ready-made environment self-activate upon\n\nreceiving data requests for a particular application and turn off after an\n\nextended period of inactivity.\n\nWorking with SaaS Services\n\nBecause SaaS-based cloud services are almost always accompanied by\n\nrefined and generic APIs, they are usually designed to be incorporated as\n\npart of larger distributed solutions. A common example of this is Google\n\nMaps, which offers a comprehensive API that enables mapping information\n\nand images to be incorporated into Web sites and Web-based applications.\n\nMany SaaS offerings are provided free of charge, although these cloud\n\nservices often come with data collecting sub-programs that harvest usage\n\ndata for the benefit of the cloud provider. When using any SaaS product that\n\nis sponsored by third parties, there is a reasonable chance that it is\n\nperforming a form of background information gathering. Reading the cloud\n\nprovider’s agreement will usually help shed light on any secondary activity\n\nthat the cloud service is designed to perform.\n\nCloud consumers using SaaS products supplied by cloud providers are\n\nrelieved of the responsibilities of implementing and administering their\n\nunderlying hosting environments. Customization options are usually\n\navailable to cloud consumers; however, these options are generally limited\n\nto the runtime usage control of the cloud service instances that are\n\ngenerated specifically by and for the cloud consumer.\n\nFor example:\n\nmanaging security-related configurations\n\nmanaging select availability and reliability options\n\nmanaging usage costs\n\nmanaging user accounts, profiles, and access authorization\n\nselecting and monitoring SLAs\n\nsetting manual and automated scalability options and limitations\n\n16.3 Case Study Example\n\nDTGOV discovers that a number of additional\n\nmechanisms and technologies need to be assembled in\n\norder to complete its IaaS management architecture\n\n(Figure 16.5):\n\nNetwork virtualization is incorporated into logical\n\nnetwork topologies, and logical network perimeters are\n\nestablished using different firewalls and virtual\n\nnetworks.\n\nThe VIM is positioned as the central tool for controlling\n\nthe IaaS platform and equipping it with self-provisioning\n\ncapabilities.\n\nAdditional virtual server and cloud storage device\n\nmechanisms are implemented through the virtualization\n\nplatform, while several virtual server images that\n\nprovide base template configurations for virtual servers\n\nare created.\n\nFigure 16.5\n\nAn overview of the DTGOV management architecture.\n\nDynamic scaling is added using the VIM’s API through\n\nthe use of automated scaling listeners.\n\nHigh-availability virtual server clusters are created using\n\nthe resource replication, load balancer, failover system,\n\nand resource cluster mechanisms.\n\nA customized application that directly uses the SSO and\n\nIAM system mechanisms is built to enable\n\ninteroperability between the remote administration\n\nsystem, network management tools, and VIM.\n\nDTGOV uses a powerful commercial network\n\nmanagement tool that is customized to store event\n\ninformation gathered by the VIM and SLA monitoring\n\nagents in an SLA measurements database. The\n\nmanagement tool and database are used as part of a\n\ngreater SLA management system. In order to enable\n\nbilling processing, DTGOV expands a proprietary\n\nsoftware tool that is based on a set of usage\n\nmeasurements from a database populated by pay-per-use\n\nmonitors. The billing software is used as the base\n\nimplementation for the billing management system\n\nmechanism.\n\nChapter 17\n\nCost Metrics and Pricing Models\n\n17.1 Business Cost Metrics\n\n17.2 Cloud Usage Cost Metrics\n\n17.3 Cost Management Considerations\n\nReducing operating costs and optimizing IT environments are pivotal to\n\nunderstanding and being able to compare the cost models behind\n\nprovisioning on-premise and cloud-based environments. The pricing\n\nstructures used by public clouds are typically based on utility-centric pay-\n\nper-usage models, enabling organizations to avoid up-front infrastructure\n\ninvestments. These models need to be assessed against the financial\n\nimplications of on-premise infrastructure investments and associated total\n\ncost-of-ownership commitments.\n\nThe following chapter provides metrics, formulas, and practices to assist\n\ncloud consumers in performing accurate financial analysis of cloud\n\nadoption plans.\n\n17.1 Business Cost Metrics\n\nThis section begins by describing the common types of metrics used to\n\nevaluate the estimated costs and business value of leasing cloud-based IT\n\nresources when compared to the purchase of on-premise IT resources.\n\nUp-Front and On-Going Costs\n\nUp-front costs are associated with the initial investments that organizations\n\nneed to make in order to fund the IT resources they intend to use. This\n\nincludes both the costs associated with obtaining the IT resources, as well\n\nas expenses required to deploy and administer them.\n\nUp-front costs for the purchase and deployment of on-premise IT resources\n\ntend to be high. Examples of up-front costs for on-premise environments\n\ncan include hardware, software, and the labor required for deployment.\n\nUp-front costs for the leasing of cloud-based IT resources tend to be low.\n\nExamples of up-front costs for cloud-based environments can include the\n\nlabor costs required to assess and set up a cloud environment.\n\nOn-going costs represent the expenses required by an organization to run\n\nand maintain IT resources it uses.\n\nOn-going costs for the operation of on-premise IT resources can vary.\n\nExamples include licensing fees, electricity, insurance, and labor.\n\nOn-going costs for the operation of cloud-based IT resources can also vary,\n\nbut often exceed the on-going costs of on-premise IT resources (especially\n\nover a longer period of time). Examples include virtual hardware leasing\n\nfees, bandwidth usage fees, licensing fees, and labor.\n\nAdditional Costs\n\nTo supplement and extend a financial analysis beyond the calculation and\n\ncomparison of standard up-front and on-going business cost metrics, several\n\nother more specialized business cost metrics can be taken into account.\n\nFor example:\n\nCost of Capital – The cost of capital is a value that represents the cost\n\nincurred by raising required funds. For example, it will generally be more\n\nexpensive to raise an initial investment of $150,000 than it will be to raise\n\nthis amount over a period of three years. The relevancy of this cost depends\n\non how the organization goes about gathering the funds it requires. If the\n\ncost of capital for an initial investment is high, then it further helps justify\n\nthe leasing of cloud-based IT resources.\n\nSunk Costs – An organization will often have existing IT resources that are\n\nalready paid for and operational. The prior investment that has been made\n\nin these on-premise IT resources is referred to as sunk costs. When\n\ncomparing up-front costs together with significant sunk costs, it can be\n\nmore difficult to justify the leasing of cloud-based IT resources as an\n\nalternative.\n\nIntegration Costs – Integration testing is a form of testing required to\n\nmeasure the effort required to make IT resources compatible and\n\ninteroperable within a foreign environment, such as a new cloud platform.\n\nDepending on the cloud deployment model and cloud delivery model being\n\nconsidered by an organization, there may be the need to further allocate\n\nfunds to carry out integration testing and additional labor related to enable\n\ninteroperability between cloud service consumers and cloud services. These\n\nexpenses are referred to as integration costs. High integration costs can\n\nmake the option of leasing cloud-based IT resources less appealing.\n\nLocked-in Costs – As explained in the Risks and Challenges section in\n\nChapter 3, cloud environments can impose portability limitations. When\n\nperforming a metrics analysis over a longer period of time, it may be\n\nnecessary to take into consideration the possibility of having to move from\n\none cloud provider to another. Due to the fact that cloud service consumers\n\ncan become dependent on proprietary characteristics of a cloud\n\nenvironment, there are locked-in costs associated with this type of move.\n\nLocked-in costs can further decrease the long-term business value of\n\nleasing cloud-based IT resources.\n\nCase Study Example\n\nATN performs a total cost-of-ownership (TCO) analysis\n\non migrating two of its legacy applications to a PaaS\n\nenvironment. The report produced by the analysis\n\nexamines comparative evaluations of on-premise and\n\ncloud-based implementations based on a three-year time\n\nframe.\n\nThe following sections provide a summary from the\n\nreport for each of the two applications.\n\nProduct Catalog Browser\n\nThe Product Catalog Browser is a globally used Web\n\napplication that interoperates with the ATN Web portal\n\nand several other systems. This application was\n\ndeployed in a virtual server cluster that is comprised of 4\n\nvirtual servers running on 2 dedicated physical servers.\n\nThe application has its own 300 GB database that resides\n\nin a separate HA cluster. Its code was recently generated\n\nfrom a refactoring project. Only minor portability issues\n\nneeded to be addressed before it was ready to proceed\n\nwith a cloud migration.\n\nThe TCO analysis reveals the following:\n\nOn-Premise Up-Front Costs\n\nLicensing: The purchase price for each physical server\n\nhosting the application is $7,500, while the software\n\nrequired to run all 4 servers totals $30,500\n\nLabor: Labor costs are estimated as $5,500, including\n\nsetup and application deployment.\n\nThe total up-front costs are: ($7,500 x 2) + $30,500 +\n\n$5,500 = $51,000\n\nThe configuration of the servers is derived from a\n\ncapacity plan that accounts for peak workloads. Storage\n\nwas not assessed as part of this plan, since the\n\napplication database is assumed to be only negligibly\n\naffected by the application’s deployment.\n\nOn-Premise On-Going Costs\n\nThe following are monthly on-going costs:\n\nEnvironmental Fees: $750\n\nLicensing Fees: $520\n\nHardware Maintenance: $100\n\nLabor: $2,600\n\nThe total on-premise on-going costs are: $750 + $520 +\n\n$100 + $2,600 = $3,970\n\nCloud-Based Up-Front Costs\n\nIf the servers are leased from a cloud provider, there is\n\nno up-front cost for hardware or software. Labor costs\n\nare estimated at $5,000, which includes expenses for\n\nsolving interoperability issues and application setup.\n\nCloud-Based On-Going Costs\n\nThe following are monthly on-going costs:\n\nServer Instance: Usage fee is calculated per virtual\n\nserver at a rate of $1.25/hour per virtual server. For 4\n\nvirtual servers, this results in: 4 x ($1.25 x 720) =\n\n$3,600. However, the application consumption is\n\nequivalent to 2.3 servers when server instance scaling is\n\nfactored in, meaning the actual on-going server usage\n\ncost is: $2,070.\n\nDatabase Server and Storage: Usage fees are calculated\n\nper database size, at a rate of $1.09/GB per month =\n\n$327.\n\nNetwork: Usage fees are calculated per outbound WAN\n\ntraffic at the rate of $0.10/GB and a monthly volume of\n\n420 GB = $42.\n\nLabor: Estimated at $800 per month, including expenses\n\nfor cloud resource administration tasks.\n\nThe total on-going costs are: $2,070 + $327 + $42 +\n\n$800 = $3,139\n\nThe TCO breakdown for the Product Catalog Browser\n\napplication is provided in Table 17.1.\n\nTable 17-1\n\nThe TCO analysis for the Product Catalog Browser\n\napplication.\n\nA comparison of the respective TCOs over a three-year\n\nperiod for both approaches reveals the following:\n\nOn-Premise TCO: $51,000 up-front + ($3,970 x 36) on-\n\ngoing = $193,920\n\nCloud-Based TCO: $5,000 up-front + ($3,139 x 36) on-\n\ngoing = $118,004\n\nBased on the results of the TCO analysis, ATN decides\n\nto migrate the application to the cloud.\n\n17.2 Cloud Usage Cost Metrics\n\nThe following sections describe a set of usage cost metrics for calculating\n\ncosts associated with cloud-based IT resource usage measurements:\n\nNetwork Usage – inbound and outbound network traffic, as well as intra-\n\ncloud network traffic\n\nServer Usage – virtual server allocation (and resource reservation)\n\nCloud Storage Device – storage capacity allocation\n\nCloud Service – subscription duration, number of nominated users, number\n\nof transactions (of cloud services and cloud-based applications)\n\nFor each usage cost metric a description, measurement unit, and\n\nmeasurement frequency are provided, along with the cloud delivery model\n\nmost applicable to the metric. Each metric is further supplemented with a\n\nbrief example.\n\nNetwork Usage\n\nDefined as the amount of data that is transferred over a network connection,\n\nnetwork usage is typically calculated using separately measured inbound\n\nnetwork usage traffic and outbound network usage traffic metrics in relation\n\nto cloud services or other IT resources.\n\nInbound Network Usage Metric\n\nDescription – inbound network traffic\n\nMeasurement – Σ, inbound network traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 1 GB free, $0.001/GB up to 10 TB a month\n\nOutbound Network Usage Metric\n\nDescription – outbound network traffic\n\nMeasurement – Σ, outbound network traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 1 GB free a month, $0.01/GB between 1 GB to 10 TB per\n\nmonth\n\nNetwork usage metrics can be applied to WAN traffic between IT resources\n\nof one cloud that are located in different geographical regions in order to\n\ncalculate costs for synchronization, data replication, and related forms of\n\nprocessing. Conversely, LAN usage and other network traffic among IT\n\nresources that reside at the same data center are typically not tracked.\n\nIntra-Cloud WAN Usage Metric\n\nDescription – network traffic between geographically diverse IT resources\n\nof the same cloud\n\nMeasurement – Σ, intra-cloud WAN traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 500 MB free daily and $0.01/GB thereafter, $0.005/GB\n\nafter 1 TB per month\n\nMany cloud providers do not charge for inbound traffic in order to\n\nencourage cloud consumers to migrate data to the cloud. Some also do not\n\ncharge for WAN traffic within the same cloud.\n\nNetwork-related cost metrics are determined by the following properties:\n\nStatic IP Address Usage – IP address allocation time (if a static IP is\n\nrequired)\n\nNetwork Load-Balancing – the amount of load-balanced network traffic (in\n\nbytes)\n\nVirtual Firewall – the amount of firewall-processed network traffic (as per\n\nallocation time)\n\nServer Usage\n\nThe allocation of virtual servers is measured using common pay-per-use\n\nmetrics in IaaS and PaaS environments that are quantified by the number of\n\nvirtual servers and ready-made environments. This form of server usage\n\nmeasurement is divided into on-demand virtual machine instance allocation\n\nand reserved virtual machine instance allocation metrics.\n\nThe former metric measures pay-per-usage fees on a short-term basis, while\n\nthe latter metric calculates up-front reservation fees for using virtual servers\n\nover extended periods. The up-front reservation fee is usually used in\n\nconjunction with the discounted pay-per-usage fees.\n\nOn-Demand Virtual Machine Instance Allocation Metric\n\nDescription – uptime of a virtual server instance\n\nMeasurement – Σ, virtual server start date to stop date\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $0.10/hour small instance, $0.20/hour medium instance,\n\n$0.90/hour large instance\n\nReserved Virtual Machine Instance Allocation Metric\n\nDescription – up-front cost for reserving a virtual server instance\n\nMeasurement – Σ, virtual server reservation start date to expiry date\n\nFrequency – daily, monthly, yearly\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $55.10/small instance, $99.90/medium instance, $249.90/large\n\ninstance\n\nAnother common cost metric for virtual server usage measures performance\n\ncapabilities. Cloud providers of IaaS and PaaS environments tend to\n\nprovision virtual servers with a range of performance attributes that are\n\ngenerally determined by CPU and RAM consumption and the amount of\n\navailable dedicated allocated storage.\n\nCloud Storage Device Usage\n\nCloud storage is generally charged by the amount of space allocated within\n\na predefined period, as measured by the on-demand storage allocation\n\nmetric. Similar to IaaS-based cost metrics, on-demand storage allocation\n\nfees are usually based on short time increments (such as on an hourly\n\nbasis). Another common cost metric for cloud storage is I/O data\n\ntransferred, which measures the amount of transferred input and output\n\ndata.\n\nOn-Demand Storage Space Allocation Metric\n\nDescription – duration and size of on-demand storage space allocation in\n\nbytes\n\nMeasurement – Σ, date of storage release / reallocation to date of storage\n\nallocation (resets upon change in storage size)\n\nFrequency – continuous\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – $0.01/GB per hour (typically expressed as GB/month)\n\nI/O Data Transferred Metric\n\nDescription – amount of transferred I/O data",
      "page_number": 827
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 849-868)",
      "start_page": 849,
      "end_page": 868,
      "detection_method": "synthetic",
      "content": "Measurement – Σ, I/O data in bytes\n\nFrequency – continuous\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $0.10/TB\n\nNote that some cloud providers do not charge for I/O usage for IaaS and\n\nPaaS implementations, and limit charges to storage space allocation only.\n\nCloud Service Usage\n\nCloud service usage in SaaS environments is typically measured using the\n\nfollowing three metrics:\n\nApplication Subscription Duration Metric\n\nDescription – duration of cloud service usage subscription\n\nMeasurement – Σ, subscription start date to expiry date\n\nFrequency – daily, monthly, yearly\n\nCloud Delivery Model – SaaS\n\nExample – $69.90 per month\n\nNumber of Nominated Users Metric\n\nDescription – number of registered users with legitimate access\n\nMeasurement – number of users\n\nFrequency – monthly, yearly\n\nCloud Delivery Model – SaaS\n\nExample – $0.90/additional user per month\n\nNumber of Transactions Users Metric\n\nDescription – number of transactions served by the cloud service\n\nMeasurement – number of transactions (request-response message\n\nexchanges)\n\nFrequency – continuous\n\nCloud Delivery Model – PaaS, SaaS\n\nExample – $0.05 per 1,000 transactions\n\n17.3 Cost Management Considerations\n\nCost management is often centered around the lifecycle phases of cloud\n\nservices, as follows:\n\nCloud Service Design and Development – During this stage, the vanilla\n\npricing models and cost templates are typically defined by the organization\n\ndelivering the cloud service.\n\nCloud Service Deployment – Prior to and during the deployment of a cloud\n\nservice, the backend architecture for usage measurement and billing-related\n\ndata collection is determined and implemented, including the positioning of\n\npay-per-use monitor and billing management system mechanisms.\n\nCloud Service Contracting – This phase consists of negotiations between\n\nthe cloud consumer and cloud provider with the goal of reaching a mutual\n\nagreement on rates based on usage cost metrics.\n\nCloud Service Offering – This stage entails the concrete offering of a cloud\n\nservice’s pricing models through cost templates, and any available\n\ncustomization options.\n\nCloud Service Provisioning – Cloud service usage and instance creation\n\nthresholds may be imposed by the cloud provider or set by the cloud\n\nconsumer. Either way, these and other provisioning options can impact\n\nusage costs and other fees.\n\nCloud Service Operation – This is the phase during which active usage of\n\nthe cloud service produces usage cost metric data.\n\nCloud Service Decommissioning – When a cloud service is temporarily or\n\npermanently deactivated, statistical cost data may be archived.\n\nBoth cloud providers and cloud consumers can implement cost management\n\nsystems that reference or build upon the aforementioned lifecycle phases\n\n(Figure 17.1). It is also possible for the cloud provider to carry out some\n\ncost management stages on behalf of the cloud consumer and to then\n\nprovide the cloud consumer with regular reports.\n\nFigure 17.1\n\nCommon cloud service lifecycle stages as they relate to cost management\n\nconsiderations.\n\nPricing Models\n\nThe pricing models used by cloud providers are defined using templates\n\nthat specify unit costs for fine-grained resource usage according to usage\n\ncost metrics. Various factors can influence a pricing model, such as:\n\nmarket competition and regulatory requirements\n\noverhead incurred during the design, development, deployment, and\n\noperation of cloud services and other IT resources\n\nopportunities to reduce expenses via IT resource sharing and data center\n\noptimization\n\nMost major cloud providers offer cloud services at relatively stable,\n\ncompetitive prices even though their own expenses can be volatile. A price\n\ntemplate or pricing plan contains a set of standardized costs and metrics that\n\nspecify how cloud service fees are measured and calculated. Price templates\n\ndefine a pricing model’s structure by setting various units of measure, usage\n\nquotas, discounts, and other codified fees. A pricing model can contain\n\nmultiple price templates, whose formulation is determined by variables like:\n\nCost Metrics and Associated Prices – These are costs that are dependent on\n\nthe type of IT resource allocation (such as on-demand versus reserved\n\nallocation).\n\nFixed and Variable Rates Definitions – Fixed rates are based on resource\n\nallocation and define the usage quotas included in the fixed price, while\n\nvariable rates are aligned with actual resource usage.\n\nVolume Discounts – More IT resources are consumed as the degree of IT\n\nresource scaling progressively increases, thereby possibly qualifying a\n\ncloud consumer for higher discounts.\n\nCost and Price Customization Options – This variable is associated with\n\npayment options and schedules. For example, cloud consumers may be able\n\nto choose monthly, semi-annual, or annual payment installments.\n\nPrice templates are important for cloud consumers that are appraising cloud\n\nproviders and negotiating rates, since they can vary depending on the\n\nadopted cloud delivery model.\n\nFor example:\n\nIaaS – Pricing is usually based on IT resource allocation and usage, which\n\nincludes the amount of transferred network data, number of virtual servers,\n\nand allocated storage capacity.\n\nPaaS – Similar to IaaS, this model typically defines pricing for network\n\ndata transferred, virtual servers, and storage. Prices are variable depending\n\non factors such as software configurations, development tools, and licensing\n\nfees.\n\nSaaS – Because this model is solely concerned with application software\n\nusage, pricing is determined by the number of application modules in the\n\nsubscription, the number of nominated cloud service consumers, and the\n\nnumber of transactions.\n\nIt is possible for a cloud service that is provided by one cloud provider to be\n\nbuilt upon IT resources provisioned from another cloud provider. Figures\n\n17.2 and 17.3 explore two sample scenarios.\n\nFigure 17.2\n\nAn integrated pricing model, whereby the cloud consumer leases a SaaS\n\nproduct from Cloud Provider A, which is leasing an IaaS environment\n\n(including the virtual server used to host the cloud service) from Cloud\n\nProvider B. The cloud consumer pays Cloud Provider A. Cloud Provider A\n\npays Cloud Provider B.\n\nFigure 17.3\n\nSeparate pricing models are used in this scenario, whereby the cloud\n\nconsumer leases a virtual server from Cloud Provider B to host the cloud\n\nservice from Cloud Provider A. Both leasing agreements may have been\n\narranged for the cloud consumer by Cloud Provider A. As part of this\n\narrangement, there may still be some fees billed directly by Cloud Provider\n\nB to Cloud Provider A.\n\nMulti-Cloud Cost Management\n\nWithin a multi-cloud environment, it becomes important to manage the\n\ndifferent billing, pricing, and provisioning arrangements that are established\n\nwith the different cloud providers (Figure 17.4).\n\nFigure 17.4\n\nAn organization using a multi-cloud architecture identifies and selects from\n\neach cloud provider those services that offer an optimal economical\n\nadvantage.\n\nSome cloud providers offer reserved IT resources for which the cloud\n\nconsumer may commit to paying for a fixed period of time, at a discount.\n\nOthers offer the purchase of “points” or “vouchers” that are calculated to\n\ncover estimated costs, allowing for pre-determined fixed monthly charges\n\nand fitting periodic budgeting requirements frequently preferred by\n\naccounting and financial areas of organizations. A third option are spot\n\ninstances that run on spurious capacity for a highly discounted price, which\n\ncan be used for development or testing purposes for a very low cost. In a\n\nmulti-cloud architecture, all of these benefits can be combined from\n\ndifferent cloud providers, allowing the organization to select only the most\n\nconvenient.\n\nBefore migrating to the cloud, an organization must predict the expenses\n\nassociated with its new IT resource provisioning. Furthermore, when\n\nconsidering the implementation of a multi-cloud architecture, specific\n\nplanning for reduced or discounted expenses must be part of the process.\n\nSome of the strategies an organization can follow to achieve this are:\n\nDesigning a Resource Plan for Each Cloud Provider – This plan should\n\ninclude specifying the true needs of the cloud consumer and enforcing them\n\nthrough standards that only allow the use of those resources, as well as\n\nsetting budgets and expense notifications in accordance to each cloud\n\nprovider’s capabilities for when budget thresholds are met. Supervising that\n\nthe plan is completed as designed is a crucial cloud governance task.\n\nTagging Resources – Utilizing tags enables a business to logically group the\n\nresources in its cloud environment for quick identification. It also allows\n\nthe organization to determine which expenses are associated with each\n\ndepartment or business unit. Each cloud provider has its own tagging\n\nsystem. Using a remote administration system, tagging can be standardized\n\nfor all cloud providers in a multi-cloud architecture.\n\nEstablishing Guidelines and Rules on Resource Deployment –\n\nOrganizations should specify how, when, and by whom different types of\n\nresources are to be deployed for every different cloud provider. The kind of\n\nresources intended to be made available should also be standardized,\n\nconsidering the various deployment options that each cloud provider offers.\n\nCase Study Example\n\nDTGOV structures their pricing model around leasing\n\npackages for virtual servers and block-based cloud\n\nstorage devices, with the assumption that resource\n\nallocation is performed either on-demand or based on\n\nalready reserved IT resources.\n\nOn-demand resource allocation is measured and charged\n\nback by the hour, while reserved resource allocation\n\nrequires a one to three-year commitment from the cloud\n\nconsumer, with fees billed monthly.\n\nAs IT resources can scale up and down automatically,\n\nany additional capacity used is charged on a pay-per-use\n\nbasis whenever a reserved IT resource is scaled beyond\n\nits allocated capacity. Windows and Linux-based virtual\n\nservers are made available in the following basic\n\nperformance profiles:\n\nSmall Virtual Server Instance – 1 virtual processor core,\n\n4 GB of virtual RAM, and 320 GB of storage space in\n\nthe root file system.\n\nMedium Virtual Server Instance – 2 virtual processor\n\ncores, 8 GB of virtual RAM, and 540 GB of storage\n\nspace in the root file system.\n\nLarge Virtual Server Instance – 8 virtual processor\n\ncores, 16 GB of virtual RAM, and 1.2 TB of storage\n\nspace in the root file system.\n\nMemory Large Virtual Server Instance – 8 virtual\n\nprocessor cores, 64 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.\n\nProcessor Large Virtual Server Instance – 32 virtual\n\nprocessor cores, 16 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.\n\nUltra-Large Virtual Server Instance – 128 virtual\n\nprocessor cores, 512 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.\n\nVirtual servers are also available in “resilient” or\n\n“clustered” formats. With the former option the virtual\n\nservers are replicated in at least two different data\n\ncenters. In the latter case, the virtual servers are run in a\n\nhigh-availability cluster that is implemented by the\n\nvirtualization platform.\n\nThe pricing model is further based on the capacity of the\n\ncloud storage devices as expressed by multiples of 1 GB,\n\nwith a minimum of 40 GB. Storage device capacity can\n\nbe fixed and administratively adjusted by the cloud\n\nconsumer to increase or decrease by increments of 40\n\nGB, while the block storage has a maximum capacity of\n\n1.2 TB. I/O transfers to and from cloud storage devices\n\nare also subject to charges in addition to pay-per-use fees\n\napplied to outbound WAN traffic. Inbound WAN and\n\nintra-cloud traffic are free of charge.\n\nA complimentary usage allowance permits cloud\n\nconsumers to lease up to three small virtual server\n\ninstances and a 60 GB block-based cloud storage device,\n\n5 GB of I/O transfers monthly, as well as 5 GB of WAN\n\noutbound traffic monthly, all in the first 90 days. As\n\nDTGOV prepares their pricing model for public release,\n\nthey realize that setting cloud service prices is more\n\nchallenging than they expected because:\n\nTheir prices need to reflect and respond to marketplace\n\nconditions while staying competitive with other cloud\n\nofferings and remaining profitable to DTGOV.\n\nThe client portfolio has not been established yet, as\n\nDTGOV is expecting new customers. Their non-cloud\n\nclients are expected to progressively migrate to the\n\ncloud, although the actual rate of migration is too\n\ndifficult to predict.\n\nAfter performing further market research, DTGOV\n\nsettles on the following price template for virtual server\n\ninstance allocation:\n\nVirtual Server On-Demand Instance Allocation\n\nMetric: on-demand instance allocation\n\nMeasurement: pay-per-use charges calculated for total\n\nservice consumption for each calendar month (hourly\n\nrate is used for the actual instance size when the instance\n\nhas been scaled up)\n\nBilling Period: monthly\n\nThe price template is outlined in Table 17.2.\n\nTable 17-2\n\nThe price template for virtual server on-demand\n\ninstance allocation.\n\nSurcharge for clustered IT resources: 120%\n\nSurcharge for resilient IT resources: 150%\n\nVirtual Server Reserved Instance Allocation\n\nMetric: reserved instance allocation\n\nMeasurement: reserved instance allocation fee charged\n\nup-front with pay-per-use fees calculated based on the\n\ntotal consumption during each calendar month\n\n(additional charges apply for periods when the instance\n\nis scaled up)\n\nBilling Period: monthly\n\nThe price template is outlined in Table 17.3.\n\nTable 17-3\n\nThe price template for virtual server reserved\n\ninstance allocation.\n\nSurcharge for clustered IT resources: 100%\n\nSurcharge for resilient IT resources: 120%\n\nDTGOV further provides the following simplified price\n\ntemplates for cloud storage device allocation and WAN\n\nbandwidth usage:\n\nCloud Storage Device\n\nMetric: on-demand storage allocation, I/O data\n\ntransferred\n\nMeasurement: pay-per-use charges calculated based on\n\ntotal consumption during each calendar month (storage\n\nallocation calculated with per hour granularity and\n\ncumulative I/O transfer volume)\n\nBilling Period: monthly\n\nPrice Template: $0.10/GB per month of allocated\n\nstorage, $0.001/GB for I/O transfers\n\nWAN Traffic\n\nMetric: outbound network usage\n\nMeasurement: pay-per-use charges calculated based on\n\ntotal consumption for each calendar month (WAN traffic\n\nvolume calculated cumulatively)",
      "page_number": 849
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 869-888)",
      "start_page": 869,
      "end_page": 888,
      "detection_method": "synthetic",
      "content": "Billing Period: monthly\n\nPrice Template: $0.01/GB for outbound network data\n\nAdditional Considerations\n\nNegotiation – Cloud provider pricing is often open to negotiation,\n\nespecially for customers willing to commit to higher volumes or longer\n\nterms. Price negotiations can sometimes be executed online via the cloud\n\nprovider’s Web site by submitting estimated usage volumes along with\n\nproposed discounts. There are even tools available for cloud consumers to\n\nhelp generate accurate IT resource usage estimates for this purpose.\n\nPayment Options – After completing each measurement period, the cloud\n\nprovider’s billing management system calculates the amount owed by a\n\ncloud consumer. There are two common payment options available to cloud\n\nconsumers: pre-payment and post-payment. With pre-paid billing, cloud\n\nconsumers are provided with IT resource usage credits that can be applied\n\nto future usage bills. With the post-payment method, cloud consumers are\n\nbilled and invoiced for each IT resource consumption period, which is\n\nusually on a monthly basis.\n\nCost Archiving – By tracking historical billing information both cloud\n\nproviders and cloud consumers can generate insightful reports that help\n\nidentify usage and financial trends.\n\nChapter 18\n\nService Quality Metrics and SLAs\n\n18.1 Service Quality Metrics\n\n18.2 Case Study Example\n\n18.3 SLA Guidelines\n\n18.4 Case Study Example\n\nService-level agreements (SLAs) are a focal point of negotiations, contract\n\nterms, legal obligations, and runtime metrics and measurements. SLAs\n\nformalize the guarantees put forth by cloud providers, and correspondingly\n\ninfluence or determine the pricing models and payment terms. SLAs set\n\ncloud consumer expectations and are integral to how organizations build\n\nbusiness automation around the utilization of cloud-based IT resources.\n\nThe guarantees made by a cloud provider to a cloud consumer are often\n\ncarried forward, in that the same guarantees are made by the cloud\n\nconsumer organization to its clients, business partners, or whomever will be\n\nrelying on the services and solutions hosted by the cloud provider. It is\n\ntherefore crucial for SLAs and related service quality metrics to be\n\nunderstood and aligned in support of the cloud consumer’s business\n\nrequirements, while also ensuring that the guarantees can, in fact, be\n\nrealistically fulfilled consistently and reliably by the cloud provider. The\n\nlatter consideration is especially relevant for cloud providers that host\n\nshared IT resources for high volumes of cloud consumers, each of which\n\nwill have been issued its own SLA guarantees.\n\n18.1 Service Quality Metrics\n\nSLAs issued by cloud providers are human-readable documents that\n\ndescribe quality-of-service (QoS) features, guarantees, and limitations of\n\none or more cloud-based IT resources.\n\nSLAs use service quality metrics to express measurable QoS characteristics.\n\nFor example:\n\nAvailability — up-time, outages, service duration\n\nReliability — minimum time between failures, guaranteed rate of successful\n\nresponses • Performance — capacity, response time, and delivery time\n\nguarantees • Scalability — capacity fluctuation and responsiveness\n\nguarantees • Resiliency — mean-time to switchover and recovery\n\nSLA management systems use these metrics to perform periodic\n\nmeasurements that verify compliance with SLA guarantees, in addition to\n\ncollecting SLA-related data for various types of statistical analyses.\n\nEach service quality metric is ideally defined using the following\n\ncharacteristics:\n\nQuantifiable — The unit of measure is clearly set, absolute, and appropriate\n\nso that the metric can be based on quantitative measurements.\n\nRepeatable — The methods of measuring the metric need to yield identical\n\nresults when repeated under identical conditions.\n\nComparable — The units of measure used by a metric need to be\n\nstandardized and comparable. For example, a service quality metric cannot\n\nmeasure smaller quantities of data in bits and larger quantities in bytes.\n\nEasily Obtainable — The metric needs to be based on a non-proprietary,\n\ncommon form of measurement that can be easily obtained and understood\n\nby cloud consumers.\n\nThe upcoming sections provide a series of common service quality metrics,\n\neach of which is documented with description, unit of measure,\n\nmeasurement frequency, and applicable cloud delivery model values, as\n\nwell as a brief example.\n\nService Availability Metrics\n\nAvailability Rate Metric\n\nThe overall availability of an IT resource is usually expressed as a\n\npercentage of up-time. For example, an IT resource that is always available\n\nwill have an up-time of 100%.\n\nDescription — percentage of service up-time\n\nMeasurement — total up-time / total time\n\nFrequency — weekly, monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — minimum 99.5% up-time\n\nAvailability rates are calculated cumulatively, meaning that unavailability\n\nperiods are combined in order to compute the total downtime (Table 18.1).\n\nTable 18-1\n\nSample availability rates measured in units of seconds.\n\nOutage Duration Metric\n\nThis service quality metric is used to define both maximum and average\n\ncontinuous outage service-level targets.\n\nDescription — duration of a single outage\n\nMeasurement — date/time of outage end — date/time of outage start •\n\nFrequency — per event\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 1 hour maximum, 15 minute average\n\nNote\n\nIn addition to being quantitatively measured, availability can\n\nbe described qualitatively using terms such as high-\n\navailability (HA), which is used to label an IT resource with\n\nexceptionally low downtime usually due to underlying\n\nresource replication and/or clustering infrastructure.\n\nService Reliability Metrics\n\nA characteristic closely related to availability, reliability is the probability\n\nthat an IT resource can perform its intended function under pre-defined\n\nconditions without experiencing failure. Reliability focuses on how often\n\nthe service performs as expected, which requires the service to remain in an\n\noperational and available state. Certain reliability metrics only consider\n\nruntime errors and exception conditions as failures, which are commonly\n\nmeasured only when the IT resource is available.\n\nMean-Time Between Failures (MTBF) Metric\n\nDescription — expected time between consecutive service failures •\n\nMeasurement — Σ normal operational period duration / number of failures •\n\nFrequency — monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 90 day average\n\nReliability Rate Metric\n\nOverall reliability is more complicated to measure and is usually defined by\n\na reliability rate that represents the percentage of successful service\n\noutcomes. This metric measures the effects of non-fatal errors and failures\n\nthat occur during up-time periods. For example, an IT resource’s reliability\n\nis 100% if it has performed as expected every time it is invoked, but only\n\n80% if it fails to perform every fifth time.\n\nDescription — percentage of successful service outcomes under pre-\n\ndefined conditions • Measurement — total number of successful responses /\n\ntotal number of requests • Frequency — weekly, monthly, yearly\n\nCloud Delivery Model — SaaS\n\nExample — minimum 99.5%\n\nService Performance Metrics\n\nService performance refers to the ability on an IT resource to carry out its\n\nfunctions within expected parameters. This quality is measured using\n\nservice capacity metrics, each of which focuses on a related measurable\n\ncharacteristic of IT resource capacity. A set of common performance\n\ncapacity metrics is provided in this section. Note that different metrics may\n\napply, depending on the type of IT resource being measured.\n\nNetwork Capacity Metric\n\nDescription — measurable characteristics of network capacity\n\nMeasurement — bandwidth / throughput in bits per second\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 10 MB per second\n\nStorage Device Capacity Metric\n\nDescription — measurable characteristics of storage device capacity •\n\nMeasurement — storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 80 GB of storage\n\nServer Capacity Metric\n\nDescription — measurable characteristics of server capacity\n\nMeasurement — number of CPUs, CPU frequency in GHz, RAM size in\n\nGB, storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 1 core at 1.7 GHz, 16 GB of RAM, 80 GB of storage\n\nWeb Application Capacity Metric\n\nDescription — measurable characteristics of Web application capacity •\n\nMeasurement — rate of requests per minute\n\nFrequency — continuous\n\nCloud Delivery Model — SaaS\n\nExample — maximum 100,000 requests per minute\n\nInstance Starting Time Metric\n\nDescription — length of time required to initialize a new instance •\n\nMeasurement — date/time of instance up — date/time of start request •\n\nFrequency — per event\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 5 minute maximum, 3 minute average\n\nResponse Time Metric\n\nDescription — time required to perform synchronous operation\n\nMeasurement — (date/time of request — date/time of response) / total\n\nnumber of requests • Frequency — daily, weekly, monthly\n\nCloud Delivery Model — SaaS\n\nExample — 5 millisecond average\n\nCompletion Time Metric\n\nDescription — time required to complete an asynchronous task\n\nMeasurement — (date of request — date of response) / total number of\n\nrequests • Frequency — daily, weekly, monthly\n\nCloud Delivery Model — PaaS, SaaS\n\nExample — 1 second average\n\nService Scalability Metrics\n\nService scalability metrics are related to IT resource elasticity capacity,\n\nwhich is related to the maximum capacity that an IT resource can achieve,\n\nas well as measurements of its ability to adapt to workload fluctuations. For\n\nexample, a server can be scaled up to a maximum of 128 CPU cores and\n\n512 GB of RAM, or scaled out to a maximum of 16 load-balanced\n\nreplicated instances.\n\nThe following metrics help determine whether dynamic service demands\n\nwill be met proactively or reactively, as well as the impacts of manual or\n\nautomated IT resource allocation processes.\n\nStorage Scalability (Horizontal) Metric\n\nDescription — permissible storage device capacity changes in response to\n\nincreased workloads • Measurement — storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 1,000 GB maximum (automated scaling)\n\nServer Scalability (Horizontal) Metric\n\nDescription — permissible server capacity changes in response to increased\n\nworkloads • Measurement — number of virtual servers in resource pool\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 1 virtual server minimum, 10 virtual server maximum\n\n(automated scaling)\n\nServer Scalability (Vertical) Metric\n\nDescription — permissible server capacity fluctuations in response to\n\nworkload fluctuations • Measurement — number of CPUs, RAM size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 512 core maximum, 512 GB of RAM\n\nService Resiliency Metrics\n\nThe ability of an IT resource to recover from operational disturbances is\n\noften measured using service resiliency metrics. When resiliency is\n\ndescribed within or in relation to SLA resiliency guarantees, it is often\n\nbased on redundant implementations and resource replication over different\n\nphysical locations, as well as various disaster recovery systems.\n\nThe type of cloud delivery model determines how resiliency is implemented\n\nand measured. For example, the physical locations of replicated virtual\n\nservers that are implementing resilient cloud services can be explicitly\n\nexpressed in the SLAs for IaaS environments, while being implicitly\n\nexpressed for the corresponding PaaS and SaaS environments.\n\nResiliency metrics can be applied in three different phases to address the\n\nchallenges and events that can threaten the regular level of a service: •\n\nDesign Phase — Metrics that measure how prepared systems and services\n\nare to cope with challenges.\n\nOperational Phase — Metrics that measure the difference in service levels\n\nbefore, during, and after a downtime event or service outage, which are\n\nfurther qualified by availability, reliability, performance, and scalability\n\nmetrics.\n\nRecovery Phase — Metrics that measure the rate at which an IT resource\n\nrecovers from downtime, such as the meantime for a system to log an\n\noutage and switch over to a new virtual server.\n\nTwo common metrics related to measuring resiliency are as follows:\n\nMean-Time to Switchover (MTSO) Metric\n\nDescription — the time expected to complete a switchover from a severe\n\nfailure to a replicated instance in a different geographical area •\n\nMeasurement — (date/time of switchover completion — date/time of\n\nfailure) / total number of failures • Frequency — monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 10 minute average\n\nMean-Time System Recovery (MTSR) Metric\n\nDescription — time expected for a resilient system to perform a complete\n\nrecovery from a severe failure • Measurement — (date/time of recovery —\n\ndate/time of failure) / total number of failures • Frequency — monthly,\n\nyearly\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 120 minute average\n\n18.2 Case Study Example\n\nAfter suffering a cloud outage that made their Web\n\nportal unavailable for about an hour, Innovartus decides\n\nto thoroughly review the terms and conditions of their\n\nSLA. They begin by researching the cloud provider’s\n\navailability guarantees, which prove to be ambiguous\n\nbecause they do not clearly state which events in the\n\ncloud provider’s SLA management system are classified\n\nas “downtime.” Innovartus also discovers that the SLA\n\nlacks reliability and resilience metrics, which had\n\nbecome essential to their cloud service operations.\n\nIn preparation for a renegotiation of the SLA terms with\n\nthe cloud provider, Innovartus decides to compile a list\n\nof additional requirements and guarantee stipulations: •\n\nThe availability rate needs to be described in greater\n\ndetail to enable more effective management of service\n\navailability conditions.\n\nTechnical data that supports service operations models\n\nneeds to be included in order to ensure that the operation\n\nof select critical services remains fault-tolerant and\n\nresilient.\n\nAdditional metrics that assist in service quality\n\nassessment need to be included.\n\nAny events that are to be excluded from what is\n\nmeasured with availability metrics need to be clearly\n\ndefined.\n\nAfter several conversations with the cloud provider sales\n\nrepresentative, Innovartus is offered a revised SLA with\n\nthe following additions: • The method by which the\n\navailability of cloud services are to be measured, in\n\naddition to any supporting IT resources on which ATN\n\ncore processes depend.\n\nInclusion of a set of reliability and performance metrics\n\napproved by Innovartus.\n\nSix months later, Innovartus performs another SLA\n\nmetrics assessment and compares the newly generated\n\nvalues with ones that were generated prior to the SLA\n\nimprovements (Table 18.2).\n\nTable 18-2\n\nThe evolution of Innovartus’ SLA evaluation, as\n\nmonitored by their cloud resource administrators.\n\n18.3 SLA Guidelines\n\nThis section provides a number of best practices and recommendations for\n\nworking with SLAs, the majority of which are applicable to cloud\n\nconsumers: • Mapping Business Cases to SLAs — It can be helpful to\n\nidentify the necessary QoS requirements for a given automation solution\n\nand to then concretely link them to the guarantees expressed in the SLAs\n\nfor IT resources responsible for carrying out the automation. This can avoid\n\nsituations where SLAs are inadvertently misaligned or perhaps\n\nunreasonably deviate in their guarantees, subsequent to IT resource usage.\n\nWorking with Cloud and On-Premise SLAs — Due to the vast infrastructure\n\navailable to support IT resources in public clouds, the QoS guarantees\n\nissued in SLAs for cloud-based IT resources are generally superior to those\n\nprovided for on-premise IT resources. This variance needs to be understood,\n\nespecially when building hybrid distributed solutions that utilize both on-\n\npremise and cloud-based services or when incorporating cross-environment\n\ntechnology architectures, such as cloud bursting.\n\nUnderstanding the Scope of an SLA — Cloud environments are comprised\n\nof many supporting architectural and infrastructure layers upon which IT\n\nresources reside and are integrated. It is important to acknowledge the\n\nextent to which a given IT resource guarantee applies. For example, an SLA\n\nmay be limited to the IT resource implementation but not its underlying\n\nhosting environment.\n\nUnderstanding the Scope of SLA Monitoring — SLAs need to specify\n\nwhere monitoring is performed and where measurements are calculated,\n\nprimarily in relation to the cloud’s firewall. For example, monitoring within\n\nthe cloud firewall is not always advantageous or relevant to the cloud\n\nconsumer’s required QoS guarantees. Even the most efficient firewalls have\n\na measurable degree of influence on performance and can further present a\n\npoint of failure.\n\nDocumenting Guarantees at Appropriate Granularity — SLA templates\n\nused by cloud providers sometimes define guarantees in broad terms. If a\n\ncloud consumer has specific requirements, the corresponding level of detail\n\nshould be used to describe the guarantees. For example, if data replication\n\nneeds to take place across particular geographic locations, then these need\n\nto be specified directly within the SLA.\n\nDefining Penalties for Non-Compliance — If a cloud provider is unable to\n\nfollow through on the QoS guarantees promised within the SLAs, recourse\n\ncan be formally documented in terms of compensation, penalties,\n\nreimbursements, or otherwise.\n\nIncorporating Non-Measurable Requirements — Some guarantees cannot\n\nbe easily measured using service quality metrics, but are relevant to QoS\n\nnonetheless, and should therefore still be documented within the SLA. For\n\nexample, a cloud consumer may have specific security and privacy\n\nrequirements for data hosted by the cloud provider that can be addressed by\n\nassurances in the SLA for the cloud storage device being leased.\n\nDisclosure of Compliance Verification and Management — Cloud providers\n\nare often responsible for monitoring IT resources to ensure compliance with\n\ntheir own SLAs. In this case, the SLAs themselves should state what tools\n\nand practices are being used to carry out the compliance checking process,\n\nin addition to any legal-related auditing that may be occurring.\n\nInclusion of Specific Metric Formulas — Some cloud providers do not\n\nmention common SLA metrics or the metrics-related calculations in their\n\nSLAs, instead focusing on service-level descriptions that highlight the use\n\nof best practices and customer support. Metrics being used to measure\n\nSLAs should be part of the SLA document, including the formulas and\n\ncalculations that the metrics are based upon.\n\nConsidering Independent SLA Monitoring — Although cloud providers will\n\noften have sophisticated SLA management systems and SLA monitors, it\n\nmay be in the best interest of a cloud consumer to hire a third-party\n\norganization to perform independent monitoring as well, especially if there\n\nare suspicions that SLA guarantees are not always being met by the cloud\n\nprovider (despite the results shown on periodically issued monitoring\n\nreports).\n\nArchiving SLA Data — The SLA-related statistics collected by SLA\n\nmonitors are commonly stored and archived by the cloud provider for future\n\nreporting purposes. If a cloud provider intends to keep SLA data specific to\n\na cloud consumer even after the cloud consumer no longer continues its\n\nbusiness relationship with the cloud provider, then this should be disclosed.\n\nThe cloud consumer may have data privacy requirements that disallow the\n\nunauthorized storage of this type of information. Similarly, during and after",
      "page_number": 869
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 889-911)",
      "start_page": 889,
      "end_page": 911,
      "detection_method": "synthetic",
      "content": "a cloud consumer’s engagement with a cloud provider, it may want to keep\n\na copy of historical SLA-related data as well. It may be especially useful for\n\ncomparing cloud providers in the future.\n\nDisclosing Cross-Cloud Dependencies — Cloud providers may be leasing\n\nIT resources from other cloud providers, which results in a loss of control\n\nover the guarantees they are able to make to cloud consumers. Although a\n\ncloud provider will rely on the SLA assurances made to it by other cloud\n\nproviders, the cloud consumer may want disclosure of the fact that the IT\n\nresources it is leasing may have dependencies beyond the environment of\n\nthe cloud provider organization that it is leasing them from.\n\n18.4 Case Study Example\n\nDTGOV begins its SLA template authoring process by\n\nworking with a legal advisory team that has been\n\nadamant about an approach whereby cloud consumers\n\nare presented with an online Web page outlining the SLA\n\nguarantees, along with a “click-once-to-accept” button.\n\nThe default agreement contains extensive limitations to\n\nDTGOV’s liability in relation to possible SLA non-\n\ncompliance, as follows: • The SLA defines guarantees\n\nonly for service availability.\n\nService availability is defined for all of the cloud\n\nservices simultaneously.\n\nService availability metrics are loosely defined to\n\nestablish a level of flexibility regarding unexpected\n\noutages.\n\nThe terms and conditions are linked to the Cloud\n\nServices Customer Agreement, which is accepted\n\nimplicitly by all of the cloud consumers that use the self-\n\nservice portal.\n\nExtended periods of unavailability are to be\n\nrecompensed by monetary “service credits,” which are to\n\nbe discounted on future invoices and have no actual\n\nmonetary value.\n\nProvided here are key excerpts from DTGOV’s SLA\n\ntemplate:\n\nScope and Applicability\n\nThis Service Level Agreement (“SLA”) establishes the\n\nservice quality parameters that are to be applied to the\n\nuse of DTGOV’s cloud services (“DTGOV cloud”), and\n\nis part of the DTGOV Cloud Services Customer\n\nAgreement (“DTGOV Cloud Agreement”).\n\nThe terms and conditions specified in this agreement\n\napply solely to virtual server and cloud storage device\n\nservices, herein called “Covered Services.” This SLA\n\napplies separately to each cloud consumer (“Consumer”)\n\nthat is using the DTGOV Cloud. DTGOV reserves the\n\nright to change the terms of this SLA in accordance with\n\nthe DTGOV Cloud Agreement at any time.\n\nService Quality Guarantees\n\nThe Covered Services will be operational and available\n\nto Consumers at least 99.95% of the time in any calendar\n\nmonth. If DTGOV does not meet this SLA requirement\n\nwhile the Consumer succeeds in meeting its SLA\n\nobligations, the Consumer will be eligible to receive\n\nFinancial Credits as compensation. This SLA states the\n\nConsumer’s exclusive right to compensation for any\n\nfailure on DTGOV’s part to fulfill the SLA\n\nrequirements.\n\nDefinitions\n\nThe following definitions are to be applied to DTGOV’s\n\nSLA:\n\n“Unavailability” is defined as the entirety of the\n\nConsumer’s running instances as having no external\n\nconnectivity for a duration that is at least five\n\nconsecutive minutes in length, during which the\n\nConsumer is unable to launch commands against the\n\nremote administration system through either the Web\n\napplication or Web service API.\n\n“Downtime Period” is defined as a period of five or\n\nmore consecutive minutes of the service remaining in a\n\nstate of Unavailability. Periods of “Intermittent\n\nDowntime” that are less than five minutes long do not\n\ncount towards Downtime Periods.\n\n“Monthly Up-time Percentage” (MUP) is calculated as:\n\n(total number of minutes in a month — total number of\n\ndowntime period minutes in a month) / (total number of\n\nminutes in a month) • “Financial Credit” is defined as\n\nthe percentage of the monthly invoice total that is\n\ncredited towards future monthly invoices of the\n\nConsumer, which is calculated as follows: 99.00% <\n\nMUP % < 99.95% — 10% of the monthly invoice is\n\ncredited in favor of the Consumer’s invoice 89.00% <\n\nMUP % < 99.00% — 30% of the monthly invoice is\n\ncredited in favor of the Consumer’s invoice MUP % <\n\n89.00% — 100% of the monthly invoice is credited in\n\nfavor of the -Consumer’s invoice Usage of Financial\n\nCredits\n\nThe MUP for each billing period is to be displayed on\n\neach monthly invoice. The Consumer is to submit a\n\nrequest for Financial Credit in order to be eligible to\n\nredeem Financial Credits. For that purpose, the\n\nConsumer is to notify DTGOV within thirty days from\n\nthe time the Consumer receives the invoice that states\n\nthe MUP beneath the defined SLA. Notification is to be\n\nsent to DTGOV via e-mail. Failure to comply with this\n\nrequirement forfeits the Consumer’s right to the\n\nredemption of Financial Credits.\n\nSLA Exclusions\n\nThe SLA does not apply to any of the following:\n\nUnavailability periods caused by factors that cannot be\n\nreasonably foreseen or prevented by DTGOV.\n\nUnavailability periods resulting from the malfunctioning\n\nof the Consumer’s software and/or hardware, third party\n\nsoftware and/or hardware, or both.\n\nUnavailability periods resulting from abuse or\n\ndetrimental behavior and actions that are in violation of\n\nthe DTGOV Cloud Agreement.\n\nConsumers with overdue invoices or are otherwise not\n\nconsidered in good standing with DTGOV.\n\nPart V\n\nAppendices\n\nAppendix A: Case Study Conclusions\n\nAppendix B: Common Containerization Technologies\n\nAppendix A\n\nCase Study Conclusions\n\nA.1 ATN\n\nA.2 DTGOV\n\nA.3 Innovartus\n\nThis appendix briefly concludes the storylines of the three case studies that\n\nwere first introduced in Chapter 2.\n\nA.1 ATN\n\nThe cloud initiative necessitated migrating selected applications and IT\n\nservices to the cloud, allowing for the consolidation and retirement of\n\nsolutions in a crowded application portfolio. Not all of the applications\n\ncould be migrated, and selecting appropriate applications was a major issue.\n\nSome of the chosen applications required significant re-development effort\n\nto adapt to the new cloud environment.\n\nCosts were effectively reduced for most of the applications that were moved\n\nto the cloud. This was discovered after six months of expenditures were\n\ncompared with the costs of the traditional applications over a three year\n\nperiod. Both capital and operational expenses were used in the ROI\n\nevaluation.\n\nATN’s level of service has improved in business areas that use cloud-based\n\napplications. In the past, most of these applications showed a noticeable\n\nperformance deterioration during peak usage periods. The cloud-based\n\napplications can now scale out whenever a peak workload arises.\n\nATN is currently evaluating other applications for potential cloud\n\nmigration.\n\nA.2 DTGOV\n\nAlthough DTGOV has been outsourcing IT resources for public sector\n\norganizations for more than 30 years, establishing the cloud and its\n\nassociated IT infrastructure was a major undertaking that took over two\n\nyears. DTGOV now offers IaaS services to the government sector and is\n\nbuilding a new cloud service portfolio that targets private sector\n\norganizations.\n\nDiversification of its client and service portfolios is the next logical step for\n\nDTGOV, after all of the changes they made to their technology architecture\n\nto produce a mature cloud. Before proceeding with this next phase,\n\nDTGOV produces a report to document aspects of its completed transition\n\nto cloud adoption. A summary of the report is documented in Table A.1.\n\nTable A.1\n\nThe results of an analysis of DTGOV’s cloud initiative.\n\nA.3 Innovartus\n\nThe business objective of increasing company growth required the original\n\ncloud to undergo major modifications, since they needed to move from their\n\nregional cloud provider to a large-scale global cloud provider. Portability\n\nissues were discovered only after the move, and a new cloud provider\n\nprocurement process had to be created when the regional cloud provider\n\nwas unable to meet all of their needs. Data recovery, application migration,\n\nand interoperability issues were also addressed.\n\nHighly available computing IT resources and the pay-per-use feature were\n\nkey in developing Innovartus’ business feasibility, since access to funding\n\nand investment resources were not initially available.\n\nInnovartus has defined several business goals they plan to achieve over the\n\nnext couple of years:\n\nAdditional applications will be migrated to different clouds, using multiple\n\ncloud providers in order to improve resiliency and reduce dependency on\n\nindividual cloud provider vendors.\n\nA new mobile-only business area is to be created, since mobile access to\n\ntheir cloud services has experienced 20% growth.\n\nThe application platform developed by Innovartus is being evaluated as a\n\nvalue-added PaaS to be offered to companies that require enhanced and\n\ninnovative UI-centric features for both web-based and mobile application\n\ndevelopment.\n\nAppendix B\n\nCommon Containerization Technologies\n\nB.1 Docker\n\nB.2 Kubernetes\n\nAs a supplement to Chapter 6, the appendix explores the Docker container\n\nengine and the Kubernetes containerization platform and further explains\n\nhow they are commonly utilized. This content helps illustrate how the\n\nterms, concepts and technologies described in Chapter 6 exist in real-world\n\nenvironments.\n\nNote the following:\n\nA Kubernetes platform needs to be deployed on a host cluster. The Docker\n\ncontainer engine needs to be separately deployed on each host in that\n\ncluster.\n\nBoth Docker and Kubernetes introduce distinct terminology. Wherever\n\napplicable, the terms established in Chapter 6 are referenced in the\n\nupcoming sections. Often, they are shown in parenthesis next to the\n\ncorresponding Docker or Kubernetes terms.\n\nB.1 Docker\n\nDocker is the first containerization engine to have become widely popular\n\nin the industry. Docker containers, also known as Dockers, introduce many\n\nimportant benefits and features, which will be covered in the following\n\nsections.\n\nFrom an architecture perspective, a Docker container solution can be\n\ndivided into the following four key areas:\n\nDocker Server\n\nDocker Client\n\nDocker Registry\n\nDocker Objects\n\nDocker Server\n\nA Docker server, also known as a Docker host, is a host that is running a\n\nDocker containerization engine. From a technology architecture\n\nperspective, a Docker host is a physical server or virtual machine running a\n\nWindows or Linux operating system. A Docker server can be installed on\n\nany Windows or Linux machine supporting X86-64 or ARM and a few\n\nother CPU architectures. A Docker container engine can be installed on any\n\nsystem capable of running these operating systems.\n\nDocker servers provide the following key features to a containerization\n\nsolution:\n\nDocker servers provide the key functional services required to run the\n\ncontainerization solution and to containerize applications. These services\n\nare provided by the Docker daemon, which is the containerization engine in\n\nthe Docker container solution. The Docker daemon has many different\n\ncomponents and subsystems designed and deployed as part of each version\n\nof the Docker software. It is responsible for scheduling, restarting and\n\nshutting down containers, as well as managing any container interaction.\n\nDocker servers host the containers that host the applications. Each container\n\nis deployed on a container host. A solution may have one or more Docker\n\nservers hosting containers and their applications.\n\nDocker servers also host the images used by the containers that they host,\n\nwhich allows different containers to use and share the same base image\n\nwithout the need to deploy multiple images for multiple containers.\n\nDocker Client\n\nA Docker client is a component that runs different tools which enable users\n\nand service consumers to interact with the Docker server and its services.\n\nDocker container solutions support the following two types of clients:\n\nApplication Programming Interface (API)\n\nCommand Line Interface (CLI)\n\nA Docker container does not provide a graphical user interface (GUI) for\n\ninteracting with or configuring services. This reduces its footprint because it\n\ndoes not require heavy GUIs to be rendered and made available for users to\n\ninteract with. This also results in less code required to be maintained and\n\nmanaged, which further reduces the security risk of the containerization\n\nsolution.\n\nAs shown in Figure B.1, the Docker client can be used to interact with the\n\nDocker server or Docker host in order to deploy, maintain and manage\n\ncontainers and their applications. All of the interactions shown in the\n\ndiagram occur through the use of the Docker daemon service, which acts as\n\nthe container engine in Docker solutions. The Docker daemon controls\n\naccess to the containers and provides a way for clients to interact with each\n\ncontainer.\n\nFigure B.1\n\nThe Docker client uses an API or CLI to interact with containers via the\n\nDocker daemon service.\n\nThe Docker daemon service provides REST-based APIs that the Docker\n\nclient can consume as an API client. The purpose of these APIs is to provide\n\nstandard interfaces through which the service consumer can interact with\n\nthe Docker engine in order to deploy and manage their containers.\n\nA Docker client can be run on a variety of operating systems, including\n\nWindows, Linux and Mac.\n\nDocker Registry\n\nA Docker registry is a repository (image registry) that is used to store\n\ndifferent types of Docker images, which are used by the Docker host to\n\ndeploy containers (Figure B.2). The Docker registry is able to host and\n\ndeploy multiple container images, as well as different versions of the same\n\nimages. This allows application owners and system administrators to decide\n\nwhat container image or container image version to use when deploying\n\ncontainers and their applications.\n\nFigure B.2\n\nThe Docker registry hosts the container images that can be used to deploy\n\nDocker containers.\n\nThe separation of the registry from the host and the Docker daemon service\n\n(container engine) has further made it possible for Docker containers to\n\nprovide a repository of different images without increasing any load or\n\nstorage footprint on the Docker host.\n\nDocker provides a public repository of different images based on standard\n\noperating systems, such as Windows and Linux. This public repository is\n\nalso known as Docker hub, and the images within the repository can be\n\nused to deploy containers. If the standard Docker images provided by the\n\nDocker hub are being used, it is not necessary to deploy the Docker registry\n\nor allocate any additional storage to build a registry.\n\nDocker further allows users to have private Docker registries. For instance,\n\ndue to security requirements, Docker images may need to be kept internal to\n\nan organization and not accessible to anyone outside of the organization. In\n\nthis case, a private Docker registry can be deployed to house the Docker\n\nimages that will be configured and used for the containerization solution.\n\nDocker provides the following three key commands:\n\nDocker Push – used to add an image to the registry\n\nDocker Pull – used to download an image from the Docker registry in order\n\nto run a container\n\nDocker Run – used to run and start the container using a specific image\n\nDocker Objects\n\nA Docker container solution can have several different subcomponents and\n\nkey elements collectively referred to as Docker objects. This section\n\nintroduces the following key Docker objects:\n\nDocker Container – A Docker container is an instance of a container image\n\nand represents the actual container that will host the application. A\n\ncontainer can be stopped, started, deleted or scheduled to run at a specific\n\ntime.\n\nDocker Images – Images are read-only templates that are created and\n\ndeployed in the Docker registry and can be used to develop and run\n\ncontainers. For example, a base image can be created for the Linux\n\noperating system and can then be used to deploy several different\n\ncontainers. Each container can then make specific changes on top of the\n\nbase image to make the container environment suitable for different\n\napplications. However, the containers cannot modify the base image.\n\nServices – Docker containers introduce and use many different services to\n\nrun the Docker container solution, many of which are internal to the Docker\n\nengine and are not accessible for direct interaction. The most critical\n\nDocker services are the Docker daemon (container engine) and swarm\n\n(container orchestrator).\n\nNamespaces – In order to provide the capability of hosting several different\n\ncontainers on the same Docker host, the Docker containerization engine\n\nrequires a means of isolating containers from each other to provide a secure\n\nenvironment where multiple containers can be deployed. Docker uses a\n\ntechnology called namespaces to provide secure isolation of containers\n\nfrom each other. This further enables Docker containers to securely isolate\n\nthe processes in network interfaces and many other elements of containers\n\nfrom each other, even though they are hosted on the same Docker host.\n\nDocker Control Groups – In order to ensure that Docker containers use\n\ncertain system resources to run and become operational and functional, the\n\nDocker host and Docker daemon need to enable the container to access the\n\nresources provided by the Docker host. This is established through the\n\ncontrol groups, which limit an application deployed inside a container to a\n\nspecific set of Docker host resources.\n\nUnion File System (container image layers) – Union file systems, also\n\nreferred to as unionFS, are the file systems that enable a Docker container\n\nengine to create lightweight and fast writeable layers on top of the base\n\ncontainer image in order to create a writable environment for applications\n\nand containers to make their own specific configurations, as required. This\n\nallows the container engine to operate as expected without the need to\n\nmodify the base image.\n\nDocker Orchestrator (container orchestrator) – Docker containers provide\n\ntheir own embedded orchestration components which can be used to\n\norchestrate the container solution in order to improve its productivity and\n\nautomate repeatable tasks that it is required to perform. The Docker\n\norchestrator is embedded in the Docker container engine and can be used\n\nby system administrators or application developers to orchestrate tasks.\n\nDocker Swarm (container orchestrator)\n\nAlthough deploying multiple different containers on the same Docker host\n\ncan introduce many benefits in terms of saving and sharing resources\n\namong containers and applications, it can also introduce risk whereby if the\n\nhost is lost, the applications and containers hosting those applications will\n\nalso be lost. To prevent this issue, Docker containers can use Docker\n\nSwarm.\n\nDocker Swarm is a container orchestrator that is deployed as part of a\n\nDocker container solution. The Docker Swarm functionality is controlled by\n\nthe swarm service, a key Docker container service that is used to create and\n\nmanage a cluster of Docker hosts, also referred to as a swarm, in order to\n\nbalance the load across different physical hosts. It can also be used to\n\nimprove the availability of the Docker containers, allowing application\n\nowners to deploy multiple instances of the same container on different\n\nDocker hosts while the cluster is managed by Docker Swarm. Each cluster\n\nof Docker container hosts inside the swarm is managed by a service known\n\nas the cluster manager.\n\nFigure B.3 shows the logical architecture of Docker Swarm.\n\nFigure B.3\n\nThe logical view of a swarm cluster comprised of three Docker hosts.\n\nDocker container solutions can be deployed on private systems in private\n\ndata centers or servers, or on different cloud platforms from public cloud\n\nproviders that provide containers as a service with the use of Docker,\n\nincluding Amazon Web Services, Microsoft Azure and Google Cloud.\n\nB.2 Kubernetes\n\nKubernetes, also known as K8s, is an open source container orchestrator\n\nthat provides key benefits and features that enhance Docker. Kubernetes\n\nintroduces the concept of clusters that can span several different hosts. This\n\nsystem takes the container functionality provided by a containerization\n\nengine like Docker to the next level by providing an enterprise-grade",
      "page_number": 889
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 912-930)",
      "start_page": 912,
      "end_page": 930,
      "detection_method": "synthetic",
      "content": "architecture and construct that is more suitable for enterprise-grade\n\napplications and larger scale distributed systems, which is well-suited for\n\ncomplex applications. This section introduces the key components of a\n\nKubernetes solution.\n\nKubernetes Node (host)\n\nIn a Kubernetes architecture running Kubernetes software to host its\n\ncontainers, a Kubernetes node is the equivalent to the containerization host\n\nor Docker host discussed in the preceding Docker section. Each Kubernetes\n\ncluster (host cluster) can have one or more nodes.\n\nFigure B.4 shows a Kubernetes node, also known as a Kubernetes host, as\n\nthe key component of a Kubernetes solution.\n\nFigure B.4\n\nThe Kubernetes node is used to host containers, in this case two instances\n\nof Kubernetes pods.\n\nEach Kubernetes node (host) has three key components that enable\n\nKubernetes to host containers in pods, which will be explained later in this\n\nsection:\n\nKubelet\n\nKube-Proxy\n\nContainer Runtime\n\nKubernetes Pod\n\nA Kubernetes pod is a logical boundary or logical group of different\n\ncontainers that share storage and network resources on the same Kubernetes\n\nnode. The containers deployed in each pod also share the same\n\nconfiguration and specifications regarding how to run them. For example,\n\nevery container hosted inside a pod will always be hosted together on the\n\nsame Kubernetes node in the cluster.\n\nFigure B.5 shows a logical view of a pod and how the pod is used for the\n\nlogical separation of containers on the same host.\n\nFigure B.5\n\nPods can be used to logically group and isolate a set of containers from\n\nother containers.\n\nKubelet\n\nA kubelet is a service agent that is deployed on each node inside a cluster. It\n\nis responsible for ensuring that containers configured to run in each pod are\n\noperational and running as expected.\n\nKube-Proxy\n\nA kube-proxy is a service that runs in each Kubernetes node. It acts as a\n\nservice proxy that enables the containers deployed inside a pod to access\n\nthe network resources and further communicate with the external world.\n\nEach kube-proxy maintains network rules on nodes. These network rules\n\ncan be defined by system administrators and are used to allow internal and\n\nexternal network communication to each pod in a Kubernetes cluster.\n\nFigure B.6 shows the concept of the Kubernetes node and its kubelet and\n\nkube-proxy components.\n\nFigure B.6\n\nThe Kubernetes cluster is comprised of two nodes: Kubernetes Node A and\n\nKubernetes Node B. Each node has its own kubelet and kube-proxy to serve\n\nits pods.\n\nContainer Runtime (container engine)\n\nIn a Kubernetes architecture, the container engine (referred to as the\n\ncontainer runtime) enables a solution to leverage the Kubernetes\n\ntechnology architecture and its features to deploy a variety of containers. In\n\naddition to supporting different container runtimes, Kubernetes also offers\n\nits own, known as the container runtime interface (CRI). It is similar to a\n\nDocker container engine. As an alternative to using CRI to host containers,\n\na Docker container engine can be deployed to host Docker containers on\n\ntop of Kubernetes nodes (Figures B.7 and B.8).\n\nFigure B.7\n\nA Kubernetes node hosting containers using CRI as a container engine.\n\nFigure B.8\n\nA Kubernetes node running a Docker container engine at runtime to offer\n\ncontainers.\n\nCluster\n\nIn a Kubernetes architecture, a cluster is a group of nodes that work\n\ntogether to provide highly scalable and available solutions for deploying\n\ncontainers to host applications. As shown in Figure B.9, a cluster can\n\ncontain several different nodes.\n\nFigure B.9\n\nAn example of a Kubernetes cluster containing three different nodes.\n\nUnlike Docker containers, which introduce the concept of a swarm for\n\ngrouping multiple different Docker container hosts, Kubernetes introduces a\n\nmuch more comprehensive concept and technology architecture for creating\n\na cluster of containerization hosts that is suitable for enterprise applications.\n\nKubernetes Control Plane\n\nA control plane can be used in a Kubernetes cluster architecture in order to\n\noffer better services and more capabilities that enable application owners\n\nand system administrators to utilize a containerization solution to its full\n\npotential. The control plane is responsible for making decisions that are\n\napplicable to the entire cluster, granting system administrators or\n\napplication owners a common set of tools for managing the nodes in a\n\ncluster. This section introduces the key components of a control plane\n\nwithin a Kubernetes cluster.\n\nKubernetes API – The Kubernetes API provides a method for application\n\nowners, system administrators and developers to interact with the\n\nKubernetes architecture, its nodes and the containers deployed in each\n\nKubernetes cluster.\n\nkube-apiserver – The kube-apiserver exposes Kubernetes APIs to service\n\nconsumers so they can interact with the Kubernetes cluster and its\n\ncomponents, as well as the containers deployed inside the cluster, through\n\nthe API. The kube-apiserver is deployed as an independent component that\n\ncan be horizontally scaled to handle a greater volume of API calls and\n\nrequests from service consumers in order to accommodate the performance\n\nrequired by a solution as it scales.\n\netcd – The etcd service is used to store the configuration of the cluster data\n\ninside the control plane. This does not include any user data or application\n\ndata from the containerized application.\n\nkube-scheduler – The kube-scheduler is responsible for scheduling and\n\nrunning containers. Each time a new container is deployed, the kube-\n\nscheduler checks the resource utilization of the nodes inside the clusters, as\n\nwell as the different pods deployed on each node, in order to identify the\n\nbest place to schedule and run the new container.\n\nkube-controller-manager – The kube-controller-manager component is\n\nresponsible for running and managing the control plane processes. In the\n\ncontext of a Kubernetes solution, each of the above control plane\n\ncomponents will run as their own independent and separate process. The\n\nkube-controller-manager provides a simple way of managing all of the\n\nabove components and their associated processes from a central point of\n\nview, thus offering system administrators a way to manage the control plane\n\nof a Kubernetes solution.\n\ncloud-controller-manager – The cloud-controller-manager component\n\ncomes into play when a solution is deployed in a public cloud or any type of\n\ncloud that allows the cloud provider’s APIs to be accessed. For example, if\n\na Kubernetes solution is deployed on Amazon Web Services, Microsoft\n\nAzure or Google Cloud, then this component exposes those specific\n\nenvironments’ APIs to the cluster. However, if the solution is not deployed\n\nin a cloud environment, then this component is not required.\n\nFigure B.10 shows an example of an overall Kubernetes deployment\n\narchitecture.\n\nFigure B.10\n\nA Kubernetes cluster with two nodes and a control plane, including the\n\ncontrol plane’s components.\n\nAs illustrated in the previous figure, the cluster control plane is deployed on\n\na separate server than the Kubernetes nodes. This is done with the purpose\n\nof eliminating any interdependencies regarding the availability of the\n\ncomponents required for managing the cluster, as well as to ensure that the\n\ncontrol plane is not impacted if a node fails.",
      "page_number": 912
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 931-957)",
      "start_page": 931,
      "end_page": 957,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 931
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 958-983)",
      "start_page": 958,
      "end_page": 983,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 958
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 984-1001)",
      "start_page": 984,
      "end_page": 1001,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 984
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 1002-1002)",
      "start_page": 1002,
      "end_page": 1002,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 1002
    }
  ],
  "pages": [
    {
      "page_number": 4,
      "content": "Cloud Computing\n\nConcepts, Technology, Security, and Architecture\n\nSecond Edition\n\nThomas Erl\n\nEric Barcelo\n\nPearson",
      "content_length": 116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Contents at a Glance\n\nChapter 1 Introduction\n\nChapter 2 Case Study Background\n\nChapter 3 Understanding Cloud Computing\n\nPart I Fundamental Cloud Computing\n\nChapter 4 Fundamental Concepts and Models\n\nChapter 5 Cloud-Enabling Technology\n\nChapter 6 Understanding Containerization\n\nChapter 7 Understanding Cloud Security and Cybersecurity\n\nPart II Cloud Computing Mechanisms\n\nChapter 8 Cloud Infrastructure Mechanisms\n\nChapter 9 Specialized Cloud Mechanisms\n\nChapter 10 Cloud and Cybersecurity Access-Oriented Mechanisms\n\nChapter 11 Cloud and Cyber Security Data-Oriented Mechanisms\n\nChapter 12 Cloud Management Mechanisms\n\nPart III Cloud Computing Architecture\n\nChapter 13 Fundamental Cloud Architectures\n\nChapter 14 Advanced Cloud Architectures\n\nChapter 15 Specialized Cloud Architectures\n\nPart IV Working with Clouds\n\nChapter 16 Cloud Delivery Model Considerations\n\nChapter 17 Cost Metrics and Pricing Models\n\nChapter 18 Service Quality Metrics and SLAs",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Part V Appendices\n\nAppendix A Case Study Conclusions\n\nAppendix B Common Containerization Technologies",
      "content_length": 101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Table of Contents\n\nChapter 1 Introduction\n\n1.1 Objectives of This Book\n\n1.2 What This Book Does Not Cover\n\n1.3 Who This Book Is For\n\n1.4 How This Book Is Organized\n\n1.5 Resources\n\nChapter 2 Case Study Background\n\n2.1 Case Study #1: ATN\n\n2.2 Case Study #2: DTGOV\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nChapter 3 Understanding Cloud Computing\n\n3.1 Origins and Influences\n\n3.2 Basic Concepts and Terminology\n\n3.3 Goals and Benefits\n\n3.4 Risks and Challenges\n\nPart I Fundamental Cloud Computing\n\nChapter 4 Fundamental Concepts and Models\n\n4.1 Roles and Boundaries\n\n4.2 Cloud Characteristics\n\n4.3 Cloud Delivery Models\n\n4.4 Cloud Deployment Models\n\nChapter 5 Cloud-Enabling Technology",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "5.1 Networks and Internet Architecture\n\n5.2 Cloud Data Center Technology\n\n5.3 Modern Virtualization\n\n5.4 Multitenant Technology\n\n5.5 Service Technology and Service APIs\n\n5.6 Case Study Example\n\nChapter 6 Understanding Containerization\n\n6.1 Origins and Influences\n\n6.2 Fundamental Virtualization and Containerization\n\n6.3 Understanding Containers\n\n6.4 Understanding Container Images\n\n6.5 Multi-Container Types\n\n6.6 Case Study Example\n\nChapter 7 Understanding Cloud Security and Cybersecurity\n\n7.1 Basic Security Terminology\n\n7.2 Basic Threat Terminology\n\n7.3 Threat Agents\n\n7.4 Common Threats\n\n7.5 Case Study Example\n\n7.6 Additional Considerations\n\n7.7 Case Study Example\n\nPart II Cloud Computing Mechanisms\n\nChapter 8 Cloud Infrastructure Mechanisms\n\n8.1 Logical Network Perimeter",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "8.2 Virtual Server\n\n8.3 Hypervisor\n\n8.4 Cloud Storage Device\n\n8.5 Cloud Usage Monitor\n\n8.6 Resource Replication\n\n8.7 Ready-Made Environment\n\n8.8 Container\n\nChapter 9 Specialized Cloud Mechanisms\n\n9.1 Automated Scaling Listener\n\n9.2 Load Balancer\n\n9.3 SLA Monitor\n\n9.4 Pay-Per-Use Monitor\n\n9.5 Audit Monitor\n\n9.6 Failover System\n\n9.7 Resource Cluster\n\n9.8 Multi-Device Broker\n\n9.9 State Management Database\n\nChapter 10 Cloud and Cybersecurity Access-Oriented Mechanisms\n\n10.1 Encryption\n\n10.2 Hashing\n\n10.3 Digital Signature\n\n10.4 Cloud-Based Security Groups\n\n10.5 Public Key Infrastructure (PKI) System\n\n10.6 Single Sign-On (SSO) System",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "10.7 Hardened Virtual Server Image\n\n10.8 Firewall\n\n10.9 Virtual Private Network (VPN)\n\n10.10 Biometric Scanner\n\n10.11 Multi-Factor Authentication (MFA) System\n\n10.12 Identity and Access Management (IAM) System\n\n10.13 Intrusion Detection System (IDS)\n\n10.14 Penetration Testing Tool\n\n10.15 User Behavior Analytics (UBA) System\n\n10.16 Third-Party Software Update Utility\n\n10.17 Network Intrusion Monitor\n\n10.18 Authentication Log Monitor\n\n10.19 VPN Monitor\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nChapter 11 Cloud and Cyber Security Data-Oriented Mechanisms\n\n11.1 Digital Virus Scanning and Decryption System\n\n11.2 Digital Immune System\n\n11.3 Malicious Code Analysis System\n\n11.4 Data Loss Prevention (DLP) System\n\n11.5 Trusted Platform Module (TPM)\n\n11.6 Data Backup and Recovery System\n\n11.7 Activity Log Monitor\n\n11.8 Traffic Monitor",
      "content_length": 872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "11.9 Data Loss Protection Monitor\n\nChapter 12 Cloud Management Mechanisms\n\n12.1 Remote Administration System\n\n12.2 Resource Management System\n\n12.3 SLA Management System\n\n12.4 Billing Management System\n\nPart III Cloud Computing Architecture\n\nChapter 13 Fundamental Cloud Architectures\n\n13.1 Workload Distribution Architecture\n\n13.2 Resource Pooling Architecture\n\n13.3 Dynamic Scalability Architecture\n\n13.4 Elastic Resource Capacity Architecture\n\n13.5 Service Load Balancing Architecture\n\n13.6 Cloud Bursting Architecture\n\n13.7 Elastic Disk Provisioning Architecture\n\n13.8 Redundant Storage Architecture\n\n13.9 Multi-Cloud Architecture\n\n13.10 Case Study Example\n\nChapter 14 Advanced Cloud Architectures\n\n14.1 Hypervisor Clustering Architecture\n\n14.2 Virtual Server Clustering Architecture\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\n14.4 Non-Disruptive Service Relocation Architecture\n\n14.5 Zero Downtime Architecture",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "14.6 Cloud Balancing Architecture\n\n14.7 Resilient Disaster Recovery Architecture\n\n14.8 Distributed Data Sovereignty Architecture\n\n14.9 Resource Reservation Architecture\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\n14.11 Rapid Provisioning Architecture\n\n14.12 Storage Workload Management Architecture\n\n14.13 Virtual Private Cloud Architecture\n\n14.14 Case Study Example\n\nChapter 15 Specialized Cloud Architectures\n\n15.1 Direct I/O Access Architecture\n\n15.2 Direct LUN Access Architecture\n\n15.3 Dynamic Data Normalization Architecture\n\n15.4 Elastic Network Capacity Architecture\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\n15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\n15.7 Load Balanced Virtual Switches Architecture\n\n15.8 Multipath Resource Access Architecture\n\n15.9 Persistent Virtual Network Configuration Architecture\n\n15.10 Redundant Physical Connection for Virtual Servers\n\nArchitecture\n\n15.11 Storage Maintenance Window Architecture\n\n15.12 Edge Computing Architecture\n\n15.13 Fog Computing Architecture",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Part IV Working with Clouds\n\nChapter 16 Cloud Delivery Model Considerations\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\n16.3 Case Study Example\n\nChapter 17 Cost Metrics and Pricing Models\n\n17.1 Business Cost Metrics\n\n17.2 Cloud Usage Cost Metrics\n\n17.3 Cost Management Considerations\n\nChapter 18 Service Quality Metrics and SLAs\n\n18.1 Service Quality Metrics\n\n18.2 Case Study Example\n\n18.3 SLA Guidelines\n\n18.4 Case Study Example\n\nPart V Appendices\n\nAppendix A Case Study Conclusions\n\nA.1 ATN\n\nA.2 DTGOV\n\nA.3 Innovartus\n\nAppendix B Common Containerization Technologies\n\nB.1 Docker\n\nB.2 Kubernetes",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Chapter 1\n\nIntroduction\n\n1.1 Objectives of This Book\n\n1.2 What This Book Does Not Cover\n\n1.3 Who This Book Is For\n\n1.4 How This Book Is Organized\n\n1.5 Resources\n\nCloud computing is, at its essence, a form of service provisioning. As with\n\nany type of service we intend to hire or outsource (IT-related or otherwise),\n\nit is commonly understood that we will be confronted with a marketplace\n\ncomprised of service providers of varying quality and reliability. Some may\n\noffer attractive rates and terms, but may have unproven business histories or\n\nhighly proprietary environments. Others may have a solid business\n\nbackground, but may demand higher rates and less flexible terms. Others\n\nyet, may simply be insincere or temporary business ventures that\n\nunexpectedly disappear or are acquired within a short period of time.\n\nThere is no greater danger to a business than approaching cloud computing\n\nadoption with ignorance. The magnitude of a failed adoption effort not only\n\ncorrespondingly impacts IT departments, but can actually regress a business",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "to a point where it finds itself steps behind from where it was prior to the\n\nadoption—and, perhaps, even more steps behind competitors that have been\n\nsuccessful at achieving their goals in the meantime.\n\nCloud computing has much to offer but its roadmap is riddled with pitfalls,\n\nambiguities, and mistruths. The best way to navigate this landscape is to\n\nchart each part of the journey by making educated decisions about how and\n\nto what extent your project should proceed. The scope of an adoption is\n\nequally important to its approach, and both of these aspects need to be\n\ndetermined by business requirements. Not by a product vendor, not by a\n\ncloud vendor, and not by self-proclaimed cloud experts. Your organization’s\n\nbusiness goals must be fulfilled in a concrete and measurable manner with\n\neach completed phase of the adoption. This validates your scope, your\n\napproach, and the overall direction of the project. In other words, it keeps\n\nyour project aligned.\n\nGaining a vendor-neutral understanding of cloud computing from an\n\nindustry perspective empowers you with the clarity necessary to determine\n\nwhat is factually cloud-related and what is not, as well as what is relevant to\n\nyour business requirements and what is not. With this information you can\n\nestablish criteria that will allow you to filter out the parts of the cloud\n\ncomputing product and service provider marketplaces to focus on what has\n\nthe most potential to help you and your business to succeed. We developed\n\nthis book to assist you with this goal.",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "—Thomas Erl\n\n1.1 Objectives of This Book\n\nThis book is the result of much research and analysis of the commercial\n\ncloud computing industry, cloud computing vendor platforms, and further\n\ninnovation and contributions made by cloud computing industry standards\n\norganizations and practitioners. The purpose of this book is to break down\n\nproven and mature cloud computing technologies and practices into a series\n\nof well-defined concepts, models, and technology mechanisms and\n\narchitectures. The resulting chapters establish concrete, academic coverage\n\nof fundamental aspects of cloud computing concepts and technologies. The\n\nrange of topics covered is documented using vendor-neutral terms and\n\ndescriptions, carefully defined to ensure full alignment with the cloud\n\ncomputing industry as a whole.\n\n1.2 What This Book Does Not Cover\n\nDue to the vendor-neutral basis of this book, it does not contain any\n\nsignificant coverage of cloud computing vendor products, services, or\n\ntechnologies. This book is complementary to other titles that provide\n\nproduct-specific coverage and to vendor product literature itself. If you are\n\nnew to the commercial cloud computing landscape, you are encouraged to\n\nuse this book as a starting point before proceeding to books and courses that\n\nare proprietary to vendor product lines.",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "1.3 Who This Book Is For\n\nThis book is aimed at the following target audience:\n\nIT practitioners and professionals who require vendor-neutral coverage of\n\ncloud computing technologies, concepts, mechanisms, and models\n\nIT managers and decision-makers who seek clarity regarding the business\n\nand technological implications of cloud computing\n\nprofessors and students and educational institutions that require well-\n\nresearched and well-defined academic coverage of fundamental cloud\n\ncomputing topics\n\nbusiness managers who need to assess the potential economic gains and\n\nviability of adopting cloud computing resources\n\ntechnology architects and developers who want to understand the different\n\nmoving parts that comprise contemporary cloud platforms\n\n1.4 How This Book Is Organized\n\nThe book begins with Chapters 1 and 2 providing introductory content and\n\nbackground information for the case studies. All subsequent chapters are\n\norganized into the following parts:\n\nPart I: Fundamental Cloud Computing",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Part II: Cloud Computing Mechanisms\n\nPart III: Cloud Computing Architecture\n\nPart IV: Working with Clouds\n\nPart V: Appendices\n\nPart I: Fundamental Cloud Computing\n\nThe five chapters in this part cover introductory topics in preparation for all\n\nsubsequent chapters. Note that Chapters 3 and 4 do not contain case study\n\ncontent.\n\nChapter 3: Understanding Cloud Computing\n\nFollowing a brief history of cloud computing and a discussion of business\n\ndrivers and technology innovations, basic terminology and concepts are\n\nintroduced, along with descriptions of common benefits and challenges of\n\ncloud computing adoption.\n\nChapter 4: Fundamental Concepts and Models\n\nCloud delivery and cloud deployment models are discussed in detail,\n\nfollowing sections that establish common cloud characteristics and roles\n\nand boundaries.",
      "content_length": 822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 5: Cloud-Enabling Technology\n\nContemporary technologies that realize modern-day cloud computing\n\nplatforms and innovations are discussed, including data centers,\n\nvirtualization, containerization, and Web-based technologies.\n\nChapter 6: Understanding Containerization\n\nA comparison of virtualization and containerization is provided, along with\n\nin-depth coverage of containerization environments and components.\n\nChapter 7: Understanding Cloud Security and Cybersecurity\n\nCloud security and cybersecurity topics and concepts relevant and distinct\n\nto cloud computing are introduced, including descriptions of common cloud\n\nsecurity threats and attacks.\n\nPart II: Cloud Computing Mechanisms\n\nTechnology mechanisms represent well-defined IT artifacts that are\n\nestablished within an IT industry and commonly distinct to a certain\n\ncomputing model or platform. The technology-centric nature of cloud\n\ncomputing requires the establishment of a formal level of mechanisms to be\n\nable to explore how solutions can be assembled via different combinations\n\nof mechanism implementations.",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "This part formally documents over 50 technology mechanisms that are used\n\nwithin cloud environments to enable generic and specialized forms of\n\nfunctionality. Each mechanism description is accompanied by a case study\n\nexample that demonstrates its usage. The utilization of select mechanisms is\n\nfurther explored throughout the technology architectures covered in Part III.\n\nChapter 8: Cloud Infrastructure Mechanisms\n\nTechnology mechanisms foundational to cloud platforms are covered,\n\nincluding Logical Network Perimeter, Virtual Server, Cloud Storage\n\nDevice, Cloud Usage Monitor, Resource Replication, Hypervisor, Ready-\n\nMade Environment and Container.\n\nChapter 9: Specialized Cloud Mechanisms\n\nA range of specialized technology mechanisms is described, including\n\nAutomated Scaling Listener, Load Balancer, SLA Monitor, Pay-Per-Use\n\nMonitor, Audit Monitor, Failover System, Resource Cluster, Multi-Device\n\nBroker, and State Management Database.\n\nChapter 10: Cloud and Cyber Security Access-Oriented Mechanisms\n\nAccess-related security mechanisms that can be used to counter and prevent\n\nsome of the threats described in Chapter 7 are covered, including\n\nEncryption, Hashing, Digital Signature, Cloud-Based Security Groups,\n\nPublic Key Infrastructure (PKI) System, Single Sign-On (SSO) System,\n\nHardened Virtual Server Image, Firewall, Virtual Private Network (VPN),",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Biometric Scanner, Multi-Factor Authentication (MFA) System, Identity\n\nand Access Management (IAM) System, Intrusion Detection System (IDS),\n\nPenetration Testing Tool, User Behavior Analytics (UBA) System, Third-\n\nParty Software Update Utility, Network Intrusion Monitor, Authentication\n\nLog Monitor, and VPN Monitor.\n\nChapter 11: Cloud and Cyber Security Data-Oriented Mechanisms\n\nData-related security mechanisms that can be used to counter and prevent\n\nsome of the threats described in Chapter 7 are covered, including Digital\n\nVirus Scanning and Decryption System, Digital Immune System, Malicious\n\nCode Analysis System, Data Loss Prevention (DLP) System, Trusted\n\nPlatform Module (TPM), Data Backup and Recovery System, Activity Log\n\nMonitor, Traffic Monitor, and Data Loss Protection Monitor.\n\nChapter 12: Cloud Management Mechanisms\n\nMechanisms that enable the hands-on administration and management of\n\ncloud-based IT resources are explained, including Remote Administration\n\nSystem, Resource Management System, SLA Management System, and\n\nBilling Management System.\n\nPart III: Cloud Computing Architecture\n\nTechnology architecture within the realm of cloud computing introduces\n\nrequirements and considerations that manifest themselves in broadly scoped\n\narchitectural layers and numerous distinct architectural models.",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "This set of chapters builds upon the coverage of cloud computing\n\nmechanisms from Part II by formally documenting over 30 cloud-based\n\ntechnology architectures and scenarios in which different combinations of\n\nthe mechanisms are documented in relation to fundamental, advanced, and\n\nspecialized cloud architectures.\n\nChapter 13: Fundamental Cloud Architectures\n\nFundamental cloud architectural models establish baseline functions and\n\ncapabilities. The architectures covered in this chapter are Workload\n\nDistribution, Resource Pooling, Dynamic Scalability, Elastic Resource\n\nCapacity, Service Load Balancing, Cloud Bursting, Elastic Disk\n\nProvisioning, Redundant Storage and Multi-Cloud.\n\nChapter 14: Advanced Cloud Architectures\n\nAdvanced cloud architectural models establish sophisticated and complex\n\nenvironments, several of which directly build upon fundamental models.\n\nThe architectures covered in this chapter are Hypervisor Clustering, Virtual\n\nServer Clustering, Load Balanced Virtual Server Instances, Non-Disruptive\n\nService Relocation, Zero Downtime, Cloud Balancing, Resilient Disaster\n\nRecovery, Distributed Data Sovereignty, Resource Reservation, Dynamic\n\nFailure Detection and Recovery, Rapid Provisioning, Storage Workload\n\nManagement, and Virtual Private Cloud.",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 15: Specialized Cloud Architectures\n\nSpecialized cloud architectural models address distinct functional areas.\n\nThe architectures covered in this chapter are Direct I/O Access, Direct LUN\n\nAccess, Dynamic Data Normalization, Elastic Network Capacity, Cross-\n\nStorage Device Vertical Tiering, Intra-Storage Device Vertical Data Tiering,\n\nLoad-Balanced Virtual Switches, Multipath Resource Access, Persistent\n\nVirtual Network Configuration, Redundant Physical Connection for Virtual\n\nServers, Storage Maintenance Window, Edge Computing, and Fog\n\nComputing.\n\nPart IV: Working with Clouds\n\nCloud computing technologies and environments can be adopted to varying\n\nextents. An organization can migrate select IT resources to a cloud, while\n\nkeeping all other IT resources on-premise—or it can form significant\n\ndependencies on a cloud platform by migrating larger amounts of IT\n\nresources or even using the cloud environment to create them.\n\nFor any organization, it is important to assess a potential adoption from a\n\npractical and business-centric perspective in order to pinpoint the most\n\ncommon factors that pertain to financial investments, business impact, and\n\nvarious legal considerations. This set of chapters explores these and other\n\ntopics related to the real-world considerations of working with cloud-based\n\nenvironments.",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Chapter 16: Cloud Delivery Model Considerations\n\nCloud environments need to be built and evolved by cloud providers in\n\nresponse to cloud consumer requirements. Cloud consumers can use clouds\n\nto create or migrate IT resources to, subsequent to their assuming\n\nadministrative responsibilities. This chapter provides a technical\n\nunderstanding of cloud delivery models from both the provider and\n\nconsumer perspectives, each of which offers revealing insights into the\n\ninner workings and architectural layers of cloud environments.\n\nChapter 17: Cost Metrics and Pricing Models\n\nCost metrics for network, server, storage, and software usage are described,\n\nalong with various formulas for calculating integration and ownership costs\n\nrelated to cloud environments. The chapter concludes with a discussion of\n\ncost management topics as they relate to common business terms used by\n\ncloud provider vendors.\n\nChapter 18: Service Quality Metrics and SLAs\n\nService level agreements establish the guarantees and usage terms for cloud\n\nservices and are often determined by the business terms agreed upon by\n\ncloud consumers and cloud providers. This chapter provides detailed insight\n\ninto how cloud provider guarantees are expressed and structured via SLAs,\n\nalong with metrics and formulas for calculating common SLA values, such\n\nas availability, reliability, performance, scalability, and resiliency.",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Part V: Appendices\n\nAppendix A: Case Study Conclusions\n\nThe individual storylines of the case studies are concluded and the results of\n\neach organization’s cloud computing adoption efforts are summarized.\n\nAppendix B: Common Containerization Technologies\n\nThis appendix acts as a supplement to Chapter 6 by providing a breakdown\n\nof the Docker and Kubernetes environments and relating those\n\nenvironments to the terms and components established in Chapter 6.\n\n1.5 Resources\n\nThese sections provide supplementary information and resources.\n\nPearson Digital Enterprise Book Series\n\nInformation about the books in the Pearson Digital Enterprise Series from\n\nThomas Erl and various supporting resources can be found at:\n\nwww.thomaserl.com/books\n\nThomas Erl on YouTube\n\nSubscribe to the Thomas Erl YouTube channel for animated videos with\n\nstorytelling and podcasts with industry experts. This YouTube channel is",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "dedicated to digital technology, digital business and digital transformation.\n\nSubscribe at:\n\nwww.youtube.com/@terl\n\nDigital Enterprise Newsletter on LinkedIn\n\nThe Digital Enterprise newsletter on LinkedIn publishes regular articles and\n\nvideos relevant to contemporary digital technology and business topics.\n\nSubscribe at: www.linkedin.com/newsletters/6909573501767028736\n\nCloud Certified Professional (CCP) Program\n\nArcitura Education offers vendor-neutral training and accreditation\n\nprograms with a portfolio of over 100 course modules and 40 certifications.\n\nThis text book is an official part of Arcitura’s Cloud Certified Professional\n\n(CCP) curriculum.\n\nLearn more at:\n\nwww.arcitura.com",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 2\n\nCase Study Background\n\n2.1 Case Study #1: ATN\n\n2.2 Case Study #2: DTGOV\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nCase study examples provide scenarios in which organizations assess, use,\n\nand manage cloud computing models and technologies. Three organizations\n\nfrom different industries are presented for analysis in this book, each of\n\nwhich has distinctive business, technological, and architectural objectives\n\nthat are introduced in this chapter.\n\nThe organizations presented for case study are:\n\nAdvanced Telecom Networks (ATN) – a global company that supplies\n\nnetwork equipment to the telecommunications industry\n\nDTGOV – a public organization that specializes in IT infrastructure and\n\ntechnology services for public sector organizations\n\nInnovartus Technologies Inc. – a medium-sized company that develops\n\nvirtual toys and educational entertainment products for children",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Most chapters after Part I include one or more Case Study Example\n\nsections. A conclusion to the storylines is provided in Appendix A.\n\n2.1 Case Study #1: ATN\n\nATN is a company that provides network equipment to telecommunications\n\nindustries across the globe. Over the years, ATN has grown considerably\n\nand their product portfolio has expanded to accommodate several\n\nacquisitions, including companies that specialize in infrastructure\n\ncomponents for Internet, GSM, and cellular providers. ATN is now a\n\nleading supplier of a diverse range of telecommunications infrastructure.\n\nIn recent years, market pressure has been increasing. ATN has begun\n\nlooking for ways to increase its competitiveness and efficiency by taking\n\nadvantage of new technologies, especially those that can assist in cost\n\nreduction.\n\nTechnical Infrastructure and Environment\n\nATN’s various acquisitions have resulted in a highly complex and\n\nheterogeneous IT landscape. A cohesive consolidation program was not\n\napplied to the IT environment after each acquisition round, resulting in\n\nsimilar applications running concurrently and an increase in maintenance\n\ncosts. Years ago, ATN merged with a major European telecommunications\n\nsupplier, adding another applications portfolio to its inventory. The IT",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "complexity snowballed into a serious obstruction and became a source of\n\ncritical concern to ATN’s board of directors.\n\nBusiness Goals and New Strategy\n\nATN management decided to pursue a consolidation initiative and\n\noutsource applications maintenance and operations overseas. This lowered\n\ncosts but unfortunately did not address their overall operational inefficiency.\n\nApplications still had overlapping functions that could not be easily\n\nconsolidated. It eventually became apparent that outsourcing was\n\ninsufficient as consolidation became a possibility only if the architecture of\n\nthe entire IT landscape changed.\n\nAs a result, ATN decided to explore the potential of adopting cloud\n\ncomputing. However, subsequent to their initial inquiries they became\n\noverwhelmed by the plenitude of cloud providers and cloud-based products.\n\nRoadmap and Implementation Strategy\n\nATN is unsure of how to choose the right set of cloud computing\n\ntechnologies and vendors—many solutions appear to still be immature and\n\nnew cloud-based offerings continue to emerge in the market.\n\nA preliminary cloud computing adoption roadmap is discussed to address a\n\nnumber of key points:",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "IT Strategy – The adoption of cloud computing needs to promote\n\noptimization of the current IT framework, and produce both lower short-\n\nterm investments and consistent long-term cost reduction.\n\nBusiness Benefits – ATN needs to evaluate which of the current applications\n\nand IT infrastructure can leverage cloud computing technology to achieve\n\nthe desired optimization and cost reductions. Additional cloud computing\n\nbenefits such as greater business agility, scalability, and reliability need to\n\nbe realized to promote business value.\n\nTechnology Considerations – Criteria need to be established to help choose\n\nthe most appropriate cloud delivery and deployment models and cloud\n\nvendors and products.\n\nCloud Security – The risks associated with migrating applications and data\n\nto the cloud must be determined.\n\nATN fears that they might lose control over their applications and data if\n\nentrusted to cloud providers, leading to incompliance with internal policies\n\nand telecom market regulations. They also wonder how their existing\n\nlegacy applications would be integrated into the new cloud-based domain.\n\nTo define a succinct plan of action, ATN hires an independent IT consulting\n\ncompany called CloudEnhance, who are well recognized for their\n\ntechnology architecture expertise in the transition and integration of cloud",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "computing IT resources. CloudEnhance consultants begin by suggesting an\n\nappraisal process comprised of five steps:\n\n. A brief evaluation of existing applications to measures factors, such as\n\ncomplexity, business-criticality, usage frequency, and number of active\n\nusers. The identified factors are then placed in a hierarchy of priority to help\n\ndetermine the most suitable candidate applications for migration to a cloud\n\nenvironment.\n\n. A more detailed evaluation of each selected application using a proprietary\n\nassessment tool.\n\n. The development of a target application architecture that exhibits the\n\ninteraction between cloud-based applications, their integration with ATN’s\n\nexisting infrastructure and legacy systems, and their development and\n\ndeployment processes.\n\n. The authoring of a preliminary business case that documents projected cost\n\nsavings based on performance indicators, such as cost of cloud readiness,\n\neffort for application transformation and interaction, ease of migration and\n\nimplementation, and various potential long-term benefits.\n\n. The development of a detailed project plan for a pilot application.\n\nATN proceeds with the process and resultantly builds its first prototype by\n\nfocusing on an application that automates a low-risk business area. During",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "this project ATN ports several of the business area’s smaller applications\n\nthat were running on different technologies over to a PaaS platform. Based\n\non positive results and feedback received for the prototype project, ATN\n\ndecides to embark on a strategic initiative to garner similar benefits for\n\nother areas of the company.\n\n2.2 Case Study #2: DTGOV\n\nDTGOV is a public company that was created in the early 1980s by the\n\nMinistry of Social Security. The decentralization of the ministry’s IT\n\noperations to a public company under private law gave DTGOV an\n\nautonomous management structure with significant flexibility to govern and\n\nevolve its IT enterprise.\n\nAt the time of its creation, DTGOV had approximately 1,000 employees,\n\noperational branches in 60 localities nation-wide, and operated two\n\nmainframe-based data centers. Over time, DTGOV has expanded to more\n\nthan 3,000 employees and branch offices in more than 300 localities, with\n\nthree data centers running both mainframe and low-level platform\n\nenvironments. Its main services are related to processing social security\n\nbenefits across the country.\n\nDTGOV has enlarged its customer portfolio in the last two decades. It now\n\nserves other public-sector organizations and provides basic IT infrastructure\n\nand services, such as server hosting and server colocation. Some of its",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "customers have also outsourced the operation, maintenance, and\n\ndevelopment of applications to DTGOV.\n\nDTGOV has sizable customer contracts that encompass various IT\n\nresources and services. However, these contracts, services, and associated\n\nservice levels are not -standardized—negotiated service provisioning\n\nconditions are typically customized for each customer individually.\n\nDTGOV’s operations are resultantly becoming increasingly complex and\n\ndifficult to manage, which has led to inefficiencies and inflated costs.\n\nThe DTGOV board realized, some time ago, that the overall company\n\nstructure could be improved by standardizing its services portfolio, which\n\nimplies the reengineering of both IT operational and management models.\n\nThis process has started with the standardization of the hardware platform\n\nthrough the creation of a clearly defined technological lifecycle, a\n\nconsolidated procurement policy, and the establishment of new acquisition\n\npractices.\n\nTechnical Infrastructure and Environment\n\nDTGOV operates three data centers: one is exclusively dedicated to low-\n\nlevel platform servers while the other two have both mainframe and low-\n\nlevel platforms. The mainframe systems are reserved for the Ministry of\n\nSocial Security and therefore not available for outsourcing.",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "The data center infrastructure occupies approximately 20,000 square feet of\n\ncomputer room space and hosts more than 100,000 servers with different\n\nhardware configurations. The total storage capacity is approximately 10,000\n\nterabytes. DTGOV’s network has redundant high-speed data links\n\nconnecting the data centers in a full mesh topology. Their Internet\n\nconnectivity is considered to be provider-independent since their network\n\ninterconnects all of the major national telecom carriers.\n\nServer consolidation and virtualization projects have been in place for five\n\nyears, considerably decreasing the diversity of hardware platforms. As a\n\nresult, systematic tracking of the investments and operational costs related\n\nto the hardware platform has revealed significant improvement. However,\n\nthere is still remarkable diversity in their software platforms and\n\nconfigurations due to customer service customization requirements.\n\nBusiness Goals and New Strategy\n\nA chief strategic objective of the standardization of DTGOV’s service\n\nportfolio is to achieve increased levels of cost effectiveness and operational\n\noptimization. An internal executive-level commission was established to\n\ndefine the directions, goals, and strategic roadmap for this initiative. The\n\ncommission has identified cloud computing as a guidance option and an\n\nopportunity for further diversification and improvement of services and\n\ncustomer portfolios.\n\nThe roadmap addresses the following key points:",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Business Benefits – Concrete business benefits associated with the\n\nstandardization of service portfolios under the umbrella of cloud computing\n\ndelivery models need to be defined. For example, how can the optimization\n\nof IT infrastructure and operational models result in direct and measurable\n\ncost reductions?\n\nService Portfolio – Which services should become cloud-based, and which\n\ncustomers should they be extended to?\n\nTechnical Challenges – The limitations of the current technology\n\ninfrastructure in relation to the runtime processing requirements of cloud\n\ncomputing models must be understood and documented. Existing\n\ninfrastructure must be leveraged to whatever extent possible to optimize up-\n\nfront costs assumed by the development of the cloud-based service\n\nofferings.\n\nPricing and SLAs – An appropriate contract, pricing, and service quality\n\nstrategy needs to be defined. Suitable pricing and service-level agreements\n\n(SLAs) must be determined to support the initiative.\n\nOne outstanding concern relates to changes to the current format of\n\ncontracts and how they may impact business. Many customers may not\n\nwant to—or may not be prepared to—adopt cloud contracting and service\n\ndelivery models. This becomes even more critical when considering the fact\n\nthat 90% of DTGOV’s current customer portfolio is comprised of public",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "organizations that typically do not have the autonomy or the agility to\n\nswitch operating methods on such short notice. Therefore, the migration\n\nprocess is expected to be long term, which may become risky if the\n\nroadmap is not properly and clearly defined. A further outstanding issue\n\npertains to IT contract regulations in the public sector—-existing\n\nregulations may become irrelevant or unclear when applied to cloud\n\ntechnologies.\n\nRoadmap and Implementation Strategy\n\nSeveral assessment activities were initiated to address the aforementioned\n\nissues. The first was a survey of existing customers to probe their level of\n\nunderstanding, on-going initiatives, and plans regarding cloud computing.\n\nMost of the respondents were aware of and knowledgeable about cloud\n\ncomputing trends, which was considered a positive finding.\n\nAn investigation of the service portfolio revealed clearly identified\n\ninfrastructure services relating to hosting and colocation. Technical\n\nexpertise and infrastructure were also evaluated, determining that data\n\ncenter operation and management are key areas of expertise of DTGOV IT\n\nstaff.\n\nWith these findings, the commission decided to:\n\n. choose IaaS as the target delivery platform to start the cloud computing\n\nprovisioning initiative",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": ". hire a consulting firm with sufficient cloud provider expertise and\n\nexperience to correctly identify and rectify any business and technical\n\nissues that may afflict the initiative\n\n. deploy new hardware resources with a uniform platform into two different\n\ndata centers, aiming to establish a new, reliable environment to use for the\n\nprovisioning of initial IaaS-hosted services\n\n. identify three customers that plan to acquire cloud-based services in order\n\nto establish pilot projects and define contractual conditions, pricing, and\n\nservice-level policies and models\n\n. evaluate service provisioning of the three chosen customers for the initial\n\nperiod of six months before publicly offering the service to other customers\n\nAs the pilot project proceeds, a new Web-based management environment\n\nis released to allow for the self-provisioning of virtual servers, as well as\n\nSLA and financial tracking functionality in realtime. The pilot projects are\n\nconsidered highly successful, leading to the next step of opening the cloud-\n\nbased services to other customers.\n\n2.3 Case Study #3: Innovartus Technologies Inc.\n\nThe primary business line of Innovartus Technologies Inc. is the\n\ndevelopment of virtual toys and educational entertainment products for\n\nchildren. These services are provided through a Web portal that employs a",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "role-playing model to create customized virtual games for PCs and mobile\n\ndevices. The games allow users to create and manipulate virtual toys (cars,\n\ndolls, pets) that can be outfitted with virtual accessories that are obtained by\n\ncompleting simple educational quests. The main demographic is children\n\nunder 12 years. Innovartus further has a social network environment that\n\nenables users to exchange items and collaborate with others. All of these\n\nactivities can be monitored and tracked by the parents, who can also\n\nparticipate in a game by creating specific quests for their children.\n\nThe most valuable and revolutionary feature of Innovartus’ applications is\n\nan experimental end-user interface that is based on natural interface\n\nconcepts. Users can interact via voice commands, simple gestures that are\n\ncaptured with a Webcam, and directly by touching tablet screens.\n\nThe Innovartus portal has always been cloud-based. It was originally\n\ndeveloped via a PaaS platform and has been hosted by the same cloud\n\nprovider ever since. However, recently this environment has revealed\n\nseveral technical limitations that impact features of Innovartus’ user\n\ninterface programming frameworks.\n\nTechnical Infrastructure and Environment\n\nMany of Innovartus’ other office automation solutions, such as shared file\n\nrepositories and various productivity tools, are also cloud-based. The on-\n\npremise corporate IT environment is relatively small, comprised mainly of\n\nwork area devices, laptops, and graphic design workstations.",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Business Goals and Strategy\n\nInnovartus has been diversifying the functionality of the IT resources that\n\nare used for their Web-based and mobile applications. The company has\n\nalso increased efforts to internationalize their applications; both the Web\n\nsite and the mobile applications are currently offered in five different\n\nlanguages.\n\nRoadmap and Implementation Strategy\n\nInnovartus intends to continue building upon its cloud-based solutions;\n\nhowever, the current cloud hosting environment has limitations that need to\n\nbe overcome:\n\nscalability needs to be improved to accommodate increased and less\n\npredictable cloud consumer interaction\n\nservice levels need to be improved to avoid outages that are currently more\n\nfrequent than expected\n\ncost effectiveness needs to be improved, as leasing rates are higher with the\n\ncurrent cloud provider when compared to others\n\nThese and other factors have led Innovartus to decide to migrate to a larger,\n\nmore globally established cloud provider.\n\nThe roadmap for this migration project includes:",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "a technical and economic report about the risks and impacts of the planned\n\nmigration\n\na decision tree and a rigorous study initiative focused on the criteria for\n\nselecting the new cloud provider\n\nportability assessments of applications to determine how much of each\n\nexisting cloud service architecture is proprietary to the current cloud\n\nprovider’s environment\n\nInnovartus is further concerned about how and to what extent the current\n\ncloud provider will support and cooperate with the migration process.",
      "content_length": 509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Part I\n\nFundamental Cloud Computing\n\nChapter 3: Understanding Cloud Computing\n\nChapter 4: Fundamental Concepts and Models\n\nChapter 5: Cloud-Enabling Technology\n\nChapter 6: Understanding Containerization\n\nChapter 7: Understanding Cloud Security and Cybersecurity\n\nThe upcoming chapters establish concepts and terminology that are\n\nreferenced throughout subsequent chapters and parts in this book. It is\n\nrecommended that Chapters 3 and 4 be reviewed, even for those already\n\nfamiliar with cloud computing fundamentals. Sections in Chapters 5, 6 and\n\n7 can be selectively skipped by those already familiar with the\n\ncorresponding technology and security topics.",
      "content_length": 659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Chapter 3\n\nUnderstanding Cloud Computing\n\n3.1 Origins and Influences\n\n3.2 Basic Concepts and Terminology\n\n3.3 Goals and Benefits\n\n3.4 Risks and Challenges\n\nThis is the first of two chapters that provide an overview of introductory\n\ncloud computing topics. It begins with a brief history of cloud computing\n\nalong with short descriptions of its business and technology drivers. This is\n\nfollowed by definitions of basic concepts and terminology, in addition to\n\nexplanations of the primary benefits and challenges of cloud computing\n\nadoption.\n\n3.1 Origins and Influences\n\nA Brief History\n\nThe idea of computing in a “cloud” traces back to the origins of utility\n\ncomputing, a concept that computer scientist John McCarthy publicly\n\nproposed in 1961:",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "“If computers of the kind I have advocated become the computers of the\n\nfuture, then computing may someday be organized as a public utility just as\n\nthe telephone system is a public utility. … The computer utility could\n\nbecome the basis of a new and important industry.”\n\nIn 1969, Leonard Kleinrock, a chief scientist of the Advanced Research\n\nProjects Agency Network or ARPANET project that seeded the Internet,\n\nstated:\n\n“As of now, computer networks are still in their infancy, but as they grow up\n\nand become sophisticated, we will probably see the spread of ‘computer\n\nutilities’ …”.\n\nThe general public has been leveraging forms of Internet-based computer\n\nutilities since the mid-1990s through various incarnations of search engines,\n\nemail services, open publishing platforms, and other types of social media.\n\nThough consumer-centric, these services popularized and validated core\n\nconcepts that form the basis of modern-day cloud computing.\n\nIn 1999, Salesforce.com pioneered the notion of bringing remotely\n\nprovisioned services into the enterprise. In 2006, Amazon.com launched the\n\nAmazon Web Services (AWS) platform, a suite of enterprise-oriented\n\nservices that provide remotely provisioned storage, computing resources,\n\nand business functionality.",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "A slightly different evocation of the term “Network Cloud” or “Cloud” was\n\nintroduced in the early 1990s throughout the networking industry. It\n\nreferred to an abstraction layer derived in the delivery methods of data\n\nacross heterogeneous public and semi-public networks that were primarily\n\npacket-switched, although cellular networks used the “Cloud” term as well.\n\nThe networking method at this point supported the transmission of data\n\nfrom one end-point (local network) to the “Cloud” (wide area network) and\n\nthen further decomposed to another intended end-point. This is relevant, as\n\nthe networking industry still references the use of this term, and is\n\nconsidered an early adopter of the concepts that underlie utility computing.\n\nIt wasn’t until 2006 that the term “cloud computing” emerged in the\n\ncommercial arena. It was during this time that Amazon launched its Elastic\n\nCompute Cloud (EC2) services that enabled organizations to “lease”\n\ncomputing capacity and processing power to run their enterprise\n\napplications. Google Apps also began providing browser-based enterprise\n\napplications in the same year, and three years later, the Google App Engine\n\nbecame another historic milestone.\n\nDefinitions\n\nA Gartner report listing cloud computing at the top of its strategic\n\ntechnology areas further reaffirmed its prominence as an industry trend by\n\nannouncing its formal definition as:",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "“…a style of computing in which scalable and elastic IT-enabled\n\ncapabilities are delivered as a service to external customers using Internet\n\ntechnologies.”\n\nThis is a slight revision of Gartner’s original definition from 2008, in which\n\n“massively scalable” was used instead of “scalable and elastic.” This\n\nacknowledges the importance of scalability in relation to the ability to scale\n\nvertically and not just to enormous proportions.\n\nForrester Research provided its own definition of cloud computing as:\n\n“…a standardized IT capability (services, software, or infrastructure)\n\ndelivered via Internet technologies in a pay-per-use, self-service way.”\n\nThe definition that received industry-wide acceptance was composed by the\n\nNational Institute of Standards and Technology (NIST). NIST published its\n\noriginal definition back in 2009, followed by a revised version after further\n\nreview and industry input that was published in September of 2011:\n\n“Cloud computing is a model for enabling ubiquitous, convenient, on-\n\ndemand network access to a shared pool of configurable computing\n\nresources (e.g., networks, servers, storage, applications, and services) that\n\ncan be rapidly provisioned and released with minimal management effort or\n\nservice provider interaction. This cloud model is composed of five essential\n\ncharacteristics, three service models, and four deployment models.”",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "This book provides a more concise definition:\n\n“Cloud computing is a specialized form of distributed computing that\n\nintroduces utilization models for remotely provisioning scalable and\n\nmeasured resources.”\n\nThis simplified definition is in line with all of the preceding definition\n\nvariations that were put forth by other organizations within the cloud\n\ncomputing industry. The characteristics, service models, and deployment\n\nmodels referenced in the NIST definition are further covered in Chapter 4.\n\nBusiness Drivers\n\nBefore delving into the layers of technologies that underlie clouds, the\n\nmotivations that led to their creation by industry leaders must first be\n\nunderstood. Several of the primary business drivers that fostered modern\n\ncloud-based technology are presented in this section.\n\nThe origins and inspirations of many of the characteristics, models, and\n\nmechanisms covered throughout subsequent chapters can be traced back to\n\nthe upcoming business drivers. It is important to note that these influences\n\nshaped clouds and the overall cloud computing market from both ends.\n\nThey have motivated organizations to adopt cloud computing in support of\n\ntheir business automation requirements. They have correspondingly\n\nmotivated other organizations to become providers of cloud environments",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "and cloud technology vendors in order to create and meet the demand to\n\nfulfill consumer needs.\n\nCost Reduction\n\nA direct alignment between IT costs and business performance can be\n\ndifficult to maintain. The growth of IT environments often corresponds to\n\nthe assessment of their maximum usage requirements. This can make the\n\nsupport of new and expanded business automations an ever-increasing\n\ninvestment. Much of this required investment is funneled into infrastructure\n\nexpansion because the usage potential of a given automation solution will\n\nalways be limited by the processing power of its underlying infrastructure.\n\nTwo costs need to be accounted for: the cost of acquiring new\n\ninfrastructure, and the cost of its on-going ownership. Operational overhead\n\nrepresents a considerable share of IT budgets, often exceeding up-front\n\ninvestment costs.\n\nCommon forms of infrastructure-related operating overhead include the\n\nfollowing:\n\ntechnical personnel required to keep the environment operational\n\nupgrades and patches that introduce additional testing and deployment\n\ncycles\n\nutility bills and capital expense investments for power and cooling",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "security and access control measures that need to be maintained and\n\nenforced to protect infrastructure resources\n\nadministrative and accounts staff that may be required to keep track of\n\nlicenses and support arrangements\n\nThe on-going ownership of internal technology infrastructure can\n\nencompass burdensome responsibilities that impose compound impacts on\n\ncorporate budgets. An IT department can consequently become a significant\n\n—and at times overwhelming—drain on the business, potentially inhibiting\n\nits responsiveness, profitability, and overall evolution.\n\nBusiness Agility\n\nBusinesses need the ability to adapt and evolve to successfully face change\n\ncaused by both internal and external factors. Business agility (or\n\norganizational agility) is the measure of an organization’s responsiveness to\n\nchange.\n\nAn IT enterprise often needs to respond to business change by scaling its IT\n\nresources beyond the scope of what was previously predicted or planned\n\nfor. For example, infrastructure may be subject to limitations that prevent\n\nthe organization from responding to usage fluctuations—even when\n\nanticipated—if previous capacity planning efforts were restricted by\n\ninadequate budgets.",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "In other cases, changing business needs and priorities may require IT\n\nresources to be more available and reliable than before. Even if sufficient\n\ninfrastructure is in place for an organization to support anticipated usage\n\nvolumes, the nature of the usage may generate runtime exceptions that\n\nbring down hosting servers. Due to a lack of reliability controls within the\n\ninfrastructure, responsiveness to consumer or customer requirements may\n\nbe reduced to a point whereby a business’ overall continuity is threatened.\n\nOn a broader scale, the up-front investments and infrastructure ownership\n\ncosts that are required to enable new or expanded business automation\n\nsolutions may themselves be prohibitive enough for a business to settle for\n\nIT infrastructure of less-than-ideal quality, thereby decreasing its ability to\n\nmeet real-world requirements.\n\nWorse yet, the business may decide against proceeding with an automation\n\nsolution altogether upon review of its infrastructure budget, because it\n\nsimply cannot afford to. This form of inability to respond can inhibit an\n\norganization from keeping up with market demands, competitive pressures,\n\nand its own strategic business goals.\n\nTechnology Innovations\n\nEstablished technologies are often used as inspiration and, at times, the\n\nactual foundations upon which new technology innovations are derived and\n\nbuilt. This section briefly describes the pre-existing technologies considered\n\nto be the primary influences on cloud computing.",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Clustering\n\nA cluster is a group of independent IT resources that are interconnected and\n\nwork as a single system. System failure rates are reduced while availability\n\nand reliability are increased, since redundancy and failover features are\n\ninherent to the cluster.\n\nA general prerequisite of hardware clustering is that its component systems\n\nhave reasonably identical hardware and operating systems to provide\n\nsimilar performance levels when one failed component is to be replaced by\n\nanother. Component devices that form a cluster are kept in synchronization\n\nthrough dedicated, high-speed communication links.\n\nThe basic concept of built-in redundancy and failover is core to cloud\n\nplatforms. Clustering technology is explored further in Chapter 9 as part of\n\nthe Resource Cluster mechanism description.\n\nGrid Computing\n\nA computing grid (or “computational grid”) provides a platform in which\n\ncomputing resources are organized into one or more logical pools. These\n\npools are collectively coordinated to provide a high performance distributed\n\ngrid, sometimes referred to as a “super virtual computer.” Grid computing\n\ndiffers from clustering in that grid systems are much more loosely coupled\n\nand distributed. As a result, grid computing systems can involve computing",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "resources that are heterogeneous and geographically dispersed, which is\n\ngenerally not possible with cluster computing-based systems.\n\nGrid computing has been an on-going research area in computing science\n\nsince the early 1990s. The technological advancements achieved by grid\n\ncomputing projects have influenced various aspects of cloud computing\n\nplatforms and mechanisms, specifically in relation to common feature-sets\n\nsuch as networked access, resource pooling, and scalability and resiliency.\n\nThese types of features can be established by both grid computing and\n\ncloud computing, in their own distinctive approaches.\n\nFor example, grid computing is based on a middleware layer that is\n\ndeployed on computing resources. These IT resources participate in a grid\n\npool that implements a series of workload distribution and coordination\n\nfunctions. This middle tier can contain load balancing logic, failover\n\ncontrols, and autonomic configuration management, each having previously\n\ninspired similar—and several more sophisticated—cloud computing\n\ntechnologies. It is for this reason that some classify cloud computing as a\n\ndescendant of earlier grid computing initiatives.\n\nCapacity Planning\n\nCapacity planning is the process of determining and fulfilling future\n\ndemands of an organization’s IT resources, products, and services. Within\n\nthis context, capacity represents the maximum amount of work that an IT\n\nresource is capable of delivering in a given period of time. A discrepancy",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "between the capacity of an IT resource and its demand can result in a\n\nsystem becoming either inefficient (over-provisioning) or unable to fulfill\n\nuser needs (under-provisioning). Capacity planning is focused on\n\nminimizing this discrepancy to achieve predictable efficiency and\n\nperformance.\n\nDifferent capacity planning strategies exist:\n\nLead Strategy – adding capacity to an IT resource in anticipation of demand\n\nLag Strategy – adding capacity when the IT resource reaches its full\n\ncapacity\n\nMatch Strategy – adding IT resource capacity in small increments, as\n\ndemand increases\n\nPlanning for capacity can be challenging because it requires estimating\n\nusage load fluctuations. There is a constant need to balance peak usage\n\nrequirements without unnecessary over-expenditure on infrastructure. An\n\nexample is outfitting IT infrastructure to accommodate maximum usage\n\nloads which can impose unreasonable financial investments. In such cases,\n\nmoderating investments can result in under-provisioning, leading to\n\ntransaction losses and other usage limitations from lowered usage\n\nthresholds.",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Virtualization\n\nVirtualization is the process of converting a physical IT resource into a\n\nvirtual IT resource.\n\nMost types of IT resources can be virtualized, including:\n\nServers – A physical server can be abstracted into a virtual server.\n\nStorage – A physical storage device can be abstracted into a virtual storage\n\ndevice or a virtual disk.\n\nNetwork – Physical routers and switches can be abstracted into logical\n\nnetwork fabrics, such as VLANs.\n\nPower – A physical UPS and power distribution units can be abstracted into\n\nwhat are commonly referred to as virtual UPSs.\n\nNote\n\nThe terms virtual server and virtual machine (VM) are used\n\nsynonymously throughout this book.\n\nA layer of virtualization software allows physical IT resources to provide\n\nmultiple virtual images of themselves so that their underlying processing\n\ncapabilities can be shared by multiple users.",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "The first step in creating a new virtual server through virtualization\n\nsoftware is the allocation of physical IT resources, followed by the\n\ninstallation of an operating system. Virtual servers use their own guest\n\noperating systems, which are independent of the operating system in which\n\nthey were created.\n\nBoth the guest operating system and the application software running on the\n\nvirtual server are unaware of the virtualization process, meaning these\n\nvirtualized IT resources are installed and executed as if they were running\n\non a separate physical server. This uniformity of execution that allows\n\nprograms to run on physical systems as they would on virtual systems is a\n\nvital characteristic of virtualization. Guest operating systems typically\n\nrequire seamless usage of software products and applications that do not\n\nneed to be customized, configured, or patched in order to run in a\n\nvirtualized environment.\n\nVirtualization software runs on a physical server called a host or physical\n\nhost, whose underlying hardware is made accessible by the virtualization\n\nsoftware. The virtualization software functionality encompasses system\n\nservices that are specifically related to virtual machine management and not\n\nnormally found on standard operating systems. This is why this software is\n\nsometimes referred to as a virtual machine manager or a virtual machine\n\nmonitor (VMM), but most commonly known as a hypervisor. (The\n\nhypervisor is formally described as a cloud computing mechanism in\n\nChapter 8.)",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Prior to the advent of virtualization technologies, software was limited to\n\nresiding on and being coupled with static hardware environments. The\n\nvirtualization process severs this software-hardware dependency, as\n\nhardware requirements can be simulated by emulation software running in\n\nvirtualized environments.\n\nEstablished virtualization technologies can be traced to several cloud\n\ncharacteristics and cloud computing mechanisms, having inspired many of\n\ntheir core features. As cloud computing evolved, a generation of modern\n\nvirtualization technologies emerged to overcome the performance,\n\nreliability, and scalability limitations of traditional virtualization platforms.\n\nModern virtualization technologies are discussed in Chapter 5.\n\nContainerization\n\nContainerization is a form of virtualization technology that allows for the\n\ncreation of virtual hosting environments referred to as “containers” without\n\nthe need to deploy a virtual server for each solution. A container is similar\n\nin concept to a virtual server in that it provides a virtual environment with\n\noperating system resources that can be used to host software programs and\n\nother IT resources.\n\nContainers are briefly introduced in the upcoming Basic Concepts and\n\nTerminology section and containerization technology is covered in detail in\n\nChapter 6.",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Serverless Environments\n\nA serverless environment is a special operational runtime environment that\n\ndoes not require developers or system administrators to deploy or provision\n\nservers. Instead, it is equipped with technology that allows for the\n\ndeployment of special software packages that already include the required\n\nserver components and configuration information.\n\nUpon deployment, the serverless environment automatically implements\n\nand activates an application deployment together with its packaged server,\n\nwithout the administrator having to do anything further. Programs are\n\ndesigned, coded and deployed alongside the descriptor of the underlying\n\nrequired runtime and any dependencies that may exist. Once deployed, the\n\nserverless environment can run and scale the application and ensure its on-\n\ngoing availability and scalability.\n\nContemporary software architectures deployed in clouds can benefit greatly\n\nfrom serverless environments. More details regarding serverless technology\n\nare provided in Chapter 5.\n\n3.2 Basic Concepts and Terminology\n\nThis section establishes a set of basic terms that represent the fundamental\n\nconcepts and aspects pertaining to the notion of a cloud and its most\n\nprimitive artifacts.",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Cloud\n\nA cloud refers to a distinct IT environment that is designed for the purpose\n\nof remotely provisioning scalable and measured IT resources. The term\n\noriginated as a metaphor for the Internet which is, in essence, a network of\n\nnetworks providing remote access to a set of decentralized IT resources.\n\nPrior to cloud computing becoming its own formalized IT industry segment,\n\nthe symbol of a cloud was commonly used to represent the Internet in a\n\nvariety of specifications and mainstream documentation of Web-based\n\narchitectures. This same symbol is now used to specifically represent the\n\nboundary of a cloud environment, as shown in Figure 3.1.",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Figure 3.1\n\nThe symbol used to denote the boundary of a cloud environment.\n\nIt is important to distinguish the term “cloud” and the cloud symbol from\n\nthe Internet. As a specific environment used to remotely provision IT\n\nresources, a cloud has a finite boundary. There are many individual clouds\n\nthat are accessible via the Internet. Whereas the Internet provides open\n\naccess to many Web-based IT resources, a cloud is typically privately\n\nowned and offers access to IT resources that is metered.",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Much of the Internet is dedicated to the access of content-based IT\n\nresources published via the World Wide Web. IT resources provided by\n\ncloud environments, on the other hand, are dedicated to supplying back-end\n\nprocessing capabilities and user-based access to these capabilities. Another\n\nkey distinction is that it is not necessary for clouds to be Web-based even if\n\nthey are commonly based on Internet protocols and technologies. Protocols\n\nrefer to standards and methods that allow computers to communicate with\n\neach other in a pre-defined and structured manner. A cloud can be based on\n\nthe use of any protocols that allow for the remote access to its IT resources.\n\nNote\n\nDiagrams in this book depict the Internet using the globe\n\nsymbol.\n\nContainer\n\nContainers (Figure 3.2) are commonly used in clouds to provide highly\n\noptimized virtual hosting environments capable of providing only the\n\nresources required for the software programs they host.",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Figure 3.2\n\nThe figure on the left is the general symbol used to represent a container.\n\nThe figure on the right (with rounded edges) is used in architectural\n\ndiagrams to represent a container, especially when the contents of the\n\ncontainer need to be shown.\n\nIT Resource\n\nAn IT resource is a physical or virtual IT-related artifact that can be either\n\nsoftware-based, such as a virtual server or a custom software program, or\n\nhardware-based, such as a physical server or a network device (Figure 3.3).",
      "content_length": 504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Figure 3.3\n\nExamples of common IT resources and their corresponding symbols.\n\nFigure 3.4 illustrates how the cloud symbol can be used to define a\n\nboundary for a cloud-based environment that hosts and provisions a set of\n\nIT resources. The displayed IT resources are consequently considered to be\n\ncloud-based IT resources.",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Figure 3.4\n\nA cloud is hosting eight IT resources: three virtual servers, two cloud\n\nservices, and three storage devices.\n\nTechnology architectures and various interaction scenarios involving IT\n\nresources are illustrated in diagrams like the one shown in Figure 3.4. It is",
      "content_length": 273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "important to note the following points when studying and working with\n\nthese diagrams:\n\nThe IT resources shown within the boundary of a given cloud symbol\n\nusually do not represent all of the available IT resources hosted by that\n\ncloud. Subsets of IT resources are generally highlighted to demonstrate a\n\nparticular topic.\n\nFocusing on the relevant aspects of a topic requires many of these diagrams\n\nto intentionally provide abstracted views of the underlying technology\n\narchitectures. This means that only a portion of the actual technical details\n\nare shown.\n\nFurthermore, some diagrams will display IT resources outside of the cloud\n\nsymbol. This convention is used to indicate IT resources that are not cloud-\n\nbased.\n\nNote\n\nThe virtual server IT resource displayed in Figure 3.3 is\n\nfurther discussed in Chapters 5 and 8. Physical servers are\n\nsometimes referred to as physical hosts (or just hosts) in\n\nreference to the fact that they are responsible for hosting\n\nvirtual servers.",
      "content_length": 989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "On-Premise\n\nAs a distinct and remotely accessible environment, a cloud represents an\n\noption for the deployment of IT resources. An IT resource that is hosted in a\n\nconventional IT enterprise within an organizational boundary (that does not\n\nspecifically represent a cloud) is considered to be located on the premises of\n\nthe IT enterprise. In this book the term on-premise is used as another way of\n\nstating “on the premises of a controlled IT environment that is not cloud-\n\nbased.” This term is used to help distinguish an IT resource residing within\n\nthe premises of an IT enterprise from one that is “cloud-based.” In other\n\nwords, an IT resource that is on-premise cannot be cloud-based, and vice-\n\nversa.\n\nNote the following key points:\n\nAn on-premise IT resource can access and interact with a cloud-based IT\n\nresource.\n\nAn on-premise IT resource can be moved to a cloud, thereby changing it to\n\na cloud-based IT resource.\n\nRedundant deployments of an IT resource can exist in both on-premise and\n\ncloud-based environments.\n\nIf the distinction between on-premise and cloud-based IT resources is\n\nconfusing in relation to private clouds (described in the Cloud Deployment",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Models section of Chapter 4), then an alternative qualifier can be used.\n\nCloud Consumers and Cloud Providers\n\nThe party that provides cloud-based IT resources is the cloud provider. The\n\nparty that uses cloud-based IT resources is the cloud consumer. These terms\n\nrepresent roles usually assumed by organizations in relation to clouds and\n\ncorresponding cloud provisioning contracts. These roles are formally\n\ndefined in Chapter 4, as part of the Roles and Boundaries section.\n\nScaling\n\nScaling, from an IT resource perspective, represents the ability of the IT\n\nresource to handle increased or decreased usage demands.\n\nThe following are types of scaling:\n\nHorizontal Scaling – scaling out and scaling in\n\nVertical Scaling – scaling up and scaling down\n\nThe next two sections briefly describe each.\n\nHorizontal Scaling\n\nThe allocating or releasing of IT resources that are of the same type is\n\nreferred to as horizontal scaling (Figure 3.5). The horizontal allocation of\n\nresources is referred to as scaling out and the horizontal releasing of",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "resources is referred to as scaling in. Horizontal scaling is a common form\n\nof scaling within cloud environments.\n\nFigure 3.5\n\nAn IT resource (Virtual Server A) is scaled out by adding more of the same\n\nIT resources (Virtual Servers B and C).\n\nVertical Scaling\n\nWhen an existing IT resource is replaced by another with higher or lower\n\ncapacity, vertical scaling is considered to have occurred (Figure 3.6).\n\nSpecifically, the replacing of an IT resource with another that has a higher",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "capacity is referred to as scaling up and the replacing of an IT resource with\n\nanother that has a lower capacity is considered scaling down. Vertical\n\nscaling is less common in cloud environments due to the downtime required\n\nwhile the replacement is taking place.\n\nFigure 3.6",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "An IT resource (a virtual server with two CPUs) is scaled up by replacing it\n\nwith a more powerful IT resource with increased capacity for data storage\n\n(a physical server with four CPUs).\n\nTable 3.1 provides a brief overview of common pros and cons associated\n\nwith horizontal and vertical scaling.\n\nTable 3-1\n\nA comparison of horizontal and vertical scaling.",
      "content_length": 360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Cloud Service\n\nAlthough a cloud is a remotely accessible environment, not all IT resources\n\nresiding within a cloud can be made available for remote access. For\n\nexample, a database or a physical server deployed within a cloud may only",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "be accessible by other IT resources that are within the same cloud. A\n\nsoftware program with a published API may be deployed specifically to\n\nenable access by remote clients.\n\nA cloud service is any IT resource that is made remotely accessible via a\n\ncloud. Unlike other IT fields that fall under the service technology umbrella\n\n—such as service-oriented architecture—the term “service” within the\n\ncontext of cloud computing is especially broad. A cloud service can exist as\n\na simple Web-based software program with a technical interface invoked\n\nvia the use of a messaging protocol, or as a remote access point for\n\nadministrative tools or larger environments and other IT resources.\n\nIn Figure 3.7, the yellow circle symbol is used to represent the cloud service\n\nas a simple Web-based software program. A different IT resource symbol\n\nmay be used in the latter case, depending on the nature of the access that is\n\nprovided by the cloud service.",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Figure 3.7\n\nA cloud service with a published technical interface is being accessed by a\n\nconsumer outside of the cloud (left). A cloud service that exists as a virtual\n\nserver is also being accessed from outside of the cloud’s boundary (right).\n\nThe cloud service on the left is likely being invoked by a consumer program\n\nthat was designed to access the cloud service’s published technical\n\ninterface. The cloud service on the right may be accessed by a human user\n\nthat has remotely logged on to the virtual server.\n\nThe driving motivation behind cloud computing is to provide IT resources\n\nas services that encapsulate other IT resources, while offering functions for\n\nclients to use and leverage remotely. A multitude of models for generic",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "types of cloud services have emerged, most of which are labeled with the\n\n“as-a-service” suffix.\n\nNote\n\nCloud service usage conditions are typically expressed in a\n\nservice-level agreement (SLA) that is the human-readable\n\npart of a service contract between a cloud provider and\n\ncloud consumer that describes QoS (Quality of Service)\n\nfeatures, behaviors, and limitations of a cloud-based service\n\nor other provisions.\n\nAn SLA provides details of various measurable\n\ncharacteristics related to IT outcomes, such as uptime,\n\nsecurity characteristics, and other specific QoS features,\n\nincluding availability, reliability, and performance. Since the\n\nimplementation of a service is hidden from the cloud\n\nconsumer, an SLA becomes a critical specification. SLAs\n\nare covered in detail in Chapter 18.\n\nCloud Service Consumer\n\nThe cloud service consumer is a temporary runtime role assumed by a\n\nsoftware program when it accesses a cloud service.",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "As shown in Figure 3.8, common types of cloud service consumers can\n\ninclude software programs and services capable of remotely accessing\n\ncloud services with published service contracts, as well as workstations,\n\nlaptops and mobile devices running software capable of remotely accessing\n\nother IT resources positioned as cloud services.\n\nFigure 3.8\n\nExamples of cloud service consumers. Depending on the nature of a given\n\ndiagram, an artifact labeled as a cloud service consumer may be a software\n\nprogram or a hardware device (in which case it is implied that it is running\n\na software program capable of acting as a cloud service consumer).\n\n3.3 Goals and Benefits\n\nThe common benefits associated with adopting cloud computing are\n\nexplained in this section.\n\nNote",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "The following sections make reference to the terms “public\n\ncloud” and “private cloud.” These terms are described in the\n\nCloud Deployment Models section in Chapter 4.\n\nIncreased Responsiveness\n\nCloud computing plays an essential role in enhancing an organization’s\n\nbusiness agility by empowering it to be more responsive to business and\n\nusage scenarios that can be more effectively addressed by leveraging native\n\ncloud capabilities, such as scalability on-demand, data availability, reduced\n\ninfrastructure maintenance, reduced business complexity, automation, and\n\nincreased up-time.\n\nFor example, greater data availability can enable employees to more easily\n\nwork remotely, thereby providing staff with increased flexibility and\n\nproductivity.\n\nUtilizing platforms maintained by cloud providers, frees organizations from\n\nthe responsibilities they would normally have for administering them\n\ninternally. This can also reduce the complexity infrastructure environments,\n\nthereby introducing novel ways for employees to collaborate and work\n\nthrough faster and less complex technology rollouts for new business\n\ninitiatives.",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Ultimately, cloud computing empowers organizations to become\n\nsignificantly more responsive removing or reducing many organizational\n\nburdens associated with business solution development, deployment and\n\nmaintenance, and by reducing time-to-market timelines.\n\nReduced Investments and Proportional Costs\n\nSimilar to a product wholesaler that purchases goods in bulk for lower price\n\npoints, public cloud providers base their business model on the mass-\n\nacquisition of IT resources that are then made available to cloud consumers\n\nvia attractively priced leasing packages. This opens the door for\n\norganizations to gain access to powerful infrastructure without having to\n\npurchase it themselves.\n\nThe most common economic rationale for investing in cloud-based IT\n\nresources is in the reduction or outright elimination of up-front IT\n\ninvestments, namely hardware and software purchases and ownership costs.\n\nA cloud’s Measured Usage characteristic represents a feature-set that allows\n\nmeasured operational expenditures (directly related to business\n\nperformance) to replace anticipated capital expenditures. This is also\n\nreferred to as proportional costs.\n\nThis elimination or minimization of up-front financial commitments allows\n\nenterprises to start small and accordingly increase IT resource allocation as\n\nrequired. Moreover, the reduction of up-front capital expenses allows for\n\nthe capital to be redirected to the core business investment. In its most basic",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "form, opportunities to decrease costs are derived from the deployment and\n\noperation of large-scale data centers by major cloud providers. Such data\n\ncenters are commonly located in destinations where real estate, IT\n\nprofessionals, and network bandwidth can be obtained at lower costs,\n\nresulting in both capital and operational savings.\n\nThe same rationale applies to operating systems, middleware or platform\n\nsoftware, and application software. Pooled IT resources are made available\n\nto and shared by multiple cloud consumers, resulting in increased or even\n\nmaximum possible utilization. Operational costs and inefficiencies can be\n\nfurther reduced by applying proven practices and patterns for optimizing\n\ncloud architectures, their management, and their governance.\n\nCommon measurable benefits to cloud consumers include:\n\nOn-demand access to pay-as-you-go computing resources on a short-term\n\nbasis (such as processors by the hour), and the ability to release these\n\ncomputing resources when they are no longer needed.\n\nThe perception of having unlimited computing resources that are available\n\non-demand, thereby reducing the need to prepare for provisioning.\n\nThe ability to add or remove IT resources at a fine-grained level, such as\n\nmodifying available storage disk space by single gigabyte increments.",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Abstraction of the infrastructure so applications are not locked into devices\n\nor locations and can be easily moved if needed.\n\nFor example, a company with sizable batch-centric tasks can complete them\n\nas quickly as their application software can scale. Using 100 servers for one\n\nhour costs the same as using one server for 100 hours. This “elasticity” of\n\nIT resources, achieved without requiring steep initial investments to create a\n\nlarge-scale computing infrastructure, can be extremely compelling.\n\nDespite the ease with which many identify the financial benefits of cloud\n\ncomputing, the actual economics can be complex to calculate and assess.\n\nThe decision to proceed with a cloud computing adoption strategy will\n\ninvolve much more than a simple comparison between the cost of leasing\n\nand the cost of purchasing. For example, the financial benefits of dynamic\n\nscaling and the risk transference of both over-provisioning (under-\n\nutilization) and under-provisioning (over-utilization) must also be\n\naccounted for. Chapter 17 explores common criteria and formulas for\n\nperforming detailed financial comparisons and assessments.\n\nNote\n\nAnother area of cost savings offered by clouds is the “as-a-\n\nservice” usage model, whereby technical and operational\n\nimplementation details of IT resource provisioning are\n\nabstracted from cloud consumers and packaged into “ready-",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "to-use” or “off-the-shelf” solutions. These services-based\n\nproducts can simplify and expedite the development,\n\ndeployment, and administration of IT resources when\n\ncompared to performing equivalent tasks with on-premise\n\nsolutions. The resulting savings in time and required IT\n\nexpertise can be significant and can contribute to the\n\njustification of adopting cloud computing.\n\nIncreased Scalability\n\nBy providing pools of IT resources, along with tools and technologies\n\ndesigned to leverage them collectively, clouds can instantly and\n\ndynamically allocate IT resources to cloud consumers, on-demand or via\n\nthe cloud consumer’s direct configuration. This empowers cloud consumers\n\nto scale their cloud-based IT resources to accommodate processing\n\nfluctuations and peaks automatically or manually. Similarly, cloud-based IT\n\nresources can be released (automatically or manually) as processing\n\ndemands decrease. A simple example of usage demand fluctuations\n\nthroughout a 24 hour period is provided in Figure 3.9.",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Figure 3.9\n\nAn example of an organization’s changing demand for an IT resource over\n\nthe course of a day.\n\nThe inherent, built-in feature of clouds to provide flexible levels of\n\nscalability to IT resources is directly related to the aforementioned",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "proportional costs benefit. Besides the evident financial gain to the\n\nautomated reduction of scaling, the ability of IT resources to always meet\n\nand fulfill unpredictable usage demands avoids potential loss of business\n\nthat can occur when usage thresholds are met.\n\nNote\n\nWhen associating the benefit of Increased Scalability with\n\nthe capacity planning strategies introduced earlier in the\n\nBusiness Drivers section, the Lag and Match Strategies are\n\ngenerally more applicable due to a cloud’s ability to scale IT\n\nresources on-demand.\n\nIncreased Availability and Reliability\n\nThe availability and reliability of IT resources are directly associated with\n\ntangible business benefits. Outages limit the time an IT resource can be\n\n“open for business” for its customers, thereby limiting its usage and\n\nrevenue generating potential. Runtime failures that are not immediately\n\ncorrected can have a more significant impact during high-volume usage\n\nperiods. Not only is the IT resource unable to respond to customer requests,\n\nits unexpected failure can decrease overall customer confidence.\n\nA hallmark of the typical cloud environment is its intrinsic ability to\n\nprovide extensive support for increasing the availability of a cloud-based IT",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "resource to minimize or even eliminate outages, and for increasing its\n\nreliability so as to minimize the impact of runtime failure conditions.\n\nSpecifically:\n\nAn IT resource with increased availability is accessible for longer periods of\n\ntime (for example, 22 hours out of a 24 hour day). Cloud providers\n\ngenerally offer “resilient” IT resources for which they are able to guarantee\n\nhigh levels of availability.\n\nAn IT resource with increased reliability is able to better avoid and recover\n\nfrom exception conditions. The modular architecture of cloud environments\n\nprovides extensive failover support that increases reliability.\n\nIt is important that organizations carefully examine the SLAs offered by\n\ncloud providers when considering the leasing of cloud-based services and\n\nIT resources. Although many cloud environments are capable of offering\n\nremarkably high levels of availability and reliability, it comes down to the\n\nguarantees made in the SLA that typically represent their actual contractual\n\nobligations.\n\n3.4 Risks and Challenges\n\nSeveral of the most critical cloud computing challenges are presented and\n\nexamined in this section.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Increased Vulnerability Due to Overlapping Trust Boundaries\n\nMoving business data to the cloud means that the responsibility over data\n\nsecurity is shared with the cloud provider. The remote usage of IT resources\n\nrequires an expansion of trust boundaries by the cloud consumer to include\n\nthe cloud, external to the organization. It can be difficult to establish a\n\nsecurity architecture that spans such a trust boundary without introducing\n\nvulnerabilities unless cloud consumers and cloud providers happen to\n\nsupport the same or compatible security frameworks, which is improbable\n\nwith public clouds.\n\nAnother consequence of overlapping trust boundaries relates to the cloud\n\nprovider’s privileged access to cloud consumer data. The extent to which\n\nthe data is secure is now limited to the security controls and policies applied\n\nby both the cloud consumer and cloud provider. Furthermore, there can be\n\noverlapping trust boundaries from different cloud consumers because cloud-\n\nbased IT resources are commonly shared.\n\nThe overlapping of trust boundaries and the increased exposure of data can\n\nprovide malicious cloud consumers (human and automated) with greater\n\nopportunities to attack IT resources and steal or damage business data.\n\nFigure 3.10 illustrates a scenario whereby two organizations accessing the\n\nsame cloud service are required to extend their respective trust boundaries\n\nto the cloud, resulting in overlapping trust boundaries. Cloud providers are",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "required to offer security mechanisms that accommodate the security\n\nrequirements of both cloud service consumers.\n\nFigure 3.10",
      "content_length": 127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "The shaded area with diagonal lines indicates the overlap of two\n\norganizations’ trust boundaries.\n\nOverlapping trust boundaries is a security threat that is discussed in more\n\ndetail in Chapter 7.\n\nIncreased Vulnerability Due to Shared Security Responsibility\n\nInformation security related to on-premise resources is clearly the\n\nresponsibility of the organization that owns those resources. However,\n\ninformation security related to cloud-based resources is not the sole\n\nresponsibility of the cloud provider, even if the cloud-based resources are\n\nowned by the cloud provider. This is because the information stored and\n\nprocessed in them is owned by the cloud consumer.\n\nAs a result, information security in the cloud is a shared responsibility, with\n\nboth the cloud provider and the cloud consumer having a role to play in\n\nsecuring the cloud environment. It is important to be able to understand and\n\nidentify where the responsibility for each role begins and ends, as well as\n\nknowing how to address the security requirements that correspond to the\n\ncloud consumer.\n\nA cloud provider will typically propose a cloud shared responsibility model\n\nas part of the SLA. This model essentially outlines the respective\n\nresponsibilities of cloud provider and cloud consumer when it comes to\n\nsecuring data and applications in the cloud.",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Increased Exposure to Cyber Threats\n\nThe increased adoption of contemporary digital technologies and digital\n\ntransformation practices has led organizations to move more IT resources\n\ntoward and build more solutions within cloud environments. This has\n\nopened the door to cybersecurity threats and risks that may be new to\n\norganizations and for which they need to be prepared (Figure 3.11).\n\nFigure 3.11",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "An organization that moves from only consuming content and services from\n\nthe Internet to offering its own content and services through the Internet\n\nincreases its exposure to cyber threats.\n\nAugmented exposure to cybersecurity threats due to increased exposure to\n\nthe Internet requires organizations to take action in the protection of IT\n\nassets, both on-premise and cloud-based. Cloud-based IT resources have the\n\nbenefit of shared responsibility for security and access control, both from\n\nthe cloud provider and from the cloud consumer. However, the ultimate\n\nresponsibility lies with the cloud consumer, who needs to address risk\n\nmanagement and cybersecurity risks responsibly and methodologically.\n\nReduced Operational Governance Control\n\nCloud consumers are usually allotted a level of governance control that is\n\nlower than that over on-premise IT resources. This can introduce risks\n\nassociated with how the cloud provider operates its cloud, as well as the\n\nexternal connections that are required for communication between the cloud\n\nand the cloud consumer.\n\nConsider the following examples:\n\nAn unreliable cloud provider may not maintain the guarantees it makes in\n\nthe SLAs that were published for its cloud services. This can jeopardize the\n\nquality of the cloud consumer solutions that rely on these cloud services.",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Longer geographic distances between the cloud consumer and cloud\n\nprovider can require additional network hops that introduce fluctuating\n\nlatency and potential bandwidth constraints.\n\nThe latter scenario is illustrated in Figure 3.12.\n\nFigure 3.12\n\nAn unreliable network connection compromises the quality of\n\ncommunication between cloud consumer and cloud provider environments.",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Legal contracts, when combined with SLAs, technology inspections, and\n\nmonitoring, can mitigate governance risks and issues. A cloud governance\n\nsystem is established through SLAs, given the “as-a-service” nature of\n\ncloud computing. A cloud consumer must keep track of the actual service\n\nlevel being offered and the other warranties that are made by the cloud\n\nprovider.\n\nNote that different cloud delivery models offer varying degrees of\n\noperational control granted to cloud consumers, as further explained in\n\nChapter 4.\n\nLimited Portability Between Cloud Providers\n\nDue to a lack of established industry standards within the cloud computing\n\nindustry, public clouds are commonly proprietary to various extents. For\n\ncloud consumers that have custom-built solutions with dependencies on\n\nthese proprietary environments, it can be challenging to move from one\n\ncloud provider to another.\n\nPortability is a measure used to determine the impact of moving cloud\n\nconsumer IT resources and data between clouds (Figure 3.13).",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Figure 3.13\n\nA cloud consumer’s application has a decreased level of portability when\n\nassessing a potential migration from Cloud A to Cloud B, because the cloud",
      "content_length": 161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "provider of Cloud B does not support the same security technologies as\n\nCloud A.\n\nMulti-Regional Compliance and Legal Issues\n\nThird-party cloud providers will frequently establish data centers in\n\naffordable or convenient geographical locations. Cloud consumers will\n\noften not be aware of the physical location of their IT resources and data\n\nwhen hosted by public clouds. For some organizations, this can pose serious\n\nlegal concerns pertaining to industry or government regulations that specify\n\ndata privacy and storage policies. For example, some UK laws require\n\npersonal data belonging to UK citizens to be kept within the United\n\nKingdom.\n\nAnother potential legal issue pertains to the accessibility and disclosure of\n\ndata. Countries have laws that require some types of data to be disclosed to\n\ncertain government agencies or to the subject of the data. For example, a\n\nEuropean cloud consumer’s data that is located in the U.S. can be more\n\neasily accessed by government agencies (due to the U.S. Patriot Act) when\n\ncompared to data located in many European Union countries.\n\nMost regulatory frameworks recognize that cloud consumer organizations\n\nare ultimately responsible for the security, integrity, and storage of their\n\nown data, even when it is held by an external cloud provider.",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Cost Overruns\n\nCreating a business case for cloud computing can be a difficult undertaking\n\ndue to the number of requirements, considerations and stakeholders that\n\nneed to be accommodated. Many organizations proceed with cloud\n\nmigration initiatives without a proper business case at all. This has been one\n\nof the root causes of cost overruns in cloud projects and has led to poor\n\nplanning, poor or absent governance and costly cloud risk mitigation\n\npolicies.\n\nTraditionally, a business case process is triggered by the need to justify\n\nlarge capital investments. However, with cloud environments that allow\n\nusers to quickly obtain desired capabilities, organizations can incorrectly\n\nassume that they will not require additional capital investments. As cloud\n\nadoption then grows, there is eventually a recognition that this operating\n\nmodel requires capital investment beyond the migration itself, but there\n\nmay not be a method of estimating the amount of the investment necessary\n\nprior to adoption or migration.",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Chapter 4\n\nFundamental Concepts and Models\n\n4.1 Roles and Boundaries\n\n4.2 Cloud Characteristics\n\n4.3 Cloud Delivery Models\n\n4.4 Cloud Deployment Models\n\nThe upcoming sections cover introductory topic areas pertaining to the\n\nfundamental models used to categorize and define clouds and their most\n\ncommon service offerings, along with definitions of organizational roles\n\nand the specific set of characteristics that collectively distinguish a cloud.\n\n4.1 Roles and Boundaries\n\nOrganizations and humans can assume different types of pre-defined roles\n\ndepending on how they relate to and/or interact with a cloud and its hosted\n\nIT resources. Each of the upcoming roles participates in and carries out\n\nresponsibilities in relation to cloud-based activity. The following sections\n\ndefine these roles and identify their main interactions.",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Cloud Provider\n\nThe organization that provides cloud-based IT resources is the cloud\n\nprovider. When assuming the role of cloud provider, an organization is\n\nresponsible for making cloud services available to cloud consumers, as per\n\nagreed upon SLA guarantees. The cloud provider is further tasked with any\n\nrequired management and administrative duties to ensure the on-going\n\noperation of the overall cloud infrastructure.\n\nCloud providers normally own the IT resources that are made available for\n\nlease by cloud consumers; however, some cloud providers also “resell” IT\n\nresources leased from other cloud providers.\n\nCloud Consumer\n\nA cloud consumer is an organization (or a human) that has a formal contract\n\nor arrangement with a cloud provider to use IT resources made available by\n\nthe cloud provider. Specifically, the cloud consumer uses a cloud service\n\nconsumer to access a cloud service (Figure 4.1).",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Figure 4.1\n\nA cloud consumer (Organization A) interacts with a cloud service from a\n\ncloud provider (that owns Cloud A). Within Organization A, the cloud\n\nservice consumer is being used to access the cloud service.\n\nThe figures in this book do not always explicitly label symbols as “cloud\n\nconsumers.” Instead, it is generally implied that organizations or humans\n\nshown remotely accessing cloud-based IT resources are considered cloud\n\nconsumers.\n\nNote",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "When depicting interaction scenarios between cloud-based\n\nIT resources and consumer organizations, there are no strict\n\nrules as to how the terms “cloud service consumer” and\n\n“cloud consumer” are used in this book. The former is\n\nusually used to label software programs or applications that\n\nprogrammatically interface with a cloud service’s technical\n\ncontract or API. The latter term is broader in that it can be\n\nused to label an organization, an individual accessing a user-\n\ninterface, or a software program that assumes the role of\n\ncloud consumer when interacting with a cloud, a cloud-\n\nbased IT resource, or a cloud provider. The broad\n\napplicability of the “cloud consumer” term is intentional as\n\nit allows it to be used in figures that explore different types\n\nof consumer-provider relationships within different technical\n\nand business contexts.\n\nCloud Broker\n\nA third-party organization that assumes the responsibility of negotiating,\n\nmanaging and operating cloud services on behalf of a cloud consumer is\n\nassuming the role of cloud broker. Cloud brokers can provide mediation\n\nservices between cloud consumers and cloud providers, including\n\nintermediation, aggregation, arbitrage, and others.",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "A cloud broker commonly provides these services for multiple cloud\n\nconsumers facing multiple cloud providers alternatively or simultaneously,\n\nacting as an integrator of cloud services and an aggregator of cloud\n\nconsumers, as shown in Figure 4.2.\n\nFigure 4.2",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "A cloud broker offers the cloud-based services and IT resources from three\n\ndifferent cloud providers to its customers, Cloud Consumers A, B, and C.\n\nCloud Service Owner\n\nThe person or organization that legally owns a cloud service is called a\n\ncloud service owner. The cloud service owner can be the cloud consumer,\n\nor the cloud provider that owns the cloud within which the cloud service\n\nresides.\n\nFor example, either the cloud consumer of Cloud X or the cloud provider of\n\nCloud X could own Cloud Service A (Figures 4.3 and 4.4).",
      "content_length": 534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Figure 4.3\n\nA cloud consumer can be a cloud service owner when it deploys its own\n\nservice in a cloud.\n\nFigure 4.4\n\nA cloud provider becomes a cloud service owner if it deploys its own cloud\n\nservice, typically for other cloud consumers to use.",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Note that a cloud consumer that owns a cloud service hosted by a third-\n\nparty cloud does not necessarily need to be the user (or consumer) of the\n\ncloud service. Several cloud consumer organizations develop and deploy\n\ncloud services in clouds owned by other parties for the purpose of making\n\nthe cloud services available to the general public.\n\nThe reason a cloud service owner is not called a cloud resource owner is\n\nbecause the cloud service owner role only applies to cloud services (which,\n\nas explained in Chapter 3, are externally accessible IT resources that reside\n\nin a cloud).\n\nCloud Resource Administrator\n\nA cloud resource administrator is the person or organization responsible for\n\nadministering a cloud-based IT resource (including cloud services). The\n\ncloud resource administrator can be (or belong to) the cloud consumer or\n\ncloud provider of the cloud within which the cloud service resides.\n\nAlternatively, it can be (or belong to) a third-party organization contracted\n\nto administer the cloud-based IT resource.\n\nFor example, a cloud service owner can contract a cloud resource\n\nadministrator to administer a cloud service (Figures 4.5 and 4.6).",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Figure 4.5\n\nA cloud resource administrator can be with a cloud consumer organization\n\nand administer remotely accessible IT resources that belong to the cloud\n\nconsumer.",
      "content_length": 169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Figure 4.6\n\nA cloud resource administrator can be with a cloud provider organization\n\nfor which it can administer the cloud provider’s internally and externally\n\navailable IT resources.\n\nThe reason a cloud resource administrator is not referred to as a “cloud\n\nservice administrator” is because this role may be responsible for\n\nadministering cloud-based IT resources that don’t exist as cloud services.\n\nFor example, if the cloud resource administrator belongs to (or is contracted\n\nby) the cloud provider, IT resources not made remotely accessible may be\n\nadministered by this role (and these types of IT resources are not classified\n\nas cloud services).",
      "content_length": 656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Additional Roles\n\nThe NIST Cloud Computing Reference Architecture defines the following\n\nsupplementary roles:\n\nCloud Auditor – A third-party (often accredited) that conducts independent\n\nassessments of cloud environments assumes the role of the cloud auditor.\n\nThe typical responsibilities associated with this role include the evaluation\n\nof security controls, privacy impacts, and performance. The main purpose\n\nof the cloud auditor role is to provide an unbiased assessment (and possible\n\nendorsement) of a cloud environment to help strengthen the trust\n\nrelationship between cloud consumers and cloud providers.\n\nCloud Carrier – The party responsible for providing the wire-level\n\nconnectivity between cloud consumers and cloud providers assumes the\n\nrole of the cloud carrier. This role is often assumed by network and\n\ntelecommunication providers.\n\nWhile each is legitimate, most architectural scenarios covered in this book\n\ndo not include these roles.\n\nOrganizational Boundary\n\nAn organizational boundary represents the physical perimeter that\n\nsurrounds a set of IT resources that are owned and governed by an\n\norganization. The organizational boundary does not represent the boundary",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "of an actual organization, only an organizational set of IT assets and IT\n\nresources. Similarly, clouds have an organizational boundary (Figure 4.7).\n\nFigure 4.7\n\nOrganizational boundaries of a cloud consumer (left), and a cloud provider\n\n(right), represented by a broken line notation.\n\nTrust Boundary\n\nWhen an organization assumes the role of cloud consumer to access cloud-\n\nbased IT resources, it needs to extend its trust beyond the physical boundary\n\nof the organization to include parts of the cloud environment.",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "A trust boundary is a logical perimeter that typically spans beyond physical\n\nboundaries to represent the extent to which IT resources are trusted (Figure\n\n4.8). When analyzing cloud environments, the trust boundary is most\n\nfrequently associated with the trust issued by the organization acting as the\n\ncloud consumer.\n\nFigure 4.8\n\nAn extended trust boundary encompasses the organizational boundaries of\n\nthe cloud provider and the cloud consumer.",
      "content_length": 448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Note\n\nAnother type of boundary relevant to cloud environments is\n\nthe logical network perimeter. This type of boundary is\n\nclassified as a cloud computing mechanism and is covered in\n\nChapter 8.\n\n4.2 Cloud Characteristics\n\nAn IT environment requires a specific set of characteristics to enable the\n\nremote provisioning of scalable and measured IT resources in an effective\n\nmanner. These characteristics need to exist to a meaningful extent for the IT\n\nenvironment to be considered an effective cloud.\n\nThe following characteristics are common to the majority of cloud\n\nenvironments:\n\non-demand usage\n\nubiquitous access\n\nmultitenancy (and resource pooling)\n\nelasticity\n\nmeasured usage",
      "content_length": 684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "resiliency\n\nCloud providers and cloud consumers can assess these characteristics\n\nindividually and collectively to measure the value offering of a given cloud\n\nplatform. Although cloud-based services and IT resources will inherit and\n\nexhibit individual characteristics to varying extents, usually the greater the\n\ndegree to which they are supported and utilized, the greater the resulting\n\nvalue proposition.\n\nOn-Demand Usage\n\nA cloud consumer can unilaterally access cloud-based IT resources giving\n\nthe cloud consumer the freedom to self-provision these IT resources. Once\n\nconfigured, usage of the self-provisioned IT resources can be automated,\n\nrequiring no further human involvement by the cloud consumer or cloud\n\nprovider. This results in an on-demand usage environment. Also known as\n\n“on-demand self-service usage,” this characteristic enables the service-\n\nbased and usage-driven features found in mainstream clouds.\n\nUbiquitous Access\n\nUbiquitous access represents the ability for a cloud service to be widely\n\naccessible. Establishing ubiquitous access for a cloud service can require\n\nsupport for a range of devices, transport protocols, interfaces, and security\n\ntechnologies. To enable this level of access generally requires that the cloud",
      "content_length": 1257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "service architecture be tailored to the particular needs of different cloud\n\nservice consumers.\n\nMultitenancy (and Resource Pooling)\n\nThe characteristic of a software program that enables an instance of the\n\nprogram to serve different consumers (tenants) whereby each is isolated\n\nfrom the other, is referred to as multitenancy. A cloud provider pools its IT\n\nresources to serve multiple cloud service consumers by using multitenancy\n\nmodels that frequently rely on the use of virtualization technologies.\n\nThrough the use of multitenancy technology, IT resources can be\n\ndynamically assigned and reassigned, according to cloud service consumer\n\ndemands.\n\nResource pooling allows cloud providers to pool large-scale IT resources to\n\nserve multiple cloud consumers. Different physical and virtual IT resources\n\nare dynamically assigned and reassigned according to cloud consumer\n\ndemand, typically followed by execution through statistical multiplexing.\n\nResource pooling is commonly achieved through multitenancy technology,\n\nand therefore encompassed by this multitenancy characteristic. See the\n\nResource Pooling Architecture section in Chapter 13 for a more detailed\n\nexplanation.\n\nFigures 4.9 and 4.10 illustrate the difference between single-tenant and\n\nmultitenant environments.",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Figure 4.9\n\nIn a single-tenant environment, each cloud consumer has a separate IT\n\nresource instance.",
      "content_length": 101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Figure 4.10\n\nIn a multitenant environment, a single instance of an IT resource, such as a\n\ncloud storage device, serves multiple consumers.\n\nAs illustrated in Figure 4.10, multitenancy allows several cloud consumers\n\nto use the same IT resource or its instance while each remains unaware that\n\nit may be used by others.\n\nElasticity\n\nElasticity is the automated ability of a cloud to transparently scale IT\n\nresources, as required in response to runtime conditions or as pre-\n\ndetermined by the cloud consumer or cloud provider. Elasticity is often\n\nconsidered a core justification for the adoption of cloud computing,\n\nprimarily due to the fact that it is closely associated with the Reduced\n\nInvestment and Proportional Costs benefit. Cloud providers with vast IT\n\nresources can offer the greatest range of elasticity.\n\nMeasured Usage\n\nThe measured usage characteristic represents the ability of a cloud platform\n\nto keep track of the usage of its IT resources, primarily by cloud consumers.\n\nBased on what is measured, the cloud provider can charge a cloud\n\nconsumer only for the IT resources actually used and/or for the timeframe\n\nduring which access to the IT resources was granted. In this context,\n\nmeasured usage is closely related to the on-demand characteristic.",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Measured usage is not limited to tracking statistics for billing purposes. It\n\nalso encompasses the general monitoring of IT resources and related usage\n\nreporting (for both cloud provider and cloud consumers). Therefore,\n\nmeasured usage is also relevant to clouds that do not charge for usage\n\n(which may be applicable to the private cloud deployment model described\n\nin the upcoming Cloud Deployment Models section).\n\nResiliency\n\nResilient computing is a form of failover that distributes redundant\n\nimplementations of IT resources across physical locations. IT resources can\n\nbe pre-configured so that if one becomes deficient, processing is\n\nautomatically handed over to another redundant implementation. Within\n\ncloud computing, the characteristic of resiliency can refer to redundant IT\n\nresources within the same cloud (but in different physical locations) or\n\nacross multiple clouds. Cloud consumers can increase both the reliability\n\nand availability of their applications by leveraging the resiliency of cloud-\n\nbased IT resources (Figure 4.11).",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Figure 4.11",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "A resilient system in which Cloud B hosts a redundant implementation of\n\nCloud Service A to provide failover in case Cloud Service A on Cloud A\n\nbecomes unavailable.\n\n4.3 Cloud Delivery Models\n\nA cloud delivery model represents a specific, pre-packaged combination of\n\nIT resources offered by a cloud provider. Three common cloud delivery\n\nmodels have become widely established and formalized:\n\nInfrastructure-as-a-Service (IaaS)\n\nPlatform-as-a-Service (PaaS)\n\nSoftware-as-a-Service (SaaS)\n\nThese three models are interrelated in how the scope of one can encompass\n\nthat of another, as explored in the Combining Cloud Delivery Models\n\nsection later in this chapter.\n\nNote\n\nNote that a cloud delivery model can be referred to as a\n\ncloud service delivery model because each model is\n\nclassified as a different type of cloud service offering.",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Infrastructure-as-a-Service (IaaS)\n\nThe IaaS delivery model represents a self-contained IT environment\n\ncomprised of infrastructure-centric IT resources that can be accessed and\n\nmanaged via cloud service-based interfaces and tools. This environment can\n\ninclude hardware, network, connectivity, operating systems, and other\n\n“raw” IT resources. In contrast to traditional hosting or outsourcing\n\nenvironments, with IaaS, IT resources are typically virtualized and\n\npackaged into bundles that simplify up-front runtime scaling and\n\ncustomization of the infrastructure.\n\nThe general purpose of an IaaS environment is to provide cloud consumers\n\nwith a high level of control and responsibility over its configuration and\n\nutilization. The IT resources provided by IaaS are generally not pre-\n\nconfigured, placing the administrative responsibility directly upon the cloud\n\nconsumer. This model is therefore used by cloud consumers that require a\n\nhigh level of control over the cloud-based environment they intend to\n\ncreate.\n\nSometimes cloud providers will contract IaaS offerings from other cloud\n\nproviders to scale their own cloud environments. The types and brands of\n\nthe IT resources provided by IaaS products offered by different cloud\n\nproviders can vary. IT resources available through IaaS environments are\n\ngenerally offered as freshly initialized virtual instances. A central and\n\nprimary IT resource within a typical IaaS environment is the virtual server.",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Virtual servers are leased by specifying server hardware requirements, such\n\nas processor capacity, memory, and local storage space, as shown in Figure\n\n4.12.\n\nFigure 4.12\n\nA cloud consumer is using a virtual server within an IaaS environment.\n\nCloud consumers are provided with a range of contractual guarantees by\n\nthe cloud provider, pertaining to characteristics such as capacity,\n\nperformance, and availability.",
      "content_length": 416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Platform-as-a-Service (PaaS)\n\nThe PaaS delivery model represents a pre-defined “ready-to-use”\n\nenvironment typically comprised of already deployed and configured IT\n\nresources. Specifically, PaaS relies on (and is primarily defined by) the\n\nusage of a ready-made environment that establishes a set of pre-packaged\n\nproducts and tools used to support the entire delivery lifecycle of custom\n\napplications.\n\nCommon reasons a cloud consumer would use and invest in a PaaS\n\nenvironment include:\n\nThe cloud consumer wants to extend on-premise environments into the\n\ncloud for scalability and economic purposes.\n\nThe cloud consumer uses the ready-made environment to entirely substitute\n\nan on-premise environment.\n\nThe cloud consumer wants to become a cloud provider and deploys its own\n\ncloud services to be made available to other external cloud consumers.\n\nBy working within a ready-made platform, the cloud consumer is spared the\n\nadministrative burden of setting up and maintaining the bare infrastructure\n\nIT resources provided via the IaaS model. Conversely, the cloud consumer\n\nis granted a lower level of control over the underlying IT resources that host\n\nand provision the platform (Figure 4.13).",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Figure 4.13\n\nA cloud consumer is accessing a ready-made PaaS environment. The\n\nquestion mark indicates that the cloud consumer is intentionally shielded",
      "content_length": 152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "from the implementation details of the platform.\n\nPaaS products are available with different development stacks. For\n\nexample, Google App Engine offers a Java and Python-based environment.\n\nThe ready-made environment is further described as a cloud computing\n\nmechanism in Chapter 8.\n\nSoftware-as-a-Service (SaaS)\n\nA software program positioned as a shared cloud service and made\n\navailable as a “product” or generic utility represents the typical profile of a\n\nSaaS offering. The SaaS delivery model is typically used to make a reusable\n\ncloud service widely available (often commercially) to a range of cloud\n\nconsumers. An entire marketplace exists around SaaS products that can be\n\nleased and used for different purposes and via different terms (Figure 4.14).",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Figure 4.14\n\nThe cloud service consumer is given access to the cloud service contract,\n\nbut not to any underlying IT resources or implementation details.\n\nA cloud consumer is generally granted very limited administrative control\n\nover a SaaS implementation. It is most often provisioned by the cloud\n\nprovider, but it can be legally owned by whichever entity assumes the cloud\n\nservice owner role. For example, an organization acting as a cloud\n\nconsumer while using and working with a PaaS environment can build a\n\ncloud service that it decides to deploy in that same environment as a SaaS",
      "content_length": 590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "offering. The same organization then effectively assumes the cloud provider\n\nrole as the SaaS-based cloud service is made available to other\n\norganizations that act as cloud consumers when using that cloud service.\n\nComparing Cloud Delivery Models\n\nProvided in this section are two tables that compare different aspects of\n\ncloud delivery model usage and implementation. Table 4.1 contrasts control\n\nlevels and Table 4.2 compares typical responsibilities and usage.\n\nTable 4-1\n\nA comparison of typical cloud delivery model control levels.",
      "content_length": 538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Table 4-2\n\nTypical activities carried out by cloud consumers and cloud providers\n\nin relation to the cloud delivery models.",
      "content_length": 123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Combining Cloud Delivery Models\n\nThe three base cloud delivery models comprise a natural provisioning\n\nhierarchy, allowing for opportunities for the combined application of the\n\nmodels to be explored. The upcoming sections briefly highlight\n\nconsiderations pertaining to two common combinations.\n\nIaaS + PaaS\n\nA PaaS environment will be built upon an underlying infrastructure\n\ncomparable to the physical and virtual servers and other IT resources\n\nprovided in an IaaS environment. Figure 4.15 shows how these two models\n\ncan conceptually be combined into a simple layered architecture.",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Figure 4.15\n\nA PaaS environment based on the IT resources provided by an underlying\n\nIaaS environment.",
      "content_length": 102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "A cloud provider would not normally need to provision an IaaS\n\nenvironment from its own cloud in order to make a PaaS environment\n\navailable to cloud consumers. So how would the architectural view\n\nprovided by Figure 4.16 be useful or applicable? Let’s say that the cloud\n\nprovider offering the PaaS environment chose to lease an IaaS environment\n\nfrom a different cloud provider.\n\nThe motivation for such an arrangement may be influenced by economics\n\nor maybe because the first cloud provider is close to exceeding its existing\n\ncapacity by serving other cloud consumers. Or, perhaps a particular cloud\n\nconsumer imposes a legal requirement for data to be physically stored in a\n\nspecific region (different from where the first cloud provider’s cloud\n\nresides), as illustrated in Figure 4.16.",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Figure 4.16\n\nAn example of a contract between Cloud Providers X and Y, in which\n\nservices offered by Cloud Provider X are physically hosted on virtual\n\nservers belonging to Cloud Provider Y. Sensitive data that is legally\n\nrequired to stay in a specific region is physically kept in Cloud B, which is\n\nphysically located in that region.\n\nIaaS + PaaS + SaaS\n\nAll three cloud delivery models can be combined to establish layers of IT\n\nresources that build upon each other. For example, by adding on to the\n\npreceding layered architecture shown in Figure 4.16, the ready-made\n\nenvironment provided by the PaaS environment can be used by the cloud\n\nconsumer organization to develop and deploy its own SaaS cloud services\n\nthat it can then make available as commercial products (Figure 4.17).",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure 4.17",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "A simple layered view of an architecture comprised of IaaS and PaaS\n\nenvironments hosting three SaaS cloud service implementations.\n\nCloud Delivery Sub Models\n\nMany specialized variations of the cloud delivery models exist, each\n\ncomprised of a distinct combination of IT resources. These cloud delivery\n\nsub models are also typically named using the “as-a-Service” convention\n\nand each can be mapped to one of the three basic cloud delivery models.\n\nFor example, the Database-as-a-Service sub model (Figure 4.18) belongs to\n\nthe PaaS model, since a database system is commonly a component of the\n\nready-made environment that is part of a PaaS platform.",
      "content_length": 653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Figure 4.18\n\nThe Database-as-a-Service cloud delivery sub model is represented by a\n\ncloud provider to provide access to databases.\n\nSimilarly, Security-as-a-Service is a sub model of SaaS and is used to\n\nprovide access to features that can be used to secure cloud consumer IT\n\nassets.\n\nAnother example is the Storage-as-a-Service sub model (Figure 4.19) of\n\nIaaS that a cloud provider can use to delivers cloud storage-related services\n\nto cloud consumers.\n\nFigure 4.19",
      "content_length": 470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "A Storage-as-a-Service offering can provide different storage-related\n\nservices, such as structured and unstructured data storage, file storage,\n\nobject storage, and long-term archive storage.\n\nAlso considered a sub model of SaaS is the cloud-native delivery sub model\n\nthat allows for cloud-native applications to be built and deployed as\n\ncollections of self-contained services packaged in lightweight containers.\n\nCloud-native applications (Figure 4.20) have no preference for any\n\nparticular operating system or computer and work at a higher degree of\n\nabstraction. These types of applications run on infrastructure that is\n\nvirtualized, shared, and elastic. They may align with the underlying\n\ninfrastructure to dynamically grow and shrink in response to varied load.",
      "content_length": 772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Figure 4.20\n\nA cloud-native application deployed using multiple containers.\n\nOther examples of common cloud delivery sub models include (but are not\n\nlimited to) the following:\n\nCommunication-as-a-Service (a sub model of SaaS)\n\nIntegration-as-a-Service (a sub model of PaaS)\n\nTesting-as-a-Service (a sub model of SaaS)",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Process-as-a-Service (a sub model of SaaS)\n\nDesktop-as-a-Service (a sub model of IaaS)\n\n4.4 Cloud Deployment Models\n\nA cloud deployment model represents a specific type of cloud environment,\n\nprimarily distinguished by ownership, size, and access.\n\nThere are four common cloud deployment models:\n\nPublic cloud\n\nPrivate cloud\n\nMulti-cloud\n\nHybrid cloud\n\nThe following sections describe each model.\n\nPublic Clouds\n\nA public cloud is a publicly accessible cloud environment owned by a third-\n\nparty cloud provider. The IT resources on public clouds are usually\n\nprovisioned via the previously described cloud delivery models and are\n\ngenerally offered to cloud consumers at a cost or are commercialized via\n\nother avenues (such as advertisement).",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "The cloud provider is responsible for the creation and on-going\n\nmaintenance of the public cloud and its IT resources. Many of the scenarios\n\nand architectures explored in upcoming chapters involve public clouds and\n\nthe relationship between the providers and consumers of IT resources via\n\npublic clouds.\n\nFigure 4.21 shows a partial view of the public cloud landscape, highlighting\n\nsome of the primary vendors in the marketplace.",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Figure 4.21\n\nOrganizations act as cloud consumers when accessing cloud services and\n\nIT resources made available by different cloud providers.",
      "content_length": 142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Private Clouds\n\nA private cloud is owned by a single organization. Private clouds enable an\n\norganization to use cloud computing technology as a means of centralizing\n\naccess to IT resources by different parts, locations, or departments of the\n\norganization. When a private cloud exists as a controlled environment, the\n\nproblems described in the Risks and Challenges section from Chapter 3 do\n\nnot tend to apply.\n\nThe use of a private cloud can change how organizational and trust\n\nboundaries are defined and applied. The actual administration of a private\n\ncloud environment may be carried out by internal or outsourced staff.",
      "content_length": 628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Figure 4.22\n\nA cloud service consumer in the organization’s on-premise environment\n\naccesses a cloud service hosted on the same organization’s private cloud\n\nvia a virtual private network.\n\nWith a private cloud, the same organization is technically both the cloud\n\nconsumer and cloud provider (Figure 4.22). In order to differentiate these\n\nroles:",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "a separate organizational department typically assumes the responsibility\n\nfor provisioning the cloud (and therefore assumes the cloud provider role)\n\ndepartments requiring access to the private cloud assume the cloud\n\nconsumer role\n\nIt is important to use the terms “on-premise” and “cloud-based” correctly\n\nwithin the context of a private cloud. Even though the private cloud may\n\nphysically reside on the organization’s premises, IT resources it hosts are\n\nstill considered “cloud-based” as long as they are made remotely accessible\n\nto cloud consumers. IT resources hosted outside of the private cloud by the\n\ndepartments acting as cloud consumers are therefore considered “on-\n\npremise” in relation to the private cloud-based IT resources.\n\nMulti-Clouds\n\nWith a multi-cloud deployment model, a cloud consumer organization can\n\nuse cloud services and IT resources from different public clouds provided\n\nby multiple cloud providers, as shown in Figure 4.23.",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Figure 4.23\n\nAn organization uses the multi-cloud model to utilize cloud-based IT\n\nresources from different cloud providers.",
      "content_length": 124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "For example, this deployment model can be used to improve redundancy\n\nand system backups, to improve mobility by reducing vendor lock-in, or to\n\nleverage best-of-breed cloud services from different cloud vendors.\n\nHybrid Clouds\n\nA hybrid cloud is a cloud environment comprised of two or more different\n\ncloud deployment models. For example, a cloud consumer may choose to\n\ndeploy cloud services processing sensitive data to a private cloud and other,\n\nless sensitive cloud services to a public cloud. The result of this\n\ncombination is a hybrid deployment model (Figure 4.24).",
      "content_length": 576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Figure 4.24",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "An organization using a hybrid cloud architecture that utilizes both a\n\nprivate and public cloud.\n\nHybrid deployment architectures can be complex and challenging to create\n\nand maintain due to the potential disparity in cloud environments and the\n\nfact that management responsibilities are typically split between the private\n\ncloud provider organization and the public cloud provider.",
      "content_length": 385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 5\n\nCloud-Enabling Technology\n\n5.1 Networks and Internet Architecture\n\n5.2 Cloud Data Center Technology\n\n5.3 Modern Virtualization\n\n5.4 Multitenant Technology\n\n5.5 Service Technology and Service APIs\n\n5.6 Case Study Example\n\nModern-day clouds are underpinned by a set of primary technology\n\ncomponents that collectively enable key features and characteristics\n\nassociated with contemporary cloud computing.\n\nMost existed and matured prior to the advent of cloud computing, although\n\ncloud computing advancements helped further evolve select areas of these\n\ncloud-enabling technologies.\n\n5.1 Networks and Internet Architecture\n\nAll clouds must be connected to a network. This inevitable requirement\n\nforms an inherent dependency on internetworking.",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Internetworks, or the Internet, allow for the remote provisioning of IT\n\nresources and are directly supportive of ubiquitous network access. Cloud\n\nconsumers have the option of accessing the cloud using only private and\n\ndedicated network links in LANs, although most clouds are Internet-\n\nenabled. The potential of cloud platforms therefore generally grows in\n\nparallel with advancements in Internet connectivity and service quality.\n\nInternet Service Providers (ISPs)\n\nEstablished and deployed by ISPs, the Internet’s largest backbone networks\n\nare strategically interconnected by core routers that connect the world’s\n\nmultinational networks. As shown in Figure 5.1, an ISP network\n\ninterconnects to other ISP networks and various organizations.",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Figure 5.1\n\nMessages travel over dynamic network routes in this ISP internetworking\n\nconfiguration.\n\nThe concept of the Internet was based on a decentralized provisioning and\n\nmanagement model. ISPs can freely deploy, operate, and manage their\n\nnetworks in addition to selecting partner ISPs for interconnection. No\n\ncentralized entity comprehensively governs the Internet, although bodies\n\nlike the Internet Corporation for Assigned Names and Numbers (ICANN)\n\nsupervise and coordinate Internet communications.\n\nGovernmental and regulatory laws dictate the service provisioning\n\nconditions for organizations and ISPs both within and outside of national\n\nborders. Certain realms of the Internet still require the demarcation of\n\nnational jurisdiction and legal boundaries.\n\nThe Internet’s topology has become a dynamic and complex aggregate of\n\nISPs that are highly interconnected via its core protocols. Smaller branches\n\nextend from these major nodes of interconnection, branching outwards\n\nthrough smaller networks until eventually reaching every Internet-enabled\n\nelectronic device.",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Worldwide connectivity is enabled through a hierarchical topology\n\ncomposed of Tiers 1, 2, and 3 (Figure 5.2). The core Tier 1 is made of large-\n\nscale, international cloud providers that oversee massive interconnected\n\nglobal networks, which are connected to Tier 2’s large regional providers.\n\nThe interconnected ISPs of Tier 2 connect with Tier 1 providers, as well as\n\nthe local ISPs of Tier 3. Cloud consumers and cloud providers can connect\n\ndirectly using a Tier 1 provider, since any operational ISP can enable\n\nInternet connection.",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Figure 5.2\n\nAn abstraction of the internetworking structure of the Internet.\n\nThe communication links and routers of the Internet and ISP networks are\n\nIT resources that are distributed among countless traffic generation paths.\n\nTwo fundamental components used to construct the internetworking\n\narchitecture are connectionless packet switching (datagram networks) and\n\nrouter-based interconnectivity.\n\nConnectionless Packet Switching (Datagram Networks)\n\nEnd-to-end (sender-receiver pair) data flows are divided into packets of a\n\nlimited size that are received and processed through network switches and\n\nrouters, then queued and forwarded from one intermediary node to the next.\n\nEach packet carries the necessary location information, such as the Internet\n\nProtocol (IP) or Media Access Control (MAC) address, to be processed and\n\nrouted at every source, intermediary, and destination node.\n\nRouter-Based Interconnectivity\n\nA router is a device that is connected to multiple networks through which it\n\nforwards packets. Even when successive packets are part of the same data\n\nflow, routers process and forward each packet individually while\n\nmaintaining the network topology information that locates the next node on\n\nthe communication path between the source and destination nodes. Routers\n\nmanage network traffic and gauge the most efficient hop for packet",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "delivery, since they are privy to both the packet source and packet\n\ndestination.\n\nThe basic mechanics of internetworking are illustrated in Figure 5.3, in\n\nwhich a message is coalesced from an incoming group of disordered\n\npackets. The depicted router receives and forwards packets from multiple\n\ndata flows.",
      "content_length": 309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Figure 5.3\n\nPackets traveling through the Internet are directed by a router that\n\narranges them into a message.\n\nThe communication path that connects a cloud consumer with its cloud\n\nprovider may involve multiple ISP networks. The Internet’s mesh structure\n\nconnects Internet hosts (endpoint systems) using multiple alternative\n\nnetwork routes that are determined at runtime. Communication can\n\ntherefore be sustained even during simultaneous network failures, although\n\nusing multiple network paths can cause routing fluctuations and latency.\n\nThis applies to ISPs that implement the Internet’s internetworking layer and\n\ninteract with other network technologies, as follows:\n\nPhysical Network\n\nIP packets are transmitted through underlying physical networks that\n\nconnect adjacent nodes, such as Ethernet, ATM network, and the 3G mobile\n\nHSDPA. Physical networks comprise a data link layer that controls data\n\ntransfer between neighboring nodes, and a physical layer that transmits data\n\nbits through both wired and wireless media.\n\nTransport Layer Protocol\n\nTransport layer protocols, such as the Transmission Control Protocol (TCP)\n\nand User Datagram Protocol (UDP), use the IP to provide standardized,",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "end-to-end communication support that facilitates the navigation of data\n\npackets across the Internet.\n\nApplication Layer Protocol\n\nProtocols such as HTTP, SMTP for e-mail, BitTorrent for P2P, and SIP for\n\nIP telephony use transport layer protocols to standardize and enable specific\n\ndata packet transferring methods over the Internet. Many other protocols\n\nalso fulfill application-centric requirements and use either TCP/IP or UDP\n\nas their primary method of data transferring across the Internet and LANs.\n\nFigure 5.4 presents the Internet Reference Model and the protocol stack.",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Figure 5.4\n\nA generic view of the Internet reference model and protocol stack.\n\nTechnical and Business Considerations\n\nConnectivity Issues\n\nIn traditional, on-premise deployment models, enterprise applications and\n\nvarious IT solutions are commonly hosted on centralized servers and\n\nstorage devices residing in the organization’s own data center. End-user\n\ndevices, such as smartphones and laptops, access the data center through the\n\ncorporate network, which provides uninterrupted Internet connectivity.\n\nTCP/IP facilitates both Internet access and on-premise data exchange over\n\nLANs (Figure 5.5). Although not commonly referred to as a cloud model,\n\nthis configuration has been implemented numerous times for medium and\n\nlarge on-premise networks.",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Figure 5.5\n\nThe internetworking architecture of a private cloud. The physical IT\n\nresources that constitute the cloud are located and managed within the\n\norganization.\n\nOrganizations using this deployment model can directly access the network\n\ntraffic to and from the Internet and usually have complete control over and\n\ncan safeguard their corporate networks using firewalls and monitoring\n\nsoftware. These organizations also assume the responsibility of deploying,\n\noperating, and maintaining their IT resources and Internet connectivity.",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "End-user devices that are connected to the network through the Internet can\n\nbe granted continuous access to centralized servers and applications in the\n\ncloud (Figure 5.6).",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Figure 5.6\n\nThe internetworking architecture of an Internet-based cloud deployment\n\nmodel. The Internet is the connecting agent between non-proximate cloud\n\nconsumers, roaming end-users, and the cloud provider’s own network.\n\nA salient cloud feature that applies to end-user functionality is how\n\ncentralized IT resources can be accessed using the same network protocols\n\nregardless of whether they reside inside or outside of a corporate network.\n\nWhether IT resources are on-premise or Internet-based dictates how internal\n\nversus external end-users access services, even if the end-users themselves\n\nare not concerned with the physical location of cloud-based IT resources\n\n(Table 5.1).\n\nTable 5-1\n\nA comparison of on-premise and cloud-based internetworking.",
      "content_length": 761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Cloud providers can easily configure cloud-based IT resources to be\n\naccessible for both external and internal users through an Internet",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "connection (as previously shown in Figure 5.6). This internetworking\n\narchitecture benefits internal users that require ubiquitous access to\n\ncorporate IT solutions, as well as cloud consumers that need to provide\n\nInternet-based services to external users. Major cloud providers offer\n\nInternet connectivity that is superior to the connectivity of individual\n\norganizations, resulting in additional network usage charges as part of their\n\npricing model.\n\nNetwork Bandwidth and Latency Issues\n\nIn addition to being affected by the bandwidth of the data link that connects\n\nnetworks to ISPs, end-to-end bandwidth is determined by the transmission\n\ncapacity of the shared data links that connect intermediary nodes. ISPs need\n\nto use broadband network technology to implement the core network\n\nrequired to guarantee end-to-end connectivity. This type of bandwidth is\n\nconstantly increasing, as Web acceleration technologies, such as dynamic\n\ncaching, compression, and pre-fetching, continue to improve end-user\n\nconnectivity.\n\nAlso referred to as time delay, latency is the amount of time it takes a\n\npacket to travel from one data node to another. Latency increases with every\n\nintermediary node on the data packet’s path. Transmission queues in the\n\nnetwork infrastructure can result in heavy load conditions that also increase\n\nnetwork latency. Networks are dependent on traffic conditions in shared\n\nnodes, making Internet latency highly variable and often unpredictable.",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Packet networks with “best effort” quality-of-service (QoS) typically\n\ntransmit packets on a first-come/first-serve basis. Data flows that use\n\ncongested network paths suffer service-level degradation in the form of\n\nbandwidth reduction, latency increase, or packet loss when traffic is not\n\nprioritized.\n\nThe nature of packet switching allows data packets to choose routes\n\ndynamically as they travel through the Internet’s network infrastructure.\n\nEnd-to-end QoS can be impacted as a result of this dynamic selecting, since\n\nthe travel speed of data packets is susceptible to conditions like network\n\ncongestion and is therefore non-uniform.\n\nIT solutions need to be assessed against business requirements that are\n\naffected by network bandwidth and latency, which are inherent to cloud\n\ninterconnection. Bandwidth is critical for applications that require\n\nsubstantial amounts of data to be transferred to and from the cloud, while\n\nlatency is critical for applications with a business requirement of swift\n\nresponse times.\n\nWireless and Cellular\n\nCloud-based solutions that need to be accessible anywhere from any device,\n\nespecially those that are targeted towards mobile clients and consumers,\n\nneed to be accessible via wireless and cellular communication links. For\n\nexample, mobile edge computing (MEC), an enabling technology for the\n\nInternet of Vehicles (IoV), offers prospective solutions for sharing",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "processing capabilities across vehicles as well as other readily available\n\nresources.\n\nThe autonomous vehicular edge (AVE) is a distributed vehicular edge\n\ncomputing technology which enables sharing of nearby cars' available\n\nresources via vehicle-to-vehicle (V2V) communications. AVE is a principle\n\nthat can be applied to a broader online solution known as the hybrid\n\nvehicular edge cloud (HVC), which enables effective sharing of all\n\nobtainable computing resources, including roadside units (RSUs) and the\n\ncloud via multiaccess networks.\n\nThese are all examples of how wireless and cellular networks can be\n\nadapted or evolved to constitute valid internetworking components of\n\ncloud-based solutions by overcoming many of the natural bandwidth and\n\nlatency restrictions of wireless and cellular technologies.\n\nCloud Carrier and Cloud Provider Selection\n\nThe service levels of Internet connections between cloud consumers and\n\ncloud providers are determined by their ISPs, which are usually different\n\nand therefore include multiple ISP networks in their paths. QoS\n\nmanagement across multiple ISPs is difficult to achieve in practice,\n\nrequiring collaboration of the cloud carriers on both sides to ensure that\n\ntheir end-to-end service levels are sufficient for business requirements.",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Cloud consumers and cloud providers may need to use multiple cloud\n\ncarriers in order to achieve the necessary level of connectivity and\n\nreliability for their cloud applications, resulting in additional costs. Cloud\n\nadoption can therefore be easier for applications with more relaxed latency\n\nand bandwidth requirements.\n\n5.2 Cloud Data Center Technology\n\nGrouping IT resources in close proximity with one another, rather than\n\nhaving them geographically dispersed, allows for power sharing, higher\n\nefficiency in shared IT resource usage, and improved accessibility for IT\n\npersonnel. These are the advantages that naturally popularized the data\n\ncenter concept. Modern data centers exist as specialized IT infrastructure\n\nused to house centralized IT resources, such as servers, databases,\n\nnetworking and telecommunication devices, and software systems. Data\n\ncenters for cloud providers often require additional technologies.\n\nData centers are typically comprised of the following technologies and\n\ncomponents:\n\nVirtualization\n\nData centers consist of both physical and virtualized IT resources. The\n\nphysical IT resource layer refers to the facility infrastructure that houses\n\ncomputing/networking systems and equipment, together with hardware\n\nsystems and their operating systems (Figure 5.7). The resource abstraction",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "and control of the virtualization layer is comprised of operational and\n\nmanagement tools that are often based on virtualization platforms that\n\nabstract the physical computing and networking IT resources as virtualized\n\ncomponents that are easier to allocate, operate, release, monitor, and\n\ncontrol.",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Figure 5.7\n\nThe common components of a data center working together to provide\n\nvirtualized IT resources supported by physical IT resources.\n\nVirtualization components are discussed separately in the upcoming\n\nModern Virtualization section.\n\nStandardization and Modularity\n\nData centers are built upon standardized commodity hardware and designed\n\nwith modular architectures, aggregating multiple identical building blocks\n\nof facility infrastructure and equipment to support scalability, growth, and\n\nspeedy hardware replacements. Modularity and standardization are key\n\nrequirements for reducing investment and operational costs as they enable\n\neconomies of scale for the procurement, acquisition, deployment, operation,\n\nand maintenance processes.\n\nCommon virtualization strategies and the constantly improving capacity\n\nand performance of physical devices both favor IT resource consolidation,\n\nsince fewer physical components are needed to support complex\n\nconfigurations. Consolidated IT resources can serve different systems and\n\nbe shared among different cloud consumers.",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Autonomic Computing\n\nAutonomic computing is the ability of a system to be self-managing, which\n\nmeans that it is expected to react to external input without the need for\n\nhuman intervention. Using autonomic computing, clouds can be equipped\n\nto manage certain tasks themselves, without human involvement.\n\nThe common features of self-management can include:\n\nSelf-configuration, by which cloud services can configure themselves\n\nautomatically in response to established policies, avoiding manual\n\nintervention from cloud resource administrators. It also involves the\n\nautomatic configuration of new cloud resources when provisioned.\n\nSelf-optimization, by which cloud resources continuously strive to improve\n\ntheir performance indicators by modifying their configuration parameters at\n\nruntime, like scaling up or out dynamically.\n\nSelf-healing, by which cloud services can recover from hardware or\n\nsoftware failure, having detected and diagnosed issues automatically\n\nbeforehand.\n\nSelf-protecting, by which cloud computing platforms are able to defend\n\nthemselves from malicious attacks or cascading failure conditions. This is\n\npossible due to their ability to predict potential problem situations based on",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "the analysis of logs and diagnostics, in which data science technologies are\n\ntypically involved.\n\nRemote Operation and Management\n\nMost of the operational and administrative tasks of IT resources in data\n\ncenters are commanded through the network’s remote consoles and\n\nmanagement systems. Technical personnel are not required, and many\n\ntimes, not allowed, to visit the dedicated rooms that house servers, except to\n\nperform highly specific tasks, such as equipment handling and cabling or\n\nhardware-level installation and maintenance.\n\nHigh Availability\n\nSince any form of data center outage significantly impacts business\n\ncontinuity for the organizations that use their services, data centers are\n\ndesigned to operate with increasingly higher levels of redundancy to sustain\n\navailability. Data centers usually have redundant, uninterruptable power\n\nsupplies, cabling, and environmental control subsystems in anticipation of\n\nsystem failure, along with communication links and clustered hardware for\n\nload balancing.\n\nSecurity-Aware Design, Operation, and Management\n\nRequirements for security, such as physical and logical access controls and\n\ndata recovery strategies, need to be thorough and comprehensive for data",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "centers, since they are centralized structures that store and process business\n\ndata.\n\nDue to the sometimes-prohibitive nature of building and operating on-\n\npremise data centers, outsourcing data center-based IT resources has been a\n\ncommon industry practice for decades. However, the outsourcing models\n\noften required long-term consumer commitment and usually could not\n\nprovide elasticity, issues that a typical cloud can address via inherent\n\nfeatures, such as ubiquitous access, on-demand provisioning, rapid\n\nelasticity, and pay-per-use.\n\nFacilities\n\nData center facilities are custom-designed locations that are outfitted with\n\nspecialized computing, storage, and network equipment. These facilities\n\nhave several functional layout areas, as well as various power supplies,\n\ncabling, and environmental control stations that regulate heating,\n\nventilation, air conditioning, fire protection, and other related subsystems.\n\nThe site and layout of a given data center facility are typically demarcated\n\ninto segregated spaces. Appendix D provides a breakdown of the common\n\nrooms and utilities found in data centers.",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Computing Hardware\n\nMuch of the heavy processing in data centers is often executed by\n\nstandardized commodity servers that have substantial computing power and\n\nstorage capacity. Several computing hardware technologies are integrated\n\ninto these modular servers, such as:\n\nrackmount form factor server design composed of standardized racks with\n\ninterconnects for power, network, and internal cooling\n\nsupport for different hardware processing architectures, such as x86-32bits,\n\nx86-64, and RISC\n\na power-efficient multi-core CPU architecture that houses hundreds of\n\nprocessing cores in a space as small as a single unit of standardized racks\n\nredundant and hot-swappable components, such as hard disks, power\n\nsupplies, network interfaces, and storage controller cards\n\nComputing architectures such as blade server technologies use rack-\n\nembedded physical interconnections (blade enclosures), fabrics (switches),\n\nand shared power supply units and cooling fans. The interconnections\n\nenhance inter-component networking and management while optimizing\n\nphysical space and power. These systems typically support individual server\n\nhot-swapping, scaling, replacement, and maintenance, which benefits the\n\ndeployment of fault-tolerant systems that are based on computer clusters.",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Contemporary computing hardware platforms generally support industry-\n\nstandard and proprietary operational and management software systems that\n\nconfigure, monitor, and control hardware IT resources from remote\n\nmanagement consoles. With a properly established management console, a\n\nsingle operator can oversee hundreds to thousands of physical servers,\n\nvirtual servers, and other IT resources.\n\nStorage Hardware\n\nData centers have specialized storage systems that maintain enormous\n\namounts of digital information in order to fulfill considerable storage\n\ncapacity needs. These storage systems are containers housing numerous\n\nhard disks that are organized into arrays.\n\nStorage systems usually involve the following technologies:\n\nHard Disk Arrays – These arrays inherently divide and replicate data among\n\nmultiple physical drives, and increase performance and redundancy by\n\nincluding spare disks. This technology is often implemented using\n\nredundant arrays of independent disks (RAID) schemes, which are typically\n\nrealized through hardware disk array controllers.\n\nI/O Caching – This is generally performed through hard disk array\n\ncontrollers, which enhance disk access times and performance by data\n\ncaching.",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Hot-Swappable Hard Disks – These can be safely removed from arrays\n\nwithout requiring prior powering down.\n\nStorage Virtualization – This is realized through the use of virtualized hard\n\ndisks and storage sharing.\n\nFast Data Replication Mechanisms – These include snapshotting, which is\n\nsaving a virtual machine’s memory into a hypervisor-readable file for future\n\nreloading, and volume cloning, which is copying virtual or physical hard\n\ndisk volumes and partitions.\n\nStorage systems encompass tertiary redundancies, such as robotized tape\n\nlibraries, which are used as backup and recovery systems that typically rely\n\non removable media. This type of system can exist as a networked IT\n\nresource or direct-attached storage (DAS), in which a storage system is\n\ndirectly connected to the computing IT resource using a host bus adapter\n\n(HBA). In the former case, the storage system is connected to one or more\n\nIT resources through a network.\n\nNetworked storage devices usually fall into one of the following categories:\n\nStorage Area Network (SAN) – Physical data storage media are connected\n\nthrough a dedicated network and provide block-level data storage access\n\nusing industry standard protocols, such as the Small Computer System\n\nInterface (SCSI).",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Network-Attached Storage (NAS) – Hard drive arrays are contained and\n\nmanaged by this dedicated device, which connects through a network and\n\nfacilitates access to data using file-centric data access protocols like the\n\nNetwork File System (NFS) or Server Message Block (SMB).\n\nNAS, SAN, and other more advanced storage system options provide fault\n\ntolerance in many components through controller redundancy, cooling\n\nredundancy, and hard disk arrays that use RAID storage technology.\n\nNetwork Hardware\n\nData centers require extensive network hardware in order to enable multiple\n\nlevels of connectivity. For a simplified version of networking infrastructure,\n\nthe data center is broken down into five network subsystems, followed by a\n\nsummary of the most common elements used for their implementation.\n\nCarrier and External Networks Interconnection\n\nA subsystem related to the internetworking infrastructure, this\n\ninterconnection is usually comprised of backbone routers that provide\n\nrouting between external WAN connections and the data center’s LAN, as\n\nwell as perimeter network security devices such as firewalls and VPN\n\ngateways.",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Web-Tier Load Balancing and Acceleration\n\nThis subsystem comprises Web acceleration devices, such as XML pre-\n\nprocessors, encryption/decryption appliances, and layer 7 switching devices\n\nthat perform content-aware routing.\n\nLAN Fabric\n\nThe LAN fabric constitutes the internal LAN and provides high-\n\nperformance and redundant connectivity for all of the data center’s\n\nnetwork-enabled IT resources. It is often implemented with multiple\n\nnetwork switches that facilitate network communications and operate at\n\nspeeds of up to ten gigabits per second. These advanced network switches\n\ncan also perform several virtualization functions, such as LAN segregation\n\ninto VLANs, link aggregation, controlled routing between networks, load\n\nbalancing, and failover.\n\nSAN Fabric\n\nRelated to the implementation of storage area networks (SANs) that provide\n\nconnectivity between servers and storage systems, the SAN fabric is usually\n\nimplemented with Fibre Channel (FC), Fibre Channel over Ethernet\n\n(FCoE), and InfiniBand network switches.",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "NAS Gateways\n\nThis subsystem supplies attachment points for NAS-based storage devices\n\nand implements protocol conversion hardware that facilitates data\n\ntransmission between SAN and NAS devices.\n\nData center network technologies have operational requirements for\n\nscalability and high availability that are fulfilled by employing redundant\n\nand/or fault-tolerant configurations. These five network subsystems\n\nimprove data center redundancy and reliability to ensure that they have\n\nenough IT resources to maintain a certain level of service even in the face\n\nof multiple failures.\n\nUltra-high-speed network optical links can be used to aggregate individual\n\ngigabit-per-second channels into single optical fibers using multiplexing\n\ntechnologies like dense wavelength-division multiplexing (DWDM).\n\nSpread over multiple locations and used to interconnect server farms,\n\nstorage systems, and replicated data centers, optical links improve transfer\n\nspeeds and resiliency.\n\nServerless Environments\n\nA serverless environment consists of technologies that automatically\n\nprovide runtime resources for applications that can be deployed without the\n\nneed to set up the underlying resources required for them to run. The\n\ndeployed logic still runs on servers, whether physical, virtual, containerized,",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "or otherwise, but service administrators do not need be concerned with\n\ncapacity planning, management, resiliency, or elasticity configurations,\n\nwhich are aspects taken care of by serverless environment.\n\nServerless technologies include automation, virtualization, infrastructure\n\nand software deployment and management, Infrastructure-as-Code, and\n\nContinuous Deployment, all encompassed in a highly customized cloud\n\nservice that is allows developers to simply upload their code and an\n\naccompanying description of its runtime requirements in a language\n\nspecific to each cloud provider. The serverless environment then takes over\n\nfrom there.\n\nA serverless environment is most commonly provided and operated by a\n\npublic cloud provider that relies either on container engines or virtual\n\nmachines to isolate the runtime of one application from another. The\n\nruntime details are hidden from the cloud consumer, and the cloud provider\n\nis responsible for managing the low-level infrastructure, including operating\n\nsystems, virtual machines, and containers.\n\nResources required by programs deployed using these serverless\n\ntechnologies are billed by cloud providers only for the time that the\n\nprograms run. When the program is not running, it does not generate any\n\ncost. This can be considered one of the most important advantages of\n\nserverless technologies, along with the ease of use provided to development",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "teams by the automation of the entire deployment process all the way into\n\nproduction.\n\nNoSQL Clustering\n\nNoSQL (short for Not only SQL) refers to technologies used to develop\n\nnext-generation non-relational databases that are highly scalable and fault\n\ntolerant. These technologies achieve high levels of scalability and fault\n\ntolerance because they are designed as clusters of servers that act as a single\n\ndatabase or storage entity. This is known as NoSQL clustering.\n\nA cluster is a centrally managed group of nodes connected together via a\n\nnetwork to process tasks in parallel, where each node is responsible for a\n\nsub-task of a larger problem (Figure 5.8). Clusters enable distributed data\n\nprocessing. Ideally, a cluster comprises low-cost commodity nodes that\n\ncollectively provide increased processing capacity with inherent\n\nredundancy and fault tolerance, as it consists of physically separate nodes.",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Figure 5.8\n\nA cluster can be used as a deployment environment for various types of\n\ncloud-based solutions, including NoSQL databases.\n\nClusters are highly scalable, supporting horizontal scaling with linear\n\nperformance gains. They provide an ideal deployment environment for a",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "processing engine as large datasets can be divided into smaller datasets and\n\nthen processed in parallel in a distributed manner.\n\nClusters are a fundamental resource provided by cloud computing\n\nplatforms. Clustering technology is used to provide big data platform-\n\nrelated services, advanced container management environments,\n\ndevelopment of applications that scale automatically, and deployment\n\nenvironments (such as PaaS), among others.\n\nNoSQL clustering provides storage devices that are scalable, available,\n\nfault-tolerant, and very fast for read/write operations. However, they do not\n\nprovide the same transaction and consistency support as exhibited by\n\nrelational database management systems.\n\nBelow are some of the principal features of NoSQL storage devices:\n\nSchemaless Data Model – Data can exist in its raw form.\n\nScale Out Rather Than Scale Up – More nodes are added as required rather\n\nthan replacing an existing one with a better, higher performance node.\n\nHighly Available – Built on cluster-based technologies providing fault\n\ntolerance out of box.\n\nLower Operational Costs – Built on open-source platforms with no\n\nlicensing costs and can be deployed on commodity hardware.",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Eventual Consistency – Reads across multiple nodes may not be consistent\n\nimmediately after a write. However, all nodes will eventually be in a\n\nconsistent state.\n\nBASE, not ACID – BASE compliance requires a database to maintain high\n\navailability in the event of network or node failure, while not requiring the\n\ndatabase to be in a consistent state whenever an update occurs. The\n\ndatabase can be in a soft or inconsistent state until it eventually attains\n\nconsistency.\n\nAPI-Driven Data Access – Data access is generally supported via API-based\n\nqueries, including RESTful APIs, whereas some implementations may also\n\nprovide SQL-like query capability.\n\nAuto Sharding and Replication – To support horizontal scaling and provide\n\nhigh availability, a NoSQL storage device automatically employs sharding\n\nand replication techniques where the dataset is partitioned horizontally and\n\nthen copied over to multiple nodes.\n\nIntegrated Caching – Removes the need for a third-party distributed\n\ncaching layer, such as Memcached.\n\nDistributed Query Support – NoSQL storage devices maintain consistent\n\nquery behavior across multiple shards.",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Polyglot Persistence – The use of NoSQL device storage does not mandate\n\nretiring traditional RDBMSs. Both types of storage can be used at the same\n\ntime, thereby supporting polyglot persistence, which is an approach of\n\npersisting data using different types of storage technologies. This is good\n\nfor developing systems requiring structured as well as semi-structured or\n\nunstructured data.\n\nAggregate-Focused – Unlike relational databases that are most effective\n\nwith fully normalized data, NoSQL storage devices store de-normalized\n\naggregated data (an entity containing merged, often nested, data for an\n\nobject), thereby eliminating the need for joins and mapping between\n\napplication objects and the data stored in the database.\n\nOther Considerations\n\nIT hardware is subject to rapid technological obsolescence, with lifecycles\n\nthat typically last between five to seven years. The on-going need to replace\n\nequipment frequently results in a mix of hardware whose heterogeneity can\n\ncomplicate the entire data center’s operations and management, although\n\nthis can be partially mitigated through virtualization.\n\nSecurity is another major issue when considering the role of the data center\n\nand the vast quantities of data contained within its doors. Even with\n\nextensive security precautions in place, housing data exclusively at one data\n\ncenter facility means much more can be compromised by a successful",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "security incursion than if data was distributed across individual unlinked\n\ncomponents.\n\n5.3 Modern Virtualization\n\nModern virtualization technology is a foundation of contemporary cloud\n\nplatforms. It provides a variety of virtualization types and technology layers\n\nthat are introduced in this section.\n\nHardware Independence\n\nThe installation of an operating system’s configuration and application\n\nsoftware in a unique IT hardware platform results in many software-\n\nhardware dependencies. In a non-virtualized environment, the operating\n\nsystem is configured for specific hardware models and requires\n\nreconfiguration if these IT resources need to be modified.\n\nVirtualization is a conversion process that translates unique IT hardware\n\ninto emulated and standardized software-based copies. Through hardware\n\nindependence, virtual servers can easily be moved to another virtualization\n\nhost, automatically resolving multiple hardware-software incompatibility\n\nissues. As a result, cloning and manipulating virtual IT resources is much\n\neasier than duplicating physical hardware. The architectural models\n\nexplored in Part III of this book provide numerous examples of this.",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Server Consolidation\n\nThe coordination function that is provided by the virtualization software\n\nallows multiple virtual servers to be simultaneously created in the same\n\nvirtualization host. Virtualization technology enables different virtual\n\nservers to share one physical server. This process is called server\n\nconsolidation and is commonly used to increase hardware utilization, load\n\nbalancing, and optimization of available IT resources. The resulting\n\nflexibility is such that different virtual servers can run different guest\n\noperating systems on the same host.\n\nThis fundamental capability directly supports common cloud features, such\n\nas on-demand usage, resource pooling, elasticity, scalability, and resiliency.\n\nResource Replication\n\nVirtual servers are created as virtual disk images that contain binary file\n\ncopies of hard disk content. These virtual disk images are accessible to the\n\nhost’s operating system, meaning simple file operations, such as copy,\n\nmove, and paste, can be used to replicate, migrate, and back up the virtual\n\nserver. This ease of manipulation and replication is one of the most salient\n\nfeatures of virtualization technology as it enables:\n\nThe creation of standardized virtual machine images commonly configured\n\nto include virtual hardware capabilities, guest operating systems, and",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "additional application software, for pre-packaging in virtual disk images in\n\nsupport of instantaneous deployment.\n\nIncreased agility in the migration and deployment of a virtual machine’s\n\nnew instances by being able to rapidly scale out and up.\n\nThe ability to roll back, which is the instantaneous creation of VM\n\nsnapshots by saving the state of the virtual server’s memory and hard disk\n\nimage to a host-based file. (Operators can easily revert to these snapshots\n\nand restore the virtual machine to its prior state.)\n\nThe support of business continuity with efficient backup and restoration\n\nprocedures, as well as the creation of multiple instances of critical IT\n\nresources and applications.\n\nOperating System-Based Virtualization\n\nOperating system-based virtualization is the installation of virtualization\n\nsoftware in a pre-existing operating system, which is called the host\n\noperating system (Figure 5.9). For example, a user whose workstation is\n\ninstalled with a specific version of Windows wants to generate virtual\n\nservers and installs virtualization software into the host operating system\n\nlike any other program. This user needs to use this application to generate\n\nand operate one or more virtual servers. The user needs to use virtualization\n\nsoftware to enable direct access to any of the generated virtual servers.\n\nSince the host operating system can provide hardware devices with the",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "necessary support, operating system virtualization can rectify hardware\n\ncompatibility issues even if the hardware driver is not available to the\n\nvirtualization software.",
      "content_length": 171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Figure 5.9\n\nThe different logical layers of operating system-based virtualization, in\n\nwhich the VM is first installed into a full host operating system and\n\nsubsequently used to generate virtual machines.\n\nHardware independence that is enabled by virtualization allows hardware\n\nIT resources to be more flexibly used. For example, consider a scenario in\n\nwhich the host operating system has the software necessary for controlling\n\nfive network adapters that are available to the physical computer. The\n\nvirtualization software can make the five network adapters available to the\n\nvirtual server, even if the virtualized operating system is incapable of\n\nphysically housing five network adapters.\n\nVirtualization software translates hardware IT resources that require unique\n\nsoftware for operation into virtualized IT resources that are compatible with\n\na range of operating systems. Since the host operating system is a complete\n\noperating system in itself, many operating system-based services that are\n\navailable as administration tools can be used to manage the physical host.\n\nExamples of such services include:\n\nBackup and Recovery\n\nIntegration to Directory Services\n\nSecurity Management",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Operating system-based virtualization can introduce demands and issues\n\nrelated to performance overhead such as:\n\nThe host operating system consumes CPU, memory, and other hardware IT\n\nresources.\n\nHardware-related calls from guest operating systems need to traverse\n\nseveral layers to and from the hardware, which decreases overall\n\nperformance.\n\nLicenses are usually required for host operating systems, in addition to\n\nindividual licenses for each of their guest operating systems.\n\nA concern with operating system-based virtualization is the processing\n\noverhead required to run the virtualization software and host operating\n\nsystems. Implementing a virtualization layer will negatively affect overall\n\nsystem performance. Estimating, monitoring, and managing the resulting\n\nimpact can be challenging because it requires expertise in system\n\nworkloads, software and hardware environments, and sophisticated\n\nmonitoring tools.\n\nHardware-Based Virtualization\n\nThis option represents the installation of virtualization software directly on\n\nthe physical host hardware so as to bypass the host operating system, which\n\nis presumably engaged with operating system-based virtualization (Figure",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "5.10). Allowing the virtual servers to interact with hardware without\n\nrequiring intermediary action from the host operating system generally\n\nmakes hardware-based virtualization more efficient.\n\nFigure 5.10",
      "content_length": 207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "The different logical layers of hardware-based virtualization, which does\n\nnot require another host operating system.\n\nVirtualization software is typically referred to as a hypervisor for this type\n\nof processing. A hypervisor has a simple user-interface that requires a\n\nnegligible amount of storage space. It exists as a thin layer of software that\n\nhandles hardware management functions to establish a virtualization\n\nmanagement layer. Device drivers and system services are optimized for the\n\nprovisioning of virtual servers, although many standard operating system\n\nfunctions are not implemented. This type of virtualization system is\n\nessentially used to optimize performance overhead inherent to the\n\ncoordination that enables multiple virtual servers to interact with the same\n\nhardware platform.\n\nOne of the main issues of hardware-based virtualization concerns\n\ncompatibility with hardware devices. The virtualization layer is designed to\n\ncommunicate directly with the host hardware, meaning all of the associated\n\ndevice drivers and support software need to be compatible with the\n\nhypervisor. Hardware device drivers may not be as available to hypervisor\n\nplatforms as they are to operating systems. Host management and\n\nadministration features may further not include the range of advanced\n\nfunctions that are common to operating systems.",
      "content_length": 1352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Containers and Application-Based Virtualization\n\nApplication virtualization is a method of creating and using applications\n\nwithout operating system dependency. For many types of applications and\n\nservices, containers provide a portable, compatible, and highly manageable\n\ndeployment environment that allows independent and autonomous software\n\nprograms and systems to run on almost any platform, in accordance with\n\nthe definition of application virtualization.\n\nSoftware running in containers can be deployed almost anywhere, always\n\nproviding the same functionality independently of the runtime environment\n\nin which it is deployed. Containers are suitable for application-based\n\nvirtualization because applications running in containers can run on any\n\nplatform, regardless of the underlying operating system or hardware\n\narchitecture, as long as a compatible containerization engine is running on\n\nthat platform, as depicted in Figure 5.11.",
      "content_length": 945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Figure 5.11\n\nA virtualized application running in a container can be deployed anywhere\n\nits corresponding containerization engine is installed, regardless of\n\nunderlying hardware or operating system architectures.\n\nContainerization has become a fundamental infrastructure technology in\n\ncontemporary cloud environments and is covered in detail in Chapter 6.\n\nVirtualization Management\n\nMany administrative tasks can be performed more easily using virtual\n\nservers as opposed to using their physical counterparts. Modern",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "virtualization software provides several advanced management functions\n\nthat can automate administration tasks and reduce the overall operational\n\nburden on virtualized IT resources.\n\nVirtualized IT resource management is often supported by virtualization\n\ninfrastructure management (VIM) tools that collectively manage virtual IT\n\nresources and rely on a centralized management module, otherwise known\n\nas a controller, that runs on a dedicated computer. VIMs are commonly\n\nencompassed by the resource management system mechanism described in\n\nChapter 12.\n\nOther Considerations\n\nPerformance Overhead – Virtualization may not be ideal for complex\n\nsystems that have high workloads with little use for resource sharing and\n\nreplication. A poorly formulated virtualization plan can result in excessive\n\nperformance overhead. A common strategy used to rectify the overhead\n\nissue is a technique called para-virtualization, which presents a software\n\ninterface to the virtual machines that is not identical to that of the\n\nunderlying hardware. The software interface has instead been modified to\n\nreduce the guest operating system’s processing overhead, which is more\n\ndifficult to manage. A major drawback of this approach is the need to adapt\n\nthe guest operating system to the para-virtualization API, which can impair\n\nthe use of standard guest operating systems while decreasing solution\n\nportability.",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Special Hardware Compatibility – Many hardware vendors that distribute\n\nspecialized hardware may not have device driver versions that are\n\ncompatible with virtualization software. Conversely, the software itself may\n\nbe incompatible with recently released hardware versions. These types of\n\nincompatibility issues can be resolved using established commodity\n\nhardware platforms and mature virtualization software products. Container\n\nengines are not affected by this consideration because they run on top of the\n\nhost operating system, which abstracts any potential hardware\n\ncompatibility, making containerization a highly portable type of\n\nvirtualization technology.\n\nPortability – The programmatic and management interfaces that establish\n\nadministration environments for a virtualization program to operate with\n\nvarious virtualization solutions can introduce portability gaps due to\n\nincompatibilities. Initiatives such as the Open Virtualization Format (OVF)\n\nfor the standardization of virtual disk image formats are dedicated to\n\nalleviating this concern. Furthermore, containerization provides an\n\nalternative type of virtualization technology with very high levels of\n\nportability.\n\n5.4 Multitenant Technology\n\nThe multitenant application design was created to enable multiple users\n\n(tenants) to access the same application logic simultaneously. Each tenant\n\nhas its own view of the application that it uses, administers, and customizes",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "as a dedicated instance of the software while remaining unaware of other\n\ntenants that are using the same application.\n\nMultitenant applications ensure that tenants do not have access to data and\n\nconfiguration information that is not their own. Tenants can individually\n\ncustomize features of the application, such as:\n\nUser Interface – Tenants can define a specialized “look and feel” for their\n\napplication interface.\n\nBusiness Process – Tenants can customize the rules, logic, and workflows\n\nof the business processes that are implemented in the application.\n\nData Model – Tenants can extend the data schema of the application to\n\ninclude, exclude, or rename fields in the application data structures.\n\nAccess Control – Tenants can independently control the access rights for\n\nusers and groups.\n\nMultitenant application architecture is often significantly more complex\n\nthan that of single-tenant applications. Multitenant applications need to\n\nsupport the sharing of various artifacts by multiple users (including portals,\n\ndata schemas, middleware, and databases), while maintaining security\n\nlevels that segregate individual tenant operational environments.\n\nCommon characteristics of multitenant applications include:",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Usage Isolation – The usage behavior of one tenant does not affect the\n\napplication availability and performance of other tenants.\n\nData Security – Tenants cannot access data that belongs to other tenants.\n\nRecovery – Backup and restore procedures are separately executed for the\n\ndata of each tenant.\n\nApplication Upgrades – Tenants are not negatively affected by the\n\nsynchronous upgrading of shared software artifacts.\n\nScalability – The application can scale to accommodate increases in usage\n\nby existing tenants and/or increases in the number of tenants.\n\nMetered Usage – Tenants are charged only for the application processing\n\nand features that are actually consumed.\n\nData Tier Isolation – Tenants can have individual databases, tables, and/or\n\nschemas isolated from other tenants. Alternatively, databases, tables, and/or\n\nschemas can be designed to be intentionally shared by tenants.\n\nA multitenant application that is being concurrently used by two different\n\ntenants is illustrated in Figure 5.12. This type of application is typical with\n\nSaaS implementations.",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Figure 5.12",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "A multitenant application that is serving multiple cloud service consumers\n\nsimultaneously.\n\nMultitenancy vs. Virtualization\n\nMultitenancy is sometimes mistaken for virtualization\n\nbecause the concept of multiple tenants is similar to the\n\nconcept of virtualized instances.\n\nThe differences lie in what is multiplied within a physical\n\nserver acting as a host:\n\nWith virtualization: Multiple virtual copies of the server\n\nenvironment can be hosted by a single physical server. Each\n\ncopy can be provided to different users, can be configured\n\nindependently, and can contain its own operating systems\n\nand applications.\n\nWith multitenancy: A physical or virtual server hosting an\n\napplication is designed to allow usage by multiple different\n\nusers. Each user feels as though they have exclusive usage\n\nof the application.\n\n5.5 Service Technology and Service APIs",
      "content_length": 862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "The field of service technology is a keystone foundation of cloud\n\ncomputing that formed the basis of the “as-a-service” cloud delivery\n\nmodels. Several prominent service technologies that are used to realize and\n\nbuild upon cloud-based environments are described in this section.\n\nAbout Web-based Services\n\nReliant on the use of standardized protocols, Web-based\n\nservices are self-contained units of logic that support\n\ninteroperable machine-to-machine interaction over a\n\nnetwork. These services are generally designed to\n\ncommunicate via non-proprietary technologies in\n\naccordance with industry standards and conventions.\n\nBecause their sole function is to process data between\n\ncomputers, these services expose APIs and do not have user\n\ninterfaces. Web services and REST services represent two\n\ncommon forms of Web-based services.\n\nREST Services\n\nREST services are designed according to a set of constraints that shape the\n\nservice architecture to emulate the properties of the World Wide Web,\n\nresulting in service implementations that rely on the use of core Web\n\ntechnologies.",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "REST services do not have individual technical interfaces but instead share\n\na common technical interface that is known as the uniform contract, which\n\nis typically established via the use of HTTP methods.\n\nThe six REST design constraints are:\n\nClient-Server\n\nStateless\n\nCache\n\nInterface/Uniform Contract\n\nLayered System\n\nCode-On-Demand\n\nNote\n\nTo learn more about REST services read SOA with REST:\n\nPrinciples, Patterns & Constraints for Building Enterprise\n\nSolutions with REST from the Pearson Digital Enterprise\n\nSeries from Thomas Erl.",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Web Services\n\nAlso commonly prefixed with “SOAP-based,” Web services represent an\n\nestablished and common medium for sophisticated, Web-based service\n\nlogic. Along with XML, the core technologies behind Web services are\n\nrepresented by the following industry standards:\n\nWeb Service Description Language (WSDL) – This markup language is\n\nused to create a WSDL definition that defines the application programming\n\ninterface (API) of a Web service, including its individual operations\n\n(functions) and each operation’s input and output messages.\n\nXML Schema Definition Language (XML Schema) – Messages exchanged\n\nby Web services must be expressed using XML. XML schemas are created\n\nto define the data structure of the XML-based input and output messages\n\nexchanged by Web services. XML schemas can be directly linked to or\n\nembedded within WSDL definitions.\n\nSOAP – Formerly known as the Simple Object Access Protocol, this\n\nstandard defines a common messaging format used for request and response\n\nmessages exchanged by Web services. SOAP messages are comprised of\n\nbody and header sections. The former houses the main message content and\n\nthe latter is used to contain metadata that can be processed at runtime.\n\nUniversal Description, Discovery, and Integration (UDDI) – This standard\n\nregulates service registries in which WSDL definitions can be published as",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "part of a service catalog for discovery purposes.\n\nThese four technologies collectively form the first generation of Web\n\nservice technologies (Figure 5.13). A comprehensive set of second-\n\ngeneration Web service technologies (commonly referred to as WS-*) has\n\nbeen developed to address various additional functional areas, such as\n\nsecurity, reliability, transactions, routing, and business process automation.\n\nNote\n\nTo learn more about Web service technologies, read Web\n\nService Contract Design & Versioning for SOA from the\n\nPearson Digital Enterprise Series from Thomas Erl. This\n\ntitle covers first and second-generation Web service\n\nstandards in technical detail.",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Figure 5.13\n\nAn overview of how first-generation Web service technologies commonly\n\nrelate to each other.",
      "content_length": 105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Service Agents\n\nService agents are event-driven programs designed to intercept messages at\n\nruntime. There are active and passive service agents, both of which are\n\ncommon in cloud environments. Active service agents perform an action\n\nupon intercepting and reading the contents of a message. The action\n\ntypically requires making changes to the message contents (most commonly\n\nmessage header data and less commonly the body content) or changes to the\n\nmessage path itself. Passive service agents, on the other hand, do not\n\nchange message contents. Instead, they read the message and may then\n\ncapture certain parts of its contents, usually for monitoring, logging, or\n\nreporting purposes.\n\nCloud-based environments rely heavily on the use of system-level and\n\ncustom service agents to perform much of the runtime monitoring and\n\nmeasuring required to ensure that features, such as elastic scaling and pay-\n\nfor-use billing, can be carried out instantaneously.\n\nSeveral of the mechanisms described in Part II of this book exist as, or rely\n\non the use of, service agents.\n\nService Middleware\n\nFalling under the umbrella of service technology is the large market of\n\nmiddleware platforms that evolved from messaging-oriented middleware\n\n(MOM) platforms used primarily to facilitate integration, to sophisticated",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "service middleware platforms designed to accommodate complex service\n\ncompositions.\n\nThe two most common types of middleware platforms relevant to services\n\ncomputing are the enterprise service bus (ESB) and the orchestration\n\nplatform. The ESB encompasses a range of intermediary processing\n\nfeatures, including service brokerage, routing, and message queuing.\n\nOrchestration environments are designed to host and execute workflow\n\nlogic that drives the runtime composition of services.\n\nBoth forms of service middleware can be deployed and operated within\n\ncloud-based environments.\n\nWeb-based RPC\n\nCloud providers commonly deliver access to the resources that they offer\n\nvia RESTful services. The interaction between RESTful services and their\n\nservice consumers requires considerable amounts of bandwidth in\n\nconversations that require multiple messages to be exchanged through the\n\nnetwork.\n\nTraditional RPC frameworks can overcome some of the performance\n\nchallenges presented by RESTful architectures, but they are bound to\n\ncommunication via TCP/IP, which is a limitation incompatible with Web-\n\nbased application requirements. To address both sets of limitations, a set of\n\ncontemporary protocols was developed that leverage the performance",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "benefits of RPC, while supporting Web-based communication. These\n\ninclude:\n\ngRPC (originally developed by Google)\n\nGraphQL (originally developed by Facebook)\n\nFalcor (originally developed by Netflix)\n\nEach of these protocols was developed by an organization in response to a\n\nneed to overcome limitations with more established protocols.\n\n5.6 Case Study Example\n\nDTGOV has assembled cloud-aware infrastructures in\n\neach of its data centers, which are comprised of the\n\nfollowing components:\n\nTier-3 facility infrastructure, which provides redundant\n\nconfigurations for all of the central subsystems in the\n\ndata center facility layer.\n\nRedundant connections with utility service providers that\n\nhave installed local capacity for power generation and\n\nwater supply that activates in the event of general\n\nfailure.",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "An internetwork that supplies an ultra-high bandwidth\n\ninterconnection between the three data centers through\n\ndedicated links.\n\nRedundant Internet connections in each data center to\n\nmultiple ISPs and the .GOV extranet, which\n\ninterconnects DTGOV with its main government clients.\n\nStandardized hardware of higher aggregated capacity\n\nthat is abstracted by a cloud-aware virtualization\n\nplatform.\n\nPhysical servers are organized on server racks, each of\n\nwhich has two redundant top-of-rack router switches\n\n(layer 3) that are connected to each physical server.\n\nThese router switches are interconnected to LAN core-\n\nswitches that have been configured as a cluster. The\n\ncore-switches connect to routers that supply\n\ninternetworking capabilities and firewalls that provide\n\nnetwork access control capabilities. Figure 5.14\n\nillustrates the physical layout of the server network\n\nconnections inside of the data center.",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Figure 5.14\n\nA view of the server network connections inside the\n\nDTGOV data center.\n\nA separate network that connects the storage systems\n\nand servers is installed with clustered storage area\n\nnetwork (SAN) switches and similar redundant\n\nconnections to various devices (Figure 5.15).",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Figure 5.15\n\nA view of the storage system network connections inside\n\nthe DTGOV data center.\n\nFigure 5.16 illustrates an internetworking architecture\n\nthat is established between every data center pair within\n\nthe DTGOV corporate infrastructure.\n\nAs shown in Figures 5.15 and 5.16, combining\n\ninterconnected physical IT resources with virtualized IT\n\nresources on the physical layer enables the dynamic and\n\nwell-managed configuration and allocation of virtual IT\n\nresources.",
      "content_length": 475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Figure 5.16\n\nThe internetworking setup between two data centers that is similarly\n\nimplemented between every pair of DTGOV data centers. The DTGOV\n\ninternetwork is designed to be an autonomous system (AS) on the Internet,\n\nmeaning the links interconnecting the data centers with the LANs define the\n\nintra-AS routing domain. The interconnections to external ISPs are\n\ncontrolled through inter-AS routing technology, which shapes Internet\n\ntraffic and enables flexible configurations for load-balancing and failover.",
      "content_length": 515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Chapter 6\n\nUnderstanding Containerization\n\n6.1 Origins and Influences\n\n6.2 Fundamental Virtualization and Containerization\n\n6.3 Understanding Containers\n\n6.4 Understanding Container Images\n\n6.5 Multi-Container Types\n\n6.6 Case Study Example\n\nContainerization is a virtualization technology used to deploy and run\n\napplications and services without the need to deploy a virtual server for\n\neach solution. This chapter covers fundamental topics pertaining to\n\nvirtualization and then takes a close look at containerization technology and\n\nthe utilization of containers.\n\nNote",
      "content_length": 572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "This chapter is supplemented with coverage of the Docker\n\nand Kubernetes containerization technologies provided in\n\nAppendix B.\n\n6.1 Origins and Influences\n\nA Brief History\n\nThe concept of containers has been present since the 1970s, when it\n\noriginally referred to a capability used in Unix systems to better segregate\n\napplication code. Early containers provided an isolated environment in\n\nwhich services and applications could operate without interfering with other\n\nprocesses, resulting in an environment similar to a sandbox for testing apps,\n\nservices and other processes.\n\nContainers gained widespread usage decades later due to a slew of Linux\n\ndistributions that released new deployment and management tools.\n\nContainers running on Linux systems were turned into an operating system-\n\nlevel virtualization technique specially designed to enable several isolated\n\nLinux environments to run on a single Linux host. However, while running\n\ncontainers on a Linux platform broadened their usefulness, there remained\n\nkey obstacles to solve, including unified administration, true portability,\n\ncompatibility and control of scale.",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "The introduction of Apache Mesos, Google Borg and Facebook\n\nTupperware, all of which provided varied degrees of container orchestration\n\nand cluster management capabilities, marked a significant advancement in\n\nthe use of containers on Linux systems. These systems enabled the instant\n\ncreation of hundreds of containers, as well as automatic failover and other\n\nmission-critical functionality necessary for container management at scale.\n\nAfter Docker containers were introduced, containerization began becoming\n\npart of the IT mainstream. Docker's prominence led to the innovation of\n\nsophisticated containerization platforms, including Marathon, Kubernetes\n\nand Docker Swarm.\n\nContainerization and Cloud Computing\n\nCloud computing helped popularize virtualization technology, and further\n\nadvances in cloud computing technology helped realize contemporary\n\ncontainerization technology. Containerization is now a fundamental part of\n\ncloud computing infrastructure.\n\nThe use of containers can help support the primary business drivers behind\n\ncloud computing.\n\nThe simplified and flexible deployment architecture established by\n\ncontainerization can directly support the primary Cost Reduction and\n\nBusiness Agility business drivers behind cloud computing (as introduced in\n\nChapter 3) and can further enable cloud-based solutions to be made better\n\nrespond to fluctuating usage requirements.",
      "content_length": 1394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "6.2 Fundamental Virtualization and Containerization\n\nThis section covers the fundamental terms and concepts associated with\n\noperating systems and virtualization technology. It then proceeds to explain\n\nthe basic components of containerization and concludes with a comparison\n\nof virtualization and containerization.\n\nOperating System Basics\n\nAn operating system is software installed on a computer that provides a\n\nrange of programs, tools, libraries and other resources used to manage a\n\ncomputer, as well as programs used to host and support the on-going\n\noperations of applications installed on the operating system. The installation\n\nof an operating system can also include various consumer applications.\n\nThe operating system programs used to support the execution and active\n\noperation of applications are collectively referred to as the runtime (Figure\n\n6.1). Applications themselves may introduce their own runtime software\n\nthat operates on top of an operating system runtime environment.",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Figure 6.1\n\nThe symbol used to represent a runtime.\n\nVirtualization Basics\n\nTo best understand containerization, it is important to first establish some\n\nbasics about virtualization. As has already been explained in Chapter 3,\n\nvirtualization is the technology that enables a physical IT resource to\n\nprovide multiple virtual images of itself so that its underlying processing\n\ncapabilities can be shared by multiple solutions.",
      "content_length": 427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Physical Servers\n\nThe physical IT resource most commonly virtualized is the physical server\n\n(Figure 6.2). A physical server provides an operating system environment\n\nthat can host applications, services and other software programs.\n\nFigure 6.2\n\nThe symbol used to represent a physical server.",
      "content_length": 293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Virtual Servers\n\nWhen utilizing virtualization technology, the operating system hosting\n\nenvironment provided by the physical server can be abstracted into one or\n\nmore virtual servers (Figure 6.3).\n\nFigure 6.3\n\nThe symbol used to represent a virtual server.\n\nEach virtual server can then provide a fresh and dedicated copy (or image)\n\nof the operating system hosting environment, which can be further referred\n\nto as a guest operating system. Each virtual server can provide its",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "virtualized operating system environment to a different set of consumer\n\napplications or services that do not require any knowledge of how the\n\nunderlying physical server exists or operates (Figure 6.4). As consumer\n\nusage demands fluctuate, the physical server can be scaled accordingly.",
      "content_length": 288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Figure 6.4\n\nThree virtual servers that exist on two physical servers.\n\nThe administrator responsible for the physical server can retain\n\nadministrative control of the physical server hardware and its operating\n\nsystem. The administrators responsible for the individual virtual servers are\n\nnot given (nor require) access to the underlying physical server, but they\n\ncan independently control their respective virtual operating system\n\nenvironments.\n\nHypervisors\n\nThe component responsible for creating and running multiple virtual servers\n\nfrom a physical server is the hypervisor (Figures 6.5 and 6.6).\n\nFigure 6.5\n\nThe symbol used to represent the hypervisor.",
      "content_length": 661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Figure 6.6",
      "content_length": 10,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Three virtual servers created and run by a hypervisor that exist on two\n\nphysical servers.\n\nVirtual servers perceive the emulated hardware presented to them by the\n\nhypervisor as real hardware. Each virtual server has its own operating\n\nsystem (also known as a guest operating system) that needs to be deployed\n\ninside the virtual server and managed and maintained as if it were deployed\n\non a physical server.\n\nVirtualization Types\n\nThere are two types of virtualization environments that are primarily\n\ndistinguished by whether the physical server has an operating system\n\ninstalled.\n\nIn a Type 1 virtualization environment, the physical server does not have an\n\noperating system installed. Instead, only the hypervisor is installed on the\n\nphysical server and it is responsible for creating the virtual servers and\n\nproviding them with virtualized operating system environments (Figure\n\n6.7).",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Figure 6.7",
      "content_length": 10,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "The physical server hosts only the hypervisor that creates virtual servers,\n\neach with its own operating system.\n\nIn a Type 2 virtualization environment, the physical server has an operating\n\nsystem installed and may also have a hypervisor installed. In this case, the\n\nphysical server is accessible via its operating system and the hypervisor\n\nremains responsible for creating the virtual servers and providing them with\n\ntheir virtualized operating system environments (Figure 6.8).",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Figure 6.8\n\nThe physical server hosts its own operating system as well as a hypervisor\n\nthat creates virtual servers with their own operating system environments.\n\nContainerization Basics\n\nContainers\n\nA container (Figure 6.9) is a virtualized hosting environment that can be\n\noptimized to provide only the resources required for the software programs\n\nit hosts.\n\nFigure 6.9",
      "content_length": 373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "The symbol on the left is the container icon. The symbol on the right is also\n\nused to represent a container and to show its contents.\n\nContainers have various features and characteristics that are explored in\n\nmore detail in the upcoming Understanding Containers section.\n\nContainer Images\n\nA container image (Figure 6.10) is similar to a pre-defined template that is\n\nused to create deployed containers.\n\nFigure 6.10\n\nThe symbol used to represent a container image.\n\nThe definition and usage of container images is integral to how\n\ncontainerization platforms operate. Further details are provided in the\n\nupcoming Understanding Container Images section.",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Container Engines\n\nThe container engine (Figure 6.11), also referred to as the containerization\n\nengine, is responsible for creating containers based on pre-defined container\n\nimages. The container engine is deployed in a physical or virtual server’s\n\noperating system from where it can abstract the resources required for a\n\ngiven container.\n\nFigure 6.11\n\nThe symbol used to represent the container engine.",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "The container engine is a core part of a containerization platform and is\n\nresponsible for many of its primary processing tasks. Its implementation is\n\norganized into two “planes”, as follows:\n\nManagement Plane — the GUI and command-line tools made available to\n\nenable human administrators to configure and maintain the container engine\n\nenvironment\n\nControl Plane — all remaining container engine functions and features that\n\nthe container engine carries out automatically and in response to settings\n\nand commands issued via the management plane\n\nA given container engine can create multiple containers (Figure 6.12).",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Figure 6.12\n\nA container engine creating two different containers.\n\nPods\n\nA pod, also known as a logical pod container, is a special type of system\n\ncontainer that can be used to host a single container or a group of containers\n\n(Figure 6.13) that have shared storage and/or network resources, and also",
      "content_length": 302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "share the same configuration that determines how the containers are to be\n\nrun.\n\nFigure 6.13",
      "content_length": 92,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "A pod is depicted as a perforated outline showing the containers it is\n\nhosting.\n\nHow pods relate to container deployment is further explored in the\n\nContainers and Pods sub-section of the upcoming Understanding\n\nContainers section.\n\nHosts\n\nA host is the environment in which a container is deployed. A host can be\n\nreferred to as a server or a node. The host provides the operating system\n\nfrom which the container abstracts the resources it needs to support the\n\nprograms it is hosting. Multiple containers can be deployed and run on a\n\nsingle host (Figure 6.14).",
      "content_length": 565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Figure 6.14",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Three containers in a single pod reside on Host A, which is a physical\n\nserver.\n\nDifferent combinations of containers and pods can be deployed on different\n\nhosts (Figure 6.15). However, a single pod cannot span more than one host.\n\nFigure 6.15",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Host A has three active containers in a pod, whereas there are six\n\ncontainers in a pod operating on Host B.\n\nContainers also operate on a host without a pod when the container engine\n\ndeployed does not support pods (Figure 6.16).",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Figure 6.16",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Three containers are deployed on Host A without the involvement of a pod.\n\nHosts commonly exist as physical servers, but a host can also be a virtual\n\nserver. When a container is deployed on a virtual server, it is considered a\n\nform of nested virtualization because one virtualized system is deployed on\n\nanother.\n\nHost Clusters\n\nHost servers can be combined into “clusters” that can collectively establish\n\na pool of readily available processing resources with increased computing\n\ncapacity. Both virtual and physical hosts can be clustered (Figures 6.17 and\n\n6.18). Within clustering environments, host servers are commonly referred\n\nto as nodes.",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Figure 6.17\n\nThe symbol used to represent a physical host cluster.",
      "content_length": 66,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Figure 6.18\n\nThe symbol used to represent a virtual host cluster.\n\nCommon types of host clusters include:\n\nLoad Balanced Cluster — This type of host cluster specializes in\n\ndistributing workloads among hosts to increase resource capacity while\n\npreserving the centralization of resource management. It usually",
      "content_length": 309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "implements a load balancer that is embedded within a cluster management\n\nplatform or set up as a separate resource.\n\nHigh Availability (HA) Cluster — This type of cluster maintains system\n\navailability in the event of multiple host failures. It typically provides\n\nredundant implementations of most or all of the clustered resources and\n\nimplements a failover system that monitors failure conditions and\n\nautomatically redirects workloads away from failed host environments.\n\nScaling Cluster — This type of cluster is used to support both vertical and\n\nhorizontal scaling.\n\nContainerization platforms utilize all of the aforementioned types of host\n\ncluster models in support of high-performance and resiliency requirements,\n\nas well as in relation to optimized deployment capabilities.\n\nHost Networks and Overlay Networks\n\nEach host has its own container engine that is responsible for generating\n\ncontainer images and deploying and running containers on that host.\n\nRelated containers within a host can communicate with each other using a\n\nlocal host network. Related containers and container engines on different\n\nhosts can communicate with each other via an overlay network. Both of\n\nthese types of networks are considered container networks (Figure 6.19).",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Figure 6.19\n\nThe symbol used to represent a container network.\n\nContainer networks can be configured by administrators to support various\n\nscalability and resiliency capabilities and to control which hosted programs\n\ncan access resources outside of the container network, as further explored in\n\nthe Container Networks sub-section in the upcoming Understanding\n\nContainers section.\n\nVirtualization and Containerization\n\nThe primary distinction between a virtual server and a container is that a\n\nvirtual server provides a virtual version of a physical server’s entire",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "operating system, whereas a container only provides the subset of the\n\noperating system resources actually required by the software program (or\n\nprograms) it is hosting. As a result, a container consumes less space and\n\nperforms more efficiently than a virtual server.\n\nContainerization on Physical Servers\n\nWhen deploying containers on a physical server, the containerization\n\nplatform requires no virtualization environment since virtual servers are not\n\nrequired. The underlying physical server has an operating system installed\n\nand the containerization platform can create containers that each only\n\nabstract the subset of the operating system relevant to the software\n\nprograms it hosts (Figure 6.20).",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Figure 6.20\n\nA physical server with an operating system hosts a containerization\n\nplatform that creates containers, each with an environment that has only a",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "subset of the underlying operating system.\n\nContainerization on Virtual Servers\n\nWhen deploying containers on one or more virtual servers, the\n\ncontainerization platform can be implemented on a Type 1 virtualization\n\nenvironment (Figure 6.21) or a Type 2 virtualization environment with a\n\nhypervisor (Figure 6.22). Both types of virtualization environments allow\n\nfor the creation of virtual servers that can host containerization engines.",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Figure 6.21",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "A physical server with no operating system hosts a hypervisor that creates\n\nvirtual servers with operating systems, each of which hosts a\n\ncontainerization platform that can create containers that only have an\n\noperating system subset.",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Figure 6.22",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "A physical server with an operating system hosts a hypervisor that creates\n\nvirtual server environments with their own operating systems. Each virtual\n\nserver hosts a containerization platform that creates containers that host a\n\nsubset of the operating system.\n\nThe motivation behind deploying containers on virtual servers is often\n\nrelated to security vulnerabilities that exist when the physical server has an\n\noperating system installed. As a result, the Type 1 virtualization\n\nenvironment is more common in most production environments. Type 2\n\nvirtualization is typically used in development environments when\n\ncontainerized solutions are being built and tested.\n\nType 2 virtualization can also be used for smaller solutions or for smaller\n\norganizations when the underlying physical server needs an operating\n\nsystem in order to host additional programs and systems alongside the\n\ncontainerization platform.\n\nThe next two sections highlight key benefits and challenges of utilizing\n\ncontainerization technology with an emphasis of how containers compare to\n\nvirtual servers.\n\nContainerization Benefits\n\nThe following section highlights the key benefits of utilizing\n\ncontainerization technology. Many of these benefits are described in\n\nrelation to how containers compare to virtual servers.",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Solution Optimization — Being able to customize an isolated environment\n\nfor a solution that minimizes its footprint allows the solution to perform\n\nmore optimally while demanding only the infrastructure resources it\n\nactually requires.\n\nEnhanced Scalability — The reduced CPU, memory and storage usage\n\nfootprint of containers allows them to be more effectively and rapidly\n\nscaled in response to usage demands.\n\nEnhanced Resiliency — Using special features of container environments,\n\nresiliency can be natively provided to ensure that new solution instances are\n\nautomatically generated in response to failure conditions.\n\nEnhanced Deployment Speed — Containers can be created and deployed\n\nfaster than virtual servers, which supports rapid deployment and facilitates\n\nDevOps approaches, such as continuous integration (CI).\n\nVersion Support — Containers allow versions of software code and its\n\ndependencies to be tracked. Some platforms allow developers to maintain\n\nand track versions of a solution, inspect differences between different\n\nversions, and roll back to previous versions, when required.\n\nEnhanced Portability — A containerized solution can be more easily moved\n\nacross server hosting environments, without the need to change the solution\n\nsoftware within the container.",
      "content_length": 1288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Containerization Risks and Challenges\n\nThe following are common risks and challenges of using containerization:\n\nLack of Isolation from Host Operating System — When multiple containers\n\nare deployed on the same physical server, they end up sharing the same host\n\noperating system. This means that if the underlying physical server fails or\n\nis compromised, all containers running on the server will likely be\n\nimpacted.\n\nContainer Attack Threat — Whereas the administrator of a virtual server\n\ncannot access or modify the operating system of the underlying physical\n\nserver, the administrator of a container can, because the operating system’s\n\nkernel is shared among all containers running on the same physical server.\n\nThis introduces a significant security vulnerability when containerization\n\nplatforms are deployed without the involvement of virtual servers.\n\nIncreased Complexity — The addition of containerization technology adds\n\nnew layers and design considerations that can increase the complexity of\n\nthe underlying solution infrastructure. This can introduce effort and risk, as\n\nwell as an increased learning curve for those responsible for building the\n\nsolution and its underlying infrastructure environment.\n\nIncreased Administrative Overhead — Because a given container provides\n\na given version of a solution with only the operating system resources it\n\nrequires, on-going administrative effort may be needed to maintain the",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "creation of subsequent container versions that may be needed to\n\naccommodate the changing needs of future solution versions. In a virtual\n\nserver environment, this is less of a concern because the entire operating\n\nsystem is always provided to a solution and its subsequent versions.\n\n6.3 Understanding Containers\n\nWhile a container can contain any type of software program, it is most\n\ncommonly used to host applications or services that comprise or are part of\n\na greater automation solution (Figure 6.23).",
      "content_length": 508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Figure 6.23\n\nThe symbol on the left is used to represent a software program that is an\n\napplication or an application component. The symbol on the right\n\nrepresents a software program that is designed as a service.",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Container Hosting\n\nA single container can host a single software program and multiple\n\ncontainers can co-exist, side-by-side, in the same environment (Figure\n\n6.24). When multiple containers reside in the same underlying environment,\n\nthey are securely isolated from each other so that each container can operate\n\nindependently.\n\nFigure 6.24\n\nThree different containers host three different software programs.\n\nA single container can also be used to host multiple related or different\n\nsoftware programs (Figure 6.25).",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Figure 6.25\n\nEach of the three containers hosts one or more different software programs.\n\nContainers are dynamically generated based on pre-defined container\n\nimages, as explained shortly.\n\nContainers and Pods\n\nGrouping individual containers in a pod allows related software programs\n\nto be kept together, such as when they are part of the same overall\n\ndistributed solution (or namespace) and when they need to run under a\n\nsingle IP address (as explained later in the Container Network Addresses\n\nsection) (Figure 6.26). Containers inside a pod can find and discover each\n\nother via the host that the pod is deployed on and can communicate with\n\neach other using standard inter-process communication methods, such as\n\nshared memory. As explained shortly, containers in a pod can also share a\n\nfile system, dataset or data storage device.",
      "content_length": 839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Figure 6.26\n\nA single pod deployed on a virtual server allows the hosted services to\n\nshare the same IP address. The pod can also be deployed directly on a\n\nphysical server.",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "The pod establishes this environment, while ensuring that hosted programs\n\nare still isolated from each other. Pods further provide special\n\ncontainerization capabilities associated with container chains, orchestration\n\nand scaling. The usage of pods is therefore often required by the\n\ncontainerization platform, which is why single pods are frequently used to\n\nhost single containers.\n\nAn administrator creates and configures a pod, and the containers are then\n\nadded (Figures 6.27 and 6.28).",
      "content_length": 494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Figure 6.27\n\nAn empty Pod A is created by Administrator A.",
      "content_length": 58,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Figure 6.28\n\nAdministrator A instructs the container engine to add Containers A, B and\n\nC to Pod A.\n\nA common feature of pods is the ability for the pod to provide common\n\nstorage to the containers residing in it. The storage usually exists as a file\n\nsystem that is referred to as a volume. This form of common storage can be\n\nattractive as it offers high-speed access to stored content. Types of files\n\nstored in a volume can include log files, media files and configuration files.",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "The administrator can configure a pod to enable access to resident\n\ncontainers (Figure 6.29).\n\nFigure 6.29\n\nThe administrator allocates file system storage that is made available to the\n\ncontainers deployed inside the pod.",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "When deploying a pod hosted on a virtual server, the additional\n\nvirtualization layer may add runtime processing latency. Depending on the\n\nhosted application or service requirements, this can cause performance\n\nissues. In some deployment scenarios, the performance may be impacted by\n\nother virtual servers hosted on the same host. If the applications or services\n\ndeployed in the pod are latency sensitive, they can be especially negatively\n\nimpacted if the pod resides on a virtual server. Performance and latency can\n\nbe measured before it is determined where to best deploy the pod.\n\nContainer Instances and Clusters\n\nMultiple instances of the same container with the same software program\n\ncan be generated (Figure 6.30). This is usually required when concurrent\n\nusage of the hosted software program by multiple consumer programs is\n\nnecessary. Instances of containers are commonly referred to as replicas.",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Figure 6.30\n\nThree instances of Container A and its hosted Service A are generated. This\n\nallows each instance of Service A to interact with a different consumer\n\nprogram.\n\nContainer clusters (Figure 6.31) are pools of container instances that are\n\ninstantiated in advance of their actual usage. Container clusters can be\n\nmanually created or automatically generated. They are loaded into memory\n\nwhere they sit idle, waiting to be invoked. They can be scheduled so that\n\nthey only reside in memory during pre-determined time periods, such as\n\nanticipated peak usage times.",
      "content_length": 573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Figure 6.31\n\nThe symbol used to represent a container cluster.\n\nContainer clusters are primarily created in support of high-performance\n\nrequirements, often for service-based solutions, to ensure that the\n\ncontainerized service instances can be rapidly provisioned in response to\n\nusage demands. Container cluster enviornments can provide auto-scaling\n\ncapabilities, enabling them to dynamically adjust the size of a cluster based\n\non demand.",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Container Package Management\n\nContainer package management refers to the process of managing software\n\npackages and dependencies within containerized applications. It enables an\n\napplication and its dependencies to be grouped into a single portable unit\n\ncalled a package, which can be deployed on any system that supports the\n\ncontainerization technology.\n\nA container package manager is a tool that makes containerized application\n\npackaging and distribution easier. It allows container images and their\n\ndependencies to be grouped into a single, distributable package that can be\n\ndeployed and managed across multiple container orchestrators (a\n\nmechanism described in the following section).\n\nContainer package managers typically include a set of command-line tools\n\nfor creating, tagging, and submitting container images to a container\n\nregistry, as well as for creating and managing container images and their\n\ndependencies. They frequently allow the use of templates or configuration\n\nfiles to define the contents of the package and its dependencies, as well as a\n\nmethod to version and manage the package over time.\n\nA container package manager is used to coordinate the initial deployment of\n\ncontainers based on pre-defined workflow logic. The deployment workflow\n\nlogic is defined in a package (also known as a container deployment file)\n\n(Figure 6.32). Typically, host clusters are required to provide a pool of hosts\n\nin support of the deployment requirements.",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Figure 6.32\n\nThe symbol used to represent a package.\n\nThe container deployment file is retrieved from a package repository\n\n(Figure 6.33).",
      "content_length": 138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Figure 6.33\n\nThe symbol used to represent a package repository.\n\nThe container deployment file is then provided to the container package\n\nmanager (Figure 6.34).",
      "content_length": 160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Figure 6.34\n\nThe symbol used to represent a container package manager.\n\nBefore the container package manager carries out the deployment\n\nworkflow, a special deployment optimizer program (Figure 6.35) studies the\n\ncontents of the package and then assesses available hosts in the cluster to\n\ndetermine the optimal destination for the containers to be deployed.",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Besides the processing capacity of a candidate host, some of the other\n\nfactors that the deployment optimizer may consider include:\n\nhardware and software policy limitations\n\naffinity and anti-affinity specifications\n\ndata locality\n\ninter-workload interference\n\nOnce it has chosen a suitable destination host, the deployment optimizer\n\ninstructs the container package manager as to where the containers should\n\ngo. A deployment optimizer can further monitor already deployed\n\ncontainers to ensure their current hosts remain suitable.",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Figure 6.35\n\nThe symbol used to represent a deployment optimizer.\n\nNote\n\nWithin the context of containerization, deployment\n\noptimization is often referred to as “scheduling”.\n\nFurthermore, container package manager and deployment",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "optimizer programs are often limited to deploying containers\n\nresiding in pods.\n\nTypically, a package represents the containers that comprise an entire\n\nsolution. Container package managers are therefore created for a set of\n\nrelated containers. In this sense, the package repository can provide a means\n\nof application version management.\n\nExamples of what is defined in a package include:\n\nwhich host a given container will be deployed on\n\nwhich pod a given container will be deployed in\n\nwhat sequence a set of containers are deployed in\n\nThe administrator authors a package, stores it in the package repository and\n\nthen assigns it to the container package manager when it is time for the\n\ncontainers to be deployed (Figure 6.36).",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Figure 6.36\n\nThe container package manager coordinates the deployment of containers,\n\nas per the deployment workflow logic provided in the package and the host\n\ndeployment instructions it receives from the deployment optimizer.\n\nAfter the deployment, packages are still usually kept in the package\n\nrepository as they are often reusable. For example, if a set of containers\n\nshould need to be ported to a new host, the same container deployment file\n\ncould be revised with the new host information and then reused.\n\nDocker Compose and Helm are some popular container package managers.\n\nThese tools make it easier for developers to deploy and manage\n\ncontainerized applications on a variety of container orchestrators (described\n\nin the following section) by simplifying the packaging and distribution of\n\ncontainerized applications.\n\nContainer Orchestration\n\nThe process of automating the deployment, scaling, and management of\n\ncontainerized applications in a distributed computing environment is known\n\nas container orchestration. It entails the use of a container orchestrator,\n\nalso referred to as a container orchestration tool or container orchestration\n\nplatform.\n\nA container orchestrator performs a wide range of operations in a\n\ndistributed computing environment. These are some of the key operations",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "performed by a container orchestrator:\n\nContainer Deployment – A container orchestrator deploys containers across\n\nmultiple nodes in a cluster, ensuring that the containers are properly\n\nconfigured and networked.\n\nLoad Balancing – The orchestrator distributes traffic across multiple\n\ncontainers running the same application, helping to ensure high availability\n\nand scalability.\n\nScaling – The orchestrator automatically scales up, down, in or out the\n\nnumber of containers running an application based on demand, helping to\n\nensure optimal resource utilization and cost efficiency.\n\nHealth Monitoring – The orchestrator monitors the health of containers and\n\ncan automatically restart failed containers or replace them with healthy\n\nones.\n\nService Discovery – The orchestrator maintains a service registry, allowing\n\napplications to discover and communicate with each other across the\n\nnetwork.\n\nStorage Orchestration – The orchestrator manages the persistent storage\n\nneeds of containers, ensuring that data is stored and retrieved correctly.",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Network Orchestration – The orchestrator manages the networking needs of\n\ncontainers, providing each container with a unique IP address and routing\n\nnetwork traffic between containers.\n\nConfiguration Management – The orchestrator manages the configuration\n\nof containers and can automatically apply changes to running containers.\n\nA container orchestrator typically consists of several components that work\n\ntogether. Some of the key components of a container orchestrator are:\n\nContainer Runtime – Responsible for running and managing containers on\n\neach node in the cluster.\n\nAPI Server – Provides a central interface for interacting with the\n\norchestrator. It accepts API requests from clients and communicates with\n\nthe other components of the orchestrator to perform the requested actions.\n\nScheduler – Responsible for deciding which node in the cluster to deploy a\n\nnew container to, based on factors such as resource availability and\n\nworkload balancing.\n\nController Manager – Responsible for managing various controllers that\n\nautomate different aspects of the containerized application lifecycle, such as\n\nscaling, replication, and health monitoring.",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Distributed Key-Value Store – Used by the orchestrator to store\n\nconfiguration data, service discovery information, and other metadata.\n\nNetworking – A component that provides the necessary network\n\ninfrastructure to allow containers to communicate with each other across\n\nthe cluster, including routing and load balancing.\n\nStorage – A component that manages the persistent storage needs of\n\ncontainers, including providing access to shared storage resources and\n\nensuring data integrity.\n\nThe basic steps involved in container orchestration are:\n\nCreate a container image – Developers create a container image that\n\nincludes their application code and all its dependencies.\n\nPush the image to a container registry – The container image is pushed to a\n\ncontainer registry, which is a central remote repository of container images.\n\nDefine the application deployment – Using a container orchestrator,\n\ndevelopers define how the containerized application should be deployed,\n\nincluding the number of replicas, the network configuration, and any\n\nstorage requirements.\n\nDeploy the application – The container orchestrator deploys the application\n\nacross multiple nodes in a cluster, ensuring that the desired number of",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "replicas are running and that the application is accessible to users.\n\nMonitor and manage the application – The container orchestrator monitors\n\nthe health of the application, automatically scaling it up or down as needed,\n\nand rolling out updates and patches without causing downtime. It also\n\nprovides logging and monitoring capabilities to identify and troubleshoot\n\nany issues that arise.\n\nManage multiple applications – The container orchestrator can manage\n\nmultiple containerized applications simultaneously, ensuring that they are\n\ndeployed, scaled, and managed according to their individual requirements.\n\nContainer Package Manager vs. Container Orchestrator\n\nA container package manager and a container orchestrator serve different\n\nfunctions. The following are the key differences:\n\nFunction – A container package manager is responsible for managing\n\ncontainer images and their dependencies, while a container orchestrator is\n\nresponsible for automating the deployment, scaling, and management of\n\ncontainerized applications in a distributed computing environment.\n\nScope – Container package managers focus specifically on managing\n\ncontainer images and their dependencies, while container orchestrators\n\nmanage the entire containerized application, from deployment to scaling to\n\nmanagement.",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Level of Abstraction – Container package managers operate at a lower level\n\nof abstraction than container orchestrators. Package managers deal with\n\nindividual container images and their dependencies, while orchestrators\n\nprovide a high-level view of the entire containerized application.\n\nToolset – Container package managers typically provide a more limited set\n\nof tools focused on managing container images and their dependencies.\n\nContainer orchestrators, on the other hand, provide a range of tools and\n\nAPIs for managing containers, networks, storage, and other infrastructure\n\nresources.\n\nContainer Networks\n\nContainerization platforms generally provide virtual container networks in\n\norder to enable communication among containers that need to connect with\n\neach other. A container network is required to enable various\n\ncontainerization platform and system capabilities in support of providing:\n\ncontainer availability\n\ncontainer scalability\n\ncontainer resiliency\n\nThe container network typically exists as a virtual network (Figure 6.37)\n\nthat can be independently managed, configured and encrypted.",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Figure 6.37\n\nA container network allows containers to communicate with each other\n\nindependently from the communication among the software programs they",
      "content_length": 152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "host.\n\nAs previously described in the Fundamental Virtualization and\n\nContainerization section, there are two primary types of container\n\nnetworks:\n\nHost Network\n\nOverlay Network\n\nWhereas the host network is managed by a single container engine to\n\nsupport communication among containers on the same host, the overlay\n\nnetwork enables container engines deployed on different servers to enable\n\ncommunication between containers on different hosts.\n\nFor example, if a distributed solution encompasses two services, each in its\n\nown container, then a container network is established for the two\n\ncontainers that are part of that solution. If the containers are on the same\n\nhost, then a host network is created (Figure 6.38). If one container is on one\n\nhost and the other is on a different host, then an overlay network is created\n\n(Figure 6.39).",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Figure 6.38\n\nContainers A and B reside in separate pods on the same host and can\n\ncommunicate with each other via Host Network A.",
      "content_length": 129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Figure 6.39",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Containers A and B reside on different hosts and can communicate with\n\neach other via Overlay Network A.\n\nContainer Network Scope\n\nThe scope of a container network is usually equal to the scope of a given\n\nsolution. This is because the scope of a solution will encompass only those\n\ncontainers hosting software programs that are part of that solution.\n\nTherefore, when multiple solutions are hosted, multiple container networks\n\nwill be required.\n\nSome solutions share software programs, such as reusable utility services or\n\na shared database. If the reusable software program is in a container, then\n\nthat container can participate in multiple container networks (Figure 6.40).",
      "content_length": 679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Figure 6.40\n\nContainers A and B reside on the same host and can communicate via Host\n\nNetwork A. Container B is further part of Overlay Network B through which\n\nit can communicate with other containers.\n\nNote\n\nThe container network(s) that a given container will join can\n\nbe specified by the administrator in the container image’s\n\nbuild file and also in the container deployment package. If\n\nno network is specified, the container engine may\n\nautomatically assign a container to a “default” host network.\n\nBuild files are covered later in this chapter in the Container\n\nBuild Files sub-section of the upcoming Understanding\n\nContainer Images section.\n\nUsually, container networks by default limit communication of the\n\ncontainerized solution software programs so that they can only\n\ncommunicate with each other. However, a solution may need to be able to\n\naccess software programs or IT resources that are not containerized and\n\ntherefore reside outside of the container network. In this case, the container\n\nnetwork needs to be configured to allow the solution to communicate\n\noutside of the container network boundary (Figure 6.41).",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Figure 6.41",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Containers A and B communicate with each other via Host Network A.\n\nContainerized Applications A and B need to communicate with each other,\n\nas well as with Database A, which resides outside of Host Network A. The\n\nadministrator enables this by explicitly configuring Host Network A to\n\nallow for the required external access.\n\nContainer Network Addresses\n\nEach deployed container receives a network address that enables it to\n\nparticipate in a container network. A network address usually exists as an IP\n\naddress. If a container needs to participate in multiple container networks, it\n\nwill require a separate network address for each container network.\n\nFor example, if a container hosting a software program is being reused\n\nacross two container networks (such as in the example shown in Figure\n\n6.40), then that container will need two network addresses.\n\nNetwork addresses are usually assigned by the container engine subsequent\n\nto container deployment. They can also be manually assigned by the\n\nadministrator in a deployment package. Containers residing in the same pod\n\nshare the same network address and are individually identified through\n\ndifferent network ports.\n\nRich Containers\n\nDifferent types of containerization platforms can vary in the range of\n\nfeatures supported by a given container. Containers that are more feature-",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "rich are referred to as rich containers (Figure 6.42).\n\nFigure 6.42\n\nService A is deployed inside a rich container that provides extra features,\n\nincluding monitoring capabilities that can provide on-going status and\n\nhealth information about the service.",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "The extent to which a container can be feature-rich is determined by the\n\ncapabilities of the underlying container engine responsible for creating the\n\ncontainer.\n\nExamples of features provided by more advanced container engines\n\ninclude:\n\nContainer resources can be limited to control and govern the maximum\n\nnumber of resources that a container can consume.\n\nUsage logs can be collected for auditing and regulatory purposes.\n\nContainer restart criteria can be specified. For example, the container can be\n\nconfigured to automatically restart if a certain event or error occurs, but not\n\nif other types of events or errors occur.\n\nContainer storage management features, such as enabling an isolated file\n\nsystem to be shared by multiple containerized services.\n\nSharing storage between the host and the containers running on the host.\n\nThis may be required for regulatory and auditing purposes or to ensure that\n\nif the container is turned off, access to the data is still available.\n\nSupport for the execution of service composition logic for services\n\ndeployed inside a container hosted together on the same host.",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Some container engines further provide proxy features that allow them to\n\nact as a proxy for consumer requests to services they are hosting.\n\nOther Common Container Characteristics\n\nProvided here are some further common container characteristics:\n\nWith each program hosted in a container, numerous supporting programs\n\n(such as databases, utility programs and monitors) can also be deployed.\n\nContainers can be configured so that the amount of infrastructure resources\n\neach consumes is limited.\n\nThe visibility of external programs and IT resources accessible to a\n\ncontainer and its hosted programs can be limited.\n\nThe programs hosted by a container normally share the same lifecycle as\n\nthe container itself. This means that a hosted program will typically start,\n\nstop, pause and resume in sync with the container.\n\n6.4 Understanding Container Images\n\nContainer images are a central part of containerization platforms. They\n\nform the basis of on-going container creation. The processing of container\n\nimages is one of the primary responsibilities of the container engine.",
      "content_length": 1076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Container Image Types and Roles\n\nHow container images are used, stored and processed depends on what type\n\nthey are or what role they assume.\n\nThere are two primary types of container images:\n\nBase Container Images — These container images act as templates for\n\ncustomized container images. In this book, this type of container image is\n\nqualified as a “base” container image. Base container images are also\n\nreferred to as partial container images.\n\nCustomized Container Images — These container images are created by the\n\ncontainer engine, which then uses them to create actual, deployed\n\ncontainers. In this book, this type of container image may or may not be\n\nqualified as a “customized” container image. When a symbol is only labeled\n\nas a container image, it is implied that it has been customized.\n\nThe reason that these types of container images can be considered roles is\n\nthat a customized container image created from a base container image can,\n\nitself, become a base container image to be used as a template for future,\n\ndifferent customized container images.\n\nAs illustrated in Figure 6.43, a container image classified as a base\n\ncontainer image is published to the image registry from where it can then be",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "accessed by the container engine to form the basis of customized container\n\nimages (as explained in further detail later in this section).\n\nFigure 6.43",
      "content_length": 151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "The container engine supports four different runtimes that can be deployed\n\nin containers. Application A requires the capabilities of Runtime A. Base\n\nContainer Image A is used by the container engine to create the customized\n\nContainer Image A that is then used to create and deploy the actual\n\nContainer A for Application A.\n\nContainer Image Immutability\n\nA key characteristic of container images is that, once created, they are\n\nimmutable. This means that they cannot be altered (neither patched, nor\n\nupdated, nor any other type of alteration). If a change to a container image\n\nis required, then a new or revised build file needs to be created and a new\n\nversion of the container image needs to be generated, further resulting in the\n\nneed for a new version of the deployed container.\n\nThe scope of a container’s immutability relates to the contents of the build\n\nfile. Administrative tools allow for settings on a container to be changed\n\nthat do not relate to a build file and can therefore be made without the need\n\nto create a new version of the container.\n\nThe container engine assigns each individual container image a unique\n\nauto-generated image key that is further kept where the container image is\n\nstored, either in the image registry or in the container engine’s internal\n\nstorage.",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Container Image Abstraction\n\nA base container image will typically provide a subset of the functions\n\noffered by the underlying host operating system. This is referred to as\n\noperating system abstraction or, simply, abstraction. However, not all parts\n\nof the operating system are abstracted by the container image, as further\n\nexplained in the next two sub-sections.\n\nOperating System Kernel Abstraction\n\nEach operating system has a kernel, which exists as a set of the most\n\nessential operating system functions. Kernels in different operating systems\n\nare comprised of very similar functions. For example, the kernel of a\n\nWindows operating system has similar functions to the kernel of a Linux\n\noperating system.\n\nCommon functions provided by a kernel can include:\n\naccess to CPU resources\n\naccess to processing\n\naccess to host memory\n\naccess to input/output devices in the host\n\naccess to hardware storage",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "access to device drivers\n\naccess to server file systems\n\naccess to power management\n\nThe kernel is not abstracted by the container image. Instead, it is\n\nencompassed by the container engine. As a result, container images do not\n\nneed to copy the kernel, which helps to further reduce their footprint.\n\nThe container engine acts as somewhat of a liaison or intermediary,\n\nenabling containers to have full access to the entire set of kernel functions\n\nat runtime. Container engines are generally able to interact with kernels\n\nfrom different operating systems, which helps enable the portability of\n\ncontainers and their platforms across different hosting environments.\n\nOperating System Abstraction Beyond the Kernel\n\nThe parts of an operating system outside of the kernel can be abstracted and\n\nincluded in a container image.\n\nCommon operating system functions and resources that exist outside of the\n\nkernel can include:\n\nprogramming language libraries and compilers\n\nvarious system libraries",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "encryption platforms\n\nsystem monitors and monitoring functions\n\nconfiguration files and editors\n\nadministrative functions and platforms\n\nadministrative tools (for use by human administrators)\n\nlocalization programs\n\nThis form of abstraction represents the subset of the operating system that a\n\ngiven container image can capture to provide a customized and optimized\n\nhosting environment for the software programs that will be deployed in the\n\ncontainer generated from that container image.\n\nWhen abstracting these types of functions and resources, the container\n\nremains portable because the abstracted functions and resources are copied\n\nto and ported with the container.\n\nContainer Build Files\n\nA container build file (or just the build file) (Figure 6.44) is a human-\n\neditable, machine-processable configuration file that specifies what belongs\n\nin (or what is abstracted by) a customized container image.",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Figure 6.44\n\nThe symbol used to represent a container build file.\n\nSpecifically, the build file can identify:",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "the base container image that will be used to form the basis of the\n\ncustomized container image\n\nthe additional operating system resources to be added to (or abstracted by)\n\nthe customized container image\n\nthe container network(s) that the deployed customized container will need\n\nto participate in\n\nThe syntax and format in a given build file can vary, depending on the type\n\nof container engine being used.\n\nContainer Image Layers\n\nA container image organizes its content into layers. Each layer corresponds\n\nto a container build file statement or instruction.\n\nExamples of content in container image layers include:\n\ndata files and folders\n\nconfiguration files\n\ndatabases and repositories\n\nexecutable files\n\noperating system program files and runtimes",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Except for the very final layer, all the layers are read-only. The\n\ncontainerization platform uses a union file system as the basis of container\n\nimage layering. The use of the union file system and layering is what makes\n\nthe reusability of base container images possible.\n\nA base container image is comprised of a number of layers that represent\n\nwhat it abstracts (Figure 6.45).",
      "content_length": 381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Figure 6.45\n\nThe base container image has its own set of layers.\n\nThe customized container image that is derived from the base container\n\nimage will add layers to what is provided by the base container image. In\n\nthe customized container image, the entire base container image represents\n\nthe bottom layer (Figure 6.46).",
      "content_length": 320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Figure 6.46\n\nThe bottom layer of the customized container image is comprised of the\n\ncontents of the base container image.\n\nA software program that will be hosted by the deployed container that will\n\nbe generated from the customized container image can, itself, reside in a\n\nlayer of the customized container image (Figure 6.47).",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Figure 6.47\n\nA layer within the customized container image is comprised of the software\n\nprogram that the deployed container will be responsible for hosting.\n\nBecause container images are immutable, if a layer within an image needs\n\nto be removed or added to, a new container image version needs to be",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "created.\n\nHow Customized Container Images are Created\n\nThe container engine uses the build file together with the base container\n\nimage to generate the customized container image (Figure 6.48).",
      "content_length": 193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Figure 6.48\n\nThe administrator authors a build file for Container A (1). The\n\nadministrator provides the build file to the container engine (2). The\n\ncontainer engine retrieves the required base container image from the\n\nimage registry (3). The container engine then uses the base container image\n\nand the information from the build file to create a new customized\n\nContainer Image A from which it then generates and deploys Container A\n\n(4).\n\nOnce the actual container implementation is created from the customized\n\ncontainer image and deployed, there may no longer be a need to keep the\n\nbuild file because the container engine now has the customized container\n\nimage that it can use to create more instances of the actual container in the\n\nfuture.\n\nNote that the customized container image is not typically stored in the\n\nimage registry. Instead, it is stored in the container engine’s internal storage\n\nso that the engine can retain immediate access to it in support of efficiently\n\nand rapidly creating new container instances for scalability and resiliency\n\npurposes.",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "A customized container image can also be published to the image registry if\n\nthe administrator determines that it may be useful as a base container image\n\nto be used for new types of customized container images.\n\n6.5 Multi-Container Types\n\nSo far, the majority of containers shown have been hosting applications and\n\nservices that, presumably, are responsible for processing primary business\n\nlogic. However, for applications to function in distributed environments,\n\nadditional types of secondary (or utility) processing are also required.\n\nThis section introduces the following set of basic multi-container types,\n\neach of which adds a container with a secondary component that abstracts\n\nutility-related processing:\n\nSidecar Container\n\nAdapter Container\n\nAmbassador Container\n\nSidecar Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to process generic utility logic, the ability for the application to\n\nprocess its business logic reliably and effectively can be compromised\n\n(Figure 6.49).",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Figure 6.49\n\nApplication A is burdened with carrying out business logic and utility logic.",
      "content_length": 90,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "A secondary containerized application component (referred to as a sidecar\n\ncomponent) is added to abstract the utility logic-related processing (Figure\n\n6.50). The sidecar component is deployed in a separate container, usually\n\nwithin the same pod as the application. Depending on the nature of the\n\nutility processing, the application may or may not need to communicate\n\nwith the sidecar component.\n\nFigure 6.50\n\nThe utility logic is placed in Sidecar Component A that resides in the\n\nseparate Container B, in the same pod. This enables Application A to focus",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "exclusively on carrying out its business logic.\n\nAdapter Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to carry out data conversion logic to accommodate external\n\nconsumer applications, the ability for the application to process its business\n\nlogic reliably and effectively can be compromised (Figure 6.51).\n\nFurthermore, by embedding this conversion logic into the application, it\n\nmay become coupled to multiple different external consumer programs,\n\nwhich can become burdensome when those consumer programs change\n\nover time.",
      "content_length": 578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Figure 6.51\n\nApplication A is burdened with carrying out business logic and specific\n\nconversion logic required by Application B.",
      "content_length": 129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "A secondary containerized application component (referred to as an adapter\n\ncomponent) is added to abstract any necessary data conversion processing\n\nlogic (Figure 6.52). The adapter component is deployed in a separate\n\ncontainer, usually within the same pod as the application. A separate adapter\n\ncomponent can be deployed for each consumer application that requires a\n\ndifferent representation of the output data.\n\nFigure 6.52\n\nThe conversion logic is placed in Adapter Component A that resides in the\n\nseparate Container B, in the same pod. This enables Application A to focus",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "exclusively on carrying out its business logic.\n\nAmbassador Container\n\nWhen an application responsible for processing primary business logic is\n\nalso built to carry out external communication processing logic to connect\n\nwith external consumer applications, the ability for the application to\n\nprocess its business logic reliably and effectively can be compromised\n\n(Figure 6.53). Also, by embedding this specific communication logic (such\n\nas logic related to protocols, messaging and security) into the application, it\n\nmay become coupled to multiple different external programs, which can\n\nbecome burdensome when the APIs of those programs change over time.",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Figure 6.53\n\nApplication A is burdened with carrying out business logic and specific\n\ncommunications processing logic required to connect with Application B.",
      "content_length": 157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "A secondary containerized application component (referred to as an\n\nambassador component) is added to abstract any necessary communication\n\nprocessing logic (Figure 6.54). The ambassador component is deployed in a\n\nseparate container, usually within the same pod as the application. A\n\nseparate ambassador component can be deployed for each application that\n\nhas a set of different communication requirements.\n\nFigure 6.54",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "The communications logic is placed in Ambassador Component A that\n\nresides in the separate Container B, in the same pod. This enables\n\nApplication A to focus exclusively on carrying out its business logic.\n\nUsing Multi-Containers Together\n\nThe three types of multi-containers can be used individually or together, as\n\nrequired. For example, depending on the nature of this business logic, an\n\napplication may require some or all of the secondary containers to be\n\ndeployed with it (Figure 6.55).",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Figure 6.55\n\nApplication A is supported by three secondary containers.\n\n6.6 Case Study Example\n\nInnovartus Technologies Inc. has identified multiple benefits from using\n\ncontainerization technology in support of its technology and business\n\nstrategies, including the following:\n\nScalability can be greatly improved to accommodate increased and less\n\npredictable cloud consumer interaction.\n\nService levels can also be improved to avoid the frequent outages that are\n\ncurrently occurring more frequently than usual.\n\nCost effectiveness can be improved by reducing the number of virtual\n\nservers required for the delivery of its virtual products, as these products\n\ncan now be deployed in containers instead of virtual servers.\n\nThe virtual toys and educational entertainment products for children offered\n\nby Innovartus Technologies were designed as applications comprised of\n\nmultiple independent services that work together to provide the necessary\n\nfunctionality. This allows for every individual service to be deployed in its\n\nindividual container and scaled out dynamically in accordance with its\n\nperformance and total capacity requirements.",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Due to certain security-related requirements, three services that provide\n\naccess to parents for the configuration of their child’s virtual toys need to\n\nshare the same IP address. Deploying them in separate containers within a\n\nsingle logical pod provides the ideal deployment solution for this\n\nrequirement.\n\nMonitoring the usage, performance and security of the virtual toys and\n\nentertainment products is fundamental to its business strategy. However, to\n\nallow each service running in its own container to focus on the functionality\n\nit must deliver as part of a virtual toy or other entertainment product,\n\nsidecar containers can be used to separate utility-resalted functionality, like\n\nwriting logs or reporting data to performance and security monitoring logic,\n\nin components running in the sidecar container.\n\nTwo of the monitoring systems that the services need to send telemetry data\n\nto are deployed remotely. In this case, ambassador containers are used to\n\nallow services to delegate the communication with the remote system to the\n\nambassador and focus on the core functionality they were designed to\n\ndeliver.\n\nFinally, adapter containers are used throughout the Innovartus architecture\n\nto allow the use of their products by users via different devices (such as\n\nsmart phones, tablets and computers) with the adapter containers running\n\nthe logic necessary for every device to be accessed separately by both\n\nparents and children.",
      "content_length": 1449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Chapter 7\n\nUnderstanding Cloud Security and Cybersecurity\n\n7.1 Basic Security Terminology\n\n7.2 Basic Threat Terminology\n\n7.3 Threat Agents\n\n7.4 Common Threats\n\n7.5 Case Study Example\n\n7.6 Additional Considerations\n\n7.7 Case Study Example\n\nThis chapter introduces terms and concepts that address basic information\n\nsecurity within clouds, and then concludes by defining a set of threats and\n\nattacks common to public cloud environments. The cloud security and\n\ncybersecurity mechanisms covered in Chapters 10 and 11 establish the\n\nsecurity controls used to counter these threats.\n\n7.1 Basic Security Terminology\n\nInformation security is a complex ensemble of techniques, technologies,\n\nregulations, and behaviors that collaboratively protect the integrity of and",
      "content_length": 761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "access to computer systems and data. IT security measures aim to defend\n\nagainst threats and interference that arise from both malicious intent and\n\nunintentional user error.\n\nThe upcoming sections define fundamental security terms relevant to cloud\n\ncomputing and describe associated concepts.\n\nConfidentiality\n\nConfidentiality is the characteristic of something being made accessible\n\nonly to authorized parties (Figure 7.1). Within cloud environments,\n\nconfidentiality primarily pertains to restricting access to data in transit and\n\nstorage.\n\nFigure 7.1",
      "content_length": 557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "The message issued by the cloud consumer to the cloud service is\n\nconsidered confidential only if it is not accessed or read by an unauthorized\n\nparty.\n\nIntegrity\n\nIntegrity is the characteristic of not having been altered by an unauthorized\n\nparty (Figure 7.2). An important issue that concerns data integrity in the\n\ncloud is whether a cloud consumer can be guaranteed that the data it\n\ntransmits to a cloud service matches the data received by that cloud service.\n\nIntegrity can extend to how data is stored, processed, and retrieved by cloud\n\nservices and cloud-based IT resources.\n\nFigure 7.2",
      "content_length": 597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "The message issued by the cloud consumer to the cloud service is\n\nconsidered to have integrity if it has not been altered.\n\nAvailability\n\nAvailability is the characteristic of being accessible and usable during a\n\nspecified time period. In typical cloud environments, the availability of\n\ncloud services can be a responsibility that is shared by the cloud provider\n\nand the cloud carrier. The availability of a cloud-based solution that extends\n\nto cloud service consumers is further shared by the cloud consumer.\n\nFigure 7.3 depicts a scenario that demonstrates how a collection of security\n\ntechnologies helps ensure the confidentiality and integrity of data exchange\n\nover the Internet, as well as the availability of a central database containing\n\nprivate data.",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Figure 7.3\n\nA hospital contributes confidential medical data to a database in a cloud\n\n(1) shared by a research institution that retrieves the data (2). Supporting\n\ncybersecurity technologies provide confidentiality via encryption, integrity\n\nvia runtime scanning and availability by ensuring the on-going safety of the\n\nshared cloud-based database.",
      "content_length": 349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Authenticity\n\nAuthenticity is the characteristic of something having been provided by an\n\nauthorized source. This concept encompasses non-repudiation, which is the\n\ninability of a party to deny or challenge the authentication of an interaction.\n\nAuthentication in non-repudiable interactions provides proof that these\n\ninteractions are uniquely linked to an authorized source. For example, a\n\nuser may not be able to access a non-repudiable file after its receipt without\n\nalso generating a record of this access.\n\nSecurity Controls\n\nSecurity controls are countermeasures used to prevent or respond to\n\nsecurity threats and to reduce or avoid risk. Details on how to use security\n\ncountermeasures are typically outlined in the security policy, which\n\ncontains a set of rules and practices specifying how to implement a system,\n\nservice, or security plan for maximum protection of sensitive and critical IT\n\nresources.\n\nSecurity Mechanisms\n\nCountermeasures are typically described in terms of security mechanisms,\n\nwhich are components comprising a defensive framework that protects IT\n\nresources, information, and services. Chapters 10 and 11 describe a series of\n\ncloud security and cybersecurity mechanisms.",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Security Policies\n\nA security policy establishes a set of security rules and regulations. Often,\n\nsecurity policies will further define how these rules and regulations are\n\nimplemented and enforced. For example, the positioning and usage of\n\nsecurity controls and mechanisms can be determined by security policies.\n\n7.2 Basic Threat Terminology\n\nThis section covers some fundamental topics that help establish the primary\n\npurpose and scope of cybersecurity practices and technologies, as well as\n\nsome essential vocabulary.\n\nThe following section provides descriptions of fundamental terms\n\nassociated with cybersecurity threats.\n\nRisk\n\nRisk is the potential unwanted and unexpected loss that may result from a\n\ngiven action. Risks can pertain to various aspects of cybersecurity,\n\nincluding external threats, internal vulnerabilities, responses carried out\n\nagainst threats, as well as risks associated with possible human error,\n\ntechnology malfunctions and the overall quality of a cybersecurity\n\nenvironment.",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Vulnerability\n\nA vulnerability, within the context of cybersecurity, is a flaw, gap or\n\nweakness in an IT environment or associated policies or processes that\n\nleaves an organization open to potentially successful security breaches.\n\nVulnerabilities can be physical or digital. Attackers attempt to exploit\n\nvulnerabilities, while organizations attempt to eliminate or mitigate them.\n\nExploit\n\nAn exploit occurs when an attacker is able to take advantage of a\n\nvulnerability.\n\nZero-Day Vulnerability\n\nA zero-day vulnerability is a vulnerability that an organization is either\n\nunaware of or for which it has not been able to yet provide a patch or fix.\n\nAs a result, an attacker may be able to more easily exploit this vulnerability\n\nuntil the organization is able to address it.\n\nSecurity Breach\n\nA security breach is any incident that may result in unauthorized access to\n\ninformation or systems. It typically occurs when an attacker is able to\n\nbypass security mechanisms and controls.",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Data Breach\n\nA data breach is a type of security breach whereby an attacker is able to\n\nsteal confidential information.\n\nData Leak\n\nA data leak occurs when sensitive information is shared with unauthorized\n\nparties without an attack taking place. Data leaks can occur accidentally or\n\nintentionally and are usually carried out by humans.\n\nThreat (or Cyber Threat)\n\nA threat or a cyber threat is a known, potential attack that poses danger and\n\nrisk to an organization. The collection of threats relevant to a given\n\norganization is known as the threat landscape or cyber threat landscape.\n\nAttack (or Cyber Attack)\n\nWhen a threat is carried out by an attacker, it becomes an attack or a cyber\n\nattack.\n\nAttacker and Intruder\n\nWithin the context of cloud security and cybersecurity, an attacker is an\n\nindividual or organization that carries out cyber attacks.\n\nThere are different types of attackers:",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Cyber Criminals — Attackers that attempt to steal private information for\n\nthe purpose of profit or other types of illegal activity.\n\nMalicious Users — An authorized user, such as a rogue employee, who\n\nabuses trusted privileges to access a system with the intent to cause harm or\n\ncarry out unauthorized actions.\n\nCyber Activists — Attackers that carry out malicious activity to promote a\n\npolitical agenda, religious belief or social ideology.\n\nState-Sponsored Attackers — Attackers who are hired by a government\n\nagency.\n\nAny attacker that has successfully gained unauthorized access within an\n\norganizational boundary is known as an intruder.\n\nAttack Vector and Surface\n\nAn attack vector is the path that an attacker takes to exploit vulnerabilities.\n\nExamples of attack vectors are email attachments, pop-up windows, chat\n\nrooms and instant messages. Human error or ignorance are commonly\n\nplanned for when creating a given attack vector. An attack surface is a\n\ncollection of attack vectors from where an attacker can access a system or\n\nextract information.\n\n7.3 Threat Agents",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "A threat agent is an entity that poses a threat because it is capable of\n\ncarrying out an attack. Cloud security threats can originate either internally\n\nor externally, from humans or software programs. Corresponding threat\n\nagents are described in the upcoming sections. Figure 7.4 illustrates the role\n\na threat agent assumes in relation to vulnerabilities, threats, and risks, and\n\nthe safeguards established by security policies and security mechanisms.",
      "content_length": 457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Figure 7.4",
      "content_length": 10,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "How security policies and security mechanisms are used to counter threats,\n\nvulnerabilities, and risks caused by threat agents.\n\nAnonymous Attacker\n\nAn anonymous attacker is a non-trusted cloud service consumer without\n\npermissions in the cloud (Figure 7.5). It typically exists as an external\n\nsoftware program that launches network-level attacks through public\n\nnetworks. When anonymous attackers have limited information on security\n\npolicies and defenses, it can inhibit their ability to formulate effective\n\nattacks. Therefore, anonymous attackers often resort to committing acts like\n\nbypassing user accounts or stealing user credentials, while using methods\n\nthat either ensure anonymity or require substantial resources for\n\nprosecution.",
      "content_length": 745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Figure 7.5\n\nThe notation used for an anonymous attacker.\n\nMalicious Service Agent\n\nA malicious service agent is able to intercept and forward the network\n\ntraffic that flows within a cloud (Figure 7.6). It typically exists as a service\n\nagent (or a program pretending to be a service agent) with compromised or\n\nmalicious logic. It may also exist as an external program able to remotely\n\nintercept and potentially corrupt message contents.",
      "content_length": 439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Figure 7.6\n\nThe notation used for a malicious service agent.\n\nTrusted Attacker\n\nA trusted attacker shares IT resources in the same cloud environment as the\n\ncloud consumer and attempts to exploit legitimate credentials to target\n\ncloud providers and the cloud tenants with whom they share IT resources\n\n(Figure 7.7). Unlike anonymous attackers (which are non-trusted), trusted\n\nattackers usually launch their attacks from within a cloud’s trust boundaries\n\nby abusing legitimate credentials or via the appropriation of sensitive and\n\nconfidential information.",
      "content_length": 559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Figure 7.7\n\nThe notation that is used for a trusted attacker.\n\nTrusted attackers (also known as malicious tenants) can use cloud-based IT\n\nresources for a wide range of exploitations, including the hacking of weak\n\nauthentication processes, the breaking of encryption, the spamming of e-\n\nmail accounts, or to launch common attacks, such as denial of service\n\ncampaigns.",
      "content_length": 370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Malicious Insider\n\nMalicious insiders are human threat agents acting on behalf of or in relation\n\nto the cloud provider. They are typically current or former employees or\n\nthird parties with access to the cloud provider’s premises. This type of\n\nthreat agent carries tremendous damage potential, as the malicious insider\n\nmay have administrative privileges for accessing cloud consumer IT\n\nresources.\n\nNote\n\nA notation used to represent a general form of human-driven\n\nattack is the workstation combined with a lightning bolt\n\n(Figure 7.8). This generic symbol does not imply a specific\n\nthreat agent, only that an attack was initiated via a\n\nworkstation.",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Figure 7.8\n\nThe notation used for an attack originating from a workstation. The human\n\nsymbol is optional.\n\n7.4 Common Threats\n\nThis section introduces several common threats and vulnerabilities in cloud-\n\nbased environments and describes the roles of the aforementioned threat\n\nagents.",
      "content_length": 286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Traffic Eavesdropping\n\nTraffic eavesdropping occurs when data being transferred to or within a\n\ncloud (usually from the cloud consumer to the cloud provider) is passively\n\nintercepted by a malicious service agent for illegitimate information\n\ngathering purposes (Figure 7.9). The aim of this attack is to directly\n\ncompromise the confidentiality of the data and, possibly, the confidentiality\n\nof the relationship between the cloud consumer and cloud provider. Because\n\nof the passive nature of the attack, it can more easily go undetected for\n\nextended periods of time.\n\nFigure 7.9",
      "content_length": 582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "An externally positioned malicious service agent carries out a traffic\n\neavesdropping attack by intercepting a message sent by the cloud service\n\nconsumer to the cloud service. The service agent makes an unauthorized\n\ncopy of the message before it is sent along its original path to the cloud\n\nservice.\n\nMalicious Intermediary\n\nThe malicious intermediary threat arises when messages are intercepted and\n\naltered by a malicious service agent, thereby potentially compromising the\n\nmessage’s confidentiality and/or integrity. It may also insert harmful data\n\ninto the message before forwarding it to its destination. Figure 7.10\n\nillustrates a common example of the malicious intermediary attack.",
      "content_length": 694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Figure 7.10\n\nThe malicious service agent intercepts and modifies a message sent by a\n\ncloud service consumer to a cloud service (not shown) being hosted on a\n\nvirtual server. Because harmful data is packaged into the message, the\n\nvirtual server is compromised.\n\nNote",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "While not as common, the malicious intermediary attack can\n\nalso be carried out by a malicious cloud service consumer\n\nprogram.\n\nDenial of Service\n\nThe objective of the denial of service (DoS) attack is to overload IT\n\nresources to the point where they cannot function properly. This form of\n\nattack is commonly launched in one of the following ways:\n\nThe workload on cloud services is artificially increased with imitation\n\nmessages or repeated communication requests.\n\nThe network is overloaded with traffic to reduce its responsiveness and\n\ncripple its performance.\n\nMultiple cloud service requests are sent, each of which is designed to\n\nconsume excessive memory and processing resources.\n\nSuccessful DoS attacks produce server degradation and/or failure, as\n\nillustrated in Figure 7.11.",
      "content_length": 791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Figure 7.11\n\nCloud Service Consumer A sends multiple messages to a cloud service (not\n\nshown) hosted on Virtual Server A. This overloads the capacity of the\n\nunderlying physical server, which causes outages with Virtual Servers A and\n\nB. As a result, legitimate cloud service consumers, such as Cloud Service\n\nConsumer B, become unable to communicate with any cloud services\n\nhosted on Virtual Servers A and B.",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Note\n\nA common variation of the DoS attack is the DDoS\n\n(distributed denial of service) attack, in which multiple\n\ncompromised systems are used to flood a targeted website or\n\nnetwork with traffic, in an attempt to make it\n\nunavailable.Insufficient Authorization\n\nThe insufficient authorization attack occurs when access is granted to an\n\nattacker erroneously or too broadly, resulting in the attacker getting access\n\nto IT resources that are normally protected. This is often a result of the\n\nattacker gaining direct access to IT resources that were implemented under\n\nthe assumption that they would only be accessed by trusted consumer\n\nprograms (Figure 7.12).",
      "content_length": 662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Figure 7.12\n\nCloud Service Consumer A gains access to a database that was\n\nimplemented under the assumption that it would only be accessed through a\n\nWeb service with a published service contract (as per Cloud Service\n\nConsumer B).",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "A variation of this attack, known as weak authentication, can result when\n\nweak passwords or shared accounts are used to protect IT resources. Within\n\ncloud environments, these types of attacks can lead to significant impacts\n\ndepending on the range of IT resources and the range of access to those IT\n\nresources the attacker gains (Figure 7.13).",
      "content_length": 346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Figure 7.13",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "An attacker has cracked a weak password used by Cloud Service Consumer\n\nA. As a result, a malicious cloud service consumer (owned by the attacker)\n\nis designed to pose as Cloud Service Consumer A in order to gain access to\n\nthe cloud-based virtual server.\n\nVirtualization Attack\n\nVirtualization provides multiple cloud consumers with access to IT\n\nresources that share underlying hardware but are logically isolated from\n\neach other. Because cloud providers grant cloud consumers administrative\n\naccess to virtualized IT resources (such as virtual servers), there is an\n\ninherent risk that cloud consumers could abuse this access to attack the\n\nunderlying physical IT resources.\n\nA virtualization attack exploits vulnerabilities in the virtualization platform\n\nto jeopardize its confidentiality, integrity, and/or availability. This threat is\n\nillustrated in Figure 7.14, where a trusted attacker successfully accesses a\n\nvirtual server to compromise its underlying physical server. With public\n\nclouds, where a single physical IT resource may be providing virtualized IT\n\nresources to multiple cloud consumers, such an attack can have significant\n\nrepercussions.",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Figure 7.14\n\nAn authorized cloud service consumer carries out a virtualization attack by\n\nabusing its administrative access to a virtual server to exploit the\n\nunderlying hardware.\n\nOverlapping Trust Boundaries\n\nIf physical IT resources within a cloud are shared by different cloud service\n\nconsumers, these cloud service consumers have overlapping trust\n\nboundaries. Malicious cloud service consumers can target shared IT\n\nresources with the intention of compromising cloud consumers or other IT\n\nresources that share the same trust boundary. The consequence is that some\n\nor all of the other cloud service consumers could be impacted by the attack",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "and/or the attacker could use virtual IT resources against others that happen\n\nto also share the same trust boundary.\n\nFigure 7.15 illustrates an example in which two cloud service consumers\n\nshare virtual servers hosted by the same physical server and, resultantly,\n\ntheir respective trust boundaries overlap.\n\nFigure 7.15",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Cloud Service Consumer A is trusted by the cloud and therefore gains\n\naccess to a virtual server, which it then attacks with the intention of\n\nattacking the underlying physical server and the virtual server used by\n\nCloud Service Consumer B.\n\nContainerization Attack\n\nThe use of containerization introduces a lack of isolation from the host\n\noperating system level. Since containers deployed on the same machine\n\nshare the same host operating system, security threats can increase because\n\naccess to the entire system can be gained. If the underlying host is\n\ncompromised, all containers running on the host may be impacted.\n\nContainers can be created from within an operating system running on a\n\nvirtual server. This can help ensure that if a security breach occurs that\n\nimpacts the operating system a container is running on, the attacker can\n\nonly gain access to and alter the virtual server’s operating system or the\n\ncontainers running on a single virtual server, while other virtual servers (or\n\nphysical servers) remain intact.\n\nAnother option is a one-service per physical server deployment model\n\nwhere all container images deployed on the same host are the same. This\n\ncan reduce risk without the need to virtualize the IT resources. In this case,\n\na security breach to one cloud service instance would only allow access to\n\nother instances, and the residual risk could be considered as acceptable.\n\nHowever, this approach may not be optimal for deploying many different",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "cloud services because it can significantly increase the total number of\n\nphysical IT resources that need to be deployed and managed while further\n\nincreasing cost and operational complexity.\n\nMalware\n\nMalware, also referred to as malicious software, is a type of software\n\nprogram designed to cause harm to a computer system or network.\n\nMalware can be used to perform a variety of malicious activities, including:\n\nstealing protected data\n\ndeleting confidential documents\n\nlistening to private communication\n\ncollecting information about confidential activity\n\nThe fundamental basis of a malware attack is the installation of\n\nunauthorized software on the victim’s computer (Figure 7.16).",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Figure 7.16\n\nAn attacker making a server available to a user (via a website, for example)\n\nwho inadvertently downloads malware to a local workstation.\n\nThe following are common types of malware-based cyber attacks:\n\nVirus — Malware that can spread by infecting systems and files with code\n\nthat enables the virus to replicate and perform additional actions on the\n\ninfected system.",
      "content_length": 381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Trojan — A piece of malicious software that appears to be a legitimate\n\napplication or service. A trojan can engage in malicious behavior, often as\n\npart of background processes, to carry out activities such as installing\n\nbackdoor code and injecting code into other running processes. A trojan\n\nmay or may not contain a virus.\n\nSpyware — A type of malware that collects information about users or\n\norganizations without their knowledge.\n\nAdware — Software designed to display unwanted advertisements or pop-\n\nups. Adware can be considered a security threat because it can collect\n\nsensitive information and may slow down systems and make them\n\nvulnerable to other types of malware.\n\nRansomware — Malware that restricts or prevents data usage or access with\n\nthe purpose of demanding payment of a fee to decrypt or release the data.\n\nAn on-going ransomware attack can be carried out using remote code\n\nexecution (as covered later in this section).\n\nBot — Malware capable of remotely receiving commands and reporting\n\ninformation to a remote destination. A bot is usually designed to work\n\ntogether with other bots (as per the Botnet threat covered later in this\n\nsection).\n\nRogue Antivirus — An application claiming to be an antivirus program that,\n\nonce installed, falsely reports security issues to mislead victims into",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "purchasing a “full” version of the program.\n\nCrypto Jacking — The practice of using browser-based programs that run\n\nscripts embedded in web content to mine cryptocurrency without the user’s\n\nknowledge or consent.\n\nWorm — A self-replicating, self-propagating, self-contained program that\n\nuses network mechanisms to spread itself. Worms do not usually cause\n\nmuch harm beyond using up computing resources and are generally not\n\ncommon.\n\nData science technologies can be used to support malware attacks by\n\nanalyzing systems in order to discover and identify new vulnerabilities that\n\ncan be exploited by malware programs. These technologies can further\n\nenable the development of reactive malicious code that can, itself, look for\n\nnew vulnerabilities.\n\nInsider Threat\n\nAn insider threat is associated with potential damage that can be inflicted\n\nby an organization’s staff and others that may have access to the\n\norganization’s premises or systems.\n\nCommon types of insider threats include the following (Figure 7.17):\n\nmalicious — attempts by an insider (such as a disgruntled employee) to\n\naccess and potentially harm an organization's data, systems or IT",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "infrastructure\n\naccidental — accidental damage caused by insiders making mistakes out of\n\nignorance or due to human error, such as accidentally deleting an important\n\nfile or inadvertently sharing confidential data with an unauthorized party\n\nnegligent — accidental damage caused by insiders due to carelessness or an\n\nunwillingness to following established cybersecurity standards and policies\n\nFigure 7.17\n\nExamples of malicious (left), negligent (middle) and accidental (right)\n\ninsiders posing threats to an organization.\n\nInsider threats can put organizational assets in danger, including physical\n\nhardware, physical product inventory, corporate websites, social media\n\ncommunication and information assets.\n\nSocial Engineering and Phishing\n\nSocial engineering is a form of attack where individuals are tricked into\n\nrevealing sensitive information or performing potentially damaging actions,",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "such as granting access to unauthorized parties (Figure 7.18). Social\n\nengineering tactics are popular because it can be easier to exploit people\n\nthan it is to exploit technology.\n\nFigure 7.18\n\nAn example of an attacker attempting to carry out a social engineering\n\nattack by extracting sensitive information from an employee who may be\n\nworking for a cloud consumer or cloud provider organization.\n\nPhishing is a form of social engineering that uses electronic\n\ncommunication, such as sending fraudulent emails that appear to come from\n\nvalid sources, in an attempt to coerce users into releasing sensitive",
      "content_length": 608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "information, triggering a security breach or performing other damaging\n\nactions.\n\nBotnet\n\nAs described earlier in the Malware section, a bot is a form of malware\n\ncapable of receiving and acting upon instructions issued by a remote\n\nattacker. A botnet attack utilizes multiple bots that are distributed across\n\ndifferent hosts in order to carry out an attack via a coordinated network of\n\nbots (a bot-net).\n\nA common technique for carrying out a botnet attack is to start with an\n\ninitial malware infection to create “zombie” hosts. A zombie host is a\n\ncomputer that belongs to an unsuspecting, legitimate organization that an\n\nattacker has taken control of. The attacker then typically uses the zombie\n\nhost to carry out attacks against another party (Figure 7.19).",
      "content_length": 766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Figure 7.19\n\nAn attacker has turned a regular server at Organization A into a zombie\n\nserver that is controlled by the attacker to transmit malware to a user’s\n\ncomputer at Organization B.\n\nA botnet can be comprised of bots located on host servers that belong to the\n\nattacker, as well as zombie servers. Once installed, a bot seeks to connect\n\nwith other bots on other infected hosts and devices to form a network that\n\nthe attacker can use to perform malicious actions (Figure 7.20), such as\n\ncarrying out large-scale DDoS attacks, crypto jacking attacks, sending mass\n\nemails with harmful content, stealing data or even recruiting new bots.",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Botnets can be purchased on the dark web and can be even rented for short\n\nperiods.",
      "content_length": 83,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Figure 7.20\n\nAn attacker has turned regular servers at Organizations A and B into\n\nzombie servers. The attacker uses these servers together with some local\n\nservers to carry out an attack on Organization C.\n\nNote that botnet attacks often encompass other attacks and techniques, such\n\nas remote code execution, privilege escalation, social engineering and\n\ninsider threats.\n\nPrivilege Escalation\n\nA privilege escalation attack occurs when an attacker attempts to gain\n\nadministrator permissions after compromising a user account with limited\n\naccess privileges (Figure 7.21). This can be done by exploiting\n\nvulnerabilities that unintentionally allow a user account’s access levels to be\n\nincreased.",
      "content_length": 699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Figure 7.21\n\nAn attacker is able to infiltrate an employee’s user account and then exploit\n\na vulnerability to upgrade access privileges.\n\nData science technologies can be used to support privilege escalation\n\nattacks by developing models that can be used to continuously search and\n\nanalyze potential victim user accounts and systems for exploitable\n\nvulnerabilities. For example, another attack could be employed to collect",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "data about how current systems in a network are with regards to third-party\n\nsoftware patches. The data science system may be able to process this\n\ninformation, along with additional data about the third-party software\n\nprograms and how they are configured in the target environment in order to\n\nproduce a set of recommended target areas.\n\nBrute Force\n\nIn a brute force attack, an attacker attempts a broad range of possible\n\nusername and password combinations to try to determine which one\n\ncombination is correct and enables the attacker to gain unauthorized access\n\nto a system (Figure 7.22).",
      "content_length": 595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Figure 7.22\n\nAn attacker issues a brute force attack by bombarding a website with a\n\nseries of username and password combinations.\n\nAs such, password-only systems are most vulnerable to brute force attacks\n\nand user accounts with weak passwords are those most easily accessed.\n\nThe simplest type of brute force attack is a dictionary attack in which the\n\nattacker reads from a dictionary of possible passwords and essentially tries\n\nthem all. Credential recycling is another variation whereby usernames and\n\npasswords from prior data breaches are reused to try to break into other\n\nsystems.\n\nRemote Code Execution\n\nRemote code execution is a cyber attack in which an attacker remotely\n\nexecutes commands on a third party’s computing device.\n\nExamples of how this attack can succeed include:\n\nmalicious software (malware) being downloaded by the host (Figure 7.23)\n\nusing tunneling to gain remote access to run host server or database\n\ncommands or to control system and OS services\n\nThis attack can also be enabled by the attacker obtaining login credentials\n\nto the host computer via a brute force or Wi-Fi deauthentication attack, or",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "via social engineering and insider threats. A remote code execution attack is\n\nusually prefaced by an information gathering process in which the attacker\n\nuses an automated scanning tool to identify vulnerabilities.\n\nFigure 7.23\n\nUsing an already installed malware program, an attacker is able to issue it\n\ncommands to carry out damaging actions on an organization’s server.\n\nThe remote code execution technique can be utilized by other cyber attacks,\n\nsuch as botnet attacks and malware attacks that utilize ransomware or\n\ntrojans.",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "SQL Injection\n\nSQL injection is a technique used to attack applications in which malicious\n\ncode in the form of SQL statements is inserted into an entry field on a web\n\napplication user interface, causing the web application server to execute the\n\nmalicious code (Figure 7.24).\n\nFigure 7.24\n\nAn attacker inserts harmful SQL code into a web application’s user\n\ninterface.\n\nWhen successful, access to the server can be compromised, resulting in\n\nmalware being written into the server’s database. Attackers often use search",
      "content_length": 520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "engines to identify vulnerable sites that can be altered using SQL injection.\n\nNote\n\nSQL (or the Structured Query Language) is a syntax used to\n\nissue commands to a database, such as queries and updates.\n\nData science technologies can be used to support SQL injection attacks by\n\nanalyzing historical SQL commands that have been issued against a given\n\nweb application to better determine which are more or less effective. The\n\ndata science system itself can help generate different combinations of SQL\n\ncode for automated attacks, learning from and improving over time, based\n\non the successful or failed outcome of each code submission.\n\nTunneling\n\nTunneling is a technique whereby data is embedded in an authorized\n\nprotocol packet to bypass firewall controls, allowing sensitive data to exit\n\nthe network and unauthorized or malicious data to enter without ever\n\ntriggering an alert or log entry (Figure 7.25). Tunneling can be difficult to\n\ndetect and block because tunneling packets are designed to adhere to the\n\nrules of firewalls.",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Figure 7.25\n\nAn attacker is able to get a malicious packet through an organization’s\n\nfirewall, thereby enabling the attacker to set up a tunnel to an internal\n\nserver.\n\nIn order to “tunnel” the data, the attacker uses a software program that can\n\npretend to talk to the protocol but, in reality, transfers data for some other\n\npurpose. For example, an established tunnel can be used to place malware\n\non the victim’s computer, such as spyware that remains on a host for a\n\nprolonged period to collect confidential information. It can also be used\n\nwith remote code execution in support of a botnet attack by enabling the",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "attacker to place bots on hosts for the purpose of turning them into zombie\n\nservers.\n\nNote\n\nCommonly used protocols for attacking systems using\n\ntunneling techniques include HTTP, SSH, DNS, and ICMP.\n\nAdvanced Persistent Threat (APT)\n\nAn advanced persistent threat (APT) is a method whereby an attacker uses\n\nmultiple attacks to breach security. Often, the attacks are coordinated to\n\ntake place over a longer period of time (Figure 7.26). APTs require\n\nsophisticated technology and long-term preparation and planning by the\n\nattacker, and are therefore more common with attackers that target high-\n\nvalue organizations.",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Figure 7.26\n\nA set of coordinated attacks is carried out against an organization over a\n\nperiod of time. The different attacks are carried out in a specific sequence\n\nand in support of a common objective.\n\nAn objective behind APTs can be to position resources within an\n\norganization’s environment subsequent to gaining access via a security\n\nbreach. For example, an APT attack may succeed in gaining access to a\n\nnetwork after which the attacker tries to establish a foothold by implanting\n\nmalware that creates backdoors and tunnels that are then used to continue to\n\npersistently attack systems over a longer period of time.\n\nThe attacker may then attempt to deepen access by using techniques, such\n\nas brute force attacks, to gain administrator rights that then enable the\n\nattacker to take control of system resources and perhaps even lock others\n\nout.\n\nBecause successful APT attacks are carried out as a larger campaign over\n\nlonger periods of time, they enable attackers to observe and learn about an",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "environment to discover further ways to harvest information or value (or do\n\nmore damage) than the attacker originally planned.\n\nA critical success factor with APT attacks is often human involvement.\n\nMany APT attacks succeed as a result of an insider threat, which can be a\n\nhuman who (possibly inadvertently) has compromised security via social\n\nengineering or phishing techniques.\n\nNote\n\nGroups of attackers that collectively carry out APT attacks\n\nare known as APT groups. An APT group can include access\n\nbrokers that only obtain and sell access information to\n\nattackers. For example, the use of access brokers is common\n\namong ransomware attackers.\n\n7.5 Case Study Example\n\nDTGOV, acting as a third-party provider to so many\n\ndifferent government organizations, undergoes a review\n\nto identify which threats it will likely be most vulnerable\n\nto.\n\nThe results indicate the following primary concerns:",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "virtualization attacks, because it is a completely new\n\ntype of attack it had not been prepared for before using\n\ncloud services on behalf of its customers,\n\noverlapping trust boundaries, given that all of its\n\ncustomers will now be sharing resources from a cloud\n\nprovider, they will be subject to this new threat,\n\nsocial engineering and phishing, due to the fact that, as a\n\nservice provider, it has no control over the behavior of\n\nthe end users of the systems it runs and manages.\n\nDTGOV plans to mitigate all of these threats by revising\n\nits customer agreements and by utilizing a number of the\n\nsecurity mechanisms covered in Chapters 10 and 11.\n\n7.6 Additional Considerations\n\nThis section provides a diverse checklist of issues and guidelines that relate\n\nto cloud security. The listed considerations are in no particular order.\n\nFlawed Implementations\n\nThe substandard design, implementation, or configuration of cloud service\n\ndeployments can have undesirable consequences, beyond runtime\n\nexceptions and failures. If the cloud provider’s software and/or hardware\n\nhave inherent security flaws or operational weaknesses, attackers can",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "exploit these vulnerabilities to impair the integrity, confidentiality, and/or\n\navailability of cloud provider IT resources and cloud consumer IT resources\n\nhosted by the cloud provider.\n\nFigure 7.27 depicts a poorly implemented cloud service that results in a\n\nserver shutdown. Although in this scenario the flaw is exposed accidentally\n\nby a legitimate cloud service consumer, it could have easily been\n\ndiscovered and exploited by an attacker.\n\nFigure 7.27",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Cloud Service Consumer A’s message triggers a configuration flaw in\n\nCloud Service A, which in turn causes the virtual server that is also hosting\n\nCloud Services B and C to crash.\n\nSecurity Policy Disparity\n\nWhen a cloud consumer places IT resources with a public cloud provider, it\n\nmay need to accept that its traditional information security approach may\n\nnot be identical or even similar to that of the cloud provider. This\n\nincompatibility needs to be assessed to ensure that any data or other IT\n\nassets being relocated to a public cloud are adequately protected. Even\n\nwhen leasing raw infrastructure-based IT resources, the cloud consumer\n\nmay not be granted sufficient administrative control or influence over\n\nsecurity policies that apply to the IT resources leased from the cloud\n\nprovider. This is primarily because those IT resources are still legally\n\nowned by the cloud provider and continue to fall under its responsibility.\n\nFurthermore, with some public clouds, additional third parties, such as\n\nsecurity brokers and certificate authorities, may introduce their own distinct\n\nset of security policies and practices, further complicating any attempts to\n\nstandardize the protection of cloud consumer assets.\n\nContracts\n\nCloud consumers need to carefully examine contracts and SLAs put forth\n\nby cloud providers to ensure that security policies, and other relevant\n\nguarantees, are satisfactory when it comes to asset security. There needs to",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "be clear language that indicates the amount of liability assumed by the\n\ncloud provider and/or the level of indemnity the cloud provider may ask for.\n\nThe greater the assumed liability by the cloud provider, the lower the risk to\n\nthe cloud consumer.\n\nAnother aspect to contractual obligations is where the lines are drawn\n\nbetween cloud consumer and cloud provider assets. A cloud consumer that\n\ndeploys its own solution upon infrastructure supplied by the cloud provider\n\nwill produce a technology architecture comprised of artifacts owned by\n\nboth the cloud consumer and cloud provider. If a security breach (or other\n\ntype of runtime failure) occurs, how is blame determined? Furthermore, if\n\nthe cloud consumer can apply its own security policies to its solution, but\n\nthe cloud provider insists that its supporting infrastructure be governed by\n\ndifferent (and perhaps incompatible) security policies, how can the resulting\n\ndisparity be overcome?\n\nSometimes the best solution is to look for a different cloud provider with\n\nmore compatible contractual terms.\n\nRisk Management\n\nWhen assessing the potential impacts and challenges pertaining to cloud\n\nadoption, cloud consumers are encouraged to perform a formal risk\n\nassessment as part of a risk management strategy. A cyclically executed\n\nprocess used to enhance strategic and tactical security, risk management is\n\ncomprised of a set of coordinated activities for overseeing and controlling",
      "content_length": 1449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "risks. The main activities are generally defined as risk assessment, risk\n\ntreatment, and risk control (Figure 7.28).\n\nRisk Assessment — In the risk assessment stage, the cloud environment is\n\nanalyzed to identify potential vulnerabilities and shortcomings that threats\n\ncan exploit. The cloud provider can be asked to produce statistics and other\n\ninformation about past attacks (successful and unsuccessful) carried out in\n\nits cloud. The identified risks are quantified and qualified according to the\n\nprobability of occurrence and the degree of impact in relation to how the\n\ncloud consumer plans to utilize cloud-based IT resources.",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Figure 7.28",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "The on-going risk management process, which can be initiated from any of\n\nthe three stages.\n\nRisk Treatment — Mitigation policies and plans are designed during the risk\n\ntreatment stage with the intent of successfully treating the risks that were\n\ndiscovered during risk assessment. Some risks can be eliminated, others can\n\nbe mitigated, while others can be dealt with via outsourcing or even\n\nincorporated into the insurance and/or operating loss budgets. The cloud\n\nprovider itself may agree to assume responsibility as part of its contractual\n\nobligations.\n\nRisk Control — The risk control stage is related to risk monitoring, a three-\n\nstep process that is comprised of surveying related events, reviewing these\n\nevents to determine the effectiveness of previous assessments and\n\ntreatments, and identifying any policy adjustment needs. Depending on the\n\nnature of the monitoring required, this stage may be carried out or shared by\n\nthe cloud provider.\n\nThe threat agents and cloud security threats covered in this chapter (as well\n\nas others that may surface) can be identified and documented as part of the\n\nrisk assessment stage. The cloud security and cybersecurity mechanisms\n\ncovered in Chapters 10 and 11 can be documented and referenced as part of\n\nthe corresponding risk treatment.\n\n7.7 Case Study Example",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Based on an assessment of its internal applications, ATN\n\nanalysts identify a set of risks. One such risk is\n\nassociated with the myTrendek application that was\n\nadopted from OTC, a company ATN recently acquired.\n\nThis application includes a feature that analyzes\n\ntelephone and Internet usage, and enables a multi-user\n\nmode that grants varying access rights. Administrators,\n\nsupervisors, auditors, and regular users can therefore be\n\nassigned different privileges. The application’s user-base\n\nencompasses internal users and external users, such as\n\nbusiness partners and contractors.\n\nThe myTrendek application poses a number of security\n\nchallenges pertaining to usage by internal staff:\n\nauthentication does not require or enforce complex\n\npasswords\n\ncommunication with the application is not encrypted\n\nEuropean regulations (ETelReg) require that certain\n\ntypes of data collected by the application be deleted after\n\nsix months\n\nATN is planning to migrate this application to a cloud\n\nvia a PaaS environment, but the weak authentication",
      "content_length": 1043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "threat and the lack of confidentiality supported by the\n\napplication make them reconsider. A subsequent risk\n\nassessment further reveals that if the application is\n\nmigrated to a PaaS environment hosted by a cloud that\n\nresides outside of Europe, local regulations may be in\n\nconflict with ETelReg. Given that the cloud provider is\n\nnot concerned with ETelReg compliance, this could\n\neasily result in monetary penalties being assessed to\n\nATN. Based on the results of the risk assessment, ATN\n\ndecides not to proceed with its cloud migration plan.",
      "content_length": 547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Part II\n\nCloud Computing Mechanisms\n\nChapter 8: Cloud Infrastructure Mechanisms\n\nChapter 9: Specialized Cloud Mechanisms\n\nChapter 10: Cloud and Cyber Security Access-Oriented Mechanisms\n\nChapter 11: Cloud and Cyber Security Data-Oriented Mechanisms\n\nChapter 12: Cloud Management Mechanisms\n\nTechnology mechanisms represent well-defined IT artifacts that are\n\nestablished within the IT industry and commonly distinct to a certain\n\ncomputing model or platform. The technology-centric nature of cloud\n\ncomputing requires the establishment of a formal set of mechanisms that act\n\nas building blocks for cloud technology architectures.\n\nThe chapters in this part of the book define over 50 common cloud\n\ncomputing mechanisms that can be combined in different and alternative\n\nvariations.\n\nSelect mechanisms are further referenced in the architectural models\n\ncovered in Part III: Cloud Computing Architecture.",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Chapter 8\n\nCloud Infrastructure Mechanisms\n\n8.1 Logical Network Perimeter\n\n8.2 Virtual Server\n\n8.3 Hypervisor\n\n8.4 Cloud Storage Device\n\n8.5 Cloud Usage Monitor\n\n8.6 Resource Replication\n\n8.7 Ready-Made Environment\n\n8.8 Container\n\nCloud infrastructure mechanisms are foundational building blocks of cloud\n\nenvironments that establish primary artifacts to form the basis of\n\nfundamental cloud technology architecture.\n\nThe following cloud infrastructure mechanisms are described in this\n\nchapter:\n\nLogical Network Perimeter",
      "content_length": 522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Virtual Server\n\nHypervisor\n\nCloud Storage Device\n\nCloud Usage Monitor\n\nResource Replication\n\nReady-Made Environment\n\nContainer\n\nNot all of these mechanisms are necessarily broad-reaching, nor does each\n\nestablish an individual architectural layer. Instead, they should be viewed as\n\ncore components that are common to cloud platforms.\n\n8.1 Logical Network Perimeter\n\nDefined as the isolation of a network environment from the rest of a\n\ncommunications network, the logical network perimeter establishes a\n\nvirtual network boundary that can encompass and isolate a group of related\n\ncloud-based IT resources that may be physically distributed (Figure 8.1).",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "Figure 8.1\n\nThe dashed line notation used to indicate the boundary of a logical network\n\nperimeter.\n\nThis mechanism can be implemented to:\n\nisolate IT resources in a cloud from non-authorized users",
      "content_length": 197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "isolate IT resources in a cloud from non-users\n\nisolate IT resources in a cloud from cloud consumers\n\ncontrol the bandwidth that is available to isolated IT resources\n\nLogical network perimeters are typically established via network devices\n\nthat supply and control the connectivity of a data center and are commonly\n\ndeployed as virtualized IT environments that include:\n\nVirtual Firewall – An IT resource that actively filters network traffic to and\n\nfrom the isolated network while controlling its interactions with the\n\nInternet.\n\nVirtual Network – Usually acquired through VLANs, this IT resource\n\nisolates the network environment within the data center infrastructure.\n\nFigure 8.2 introduces the notation used to denote these two IT resources.\n\nFigure 8.3 depicts a scenario in which one logical network perimeter\n\ncontains a cloud consumer’s on-premise environment, while another\n\ncontains a cloud provider’s cloud-based environment. These perimeters are\n\nconnected through a VPN that protects communications, since the VPN is\n\ntypically implemented by point-to-point encryption of the data packets sent\n\nbetween the communicating endpoints.",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Figure 8.2\n\nThe symbols used to represent a virtual firewall (top) and a virtual network\n\n(bottom).\n\nFigure 8.3\n\nTwo logical network perimeters surround the cloud consumer and cloud\n\nprovider environments.",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Case Study Example\n\nDTGOV has virtualized its network infrastructure to\n\nproduce a logical network layout favoring network\n\nsegmentation and isolation. Figure 8.4 depicts the logical\n\nnetwork perimeter implemented at each DTGOV data\n\ncenter, as follows:\n\nThe routers that connect to the Internet and extranet are\n\nnetworked to external firewalls, which provide network\n\ncontrol and protection to the furthest external network\n\nboundaries using virtual networks that logically abstract\n\nthe external network and extranet perimeters. Devices\n\nconnected to these network perimeters are loosely\n\nisolated and protected from external users. No cloud\n\nconsumer IT resources are available within these\n\nperimeters.\n\nA logical network perimeter classified as a demilitarized\n\nzone (DMZ) is established between the external\n\nfirewalls and its own firewalls. The DMZ is abstracted\n\nas a virtual network hosting the proxy servers (not\n\nshown in Figure 8.3) that intermediate access to\n\ncommonly used network services (DNS, e-mail, Web",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "portal), as well as Web servers with external\n\nmanagement functions.\n\nThe network traffic leaving the proxy servers passes\n\nthrough a set of management firewalls that isolate the\n\nmanagement network perimeter, which hosts the servers\n\nproviding the bulk of the management services that\n\ncloud consumers can externally access. These services\n\nare provided in direct support of self-service and on-\n\ndemand allocation of cloud-based IT resources.\n\nAll of the traffic to cloud-based IT resources flows\n\nthrough the DMZ to the cloud service firewalls that\n\nisolate every cloud consumer’s perimeter network,\n\nwhich is abstracted by a virtual network that is also\n\nisolated from other networks.\n\nBoth the management perimeter and isolated virtual\n\nnetworks are connected to the intra-data center firewalls,\n\nwhich regulate the network traffic to and from the other\n\nDTGOV data centers that are also connected to intra-\n\ndata center routers at the intra-data center network\n\nperimeter.",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "The virtual firewalls are allocated to and controlled by a\n\nsingle cloud consumer in order to regulate its virtual IT\n\nresource traffic. These IT resources are connected\n\nthrough a virtual network that is isolated from other\n\ncloud consumers. The virtual firewall and the isolated\n\nvirtual network jointly form the cloud consumer’s\n\nlogical network perimeter.",
      "content_length": 359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Figure 8.4\n\nA logical network layout is established through a set of logical network\n\nperimeters using various firewalls and virtual networks.\n\n8.2 Virtual Server\n\nA virtual server is a form of virtualization software that emulates a physical\n\nserver. Virtual servers are used by cloud providers to share the same\n\nphysical server with multiple cloud consumers by providing cloud\n\nconsumers with individual virtual server instances. Figure 8.5 shows three\n\nvirtual servers being hosted by two physical servers. The number of\n\ninstances a given physical server can share is limited by its capacity.",
      "content_length": 597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Figure 8.5\n\nThe first physical server hosts two virtual servers, while the second physical\n\nserver hosts one virtual server.",
      "content_length": 124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Note\n\nThe terms virtual server and virtual machine (VM) are used\n\nsynonymously throughout this book.\n\nThe virtual infrastructure manager (VIM) referenced in this\n\nchapter is described in Chapter 12 as part of the Resource\n\nManagement System section.\n\nAs a commodity mechanism, the virtual server represents the most\n\nfoundational building block of cloud environments. Each virtual server can\n\nhost numerous IT resources, cloud-based solutions, and various other cloud\n\ncomputing mechanisms. The instantiation of virtual servers from image\n\nfiles is a resource allocation process that can be completed rapidly and on-\n\ndemand.\n\nCloud consumers that install or lease virtual servers can customize their\n\nenvironments independently from other cloud consumers that may be using\n\nvirtual servers hosted by the same underlying physical server. Figure 8.6\n\ndepicts a virtual server that hosts a cloud service being accessed by Cloud\n\nService Consumer B, while Cloud Service Consumer A accesses the virtual\n\nserver directly to perform an administration task.",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Figure 8.6\n\nA virtual server hosts an active cloud service and is further accessed by a\n\ncloud consumer for administrative purposes.",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Case Study Example\n\nDTGOV’s IaaS environment contains hosted virtual\n\nservers that were instantiated on physical servers\n\nrunning the same hypervisor software that controls the\n\nvirtual servers. Their VIM is used to coordinate the\n\nphysical servers in relation to the creation of virtual\n\nserver instances. This approach is used at each data\n\ncenter to apply a uniform implementation of the\n\nvirtualization layer.\n\nFigure 8.7 depicts several virtual servers running over\n\nphysical servers, all of which are jointly controlled by a\n\ncentral VIM.",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Figure 8.7\n\nVirtual servers are created via the physical servers’\n\nhypervisors and a central VIM.\n\nIn order to enable the on-demand creation of virtual\n\nservers, DTGOV provides cloud consumers with a set of\n\ntemplate virtual servers that are made available through\n\npre-made VM images.",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "These VM images are files that represent the virtual disk\n\nimages used by the hypervisor to boot the virtual server.\n\nDTGOV enables the template virtual servers to have\n\nvarious initial configuration options that differ, based on\n\noperating system, drivers, and management tools being\n\nused. Some template virtual servers also have additional,\n\npre-installed application server software.\n\nThe following virtual server packages are offered to\n\nDTGOV’s cloud consumers. Each package has different\n\npre-defined performance configurations and limitations:\n\nSmall Virtual Server Instance – 1 virtual processor core,\n\n4 GB of virtual RAM, 20 GB of storage space in the root\n\nfile system\n\nMedium Virtual Server Instance – 2 virtual processor\n\ncores, 8 GB of virtual RAM, 20 GB of storage space in\n\nthe root file system\n\nLarge Virtual Server Instance – 8 virtual processor\n\ncores, 16 GB of virtual RAM, 20 GB of storage space in\n\nthe root file system\n\nMemory Large Virtual Server Instance – 8 virtual\n\nprocessor cores, 64 GB of virtual RAM, 20 GB of",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "storage space in the root file system\n\nProcessor Large Virtual Server Instance – 32 virtual\n\nprocessor cores, 16 GB of virtual RAM, 20 GB of\n\nstorage space in the root file system\n\nUltra-Large Virtual Server Instance – 128 virtual\n\nprocessor cores, 512 GB of virtual RAM, 40 GB of\n\nstorage space in the root file system\n\nAdditional storage capacity can be added to a virtual\n\nserver by attaching a virtual disk from a cloud storage\n\ndevice. All of the template virtual machine images are\n\nstored on a common cloud storage device that is\n\naccessible only through the cloud consumers’\n\nmanagement tools that are used to control the deployed\n\nIT resources. Once a new virtual server needs to be\n\ninstantiated, the cloud consumer can choose the most\n\nsuitable virtual server template from the list of available\n\nconfigurations. A copy of the virtual machine image is\n\nmade and allocated to the cloud consumer, who can then\n\nassume the administrative responsibilities.\n\nThe allocated VM image is updated whenever the cloud\n\nconsumer customizes the virtual server. After the cloud",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "consumer initiates the virtual server, the allocated VM\n\nimage and its associated performance profile is passed to\n\nthe VIM, which creates the virtual server instance from\n\nthe appropriate physical server.\n\nDTGOV uses the process described in Figure 8.8 to\n\nsupport the creation and management of virtual servers\n\nthat have different initial software configurations and\n\nperformance characteristics.",
      "content_length": 399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "Figure 8.8\n\nThe cloud consumer uses the self-service portal to select\n\na template virtual server for creation (1). A copy of the\n\ncorresponding VM image is created in a cloud\n\nconsumer-controlled cloud storage device (2). The cloud\n\nconsumer initiates the virtual server using the usage and\n\nadministration portal (3), which interacts with the VIM\n\nto create the virtual server instance via the underlying\n\nhardware (4). The cloud consumer is able to use and\n\ncustomize the virtual server via other features on the\n\nusage and administration portal (5). (Note that the self-\n\nservice portal and usage and administration portal are\n\nexplained in Chapter 12.)\n\n8.3 Hypervisor\n\nThe hypervisor mechanism is a fundamental part of virtualization\n\ninfrastructure that is primarily used to generate virtual server instances of a\n\nphysical server. A hypervisor is generally limited to one physical server and\n\ncan therefore only create virtual images of that server (Figure 8.9).\n\nSimilarly, a hypervisor can only assign virtual servers it generates to\n\nresource pools that reside on the same underlying physical server. A\n\nhypervisor has limited virtual server management features, such as",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "increasing the virtual server’s capacity or shutting it down. The VIM\n\nprovides a range of features for administering multiple hypervisors across\n\nphysical servers.\n\nFigure 8.9\n\nVirtual servers are created via individual hypervisor on individual physical\n\nservers. All three hypervisors are jointly controlled by the same VIM.\n\nHypervisor software can be installed directly in bare-metal servers and\n\nprovides features for controlling, sharing and scheduling the usage of",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "hardware resources, such as processor power, memory, and I/O. These can\n\nappear to each virtual server’s operating system as dedicated resources.\n\nCase Study Example\n\nDTGOV has established a virtualization platform in\n\nwhich the same hypervisor software product is running\n\non all physical servers. The VIM coordinates the\n\nhardware resources in each data center so that virtual\n\nserver instances can be created from the most expedient\n\nunderlying physical server.\n\nAs a result, cloud consumers are able to lease virtual\n\nservers with auto-scaling features. In order to offer\n\nflexible configurations, the DTGOV virtualization\n\nplatform provides live VM migration of virtual servers\n\namong physical servers inside the same data center. This\n\nis illustrated in Figures 8.10 and 8.11, where a virtual\n\nserver live-migrates from one busy physical server to\n\nanother that is idle, allowing it to scale up in response to\n\nan increase in its workload.",
      "content_length": 945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Figure 8.10",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "A virtual server capable of auto-scaling experiences an\n\nincrease in its workload (1). The VIM decides that the\n\nvirtual server cannot scale up because its underlying\n\nphysical server host is being used by other virtual\n\nservers (2).",
      "content_length": 233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "Figure 8.11\n\nThe VIM commands the hypervisor on the busy physical\n\nserver to suspend execution of the virtual server (3). The\n\nVIM then commands the instantiation of the virtual\n\nserver on the idle physical server. State information\n\n(such as dirty memory pages and processor registers) is\n\nsynchronized via a shared cloud storage device (4). The\n\nVIM commands the hypervisor at the new physical\n\nserver to resume the virtual server processing (5).\n\n8.4 Cloud Storage Device\n\nThe cloud storage device mechanism represents storage devices that are\n\ndesigned specifically for cloud-based provisioning. Instances of these\n\ndevices can be virtualized, similar to how physical servers can spawn\n\nvirtual server images. Cloud storage devices are commonly able to provide\n\nfixed-increment capacity allocation in support of the pay-per-use\n\nmechanism. Cloud storage devices can be exposed for remote access via\n\ncloud storage services.\n\nNote\n\nThis is a parent mechanism that represents cloud storage\n\ndevices in general. There are numerous specialized cloud",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "storage devices, several of which are described in the\n\narchitectural models covered in Part III of this book.\n\nA primary concern related to cloud storage is the security, integrity, and\n\nconfidentiality of data, which becomes more prone to being compromised\n\nwhen entrusted to external cloud providers and other third parties. There\n\ncan also be legal and regulatory implications that result from relocating data\n\nacross geographical or national boundaries. Another issue applies\n\nspecifically to the performance of large databases. LANs provide locally\n\nstored data with network reliability and latency levels that are superior to\n\nthose of WANs.\n\nCloud Storage Levels\n\nCloud storage device mechanisms provide common logical units of data\n\nstorage, such as:\n\nFiles – Collections of data are grouped into files that are located in folders.\n\nBlocks – The lowest level of storage and the closest to the hardware, a block\n\nis the smallest unit of data that is still individually accessible.\n\nDatasets – Sets of data are organized into a table-based, delimited, or record\n\nformat.\n\nObjects – Data and its associated metadata are organized as Web-based\n\nresources.",
      "content_length": 1160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "Each of these data storage levels is commonly associated with a certain type\n\nof technical interface which corresponds to a particular type of cloud\n\nstorage device and cloud storage service used to expose its API (Figure\n\n8.12).",
      "content_length": 229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "Figure 8.12\n\nDifferent cloud service consumers utilize different technologies to interface\n\nwith virtualized cloud storage devices. (Adapted from the CDMI Cloud\n\nStorage Reference Model.)\n\nNetwork Storage Interfaces\n\nLegacy network storage most commonly falls under the category of\n\nnetwork storage interfaces. It includes storage devices in compliance with\n\nindustry standard protocols, such as SCSI for storage blocks and the server\n\nmessage block (SMB), common Internet file system (CIFS), and network\n\nfile system (NFS) for file and network storage. File storage entails storing\n\nindividual data in separate files that can be different sizes and formats and\n\norganized into folders and subfolders. Original files are often replaced by\n\nthe new files that are created when data has been modified.\n\nWhen a cloud storage device mechanism is based on this type of interface,\n\nits data searching and extraction performance will tend to be suboptimal.\n\nStorage processing levels and thresholds for file allocation are usually\n\ndetermined by the file system itself. Block storage requires data to be in a\n\nfixed format (known as a data block), which is the smallest unit that can be\n\nstored and accessed and the storage format closest to hardware. Using either\n\nthe logical unit number (LUN) or virtual volume block-level storage will\n\ntypically have better performance than file-level storage.",
      "content_length": 1391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "Object Storage Interfaces\n\nVarious types of data can be referenced and stored as Web resources. This is\n\nreferred to as object storage, which is based on technologies that can\n\nsupport a range of data and media types. Cloud Storage Device mechanisms\n\nthat implement this interface can typically be accessed via REST or Web\n\nservice-based cloud services using HTTP as the prime protocol. The\n\nStorage Networking Industry Association’s Cloud Data Management\n\nInterface (SNIA’s CDMI) supports the use of object storage interfaces.\n\nDatabase Storage Interfaces\n\nCloud storage device mechanisms based on database storage interfaces\n\ntypically support a query language in addition to basic storage operations.\n\nStorage management is carried out using a standard API or an\n\nadministrative user-interface.\n\nThis classification of storage interface is divided into two main categories\n\naccording to storage structure, as follows.\n\nRelational Data Storage\n\nTraditionally, many on-premise IT environments store data using relational\n\ndatabases or relational database management systems (RDBMSs).\n\nRelational databases (or relational storage devices) rely on tables to\n\norganize similar data into rows and columns. Tables can have relationships\n\nwith each other to give the data increased structure, to protect data integrity,",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "and to avoid data redundancy (which is referred to as data normalization).\n\nWorking with relational storage commonly involves the use of the industry\n\nstandard Structured Query Language (SQL).\n\nA cloud storage device mechanism implemented using relational data\n\nstorage could be based on any number of commercially available database\n\nproducts, such as IBM DB2, Oracle Database, Microsoft SQL Server, and\n\nMySQL.\n\nChallenges with cloud-based relational databases commonly pertain to\n\nscaling and performance. Scaling a relational cloud storage device\n\nvertically can be more complex and cost-ineffective than horizontal scaling.\n\nDatabases with complex relationships and/or containing large volumes of\n\ndata can be afflicted with higher processing overhead and latency,\n\nespecially when accessed remotely via cloud services.\n\nNon-Relational Data Storage\n\nNon-relational storage (also commonly referred to as NoSQL storage)\n\nmoves away from the traditional relational database model in that it\n\nestablishes a “looser” structure for stored data with less emphasis on\n\ndefining relationships and realizing data normalization. The primary\n\nmotivation for using non-relational storage is to avoid the potential\n\ncomplexity and processing overhead that can be imposed by relational\n\ndatabases. Also, non-relational storage can be more horizontally scalable\n\nthan relational storage.",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "The trade-off with non-relational storage is that the data loses much of the\n\nnative form and validation due to limited or primitive schemas or data\n\nmodels. Furthermore, non-relational repositories don’t tend to support\n\nrelational database functions, such as transactions or joins.\n\nNormalized data exported into a non-relational storage repository will\n\nusually become denormalized, meaning that the size of the data will\n\ntypically grow. An extent of normalization can be preserved, but usually not\n\nfor complex relationships. Cloud providers often offer non-relational\n\nstorage that provides scalability and availability of stored data over multiple\n\nserver environments. However, many non-relational storage mechanisms\n\nare proprietary and therefore can severely limit data portability.\n\nCase Study Example\n\nDTGOV provides cloud consumers access to a cloud\n\nstorage device based on an object storage interface. The\n\ncloud service that exposes this API offers basic functions\n\non stored objects, such as search, create, delete, and\n\nupdate. The search function uses a hierarchical object\n\narrangement that resembles a file system. DTGOV\n\nfurther offers a cloud service that is used exclusively\n\nwith virtual servers and enables the creation of cloud\n\nstorage devices via a block storage network interface.",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "Both cloud services use APIs that are compliant with\n\nSNIA’s CDMI v1.0.\n\nThe object-based cloud storage device has an underlying\n\nstorage system with variable storage capacity, which is\n\ndirectly controlled by a software component that also\n\nexposes the interface. This software enables the creation\n\nof isolated cloud storage devices that are allocated to\n\ncloud consumers. The storage system uses a security\n\ncredential management system to administer user-based\n\naccess control to the device’s data objects (Figure 8.13).",
      "content_length": 524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "Figure 8.13\n\nThe cloud consumer interacts with the usage and\n\nadministration portal to create a cloud storage device\n\nand define access control policies (1). The usage and\n\nadministration portal interact with the cloud storage\n\nsoftware to create the cloud storage device instance and\n\napply the required access policy to its data objects (2).\n\nEach data object is assigned to a cloud storage device\n\nand all of the data objects are stored in the same virtual\n\nstorage volume. The cloud consumer uses the\n\nproprietary cloud storage device UI to interact directly\n\nwith the data objects (3). (Note that the usage and\n\nadministration portal is explained in Chapter 12.)\n\nAccess control is granted on a per-object basis and uses\n\nseparate access policies for creating, reading from, and\n\nwriting to each data object. Public access permissions\n\nare allowed, although they are read-only. Access groups\n\nare formed by nominated users that must be previously\n\nregistered via the credential management system. Data\n\nobjects can be accessed from both Web applications and\n\nWeb service interfaces, which are implemented by the\n\ncloud storage software.",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "The creation of the cloud consumers’ block-based cloud\n\nstorage devices is managed by the virtualization\n\nplatform, which instantiates the LUN’s implementation\n\nof the virtual storage (Figure 8.14). The cloud storage\n\ndevice (or the LUN) must be assigned by the VIM to an\n\nexisting virtual server before it can be used. The capacity\n\nof block-based cloud storage devices is expressed by one\n\nGB increments. It can be created as fixed storage that\n\ncloud consumers can modify administratively or as\n\nvariable size storage that has an initial 5 GB capacity\n\nthat automatically increases and decreases by 5 GB\n\nincrements according to usage demands.",
      "content_length": 646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "Figure 8.14\n\nThe cloud consumer uses the usage and administration\n\nportal to create and assign a cloud storage device to an\n\nexisting virtual server (1). The usage and administration\n\nportal interacts with the VIM software (2a), which\n\ncreates and configures the appropriate LUN (2b). Each\n\ncloud storage device uses a separate LUN controlled by\n\nthe virtualization platform. The cloud consumer remotely\n\nlogs into the virtual server directly (3a) to access the\n\ncloud storage device (3b).\n\n8.5 Cloud Usage Monitor\n\nThe cloud usage monitor mechanism is a lightweight and autonomous\n\nsoftware program responsible for collecting and processing IT resource\n\nusage data.\n\nNote\n\nThis is a parent mechanism that represents a broad range of\n\ncloud usage monitors, several of which are established as\n\nspecialized mechanisms in Chapter 9 and several more of\n\nwhich are described in the cloud architectural models\n\ncovered in Part III of this book.",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Depending on the type of usage metrics they are designed to collect and the\n\nmanner in which usage data needs to be collected, cloud usage monitors can\n\nexist in different formats. The upcoming sections describe three common\n\nagent-based implementation formats. Each can be designed to forward\n\ncollected usage data to a log database for post-processing and reporting\n\npurposes.\n\nMonitoring Agent\n\nA monitoring agent is an intermediary, event-driven program that exists as a\n\nservice agent and resides along existing communication paths to\n\ntransparently monitor and analyze dataflows (Figure 8.15). This type of\n\ncloud usage monitor is commonly used to measure network traffic and\n\nmessage metrics.",
      "content_length": 699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Figure 8.15\n\nA cloud service consumer sends a request message to a cloud service (1).\n\nThe monitoring agent intercepts the message to collect relevant usage data\n\n(2) before allowing it to continue to the cloud service (3a). The monitoring\n\nagent stores the collected usage data in a log database (3b). The cloud\n\nservice replies with a response message (4) that is sent back to the cloud\n\nservice consumer without being intercepted by the monitoring agent (5).\n\nResource Agent\n\nA resource agent is a processing module that collects usage data by having\n\nevent-driven interactions with specialized resource software (Figure 8.16).\n\nThis module is used to monitor usage metrics based on pre-defined,",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "observable events at the resource software level, such as initiating,\n\nsuspending, resuming, and vertical scaling.\n\nFigure 8.16\n\nThe resource agent is actively monitoring a virtual server and detects an\n\nincrease in usage (1). The resource agent receives a notification from the\n\nunderlying resource management program that the virtual server is being\n\nscaled up and stores the collected usage data in a log database, as per its\n\nmonitoring metrics (2).",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "Polling Agent\n\nA polling agent is a processing module that collects cloud service usage\n\ndata by polling IT resources. This type of cloud service monitor is\n\ncommonly used to periodically monitor IT resource status, such as uptime\n\nand downtime (Figure 8.17).\n\nFigure 8.17",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "A polling agent monitors the status of a cloud service hosted by a virtual\n\nserver by sending periodic polling request messages and receiving polling\n\nresponse messages that report usage status “A” after a number of polling\n\ncycles, until it receives a usage status of “B” (1), upon which the polling\n\nagent records the new usage status in the log database (2).\n\nCase Study Example\n\nOne of the challenges encountered during DTGOV’s\n\ncloud adoption initiative has been ensuring that their\n\ncollected usage data is accurate. The resource allocation\n\nmethods of previous IT outsourcing models had resulted\n\nin their clients being billed chargeback fees based on the\n\nnumber of physical servers that was listed in annual\n\nleasing contracts, regardless of actual usage.\n\nDTGOV now needs to define a model that allows virtual\n\nservers of varying performance levels to be leased and\n\nbilled hourly. Usage data needs to be at an extremely\n\ngranular level in order to achieve the necessary degree of\n\naccuracy. DTGOV implements a resource agent that\n\nrelies on the resource usage events generated by the VIM\n\nplatform to calculate the virtual server usage data.\n\nThe resource agent is designed with logic and metrics\n\nthat are based on the following rules:",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": ". Each resource usage event that is generated by the VIM\n\nsoftware can contain the following data:\n\nEvent Type (EV_TYPE) – Generated by the VIM\n\nplatform, there are five types of events:\n\nVM Starting (creation at the hypervisor)\n\nVM Started (completion of the boot procedure)\n\nVM Stopping (shutting down)\n\nVM Stopped (termination at the hypervisor)\n\nVM Scaled (change of performance parameters)\n\nVM Type (VM_TYPE) – This represents a type of\n\nvirtual server, as dictated by its performance parameters.\n\nA predefined list of possible virtual server configurations\n\nprovides the parameters that are described by the\n\nmetadata whenever a VM starts or scales.\n\nUnique VM Identifier (VM_ID) – This identifier is\n\nprovided by the VIM platform.\n\nUnique Cloud Consumer Identifier (CS_ID) – Another\n\nidentifier provided by the VIM platform to represent the",
      "content_length": 847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "cloud consumer.\n\nEvent Timestamp (EV_T) – An identification of an event\n\noccurrence that is expressed in date-time format, with\n\nthe time zone of the data center and referenced to UTC\n\nas defined in RFC 3339 (as per the ISO 8601 profile).\n\n. Usage measurements are recorded for every virtual\n\nserver that a cloud consumer creates.\n\n. Usage measurements are recorded for a measurement\n\nperiod whose length is defined by two timestamps called\n\nt\n\nstart\n\nand t\n\nend\n\n. The start of the measurement period\n\ndefaults to the beginning of the calendar month (t\n\nstart\n\n=\n\n2012-12-01T00:00:00-08:00) and finishes at the end of\n\nthe calendar month (t\n\nend\n\n= 2012-12-31T23:59:59-08:00).\n\nCustomized measurement periods are also supported.\n\n. Usage measurements are recorded at each minute of\n\nusage. The virtual server usage measurement period\n\nstarts when the virtual server is created at the hypervisor\n\nand stops at its termination.\n\n. Virtual servers can be started, scaled, and stopped\n\nmultiple times during the measurement period. The time\n\ninterval between each occurrence i (i = 1, 2, 3, …) of",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "these pairs of successive events that are declared for a\n\nvirtual server is called a usage cycle that is known as\n\nT\n\ncycle_i\n\n:\n\nVM_Starting, VM_Stopping – VM size is unchanged at\n\nthe end of the cycle\n\nVM_Starting, VM_Scaled – VM size has changed at the\n\nend of the cycle\n\nVM_Scaled, VM_Scaled – VM size has changed while\n\nscaling, at the end of the cycle\n\nVM_Scaled, VM_Stopping – VM size has changed at\n\nthe end of the cycle\n\n. The total usage, U\n\ntotal\n\n, for each virtual server during the\n\nmeasurement period is calculated using the following\n\nresource usage event log database equations:\n\nFor each VM_TYPE and VM_ID in the log database:\n\nAs per the total usage time that is measured for each\n\nVM_TYPE, the vector of usage for each VM_ID is",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "U\n\ntotal\n\n: U\n\ntotal\n\n= {type 1, U\n\ntotal_VM_type_1\n\n, type 2,\n\nU\n\ntotal_VM_type_2\n\n, …}\n\nFigure 8.18 depicts the resource agent interacting with\n\nthe VIM’s event-driven API.",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "Figure 8.18\n\nThe cloud consumer (CS_ID = CS1) requests the\n\ncreation of a virtual server (VM_ID = VM1) of\n\nconfiguration size type 1 (VM_TYPE = type1) (1). The\n\nVIM creates the virtual server (2a). The VIM’s event-\n\ndriven API generates a resource usage event with\n\ntimestamp = t1, which the cloud usage monitor software\n\nagent captures and records in the resource usage event\n\nlog database (2b). Virtual server usage increases and\n\nreaches the auto-scaling threshold (3). The VIM scales\n\nup Virtual Server VM1 (4a) from configuration type 1 to\n\ntype 2 (VM_TYPE = type2). The VIM’s event-driven API\n\ngenerates a resource usage event with timestamp = t2,\n\nwhich is captured and recorded at the resource usage\n\nevent log database by the cloud usage monitor software\n\nagent (4b). The cloud consumer shuts down the virtual\n\nserver (5). The VIM stops Virtual Server VM1 (6a) and\n\nits event-driven API generates a resource usage event\n\nwith timestamp = t3, which the cloud usage monitor\n\nsoftware agent captures and records at the log database\n\n(6b). The usage and administration portal accesses the",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "log database and calculates the total usage (Utotal) for\n\nVirtual Server Utotal VM1 (7).\n\n8.6 Resource Replication\n\nDefined as the creation of multiple instances of the same IT resource,\n\nreplication is typically performed when an IT resource’s availability and\n\nperformance need to be enhanced. Virtualization technology is used to\n\nimplement the resource replication mechanism to replicate cloud-based IT\n\nresources (Figure 8.19).",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "Figure 8.19\n\nThe hypervisor replicates several instances of a virtual server, using a\n\nstored virtual server image.\n\nNote",
      "content_length": 121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "This is a parent mechanism that represents different types of\n\nsoftware programs capable of replicating IT resources. The\n\nmost common example is the hypervisor mechanism\n\ndescribed in this chapter. For example, the virtualization\n\nplatform’s hypervisor can access a virtual server image to\n\ncreate several instances, or to deploy and replicate ready-\n\nmade environments and entire applications. Other common\n\ntypes of replicated IT resources include cloud service\n\nimplementations and various forms of data and cloud\n\nstorage device replication.\n\nCase Study Example\n\nDTGOV establishes a set of high-availability virtual\n\nservers that can be automatically relocated to physical\n\nservers running in different data centers in response to\n\nsevere failure conditions. This is illustrated in the\n\nscenario depicted in Figures 8.20 to 8.22, where a virtual\n\nserver that resides on a physical server running at one\n\ndata center experiences a failure condition. VIMs from\n\ndifferent data centers coordinate to overcome the\n\nunavailability by reallocating the virtual server to a\n\ndifferent physical server running in another data center.",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Figure 8.20\n\nA high-availability virtual server is running in Data\n\nCenter A. VIM instances in Data Centers A and B are\n\nexecuting a coordination function that allows detection\n\nof failure conditions. Stored VM images are replicated\n\nbetween data centers as a result of the high-availability\n\narchitecture.",
      "content_length": 306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "Figure 8.21\n\nThe virtual server becomes unavailable in Data Center\n\nA. The VIM in Data Center B detects the failure\n\ncondition and starts to reallocate the high-availability\n\nserver from Data Center A to Data Center B.",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "Figure 8.22\n\nA new instance of the virtual server is created and made\n\navailable in Data Center B.\n\n8.7 Ready-Made Environment\n\nThe ready-made environment mechanism (Figure 8.23) is a defining\n\ncomponent of the PaaS cloud delivery model that represents a pre-defined,\n\ncloud-based platform comprised of a set of already installed IT resources,\n\nready to be used and customized by a cloud consumer. These environments\n\nare utilized by cloud consumers to remotely develop and deploy their own\n\nservices and applications within a cloud. Typical ready-made environments\n\ninclude pre-installed IT resources, such as databases, middleware,\n\ndevelopment tools, and governance tools.",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "Figure 8.23\n\nA cloud consumer accesses a ready-made environment hosted on a virtual\n\nserver.\n\nA ready-made environment is generally equipped with a complete software\n\ndevelopment kit (SDK) that provides cloud consumers with programmatic\n\naccess to the development technologies that comprise their preferred\n\nprogramming stacks.\n\nMiddleware is available for multitenant platforms to support the\n\ndevelopment and deployment of Web applications. Some cloud providers",
      "content_length": 463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "offer runtime execution environments for cloud services that are based on\n\ndifferent runtime performance and billing parameters. For example, a front-\n\nend instance of a cloud service can be configured to respond to time-\n\nsensitive requests more effectively than a back-end instance. The former\n\nvariation will be billed at a different rate than the latter.\n\nAs further demonstrated in the upcoming case study example, a solution\n\ncan be partitioned into groups of logic that can be designated for both\n\nfrontend and backend instance invocation so as to optimize runtime\n\nexecution and billing.\n\nCase Study Example\n\nATN developed and deployed several non-critical\n\nbusiness applications using a leased PaaS environment.\n\nOne was a Java-based Part Number Catalog Web\n\napplication used for the switches and routers they\n\nmanufacture. This application is used by different\n\nfactories, but it does not manipulate transaction data,\n\nwhich is instead processed by a separate stock control\n\nsystem.\n\nThe application logic was split into front-end and back-\n\nend processing logic. The front-end logic was used to\n\nprocess simple queries and updates to the catalog. The",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "back-end part contains the logic required to render the\n\ncomplete catalog and correlate similar components and\n\nlegacy part numbers.\n\nFigure 8.24 illustrates the development and deployment\n\nenvironment for ATN’s Part Number Catalog\n\napplication. Note how the cloud consumer assumes both\n\nthe developer and end-user roles.",
      "content_length": 321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Figure 8.24\n\nThe developer uses the provided SDK to develop the\n\nPart Number Catalog Web application (1). The",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "application software is deployed on a Web platform that\n\nwas established by two ready-made environments called\n\nthe front-end instance (2a) and the back-end instance\n\n(2b). The application is made available for usage and\n\none end-user accesses its front-end instance (3). The\n\nsoftware running in the front-end instance invokes a\n\nlong-running task at the back-end instance that\n\ncorresponds to the processing required by the end-user\n\n(4). The application software deployed at both the front-\n\nend and back-end instances is backed by a cloud storage\n\ndevice that provides persistent storage of the application\n\ndata (5).\n\n8.8 Container\n\nContainers can provide an effective means of deploying and delivering\n\ncloud services. A container is represented by a symbol that similar to the\n\norganizational boundary symbol introduced in Roles and Boundaries\n\nsection in Chapter 4, except that it has rounded corners instead of sharp\n\ncorners Containerization technology is explained in Chapter 6.\n\nThe symbol used to represent a container.",
      "content_length": 1032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "Chapter 9\n\nSpecialized Cloud Mechanisms\n\n9.1 Automated Scaling Listener\n\n9.2 Load Balancer\n\n9.3 SLA Monitor\n\n9.4 Pay-Per-Use Monitor\n\n9.5 Audit Monitor\n\n9.6 Failover System\n\n9.7 Resource Cluster\n\n9.8 Multi-Device Broker\n\n9.9 State Management Database\n\nA typical cloud technology architecture contains numerous moving parts to\n\naddress distinct usage requirements of IT resources and solutions. Each\n\nmechanism covered in this chapter fulfills a specific runtime function in\n\nsupport of one or more cloud characteristics.\n\nThe following specialized cloud mechanisms are described in this chapter:",
      "content_length": 595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Automated Scaling Listener\n\nLoad Balancer\n\nSLA Monitor\n\nPay-Per-Use Monitor\n\nAudit Monitor\n\nFailover System\n\nResource Cluster\n\nMulti-Device Broker\n\nState Management Database\n\nAll of these mechanisms can be considered extensions to cloud\n\ninfrastructure, and can be combined in numerous ways as part of distinct\n\nand custom technology architectures, many examples of which are provided\n\nin Part III of this book.\n\n9.1 Automated Scaling Listener\n\nThe automated scaling listener mechanism is a service agent that monitors\n\nand tracks communications between cloud service consumers and cloud\n\nservices for dynamic scaling purposes. Automated scaling listeners are",
      "content_length": 659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "deployed within the cloud, typically near the firewall, from where they\n\nautomatically track workload status information. Workloads can be\n\ndetermined by the volume of cloud consumer-generated requests or via\n\nback-end processing demands triggered by certain types of requests. For\n\nexample, a small amount of incoming data can result in a large amount of\n\nprocessing.\n\nAutomated scaling listeners can provide different types of responses to\n\nworkload fluctuation conditions, such as:\n\nAutomatically scaling IT resources out or in based on parameters previously\n\ndefined by the cloud consumer (commonly referred to as auto-scaling).\n\nAutomatic notification of the cloud consumer when workloads exceed\n\ncurrent thresholds or fall below allocated resources (Figure 9.1). This way,\n\nthe cloud consumer can choose to adjust its current IT resource allocation.\n\nDifferent cloud provider vendors have different names for service agents\n\nthat act as automated scaling listeners.",
      "content_length": 971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Figure 9.1\n\nThree cloud service consumers attempt to access one cloud service\n\nsimultaneously (1). The automated scaling listener scales out and initiates\n\nthe creation of three redundant instances of the service (2). A fourth cloud",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "service consumer attempts to use the cloud service (3). Programmed to\n\nallow up to only three instances of the cloud service, the automated scaling\n\nlistener rejects the fourth attempt and notifies the cloud consumer that the\n\nrequested workload limit has been exceeded (4). The cloud consumer’s\n\ncloud resource administrator accesses the remote administration\n\nenvironment to adjust the provisioning setup and increase the redundant\n\ninstance limit (5).\n\nCase Study Example\n\nNote\n\nThis case study example makes reference to the live VM\n\nmigration component, which is introduced in the\n\nHypervisor Clustering Architecture section in Chapter\n\n14, and further described and demonstrated in\n\nsubsequent architecture scenarios.\n\nDTGOV’s physical servers vertically scale virtual server\n\ninstances, starting with the smallest virtual machine\n\nconfiguration (1 virtual processor core, 4 GB of virtual\n\nRAM) to the largest (128 virtual processor cores, 512\n\nGB of virtual RAM). The virtualization platform is\n\nconfigured to automatically scale a virtual server at\n\nruntime, as follows:",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "Scaling-Down – The virtual server continues residing on\n\nthe same physical host server while being scaled down\n\nto a lower performance configuration.\n\nScaling-Up – The virtual server’s capacity is doubled on\n\nits original physical host server. The VIM may also live\n\nmigrate the virtual server to another physical server if\n\nthe original host server is overcommitted. Migration is\n\nautomatically performed at runtime and does not require\n\nthe virtual server to shut down.\n\nAuto-scaling settings controlled by cloud consumers\n\ndetermine the runtime behavior of automated scaling\n\nlistener agents, which run on the hypervisor that\n\nmonitors the resource usage of the virtual servers. For\n\nexample, one cloud consumer has it set up so that\n\nwhenever resource usage exceeds 80% of a virtual\n\nserver’s capacity for 60 consecutive seconds, the\n\nautomated scaling listener triggers the scaling-up process\n\nby sending the VIM platform a scale-up command.\n\nConversely, the automated scaling listener also\n\ncommands the VIM to scale down whenever resource\n\nusage dips 15% below capacity for 60 consecutive\n\nseconds (Figure 9.2).",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "Figure 9.2",
      "content_length": 10,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "A cloud consumer creates and starts a virtual server\n\nwith 8 virtual processor cores and 16 GB of virtual RAM\n\n(1). The VIM creates the virtual server at the cloud\n\nservice consumer’s request and allocates it to Physical\n\nServer 1 to join 3 other active virtual servers (2). Cloud\n\nconsumer demand causes the virtual server usage to\n\nincrease by over 80% of the CPU capacity for 60\n\nconsecutive seconds (3). The automated scaling listener\n\nrunning at the hypervisor detects the need to scale up\n\nand commands the VIM accordingly (4).\n\nFigure 9.3 illustrates the live migration of a virtual\n\nmachine, as performed by the VIM.",
      "content_length": 624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Figure 9.3\n\nThe VIM determines that scaling up the virtual server on\n\nPhysical Server 1 is not possible and proceeds to live\n\nmigrate it to Physical Server 2.\n\nThe scaling down of the virtual server by the VIM is\n\ndepicted in Figure 9.4.",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "Figure 9.4\n\nThe virtual server’s CPU/RAM usage remains below\n\n15% capacity for 60 consecutive seconds (6). The\n\nautomated scaling listener detects the need to scale\n\ndown and commands the VIM (7), which scales down the",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "virtual server (8) while it remains active on Physical\n\nServer 2.\n\n9.2 Load Balancer\n\nA common approach to horizontal scaling is to balance a workload across\n\ntwo or more IT resources to increase performance and capacity beyond\n\nwhat a single IT resource can provide. The load balancer mechanism is a\n\nruntime agent with logic fundamentally based on this premise.\n\nBeyond simple division of labor algorithms (Figure 9.5), load balancers can\n\nperform a range of specialized runtime workload distribution functions that\n\ninclude:\n\nAsymmetric Distribution – larger workloads are issued to IT resources with\n\nhigher processing capacities\n\nWorkload Prioritization – workloads are scheduled, queued, discarded, and\n\ndistributed workloads according to their priority levels\n\nContent-Aware Distribution – requests are distributed to different IT\n\nresources as dictated by the request content",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "Figure 9.5\n\nA load balancer implemented as a service agent transparently distributes\n\nincoming workload request messages across two redundant cloud service\n\nimplementations, which in turn maximizes performance for the cloud\n\nservice consumers.\n\nA load balancer is programmed or configured with a set of performance and\n\nQoS rules and parameters with the general objectives of optimizing IT\n\nresource usage, avoiding overloads, and maximizing throughput.\n\nThe load balancer mechanisms can exist as a:",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "multi-layer network switch\n\ndedicated hardware appliance\n\ndedicated software-based system (common in server operating systems)\n\nservice agent (usually controlled by cloud management software)\n\nThe load balancer is typically located on the communication path between\n\nthe IT resources generating the workload and the IT resources performing\n\nthe workload processing. This mechanism can be designed as a transparent\n\nagent that remains hidden from the cloud service consumers, or as a proxy\n\ncomponent that abstracts the IT resources performing their workload.\n\nCase Study Example\n\nThe ATN Part Number Catalog cloud service does not\n\nmanipulate transaction data even though it is used by\n\nmultiple factories in different regions. It has peak usage\n\nperiods during the first few days of every month that\n\ncoincide with the preparatory processing of heavy stock\n\ncontrol routines at the factories. ATN followed their\n\ncloud provider’s recommendations and upgraded the\n\ncloud service to be highly scalable in order to support\n\nthe anticipated workload fluctuations.",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "After developing the necessary upgrades, ATN decides\n\nto test the scalability by using a robot automation testing\n\ntool that simulates heavy workloads. The tests need to\n\ndetermine whether the application can seamlessly scale\n\nto serve peak workloads that are 1,000 times greater than\n\ntheir average workloads. The robots proceed to simulate\n\nworkloads that last 10 minutes.\n\nThe application’s resulting auto-scaling functionality is\n\ndemonstrated in Figure 9.6.\n\nFigure 9.6",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "New instances of the cloud services are automatically\n\ncreated to meet increasing usage requests. The load\n\nbalancer uses round-robin scheduling to ensure that the\n\ntraffic is distributed evenly among the active cloud\n\nservices.\n\n9.3 SLA Monitor\n\nThe SLA monitor mechanism is used to specifically observe the runtime\n\nperformance of cloud services to ensure that they are fulfilling the\n\ncontractual QoS requirements that are published in SLAs (Figure 9.7). The\n\ndata collected by the SLA monitor is processed by an SLA management\n\nsystem to be aggregated into SLA reporting metrics. The system can\n\nproactively repair or failover cloud services when exception conditions\n\noccur, such as when the SLA monitor reports a cloud service as “down.”",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "Figure 9.7\n\nThe SLA monitor polls the cloud service by sending over polling request\n\nmessages (M\n\nREQ1\n\nto M\n\nREQN\n\n). The monitor receives polling response\n\nmessages (M\n\nREP1\n\nto M\n\nREPN\n\n) that report that the service was “up” at each\n\npolling cycle (1a). The SLA monitor stores the “up” time—time period of\n\nall polling cycles 1 to N—in the log database (1b).\n\nThe SLA monitor polls the cloud service that sends polling request\n\nmessages (M\n\nREQN+1\n\nto M\n\nREQN+M\n\n). Polling response messages are not\n\nreceived (2a). The response messages continue to time out, so the SLA\n\nmonitor stores the “down” time—time period of all polling cycles N+1 to\n\nN+M—in the log database (2b).\n\nThe SLA monitor sends a polling request message (M\n\nREQN+M+1\n\n) and\n\nreceives the polling response message (M\n\nREPN+M+1\n\n) (3a). The SLA monitor\n\nstores the “up” time in the log database (3b).\n\nThe SLA management system mechanism is discussed in Chapter 12.",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "Case Study Example\n\nThe standard SLA for virtual servers in DTGOV’s\n\nleasing agreements defines a minimum IT resource\n\navailability of 99.95%, which is tracked using two SLA\n\nmonitors: one based on a polling agent and the other\n\nbased on a regular monitoring agent implementation.\n\nSLA Monitor Polling Agent\n\nDTGOV’s polling SLA monitor runs in the external\n\nperimeter network to detect physical server timeouts. It\n\nis able to identify data center network, hardware, and\n\nsoftware failures (with minute-granularity) that result in\n\nphysical server non-responsiveness. Three consecutive\n\ntimeouts of 20-second polling periods are required to\n\ndeclare IT resource unavailability.\n\nThree types of events are generated:\n\nPS_Timeout – the physical server polling has timed out\n\nPS_Unreachable – the physical server polling has\n\nconsecutively timed out three times",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "PS_Reachable – the previously unavailable physical\n\nserver becomes responsive to polling again\n\nSLA Monitoring Agent\n\nThe VIM’s event-driven API implements the SLA\n\nmonitor as a monitoring agent to generate the following\n\nthree events:\n\nVM_Unreachable – the VIM cannot reach the VM\n\nVM Failure – the VM has failed and is unavailable\n\nVM_Reachable – the VM is reachable\n\nThe events generated by the polling agent have\n\ntimestamps that are logged into an SLA event log\n\ndatabase and used by the SLA management system to\n\ncalculate IT resource availability. Complex rules are\n\nused to correlate events from different polling SLA\n\nmonitors and the affected virtual servers, and to discard\n\nany false positives for periods of unavailability.\n\nFigures 9.8 and 9.9 show the steps taken by SLA\n\nmonitors during a data center network failure and\n\nrecovery.",
      "content_length": 847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "Figure 9.8\n\nAt timestamp = t1, a firewall cluster has failed and all of\n\nthe IT resources in the data center become unavailable\n\n(1). The SLA monitor polling agent stops receiving\n\nresponses from physical servers and starts to issue\n\nPS_timeout events (2). The SLA monitor polling agent\n\nstarts issuing PS_unreachable events after three\n\nsuccessive PS_timeout events. The timestamp is now t2\n\n(3).",
      "content_length": 397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "Figure 9.9\n\nThe IT resource becomes operational at timestamp = t3\n\n(4). The SLA monitor polling agent receives responses\n\nfrom the physical servers and issues PS_reachable\n\nevents. The timestamp is now t4 (5). The SLA monitoring\n\nagent did not detect any unavailability since the\n\ncommunication between the VIM platform and physical\n\nservers was not affected by the failure (6).\n\nThe SLA management system uses the information\n\nstored in the log database to calculate the period of\n\nunavailability as t4 – t3, which affected all of the virtual\n\nservers in the data center.\n\nFigures 9.10 and 9.11 illustrate the steps that are taken\n\nby the SLA monitors during the failure and subsequent\n\nrecovery of a physical server that is hosting three virtual\n\nservers (VM1, VM2, VM3).",
      "content_length": 773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Figure 9.10\n\nAt timestamp = t1, the physical host server has failed\n\nand becomes unavailable (1). The SLA monitoring agent\n\ncaptures a VM_unreachable event that is generated for\n\neach virtual server in the failed host server (2a). The\n\nSLA monitor polling agent stops receiving responses\n\nfrom the host server and issues PS_timeout events (2b).\n\nAt timestamp = t2, the SLA monitoring agent captures a\n\nVM_failure event that is generated for each of the failed",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "host server’s three virtual servers (3a). The SLA monitor\n\npolling agent starts to issue PS_unavailable events after\n\nthree successive PS_timeout events at timestamp = t3\n\n(3b).\n\nFigure 9.11\n\nThe host server becomes operational at timestamp = t4\n\n(4). The SLA monitor polling agent receives responses\n\nfrom the physical server and issues PS_reachable events\n\nat timestamp = t5 (5a). At timestamp = t6, the SLA",
      "content_length": 409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "monitoring agent captures a VM_reachable event that is\n\ngenerated for each virtual server (5b). The SLA\n\nmanagement system calculates the unavailability period\n\nthat affected all of the virtual servers as t6 – t2.\n\n9.4 Pay-Per-Use Monitor\n\nThe pay-per-use monitor mechanism measures cloud-based IT resource\n\nusage in accordance with predefined pricing parameters and generates\n\nusage logs for fee calculations and billing purposes.\n\nSome typical monitoring variables are:\n\nrequest/response message quantity\n\ntransmitted data volume\n\nbandwidth consumption\n\nThe data collected by the pay-per-use monitor is processed by a billing\n\nmanagement system that calculates the payment fees. The billing\n\nmanagement system mechanism is covered in Chapter 12.\n\nFigure 9.12 shows a pay-per-use monitor implemented as a resource agent\n\nused to determine the usage period of a virtual server.",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "Figure 9.12\n\nA cloud consumer requests the creation of a new instance of a cloud service\n\n(1). The IT resource is instantiated and the pay-per-use monitor receives a\n\n“start” event notification from the resource software (2). The pay-per-use\n\nmonitor stores the value timestamp in the log database (3). The cloud\n\nconsumer later requests that the cloud service instance be stopped (4). The\n\npay-per-use monitor receives a “stop” event notification from the resource\n\nsoftware (5) and stores the value timestamp in the log database (6).\n\nFigure 9.13 illustrates a pay-per-use monitor designed as a monitoring agent\n\nthat transparently intercepts and analyzes runtime communication with a\n\ncloud service.",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Figure 9.13\n\nA cloud service consumer sends a request message to the cloud service (1).\n\nThe pay-per-use monitor intercepts the message (2), forwards it to the cloud\n\nservice (3a), and stores the usage information in accordance with its\n\nmonitoring metrics (3b). The cloud service forwards the response messages\n\nback to the cloud service consumer to provide the requested service (4).\n\nCase Study Example\n\nDTGOV decides to invest in a commercial system\n\ncapable of generating invoices based on events pre-\n\ndefined as “billable” and customizable pricing models.\n\nThe installation of the system results in two proprietary\n\ndatabases: the billing event database and the pricing\n\nscheme database.\n\nRuntime events are collected via cloud usage monitors\n\nthat are implemented as extensions to the VIM platform\n\nusing the VIM’s API. The pay-per-use monitor polling\n\nagent periodically supplies the billing system with\n\nbillable events information. A separate monitoring agent\n\nprovides further supplemental billing-related data, such\n\nas:",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "Cloud Consumer Subscription Type – This information is\n\nused to identify the type of pricing model for usage fee\n\ncalculations, including pre-paid subscription with usage\n\nquota, post-paid subscription with maximum usage\n\nquota, and post-paid subscription with unlimited usage.\n\nResource Usage Category – The billing management\n\nsystem uses this information to identify the range of\n\nusage fees that are applicable to each usage event.\n\nExamples include normal usage, reserved IT resource\n\nusage, and premium (managed) service usage.\n\nResource Usage Quota Consumption – When usage\n\ncontracts define IT resource usage quotas, usage event\n\nconditions are typically supplemented with quota\n\nconsumption and updated quota limits.\n\nFigure 9.14 illustrates the steps that are taken by\n\nDTGOV’s pay-per-use monitor during a typical usage\n\nevent.",
      "content_length": 838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Figure 9.14\n\nThe cloud consumer (CS_ID = CS1) creates and starts a\n\nvirtual server (VM_ID = VM1) of configuration size type\n\n1 (VM_TYPE = type1) (1). The VIM creates the virtual\n\nserver instance as requested (2a). The VIM’s event-\n\ndriven API generates a resource usage event with",
      "content_length": 280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "timestamp = t1, which is captured and forwarded to the\n\npay-per-use monitor by the cloud usage monitor (2b).\n\nThe pay-per-use monitor interacts with the pricing\n\nscheme database to identify the chargeback and usage\n\nmetrics that apply to the resource usage. A “started\n\nusage” billable event is generated and stored in the\n\nbillable event log database (3). The virtual server’s\n\nusage increases and reaches the auto-scaling threshold\n\n(4). The VIM scales up Virtual Server VM1 (5a) from\n\nconfiguration type 1 to type 2 (VM_TYPE = type2). The\n\nVIM’s event-driven API generates a resource usage\n\nevent with timestamp = t2, which is captured and\n\nforwarded to the pay-per-use monitor by the cloud usage\n\nmonitor (5b). The pay-per-use monitor interacts with the\n\npricing scheme database to identify the chargeback and\n\nusage metrics that apply to the updated IT resource\n\nusage. A “changed usage” billable event is generated\n\nand stored in the billable event log database (6). The\n\ncloud consumer shuts down the virtual server (7) and the\n\nVIM stops Virtual Server VM1 (8a). The VIM’s event-\n\ndriven API generates a resource usage event with\n\ntimestamp = t3, which is captured and forwarded to the\n\npay-per-use monitor by the cloud usage monitor (8b).\n\nThe pay-per-use monitor interacts with the pricing",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "scheme database to identify the chargeback and usage\n\nmetrics that apply to the updated IT resource usage. A\n\n“finished usage” billable event is generated and stored\n\nin the billable event log database (9). The billing system\n\ntool can now be used by the cloud provider to access the\n\nlog database and calculate the total usage fee for the\n\nvirtual server as (Fee(VM1)) (10).\n\n9.5 Audit Monitor\n\nThe audit monitor mechanism is used to collect audit tracking data for\n\nnetworks and IT resources in support of (or dictated by) regulatory and\n\ncontractual obligations. Figure 9.15 depicts an audit monitor implemented\n\nas a monitoring agent that intercepts “login” requests and stores the\n\nrequestor’s security credentials, as well as both failed and successful login\n\nattempts, in a log database for future audit reporting purposes.",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "Figure 9.15\n\nA cloud service consumer requests access to a cloud service by sending a\n\nlogin request message with security credentials (1). The audit monitor\n\nintercepts the message (2) and forwards it to the authentication service (3).\n\nThe authentication service processes the security credentials. A response\n\nmessage is generated for the cloud service consumer, in addition to the\n\nresults from the login attempt (4). The audit monitor intercepts the response\n\nmessage and stores the entire collected login event details in the log\n\ndatabase, as per the organization’s audit policy requirements (5). Access\n\nhas been granted, and a response is sent back to the cloud service consumer\n\n(6).",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Case Study Example\n\nA key feature of Innovartus’ role-playing solution is its\n\nunique user-interface. However, the advanced\n\ntechnologies used for its design have imposed licensing\n\nrestrictions that legally prevent Innovartus from charging\n\nusers in certain geographical regions for usage of the\n\nsolution. Innovartus’ legal department is working on\n\ngetting these issues resolved. But in the meantime, it has\n\nprovided the IT department with a list of countries in\n\nwhich the application can either not be accessed by users\n\nor in which user access needs to be free of charge.\n\nIn order to collect information about the origin of clients\n\naccessing the application, Innovartus asks its cloud\n\nprovider to establish an audit monitoring system. The\n\ncloud provider deploys an audit monitoring agent to\n\nintercept each inbound message, analyze its\n\ncorresponding HTTP header, and collect details about\n\nthe origin of the end-user. As per Innovartus’ request, the\n\ncloud provider further adds a log database to collect the\n\nregional data of each end-user request for future\n\nreporting purposes. Innovartus further upgrades its\n\napplication so that end-users from select countries are\n\nable to access the application at no charge (Figure 9.16).",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Figure 9.16\n\nAn end-user attempts access to the Role Player cloud\n\nservice (1). An audit monitor transparently intercepts the\n\nHTTP request message and analyzes the message header\n\nto determine the geographical origin of the end-user (2).\n\nThe audit monitoring agent determines that the end-user\n\nis from a region that Innovartus is not authorized to\n\ncharge a fee for access to the application. The agent\n\nforwards the message to the cloud service (3a) and\n\ngenerates the audit track information for storage in the",
      "content_length": 515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "log database (3b). The cloud service receives the HTTP\n\nmessage and grants the end-user access at no charge\n\n(4).\n\n9.6 Failover System\n\nThe failover system mechanism is used to increase the reliability and\n\navailability of IT resources by using established clustering technology to\n\nprovide redundant implementations. A failover system is configured to\n\nautomatically switch over to a redundant or standby IT resource instance\n\nwhenever the currently active IT resource becomes unavailable.\n\nFailover systems are commonly used for mission-critical programs and\n\nreusable services that can introduce a single point of failure for multiple\n\napplications. A failover system can span more than one geographical region\n\nso that each location hosts one or more redundant implementations of the\n\nsame IT resource.\n\nThe resource replication mechanism is sometimes utilized by the failover\n\nsystem to provide redundant IT resource instances, which are actively\n\nmonitored for the detection of errors and unavailability conditions.\n\nFailover systems come in two basic configurations:",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "Active-Active\n\nIn an active-active configuration, redundant implementations of the IT\n\nresource actively serve the workload synchronously (Figure 9.17). Load\n\nbalancing among active instances is required. When a failure is detected,\n\nthe failed instance is removed from the load balancing scheduler (Figure\n\n9.18). Whichever IT resource remains operational when a failure is detected\n\ntakes over the processing (Figure 9.19).",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Figure 9.17\n\nThe failover system monitors the operational status of Cloud Service A.\n\nFigure 9.18\n\nWhen a failure is detected in one Cloud Service A implementation, the\n\nfailover system commands the load balancer to switch over the workload to\n\nthe redundant Cloud Service A implementation.",
      "content_length": 290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "Figure 9.19\n\nThe failed Cloud Service A implementation is recovered or replicated into\n\nan operational cloud service. The failover system now commands the load\n\nbalancer to distribute the workload again.\n\nActive-Passive\n\nIn an active-passive configuration, a standby or inactive implementation is\n\nactivated to take over the processing from the IT resource that becomes\n\nunavailable, and the corresponding workload is redirected to the instance\n\ntaking over the operation (Figures 9.20 to 9.22).",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Figure 9.20\n\nThe failover system monitors the operational status of Cloud Service A. The\n\nCloud Service A implementation acting as the active instance is receiving\n\ncloud service consumer requests.",
      "content_length": 197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "Figure 9.21\n\nThe Cloud Service A implementation acting as the active instance\n\nencounters a failure that is detected by the failover system, which\n\nsubsequently activates the inactive Cloud Service A implementation and\n\nredirects the workload toward it. The newly invoked Cloud Service A\n\nimplementation now assumes the role of active instance.",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "Figure 9.22\n\nThe failed Cloud Service A implementation is recovered or replicated an\n\noperational cloud service, and is now positioned as the standby instance,\n\nwhile the previously invoked Cloud Service A continues to serve as the\n\nactive instance.\n\nSome failover systems are designed to redirect workloads to active IT\n\nresources that rely on specialized load balancers that detect failure\n\nconditions and exclude failed IT resource instances from the workload\n\ndistribution. This type of failover system is suitable for IT resources that do\n\nnot require execution state management and provide stateless processing",
      "content_length": 616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "capabilities. In technology architectures that are typically based on\n\nclustering and virtualization technologies, the redundant or standby IT\n\nresource implementations are also required to share their state and\n\nexecution context. A complex task that was executed on a failed IT resource\n\ncan remain operational in one of its redundant implementations.\n\nCase Study Example\n\nDTGOV creates a resilient virtual server to support the\n\nallocation of virtual server instances that are hosting\n\ncritical applications, which are being replicated in\n\nmultiple data centers. The replicated resilient virtual\n\nserver has an associated active-passive failover system.\n\nIts network traffic flow can be switched between the IT\n\nresource instances that are residing at different data\n\ncenters, if the active instance were to fail (Figure 9.23).",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "Figure 9.23\n\nA resilient virtual server is established by replicating the\n\nvirtual server instance across two different data centers,\n\nas performed by the VIM that is running at both data\n\ncenters. The active instance receives the network traffic",
      "content_length": 246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "and is vertically scaling in response, while the standby\n\ninstance has no workload and runs at the minimum\n\nconfiguration.\n\nFigure 9.24 illustrates SLA monitors detecting failure in\n\nan active instance of a virtual server.",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Figure 9.24\n\nSLA monitors detect when the active virtual server\n\ninstance becomes unavailable.\n\nFigure 9.25 shows traffic being switched over to the\n\nstandby instance, which has now become active.",
      "content_length": 196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Figure 9.25\n\nThe failover system is implemented as an event-driven\n\nsoftware agent that intercepts the message notifications\n\nthe SLA monitors send regarding server unavailability.\n\nIn response, the failover system interacts with the VIM",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "and network management tools to redirect all of the\n\nnetwork traffic to the now-active standby instance.\n\nIn Figure 9.26, the failed virtual server becomes\n\noperational and turns into the standby instance.\n\nFigure 9.26",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "The failed virtual server instance is revived and scaled\n\ndown to the minimum standby instance configuration\n\nafter it resumes normal operation.\n\n9.7 Resource Cluster\n\nCloud-based IT resources that are geographically diverse can be logically\n\ncombined into groups to improve their allocation and use. The resource\n\ncluster mechanism (Figure 9.27) is used to group multiple IT resource\n\ninstances so that they can be operated as a single IT resource. This increases\n\nthe combined computing capacity, load balancing, and availability of the\n\nclustered IT resources.\n\nFigure 9.27\n\nThe curved dashed lines are used to indicate that IT resources are clustered.\n\nResource cluster architectures rely on high-speed dedicated network\n\nconnections, or cluster nodes, between IT resource instances to\n\ncommunicate about workload distribution, task scheduling, data sharing,\n\nand system synchronization. A cluster management platform that is running\n\nas distributed middleware in all of the cluster nodes is usually responsible\n\nfor these activities. This platform implements a coordination function that",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "allows distributed IT resources to appear as one IT resource, and also\n\nexecutes IT resources inside the cluster.\n\nCommon resource cluster types include:\n\nServer Cluster – Physical or virtual servers are clustered to increase\n\nperformance and availability. Hypervisors running on different physical\n\nservers can be configured to share virtual server execution state (such as\n\nmemory pages and processor register state) in order to establish clustered\n\nvirtual servers. In such configurations, which usually require physical\n\nservers to have access to shared storage, virtual servers are able to live-\n\nmigrate from one to another. In this process, the virtualization platform\n\nsuspends the execution of a given virtual server at one physical server and\n\nresumes it on another physical server. The process is transparent to the\n\nvirtual server operating system and can be used to increase scalability by\n\nlive-migrating a virtual server that is running at an overloaded physical\n\nserver to another physical server that has suitable capacity.\n\nDatabase Cluster – Designed to improve data availability, this high-\n\navailability resource cluster has a synchronization feature that maintains the\n\nconsistency of data being stored at different storage devices used in the\n\ncluster. The redundant capacity is usually based on an active-active or\n\nactive-passive failover system committed to maintaining the\n\nsynchronization conditions.",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "Large Dataset Cluster – Data partitioning and distribution is implemented\n\nso that the target datasets can be efficiently partitioned without\n\ncompromising data integrity or computing accuracy. Each cluster node\n\nprocesses workloads without communicating with other nodes as much as\n\nin other cluster types.\n\nMany resource clusters require cluster nodes to have almost identical\n\ncomputing capacity and characteristics in order to simplify the design of\n\nand maintain consistency within the resource cluster architecture. The\n\ncluster nodes in high-availability cluster architectures need to access and\n\nshare common storage IT resources. This can require two layers of\n\ncommunication between the nodes—one for accessing the storage device\n\nand another to execute IT resource orchestration (Figure 9.28). Some\n\nresource clusters are designed with more loosely coupled IT resources that\n\nonly require the network layer (Figure 9.29).",
      "content_length": 932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "Figure 9.28\n\nLoad balancing and resource replication are implemented through a\n\ncluster-enabled hypervisor. A dedicated storage area network is used to\n\nconnect the clustered storage and the clustered servers, which are able to",
      "content_length": 227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "share common cloud storage devices. This simplifies the storage replication\n\nprocess, which is independently carried out at the storage cluster. (See the\n\nHypervisor Clustering Architecture section in Chapter 14 for a more\n\ndetailed description.)\n\nFigure 9.29\n\nA loosely coupled server cluster that incorporates a load balancer. There is\n\nno shared storage. Resource replication is used to replicate cloud storage\n\ndevices through the network by the cluster software.\n\nThere are two basic types of resource clusters:\n\nLoad Balanced Cluster – This resource cluster specializes in distributing\n\nworkloads among cluster nodes to increase IT resource capacity while",
      "content_length": 661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "preserving the centralization of IT resource management. It usually\n\nimplements a load balancer mechanism that is either embedded within the\n\ncluster management platform or set up as a separate IT resource.\n\nHA Cluster – A high-availability cluster maintains system availability in the\n\nevent of multiple node failures, and has redundant implementations of most\n\nor all of the clustered IT resources. It implements a failover system\n\nmechanism that monitors failure conditions and automatically redirects the\n\nworkload away from any failed nodes.\n\nThe provisioning of clustered IT resources can be considerably more\n\nexpensive than the provisioning of individual IT resources that have an\n\nequivalent computing capacity.\n\nCase Study Example\n\nDTGOV is considering introducing a clustered virtual\n\nserver to run in a high--availability cluster as part of the\n\nvirtualization platform (Figure 9.30). The virtual servers\n\ncan live migrate among the physical servers, which are\n\npooled in a high-availability hardware cluster that is\n\ncontrolled by coordinated cluster-enabled hypervisors.\n\nThe coordination function keeps replicated snapshots of\n\nthe running virtual servers to facilitate migration to other\n\nphysical servers in the event of a failure.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "Figure 9.30\n\nAn HA virtualization cluster of physical servers is\n\ndeployed using a cluster-enabled hypervisor, which\n\nguarantees that the physical servers are constantly in\n\nsync. Every virtual server that is instantiated in the\n\ncluster is automatically replicated in at least two\n\nphysical servers.\n\nFigure 9.31 identifies the virtual servers that are\n\nmigrated from their failed physical host server to other",
      "content_length": 411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "available physical servers.\n\nFigure 9.31\n\nAll of the virtual servers that are hosted on a physical\n\nserver experiencing failure are automatically migrated\n\nto other physical servers.\n\n9.8 Multi-Device Broker\n\nAn individual cloud service may need to be accessed by a range of cloud\n\nservice consumers differentiated by their hosting hardware devices and/or",
      "content_length": 355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "communication requirements. To overcome incompatibilities between a\n\ncloud service and a disparate cloud service consumer, mapping logic needs\n\nto be created to transform (or convert) information that is exchanged at\n\nruntime.\n\nThe multi-device broker mechanism is used to facilitate runtime data\n\ntransformation so as to make a cloud service accessible to a wider range of\n\ncloud service consumer programs and devices (Figure 9.32).\n\nFigure 9.32",
      "content_length": 446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "A multi-device broker contains the mapping logic necessary to transform\n\ndata exchanges between a cloud service and different types of cloud service\n\nconsumer devices. This scenario depicts the multi-device broker as a cloud\n\nservice with its own API. This mechanism can also be implemented as a\n\nservice agent that intercepts messages at runtime to perform necessary\n\ntransformations.\n\nMulti-device brokers commonly exist as gateways or incorporate gateway\n\ncomponents, such as:\n\nXML Gateway – transmits and validates XML data\n\nCloud Storage Gateway – transforms cloud storage protocols and encodes\n\nstorage devices to facilitate data transfer and storage\n\nMobile Device Gateway – transforms the communication protocols used by\n\nmobile devices into protocols that are compatible with a cloud service\n\nThe levels at which transformation logic can be created include:\n\ntransport protocols\n\nmessaging protocols\n\nstorage device protocols\n\ndata schemas/data models",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "For example, a multi-device broker may contain mapping logic that coverts\n\nboth transport and messaging protocols for a cloud service consumer\n\naccessing a cloud service with a mobile device.\n\nCase Study Example\n\nInnovartus has decided to make its role-playing\n\napplication available to various mobile and smartphone\n\ndevices. A complication that hindered Innovartus’\n\ndevelopment team during the mobile enhancement\n\ndesign stage was the difficulty in reproducing identical\n\nuser experiences across different mobile platforms. To\n\nresolve this issue, Innovartus implements a multi-device\n\nbroker to intercept incoming messages from devices,\n\nidentify the software platform, and convert the message\n\nformat into the native, server-side application format\n\n(Figure 9.33).",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Figure 9.33\n\nThe multi-device broker intercepts incoming messages\n\nand detects the platform (Web browser, iOS, Android) of\n\nthe source device (1). The multi-device broker\n\ntransforms the message into the standard format\n\nrequired by the Innovartus cloud service (2). The cloud\n\nservice processes the request and responds using the\n\nsame standard format (3). The multi-device broker\n\ntransforms the response message into the format\n\nrequired by the source device and delivers the message\n\n(4).\n\n9.9 State Management Database",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "A state management database is a storage device that is used to temporarily\n\npersist state data for software programs. As an alternative to caching state\n\ndata in memory, software programs can off-load state data to the database in\n\norder to reduce the amount of runtime memory they consume (Figures 9.34\n\nand 9.35). By doing so, the software programs and the surrounding\n\ninfrastructure are more scalable. State management databases are\n\ncommonly used by cloud services, especially those involved in long-\n\nrunning runtime activities.\n\nFigure 9.34\n\nDuring the lifespan of a cloud service instance it may be required to remain\n\nstateful and keep state data cached in memory, even when idle.",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "Figure 9.35\n\nBy deferring state data to a state data repository, the cloud service is able\n\nto transition to a stateless condition (or a partially stateless condition),\n\nthereby temporarily freeing system resources.\n\nCase Study Example\n\nATN is expanding its ready-made environment\n\narchitecture to allow for the deferral of state information\n\nfor extended periods by utilizing the state management\n\ndatabase mechanism. Figure 9.36 demonstrates how a\n\ncloud service consumer working with a ready-made",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "environment pauses activity, causing the environment to\n\noff-load cached state data.",
      "content_length": 84,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "Figure 9.36\n\nThe cloud consumer accesses the ready-made\n\nenvironment and requires three virtual servers to\n\nperform all activities (1). The cloud consumer pauses\n\nactivity. All of the state data needs to be preserved for\n\nfuture access to the ready-made environment (2). The\n\nunderlying infrastructure is automatically scaled in by\n\nreducing the number of virtual servers. State data is\n\nsaved in the state management database and one virtual\n\nserver remains active to allow for future logins by the\n\ncloud consumer (3). At a later point, the cloud consumer\n\nlogs in and accesses the ready-made environment to\n\ncontinue activity (4). The underlying infrastructure is\n\nautomatically scaled out by increasing the number of\n\nvirtual servers and by retrieving the state data from the\n\nstate management database (5).",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "Chapter 10\n\nCloud and Cybersecurity Access-Oriented Mechanisms\n\n10.1 Encryption\n\n10.2 Hashing\n\n10.3 Digital Signature\n\n10.4 Cloud-Based Security Groups\n\n10.5 Public Key Infrastructure (PKI) System\n\n10.6 Single Sign-On (SSO) System\n\n10.7 Hardened Virtual Server Image\n\n10.8 Firewall\n\n10.9 Virtual Private Network (VPN)\n\n10.10 Biometric Scanner\n\n10.11 Multi-Factor Authentication (MFA) System\n\n10.12 Identity and Access Management (IAM) System",
      "content_length": 441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "10.13 Intrusion Detection System (IDS)\n\n10.14 Penetration Testing Tool\n\n10.15 User Behavior Analytics (UBA) System\n\n10.16 Third-Party Software Update Utility\n\n10.17 Network Intrusion Monitor\n\n10.18 Authentication Log Monitor\n\n10.19 VPN Monitor\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nThis section describes the following mechanisms that are focused on\n\nestablishing cloud access controls that cloud access monitoring functions.\n\nEncryption\n\nHashing\n\nDigital Signature\n\nCloud-Based Security Groups",
      "content_length": 534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "Public Key Infrastructure (PKI) System\n\nSingle Sign-On (SSO) System\n\nHardened Virtual Server Image\n\nFirewall\n\nVirtual Private Network (VPN)\n\nBiometric Scanner\n\nMulti-Factor Authentication (MFA) System\n\nIdentity and Access Management (IAM) System\n\nIntrusion Detection System (IDS)\n\nPenetration Testing Tool\n\nUser Behavior Analytics (UBA) System\n\nThird-Party Software Update Utility\n\nNetwork Intrusion Monitor\n\nAuthentication Log Monitor\n\nVPN Monitor",
      "content_length": 448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "10.1 Encryption\n\nData, by default, is coded in a readable format known as plaintext. When\n\ntransmitted over a network, plaintext is vulnerable to unauthorized and\n\npotentially malicious access. The encryption mechanism is a digital coding\n\nsystem dedicated to preserving the confidentiality and integrity of data. It is\n\nused for encoding plaintext data into a protected and unreadable format.\n\nEncryption technology commonly relies on a standardized algorithm called\n\na cipher to transform original plaintext data into encrypted data, referred to\n\nas ciphertext. Access to ciphertext does not divulge the original plaintext\n\ndata, apart from some forms of metadata, such as message length and\n\ncreation date. When encryption is applied to plaintext data, the data is\n\npaired with a string of characters called an encryption key, a secret message\n\nthat is established by and shared among authorized parties. The encryption\n\nkey is used to decrypt the ciphertext back into its original plaintext format.\n\nThe encryption mechanism can help counter the traffic eavesdropping,\n\nmalicious intermediary, insufficient authorization, and overlapping trust\n\nboundaries security threats. For example, malicious service agents that\n\nattempt traffic eavesdropping are unable to decrypt messages in transit if\n\nthey do not have the encryption key (Figure 10.1).",
      "content_length": 1348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "Figure 10.1\n\nA malicious service agent is unable to retrieve data from an encrypted\n\nmessage. The retrieval attempt may furthermore be revealed to the cloud\n\nservice consumer. (Note the use of the lock symbol to indicate that a\n\nsecurity mechanism has been applied to the message contents.)\n\nThere are two common forms of encryption known as symmetric encryption\n\nand asymmetric encryption.\n\nSymmetric Encryption\n\nSymmetric encryption uses the same key for both encryption and\n\ndecryption, both of which are performed by authorized parties that use the\n\none shared key. Also known as secret key cryptography, messages that are\n\nencrypted with a specific key can be decrypted by only that same key.\n\nParties that rightfully decrypt the data are provided with evidence that the\n\noriginal encryption was performed by parties that rightfully possess the key.",
      "content_length": 854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "A basic authentication check is always performed, because only authorized\n\nparties that own the key can create messages. This maintains and verifies\n\ndata confidentiality.\n\nNote that symmetrical encryption does not have the characteristic of non-\n\nrepudiation, since determining exactly which party performed the message\n\nencryption or decryption is not possible if more than one party is in\n\npossession of the key.\n\nAsymmetric Encryption\n\nAsymmetric encryption relies on the use of two different keys, namely a\n\nprivate key and a public key. With asymmetric encryption (which is also\n\nreferred to as public key cryptography), the private key is known only to its\n\nowner while the public key is commonly available. A document that was\n\nencrypted with a private key can only be correctly decrypted with the\n\ncorresponding public key. Conversely, a document that was encrypted with\n\na public key can be decrypted only using its private key counterpart. As a\n\nresult of two different keys being used instead of just the one, asymmetric\n\nencryption is almost always computationally slower than symmetric\n\nencryption.\n\nThe level of security that is achieved is dictated by whether a private key or\n\npublic key was used to encrypt the plaintext data. As every asymmetrically\n\nencrypted message has its own private-public key pair, messages that were\n\nencrypted with a private key can be correctly decrypted by any party with",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "the corresponding public key. This method of encryption does not offer any\n\nconfidentiality protection, even though successful decryption proves that\n\nthe text was encrypted by the rightful private key owner. Private key\n\nencryption therefore offers integrity protection in addition to authenticity\n\nand non-repudiation. A message that was encrypted with a public key can\n\nonly be decrypted by the rightful private key owner, which provides\n\nconfidentiality protection. However, any party that has the public key can\n\ngenerate the ciphertext, meaning this method provides neither message\n\nintegrity nor authenticity protection due to the communal nature of the\n\npublic key.\n\nNote\n\nThe encryption mechanism, when used to secure web-based\n\ndata transmissions, is most commonly applied via HTTPS,\n\nwhich refers to the use of SSL/TLS as an underlying\n\nencryption protocol for HTTP. TLS (transport layer security)\n\nis the successor to the SSL (secure sockets layer)\n\ntechnology. Because asymmetric encryption is usually more\n\ntime-consuming than symmetric encryption, TLS uses the\n\nformer only for its key exchange method. TLS systems then\n\nswitch to symmetric encryption once the keys have been\n\nexchanged.",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "Most TLS implementations primarily support RSA as the\n\nchief asymmetrical encryption cipher, while ciphers such as\n\nRC4, Triple-DES, and AES are supported for symmetrical\n\nencryption.\n\nCase Study Example\n\nInnovartus has recently learned that users who access\n\ntheir User Registration Portal via public Wi-Fi hot zones\n\nand unsecured LANs may be transmitting personal user\n\nprofile details via plaintext. Innovartus immediately\n\nremedies this vulnerability by applying the encryption\n\nmechanism to its web portal via the use of HTTPS\n\n(Figure 10.2).",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "Figure 10.2",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "The encryption mechanism is added to the\n\ncommunication channel between outside users and\n\nInnovartus’s User Registration Portal. This safeguards\n\nmessage confidentiality via the use of HTTPS.\n\n10.2 Hashing\n\nThe hashing mechanism is used when a one-way, non-reversible form of\n\ndata protection is required. Once hashing has been applied to a message, it\n\nis locked and no key is provided for the message to be unlocked. A common\n\napplication of this mechanism is the storage of passwords.\n\nHashing technology can be used to derive a hashing code or message digest\n\nfrom a message, which is often of a fixed length and smaller than the\n\noriginal message. The message sender can then utilize the hashing\n\nmechanism to attach the message digest to the message. The recipient\n\napplies the same hash function to the message to verify that the produced\n\nmessage digest is identical to the one that accompanied the message. Any\n\nalteration to the original data results in an entirely different message digest\n\nand clearly indicates that tampering has occurred.\n\nIn addition to its utilization for protecting stored data, the cloud threats that\n\ncan be mitigated by the hashing mechanism include malicious intermediary\n\nand insufficient authorization. An example of the former is illustrated in\n\nFigure 10.3.",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "Figure 10.3\n\nA hashing function is applied to protect the integrity of a message that is\n\nintercepted and altered by a malicious service agent, before it is forwarded.\n\nThe firewall can be configured to determine that the message has been\n\naltered, thereby enabling it to reject the message before it can proceed to\n\nthe cloud service.",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "Case Study Example\n\nA subset of the applications that have been selected to be\n\nported to ATN’s PaaS platform allows users to access\n\nand alter highly sensitive corporate data. This\n\ninformation is being hosted on a cloud to enable access\n\nby trusted partners who may use it for critical calculation\n\nand assessment purposes. Concerned that the data could\n\nbe tampered with, ATN decides to apply the hashing\n\nmechanism as a means of protecting and preserving the\n\ndata’s integrity.\n\nATN cloud resource administrators work with the cloud\n\nprovider to incorporate a digest-generating procedure\n\nwith each application version that is deployed in the\n\ncloud. Current values are logged to a secure database on-\n\npremises and the procedure is regularly repeated with the\n\nresults analyzed. Figure 10.4 illustrates how ATN\n\nimplements hashing to determine whether any non-\n\nauthorized actions have been performed against the\n\nported applications.",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "Figure 10.4\n\nA hashing procedure is invoked when the PaaS\n\nenvironment is accessed (1). The applications that were\n\nported to this environment are checked (2) and their\n\nmessage digests are calculated (3). The message digests\n\nare stored in a secure database on-premises (4), and a",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "notification is issued if any of their values are not\n\nidentical to the ones in storage.\n\n10.3 Digital Signature\n\nThe digital signature mechanism is a means of providing data authenticity\n\nand integrity through authentication and non-repudiation. A message is\n\nassigned a digital signature prior to transmission, which is then rendered\n\ninvalid if the message experiences any subsequent, unauthorized\n\nmodifications. A digital signature provides evidence that the message\n\nreceived is the same as the one created by its rightful sender.\n\nBoth hashing and asymmetrical encryption are involved in the creation of a\n\ndigital signature, which essentially exists as a message digest that was\n\nencrypted by a private key and appended to the original message. The\n\nrecipient verifies the signature validity and uses the corresponding public\n\nkey to decrypt the digital signature, which produces the message digest. The\n\nhashing mechanism can also be applied to the original message to produce\n\nthis message digest. Identical results from the two different processes\n\nindicate that the message maintained its integrity.\n\nThe digital signature mechanism helps mitigate the malicious intermediary,\n\ninsufficient authorization, and overlapping trust boundaries security threats\n\n(Figure 10.5).",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "Figure 10.5\n\nCloud Service Consumer B sends a message that was digitally signed but\n\nwas altered by trusted attacker Cloud Service Consumer A. Virtual Server B\n\nis configured to verify digital signatures before processing incoming\n\nmessages even if they are within its trust boundary. The message is revealed",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "as illegitimate due to its invalid digital signature, and is therefore rejected\n\nby Virtual Server B.\n\nCase Study Example\n\nWith DTGOV’s client portfolio expanding to include\n\npublic-sector organizations, many of its cloud computing\n\npolicies have become unsuitable and require\n\nmodification. Considering that public-sector\n\norganizations frequently handle strategic information,\n\nsecurity safeguards need to be established to protect data\n\nmanipulation and to establish a means of auditing\n\nactivities that may impact government operations.\n\nDTGOV proceeds to implement the digital signature\n\nmechanism specifically to protect its web-based\n\nmanagement environment (Figure 10.6). Virtual server\n\nself-provisioning inside the IaaS environment and the\n\ntracking functionality of realtime SLA and billing are all\n\nperformed via web portals. As a result, user error or\n\nmalicious actions could result in legal and financial\n\nconsequences.",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "Figure 10.6\n\nWhenever a cloud consumer performs a management\n\naction that is related to IT resources provisioned by\n\nDTGOV, the cloud service consumer program must\n\ninclude a digital signature in the message request to\n\nprove the legitimacy of its user.\n\nDigital signatures provide DTGOV with the guarantee\n\nthat every action performed is linked to its legitimate\n\noriginator. Unauthorized access is expected to become\n\nhighly improbable, since digital signatures are only\n\naccepted if the encryption key is identical to the secret\n\nkey held by the rightful owner. Users will not have\n\ngrounds to deny attempts at message adulteration\n\nbecause the digital signatures will confirm message\n\nintegrity.\n\n10.4 Cloud-Based Security Groups\n\nSimilar to constructing dykes and levees that separate land from water, data\n\nprotection is increased by placing barriers between IT resources. Cloud",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "resource segmentation is a process by which separate physical and virtual\n\nIT environments are created for different users and groups. For example, an\n\norganization’s WAN can be partitioned according to individual network\n\nsecurity requirements. One network can be established with a resilient\n\nfirewall for external internet access, while a second is deployed without a\n\nfirewall because its users are internal and unable to access the internet.\n\nResource segmentation is used to enable virtualization by allocating a\n\nvariety of physical IT resources to virtual machines. It needs to be\n\noptimized for public cloud environments, since organizational trust\n\nboundaries from different cloud consumers overlap when sharing the same\n\nunderlying physical IT resources.\n\nThe cloud-based resource segmentation process creates cloud-based\n\nsecurity group mechanisms that are determined through security policies.\n\nNetworks are segmented into logical cloud-based security groups that form\n\nlogical network perimeters. Each cloud-based IT resource is assigned to at\n\nleast one logical cloud-based security group. Each logical cloud-based\n\nsecurity group is assigned specific rules that govern the communication\n\nbetween the security groups.\n\nMultiple virtual servers running on the same physical server can become\n\nmembers of different logical cloud-based security groups (Figure 10.7).\n\nVirtual servers can further be separated into public-private groups,",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "development-production groups, or any other designation configured by the\n\ncloud resource administrator.",
      "content_length": 104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "Figure 10.7\n\nCloud-Based Security Group A encompasses Virtual Servers A and D and is\n\nassigned to Cloud Consumer A. Cloud-Based Security Group B is\n\ncomprised of Virtual Servers B, C, and E and is assigned to Cloud\n\nConsumer B. If Cloud Service Consumer A’s credentials are compromised,\n\nthe attacker would only be able to access and damage the virtual servers in\n\nCloud-Based Security Group A, thereby protecting Virtual Servers B, C, and\n\nE.\n\nCloud-based security groups delineate areas where different security\n\nmeasures can be applied. Properly implemented cloud-based security\n\ngroups help limit unauthorized access to IT resources in the event of a\n\nsecurity breach. This mechanism can be used to help counter the denial of\n\nservice, insufficient authorization, overlapping trust boundaries,\n\nvirtualization attack and container attack threats, and is closely related to\n\nthe logical network perimeter mechanism.\n\nCase Study Example\n\nNow that DTGOV has itself become a cloud provider,\n\nsecurity concerns are raised pertaining to its hosting of\n\npublic-sector client data. A team of cloud security\n\nspecialists is brought in to define cloud-based security\n\ngroups together with the digital signature and PKI\n\nmechanisms.",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "Security policies are classified into levels of resource\n\nsegmentation before being integrated into DTGOV’s\n\nweb portal management environment. Consistent with\n\nthe security requirements guaranteed by its SLAs,\n\nDTGOV maps IT resource allocation to the appropriate\n\nlogical cloud-based security group (Figure 10.8), which\n\nhas its own security policy that clearly stipulates its IT\n\nresource isolation and control levels.",
      "content_length": 421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "Figure 10.8\n\nWhen an external cloud resource administrator accesses\n\nthe web portal to allocate a virtual server, the requested\n\nsecurity credentials are assessed and mapped to an\n\ninternal security policy that assigns a corresponding\n\ncloud-based security group to the new virtual server.\n\nDTGOV informs its clients about the availability of\n\nthese new security policies. Cloud consumers can\n\noptionally choose to utilize them and doing so results in\n\nincreased fees.\n\n10.5 Public Key Infrastructure (PKI) System\n\nA common approach for managing the issuance of asymmetric keys is\n\nbased on the public key infrastructure (PKI) system mechanism, which\n\nexists as a system of protocols, data formats, rules, and practices that enable\n\nlarge-scale systems to securely use public key cryptography. This system is\n\nused to associate public keys with their corresponding key owners (known\n\nas public key identification) while enabling the verification of key validity.\n\nPKI systems rely on the use of digital certificates, which are digitally signed\n\ndata structures that bind public keys to certificate owner identities, as well\n\nas to related information, such as validity periods. Digital certificates are\n\nusually digitally signed by a third-party certificate authority (CA), as\n\nillustrated in Figure 10.9.",
      "content_length": 1305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "Other methods of generating digital signatures can be employed, even\n\nthough the majority of digital certificates are issued by only a handful of\n\ntrusted CAs like VeriSign and Comodo. Larger organizations, such as\n\nMicrosoft, can act as their own CA and issue certificates to their clients and\n\nthe public, since even individual users can generate certificates as long as\n\nthey have the appropriate software tools.\n\nBuilding up an acceptable level of trust for a CA is time-intensive but\n\nnecessary. Rigorous security measures, substantial infrastructure\n\ninvestments, and stringent operational processes all contribute to\n\nestablishing the credibility of a CA. The higher its level of trust and\n\nreliability, the more esteemed and reputable its certificates. The PKI system\n\nis a dependable method for implementing asymmetric encryption, managing\n\ncloud consumer and cloud provider identity information, and helping to\n\ndefend against the malicious intermediary and insufficient authorization\n\nthreats.\n\nThe PKI system mechanism is primarily used to counter the insufficient\n\nauthorization threat.",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "Figure 10.9\n\nThe common steps involved during the generation of certificates by a\n\ncertificate authority.\n\nCase Study Example\n\nDTGOV requires that its clients use digital signatures to\n\naccess its web-based management environment. These\n\nare to be generated from public keys that have been\n\ncertified by a recognized certificate authority (Figure\n\n10.10).",
      "content_length": 355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "Figure 10.10\n\nAn external cloud resource administrator uses a digital\n\ncertificate to access the web-based management\n\nenvironment. DTGOV’s digital certificate is used in the\n\nHTTPS connection and then signed by a trusted CA.\n\n10.6 Single Sign-On (SSO) System\n\nPropagating the authentication and authorization information for a cloud\n\nservice consumer across multiple cloud services can be a challenge,\n\nespecially if numerous cloud services or cloud-based IT resources need to\n\nbe invoked as part of the same overall runtime activity. The single sign-on\n\n(SSO) system mechanism enables one cloud service consumer to be\n\nauthenticated by a security broker, which establishes a security context that\n\nis persisted while the cloud service consumer accesses other cloud services\n\nor cloud-based IT resources. Otherwise, the cloud service consumer would\n\nneed to re-authenticate itself with every subsequent request.\n\nThe SSO system mechanism essentially enables mutually independent\n\ncloud services and IT resources to generate and circulate runtime\n\nauthentication and authorization credentials. The credentials initially\n\nprovided by the cloud service consumer remain valid for the duration of a\n\nsession, while its security context information is shared (Figure 10.11). The\n\nSSO system mechanism’s security broker is especially useful when a cloud",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "service consumer needs to access cloud services residing on different clouds\n\n(Figure 10.12).",
      "content_length": 93,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "Figure 10.11",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "A cloud service consumer provides the security broker with login\n\ncredentials (1). The security broker responds with an authentication token\n\n(message with small lock symbol) upon successful authentication, which\n\ncontains cloud service consumer identity information (2) that is used to\n\nautomatically authenticate the cloud service consumer across Cloud\n\nServices A, B, and C (3).\n\nThis mechanism does not directly counter any of the cloud security threats\n\nlisted in Chapter 7. It primarily enhances the usability of cloud-based\n\nenvironments for access and management of distributed IT resources and\n\nsolutions.\n\nCase Study Example\n\nThe migration of applications to ATN’s new PaaS\n\nplatform was successful, but also raised a number of new\n\nconcerns pertaining to the responsiveness and\n\navailability of PaaS-hosted IT resources. ATN intends to\n\nmove more applications to a PaaS platform, but decides\n\nto do so by establishing a second PaaS environment with\n\na different cloud provider. This will allow them to\n\ncompare cloud providers during a three-month\n\nassessment period.\n\nTo accommodate this distributed cloud architecture, the\n\nSSO system mechanism is used to establish a security",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "broker capable of propagating login credentials across\n\nboth clouds (Figure 10.12). This enables a single cloud\n\nresource administrator to access IT resources on both\n\nPaaS environments without having to log in separately to\n\neach one.\n\nFigure 10.12",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "The credentials received by the security broker are\n\npropagated to ready-made environments across two\n\ndifferent clouds. The security broker is responsible for\n\nselecting the appropriate security procedure with which\n\nto contact each cloud.\n\n10.7 Hardened Virtual Server Image\n\nAs previously discussed, a virtual server is created from a template\n\nconfiguration called a virtual server image (or virtual machine image).\n\nHardening is the process of stripping unnecessary software from a system to\n\nlimit potential vulnerabilities that can be exploited by attackers. Removing\n\nredundant programs, closing unnecessary server ports, and disabling unused\n\nservices, internal root accounts, and guest access are all examples of\n\nhardening.\n\nA hardened virtual server image is a template for virtual service instance\n\ncreation that has been subjected to a hardening process (Figure 10.13). This\n\ngenerally results in a virtual server template that is significantly more\n\nsecure than the original standard image.",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "Figure 10.13\n\nA cloud provider applies its security policies to harden its standard virtual\n\nserver images. The hardened image template is saved in the VM images\n\nrepository as part of a resource management system.\n\nHardened virtual server images help counter the denial of service,\n\ninsufficient authorization, and overlapping trust boundaries threats.",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "Case Study Example\n\nOne of the security features made available to cloud\n\nconsumers as part of DTGOV adoption of cloud-based\n\nsecurity groups is an option to have some or all virtual\n\nservers within a given group hardened (Figure 10.14).\n\nEach hardened virtual server image results in an extra\n\nfee but spares cloud consumers from having to carry out\n\nthe hardening process themselves.",
      "content_length": 385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "Figure 10.14",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "The cloud resource administrator chooses the hardened\n\nvirtual server image option for the virtual servers\n\nprovisioned for Cloud-Based Security Group B.\n\n10.8 Firewall\n\nA firewall is a network gateway that limits access between networks in\n\naccordance with an established security policy. It acts as the interface of a\n\nnetwork to one or more external networks and regulates the network traffic\n\npassing through it by accepting or rejecting packets in accordance with a set\n\nof criteria. Both physical and virtual firewalls exist (Figure 10.15).\n\nFigure 10.15",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "The icons used to represent physical and virtual firewalls.\n\nFirewalls are used to protect the attack surface of an organization by\n\nintercepting all traffic that goes in and out and identifying whether any of\n\nthat traffic matches predefined rules that can be preconfigured to control the\n\ntraffic flow.\n\nA physical firewall protects physical connections to network devices.\n\nHowever, physical firewalls are not capable of filtering out traffic that\n\nbelongs to virtual networks which exist only within virtualized hosts in\n\nnetworking environments that are not represented by physical connections\n\nbetween physical devices. In such an environment, a virtual firewall can be\n\ndeployed to provide the same kind of protection for the virtual network\n\n(Figure 10.16). It is very common for virtual and physical firewalls to be\n\nintegrated so as to provide coordinated protection that encompasses\n\nphysical and virtual networks.",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "Figure 10.16\n\nA physical firewall filters traffic for a physical network, whereas a virtual\n\nfirewall filters traffic for a virtual network.",
      "content_length": 140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "Some firewall implementations will also rely on firewall agents, which are\n\nprograms deployed to run on individual software programs. These agents\n\ncan provide a more individualized level of protection. A firewall with\n\nagents can be referred to as a distributed firewall because the overall\n\nfirewall capabilities are provided collectively by the central firewall and its\n\nagents.\n\nNote\n\nContemporary firewall products can encompass the\n\ncapabilities of other mechanisms, such as features from the\n\nintrusion detection system (IDS) and digital virus scanning\n\nand decryption system. Some firewall products utilize data\n\nscience technologies, such as machine learning and artificial\n\nintelligence (AI), to enable the firewall to evolve in its\n\nability to protect network traffic.\n\nCase Study Example\n\nAs part of DTGOV’s cloud migration strategy, deploying\n\nvirtual firewalls as part of every individual client\n\nnetwork is fundamental to ensuring that all of them are\n\nprotected against unauthorized access not only from the\n\ninternet but also from each other. A virtual firewall for\n\neach client will allow DTGOV to customize network",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "access individually as required by each different\n\ngovernment organization.\n\n10.9 Virtual Private Network (VPN)\n\nA VPN (Figure 10.17) is a mechanism that exists as an encrypted\n\nconnection that allows remote users to access devices on a firewall-\n\nprotected network. The VPN provides a secure communications tunnel for\n\ndata to be transmitted between networks. It is commonly used to establish\n\nan encrypted extension of a private network across an untrusted network,\n\nsuch as the internet. VPNs are implemented as virtual networks.",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "Figure 10.17\n\nThe icon used to represent the virtual private network (VPN) mechanism.\n\nVPNs protect access to internal information assets by allowing only\n\nauthorized parties with the required security clearance to remotely access\n\ndata, while blocking other parties. VPNs are commonly built using\n\ncryptographic technologies to authenticate, authorize and cypher all of the\n\ntraffic that passes through the VPN connection.",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "There are two types of VPNs:\n\nSecure VPN – This type of VPN sends and receives traffic in an encrypted\n\nand authenticated manner. Both the server and the client agree on security\n\nproperties and no one outside of the VPN can modify these agreed\n\nproperties.\n\nTrusted VPN – This type of VPN may not use encryption, but instead, users\n\ntrust the VPN provider to ensure that no one else is using the same IP\n\naddress in the pathway of that VPN. In a trusted VPN, only the provider can\n\nchange, inject or delete data in the VPN’s communication channel.\n\nHybrid VPNs exist that combine the encryption property of a secure VPN\n\nand the dedicated connection property of a trusted VPN.\n\nNote\n\nCommon VPN protocols include Open VPN, L2TP/IPSec,\n\nSSTP, IKEv2, PPTP and Wireguard. Each offers different\n\nlevels of speed, security and ease of setup.\n\nCase Study Example\n\nDTGOV has identified certain client government\n\norganizations that need to be able to access protected\n\ndata stored in cloud-based storage servers from remote",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "locations in a secure manner. Dedicated physical\n\nconnections are not available everywhere, requiring\n\nmany of their clients to use the internet to access such\n\ndata. By implementing VPN connections, it can\n\nguarantee secure access to the protected data via the\n\ninternet.\n\n10.10 Biometric Scanner\n\nBiometrics is a technology used to determine a person’s identity based on\n\ntheir physiological or behavioral characteristics. Since biometric data is\n\ndirectly derived from these types of unique user characteristics, it cannot be\n\nlost or forgotten by the user, nor can it be easily forged by attackers. This\n\novercomes some of the problems users may have with passwords and\n\ntokens, which can be lost, forgotten, stolen or otherwise compromised by\n\nattackers.\n\nA biometric scanner (Figure 10.18) is a mechanism capable of validating a\n\nhuman’s identity by scanning or capturing a physiological or behavioral\n\ncharacteristic, such as handwriting, signatures, fingerprints, eyes, voice or\n\nfacial recognition.",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "Figure 10.18\n\nThe icon used to represent the biometric scanner mechanism.\n\nThere are two primary types of identifiers that can be validated using\n\nbiometric scanners:\n\nPhysiological Identifiers – These can be either biological or morphological.\n\nBiological identifiers include DNA, blood, saliva and urine tests, which are",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "commonly used by medical teams and police forensics and do not really\n\napply to cybersecurity protection mechanisms. Morphological identifiers\n\ninclude fingerprints, hand shape or vein pattern, eyes (including iris and\n\nretina) and face shape.\n\nBehavioral Identifiers – These include voice recognition, signature\n\ndynamics (including the speed of movement of the pen, accelerations,\n\npressure exerted and inclination), keystroke dynamics, the way we use\n\ncertain objects, gait (the sound of a person’s steps when they walk) and\n\nother types of gestures.\n\nDifferent types of identifiers and measurements do not always have the\n\nsame level of reliability. Physiological measurements usually offer the\n\nbenefit of staying stable throughout the lifetime of a person and are not\n\nsubjected to stress, whereas behavioral measurements can change with\n\ndifferent life stages and stress levels (Figure 10.19).",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "Figure 10.19",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "Over time, a person’s fingerprint won’t normally change, making it a\n\nreliable physiological identifier. However, a person’s voice may change,\n\nmaking it a less reliable behavioral identifier.\n\nSome biometric scanner mechanisms combine different types of biometric\n\nscanners in order to increase the range of security validations and the\n\naccuracy of identifications. These types of systems are referred to as\n\nmultimodal biometric scanners and they require at least two biometric\n\ncredentials to perform identification. For example, a multimodal biometric\n\nscanner system may require both facial and fingerprint recognition in order\n\nto validate a user.\n\nBiometric scanners that are limited to verifying one identifier can also be\n\nreferred to as unimodal biometric scanners.\n\nCase Study Example\n\nRecognizing how critical it is to ensure that only\n\nauthorized guardians of children using their products\n\nshould be allowed to access a cloud-based account,\n\nInnovartus decides to support the option for parents to\n\nrequire that access is only allowed via the use of a thumb\n\nscan. To enable this, Innovartus uses the biometric\n\nscanner mechanism and makes it available for users of\n\nmobile devices. This can help guarantee that only",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "parents and other authorized guardians gain access to the\n\ncloud account that stores private data.\n\n10.11 Multi-Factor Authentication (MFA) System\n\nA multi-factor authentication system (Figure 10.20) uses two or more\n\nfactors (verifiers) to achieve authentication. It works by requesting one\n\nform of verification from a user during a sign-in process, and then\n\nrequesting a second form of verification in order to complete the sign-in.\n\nThe types of authentication methods are kept independent of each other,\n\nthereby making it difficult for malicious users to gain unauthorized access.",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "Figure 10.20\n\nThe icon used to represent the multi-factor authentication (MFA) system\n\nmechanism.\n\nFactors used in MFA systems typically include:",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "something a user knows, such as a password or PIN (Figure 10.21)\n\nsomething a user has, such as a digital signature or token\n\nsome part of a user, such as a biometric identifier or measurement\n\nFigure 10.21\n\nAn MFA system is used to require multi-factor authentication steps by a\n\nuser after it is determined that the user is attempting access from a new\n\ngeographical location.",
      "content_length": 378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "MFA systems may also support:\n\nLocation-Based Authentication – A more advanced type of MFA that\n\nperforms verification based on a user’s IP address and geolocation.\n\nRisk-Based Authentication – A type of verification based on an analysis of\n\ncontext or behavior when a user is trying to access an account, such as\n\nwhen or from where the user is trying to sign in, whether sign-in is being\n\nperformed by a known or new device, how many failed sign-in attempts\n\nhave occurred, etc. This is also known as adaptive authentication.\n\nMFA systems are commonly used together with VPNs within organizations\n\nto enable employees to access corporate servers remotely.\n\nCase Study Example\n\nSome of the parents of children using Innovartus\n\nTechnologies products have requested specific persons\n\nto access their children’s accounts in the cloud on their\n\nbehalf. To ensure that the alternate “guardian” is truly\n\nthe one requesting access, Innovartus provides the option\n\nfor cloud accounts to be only accessible via multi-factor\n\nauthentication, such as a one-time password (OTP) sent\n\nvia text to the authorized party’s mobile device.\n\n10.12 Identity and Access Management (IAM) System",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "The identity and access management (IAM) system mechanism\n\nencompasses the components and policies necessary to control and track\n\nuser identities and access privileges for IT resources, environments, and\n\nsystems.\n\nSpecifically, IAM system mechanisms exist as systems comprised of four\n\nmain components:\n\nAuthentication – Username and password combinations remain the most\n\ncommon forms of user authentication credentials managed by the IAM\n\nsystem, which also can support digital signatures, digital certificates,\n\nbiometric hardware (fingerprint readers), specialized software (such as\n\nvoice analysis programs), and locking user accounts to registered IP or\n\nMAC addresses.\n\nAuthorization – The authorization component defines the correct granularity\n\nfor access controls and oversees the relationships between identities, access\n\ncontrol rights, and IT resource availability.\n\nUser Management – Related to the administrative capabilities of the\n\nsystem, the user management program is responsible for creating new user\n\nidentities and access groups, resetting passwords, defining password\n\npolicies, and managing privileges.\n\nCredential Management – The credential management system establishes\n\nidentities and access control rules for defined user accounts, which",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "mitigates the threat of insufficient authorization.\n\nAlthough its objectives are similar to those of the PKI system mechanism,\n\nthe IAM system mechanism’s scope of implementation is distinct because\n\nits structure encompasses access controls and policies in addition to\n\nassigning specific levels of user privileges.\n\nThe IAM system mechanism is primarily used to counter the insufficient\n\nauthorization, denial of service, overlapping trust boundaries threats,\n\nvirtualization attack and containerization attack threats.\n\nAn IAM system (Figure 10.22) is an established mechanism used to\n\nidentify, authenticate and authorize users based on predefined user roles and\n\naccess privileges.",
      "content_length": 686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "Figure 10.22\n\nThe icon used to represent the identity and access management (IAM)\n\nsystem mechanism.\n\nAn IAM system can:\n\nverify users",
      "content_length": 134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "assign roles to users\n\nassign levels of access to users or groups of users (Figure 10.23)\n\nFigure 10.23\n\nThe IAM system authenticates User A and identifies the user as belonging\n\nto Role X. Based on the user’s role, the IAM system authorizes the user to",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "access two specific folders on a physical file server.\n\nAn IAM system can carry out the identification, authentication and\n\nauthorization of a user by utilizing:\n\nUnique Passwords – Traditionally, the most common type of digital\n\nauthentication an IAM system uses.\n\nPre-Shared Key (PSK) – A type of digital authentication whereby the\n\npassword is shared among users authorized to access the same IT resources.\n\nIt provides convenience but is less secure than the use of individual\n\npasswords.\n\nBehavioral Authentication – For access to sensitive information or critical\n\nsystems, the IAM can encompass or be used together with a biometric\n\nscanner to provide behavioral authentication. For example, it may analyze\n\nkeystroke dynamics or mouse-use characteristics to instantly determine\n\nwhether a user’s sign-in behavior falls outside of the norm.\n\nOther Biometrics – IAM systems can use other biometric identifiers for\n\nmore precise authentication.\n\nContemporary IAM systems can include AI technology to help assess user\n\npatterns and behaviors. The system may collect historical user access data\n\nthat the AI system can use to learn about users and as a reference when\n\ncomparing recent user behavior to historically recorded user behavior.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "Case Study Example\n\nAs a result of several past corporate acquisitions, ATN’s\n\nlegacy landscape has become complex and\n\nheterogeneous. Maintenance costs have increased due to\n\nredundant and similar applications and databases\n\nrunning concurrently. Legacy repositories of user\n\ncredentials are just as assorted.\n\nNow that ATN has ported several applications to a PaaS\n\nenvironment, new identities are created and configured\n\nin order to grant users access. The CloudEnhance\n\nconsultants suggest that ATN capitalize on this\n\nopportunity by starting a pilot IAM system initiative,\n\nespecially since a new group of cloud-based identities is\n\nneeded.\n\nATN agrees, and a specialized IAM system is designed\n\nspecifically to regulate the security boundaries within\n\ntheir new PaaS environment. With this system, the\n\nidentities assigned to cloud-based IT resources differ\n\nfrom corresponding on-premises identities, which were\n\noriginally defined according to ATN’s internal security\n\npolicies.",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "10.13 Intrusion Detection System (IDS)\n\nThe intrusion detection system (IDS) mechanism (Figure 10.24) detects\n\nunauthorized or intrusion activity. It is the first line of defense for many\n\nnetworks. Intrusion detection systems (IDSs) reference a database of known\n\nattack data to help recognize suspicious activity. Contemporary systems\n\nutilize machine learning and AI technology to help recognize activity\n\nassociated with new attacks or being carried out by new attackers.\n\nFigure 10.24\n\nThe icon used to represent the intrusion detection system (IDS) mechanism.",
      "content_length": 565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "Based on the type of machine learning or AI technology used, different\n\nforms of intrusion detection can be carried out.\n\nFor example, an anomaly-based detection system works by creating a\n\nbaseline for each information asset that represents a profile of “normal\n\nbehavior”. This profile considers usage bandwidth and other metrics for\n\neach device in the organization’s attack surface, generating an alert for any\n\nactivity that deviates from this baseline. Since each information asset is\n\nunique, these customized profiles can be created to make it more difficult\n\nfor an attacker to know which specific activity can be carried out without\n\nsetting off an alarm.\n\nFeatures like this can help detect zero-day attacks, due to the fact that such\n\na system does not depend on an established database of prior known\n\nintrusions, but instead focuses on the deviations from the established\n\nbaselines.\n\nThere are two primary types of IDS mechanisms:\n\nPassive – The previously described scenario is an example of a passive\n\nIDS, as its main responsibilities are to detect intrusions and raise alerts.\n\nDynamic – A dynamic IDS (also known as an intrusion detection and\n\nprevention system) is additionally designed to take action when a suspected\n\nintrusion has been detected.",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "In general, an intrusion detection and prevention system is a combination of\n\na passive IDS and access control devices that the system executes to block\n\nan intruder.\n\nCase Study Example\n\nGiven that secret or confidential data is not allowed\n\noutside of the secure perimeter of each client\n\norganization, some law-enforcement clients of DTGOV\n\nhave been attacked with the purpose of acquiring this\n\ntype of data, such as data about open cases. Therefore,\n\nDTGOV decides to install an intrusion detection system\n\nthat will help enable it to take immediate action\n\nwhenever it is detected that attackers are attempting to\n\npenetrate a secure perimeter established by DTGOV for\n\none of those clients.\n\n10.14 Penetration Testing Tool\n\nThe penetration testing tool (Figure 10.25) is used to carry out penetration\n\ntesting, also known as pentesting, which is the practice of testing a network\n\nor system to expose security vulnerabilities. It helps organizations\n\nunderstand the current capabilities of their cybersecurity environments,\n\nprovides insight into which attacks can more successfully occur and allows\n\nfor security professionals to carry out simulations of actual attacks.",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "Figure 10.25\n\nThe icon used to represent the penetration testing tool mechanism.\n\nContemporary attack vectors need to be examined thoroughly for potential\n\nvulnerabilities using modern and improved penetration testing techniques,\n\nwhich include:\n\nautomated pentesting",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "cloud-based pentesting\n\nsocial engineering pentesting (to evaluate how humans may react to threats,\n\nsuch as phishing)\n\nFigure 10.26 illustrates several pentesting scenarios.",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "Figure 10.26\n\nA security professional uses the penetration testing tool to verify that an\n\nintrusion detection system (IDS) (1) is working correctly. The penetration",
      "content_length": 165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "testing tool then exposes a vulnerability in a virtual firewall (2). Finally, it\n\ntries (unsuccessfully) to trick a human worker into opening a phishing\n\nemail (3).\n\nPenetration testing can be performed in a fully automated manner that\n\nallows for more frequent tests to be performed on the organization’s\n\nsecurity infrastructure. This can help establish a continuous assessment of a\n\ncybersecurity environment’s effectiveness.\n\nCase Study Example\n\nDTGOV has implemented many security measures and\n\ncontrols to protect its clients’ data and IT resources.\n\nHowever, the effectiveness of all those mechanisms,\n\nindividually or as a group, has never been assessed.\n\nDTGOVE employs the use of a penetration testing tool\n\nto periodically test its security controls to ensure their\n\neffectiveness.\n\nSpecifically, the penetration testing tool is used in\n\nspecial exercises designed and scheduled to test and\n\nverify how well certain cloud security mechanisms are\n\nperforming. This helps DTGOV make further\n\nadjustments and improvements in its security\n\narchitecture.",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "10.15 User Behavior Analytics (UBA) System\n\nA UBA system (Figure 10.27) monitors the behavior of users in realtime in\n\norder to establish a baseline for “normal user activity” with the purpose of\n\nidentifying abnormal user behavior that could indicate malicious activity.\n\nMonitored behaviors can include attempts to open, view, delete and modify\n\nfiles, modifying critical system settings and initiating network\n\ncommunications. A UBA system can block suspicious behaviors in realtime\n\nand/or terminate offending software. Some advanced UBA solutions focus\n\non network and perimeter system activity such as logins and application and\n\nsystem-level events. Others may focus on more granular metadata in the\n\nsystem itself, such as user activity on files and emails.",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "Figure 10.27\n\nThe icon used to represent the user behavior analytics (UBA) system\n\nmechanism.\n\nA UBA system utilizes data science practices and technologies. The system\n\nneeds to be trained to identify normal behavior by processing activity logs,\n\nfile access, logins, network activity and other types of historical activity.\n\nThrough a variety of machine learning analysis techniques and the use of AI",
      "content_length": 402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "and neural networks, the system can establish a baseline from which it will\n\nbe possible to predict what is and is not normal (Figure 10.28).\n\nFigure 10.28\n\nA UBA system detects suspicious activity when a user demonstrates\n\nabnormal behavior.\n\nOther features of UBA systems include:\n\nProcessing High User Activity Volume – File systems can be enormous and\n\nsensitive data can be spread out scarcely. In order to be able to identify",
      "content_length": 431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "attackers, a UBA system needs be able to search through and analyze key\n\nmetadata and activity of many users across potentially massive amounts\n\ndata.\n\nRealtime Alerts – A UBA system’s attacker detection algorithms must be\n\nable to raise alerts in near realtime since the time window for when\n\nattackers access and copy sensitive data can be very short.\n\nCase Study Example\n\nDTGOV recognizes that its users cannot be effectively\n\ntrained in cloud security awareness as quickly and\n\neffectively as it would want. It utilizes a UBA system\n\nthat allows it to monitor and recognize user behavior to\n\nidentify when a given user might actually be an attacker\n\nor intruder.\n\nThe UBA system analyzes the behavior of all of\n\nDTGOV’s clients’ end users and learns their common\n\nbehavior in preparation for any unauthorized user that\n\nmay attempt to use an account that belongs to a\n\nlegitimate user.\n\n10.16 Third-Party Software Update Utility",
      "content_length": 932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "Cybersecurity-related software vulnerabilities commonly show up after a\n\nnew version of third-party software has been released. When this happens,\n\ndevelopers try to patch the vulnerability as fast as possible by releasing an\n\nupgrade or a patch that needs to be applied on all implementations of that\n\nsoftware to remediate the encountered vulnerability. The longer it takes for\n\nthe systems administrator to apply the update or patch, the higher the\n\nprobability of being attacked through said vulnerability. The third-party\n\nsoftware update utility (Figure 10.29) can help administrators automate the\n\nprocess of patching or updating their third-party software programs.",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Figure 10.29\n\nThe icon used to represent the third-party software update utility\n\nmechanism.\n\nThis mechanism typically works as follows:\n\nThe administrator defines a baseline that determines the level of update and\n\npatching that is required.",
      "content_length": 242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "All related third-party software programs are reviewed against this baseline\n\nand the required updating and patching for each are identified.\n\nPatches and updates are downloaded from a central repository, usually\n\nthrough a secure channel to ensure that the software has not been tampered\n\nwith. They are stored locally for further remediation purposes.\n\nThe remediation process (the updating, upgrading or patching activity that\n\nis performed automatically by the tool) is scheduled and/or carried out\n\nwhen required (Figure 10.30).",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "Figure 10.30\n\nThe third-party software update utility executes a series of pre-scheduled\n\nupdates and patches to a number of third-party programs, including a\n\nlegacy system, a software component, a service agent program and a virtual\n\nserver operating system.\n\nNote\n\nThis mechanism is specific to third-party software programs.\n\nCustom-developed software and applications can be more\n\neffectively updated and patched by an organization’s\n\ndevelopment team using methodologies, such as DevOps.\n\nCase Study Example\n\nDTGOV manages a very large amount of cloud\n\nresources, including thousands of virtual servers with\n\noperating systema that need to be updated periodically to\n\nensure that any vulnerabilities have been patched as soon\n\nas they are fixed by the operating system developer.\n\nPerforming this task manually is not feasible due to the",
      "content_length": 843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "sheer amount of cloud-based virtual servers that have to\n\nbe updated periodically.\n\nDTGOV enlists the use of a third-party software update\n\nutility for each different operating system installed on\n\nthe virtual servers it manages. This allows it to ensure\n\nthat the operating systems are updated with all necessary\n\nsecurity vulnerability patches and fixes as soon as they\n\nbecome available.\n\n10.17 Network Intrusion Monitor\n\nThe network intrusion monitor (Figure 10.31) is dedicated to monitoring\n\nnetwork packets across different sub-networks in order to find any\n\nsuspicious activity. It reports back its findings to a centralized network\n\nintrusion detection system (IDS) that coordinates its activity.",
      "content_length": 705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "Figure 10.31\n\nThe icon used to represent the network intrusion monitor mechanism.\n\nThis mechanism can be signature-based or anomaly detection-based. The\n\nformer is reactive while the latter is proactive, autonomous and can be\n\nconfigured to respond automatically to identified threats.",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "Case Study Example\n\nAs a provider of networking equipment itself for the\n\ntelecommunications industry, ATN is concerned about\n\nthe security of the virtual networks that connect all its\n\ncloud-based resources. ATN knows how networks can be\n\nbreached and wants to ensure that if that happened to its\n\nown cloud-based network, ATN can take immediate\n\naction to deal with the intruder appropriately.\n\nATN recognizes in the network intrusion monitor a\n\nmechanism that will notify the interested parties within\n\nthe organization when a network has been breached. It\n\nwill even provide sufficient information about the breach\n\nto allow IT security specialists from ATN to respond in\n\ntime and avert all potential damages to the organization.\n\n10.18 Authentication Log Monitor\n\nThe authentication log monitor (Figure 10.32) scans historical logs that\n\ninclude information about authentication events that occurred when users\n\nattempted to access protected network resources. This information may be\n\nused to solve access difficulties and to change authentication policy rules.",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "Figure 10.32\n\nThe icon used to represent the authentication log monitor mechanism.\n\nAmong the data collected by this monitor is also authentication rule data,\n\nsuch as timeouts that represent the period of time during which a user can",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "access a resource after first being authenticated for its access.\n\nCase Study Example\n\nDTGOV needs to manage access for a very large number\n\nof users. This is a burdensome responsibility that has\n\nbeen performed manually by the administrators of the\n\ndifferent cloud-based resources that DTGOV manages\n\non behalf of its clients. Manual data entry has been\n\nprone to human error, which has resulted in complaints\n\nabout possible unauthorized access to users’ accounts.\n\nDTGOV decided to use an authentication log monitor to\n\nregularly analyze access-related information for users\n\nthat complain about their access privileges being used\n\nimproperly. This helps them determine when actual\n\nintrusion events may have occurred. With this\n\ninformation, DTGOV can proceed to review the access\n\nprivileges given to affected users to see if access was\n\ncarried out in compliance with the originally requested\n\nprivileges.\n\n10.19 VPN Monitor\n\nA VPN monitor (Figure 10.33) tracks and collects information about VPN\n\nconnections, such as which users have connected (or are currently",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "connected), the kinds of connections used and the volume of data\n\nexchanged over a certain period. In case of failed connection attempts, it\n\nrecords connection issues and sends notifications to administrators. This\n\nmechanism helps identify network anomalies.\n\nFigure 10.33",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "The icon used to represent the VPN monitor mechanism.\n\nCase Study Example\n\nSeveral of DTGOV’s clients that allow remote access to\n\ntheir cloud-based data and systems via a VPN have\n\ncomplained that their data may have been accessed by\n\nunauthorized parties. To be able to verify this behavior,\n\nDTGOV uses a VPN monitor. DTGOV analyzes the\n\ninformation collected by the VPN monitor to identify\n\npotential unauthorized access through the VPN.\n\n10.20 Additional Cloud Security Access-Oriented Practices and\n\nTechnologies\n\nThe following is a list of further third-party cloud security access-oriented\n\npractices and technologies:\n\nCloud Access Security Brokers (CASB) – Security solutions designed to\n\nprotect cloud-based applications and services. These solutions are typically\n\ndeployed between cloud service consumers and providers, allowing\n\norganizations to enforce security policies and gain visibility into cloud\n\nusage.\n\nSecure Access Service Edge (SASE) – A network architecture that combines\n\nnetwork security and wide-area networking capabilities to deliver secure\n\naccess to cloud-based applications and resources.",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "Cloud Security Posture Management (CSPM) – A cloud security solution\n\nthat provides continuous monitoring and management of an organization's\n\ncloud infrastructure to ensure compliance with security policies and\n\nregulations.\n\nCloud Workflow Protection Platforms (CWPP) – A type of cloud security\n\ntool that is designed to protect and secure the various workflows and\n\nprocesses that occur within cloud-based environments. These platforms\n\nhelp to ensure that these workflows are safe from unauthorized access, data\n\nbreaches, and other security threats.\n\nCloud Infrastructure Entitlement Management (CIEM) – A type of security\n\nsolution designed to manage and monitor access to cloud resources, such as\n\nservers, databases, and applications. CIEM helps organizations ensure that\n\nonly authorized personnel have access to their cloud infrastructure,\n\nreducing the risk of data breaches and unauthorized modifications. The\n\nsolution provides visibility into user access permissions and activity,\n\nallowing organizations to detect and respond to suspicious behavior in real-\n\ntime.",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "Chapter 11\n\nCloud and Cyber Security Data-Oriented Mechanisms\n\n11.1 Digital Virus Scanning and Decryption System\n\n11.2 Digital Immune System\n\n11.3 Malicious Code Analysis System\n\n11.4 Data Loss Prevention (DLP) System\n\n11.5 Trusted Platform Module (TPM)\n\n11.6 Data Backup and Recovery System\n\n11.7 Activity Log Monitor\n\n11.8 Traffic Monitor\n\n11.9 Data Loss Protection Monitor\n\nThis section describes the following mechanisms that are focused on\n\nestablishing data access controls that cloud data monitoring functions.\n\nDigital Virus Scanning and Decryption System\n\nDigital Immune System",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Malicious Code Analysis System\n\nData Loss Prevention (DLP) System\n\nTrusted Platform Module (TPM)\n\nData Backup and Recovery System\n\nActivity Log Monitor\n\nTraffic Monitor\n\nData Loss Protection Monitor\n\n11.1 Digital Virus Scanning and Decryption System\n\nThe digital virus scanning and decryption system (Figure 11.1) is\n\nessentially an advanced anti-virus system comprised of client-side and\n\nserver-side components. The client-side component detects viruses by\n\nscanning files using detection methods that include specific pattern matches\n\nwithin executable files or heuristic methods to detect viral activity. It\n\nattempts to clean an identified virus infection by removing the virus’s code\n\nand restoring the original file’s contents.",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Figure 11.1\n\nThe icon used to represent the digital virus scanning and decryption system\n\nmechanism.\n\nThe server-side component is responsible for maintaining a database of\n\ncollected virus information and using data science technologies to analyze",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "and learn from the available information to help identify and counter new\n\npotential viruses or virus variants.\n\nThe client-side component periodically receives updated intelligence from\n\nthe server-side component to improve its ability to detect and remove\n\nviruses.\n\nThe digital virus scanning and decryption system commonly also provides\n\nthe following features:\n\nGeneric Decryption\n\nThis feature enables the system to detect highly complex viruses while\n\nmaintaining fast scanning speeds. Executable files are run through a generic\n\ndecryption scanner, which consists of three fundamental elements:\n\nCPU Emulator — A software-based virtual computer where an executable\n\nfile is run rather than executing it on the underlying processor.\n\nVirus Signature Scanner — Software that scans the executable file looking\n\nfor known virus signatures.\n\nEmulation Control Module — Software that controls the execution of the\n\nexecutable file.",
      "content_length": 933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "Case Study Example\n\nSince the numerous users working for DTGOV’s clients\n\nconnect to systems and IT resources that DTGOV\n\ndeploys and manages in the cloud on their behalf,\n\nseveral viruses have successfully attacked parts of that\n\ninfrastructure since the migration to the cloud first\n\nbegan.\n\nDTGOV employs a digital virus scanning and decryption\n\nsystem as part of their new cloud security defense\n\nstrategy. This system significantly reduces the spread of\n\nviruses throughout its cloud-based resources.\n\n11.2 Digital Immune System\n\nThis mechanism enables the system to capture a virus, strip it of\n\nconfidential information and then automatically submit it to a central virus\n\nanalysis center, where the virus is examined and a virus signature is created.\n\nThe virus signature is then tested against the original sample, and if\n\nsuccessful, it is sent back to the server to be deployed on the client-side\n\nmechanism component (Figure 11.2).",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "Figure 11.2\n\nThe client-side component of the digital virus scanning and decryption\n\nsystem detects a virus on a workstation (1). The server-side component logs\n\nthe virus information in a central database (2) and further forwards it to a\n\ncentral virus analysis center (3), where it is assigned a virus signature. The\n\nvirus signature is returned (4), logged by the server-side component (5) and\n\ndistributed to all workstations under the system’s protection (6).\n\nCase Study Example\n\nDTGOV needs to contend with a variety of end users\n\nthat have poor knowledge about how to recognize\n\nmalicious email messages and potentially infected media\n\nin removable devices. This has resulted in virus and\n\nmalware programs being able to infect some servers.\n\nSince several of its client government organizations take\n\na long time to train their personnel, DTGOV makes use\n\nof a digital immune system to obtain maximum\n\nprotection from potential virus and other malware\n\nattacks.\n\n11.3 Malicious Code Analysis System",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "The malicious code analysis system (Figure 11.3) is a mechanism that is\n\nable to perform analysis of massive volumes of malicious code to quickly\n\nproduce a report that a human analyst can use to determine what actions the\n\nmalicious code took. Contemporary malicious code analysis systems rely\n\non machine learning technology to carry out and constantly improve\n\nmalware detection capabilities.\n\nFigure 11.3\n\nThe icon used to represent the malicious code analysis system mechanism.\n\nThe large processing capabilities of these systems enable them to accelerate\n\nsecurity investigations with detailed data about workload-related events,\n\napplication logs, infrastructure metrics, audits and other sources of",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "malicious code behavior information. The malicious code analysis system is\n\nfurther able to issue alerts about malicious or anomalous patterns (Figure\n\n11.4).\n\nFigure 11.4\n\nAn automated malicious code analysis system detects malicious code on a\n\nworkstation (1) and analyzes the code in realtime (2) to alert and provide a\n\nreport to a security professional to review (3).",
      "content_length": 372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "The use of this mechanism can help an organization defend against zero-\n\nday attacks since the intelligence gathered is not necessarily based on\n\nhistorical intrusion detection, but rather on the results of data analysis\n\nprovided by models capable of identifying new malware in realtime.\n\nThere are two primary types of malicious code analysis systems:\n\nStatic — This type of system is capable of executing malicious code in a\n\nsafe and isolated environment called a sandbox, a controlled environment\n\nthat enables security professionals to watch the malware in action without\n\nrisking its potential effects on the organization’s business environment.\n\nDynamic — This type of system can provide deep insight into the\n\ncapabilities of malicious code. It utilizes automated sandboxing that\n\neliminates the time that it would take to reverse engineer a file after\n\nmalicious code has performed actions on it.\n\nSome attackers prepare their malicious programs to remain dormant while\n\nrunning in a sandbox environment. Therefore, a hybrid combination of\n\nstatic and dynamic malicious code analysis systems can be used to provide\n\na reliable means of detecting more sophisticated malicious code by hiding\n\nthe presence of a sandbox.",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "Case Study Example\n\nInnovartus Technologies has been the target of multiple\n\ndifferent virus attacks and has therefore decided to\n\nimplement a malicious code analysis system to prevent\n\nsuch attacks in the future.\n\nThis system has especially helped Innovartus identify\n\nthe more sophisticated attacks that require specialized\n\nand profound code analysis to identify.\n\n11.4 Data Loss Prevention (DLP) System\n\nA DLP system (Figure 11.5) is a tool that enables security professionals to\n\nmanage the security of and configure access to distributed information\n\nassets, which becomes more difficult the more remote the workforce. It is\n\ncommonly used to avoid the unauthorized or accidental sharing of\n\nconfidential data by internal staff.",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "Figure 11.5\n\nThe icon used to represent the data loss prevention (DLP) system\n\nmechanism.\n\nThe DLP mechanism’s capabilities can include:\n\nDevice Control — This allows the administrator to control which devices\n\nusers can store or copy data on. For example, it can be used to block users\n\nfrom storing potentially confidential data on USB drives or SD cards.\n\nContent Aware Protection — This allows the administrator to monitor and\n\ncontrol files, emails and other kinds of artifacts that can hold data primarily",
      "content_length": 511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "to ensure that no confidential information can be extracted from them.\n\nData Scanning — This function can scan files, emails and digital\n\ndocuments across different devices in order to mark those that can be\n\nconsidered confidential. Information assets can be labeled as confidential\n\nfor future reference by other mechanisms.\n\nForced Encryption — This is used to ensure that any content that is allowed\n\nto leave the organization is encrypted to ensure that it will be accessed only\n\nby authorized parties.\n\nFigure 11.6 demonstrates some of these capabilities.",
      "content_length": 561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "Figure 11.6\n\nA security professional with a DLP system blocks a user from storing\n\ncompany data on a USB (1), scans a corporate server with files in folders to\n\nidentify the ones with confidential data (2) and forces an email going\n\noutside of the organization boundary to be encrypted (3).\n\nData loss prevention (DLP) systems can exist as cloud-based services used\n\nto monitor cloud-based file sharing applications and sites.\n\nCase Study Example\n\nSome of DTGOV’s clients are law-enforcement\n\ngovernment organizations that need to keep certain data\n\nconfidential and secret. To prevent classified data from\n\nbeing shared in an unauthorized manner, a cloud-based\n\ndata loss prevention system is established specifically\n\nfor those clients.\n\nThis system ensures that any data copied or moved is\n\nchecked to see if it is confidential or secret. If that is the\n\ncase, it will not be allowed outside of a specified\n\nperimeter withing that client’s cloud environment.\n\n11.5 Trusted Platform Module (TPM)",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "A TPM (Figure 11.7) is a mechanism that stores artifacts that are used to\n\nauthenticate devices (“platforms”), such as a PC, laptop, mobile phone or\n\ntablet. A TPM can exist as a chip that has a unique and secret key burned in\n\nduring production.\n\nFigure 11.7\n\nThe icon used to represent a trusted platform module (TPM) mechanism.\n\nThe TPM chip performs certain measurements each time the device starts.\n\nThese measurements include taking hashes of the BIOS code, BIOS\n\nsettings, TPM settings bootloader and OS kernel so that alternative versions\n\nof the measured modules cannot be easily produced, and so that the hashes",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "lead to identical measurements. These measurements are used to validate\n\nagainst known good values.\n\nDuring boot-up, the mechanism verifies the characteristics of the hardware\n\ncomponents connected to the processor against the device information\n\nstored in the TPM. If the outcomes differ, then it is confirmed that the\n\nhardware has been compromised (Figure 11.8).",
      "content_length": 365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "Figure 11.8\n\nOn Day 1, an administrator starts up a physical server. The TPM\n\nmechanism verifies that the hardware is okay. On Day 2, the administrator\n\nstarts up the same server, only this time the TPM mechanism indicates that\n\nthe hardware confirmation does not match its measurements. The\n\nadministrator is made aware that the server could have been tampered with.\n\nCase Study Example\n\nThe security of children while using the virtual toys\n\nprovided by Innovartus Technologies is of critical\n\nimportance to their parents. It is therefore important for\n\nInnovartus to ensure that no malicious code can run on\n\nany of its cloud-based virtual servers.\n\nTo achieve this, it installs a TPM on each of its physical\n\nservers that host its cloud-based virtual servers. It uses\n\nthe TPM to verify that the hypervisor and every\n\noperating system instance running on those servers is\n\nverified for authenticity before it is loaded into memory.\n\nThis guarantees that no tampering with the physical\n\nhardware firmware or any other logic that runs on those",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "servers can occur. This, in turn, helps eliminates the\n\npossibility of malware running together with their virtual\n\ntoy products.\n\n11.6 Data Backup and Recovery System\n\nThe data backup and recovery system (Figure 11.9) is a mechanism that is\n\nused to provide fast data recovery in the event of data loss or corruption\n\nresulting from cyber-attacks, cyber theft, physical theft and hardware and\n\nsoftware failure.\n\nFigure 11.9\n\nThe icon used to represent a data backup and recovery system mechanism.",
      "content_length": 498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "The data backup and recovery system essentially copies important data to\n\nseparate storage repositories to provide a constant fallback for an\n\norganization to recover data (Figure 11.10).",
      "content_length": 187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "Figure 11.10\n\nA common technique for using the data backup and recovery system\n\nmechanism is known as the “3-2-1 approach,” which requires that data be\n\nkept in three separate locations, utilizing two different storage formats and\n\nwith one extra copy kept elsewhere, in a different geographical region.\n\nMany variations of this mechanism rely on placing backup data in clouds.\n\nCloud providers often provide backup-as-a-service (BaaS) offerings, which\n\ncan simplify data backup and recovery because they do not require the need\n\nto install and configure a storage device and additional software, such as an\n\noperating system.\n\nCase Study Example\n\nThe sheer volume of data that DTGOV is storing and\n\nprocessing in the cloud on behalf of its many clients\n\nplaces a significant responsibility on DTGOV to ensure\n\nthat it is always made available to its clients, regardless\n\nof any failure conditions or disruptions that may occur in\n\nthe cloud environment.\n\nA data backup and recovery system helps DTGOV\n\nensure that the most critical data it stores and processes\n\non behalf of its clients is copied in a safe and available",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "medium, in a location that is not subject to the same\n\nenvironmental or operational hazards that the original\n\ndata may be exposed to. This way, in case the original\n\ndata becomes unavailable, the copy can be used to\n\nrestore that data.\n\n11.7 Activity Log Monitor\n\nThe activity log monitor (Figure 11.11) scans historical log files or\n\ndatabases to attempt to find patterns of activity on networks which may\n\nprovide indicators of possible security breaches. Activity log data can come\n\nfrom event logs, device configuration logs, operating system logs, etc.",
      "content_length": 558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "Figure 11.11\n\nThe icon used to represent the activity log monitor mechanism.",
      "content_length": 76,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "Case Study Example\n\nWhen parents complain to Innovartus Technologies\n\nabout possible unauthorized access to their cloud-based\n\naccounts, Innovartus needs to be able to verify such\n\nclaims.\n\nFor this purpose, they use an activity log monitor to\n\nsearch through all recorded access attempts, whether\n\nsuccessful or not, to the account in question.\n\nThis monitor provides information about any activity\n\npatterns that may indicate malicious behavior, which\n\nInnovartus can study to verify the legitimacy of each\n\ncomplaint.\n\n11.8 Traffic Monitor\n\nThe traffic monitor mechanism (Figure 11.12) is responsible for monitoring\n\nnetwork traffic in order to review and analyze traffic activity in search of\n\nabnormalities that may be adversely affecting network performance,\n\navailability and/or security. This mechanism provides network\n\nadministrators with realtime data and long-term usage trends for network\n\ndevices.",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "Figure 11.12\n\nThe icon used to represent the traffic monitor mechanism.\n\nCase Study Example\n\nMultiple types of security incidents trigger specific\n\nnetwork-related events within the virtual networks that",
      "content_length": 203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "interconnect cloud-based resources. Therefore, as a\n\ncomplement to the network intrusion monitor, ATN\n\ninstalls a traffic monitor mechanism to gather data about\n\nthe behavior of the network, which can be correlated\n\nwith information from the network intrusion monitor to\n\nidentify more specifically the type of intrusion or\n\nnetwork breach that happened, allowing ATN to take the\n\nmost effective action against the intrusion.\n\n11.9 Data Loss Protection Monitor\n\nA data loss protection monitor (Figure 11.13) is designed to safeguard vital\n\ndata by utilizing capture technology that acts as a digital recorder and\n\nreplays after-the-fact data loss incidents. These recordings can be used for\n\nsubsequent investigations. This mechanism can streamline remediation by\n\nalerting senders, recipients, content owners and system administrators.",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "Figure 11.13\n\nThe icon used to represent the data loss protection monitor mechanism.\n\nThe data loss protection monitor is commonly used for an organization’s\n\nmost important information assets (such as source code, internal memos,\n\npatent applications, etc.). It detects many different content types traversing",
      "content_length": 310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "any port or protocol to uncover unknown threats. The monitor can find and\n\nanalyze sensitive information traveling across the network and apply rules\n\nto prevent future risks. This mechanism can further provide input for reports\n\nthat explain who sent data, where it went and how it was sent.\n\nNote\n\nA data loss protection monitor can help an organization meet\n\ndata loss monitoring regulatory requirements, such as PCI,\n\nGLBA, HIPAA and SOX.\n\nCase Study Example\n\nIn support of the strict data requirements of its law-\n\nenforcement clients, DTGOV relies on a data loss\n\nprotection monitor to keep it informed when any activity,\n\nsuch as copying or moving of data, occurs that is not in\n\ncompliance with their regulations and policies.",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "Chapter 12\n\nCloud Management Mechanisms\n\n12.1 Remote Administration System\n\n12.2 Resource Management System\n\n12.3 SLA Management System\n\n12.4 Billing Management System\n\nCloud-based IT resources need to be set up, configured, maintained, and\n\nmonitored. The systems covered in this chapter are mechanisms that\n\nencompass and enable these types of management tasks. They form key\n\nparts of cloud technology architectures by facilitating the control and\n\nevolution of the IT resources that form cloud platforms and solutions.\n\nThe following management-related mechanisms are described in this\n\nchapter:\n\nRemote Administration System\n\nResource Management System\n\nSLA Management System",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "Billing Management System\n\nThese systems typically provide integrated APIs and can be offered as\n\nindividual products, custom applications, or combined into various product\n\nsuites or multi-function applications.\n\n12.1 Remote Administration System\n\nThe remote administration system mechanism (Figure 12.1) provides tools\n\nand user—interfaces for external cloud resource administrators to configure\n\nand administer cloud-based IT resources.\n\nFigure 12.1",
      "content_length": 452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": "The symbol used in this book for the remote administration system. The\n\ndisplayed user-interface will typically be labeled to indicate a specific type\n\nof portal.\n\nA remote administration system can establish a portal for access to\n\nadministration and management features of various underlying systems,\n\nincluding the resource management, SLA management, and billing\n\nmanagement systems described in this chapter (Figure 12.2).\n\nFigure 12.2\n\nThe remote administration system abstracts underlying management\n\nsystems to expose and centralize administration controls to external cloud",
      "content_length": 582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "resource administrators. The system provides a customizable user console,\n\nwhile programmatically interfacing with underlying management systems\n\nvia their APIs.\n\nThe tools and APIs provided by a remote administration system are\n\ngenerally used by the cloud provider to develop and customize online\n\nportals that provide cloud consumers with a variety of administrative\n\ncontrols.\n\nThe following are the two primary types of portals that are created with the\n\nremote administration system:\n\nUsage and Administration Portal — A general purpose portal that\n\ncentralizes management controls to different cloud-based IT resources and\n\ncan further provide IT resource usage reports. This portal is part of\n\nnumerous cloud technology architectures covered in Chapters 13 to 15.",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "Self-Service Portal — This is essentially a shopping portal that allows cloud\n\nconsumers to search an up-to-date list of cloud services and IT resources\n\nthat are available from a cloud provider (usually for lease). The cloud\n\nconsumer submits its chosen items to the cloud provider for provisioning.\n\nThis portal is primarily associated with the rapid provisioning architecture\n\ndescribed in Chapter 14.\n\nFigure 12.3 illustrates a scenario involving a remote administration system\n\nand both usage and administration and self-service portals.\n\nFigure 12.3",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "A cloud resource administrator uses the usage and administration portal to\n\nconfigure an already leased virtual server (not shown) to prepare it for\n\nhosting (1). The cloud resource administrator then uses the self-service\n\nportal to select and request the provisioning of a new cloud service (2). The\n\ncloud resource administrator then accesses the usage and administration\n\nportal again to configure the newly provisioned cloud service that is hosted\n\non the virtual server (3). Throughout these steps, the remote administration\n\nsystem interacts with the necessary management systems to perform the\n\nrequested actions (4).\n\nDepending on:\n\nthe type of cloud product or cloud delivery model the cloud consumer is\n\nleasing or using from the cloud provider,\n\nthe level of access control granted by the cloud provider to the cloud\n\nconsumer, and\n\nwhich underlying management systems the remote administration system\n\ninterfaces with,\n\n…the following tasks can commonly be performed by cloud consumers via\n\na remote administration console:\n\nconfiguring and setting up cloud services\n\nprovisioning and releasing IT resource for on-demand cloud services",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "monitoring cloud service status, usage, and performance\n\nmonitoring QoS and SLA fulfillment\n\nmanaging leasing costs and usage fees\n\nmanaging user accounts, security credentials, authorization, and access\n\ncontrol\n\ntracking internal and external access to leased services\n\nplanning and assessing IT resource provisioning\n\ncapacity planning\n\nWhile the user-interface provided by the remote administration system will\n\ntend to be proprietary to the cloud provider, there is a preference among\n\ncloud consumers to work with remote administration systems that offer\n\nstandardized APIs. This allows a cloud consumer to invest in the creation of\n\nits own front-end with the fore-knowledge that it can reuse this console if it\n\ndecides to move to another cloud provider that supports the same\n\nstandardized API. Additionally, the cloud consumer would be able to further\n\nleverage standardized APIs if it is interested in leasing and centrally\n\nadministering IT resources from multiple cloud providers and/or IT\n\nresources residing in cloud and on-premise environments (Figure 12.4).",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "Figure 12.4",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "Standardized APIs published by remote administration systems from\n\ndifferent clouds enable a cloud consumer to develop a custom portal that\n\ncentralizes a single IT resource management portal for both cloud-based\n\nand on-premise IT resources.\n\nCase Study Example\n\nDTGOV has been offering its cloud consumers a user-\n\nfriendly remote administration system for some time,\n\nand recently determined that upgrades are required in\n\norder to accommodate the growing number of cloud\n\nconsumers and increasing diversity of requests. DTGOV\n\nis planning a development project to extend the remote\n\nadministration system to fulfill the following\n\nrequirements:\n\nCloud consumers need to be able to self-provision\n\nvirtual servers and virtual storage devices. The system\n\nspecifically needs to interoperate with the cloud-enabled\n\nVIM platform’s proprietary API to enable self-\n\nprovisioning capabilities.\n\nA single sign-on mechanism (described in Chapter 10)\n\nneeds to be incorporated to centrally authorize and\n\ncontrol cloud consumer access.",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 639,
      "content": "An API that supports the provisioning, starting, stopping,\n\nreleasing, up-down scaling, and replicating of commands\n\nfor virtual servers and cloud storage devices needs to be\n\nexposed.\n\nIn support of these features, a self-service portal is\n\ndeveloped and the feature-set of DTGOV’s existing\n\nusage and administration portal is extended.\n\n12.2 Resource Management System\n\nThe resource management system mechanism helps coordinate IT resources\n\nin response to management actions performed by both cloud consumers and\n\ncloud providers (Figure 12.5). Core to this system is the virtual\n\ninfrastructure manager (VIM) that coordinates the server hardware so that\n\nvirtual server instances can be created from the most expedient underlying\n\nphysical server. A VIM is a commercial product that can be used to manage\n\na range of virtual IT resources across multiple physical servers. For\n\nexample, a VIM can create and manage multiple instances of a hypervisor\n\nacross different physical servers or allocate a virtual server on one physical\n\nserver to another (or to a resource pool).",
      "content_length": 1076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "Figure 12.5",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "A resource management system encompassing a VIM platform and a virtual\n\nmachine image repository. The VIM may have additional repositories,\n\nincluding one dedicated to storing operational data.\n\nTasks that are typically automated and implemented through the resource\n\nmanagement system include:\n\nmanaging virtual IT resource templates that are used to create pre-built\n\ninstances, such as virtual server images\n\nallocating and releasing virtual IT resources into the available physical\n\ninfrastructure in response to the starting, pausing, resuming, and\n\ntermination of virtual IT resource instances\n\ncoordinating IT resources in relation to the involvement of other\n\nmechanisms, such as resource replication, load balancer, and failover\n\nsystem\n\nenforcing usage and security policies throughout the lifecycle of cloud\n\nservice instances\n\nmonitoring operational conditions of IT resources\n\nResource management system functions can be accessed by cloud resource\n\nadministrators employed by the cloud provider or cloud consumer. Those\n\nworking on behalf of a cloud provider will often be able to directly access\n\nthe resource management system’s native console.",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "Resource management systems typically expose APIs that allow cloud\n\nproviders to build remote administration system portals that can be\n\ncustomized to selectively offer resource management controls to external\n\ncloud resource administrators acting on behalf of cloud consumer\n\norganizations via usage and administration portals.\n\nBoth forms of access are depicted in Figure 12.6.\n\nFigure 12.6",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "The cloud consumer’s cloud resource administrator accesses a usage and\n\nadministration portal externally to administer a leased IT resource (1). The\n\ncloud provider’s cloud resource administrator uses the native user-interface\n\nprovided by the VIM to perform internal resource management tasks (2).\n\nCase Study Example\n\nThe DTGOV resource management system is an\n\nextension of a new VIM product it purchased, and\n\nprovides the following primary features:\n\nmanagement of virtual IT resources with a flexible\n\nallocation of pooled IT resources across different data\n\ncenters\n\nmanagement of cloud consumer databases\n\nisolation of virtual IT resources at logical perimeter\n\nnetworks\n\nmanagement of a template virtual server image\n\ninventory available for immediate instantiation\n\nautomated replication (“snapshotting”) of virtual server\n\nimages for virtual server creation",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "automated up-down scaling of virtual servers according\n\nto usage thresholds to enable live VM migration among\n\nphysical servers\n\nan API for the creation and management of virtual\n\nservers and virtual storage devices\n\nan API for the creation of network access control rules\n\nan API for the up-down scaling of virtual IT resources\n\nan API for the migration and replication of virtual IT\n\nresources across multiple data centers\n\ninteroperation with a single sign-on mechanism through\n\nan LDAP interface\n\nCustom-designed SNMP command scripts are further\n\nimplemented to interoperate with the network\n\nmanagement tools to establish isolated virtual networks\n\nacross multiple data centers.\n\n12.3 SLA Management System\n\nThe SLA management system mechanism represents a range of\n\ncommercially available cloud management products that provide features",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "pertaining to the administration, collection, storage, reporting, and runtime\n\nnotification of SLA data (Figure 12.7).\n\nFigure 12.7",
      "content_length": 131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "An SLA management system encompassing an SLA manager and QoS\n\nmeasurements repository.\n\nAn SLA management system deployment will generally include a\n\nrepository used to store and retrieve collected SLA data based on pre-\n\ndefined metrics and reporting parameters. It will further rely on one or more\n\nSLA monitor mechanisms to collect the SLA data that can then be made\n\navailable in near-real time to usage and administration portals to provide\n\non-going feedback regarding active cloud services (Figure 12.8). The\n\nmetrics monitored for individual cloud services are aligned with the SLA\n\nguarantees in corresponding cloud provisioning contracts.",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "Figure 12.8\n\nA cloud service consumer interacts with a cloud service (1). An SLA\n\nmonitor intercepts the exchanged messages, evaluates the interaction, and\n\ncollects relevant runtime data in relation to quality-of-service guarantees\n\ndefined in the cloud service’s SLA (2A). The data collected is stored in a\n\nrepository (2B) that is part of the SLA management system (3). Queries can\n\nbe issued and reports can be generated for an external cloud resource\n\nadministrator via a usage and administration portal (4) or for an internal",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "cloud resource administrator via the SLA management system’s native user-\n\ninterface (5).\n\nCase Study Example\n\nDTGOV implements an SLA management system that\n\ninteroperates with its existing VIM. This integration\n\nallows DTGOV cloud resource administrators to monitor\n\nthe availability of a range of hosted IT resources via\n\nSLA monitors.\n\nDTGOV works with the SLA management system’s\n\nreport design features to create the following pre-defined\n\nreports that are made available via custom dashboards:\n\nPer-Data Center Availability Dashboard — Publicly\n\naccessible through DTGOV’s corporate cloud portal, this\n\ndashboard shows the overall operational conditions of\n\neach group of IT resources at each data center, in\n\nrealtime.\n\nPer-Cloud Consumer Availability Dashboard — This\n\ndashboard displays realtime operational conditions of\n\nindividual IT resources. Information about each IT\n\nresource can only be accessed by the cloud provider and\n\nthe cloud consumer leasing or owning the IT resource.",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "Per-Cloud Consumer SLA Report — This report\n\nconsolidates and summarizes SLA statistics for cloud\n\nconsumer IT resources, including downtimes and other\n\ntime-stamped SLA events.\n\nThe SLA events generated by the SLA monitors\n\nrepresent the status and performance of physical and\n\nvirtual IT resources that are controlled by the\n\nvirtualization platform. The SLA management system\n\ninteroperates with the network management tools\n\nthrough a custom-designed SNMP software agent that\n\nreceives the SLA event notifications.\n\nThe SLA management system also interacts with the\n\nVIM through its proprietary API to associate each\n\nnetwork SLA event to the affected virtual IT resource.\n\nThe system includes a proprietary database used to store\n\nSLA events (such as virtual server and network\n\ndowntimes).\n\nThe SLA management system exposes a REST API that\n\nDTGOV uses to interface with its central remote\n\nadministration system. The proprietary API has a\n\ncomponent service implementation that can be used for\n\nbatch-processing with the billing management system.",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "DTGOV utilizes this to periodically provide downtime\n\ndata that translates into credit applied to cloud consumer\n\nusage fees.\n\n12.4 Billing Management System\n\nThe billing management system mechanism is dedicated to the collection\n\nand processing of usage data as it pertains to cloud provider accounting and\n\ncloud consumer billing. Specifically, the billing management system relies\n\non pay-per-use monitors to gather runtime usage data that is stored in a\n\nrepository that the system components then draw from for billing, reporting,\n\nand invoicing purposes (Figures 12.9 and 12.10).",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "Figure 12.9\n\nA billing management system comprised of a pricing and contract manager\n\nand a pay-per-use measurements repository.",
      "content_length": 128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "Figure 12.10\n\nA cloud service consumer exchanges messages with a cloud service (1). A\n\npay-per-use monitor keeps track of the usage and collects data relevant to\n\nbilling (2A), which is forwarded to a repository that is part of the billing\n\nmanagement system (2B). The system periodically calculates the\n\nconsolidated cloud service usage fees and generates an invoice for the\n\ncloud consumer (3). The invoice may be provided to the cloud consumer\n\nthrough the usage and administration portal (4).\n\nThe billing management system allows for the definition of different pricing\n\npolicies, as well as custom pricing models on a per cloud consumer and/or\n\nper IT resource basis. Pricing models can vary from the traditional pay-per-\n\nuse models, to flat-rate or pay-per-allocation modes, or combinations\n\nthereof.\n\nBilling arrangements be based on pre-usage and post-usage payments. The\n\nlatter type can include pre-defined limits or it can be set up (with the mutual\n\nagreement of the cloud consumer) to allow for unlimited usage (and,\n\nconsequently, no limit on subsequent billing). When limits are established,\n\nthey are usually in the form of usage quotas. When quotas are exceeded, the\n\nbilling management system can block further usage requests by cloud\n\nconsumers.",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Case Study Example\n\nDTGOV decides to establish a billing management\n\nsystem that enables them to create invoices for custom-\n\ndefined billable events, such as subscriptions and IT\n\nresource volume usage. The billing management system\n\nis customized with the necessary events and pricing\n\nscheme metadata.\n\nIt includes the following two corresponding proprietary\n\ndatabases:\n\nbillable event repository\n\npricing scheme repository\n\nUsage events are collected from pay-per-use monitors\n\nthat are implemented as extensions to the VIM platform.\n\nThin-granularity usage events, such as virtual server\n\nstarting, stopping, up-down scaling, and\n\ndecommissioning, are stored in a repository managed by\n\nthe VIM platform.\n\nThe pay-per-use monitors further regularly supply the\n\nbilling management system with the appropriate billable\n\nevents. A standard pricing model is applied to most",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "cloud consumer contracts, although it can be customized\n\nwhen special terms are negotiated.",
      "content_length": 91,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "Part III\n\nCloud Computing Architecture\n\nChapter 13: Fundamental Cloud Architectures\n\nChapter 14: Advanced Cloud Architectures\n\nChapter 15: Specialized Cloud Architectures\n\nCloud technology architectures formalize functional domains within cloud\n\nenvironments by establishing well-defined solutions comprised of\n\ninteractions, behaviors, and distinct combinations of cloud computing\n\nmechanisms and other specialized cloud technology components.\n\nThe fundamental cloud architectural models covered in Chapter 13 establish\n\nfoundational layers of technology architecture common to most clouds.\n\nMany of the advanced and specialized models described in Chapters 14 and\n\n15 build upon these foundations to add complex and narrower-focused\n\nsolution architectures.\n\nNote\n\nMost of the cloud architectures described over the next three\n\nchapters are documented in greater detail in the book Cloud\n\nComputing Design Patterns (by Thomas Erl and Amin",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "Naserpour), also part of the Pearson Digital Enterprise\n\nSeries from Thomas Erl. Visit www.thomaserl.com/books\n\nfor more information.",
      "content_length": 133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "Chapter 13\n\nFundamental Cloud Architectures\n\n13.1 Workload Distribution Architecture\n\n13.2 Resource Pooling Architecture\n\n13.3 Dynamic Scalability Architecture\n\n13.4 Elastic Resource Capacity Architecture\n\n13.5 Service Load Balancing Architecture\n\n13.6 Cloud Bursting Architecture\n\n13.7 Elastic Disk Provisioning Architecture\n\n13.8 Redundant Storage Architecture\n\n13.9 Multi-Cloud Architecture\n\n13.10 Case Study Example\n\nThis chapter describes the following foundational cloud architectural\n\nmodels:\n\nWorkload Distribution",
      "content_length": 522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "Resource Pooling\n\nDynamic Scalability\n\nElastic Resource Capacity\n\nService Load Balancing\n\nCloud Bursting\n\nElastic Disk Provisioning\n\nRedundant Storage\n\nMulti-Cloud\n\nFor each architecture, the typical involvement of cloud computing\n\nmechanisms (previously covered in Part II) is documented.\n\n13.1 Workload Distribution Architecture\n\nIT resources can be horizontally scaled via the addition of one or more\n\nidentical IT resources, and a load balancer that provides runtime logic\n\ncapable of evenly distributing the workload among the available IT\n\nresources (Figure 13.1). The resulting workload distribution architecture\n\nreduces both IT resource over-utilization and under-utilization to an extent",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "dependent upon the sophistication of the load balancing algorithms and\n\nruntime logic.\n\nFigure 13.1\n\nA redundant copy of Cloud Service A is implemented on Virtual Server B.\n\nThe load balancer intercepts cloud service consumer requests and directs\n\nthem to both Virtual Servers A and B to ensure even workload distribution.\n\nThis fundamental architectural model can be applied to any IT resource,\n\nwith workload distribution commonly carried out in support of distributed\n\nvirtual servers, cloud storage devices, and cloud services. Load balancing",
      "content_length": 546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "systems applied to specific IT resources usually produce specialized\n\nvariations of this architecture that incorporate aspects of load balancing,\n\nsuch as:\n\nthe service load balancing architecture explained later in this chapter\n\nthe load balanced virtual server architecture covered in Chapter 14\n\nthe load balanced virtual switches architecture described in Chapter 15\n\nIn addition to the base load balancer mechanism, and the virtual server and\n\ncloud storage device mechanisms to which load balancing can be applied,\n\nthe following mechanisms can also be part of this cloud architecture:\n\nAudit Monitor — When distributing runtime workloads, the type and\n\ngeographical location of the IT resources that process the data can\n\ndetermine whether monitoring is necessary to fulfill legal and regulatory\n\nrequirements.\n\nCloud Usage Monitor — Various monitors can be involved to carry out\n\nruntime workload tracking and data processing.\n\nHypervisor — Workloads between hypervisors and the virtual servers that\n\nthey host may require distribution.\n\nLogical Network Perimeter — The logical network perimeter isolates cloud\n\nconsumer network boundaries in relation to how and where workloads are",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "distributed.\n\nResource Cluster — Clustered IT resources in active/active mode are\n\ncommonly used to support workload balancing between different cluster\n\nnodes.\n\nResource Replication — This mechanism can generate new instances of\n\nvirtualized IT resources in response to runtime workload distribution\n\ndemands.\n\n13.2 Resource Pooling Architecture\n\nA resource pooling architecture is based on the use of one or more resource\n\npools, in which identical IT resources are grouped and maintained by a\n\nsystem that automatically ensures that they remain synchronized.\n\nProvided here are common examples of resource pools:",
      "content_length": 615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "Physical server pools are composed of networked servers that have been\n\ninstalled with operating systems and other necessary programs and/or\n\napplications and are ready for immediate use.\n\nVirtual server pools are usually configured using one of several available\n\ntemplates chosen by the cloud consumer during provisioning. For example,\n\na cloud consumer can set up a pool of mid-tier Windows servers with 4 GB\n\nof RAM or a pool of low-tier Ubuntu servers with 2 GB of RAM.\n\nStorage pools, or cloud storage device pools, consist of file-based or block-\n\nbased storage structures that contain empty and/or filled cloud storage\n\ndevices.\n\nNetwork pools (or interconnect pools) are composed of different\n\npreconfigured network connectivity devices. For example, a pool of virtual\n\nfirewall devices or physical network switches can be created for redundant\n\nconnectivity, load balancing, or link aggregation.\n\nCPU pools are ready to be allocated to virtual servers, and are typically\n\nbroken down into individual processing cores.\n\nPools of physical RAM can be used in newly provisioned physical servers\n\nor to vertically scale physical servers.",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "Dedicated pools can be created for each type of IT resource and individual\n\npools can be grouped into a larger pool, in which case each individual pool\n\nbecomes a sub-pool (Figure 13.2).\n\nFigure 13.2\n\nA sample resource pool that is comprised of four sub-pools of CPUs,\n\nmemory, cloud storage devices, and virtual network devices.\n\nResource pools can become highly complex, with multiple pools created for\n\nspecific cloud consumers or applications. A hierarchical structure can be\n\nestablished to form parent, sibling, and nested pools in order to facilitate the\n\norganization of diverse resource pooling requirements (Figure 13.3).",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "Figure 13.3\n\nPools B and C are sibling pools that are taken from the larger Pool A,\n\nwhich has been allocated to a cloud consumer. This is an alternative to\n\ntaking the IT resources for Pool B and Pool C from a general reserve of IT\n\nresources that is shared throughout the cloud.\n\nSibling resource pools are usually drawn from physically grouped IT\n\nresources, as opposed to IT resources that are spread out over different data\n\ncenters. Sibling pools are isolated from one another so that each cloud\n\nconsumer is only provided access to its respective pool.\n\nIn the nested pool model, larger pools are divided into smaller pools that\n\nindividually group the same type of IT resources together (Figure 13.4).\n\nNested pools can be used to assign resource pools to different departments\n\nor groups in the same cloud consumer organization.",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "Figure 13.4\n\nNested Pools A.1 and Pool A.2 are comprised of the same IT resources as\n\nPool A, but in different quantities. Nested pools are typically used to\n\nprovision cloud services that need to be rapidly instantiated using the same\n\ntype of IT resources with the same configuration settings.",
      "content_length": 295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "After resources pools have been defined, multiple instances of IT resources\n\nfrom each pool can be created to provide an in-memory pool of “live” IT\n\nresources.\n\nIn addition to cloud storage devices and virtual servers, which are\n\ncommonly pooled mechanisms, the following mechanisms can also be part\n\nof this cloud architecture:\n\nAudit Monitor — This mechanism monitors resource pool usage to ensure\n\ncompliance with privacy and regulation requirements, especially when\n\npools contain cloud storage devices or data loaded into memory.\n\nCloud Usage Monitor — Various cloud usage monitors are involved in the\n\nruntime tracking and synchronization that are required by the pooled IT\n\nresources and any underlying management systems.\n\nHypervisor — The hypervisor mechanism is responsible for providing\n\nvirtual servers with access to resource pools, in addition to hosting the\n\nvirtual servers and sometimes the resource pools themselves.\n\nLogical Network Perimeter — The logical network perimeter is used to\n\nlogically organize and isolate resource pools.\n\nPay-Per-Use Monitor — The pay-per-use monitor collects usage and billing\n\ninformation on how individual cloud consumers are allocated and use IT\n\nresources from various pools.",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "Remote Administration System — This mechanism is commonly used to\n\ninterface with backend systems and programs in order to provide resource\n\npool administration features via a front-end portal.\n\nResource Management System — The resource management system\n\nmechanism supplies cloud consumers with the tools and permission\n\nmanagement options for administering resource pools.\n\nResource Replication — This mechanism is used to generate new instances\n\nof IT resources for resource pools.\n\n13.3 Dynamic Scalability Architecture\n\nThe dynamic scalability architecture is an architectural model based on a\n\nsystem of predefined scaling conditions that trigger the dynamic allocation\n\nof IT resources from resource pools. Dynamic allocation enables variable\n\nutilization as dictated by usage demand fluctuations, since unnecessary IT\n\nresources are efficiently reclaimed without requiring manual interaction.\n\nThe automated scaling listener is configured with workload thresholds that\n\ndictate when new IT resources need to be added to the workload processing.\n\nThis mechanism can be provided with logic that determines how many\n\nadditional IT resources can be dynamically provided, based on the terms of\n\na given cloud consumer’s provisioning contract.\n\nThe following types of dynamic scaling are commonly used:",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "Dynamic Horizontal Scaling — IT resource instances are scaled out and in\n\nto handle fluctuating workloads. The automatic scaling listener monitors\n\nrequests and signals resource replication to initiate IT resource duplication,\n\nas per requirements and permissions.\n\nDynamic Vertical Scaling — IT resource instances are scaled up and down\n\nwhen there is a need to adjust the processing capacity of a single IT\n\nresource. For example, a virtual server that is being overloaded can have its\n\nmemory dynamically increased or it may have a processing core added.\n\nDynamic Relocation — The IT resource is relocated to a host with more\n\ncapacity. For example, a database may need to be moved from a tape-based\n\nSAN storage device with 4 GB per second I/O capacity to another disk-\n\nbased SAN storage device with 8 GB per second I/O capacity.\n\nFigures 13.5 to 13.7 illustrate the process of dynamic horizontal scaling.",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "Figure 13.5\n\nCloud service consumers are sending requests to a cloud service (1). The\n\nautomated scaling listener monitors the cloud service to determine if\n\npredefined capacity thresholds are being exceeded (2).",
      "content_length": 212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "Figure 13.6\n\nThe number of requests coming from cloud service consumers increases (3).\n\nThe workload exceeds the performance thresholds. The automated scaling\n\nlistener determines the next course of action based on a predefined scaling\n\npolicy (4). If the cloud service implementation is deemed eligible for\n\nadditional scaling, the automated scaling listener initiates the scaling\n\nprocess (5).",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "Figure 13.7\n\nThe automated scaling listener sends a signal to the resource replication\n\nmechanism (6), which creates more instances of the cloud service (7). Now\n\nthat the increased workload has been accommodated, the automated\n\nscaling listener resumes monitoring and detracting and adding IT\n\nresources, as required (8).\n\nThe dynamic scalability architecture can be applied to a range of IT\n\nresources, including virtual servers and cloud storage devices. Besides the\n\ncore automated scaling listener and resource replication mechanisms, the\n\nfollowing mechanisms can also be used in this form of cloud architecture:",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "Cloud Usage Monitor — Specialized cloud usage monitors can track\n\nruntime usage in response to dynamic fluctuations caused by this\n\narchitecture.\n\nHypervisor — The hypervisor is invoked by a dynamic scalability system to\n\ncreate or remove virtual server instances, or to be scaled itself.\n\nPay-Per-Use Monitor — The pay-per-use monitor is engaged to collect\n\nusage cost information in response to the scaling of IT resources.\n\n13.4 Elastic Resource Capacity Architecture\n\nThe elastic resource capacity architecture is primarily related to the\n\ndynamic provisioning of virtual servers, using a system that allocates and\n\nreclaims CPUs and RAM in immediate response to the fluctuating\n\nprocessing requirements of hosted IT resources (Figures 13.8 and 13.9).",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "Figure 13.8",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "Cloud service consumers are actively sending requests to a cloud service\n\n(1), which are monitored by an automated scaling listener (2). An\n\nintelligent automation engine script is deployed with workflow logic (3) that\n\nis capable of notifying the resource pool using allocation requests (4).",
      "content_length": 292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "Figure 13.9",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "Cloud service consumer requests increase (5), causing the automated\n\nscaling listener to signal the intelligent automation engine to execute the\n\nscript (6). The script runs the workflow logic that signals the hypervisor to\n\nallocate more IT resources from the resource pools (7). The hypervisor\n\nallocates additional CPU and RAM to the virtual server, enabling the\n\nincreased workload to be handled (8).\n\nResource pools are used by scaling technology that interacts with the\n\nhypervisor and/or VIM to retrieve and return CPU and RAM resources at\n\nruntime. The runtime processing of the virtual server is monitored so that\n\nadditional processing power can be leveraged from the resource pool via\n\ndynamic allocation, before capacity thresholds are met. The virtual server\n\nand its hosted applications and IT resources are vertically scaled in\n\nresponse.\n\nThis type of cloud architecture can be designed so that the intelligent\n\nautomation engine script sends its scaling request via the VIM instead of to\n\nthe hypervisor directly. Virtual servers that participate in elastic resource\n\nallocation systems may require rebooting in order for the dynamic resource\n\nallocation to take effect.",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "Intelligent\n\nAutomation Engine\n\nThe intelligent automation engine automates\n\nadministration tasks by executing scripts that contain\n\nworkflow logic.\n\nSome additional mechanisms that can be included in this cloud architecture\n\nare the following:\n\nCloud Usage Monitor — Specialized cloud usage monitors collect resource\n\nusage information on IT resources before, during, and after scaling, to help\n\ndefine the future processing capacity thresholds of the virtual servers.\n\nPay-Per-Use Monitor — The pay-per-use monitor is responsible for\n\ncollecting resource usage cost information as it fluctuates with the elastic\n\nprovisioning.\n\nResource Replication — Resource replication is used by this architectural\n\nmodel to generate new instances of the scaled IT resources.\n\n13.5 Service Load Balancing Architecture\n\nThe service load balancing architecture can be considered a specialized\n\nvariation of the workload distribution architecture that is geared specifically",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "for scaling cloud service implementations. Redundant deployments of cloud\n\nservices are created, with a load balancing system added to dynamically\n\ndistribute workloads.\n\nThe duplicate cloud service implementations are organized into a resource\n\npool, while the load balancer is positioned as either an external or built-in\n\ncomponent to allow the host servers to balance the workloads themselves.\n\nDepending on the anticipated workload and processing capacity of host\n\nserver environments, multiple instances of each cloud service\n\nimplementation can be generated as part of a resource pool that responds to\n\nfluctuating request volumes more efficiently.\n\nThe load balancer can be positioned either independent of the cloud\n\nservices and their host servers (Figure 13.10), or built-in as part of the\n\napplication or server’s environment. In the latter case, a primary server with\n\nthe load balancing logic can communicate with neighboring servers to\n\nbalance the workload (Figure 13.11).",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "Figure 13.10",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "The load balancer intercepts messages sent by cloud service consumers (1)\n\nand forwards them to the virtual servers so that the workload processing is\n\nhorizontally scaled (2).",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "Figure 13.11\n\nCloud service consumer requests are sent to Cloud Service A on Virtual\n\nServer A (1). The cloud service implementation includes built-in load\n\nbalancing logic that is capable of distributing requests to the neighboring\n\nCloud Service A implementations on Virtual Servers B and C (2).\n\nThe service load balancing architecture can involve the following\n\nmechanisms in addition to the load balancer:\n\nCloud Usage Monitor — Cloud usage monitors may be involved with\n\nmonitoring cloud service instances and their respective IT resource\n\nconsumption levels, as well as various runtime monitoring and usage data\n\ncollection tasks.\n\nResource Cluster — Active-active cluster groups are incorporated in this\n\narchitecture to help balance workloads across different members of the\n\ncluster.\n\nResource Replication — The resource replication mechanism is utilized to\n\ngenerate cloud service implementations in support of load balancing\n\nrequirements.\n\n13.6 Cloud Bursting Architecture\n\nThe cloud bursting architecture establishes a form of dynamic scaling that\n\nscales or “bursts out” on-premise IT resources into a cloud whenever",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "predefined capacity thresholds have been reached. The corresponding\n\ncloud-based IT resources are redundantly pre-deployed but remain inactive\n\nuntil cloud bursting occurs. After they are no longer required, the cloud-\n\nbased IT resources are released and the architecture “bursts in” back to the\n\non-premise environment.\n\nCloud bursting is a flexible scaling architecture that provides cloud\n\nconsumers with the option of using cloud-based IT resources only to meet\n\nhigher usage demands. The foundation of this architectural model is based\n\non the automated scaling listener and resource replication mechanisms.\n\nThe automated scaling listener determines when to redirect requests to\n\ncloud-based IT resources, and resource replication is used to maintain\n\nsynchronicity between on-premise and cloud-based IT resources in relation\n\nto state information (Figure 13.12).",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "Figure 13.12\n\nAn automated scaling listener monitors the usage of on-premise Service A,\n\nand redirects Service Consumer C’s request to Service A’s redundant\n\nimplementation in the cloud (Cloud Service A) once Service A’s usage\n\nthreshold has been exceeded (1). A resource replication system is used to\n\nkeep state management databases synchronized (2).\n\nIn addition to the automated scaling listener and resource replication,\n\nnumerous other mechanisms can be used to automate the burst in and out\n\ndynamics for this architecture, depending primarily on the type of IT\n\nresource being scaled.",
      "content_length": 592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": "13.7 Elastic Disk Provisioning Architecture\n\nCloud consumers are commonly charged for cloud-based storage space\n\nbased on fixed-disk storage allocation, meaning the charges are\n\npredetermined by disk capacity and not aligned with actual data storage\n\nconsumption. Figure 13.13 demonstrates this by illustrating a scenario in\n\nwhich a cloud consumer provisions a virtual server with the Windows\n\nServer operating system and three 150 GB hard drives. The cloud consumer\n\nis billed for using 450 GB of storage space after installing the operating\n\nsystem, even though it has not yet installed any software.",
      "content_length": 603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": "Figure 13.13\n\nThe cloud consumer requests a virtual server with three hard disks, each\n\nwith a capacity of 150 GB (1). The virtual server is provisioned according\n\nto the elastic disk provisioning architecture, with a total of 450 GB of disk\n\nspace (2). The 450 GB is allocated to the virtual server by the cloud\n\nprovider (3). The cloud consumer has not installed any software yet,",
      "content_length": 382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "meaning the actual used space is currently 0 GB (4). Because the 450 GB\n\nare already allocated and reserved for the cloud consumer, it will be\n\ncharged for 450 GB of disk usage as of the point of allocation (5).\n\nThe elastic disk provisioning architecture establishes a dynamic storage\n\nprovisioning system that ensures that the cloud consumer is granularly\n\nbilled for the exact amount of storage that it actually uses. This system uses\n\nthin-provisioning technology for the dynamic allocation of storage space,\n\nand is further supported by runtime usage monitoring to collect accurate\n\nusage data for billing purposes (Figure 13.14).",
      "content_length": 635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "Figure 13.14\n\nThe cloud consumer requests a virtual server with three hard disks, each\n\nwith a capacity of 150 GB (1). The virtual server is provisioned by this\n\narchitecture with a total of 450 GB of disk space (2). The 450 GB are set as\n\nthe maximum disk usage that is allowed for this virtual server, although no",
      "content_length": 315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "physical disk space has been reserved or allocated yet (3). The cloud\n\nconsumer has not installed any software, meaning the actual used space is\n\ncurrently at 0 GB (4). Because the allocated disk space is equal to the\n\nactual used space (which is currently at zero), the cloud consumer is not\n\ncharged for any disk space usage (5).\n\nThin-provisioning software is installed on virtual servers that process\n\ndynamic storage allocation via the hypervisor, while the pay-per-use\n\nmonitor tracks and reports granular billing-related disk usage data (Figure\n\n13.15).",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "Figure 13.15\n\nA request is received from a cloud consumer, and the provisioning of a new\n\nvirtual server instance begins (1). As part of the provisioning process, the\n\nhard disks are chosen as dynamic or thin-provisioned disks (2). The\n\nhypervisor calls a dynamic disk allocation component to create thin disks\n\nfor the virtual server (3). Virtual server disks are created via the thin-\n\nprovisioning program and saved in a folder of near-zero size. The size of\n\nthis folder and its files grow as operating applications are installed and",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": "additional files are copied onto the virtual server (4). The pay-per-use\n\nmonitor tracks the actual dynamically allocated storage for billing\n\npurposes (5).\n\nThe following mechanisms can be included in this architecture in addition\n\nto the cloud storage device, virtual server, hypervisor, and pay-per-use\n\nmonitor:\n\nCloud Usage Monitor — Specialized cloud usage monitors can be used to\n\ntrack and log storage usage fluctuations.\n\nResource Replication — Resource replication is part of an elastic disk\n\nprovisioning system when conversion of dynamic thin-disk storage into\n\nstatic thick-disk storage is required.\n\n13.8 Redundant Storage Architecture\n\nCloud storage devices are occasionally subject to failure and disruptions\n\nthat are caused by network connectivity issues, controller or general\n\nhardware failure, or security breaches. A compromised cloud storage\n\ndevice’s reliability can have a ripple effect and cause impact failure across\n\nall of the services, applications, and infrastructure components in the cloud\n\nthat are reliant on its availability.",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "LUN\n\nA logical unit number (LUN) is a logical drive that\n\nrepresents a partition of a physical drive.",
      "content_length": 101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "Storage service gateway\n\nThe storage service gateway is a component that acts as\n\nthe external interface to cloud storage services, and is\n\ncapable of automatically redirecting cloud consumer\n\nrequests whenever the location of the requested data has\n\nchanged.\n\nThe redundant storage architecture introduces a secondary duplicate cloud\n\nstorage device as part of a failover system that synchronizes its data with\n\nthe data in the primary cloud storage device. A storage service gateway\n\ndiverts cloud consumer requests to the secondary device whenever the\n\nprimary device fails (Figures 13.16 and 13.17).",
      "content_length": 603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "Figure 13.16\n\nThe primary cloud storage device is routinely replicated to the secondary\n\ncloud storage device (1).",
      "content_length": 114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "Figure 13.17\n\nThe primary storage becomes unavailable and the storage service gateway\n\nforwards the cloud consumer requests to the secondary storage device (2).\n\nThe secondary storage device forwards the requests to the LUNs, allowing\n\ncloud consumers to continue to access their data (3).\n\nThis cloud architecture primarily relies on a storage replication system that\n\nkeeps the primary cloud storage device synchronized with its duplicate\n\nsecondary cloud storage devices (Figure 13.18).",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "Figure 13.18\n\nStorage replication is used to keep the redundant storage device\n\nsynchronized with the primary storage device.",
      "content_length": 125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "Storage replication\n\nStorage replication is a variation of the resource\n\nreplication mechanisms used to synchronously or\n\nasynchronously replicate data from a primary storage\n\ndevice to a secondary storage device. It can be used to\n\nreplicate partial and entire LUNs.\n\nCloud providers may locate secondary cloud storage devices in a different\n\ngeographical region than the primary cloud storage device, usually for",
      "content_length": 414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "economic reasons. However, this can introduce legal concerns for some\n\ntypes of data. The location of the secondary cloud storage devices can\n\ndictate the protocol and method used for synchronization, as some\n\nreplication transport protocols have distance restrictions.\n\nSome cloud providers use storage devices with dual array and storage\n\ncontrollers to improve device redundancy, and place secondary storage\n\ndevices in a different physical location for cloud balancing and disaster\n\nrecovery purposes. In this case, cloud providers may need to lease a\n\nnetwork connection via a third-party cloud provider in order to establish the\n\nreplication between the two devices.\n\n13.9 Multi-Cloud Architecture\n\nA cloud architecture that combines two or more public clouds is referred to\n\nas a multi-cloud architecture (Figure 13.19). The different clouds combined\n\nin this type of architecture may offer their resources through any of the\n\ncloud delivery models, namely IaaS, PaaS, or SaaS. One of the fundamental\n\nreasons to utilize a multi-cloud architecture is avoiding vendor lock-in that\n\nis caused by forming dependencies on only a single cloud provider.",
      "content_length": 1154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "Figure 13.19\n\nAn organization uses different types of resources from different clouds,\n\ntaking advantage of those resources that each cloud is better at and\n\navoiding vendor lock-in.",
      "content_length": 182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "When using multi-cloud architectures, cloud consumers commonly select\n\nproviders for specific resources or services, based on advantages or benefits\n\nthey might have over others.\n\nReasons for selecting one cloud provider over another can be any of the\n\nfollowing:\n\ngeographical – when the physical geographical location of resources\n\nrequires cloud consumers to use local cloud providers for regulatory\n\npurposes\n\neconomical – prices or billing models\n\noperational – seeking higher capacity, more resiliency, or better peformance\n\nfunctional – looking for more features, specific capabilities required by the\n\ncloud consumer, or better quality in general\n\nFor cloud consumers to be able to make use of IT resources distributed\n\nacross different clouds, the cloud resource administrator uses a centralized\n\nremote administration system mechanism that connects to the management\n\nsystems of each individual cloud provider via their respective APIs (Figure\n\n13.20). This allows the cloud consumer to manage all cloud-based IT\n\nresources from a central location and then use and access them as easily as\n\nif if they were coming from a single cloud.",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "Figure 13.20\n\nA cloud resource administrator utilizes a remote administration system\n\nmechanism to connect to the individual management systems of each\n\ndifferent cloud provider in order to manage their resources from a central\n\nmanagement location.",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "The ultimate business benefits resulting from the use of a multi-cloud\n\narchitecture can be very different for each individual cloud consumer.\n\nWhether the goal of an organizations is to maximize agility, minimize\n\ntechnology costs, or optimize its profit margins, a multi-cloud architecture\n\nwill allow the organization to mix-and-match and choose best-of-breed\n\ncloud-based resources and services from multiple, competing cloud\n\nproviders.\n\n13.10 Case Study Example\n\nAn in-house solution that ATN did not migrate to the\n\ncloud is the Remote Upload Module, a program that is\n\nused by their clients to upload accounting and legal\n\ndocuments to a central archive on a daily basis. Usage\n\npeaks occur without warning, since the quantity of\n\ndocuments received on a day-by-day basis is\n\nunpredictable.\n\nThe Remote Upload Module currently rejects upload\n\nattempts when it is operating at capacity, which is\n\nproblematic for users that need to archive certain\n\ndocuments before the end of a business day or prior to a\n\ndeadline.",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "ATN decides to take advantage of its cloud-based\n\nenvironment by creating a cloud-bursting architecture\n\naround the on-premise Remote Upload Module service\n\nimplementation. This enables it to burst out into the\n\ncloud whenever on-premise processing thresholds are\n\nexceeded (Figures 13.21 and 13.22).\n\nFigure 13.21",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "A cloud-based version of the on-premise Remote Upload\n\nModule service is deployed on ATN’s leased ready-made\n\nenvironment (1). The automated scaling listener\n\nmonitors service consumer requests (2).",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "Figure 13.22\n\nThe automated scaling listener detects that service\n\nconsumer usage has exceeded the local Remote Upload\n\nModule service’s usage threshold, and begins diverting\n\nexcess requests to the cloud-based Remote Upload\n\nModule implementation (3). The cloud provider’s pay-\n\nper-use monitor tracks the requests received from the on-\n\npremise automated scaling listener to collect billing\n\ndata, and Remote Upload Module cloud service\n\ninstances are created on-demand via resource\n\nreplication (4).\n\nA “burst in” system is invoked after the service usage\n\nhas decreased enough so that service consumer requests\n\ncan be processed by the on-premise Remote Upload\n\nModule implementation again. Instances of the cloud\n\nservices are released, and no additional cloud-related\n\nusage fees are incurred.",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "Chapter 14\n\nAdvanced Cloud Architectures\n\n14.1 Hypervisor Clustering Architecture\n\n14.2 Virtual Server Clustering Architecture\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\n14.4 Non-Disruptive Service Relocation Architecture\n\n14.5 Zero Downtime Architecture\n\n14.6 Cloud Balancing Architecture\n\n14.7 Resilient Disaster Recovery Architecture\n\n14.8 Distributed Data Sovereignty Architecture\n\n14.9 Resource Reservation Architecture\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\n14.11 Rapid Provisioning Architecture\n\n14.12 Storage Workload Management Architecture",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "14.13 Virtual Private Cloud Architecture\n\n14.14 Case Study Example\n\nThe following cloud technology architectures are explored in this chapter:\n\nHypervisor Clustering\n\nVirtual Server Clustering\n\nLoad Balanced Virtual Server Instances\n\nNon-Disruptive Service Relocation\n\nZero Downtime\n\nCloud Balancing\n\nResilient Disaster Recovery\n\nDistributed Data Sovereignty\n\nResource Reservation\n\nDynamic Failure Detection and Recovery\n\nRapid Provisioning\n\nStorage Workload Management",
      "content_length": 469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "Virtual Private Cloud\n\nThese models represent distinct and sophisticated architectural layers,\n\nseveral of which can be built upon the more foundational environments\n\nestablished by the architectures covered in Chapter 13. For each\n\narchitecture, the associated mechanisms are also documented.\n\n14.1 Hypervisor Clustering Architecture\n\nHypervisors can be responsible for creating and hosting multiple virtual\n\nservers. Because of this dependency, any failure conditions that affect a\n\nhypervisor can cascade to its virtual servers (Figure 14.1).",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "Figure 14.1\n\nPhysical Server A is hosting a hypervisor that hosts Virtual Servers A and B\n\n(1). When Physical Server A fails, the hypervisor and two virtual servers\n\nconsequently fail as well (2).",
      "content_length": 196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "Heartbeats\n\nHeartbeats are -system-level messages exchanged\n\nbetween hypervisors, between hypervisors and -virtual\n\nservers, and between hypervisors and VIMs.\n\nThe hypervisor clustering architecture establishes a high-availability cluster\n\nof hypervisors across multiple physical servers. If a given hypervisor or its\n\nunderlying physical server becomes unavailable, the hosted virtual servers\n\ncan be moved to another physical server or hypervisor to maintain runtime\n\noperations (Figure 14.2).",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "Figure 14.2\n\nPhysical Server A becomes unavailable and causes its hypervisor to fail.\n\nVirtual Server A is migrated to Physical Server B, which has another",
      "content_length": 155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "hypervisor that is part of the cluster to which Physical Server A belongs.\n\nThe hypervisor cluster is controlled via a central VIM, which sends regular\n\nheartbeat messages to the hypervisors to confirm that they are up and\n\nrunning. Unacknowledged heartbeat messages cause the VIM to initiate the\n\nlive VM migration program, in order to dynamically move the affected\n\nvirtual servers to a new host.\n\nLive VM Migration\n\nLive VM migration is a system that is capable of\n\nrelocating virtual servers or virtual server instances at\n\nruntime.\n\nThe hypervisor cluster uses a shared cloud storage device to live-migrate\n\nvirtual servers, as illustrated in Figures 14.3 to 14.6.",
      "content_length": 669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "Figure 14.3\n\nHypervisors are installed on Physical Servers A, B, and C (1). Virtual\n\nservers are created by the hypervisors (2). A shared cloud storage device",
      "content_length": 158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "containing virtual server configuration files is positioned in a shared cloud\n\nstorage device for access by all hypervisors (3). The hypervisor cluster is\n\nenabled on the three physical server hosts via a central VIM (4).",
      "content_length": 221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "Figure 14.4\n\nThe physical servers exchange heartbeat messages with one another and\n\nthe VIM according to a pre-defined schedule (5).",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 719,
      "content": "Figure 14.5\n\nPhysical Server B fails and becomes unavailable, jeopardizing Virtual\n\nServer C (6). The other physical servers and the VIM stop receiving\n\nheartbeat messages from Physical Server B (7).",
      "content_length": 199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 720,
      "content": "Figure 14.6",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 721,
      "content": "The VIM chooses Physical Server C as the new host to take ownership of\n\nVirtual Server C after assessing the available capacity of other hypervisors\n\nin the cluster (8). Virtual Server C is live-migrated to the hypervisor\n\nrunning on Physical Server C, where restarting may be necessary before\n\nnormal operations can be resumed (9).\n\nIn addition to the hypervisor and resource cluster mechanisms that form the\n\ncore of this architectural model and the virtual servers that are protected by\n\nthe clustered environment, the following mechanisms can be incorporated:\n\nLogical Network Perimeter — The logical boundaries created by this\n\nmechanism ensure that none of the hypervisors of other cloud consumers\n\nare accidentally included in a given cluster.\n\nResource Replication — Hypervisors in the same cluster inform one\n\nanother about their status and availability. Updates on any changes that\n\noccur in the cluster, such as the creation or deletion of a virtual switch, need\n\nto be replicated to all of the hypervisors via the VIM.\n\n14.2 Virtual Server Clustering Architecture\n\nA virtual server clustering architecture represents the deployment of one or\n\nmore clusters of virtual servers on physical hosts running hypervisors. This\n\narchitecture is focused on leveraging the efficiency, resiliency, and\n\nscalability that a cloud can provide for clusters of servers through the use of\n\nvirtualization.",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 722,
      "content": "Individual virtual servers are instantiated on top of separate physical hosts\n\nrunning hypervisors (Figure 14.7). This provides the virtual infrastructure\n\non which virtual server clusters can be configured for different purposes,\n\nsuch as big data analytics, service-oriented architectures, distributed\n\nNoSQL databases, and advanced container management platforms.",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 723,
      "content": "Figure 14.7",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 724,
      "content": "Physical Servers A, B and C are running hypervisors that allow multiple\n\nvirtual servers to be hosted on each. (These virtual servers are then\n\nconfigured by a resource cluster mechanism into clusters of virtual servers.)\n\nThe following mechanisms can be included in this architecture, in addition\n\nto the hypervisor, resource cluster, and virtual server:\n\nLogical Network Perimeter – A logical network perimeter ensures that the\n\nvirtual server cluster is enclosed in a connected environment that allows all\n\nof its nodes to communicate securely with each other.\n\nResource Replication – Virtual servers in the same cluster inform one\n\nanother about their status and availability. Updates on any changes that\n\noccur in the cluster, such as the creation or deletion of a virtual switch, need\n\nto be replicated to all virtual servers.\n\n14.3 Load Balanced Virtual Server Instances Architecture\n\nKeeping cross-server workloads evenly balanced between physical servers\n\nwhose operation and management are isolated can be challenging. A\n\nphysical server can easily end up hosting more virtual servers or receive\n\nlarger workloads than its neighboring physical servers (Figure 14.8). Both\n\nphysical server over and under-utilization can increase dramatically over\n\ntime, leading to on-going performance challenges (for over-utilized servers)\n\nand constant waste (for the lost processing potential of under-utilized\n\nservers).",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 725,
      "content": "Figure 14.8\n\nThree physical servers have to host different quantities of virtual server\n\ninstances, leading to both over-utilized and under-utilized servers.\n\nThe load balanced virtual server instances architecture establishes a\n\ncapacity watchdog system that dynamically calculates virtual server\n\ninstances and associated workloads, before distributing the processing\n\nacross available physical server hosts (Figure 14.9).",
      "content_length": 424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 726,
      "content": "Figure 14.9\n\nThe virtual server instances are more evenly distributed across the physical\n\nserver hosts.",
      "content_length": 104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 727,
      "content": "The capacity watchdog system is comprised of a capacity watchdog cloud\n\nusage monitor, the live VM migration program, and a capacity planner. The\n\ncapacity watchdog monitor tracks physical and virtual server usage and\n\nreports any significant fluctuations to the capacity planner, which is\n\nresponsible for dynamically calculating physical server computing\n\ncapacities against virtual server capacity requirements. If the capacity\n\nplanner decides to move a virtual server to another host to distribute the\n\nworkload, the live VM migration program is signaled to move the virtual\n\nserver (Figures 14.10 to 14.12).",
      "content_length": 613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 728,
      "content": "Figure 14.10",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 729,
      "content": "The hypervisor cluster architecture provides the foundation upon which the\n\nload-balanced virtual server architecture is built (1). Policies and\n\nthresholds are defined for the capacity watchdog monitor (2), which\n\ncompares physical server capacities with virtual server processing (3). The\n\ncapacity watchdog monitor reports an over-utilization to the VIM (4).",
      "content_length": 361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 730,
      "content": "Figure 14.11\n\nThe VIM signals the load balancer to redistribute the workload based on\n\npre-defined thresholds (5). The load balancer initiates the live VM",
      "content_length": 154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 731,
      "content": "migration program to move the virtual servers (6). Live VM migration\n\nmoves the selected virtual servers from one physical host to another (7).\n\nFigure 14.12",
      "content_length": 157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 732,
      "content": "The workload is balanced across the physical servers in the cluster (8). The\n\ncapacity watchdog continues to monitor the workload and resource\n\nconsumption (9).\n\nThe following mechanisms can be included in this architecture, in addition\n\nto the hypervisor, resource clustering, virtual server, and (capacity\n\nwatchdog) cloud usage monitor:\n\nAutomated Scaling Listener — The automated scaling listener may be used\n\nto initiate the process of load balancing and to dynamically monitor\n\nworkload coming to the virtual servers via the hypervisors.\n\nLoad Balancer — The load balancer mechanism is responsible for\n\ndistributing the workload of the virtual servers between the hypervisors.\n\nLogical Network Perimeter — A logical network perimeter ensures that the\n\ndestination of a given relocated virtual server is in compliance with SLA\n\nand privacy regulations.\n\nResource Replication — The replication of virtual server instances may be\n\nrequired as part of the load balancing functionality.\n\n14.4 Non-Disruptive Service Relocation Architecture\n\nA cloud service can become unavailable for a number of reasons, such as:\n\nruntime usage demands that exceed its processing capacity",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 733,
      "content": "a maintenance update that mandates a temporary outage\n\npermanent migration to a new physical server host\n\nCloud service consumer requests are usually rejected if a cloud service\n\nbecomes unavailable, which can potentially result in exception conditions.\n\nRendering the cloud service temporarily unavailable to cloud consumers is\n\nnot preferred even if the outage is planned.\n\nThe non-disruptive service relocation architecture establishes a system by\n\nwhich a pre-defined event triggers the duplication or migration of a cloud\n\nservice implementation at runtime, thereby avoiding any disruption. Instead\n\nof scaling cloud services in or out with redundant implementations, cloud\n\nservice activity can be temporarily diverted to another hosting environment\n\nat runtime by adding a duplicate implementation onto a new host. Similarly,\n\ncloud service consumer requests can be temporarily redirected to a\n\nduplicate implementation when the original implementation needs to\n\nundergo a maintenance outage. The relocation of the cloud service\n\nimplementation and any cloud service activity can also be permanent to\n\naccommodate cloud service migrations to new physical server hosts.\n\nA key aspect of the underlying architecture is that the new cloud service\n\nimplementation is guaranteed to be successfully receiving and responding\n\nto cloud service consumer requests before the original cloud service\n\nimplementation is deactivated or removed. A common approach is for live",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 734,
      "content": "VM migration to move the entire virtual server instance that is hosting the\n\ncloud service. The automated scaling listener and/or load balancer\n\nmechanisms can be used to trigger a temporary redirection of cloud service\n\nconsumer requests, in response to scaling and workload distribution\n\nrequirements. Either mechanism can contact the VIM to initiate the live VM\n\nmigration process, as shown in Figures 14.13 to 14.15.",
      "content_length": 420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 736,
      "content": "Figure 14.13\n\nThe automated scaling listener monitors the workload for a cloud service\n\n(1). The cloud service’s pre-defined threshold is reached as the workload\n\nincreases (2), causing the automated scaling listener to signal the VIM to\n\ninitiate relocation (3). The VIM uses the live VM migration program to\n\ninstruct both the origin and destination hypervisors to carry out runtime\n\nrelocation (4).",
      "content_length": 401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 738,
      "content": "Figure 14.14\n\nA second copy of the virtual server and its hosted cloud service are created\n\nvia the destination hypervisor on Physical Server B (5).",
      "content_length": 148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 739,
      "content": "Figure 14.15",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 740,
      "content": "The state of both virtual server instances is synchronized (6). The first\n\nvirtual server instance is removed from Physical Server A after cloud\n\nservice consumer requests are confirmed to be successfully exchanged with\n\nthe cloud service on Physical Server B (7). Cloud service consumer\n\nrequests are now only sent to the cloud service on Physical Server B (8).\n\nVirtual server migration can occur in one of the following two ways,\n\ndepending on the location of the virtual server’s disks and configuration:\n\nA copy of the virtual server disks is created on the destination host, if the\n\nvirtual server disks are stored on a local storage device or non-shared\n\nremote storage devices attached to the source host. After the copy has been\n\ncreated, both virtual server instances are synchronized and virtual server\n\nfiles are removed from the origin host.\n\nCopying the virtual server disks is unnecessary if the virtual server’s files\n\nare stored on a remote storage device that is shared between origin and\n\ndestination hosts. Ownership of the virtual server is simply transferred from\n\nthe origin to the destination physical server host, and the virtual server’s\n\nstate is automatically synchronized.\n\nThis architecture can be supported by the persistent virtual network\n\nconfigurations architecture, so that the defined network configurations of\n\nmigrated virtual servers are preserved to retain connection with the cloud\n\nservice consumers.",
      "content_length": 1443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 741,
      "content": "Besides the automated scaling listener, load balancer, cloud storage device,\n\nhypervisor, and virtual server, other mechanisms that can be part of this\n\narchitecture include the following:\n\nCloud Usage Monitor — Different types of cloud usage monitors can be\n\nused to continuously track IT resource usage and system activity.\n\nPay-Per-Use Monitor — The pay-per-use monitor is used to collect data for\n\nservice usage cost calculations for IT resources at both source and\n\ndestination locations.\n\nResource Replication — The resource replication mechanism is used to\n\ninstantiate the shadow copy of the cloud service at its destination.\n\nSLA Management System — This management system is responsible for\n\nprocessing SLA data provided by the SLA monitor to obtain cloud service\n\navailability assurances, both during and after cloud service duplication or\n\nrelocation.\n\nSLA Monitor — This monitoring mechanism collects the SLA information\n\nrequired by the SLA management system, which may be relevant if\n\navailability guarantees rely on this architecture.\n\nNote",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 742,
      "content": "The non-disruptive service relocation technology\n\narchitecture conflicts and cannot be applied together with\n\nthe direct I/O access architecture covered in Chapter 15. A\n\nvirtual server with direct I/O access is locked into its\n\nphysical server host and cannot be moved to other hosts in\n\nthis fashion.\n\n14.5 Zero Downtime Architecture\n\nA physical server naturally acts as a single point of failure for the virtual\n\nservers it hosts. As a result, when the physical server fails or is\n\ncompromised, the availability of any (or all) hosted virtual servers can be\n\naffected. This makes the issuance of zero downtime guarantees by a cloud\n\nprovider to cloud consumers challenging.\n\nThe zero downtime architecture establishes a sophisticated failover system\n\nthat allows virtual servers to be dynamically moved to different physical\n\nserver hosts, in the event that their original physical server host fails (Figure\n\n14.16).",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 743,
      "content": "Figure 14.16\n\nPhysical Server A fails triggering the live VM migration program to\n\ndynamically move Virtual Server A to Physical Server B.",
      "content_length": 138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 744,
      "content": "Multiple physical servers are assembled into a group that is controlled by a\n\nfault tolerance system capable of switching activity from one physical\n\nserver to another, without interruption. The live VM migration component\n\nis typically a core part of this form of high availability cloud architecture.\n\nThe resulting fault tolerance assures that, in case of physical server failure,\n\nhosted virtual servers will be migrated to a secondary physical server. All\n\nvirtual servers are stored on a shared volume (as per the persistent virtual\n\nnetwork configuration architecture) so that other physical server hosts in the\n\nsame group can access their files.\n\nBesides the failover system, cloud storage device, and virtual server\n\nmechanisms, the following mechanisms can be part of this architecture:\n\nAudit Monitor — This mechanism may be required to check whether the\n\nrelocation of virtual servers also relocates hosted data to prohibited\n\nlocations.\n\nCloud Usage Monitor — Incarnations of this mechanism are used to\n\nmonitor the actual IT resource usage of cloud consumers to help ensure that\n\nvirtual server capacities are not exceeded.\n\nHypervisor — The hypervisor of each affected physical server hosts the\n\naffected virtual servers.",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 745,
      "content": "Logical Network Perimeter — Logical network perimeters provide and\n\nmaintain the isolation that is required to ensure that each cloud consumer\n\nremains within its own logical boundary subsequent to virtual server\n\nrelocation.\n\nResource Cluster — The resource cluster mechanism is applied to create\n\ndifferent types of active-active cluster groups that collaboratively improve\n\nthe availability of virtual server-hosted IT resources.\n\nResource Replication — This mechanism can create the new virtual server\n\nand cloud service instances upon primary virtual server failure.\n\n14.6 Cloud Balancing Architecture\n\nThe cloud balancing architecture establishes a specialized architectural\n\nmodel in which IT resources can be load-balanced across multiple clouds.\n\nThe cross-cloud balancing of cloud service consumer requests can help:\n\nimprove the performance and scalability of IT resources\n\nincrease the availability and reliability of IT resources\n\nimprove load-balancing and IT resource optimization\n\nCloud balancing functionality is primarily based on the combination of the\n\nautomated scaling listener and failover system mechanisms (Figure 14.17).",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 746,
      "content": "Many more components (and possibly other mechanisms) can be part of a\n\ncomplete cloud balancing architecture.",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 747,
      "content": "Figure 14.17",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 748,
      "content": "An automated scaling listener controls the cloud balancing process by\n\nrouting cloud service consumer requests to redundant implementations of\n\nCloud Service A distributed across multiple clouds (1). The failover system\n\ninstills resiliency within this architecture by providing cross-cloud failover\n\n(2).\n\nAs a starting point, the two mechanisms are utilized as follows:\n\nThe automated scaling listener redirects cloud service consumer requests to\n\none of several redundant IT resource implementations, based on current\n\nscaling and performance requirements.\n\nThe failover system ensures that redundant IT resources are capable of\n\ncross-cloud failover in the event of a failure within an IT resource or its\n\nunderlying hosting environment. IT resource failures are announced so that\n\nthe automated scaling listener can avoid inadvertently routing cloud service\n\nconsumer requests to unavailable or unstable IT resources.\n\nFor a cloud balancing architecture to function effectively, the automated\n\nscaling listener needs to be aware of all redundant IT resource\n\nimplementations within the scope of the cloud balanced architecture.\n\nNote that if the manual synchronization of cross-cloud IT resource\n\nimplementations is not possible, the resource replication mechanism may\n\nneed to be incorporated to automate the synchronization.\n\n14.7 Resilient Disaster Recovery Architecture",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 749,
      "content": "Natural or man-made disasters can occur anytime and without warning. IT\n\nenterprises can establish disaster recovery strategies to ensure that, in case\n\nan event destroys or limits the functionality of important IT systems, a\n\nsecondary remote location is available with redundant implementations of\n\nthose systems that can then take over. This is the purpose of the resilient\n\ndisaster recovery architecture.\n\nCloud providers offer cloud-based IT resources with high levels of\n\navailability, which makes cloud environments ideal secondary sites for the\n\nprotection of on-premise IT resources from disasters. The ubiquitous access\n\nand resiliency cloud characteristics support this architecture when deployed\n\nin a public cloud, since resources located therein are available anytime,\n\nanywhere and accessible by many means.\n\nA resilient disaster recovery architecture uses resource replication\n\nmechanisms to create redundant copies of all the critical resources in an\n\nenterprise technology architecture. These copies are then placed in a remote\n\nlocation where they are expected to stay synchronized with their original\n\ncopies, ready to replace the originals in case a major catastrophe happens in\n\nthe original location (Figure 14.18).",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 750,
      "content": "Figure 14.18",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 751,
      "content": "An organization uses a resource replication mechanism to create duplicate\n\nvirtual instances of its physical infrastructure in a pubic cloud. The storage\n\nreplication mechanism synchronizes on-premise data sources with their\n\nduplicates in the cloud.\n\nThe resource replication mechanism keeps the replicated IT resources in the\n\ncloud-based section of the architecture in constant synchronization with its\n\noriginal copies. Other mechanisms that can be part of this architecture\n\ninclude the following:\n\nHypervisor – The hypervisor mechanism allows physical hosts in the cloud\n\nenvironment selected for redundancy to host virtual servers that are replicas\n\nof the physical or virtual servers on-premise.\n\nVirtual Server – The virtual server mechanism is used to keep synchronized\n\nreplicas of original on-premise physical of virtual servers in the redundant\n\ncloud architecture.\n\nCloud Storage Device – The cloud storage device mechanism stores\n\nredundant copies of data from the original on-premise site in the replicated\n\ncloud-based site.\n\n14.8 Distributed Data Sovereignty Architecture\n\nRegulations regarding the appropriate governance of data, particularly\n\npersonal data, can vary across different countries and regions. Typically,\n\nsuch regulations require data holders to ensure that the data protected by the",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 752,
      "content": "regulation is physically located within a particular geographical boundary.\n\nUsually cloud consumers are considered the official data holders of cloud-\n\nbased data, while cloud providers are not commonly required to comply\n\nwith these types of regulations.\n\nCloud providers commonly use sophisticated data replication systems to\n\nachieve redundancy levels that allow them to provide high availability for\n\nthe cloud storage services they offer. The replicas are often geographically\n\ndistributed in order to guarantee the highest level of availability possible\n\nbecause this distribution provides a higher degree of isolation from\n\npotential failures.\n\nHowever, geographical distribution might result in a cloud provider keeping\n\ncopies of protected data in locations that may be in violation of data\n\nprotection regulations that its cloud consumers are required to comply to.\n\nThe distributed data sovereignty architecture is a model that can used to\n\navoid this situation by ensuring that distributed data is in compliance with\n\nregulations.\n\nA distributed data sovereignty architecture is designed to guarantee that\n\nprotected data is stored in one or more specific physical locations.\n\nAn important design consideration for this architecture is making sure that\n\nthe data replication mechanisms used by the cloud provider can be\n\nconfigured for regulatory compliance. The distributed data sovereignty",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 753,
      "content": "architecture further relies on a data governance management mechanism to\n\ncoordinate the appropriate storage of protected data within the region\n\nnecessary to comply with different local or regional regulations (Figure\n\n14.19).\n\nFigure 14.19\n\nAn organization uses a data governance manager mechanism to ensure that\n\nits cloud-based data is located in the region in which it must reside\n\naccording to regional data protection regulations.\n\nAdditionally, the following mechanisms are also part of this architecture:",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 754,
      "content": "Cloud Storage Device – The cloud storage device mechanism stores the\n\nprotected data in the location that allows the organization to comply with\n\nregional regulations.\n\nAudit Monitor – This mechanism may be required to check whether the\n\nlocal data has been replicated to prohibited locations.\n\nStorage Replication – The storage replication mechanism keeps copies of\n\ndata made for resiliency purposes in storage devices that are geographically\n\nlocated in accordance to data protection regulations.\n\nNote\n\nAn alternative approach is for cloud consumers to identify\n\nlocal cloud providers in every different region for which\n\nregulatory compliance is necessary, thereby establishing a\n\nmulti-cloud architecture (as described in Chapter 13) in\n\nwhich each cloud belongs to a different cloud provider.\n\n14.9 Resource Reservation Architecture\n\nDepending on how IT resources are designed for shared usage and\n\ndepending on their available levels of capacity, concurrent access can lead\n\nto a runtime exception condition called resource constraint. A resource\n\nconstraint is a condition that occurs when two or more cloud consumers",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 755,
      "content": "have been allocated to share an IT resource that does not have the capacity\n\nto accommodate the total processing requirements of the cloud consumers.\n\nAs a result, one or more of the cloud consumers encounter degraded\n\nperformance or may be rejected altogether. The cloud service itself may go\n\ndown, resulting in all cloud consumers being rejected.\n\nOther types of runtime conflicts can occur when an IT resource (especially\n\none not specifically designed to accommodate sharing) is concurrently\n\naccessed by different cloud service consumers. For example, nested and\n\nsibling resource pools introduce the notion of resource borrowing, whereby\n\none pool can temporarily borrow IT resources from other pools. A runtime\n\nconflict can be triggered when the borrowed IT resource is not returned due\n\nto prolonged usage by the cloud service consumer that is borrowing it. This\n\ncan inevitably lead back to the occurrence of resource constraints.\n\nThe resource reservation architecture establishes a system whereby one of\n\nthe following is set aside exclusively for a given cloud consumer (Figures\n\n14.20 to 14.22):\n\nsingle IT resource\n\nportion of an IT resource\n\nmultiple IT resources",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 756,
      "content": "This protects cloud consumers from each other by avoiding the\n\naforementioned resource constraint and resource borrowing conditions.",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 757,
      "content": "Figure 14.20\n\nA physical resource group is created (1), from which a parent resource pool\n\nis created as per the resource pooling architecture (2). Two smaller child\n\npools are created from the parent resource pool, and resource limits are\n\ndefined using the resource management system (3). Cloud consumers are\n\nprovided with access to their own exclusive resource pools (4).",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 758,
      "content": "Figure 14.21",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 759,
      "content": "An increase in requests from Cloud Consumer A results in more IT\n\nresources being allocated to that cloud consumer (5), meaning some IT\n\nresources need to be borrowed from Pool 2. The amount of borrowed IT\n\nresources is confined by the resource limit that was defined in Step 3, to\n\nensure that Cloud Consumer B will not face any resource constraints (6).",
      "content_length": 355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 760,
      "content": "Figure 14.22",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 761,
      "content": "Cloud Consumer B now imposes more requests and usage demands and\n\nmay soon need to utilize all available IT resources in the pool (6). The\n\nresource management system forces Pool 1 to release the IT resources and\n\nmove them back to Pool 2 to become available for Cloud Consumer B (7).\n\nThe creation of an IT resource reservation system can require involving the\n\nresource management system mechanism, which is used to define the usage\n\nthresholds for individual IT resources and resource pools. Reservations lock\n\nthe amount of IT resources that each pool needs to keep, with the balance of\n\nthe pool’s IT resources still available for sharing and borrowing. The remote\n\nadministration system mechanism is also used to enable front-end\n\ncustomization, so that cloud consumers have administration controls for the\n\nmanagement of their reserved IT resource allocations.\n\nThe types of mechanisms that are commonly reserved within this\n\narchitecture are cloud storage devices and virtual servers. Other\n\nmechanisms that may be part of the architecture can include:\n\nAudit Monitor — The audit monitor is used to check whether the resource\n\nreservation system is complying with cloud consumer auditing, privacy, and\n\nother regulatory requirements. For example, it may track the geographical\n\nlocation of reserved IT resources.\n\nCloud Usage Monitor — A cloud usage monitor may oversee the thresholds\n\nthat trigger the allocation of reserved IT resources.",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 762,
      "content": "Hypervisor — The hypervisor mechanism may apply reservations for\n\ndifferent cloud consumers to ensure that they are correctly allocated to their\n\nguaranteed IT resources.\n\nLogical Network Perimeter — This mechanism establishes the boundaries\n\nnecessary to ensure that reserved IT resources are made exclusively\n\navailable to cloud consumers.\n\nResource Replication — This component needs to stay informed about each\n\ncloud consumer’s limits for IT resource consumption, in order to replicate\n\nand provision new IT resource instances expediently.\n\n14.10 Dynamic Failure Detection and Recovery Architecture\n\nCloud-based environments can be comprised of vast quantities of IT\n\nresources that are simultaneously accessed by numerous cloud consumers.\n\nAny of those IT resources can experience failure conditions that require\n\nmore than manual intervention to resolve. Manually administering and\n\nsolving IT resource failures is generally inefficient and impractical.\n\nThe dynamic failure detection and recovery architecture establishes a\n\nresilient watchdog system to monitor and respond to a wide range of pre-\n\ndefined failure scenarios (Figures 14.23 and 14.24). This system notifies\n\nand escalates the failure conditions that it cannot automatically resolve\n\nitself. It relies on a specialized cloud usage monitor called the intelligent",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 763,
      "content": "watchdog monitor to actively track IT resources and take pre-defined\n\nactions in response to pre-defined events.\n\nFigure 14.23\n\nThe intelligent watchdog monitor keeps track of cloud consumer requests\n\n(1) and detects that a cloud service has failed (2).",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 764,
      "content": "Figure 14.24\n\nThe intelligent watchdog monitor notifies the watchdog system (3), which\n\nrestores the cloud service based on pre-defined policies. The cloud service\n\nresumes its runtime operation (4).",
      "content_length": 199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 765,
      "content": "The resilient watchdog system performs the following five core functions:\n\nwatching\n\ndeciding upon an event\n\nacting upon an event\n\nreporting\n\nescalating\n\nSequential recovery policies can be defined for each IT resource to\n\ndetermine the steps that the intelligent watchdog monitor needs to take\n\nwhen a failure condition occurs. For example, a recovery policy can state\n\nthat one recovery attempt needs to be automatically carried out before\n\nissuing a notification (Figure 14.25).",
      "content_length": 481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 766,
      "content": "Figure 14.25\n\nIn the event of a failure, the intelligent watchdog monitor refers to its pre-\n\ndefined policies to recover the cloud service step-by-step, escalating the\n\nprocess when a problem proves to be deeper than expected.\n\nSome of the actions the intelligent watchdog monitor commonly takes to\n\nescalate an issue include:",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 767,
      "content": "running a batch file\n\nsending a console message\n\nsending a text message\n\nsending an email message\n\nsending an SNMP trap\n\nlogging a ticket\n\nThere are varieties of programs and products that can act as intelligent\n\nwatchdog monitors. Most can be integrated with standard ticketing and\n\nevent management systems.\n\nThis architectural model can further incorporate the following mechanisms:\n\nAudit Monitor — This mechanism is used to track whether data recovery is\n\ncarried out in compliance with legal or policy requirements.\n\nFailover System — The failover system mechanism is usually used during\n\nthe initial attempts to recover failed IT resources.\n\nSLA Management System and SLA Monitor — Since the functionality\n\nachieved by applying this architecture is closely associated with SLA",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 768,
      "content": "guarantees, the system commonly relies on the information that is managed\n\nand processed by these mechanisms.\n\n14.11 Rapid Provisioning Architecture\n\nA conventional provisioning process can involve a number of tasks that are\n\ntraditionally completed manually by administrators and technology experts\n\nthat prepare the requested IT resources as per pre-packaged specifications\n\nor custom client requests. In cloud environments, where higher volumes of\n\ncustomers are serviced and where the average customer requests higher\n\nvolumes of IT resources, manual provisioning processes are inadequate and\n\ncan even lead to unreasonable risk due to human error and inefficient\n\nresponse times.\n\nFor example, a cloud consumer that requests the installation, configuration,\n\nand updating of twenty-five Windows servers with several applications\n\nrequires that half of the applications be identical installations, while the\n\nother half be customized. Each operating system deployment can take up to\n\n30 minutes, followed by additional time for security patches and operating\n\nsystem updates that require server rebooting. The applications finally need\n\nto be deployed and configured. Using a manual or semi-automated\n\napproach requires excessive amounts of time, and introduces a probability\n\nof human error that increases with each installation.",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 769,
      "content": "The rapid provisioning architecture establishes a system that automates the\n\nprovisioning of a wide range of IT resources, either individually or as a\n\ncollective. The underlying technology architecture for rapid IT resource\n\nprovisioning can be sophisticated and complex, and relies on a system\n\ncomprised of an automated provisioning program, rapid provisioning\n\nengine, and scripts and templates for on-demand provisioning.\n\nBeyond the components displayed in Figure 14.26, many additional\n\narchitectural artifacts are available to coordinate and automate the different\n\naspects of IT resource provisioning, such as:\n\nServer Templates — Templates of virtual image files that are used to\n\nautomate the instantiation of new virtual servers.",
      "content_length": 741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 770,
      "content": "Figure 14.26\n\nA cloud resource administrator requests a new cloud service through the\n\nself-service portal (1). The self-service portal passes the request to the\n\nautomated service provisioning program installed on the virtual server (2),\n\nwhich passes the necessary tasks to be performed to the rapid provisioning\n\nengine (3). The rapid provisioning engine announces when the new cloud\n\nservice is ready (4). The automated service provisioning program finalizes\n\nand publishes the cloud service on the usage and administration portal for\n\ncloud consumer access (5).\n\nServer Images — These images are similar to virtual server templates, but\n\nare used to provision physical servers.\n\nApplication Packages — Collections of applications and other software that\n\nare packaged for automated deployment.\n\nApplication Packager — The software used to create application packages.\n\nCustom Scripts — Scripts that automate administrative tasks, as part of an\n\nintelligent automation engine.\n\nSequence Manager — A program that organizes sequences of automated\n\nprovisioning tasks.\n\nSequence Logger — A component that logs the execution of automated\n\nprovisioning task sequences.",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 771,
      "content": "Operating System Baseline — A configuration template that is applied after\n\nthe operating system is installed, to quickly prepare it for usage.\n\nApplication Configuration Baseline — A configuration template with the\n\nsettings and environmental parameters that are needed to prepare new\n\napplications for use.\n\nDeployment Data Store — The repository that stores virtual images,\n\ntemplates, scripts, baseline configurations, and other related data.\n\nThe following step-by-step description helps provide some insight into the\n\ninner workings of a rapid provisioning engine, involving a number of the\n\npreviously listed system components:\n\n. A cloud consumer requests a new server through the self-service portal.\n\n. The sequence manager forwards the request to the deployment engine for\n\nthe preparation of an operating system.\n\n. The deployment engine uses the virtual server templates for provisioning if\n\nthe request is for a virtual server. Otherwise, the deployment engine sends\n\nthe request to provision a physical server.\n\n. The pre-defined image for the requested type of operating system is used\n\nfor the provisioning of the operating system, if available. Otherwise, the\n\nregular deployment process is executed to install the operating system.",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 772,
      "content": ". The deployment engine informs the sequence manager when the operating\n\nsystem is ready.\n\n. The sequence manager updates and sends the logs to the sequence logger\n\nfor storage.\n\n. The sequence manager requests that the deployment engine apply the\n\noperating system baseline to the provisioned operating system.\n\n. The deployment engine applies the requested operating system baseline.\n\n. The deployment engine informs the sequence manager that the operating\n\nsystem baseline has been applied.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.\n\nThe sequence manager requests that the deployment engine install the\n\napplications.\n\n. The deployment engine deploys the applications on the provisioned server.\n\n. The deployment engine informs the sequence manager that the applications\n\nhave been installed.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 773,
      "content": ". The sequence manager requests that the deployment engine apply the\n\napplication’s configuration baseline.\n\n. The deployment engine applies the configuration baseline.\n\n. The deployment engine informs the sequence manager that the\n\nconfiguration baseline has been applied.\n\n. The sequence manager updates and sends the logs of completed steps to the\n\nsequence logger for storage.\n\nThe cloud storage device mechanism is used to provide storage for\n\napplication baseline information, templates, and scripts, while the\n\nhypervisor rapidly creates, deploys, and hosts the virtual servers that are\n\neither provisioned themselves, or host other provisioned IT resources. The\n\nresource replication mechanism is usually used to generate replicated\n\ninstances of IT resources in response to rapid provisioning requirements.\n\n14.12 Storage Workload Management Architecture\n\nOver-utilized cloud storage devices increase the workload on the storage\n\ncontroller and can cause a range of performance challenges. Conversely,\n\ncloud storage devices that are under-utilized are wasteful due to lost\n\nprocessing and storage capacity potential (Figure 14.27).",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 774,
      "content": "Figure 14.27\n\nAn unbalanced cloud storage architecture has six storage LUNs in Storage\n\n1 for cloud consumers to use, while Storage 2 is hosting one LUN and",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 775,
      "content": "Storage 3 is hosting two. The majority of the workload ends up with Storage\n\n1, since it is hosting the most LUNs.\n\nLUN Migration\n\nLUN migration is a specialized storage program that is\n\nused to move LUNs from one storage device to another\n\nwithout interruption, while remaining transparent to\n\ncloud consumers.\n\nThe storage workload management architecture enables LUNs to be evenly\n\ndistributed across available cloud storage devices, while a storage capacity\n\nsystem is established to ensure that runtime workloads are evenly\n\ndistributed across the LUNs (Figure 14.28).",
      "content_length": 573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 776,
      "content": "Figure 14.28\n\nLUNs are dynamically distributed across cloud storage devices, resulting in\n\nmore even distribution of associated types of workloads.\n\nCombining cloud storage devices into a group allows LUN data to be\n\ndistributed between available storage hosts equally. A storage management",
      "content_length": 290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 777,
      "content": "system is configured and an automated scaling listener is positioned to\n\nmonitor and equalize runtime workloads among the grouped cloud storage\n\ndevices, as illustrated in Figures 14.29 to 14.31.\n\nFigure 14.29",
      "content_length": 209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 778,
      "content": "The storage capacity system and storage capacity monitor are configured to\n\nsurvey three storage devices in realtime, whose workload and capacity\n\nthresholds are pre-defined (1). The storage capacity monitor determines\n\nthat the workload on Storage 1 is reaching its threshold (2).\n\nFigure 14.30",
      "content_length": 295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 779,
      "content": "The storage capacity monitor informs the storage capacity system that\n\nStorage 1 is over-utilized (3). The storage capacity system identifies the\n\nLUNs to be moved from Storage 1 (4).\n\nFigure 14.31\n\nThe storage capacity system calls for LUN migration to move some of the\n\nLUNs from Storage 1 to the other two storage devices (5). LUN migration",
      "content_length": 343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 780,
      "content": "transitions LUNs to Storage 2 and 3 to balance the workload (6).\n\nThe storage capacity system can keep the hosting storage device in power-\n\nsaving mode for the periods when the LUNs are being accessed less\n\nfrequently or only at specific times.\n\nSome other mechanisms that can be included in the storage workload\n\nmanagement architecture to accompany the cloud storage device are as\n\nfollows:\n\nAudit Monitor — This monitoring mechanism is used to check for\n\ncompliance with regulatory, privacy, and security requirements, since the\n\nsystem established by this architecture can physically relocate data.\n\nAutomated Scaling Listener — The automated scaling listener is used to\n\nwatch and respond to workload fluctuations.\n\nCloud Usage Monitor — In addition to the capacity workload monitor,\n\nspecialized cloud usage monitors are used to track LUN movements and\n\ncollect workload distribution statistics.\n\nLoad Balancer — This mechanism can be added to horizontally balance\n\nworkloads across available cloud storage devices.\n\nLogical Network Perimeter — Logical network perimeters provide levels of\n\nisolation so that cloud consumer data that undergoes relocation remains\n\ninaccessible to unauthorized parties.",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 781,
      "content": "14.13 Virtual Private Cloud Architecture\n\nThe virtual private cloud architecture establishes a private cloud with\n\nunderlying infrastructure that belongs to a public cloud provider but that is\n\nexclusively dedicated to one specific cloud consumer for whom the private\n\ncloud is delivered. This can be useful for an organization who wants to have\n\na private cloud but does not have the necessary infrastructure to support it\n\non-premise.\n\nTo the cloud consumer with exclusive access, this is a private cloud.\n\nHowever, from the cloud provider’s point of view, it is part of its\n\ninfrastructure, which is why it is referred to as a “virtual” private cloud. The\n\nunderlying physical resources, which are typically virtualized for more\n\nefficient utilization, are not shared with other cloud consumers. Instead,\n\nthey are solely dedicated to the “owner” (the cloud consumer) of the virtual\n\nprivate cloud.\n\nThe physical resources used to build this architecture need special isolation\n\nfrom the rest of the cloud provider’s infrastructure, including a separate\n\nphysical network to which the cloud consumer connects via a secure virtual\n\nprivate network (VPN), as shown in Figure 14.32. Sometimes this VPN can\n\nbe replaced by a dedicated physical link from the cloud provider to the\n\ncloud consumer (although that could result in a much more expensive\n\narchitecture).",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 782,
      "content": "Figure 14.32",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 783,
      "content": "A virtual private cloud architecture utilizes physical resources from a public\n\ncloud provider dedicated for exclusive use by a specific cloud consumer,\n\naccessible via a secure connection, such as can be provided by a VPN.\n\nThe mechanisms involved in this architecture are the same mechanisms\n\nrequired to build any other private cloud, with the exception of the VPN,\n\nwhich is normally not required when a private cloud is deployed on\n\ninfrastructure that resides within the physical boundary of an organization.\n\nThese mechanisms include:\n\nHypervisor – The hypervisor mechanism provides an efficient way for\n\nutilizing physical servers by allowing for the deployment of virtual servers\n\non physical servers.\n\nVirtual Server – The virtual server mechanism provides the most common\n\ntype of resource used in cloud environments to host workloads of all types.\n\nCloud Storage Device – The cloud storage device mechanism provides\n\nstorage capabilities within the virtual private cloud.\n\nVirtual Switch – The virtual switch mechanism provides connectivity\n\nbetween virtual servers and the rest of the resources in the virtual private\n\ncloud.\n\n14.14 Case Study Example",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 784,
      "content": "Innovartus is leasing two cloud-based environments\n\nfrom two different cloud providers, and intends to take\n\nadvantage of this opportunity to establish a pilot cloud-\n\nbalancing architecture for its Role Player cloud service.\n\nAfter assessing its requirements against the respective\n\nclouds, Innovartus’ cloud architects produce a design\n\nspecification that is based on each cloud having multiple\n\nimplementations of the cloud service. This architecture\n\nincorporates separate automated scaling listener and\n\nfailover system implementations, together with a central\n\nload balancer mechanism (Figure 14.33).",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 786,
      "content": "Figure 14.33\n\nA load-balancing service agent routes cloud service\n\nconsumer requests according to a pre-defined algorithm\n\n(1). Requests are received by the local or external\n\nautomated scaling listener (2A, 2B), which forward each\n\nrequest to a cloud service implementation (3). Failover\n\nsystem monitors are used to detect and respond to cloud\n\nservice failure (4).\n\nThe load balancer distributes cloud service consumer\n\nrequests across clouds using a workload distribution\n\nalgorithm, while each cloud’s automated scaling listener\n\nroutes requests to local cloud service implementations.\n\nThe failover systems can failover to the redundant cloud\n\nservice implementations that are both within and across\n\nclouds. Inter-cloud failover is carried out primarily when\n\nlocal cloud service implementations are nearing their\n\nprocessing thresholds, or if a cloud is encountering a\n\nsevere platform failure.",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 787,
      "content": "Chapter 15\n\nSpecialized Cloud Architectures\n\n15.1 Direct I/O Access Architecture\n\n15.2 Direct LUN Access Architecture\n\n15.3 Dynamic Data Normalization Architecture\n\n15.4 Elastic Network Capacity Architecture\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\n15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\n15.7 Load Balanced Virtual Switches Architecture\n\n15.8 Multipath Resource Access Architecture\n\n15.9 Persistent Virtual Network Configuration Architecture\n\n15.10 Redundant Physical Connection for Virtual Servers Architecture\n\n15.11 Storage Maintenance Window Architecture\n\n15.12 Edge Computing Architecture",
      "content_length": 633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 788,
      "content": "15.13 Fog Computing Architecture\n\nThe architectural models that are covered in this chapter span a broad range\n\nof functional areas and topics to offer creative combinations of mechanisms\n\nand specialized components.\n\nThe following architectures are covered:\n\nDirect I/O Access\n\nDirect LUN Access\n\nDynamic Data Normalization\n\nElastic Network Capacity\n\nCross-Storage Device Vertical Tiering\n\nIntra-Storage Device Vertical Data Tiering\n\nLoad Balanced Virtual Switches\n\nMultipath Resource Access\n\nPersistent Virtual Network Configuration\n\nRedundant Physical Connection for Virtual Servers",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 789,
      "content": "Storage Maintenance Window\n\nEdge Computing\n\nFog Computing\n\nWhere applicable, the involvement of related cloud mechanisms is\n\ndescribed.\n\n15.1 Direct I/O Access Architecture\n\nAccess to the physical I/O cards that are installed on a physical server is\n\nusually provided to hosted virtual servers via a hypervisor-based layer of\n\nprocessing called I/O virtualization. However, virtual servers sometimes\n\nneed to connect to and use I/O cards without any hypervisor interaction or\n\nemulation.\n\nWith the direct I/O access architecture, virtual servers are allowed to\n\ncircumvent the hypervisor and directly access the physical server’s I/O card\n\nas an alternative to emulating a connection via the hypervisor (Figures 15.1\n\nto 15.3).",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 791,
      "content": "Figure 15.1\n\nCloud service consumers access a virtual server, which accesses a database\n\non a SAN storage LUN (1). Connectivity from the virtual server to the\n\ndatabase occurs via a virtual switch.",
      "content_length": 197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 793,
      "content": "Figure 15.2\n\nThere is an increase in the amount of cloud service consumer requests (2),\n\ncausing the bandwidth and performance of the virtual switch to become\n\ninadequate (3).",
      "content_length": 175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 794,
      "content": "Figure 15.3",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 795,
      "content": "The virtual server bypasses the hypervisor to connect to the database server\n\nvia a direct physical link to the physical server (4). The increased workload\n\ncan now be properly handled.\n\nTo achieve this solution and access the physical I/O card without hypervisor\n\ninteraction, the host CPU needs to support this type of access with the\n\nappropriate drivers installed on the virtual server. The virtual server can\n\nthen recognize the I/O card as a hardware device after the drivers are\n\ninstalled.\n\nOther mechanisms that can be involved in this architecture in addition to\n\nthe virtual server and hypervisor include:\n\nCloud Usage Monitor — The cloud service usage data that is collected by\n\nruntime monitors can include and separately classify direct I/O access.\n\nLogical Network Perimeter — The logical network perimeter ensures that\n\nthe allocated physical I/O card does not allow cloud consumers to access\n\nother cloud consumers’ IT resources.\n\nPay-Per-Use Monitor — This monitor collects usage cost information for\n\nthe allocated physical I/O card.\n\nResource Replication — Replication technology is used to replace virtual\n\nI/O cards with physical I/O cards.\n\n15.2 Direct LUN Access Architecture",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 796,
      "content": "Storage LUNs are typically mapped via a host bus adapter (HBA) on the\n\nhypervisor, with the storage space emulated as file-based storage to virtual\n\nservers (Figure 15.4). However, virtual servers sometimes need direct\n\naccess to RAW block-based storage. For example, access via an emulated\n\nadapter is insufficient when a cluster is implemented and a LUN is used as\n\nthe shared cluster storage device between two virtual servers.",
      "content_length": 430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 797,
      "content": "Figure 15.4",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 798,
      "content": "The cloud storage device is installed and configured (1). The LUN mapping\n\nis defined so that each hypervisor has access to its own LUN and can also\n\nsee all of the mapped LUNs (2). The hypervisor shows the mapped LUNs to\n\nthe virtual servers as normal file-based storage to be used as such (3).\n\nThe direct LUN access architecture provides virtual servers with LUN\n\naccess via a physical HBA card, which is effective because virtual servers\n\nin the same cluster can use the LUN as a shared volume for clustered\n\ndatabases. After implementing this solution, the virtual servers’ physical\n\nconnectivity to the LUN and cloud storage device is enabled by the physical\n\nhosts.\n\nThe LUNs are created and configured on the cloud storage device for LUN\n\npresentation to the hypervisors. The cloud storage device needs to be\n\nconfigured using raw device mapping to make the LUNs visible to the\n\nvirtual servers as a block-based RAW SAN LUN, which is unformatted, un-\n\npartitioned storage. The LUN needs to be represented with a unique LUN\n\nID to be used by all of the virtual servers as shared storage. Figures 15.5\n\nand 15.6 illustrate how virtual servers are given direct access to block-based\n\nstorage LUNs.",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 799,
      "content": "Figure 15.5",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 800,
      "content": "The cloud storage device is installed and configured (1). The required\n\nLUNs are created and presented to the hypervisors (2), which map the\n\npresented LUNs directly to the virtual servers (3). The virtual servers can\n\nsee the LUNs as RAW block-based storage and can access them directly (4).",
      "content_length": 292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 801,
      "content": "Figure 15.6\n\nThe virtual servers’ storage commands are received by the hypervisors (5),\n\nwhich process and forward the requests to the storage processor (6).",
      "content_length": 157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 802,
      "content": "Besides the virtual server, hypervisor, and cloud storage device, the\n\nfollowing mechanisms can be incorporated into this architecture:\n\nCloud Usage Monitor — This monitor tracks and collects storage usage\n\ninformation that pertains to the direct usage of LUNs.\n\nPay-Per-Use Monitor — The pay-per-use monitor collects and separately\n\nclassifies usage cost information for direct LUN access.\n\nResource Replication — This mechanism relates to how virtual servers\n\ndirectly access block-based storage in replacement of file-based storage.\n\n15.3 Dynamic Data Normalization Architecture\n\nRedundant data can cause a range of issues in cloud-based environments,\n\nsuch as:\n\nincreased time required to store and catalog files\n\nincreased required storage and backup space\n\nincreased costs due to increased data volume\n\nincreased time required for replication to secondary storage\n\nincreased time required to backup data",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 803,
      "content": "For example, if a cloud consumer copies 100 MB of files onto a cloud\n\nstorage device and the data is redundantly copied ten times, the\n\nconsequences can be considerable:\n\nThe cloud consumer will be charged for using 10 x 100 MB of storage\n\nspace, even though only 100 MB of unique data was actually stored.\n\nThe cloud provider needs to provide an unnecessary 900 MB of space in the\n\nonline cloud storage device and any backup storage systems.\n\nSignificantly more time is required to store and catalog data.\n\nData replication duration and performance are unnecessarily taxed\n\nwhenever the cloud provider performs a site recovery, since 1,000 MB need\n\nto be replicated instead of 100 MB.\n\nThese impacts can be significantly amplified in multitenant public clouds.\n\nThe dynamic data normalization architecture establishes a de-duplication\n\nsystem, which prevents cloud consumers from inadvertently saving\n\nredundant copies of data by detecting and eliminating redundant data on\n\ncloud storage devices. This system can be applied to both block and file-\n\nbased storage devices, although it is most effective on the former. This de-\n\nduplication system checks each received block to determine whether it is\n\nredundant with a block that has already been received. Redundant blocks",
      "content_length": 1274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 804,
      "content": "are replaced with pointers to the equivalent blocks that are already in\n\nstorage (Figure 15.7).",
      "content_length": 95,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 805,
      "content": "Figure 15.7\n\nData sets containing redundant data are unnecessarily bloating storage\n\n(left). The data de-duplication system normalizes the data, so that only\n\nunique data is stored (right).\n\nThe de-duplication system examines received data prior to passing it to\n\nstorage controllers. As part of the examination process, a hash code is\n\nassigned to every piece of data that has been processed and stored. An index\n\nof hashes and pieces is also maintained. As a result, the generated hash of a\n\nnewly received block of data is compared with the hashes in storage to\n\ndetermine whether it is a new or duplicate data block. New blocks are\n\nsaved, while duplicate data is eliminated and a pointer to the original data\n\nblock is created and saved instead.\n\nThis architectural model can be used for both disk storage and backup tape\n\ndrives. One cloud provider can decide to prevent redundant data only on\n\nbackup cloud storage devices, while another can more aggressively\n\nimplement the data de-duplication system on all of its cloud storage\n\ndevices. There are different methods and algorithms for comparing blocks\n\nof data to confirm their duplicity with other blocks.\n\n15.4 Elastic Network Capacity Architecture\n\nEven if IT resources are scaled on-demand by a cloud platform,\n\nperformance and scalability can still be inhibited when remote access to the",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 806,
      "content": "IT resources is impacted by network bandwidth limitations (Figure 15.8).\n\nFigure 15.8\n\nA lack of available bandwidth causes performance issues for cloud\n\nconsumer requests.\n\nThe elastic network capacity architecture establishes a system in which\n\nadditional bandwidth is allocated dynamically to the network to avoid\n\nruntime bottlenecks. This system ensures that each cloud consumer is using\n\na different set of network ports to isolate individual cloud consumer traffic\n\nflows.",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 807,
      "content": "The automated scaling listener and intelligent automation engine scripts are\n\nused to detect when traffic reaches the bandwidth threshold, and to\n\ndynamically allocate additional bandwidth and/or network ports when\n\nrequired.\n\nThe cloud architecture can be equipped with a network resource pool\n\ncontaining network ports that are made available for shared usage. The\n\nautomated scaling listener monitors workload and network traffic, and\n\nsignals the intelligent automation engine to modify the number of allocated\n\nnetwork ports and/or bandwidth in response to usage fluctuations.\n\nNote that when this architectural model is implemented at the virtual switch\n\nlevel, the intelligent automation engine may need to run a separate script\n\nthat adds physical uplinks to the virtual switch specifically. Alternatively,\n\nthe direct I/O access architecture can also be incorporated to increase\n\nnetwork bandwidth that is allocated to the virtual server.\n\nIn addition to the automated scaling listener, the following mechanisms can\n\nbe part of this architecture:\n\nCloud Usage Monitor — These monitors are responsible for tracking elastic\n\nnetwork capacity before, during, and after scaling.\n\nHypervisor — The hypervisor provides virtual servers with access to the\n\nphysical network, via virtual switches and physical uplinks.",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 808,
      "content": "Logical Network Perimeter — This mechanism establishes the boundaries\n\nthat are needed to provide individual cloud consumers with their allocated\n\nnetwork capacity.\n\nPay-Per-Use Monitor — This monitor keeps track of any billing-related\n\ndata that pertains to dynamic network bandwidth consumption.\n\nResource Replication — Resource replication is used to add network ports\n\nto physical and virtual servers, in response to workload demands.\n\nVirtual Server — Virtual servers host the IT resources and cloud services to\n\nwhich network resources are allocated and are themselves affected by the\n\nscaling of network capacity.\n\n15.5 Cross-Storage Device Vertical Tiering Architecture\n\nCloud storage devices are sometimes unable to accommodate the\n\nperformance requirements of cloud consumers, and have more data\n\nprocessing power or bandwidth added to increase IOPS. These conventional\n\nmethods of vertical scaling are usually inefficient and time-consuming to\n\nimplement, and can become wasteful when the increased capacity is no\n\nlonger required.\n\nThe scenario in Figures 15.9 and 15.10 depicts an approach in which a\n\nnumber of requests for access to a LUN has increased, requiring its manual\n\ntransfer to a high-performance cloud storage device.",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 809,
      "content": "Figure 15.9",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 810,
      "content": "A cloud provider installs and configures a cloud storage device (1) and\n\ncreates LUNs that are made available to the cloud service consumers for\n\nusage (2). The cloud service consumers initiate data access requests to the\n\ncloud storage device (3), which forwards the requests to one of the LUNs\n\n(4).",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 811,
      "content": "Figure 15.10\n\nThe number of requests increases, resulting in high storage bandwidth and\n\nperformance demands (5). Some of the requests are rejected, or time out",
      "content_length": 160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 812,
      "content": "due to performance capacity limitations within the cloud storage device (6).\n\nThe cross-storage device vertical tiering architecture establishes a system\n\nthat survives bandwidth and data processing power constraints by vertically\n\nscaling between storage devices that have different capacities. LUNs can\n\nautomatically scale up and down across multiple devices in this system so\n\nthat requests can use the appropriate storage device level to perform cloud\n\nconsumer tasks.\n\nNew cloud storage devices with increased capacity can also be made\n\navailable, even if the automated tiering technology can move data to cloud\n\nstorage devices with the same storage processing capacity. For example,\n\nsolid-state drives (SSDs) can be suitable devices for data processing power\n\nupgrades.\n\nThe automated scaling listener monitors the requests that are sent to specific\n\nLUNs, and signals the storage management program to move the LUN to a\n\nhigher capacity device once it identifies a predefined threshold has been\n\nreached. Service interruption is prevented because there is never a\n\ndisconnection during the transfer. The original device remains up and\n\nrunning, while the LUN data scales up to another device. Cloud consumer\n\nrequests are automatically redirected to the new cloud storage device as\n\nsoon as the scaling is completed (Figures 15.11 to 15.13).",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 813,
      "content": "Figure 15.11",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 814,
      "content": "The lower capacity primary cloud storage device is responding to cloud\n\nservice consumer storage requests (1). A secondary cloud storage device\n\nwith higher capacity and performance is installed (2). The LUN migration\n\n(3) is configured via the storage management program that is configured to\n\ncategorize the storage based on device performance (4). Thresholds are\n\ndefined in the automated scaling listener that is monitoring the requests (5).\n\nCloud service consumer requests are received by the storage service\n\ngateway and sent to the primary cloud storage device (6).",
      "content_length": 573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 815,
      "content": "Figure 15.12",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 816,
      "content": "The number of cloud service consumer requests reaches the predefined\n\nthreshold (7), and the automated scaling listener notifies the storage\n\nmanagement program that scaling is required (8). The storage management\n\nprogram calls LUN migration to move the cloud consumer’s LUN to the\n\nsecondary, higher capacity storage device (9) and the LUN migration\n\nperforms this move (10).",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 818,
      "content": "Figure 15.13\n\nThe storage service gateway forwards the cloud service consumer requests\n\nfrom the LUN to the new cloud storage device (11). The original LUN is\n\ndeleted from the lower capacity device via the storage management\n\nprogram and LUN migration (12). The automated scaling listener monitors\n\ncloud service consumer requests to ensure that the request volume\n\ncontinues to require access to the higher capacity secondary storage for the\n\nmigrated LUN (13).\n\nIn addition to the automated scaling listener and cloud storage device, the\n\nmechanisms that can be incorporated in this technology architecture\n\ninclude:\n\nAudit Monitor — The auditing performed by this monitor checks whether\n\nthe relocation of cloud consumer data does not conflict with any legal or\n\ndata privacy regulations or policies.\n\nCloud Usage Monitor — This infrastructure mechanism represents various\n\nruntime monitoring requirements for tracking and recording data transfer\n\nand usage, at both source and destination storage locations.\n\nPay-Per-Use Monitor — Within the context of this architecture, the pay-\n\nper-use monitor collects storage usage information on both source and\n\ndestination locations, as well as IT resource usage information for carrying\n\nout cross-storage tiering functionality.",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 819,
      "content": "15.6 Intra-Storage Device Vertical Data Tiering Architecture\n\nSome cloud consumers may have distinct data storage requirements that\n\nrestrict the data’s physical location to a single cloud storage device.\n\nDistribution across other cloud storage devices may be disallowed due to\n\nsecurity, privacy, or various legal reasons. This type of limitation can\n\nimpose severe scalability limitations upon the device’s storage and\n\nperformance capacity. These limitations can further cascade to any cloud\n\nservices or applications that are dependent upon the use of the cloud storage\n\ndevice.\n\nThe intra-storage device vertical data tiering architecture establishes a\n\nsystem to support vertical scaling within a single cloud storage device. This\n\nintra-device scaling system optimizes the availability of different disk types\n\nwith different capacities (Figure 15.14).",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 820,
      "content": "Figure 15.14\n\nThe cloud intra-storage device system vertically scales through disk types\n\ngraded into different tiers (1). Each LUN is moved to a tier that",
      "content_length": 155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 821,
      "content": "corresponds to its processing and storage requirements (2).\n\nThis cloud storage architecture requires the use of a complex storage device\n\nthat supports different types of hard disks, especially high-performance\n\ndisks like SATAs, SASs, and SSDs. The disk types are organized into\n\ngraded tiers so that LUN migration can vertically scale the device based on\n\nthe allocation of disk types, which align with the processing and capacity\n\nrequirements.\n\nData load conditions and definitions are set after disk categorization so that\n\nthe LUNs can move to either a higher or lower grade, depending on which\n\npredefined conditions are met. These thresholds and conditions are used by\n\nthe automated scaling listener when monitoring runtime data processing\n\ntraffic (Figures 15.15 to 15.17).",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 822,
      "content": "Figure 15.15\n\nDifferent types of hard disks are installed in the enclosures of a cloud\n\nstorage device (1). Similar disk types are grouped into tiers to create\n\ndifferent grades of disk groups based on I/O performance (2).\n\nFigure 15.16",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 823,
      "content": "Two LUNs have been created on Disk Group 1 (3). The automated scaling\n\nlistener monitors the requests in relation to pre-defined thresholds (4). The\n\npay-per-use monitor tracks the actual amount of disk usage, based on free\n\nspace and disk group performance (5). The automated scaling listener\n\ndetermines that the number of requests is reaching a threshold, and informs\n\nthe storage management program that the LUN needs to be moved to a\n\nhigher performance disk group (6). The storage management program\n\nsignals the LUN migration program to perform the required move (7). The\n\nLUN migration program works with the storage controller to move the LUN\n\nto the higher capacity Disk Group 2 (8).",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 824,
      "content": "Figure 15.17\n\nThe usage price of the migrated LUN in Disk Group 2 is now higher than\n\nbefore, because a higher performance disk group is being used (9).\n\n15.7 Load Balanced Virtual Switches Architecture",
      "content_length": 202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 825,
      "content": "Virtual servers are connected to the outside world via virtual switches,\n\nwhich send and receive traffic with the same uplink. Bandwidth bottlenecks\n\nform when the network traffic on the uplink’s port increases to a point that\n\nit causes transmission delays, performance issues, packet loss, and lag time\n\n(Figures 15.18 and 15.19).",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 827,
      "content": "Figure 15.18\n\nA virtual switch is interconnecting virtual servers (1). A physical network\n\nadapter has been attached to the virtual switch to be used as an uplink to\n\nthe physical (external) network, connecting the virtual servers to cloud\n\nconsumers (2). Cloud service consumers send requests via the physical\n\nuplink (3).",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 829,
      "content": "Figure 15.19\n\nThe amount of traffic passing through the physical uplink grows in parallel\n\nwith the increasing number of requests. The number of packets that need to\n\nbe processed and forwarded by the physical network adapter also increases\n\n(4). The physical adapter cannot handle the workload, now that the network\n\ntraffic has exceeded its capacity (5). The network forms a bottleneck that\n\nresults in performance degradation and the loss of delay-sensitive data\n\npackets (6).\n\nThe load balanced virtual switches architecture establishes a load balancing\n\nsystem where multiple uplinks are provided to balance network traffic\n\nworkloads across multiple uplinks or redundant paths, which can help avoid\n\nslow transfers and data loss (Figure 15.20). Link aggregation can be\n\nexecuted to balance the traffic, which allows the workload to be distributed\n\nacross multiple uplinks at the same time so that none of the network cards\n\nare overloaded.",
      "content_length": 945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 831,
      "content": "Figure 15.20\n\nAdditional physical uplinks are added to distribute and balance network\n\ntraffic.\n\nThe virtual switch needs to be configured to support multiple physical\n\nuplinks, which are usually configured as an NIC team that has defined\n\ntraffic-shaping policies.\n\nThe following mechanisms can be incorporated into this architecture:\n\nCloud Usage Monitor — Cloud usage monitors are used to monitor\n\nnetwork traffic and bandwidth usage.\n\nHypervisor — This mechanism hosts and provides the virtual servers with\n\naccess to both the virtual switches and external network.\n\nLoad Balancer — The load balancer distributes the network workload\n\nacross the different uplinks.\n\nLogical Network Perimeter — The logical network perimeter creates\n\nboundaries that protect and limit the bandwidth usage for each cloud\n\nconsumer.\n\nResource Replication — This mechanism is used to generate additional\n\nuplinks to the virtual switch.",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 832,
      "content": "Virtual Server — Virtual servers host the IT resources that benefit from the\n\nadditional uplinks and bandwidth via virtual switches.\n\n15.8 Multipath Resource Access Architecture\n\nCertain IT resources can only be accessed using an assigned path (or\n\nhyperlink) that leads to their exact location. This path can be lost or\n\nincorrectly defined by the cloud consumer or changed by the cloud\n\nprovider. An IT resource whose hyperlink is no longer in the possession of\n\nthe cloud consumer becomes inaccessible and unavailable (Figure 15.21).\n\nException conditions that result from IT resource unavailability can\n\ncompromise the stability of larger cloud solutions that depend on the IT\n\nresource.",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 833,
      "content": "Figure 15.21",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 834,
      "content": "Physical Server A is connected to LUN A via a single fiber channel, and\n\nuses the LUN to store different types of data. The fiber channel connection\n\nbecomes unavailable due to a HBA card failure and invalidates the path\n\nused by Physical Server A, which has now lost access to LUN A and all of\n\nits stored data.\n\nThe multipath resource access architecture establishes a multipathing\n\nsystem with alternative paths to IT resources, so that cloud consumers have\n\nthe means to programmatically or manually overcome path failures (Figure\n\n15.22).",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 835,
      "content": "Figure 15.22\n\nA multipathing system is providing alternative paths to a cloud storage\n\ndevice.\n\nThis technology architecture requires the use of a multipathing system and\n\nthe creation of alternative physical or virtual hyperlinks that are assigned to\n\nspecific IT resources. The multipathing system resides on the server or\n\nhypervisor, and ensures that each IT resource can be seen via each\n\nalternative path identically (Figure 15.23).",
      "content_length": 438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 836,
      "content": "Figure 15.23\n\nPhysical Server A is connected to the LUN A cloud storage device via two\n\ndifferent paths (1). LUN A is seen as different LUNs from each of the two\n\npaths (2). The multipathing system is configured (3). LUN A is seen as one\n\nidentical LUN from both paths (4), and Physical Server A has access to\n\nLUN A from two different paths (5). A link failure occurs and one of the\n\npaths becomes unavailable (6). Physical Server A can still use LUN A\n\nbecause the other link remains active (7).",
      "content_length": 497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 837,
      "content": "This architecture can involve the following mechanisms:\n\nCloud Storage Device — The cloud storage device is a common IT resource\n\nthat requires the creation of alternative paths in order to remain accessible\n\nto solutions that rely on data access.\n\nHypervisor — Alternative paths to a hypervisor are required in order to\n\nhave redundant links to the hosted virtual servers.\n\nLogical Network Perimeter — This mechanism guarantees the maintenance\n\nof cloud consumer privacy, even when multiple paths to the same IT\n\nresource are created.\n\nResource Replication — The resource replication mechanism is required\n\nwhen a new instance of an IT resource needs to be created to generate the\n\nalternative path.\n\nVirtual Server — These servers host the IT resources that have multipath\n\naccess via different links or virtual switches. Hypervisors can provide\n\nmultipath access to the virtual servers.\n\n15.9 Persistent Virtual Network Configuration Architecture\n\nNetwork configurations and port assignments for virtual servers are\n\ngenerated during the creation of the virtual switch on the host physical\n\nserver and the hypervisor hosting the virtual server. These configurations",
      "content_length": 1168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 838,
      "content": "and assignments reside in the virtual server’s immediate hosting\n\nenvironment, meaning a virtual server that is moved or migrated to another\n\nhost will lose network connectivity because destination hosting\n\nenvironments do not have the required port assignments and network\n\nconfiguration information (Figure 15.24).",
      "content_length": 316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 839,
      "content": "Figure 15.24",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 840,
      "content": "Part A shows Virtual Server A connected to the network through Virtual\n\nSwitch A, which was created on Physical Server A. In Part B, Virtual Server\n\nA is connected to Virtual Switch B after being moved to Physical Server B.\n\nThe virtual server cannot connect to the network because its configuration\n\nsettings are missing.\n\nIn the persistent virtual network configuration architecture, network\n\nconfiguration information is stored in a centralized location and replicated\n\nto physical server hosts. This allows the destination host to access the\n\nconfiguration information when a virtual server is moved from one host to\n\nanother.\n\nThe system established with this architecture includes a centralized virtual\n\nswitch, VIM, and configuration replication technology. The centralized\n\nvirtual switch is shared by physical servers and configured via the VIM,\n\nwhich initiates replication of the configuration settings to the physical\n\nservers (Figure 15.25).",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 841,
      "content": "Figure 15.25\n\nA virtual switch’s configuration settings are maintained by the VIM, which\n\nensures that these settings are replicated to other physical servers. The\n\ncentralized virtual switch is published, and each host physical server is\n\nassigned some of its ports. Virtual Server A is moved to Physical Server B",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 842,
      "content": "when Physical Server A fails. The virtual server’s network settings are\n\nretrievable, since they are stored on a centralized virtual switch that is\n\nshared by both physical servers. Virtual Server A maintains network\n\nconnectivity on its new host, Physical Server B.\n\nIn addition to the virtual server mechanism for which this architecture\n\nprovides a migration system, the following mechanisms can be included:\n\nHypervisor — The hypervisor hosts the virtual servers that require the\n\nconfiguration settings to be replicated across the physical hosts.\n\nLogical Network Perimeter — The logical network perimeter helps ensure\n\nthat access to the virtual server and its IT resources is isolated to the rightful\n\ncloud consumer, before and after a virtual server is migrated.\n\nResource Replication — The resource replication mechanism is used to\n\nreplicate the virtual switch configurations and network capacity allocations\n\nacross the hyper-visors, via the centralized virtual switch.\n\n15.10 Redundant Physical Connection for Virtual Servers\n\nArchitecture\n\nA virtual server is connected to an external network via a virtual switch\n\nuplink port, meaning the virtual server will become isolated and\n\ndisconnected from the external network if the uplink fails (Figure 15.26).",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 843,
      "content": "Figure 15.26\n\nA physical network adapter installed on the host physical server is\n\nconnected to the physical switch on the network (1). A virtual switch is\n\ncreated for use by two virtual servers. The physical network adapter is",
      "content_length": 228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 844,
      "content": "attached to the virtual switch to act as an uplink, since it requires access to\n\nthe physical (external) network (2). The virtual servers communicate with\n\nthe external network via the attached physical uplink network card (3). A\n\nconnection failure occurs, either because of a physical link connectivity\n\nissue between the physical adapter and the physical switch (4.1), or\n\nbecause of a physical network card failure (4.2). The virtual servers lose\n\naccess to the physical external network and are no longer accessible to\n\ntheir cloud consumers (5).\n\nThe redundant physical connection for virtual servers architecture\n\nestablishes one or more redundant uplink connections and positions them in\n\nstandby mode. This architecture ensures that a redundant uplink connection\n\nis available to connect the active uplink, whenever the primary uplink\n\nconnection becomes unavailable (Figure 15.27).",
      "content_length": 891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 845,
      "content": "Figure 15.27\n\nRedundant uplinks are installed on a physical server that is hosting several\n\nvirtual servers. When an uplink fails, another uplink takes over to maintain\n\nthe virtual servers’ active network connections.\n\nIn a process that is transparent to both virtual servers and their users, a\n\nstandby uplink automatically becomes the active uplink as soon as the main",
      "content_length": 371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 846,
      "content": "uplink fails, and the virtual servers use the newly active uplink to sends\n\npackets externally.\n\nThe second NIC does not forward any traffic while the primary uplink is\n\nalive, even though it receives the virtual server’s packets. However, the\n\nsecondary uplink will start forwarding packets immediately if the primary\n\nuplink were to fail (Figures 15.28 to 15.30). The failed uplink becomes the\n\nprimary uplink again after it returns to operation, while the second NIC\n\nreturns to standby mode.",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 847,
      "content": "Figure 15.28",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 848,
      "content": "A new network adapter is added to support a redundant uplink (1). Both\n\nnetwork cards are connected to the physical external switch (2), and both\n\nphysical network adapters are configured to be used as uplink adapters for\n\nthe virtual switch (3).",
      "content_length": 246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 849,
      "content": "Figure 15.29\n\nOne physical network adapter is designated as the primary adapter (4),\n\nwhile the other is designated as the secondary adapter providing the\n\nstandby uplink. The secondary adapter does not forward any packets.",
      "content_length": 223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 850,
      "content": "Figure 15.30",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 851,
      "content": "The primary uplink becomes unavailable (5). The secondary standby uplink\n\nautomatically takes over and uses the virtual switch to forward the virtual\n\nservers’ packets to the external network (6). The virtual servers do not\n\nexperience interruptions and remain connected to the external network (7).\n\nThe following mechanisms are commonly part of this architecture, in\n\naddition to the virtual server:\n\nFailover System — The failover system performs the transition of\n\nunavailable uplinks to standby uplinks.\n\nHypervisor — This mechanism hosts virtual servers and some virtual\n\nswitches, and provides virtual networks and virtual switches with access to\n\nthe virtual servers.\n\nLogical Network Perimeter — Logical network perimeters ensure that the\n\nvirtual switches that are allocated or defined for each cloud consumer\n\nremain isolated.\n\nResource Replication — Resource replication is used to replicate the\n\ncurrent status of active uplinks to standby uplinks so as to maintain the\n\nnetwork connection.\n\n15.11 Storage Maintenance Window Architecture\n\nCloud storage devices that are subject to maintenance and administrative\n\ntasks sometimes need to be temporarily shut down, meaning cloud service",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 852,
      "content": "consumers and IT resources consequently lose access to these devices and\n\ntheir stored data (Figure 15.31).",
      "content_length": 107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 853,
      "content": "Figure 15.31",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 854,
      "content": "A pre-scheduled maintenance task carried out by a cloud resource\n\nadministrator causes an outage for the cloud storage device, which\n\nbecomes unavailable to cloud service consumers. Because cloud consumers\n\nwere previously notified of the outage, cloud consumers do not attempt any\n\ndata access.\n\nLive Storage Migration\n\nThe live storage migration program is a sophisticated\n\nsystem that utilizes the LUN migration component to\n\nreliably move LUNs by enabling the original copy to",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 855,
      "content": "remain active until after the destination copy has been\n\nverified as being fully functional.\n\nThe data of a cloud storage device that is about to undergo a maintenance\n\noutage can be temporarily moved to a secondary duplicate cloud storage\n\ndevice. The storage maintenance window architecture enables cloud service\n\nconsumers to be automatically and transparently redirected to the secondary\n\ncloud storage device, without becoming aware that their primary storage\n\ndevice has been taken offline.\n\nThis architecture uses a live storage migration program, as demonstrated in\n\nFigures 15.32 to 15.37.",
      "content_length": 598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 856,
      "content": "Figure 15.32\n\nThe cloud storage device is scheduled to undergo a maintenance outage,\n\nbut unlike the scenario depicted in Figure 15.31, the cloud service",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 857,
      "content": "consumers were not notified of the outage and continue to access the cloud\n\nstorage device.\n\nFigure 15.33",
      "content_length": 105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 858,
      "content": "Live storage migration moves the LUNs from the primary storage device to\n\na secondary storage device.\n\nFigure 15.34",
      "content_length": 115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 859,
      "content": "Requests for the data are forwarded to the duplicate LUNs on the secondary\n\nstorage device, once the LUN’s data has been migrated.\n\nFigure 15.35",
      "content_length": 144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 860,
      "content": "The primary storage is powered off for maintenance.\n\nFigure 15.36",
      "content_length": 65,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 861,
      "content": "The primary storage is brought back online, after the maintenance task is\n\nfinished. Live storage migration restores the LUN data from the secondary\n\nstorage device to the primary storage device.\n\nFigure 15.37",
      "content_length": 209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 862,
      "content": "The live storage migration process is completed and all of the data access\n\nrequests are forwarded back to the primary cloud storage device.\n\nIn addition to the cloud storage device mechanism that is principal to this\n\narchitecture, the resource replication mechanism is used to keep the primary\n\nand secondary storage devices synchronized. Both manually and\n\nautomatically initiated failover can also be incorporated into this cloud\n\narchitecture via the failover system mechanism, even though the migration\n\nis often pre-scheduled.\n\nNote\n\nEdge and fog computing architectures establish\n\nenvironments outside of clouds but are covered here because\n\nthese environments still relate to clouds and are primarily\n\ncreated in support of alleviating clouds from processing\n\nresponsibilities so as to improve the performance,\n\nresponsiveness, and scalability of consumer organization\n\nsolutions.\n\nEdge and fog computing architectures offer data processing\n\nand storage capacity closer to end user devices in order to\n\nstreamline the processing and storage of data that will\n\neventually be processed and stored in the cloud.",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 863,
      "content": "Edge and fog architectures are commonly used for IoT\n\nsolutions in support of geographically distributed IoT\n\ndevices. However, both architectures can be utilized to\n\nimprove the effectiveness of standard business automation\n\nsolutions for organizations, especially those with end users\n\nin multiple physical locations.\n\n15.12 Edge Computing Architecture\n\nAn edge computing architecture introduces an intermediate processing layer\n\nthat is physically positioned between the cloud and the cloud consumer. The\n\nedge environment is intentionally designed and located to be more\n\naccessible and performant for the consumer organization.\n\nPortions of the cloud-based solution are moved to the edge environment\n\nwhere they can be supported with dedicated infrastructure that enables them\n\nto perform faster, more responsively, and with greater scalability. Typically,\n\nthe heaver processing responsibilities will remain with the cloud, while the\n\nparts of a solution with lower-end processing responsibilities are moved to\n\nthe edge layer.\n\nEdge architectures are typically utilized by consumer organizations with\n\nmultiple, distributed physical locations. For each such location a separate\n\nedge environment can be established (Figure 15.38). Edge computing\n\nenvironments can be implemented in suitable third-party locations that have",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 864,
      "content": "the necessary resources, such as internet service providers and\n\ntelecommunication providers.\n\nFigure 15.38\n\nAn edge computing architecture with a set of edge environments, each of\n\nwhich accommodates users or devices in a separate physical location.\n\nEdge computing can benefit application architectures by reducing\n\nbandwidth requirements, optimizing resource utilization, improving\n\nsecurity (by encrypting data closer to its origin), and even reducing power\n\nconsumption.",
      "content_length": 475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 865,
      "content": "15.13 Fog Computing Architecture\n\nA fog computing architecture adds an additional processing layer in\n\nbetween edge environments and a cloud (Figure 15.39). This allows\n\nintermediate-level processing responsibilities to be moved from the cloud to\n\nfog environments, each of which can support and facilitate multiple edge\n\nenvironments.",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 866,
      "content": "Figure 15.39\n\nThe use of the fog computing architecture inserts an intermediary\n\nprocessing layer between the cloud and the edge environments.",
      "content_length": 142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 867,
      "content": "Fog computing pushes data processing capacity from the cloud to the fog\n\nlayer where gateways may exist to effectively relay data back and forth\n\nbetween the edge environments and the cloud. When edge environments\n\nneed to send massive volumes of data to the cloud, the fog environment can\n\nfirst determine what data carries more value in order to optimize the data\n\ntransfers. The gateways in the fog then first send critical data to the cloud to\n\nbe stored and processed while the remaining data relayed by edge\n\ncomputers may need to then be locally processed by resources in the fog\n\nenvironment.\n\nAs with edge computing, fog computing is also commonly used to support\n\nIoT solutions. The use of fog computing for a business automation solution\n\nis generally warranted when the solution needs to support many users\n\nacross highly distributed user bases.",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 868,
      "content": "Part IV\n\nWorking with Clouds\n\nChapter 16: Cloud Delivery Model Considerations\n\nChapter 17: Cost Metrics and Pricing Models\n\nChapter 18: Service Quality Metrics and SLAs\n\nEach of the chapters in this part of the book addresses a different topic area\n\nthat pertains to planning or using cloud environments and cloud-based\n\ntechnologies. The numerous considerations, strategies, and metrics provided\n\nin these chapters help associate topics covered in preceding chapters with\n\nreal-world requirements and constraints.",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 869,
      "content": "Chapter 16\n\nCloud Delivery Model Considerations\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\n16.3 Case Study Example\n\nMost of the preceding chapters have been focused on technologies and\n\nmodels used to define and implement infrastructure and architecture layers\n\nwithin cloud environments. This chapter revisits the cloud delivery models\n\nthat were introduced in Chapter 4 in order to address a number of real world\n\nconsiderations within the context of IaaS, PaaS, and SaaS-based\n\nenvironments.\n\nThe chapter is organized into two primary sections that explore cloud\n\ndelivery model issues pertaining to cloud providers and cloud consumers\n\nrespectively.\n\n16.1 Cloud Delivery Models: The Cloud Provider Perspective\n\nThis section explores the architecture and administration of IaaS, PaaS, and\n\nSaaS cloud delivery models from the point of view of the cloud provider.\n\nThe integration and management of these cloud-based environments as part",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 870,
      "content": "of greater environments and how they can relate to different technologies\n\nand cloud mechanism combinations are examined.\n\nBuilding IaaS Environments\n\nThe virtual server and cloud storage device mechanisms represent the two\n\nmost fundamental IT resources that are delivered as part of a standard rapid\n\nprovisioning architecture within IaaS environments. They are offered in\n\nvarious standardized configurations that are defined by the following\n\nproperties:\n\noperating system\n\nprimary memory capacity\n\nprocessing capacity\n\nvirtualized storage capacity\n\nMemory and virtualized storage capacity is usually allocated with\n\nincrements of 1 GB to simplify the provisioning of underlying physical IT\n\nresources. When limiting cloud consumer access to virtualized\n\nenvironments, IaaS offerings are preemptively assembled by cloud\n\nproviders via virtual server images that capture the pre-defined\n\nconfigurations. Some cloud providers may offer cloud consumers direct\n\nadministrative access to physical IT resources, in which case the bare-metal\n\nprovisioning architecture may come into play.",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 871,
      "content": "Snapshots can be taken of a virtual server to record its current state,\n\nmemory, and configuration of a virtualized IaaS environment for backup\n\nand replication purposes, in support of horizontal and vertical scaling\n\nrequirements. For example, a virtual server can use its snapshot to become\n\nreinitialized in another hosting environment after its capacity has been\n\nincreased to allow for vertical scaling. The snapshot can alternatively be\n\nused to duplicate a virtual server. The management of custom virtual server\n\nimages is a vital feature that is provided via the remote administration\n\nsystem mechanism. Most cloud providers also support importing and\n\nexporting options for custom-built virtual server images in both proprietary\n\nand standard formats.\n\nData Centers\n\nCloud providers can offer IaaS-based IT resources from multiple\n\ngeographically diverse data centers, which provides the following primary\n\nbenefits:\n\nMultiple data centers can be linked together for increased resiliency. Each\n\ndata center is placed in a different location to lower the chances of a single\n\nfailure forcing all of the data centers to go offline simultaneously.\n\nConnected through high-speed communications networks with low latency,\n\ndata centers can perform load balancing, IT resource backup and\n\nreplication, and increase storage capacity, while improving availability and",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 872,
      "content": "reliability. Having multiple data centers spread over a greater area further\n\nreduces network latency.\n\nData centers that are deployed in different countries make access to IT\n\nresources more convenient for cloud consumers that are constricted by legal\n\nand regulatory requirements.\n\nFigure 16.1 provides an example of a cloud provider that is managing four\n\ndata centers that are split between two different geographic regions.",
      "content_length": 428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 874,
      "content": "Figure 16.1\n\nA cloud provider is provisioning and managing an IaaS environment with\n\nIT resources from different data centers in the United States and the United\n\nKingdom.\n\nWhen an IaaS environment is used to provide cloud consumers with\n\nvirtualized network environments, each cloud consumer is segregated into a\n\ntenant environment that isolates IT resources from the rest of the cloud\n\nthrough the Internet. VLANs and network access control software\n\ncollaboratively realize the corresponding logical network perimeters.\n\nScalability and Reliability\n\nWithin IaaS environments, cloud providers can automatically provision\n\nvirtual servers via the dynamic vertical scaling type of the dynamic\n\nscalability architecture. This can be performed through the VIM, as long as\n\nthe host physical servers have sufficient capacity. The VIM can scale virtual\n\nservers out using resource replication as part of a resource pool architecture,\n\nif a given physical server has insufficient capacity to support vertical\n\nscaling. The load balancer mechanism, as part of a workload distribution",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 875,
      "content": "architecture, can be used to distribute the workload among IT resources in a\n\npool to complete the horizontal scaling process.\n\nManual scalability requires the cloud consumer to interact with a usage and\n\nadministration program to explicitly request IT resource scaling. In contrast,\n\nautomatic scalability requires the automated scaling listener to monitor the\n\nworkload and reactively scale the resource capacity. This mechanism\n\ntypically acts as a monitoring agent that tracks IT resource usage in order to\n\nnotify the resource management system when capacity has been exceeded.\n\nReplicated IT resources can be arranged in high-availability configuration\n\nthat forms a failover system for implementation via standard VIM features.\n\nAlternatively, a high-availability/high-performance resource cluster can be\n\ncreated at the physical or virtual server level, or both simultaneously. The\n\nmultipath resource access architecture is commonly employed to enhance\n\nreliability via the use of redundant access paths, and some cloud providers\n\nfurther offer the provisioning of dedicated IT resources via the resource\n\nreservation architecture.\n\nMonitoring\n\nCloud usage monitors in an IaaS environment can be implemented using the\n\nVIM or specialized monitoring tools that directly comprise and/or interface\n\nwith the virtualization platform. Several common capabilities of the IaaS\n\nplatform involve monitoring:",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 876,
      "content": "Virtual Server Lifecycles – Recording and tracking uptime periods and the\n\nallocation of IT resources, for pay-per-use monitors and time-based billing\n\npurposes.\n\nData Storage – Tracking and assigning the allocation of storage capacity to\n\ncloud storage devices on virtual servers, for pay-per-use monitors that\n\nrecord storage usage for billing purposes.\n\nNetwork Traffic – For pay-per-use monitors that measure inbound and\n\noutbound network usage and SLA monitors that track QoS metrics, such as\n\nresponse times and network losses.\n\nFailure Conditions – For SLA monitors that track IT resource and QoS\n\nmetrics to provide warning in times of failure.\n\nEvent Triggers – For audit monitors that appraise and evaluate the\n\nregulatory compliance of select IT resources.\n\nMonitoring architectures within IaaS environments typically involve service\n\nagents that communicate directly with backend management systems.\n\nSecurity\n\nCloud security mechanisms that are relevant for securing IaaS environments\n\ninclude:",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 877,
      "content": "encryption, hashing, digital signature, and PKI mechanisms for overall\n\nprotection of data transmission\n\nIAM and SSO mechanisms for accessing services and interfaces in security\n\nsystems that rely on user identification, authentication, and authorization\n\ncapabilities\n\ncloud-based security groups for isolating virtual environments through\n\nhypervisors and network segments via network management software\n\nhardened virtual server images for internal and externally available virtual\n\nserver environments\n\nvarious cloud usage monitors to track provisioned virtual IT resources to\n\ndetect abnormal usage patterns\n\nEquipping PaaS Environments\n\nPaaS environments typically need to be outfitted with a selection of\n\napplication development and deployment platforms in order to\n\naccommodate different programming models, languages, and frameworks.\n\nA separate ready-made environment is usually created for each\n\nprogramming stack that contains the necessary software to run applications\n\nspecifically developed for the platform.\n\nEach platform is accompanied by a matching SDK and IDE, which can be\n\ncustom-built or enabled by IDE plugins supplied by the cloud provider. IDE",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 878,
      "content": "toolkits can simulate the cloud runtime locally within the PaaS environment\n\nand usually include executable application servers. The security restrictions\n\nthat are inherent to the runtime are also simulated in the development\n\nenvironment, including checks for unauthorized attempts to access system\n\nIT resources.\n\nCloud providers often offer a resource management system mechanism that\n\nis customized for the PaaS platform so that cloud consumers can create and\n\ncontrol customized virtual server images with ready-made environments.\n\nThis mechanism also provides features specific to the PaaS platform, such\n\nas managing deployed applications and configuring multitenancy. Cloud\n\nproviders further rely on a variation of the rapid provisioning architecture\n\nknown as platform provisioning, which is designed specifically to provision\n\nready-made environments.\n\nScalability and Reliability\n\nThe scalability requirements of cloud services and applications that are\n\ndeployed within PaaS environments are generally addressed via dynamic\n\nscalability and workload distribution architectures that rely on the use of\n\nnative automated scaling listeners and load balancers. The resource pooling\n\narchitecture is further utilized to provision IT resources from resource pools\n\nmade available to multiple cloud consumers.\n\nCloud providers can evaluate network traffic and server-side connection\n\nusage against the instance’s workload, when determining how to scale an",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 879,
      "content": "overloaded application as per parameters and cost limitations provided by\n\nthe cloud consumer. Alternatively, cloud consumers can configure the\n\napplication designs to customize the incorporation of available mechanisms\n\nthemselves.\n\nThe reliability of ready-made environments and hosted cloud services and\n\napplications can be supported with standard failover system mechanisms\n\n(Figure 16.2), as well as the non-disruptive service relocation architecture,\n\nso as to shield cloud consumers from failover conditions. The resource\n\nreservation architecture may also be in place to offer exclusive access to\n\nPaaS-based IT resources. As with other IT resources, ready-made\n\nenvironments can also span multiple data centers and geographical regions\n\nto further increase availability and resiliency.",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 881,
      "content": "Figure 16.2\n\nLoad balancers are used to distribute ready-made environment instances\n\nthat are part of a failover system, while automated scaling listeners are\n\nused to monitor the network and instance workloads (1). The ready-made\n\nenvironments are scaled out in response to an increase in workload (2), and\n\nthe failover system detects a failure condition and stops replicating a failed\n\nready-made environment (3).\n\nMonitoring\n\nSpecialized cloud usage monitors in PaaS environments are used to monitor\n\nthe following:\n\nReady-Made Environment Instances – The applications of these instances\n\nare recorded by pay-per-use monitors for the calculation of time-based\n\nusage fees.\n\nData Persistence – This statistic is provided by pay-per-use monitors that\n\nrecord the number of objects, individual occupied storage sizes, and\n\ndatabase transactions per billing period.",
      "content_length": 865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 882,
      "content": "Network Usage – Inbound and outbound network usage is tracked for pay-\n\nper-use monitors and SLA monitors that track network-related QoS metrics.\n\nFailure Conditions – SLA monitors that track the QoS metrics of IT\n\nresources need to capture failure statistics.\n\nEvent Triggers – This metric is primarily used by audit monitors that need\n\nto respond to certain types of events.\n\nSecurity\n\nThe PaaS environment, by default, does not usually introduce the need for\n\nnew cloud security mechanisms beyond those that are already provisioned\n\nfor IaaS environments.\n\nOptimizing SaaS Environments\n\nIn SaaS implementations, cloud service architectures are generally based on\n\nmultitenant environments that enable and regulate concurrent cloud\n\nconsumer access (Figure 16.3). SaaS IT resource segregation does not\n\ntypically occur at the infrastructure level in SaaS environments, as it does in\n\nIaaS and PaaS environments.",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 883,
      "content": "Figure 16.3\n\nThe SaaS-based cloud service is hosted by a multitenant environment\n\ndeployed in a high-performance virtual server cluster. A usage and\n\nadministration portal is used by the cloud consumer to access and\n\nconfigure the cloud service.",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 884,
      "content": "SaaS implementations rely heavily on the features provided by the native\n\ndynamic scalability and workload distribution architectures, as well as non-\n\ndisruptive service relocation to ensure that failover conditions do not impact\n\nthe availability of SaaS-based cloud services.\n\nHowever, it is vital to acknowledge that, unlike the relatively vanilla\n\ndesigns of IaaS and PaaS products, each SaaS deployment will bring with it\n\nunique architectural, functional, and runtime requirements. These\n\nrequirements are specific to the nature of the business logic the SaaS-based\n\ncloud service is programmed with, as well as the distinct usage patterns it is\n\nsubjected to by its cloud service consumers.\n\nFor example, consider the diversity in functionality and usage of the\n\nfollowing recognized online SaaS offerings:\n\ncollaborative authoring and information-sharing (Wikipedia, Blogger)\n\ncollaborative management (Zimbra, Google Apps)\n\nconferencing services for instant messaging, audio/video communications\n\n(Zoom, Skype, Google Meet)\n\nenterprise management systems (ERP, CRM, CM)\n\nfile-sharing and content distribution (YouTube, Dropbox)\n\nindustry-specific software (engineering, bioinformatics)",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 885,
      "content": "messaging systems (e-mail, voicemail)\n\nmobile application marketplaces (Google Play Store, Apple App Store)\n\noffice productivity software suites (Microsoft Office, Adobe Creative\n\nCloud)\n\nsearch engines (Google, Yahoo)\n\nsocial networking media (Twitter, LinkedIn)\n\nNow consider that many of the previously listed cloud services are offered\n\nin one or more of the following implementation mediums:\n\nmobile application\n\nREST service\n\nWeb service\n\nEach of these SaaS implementation mediums provide Web-based APIs for\n\ninterfacing by cloud consumers. Examples of online SaaS-based cloud\n\nservices with Web-based APIs include:\n\nelectronic payment services (PayPal)\n\nmapping and routing services (Google Maps)",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 886,
      "content": "publishing tools (WordPress)\n\nMobile-enabled SaaS implementations are commonly supported by the\n\nmulti-device broker mechanism, unless the cloud service is intended\n\nexclusively for access by specific mobile devices.\n\nThe potentially diverse nature of SaaS functionality, the variation in\n\nimplementation technology, and the tendency to offer a SaaS-based cloud\n\nservice redundantly with multiple different implementation mediums makes\n\nthe design of SaaS environments highly specialized. Though not essential to\n\na SaaS implementation, specialized processing requirements can prompt the\n\nneed to incorporate architectural models, such as:\n\nService Load Balancing – for workload distribution across redundant SaaS-\n\nbased cloud service implementations\n\nDynamic Failure Detection and Recovery – to establish a system that can\n\nautomatically resolve some failure conditions without disruption in service\n\nto the SaaS implementation\n\nStorage Maintenance Window – to allow for planned maintenance outages\n\nthat do not impact SaaS implementation availability\n\nElastic Resource Capacity/Elastic Network Capacity – to establish inherent\n\nelasticity within the SaaS-based cloud service architecture that enables it to\n\nautomatically accommodate a range of runtime scalability requirements",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 887,
      "content": "Cloud Balancing – to instill broad resiliency within the SaaS\n\nimplementation, which can be especially important for cloud services\n\nsubjected to extreme concurrent usage volumes\n\nSpecialized cloud usage monitors can be used in SaaS environments to\n\ntrack the following types of metrics:\n\nTenant Subscription Period – This metric is used by pay-per-use monitors\n\nto record and track application usage for time-based billing. This type of\n\nmonitoring usually incorporates application licensing and regular\n\nassessments of leasing periods that extend beyond the hourly periods of\n\nIaaS and PaaS environments.\n\nApplication Usage – This metric, based on user or security groups, is used\n\nwith pay-per-use monitors to record and track application usage for billing\n\npurposes.\n\nTenant Application Functional Module – This metric is used by pay-per-use\n\nmonitors for function-based billing. Cloud services can have different\n\nfunctionality tiers according to whether the cloud consumer is free-tier or a\n\npaid subscriber.\n\nSimilar to the cloud usage monitoring that is performed in IaaS and PaaS\n\nimplementations, SaaS environments are also commonly monitored for data\n\nstorage, network traffic, failure conditions, and event triggers.",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 888,
      "content": "Security\n\nSaaS implementations generally rely on a foundation of security controls\n\ninherent to their deployment environment. Distinct business processing\n\nlogic will then add layers of additional cloud security mechanisms or\n\nspecialized security technologies.\n\n16.2 Cloud Delivery Models: The Cloud Consumer Perspective\n\nThis section raises various considerations concerning the different ways in\n\nwhich cloud delivery models are administered and utilized by cloud\n\nconsumers.\n\nWorking with IaaS Environments\n\nVirtual servers are accessed at the operating system level through the use of\n\nremote terminal applications. Accordingly, the type of client software used\n\ndirectly depends on the type of operating system that is running at the\n\nvirtual server, of which two common options are:\n\nRemote Desktop (or Remote Desktop Connection) Client – for Windows-\n\nbased environments and presents a Windows GUI desktop\n\nSSH Client – for Mac and Linux-based environments to allow for secure\n\nchannel connections to text-based shell accounts running on the server OS",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 889,
      "content": "Figure 16.4 illustrates a typical usage scenario for virtual servers that are\n\nbeing offered as IaaS services after having been created with management\n\ninterfaces.\n\nFigure 16.4\n\nA cloud resource administrator uses the Windows-based Remote Desktop\n\nclient to administer a Windows-based virtual server and the SSH client for\n\nthe Linux-based virtual server.\n\nA cloud storage device can be attached directly to the virtual servers and\n\naccessed through the virtual servers’ functional interface for management\n\nby the operating system. Alternatively, a cloud storage device can be\n\nattached to an IT resource that is being hosted outside of the cloud, such as",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 890,
      "content": "an on-premise device over a WAN or VPN. In these cases, the following\n\nformats for the manipulation and transmission of cloud storage data are\n\ncommonly used:\n\nNetworked File System – System-based storage access, whose rendering of\n\nfiles is similar to how folders are organized in operating systems (NFS,\n\nCIFS)\n\nStorage Area Network Devices – Block-based storage access collates and\n\nformats geographically diverse data into cohesive files for optimal network\n\ntransmission (iSCSI, Fibre Channel)\n\nWeb-Based Resources – Object-based storage access by which an interface\n\nthat is not integrated into the operating system logically represents files,\n\nwhich can be accessed through a Web-based interface (Amazon S3)\n\nIT Resource Provisioning Considerations\n\nCloud consumers have a high degree of control over how and to what extent\n\nIT resources are provisioned as part of their IaaS environments.\n\nFor example:\n\ncontrolling scalability features (automated scaling, load balancing)\n\ncontrolling the lifecycle of virtual IT resources (shutting down, restarting,\n\npowering up of virtual devices)",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 891,
      "content": "controlling the virtual network environment and network access rules\n\n(firewalls, logical network perimeters)\n\nestablishing and displaying service provisioning agreements (account\n\nconditions, usage terms)\n\nmanaging the attachment of cloud storage devices\n\nmanaging the pre-allocation of cloud-based IT resources (resource\n\nreservation)\n\nmanaging credentials and passwords for cloud resource administrators\n\nmanaging credentials for cloud-based security groups that access virtualized\n\nIT resources through an IAM\n\nmanaging security-related configurations\n\nmanaging customized virtual server image storage (importing, exporting,\n\nbackup)\n\nselecting high-availability options (failover, IT resource clustering)\n\nselecting and monitoring SLA metrics\n\nselecting basic software configurations (operating system, pre-installed\n\nsoftware for new virtual servers)",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 892,
      "content": "selecting IaaS resource instances from a number of available hardware-\n\nrelated configurations and options (processing capabilities, RAM, storage)\n\nselecting the geographical regions in which cloud-based IT resources\n\nshould be hosted\n\ntracking and managing costs\n\nThe management interface for these types of provisioning tasks is usually a\n\nusage and administration portal, but may also be offered via the use of\n\ncommand line interface (CLI) tools that can simplify the execution of many\n\nscripted administrative actions.\n\nEven though standardizing the presentation of administrative features and\n\ncontrols is typically preferred, using different tools and user-interfaces can\n\nsometimes be justified. For example, a script can be made to turn virtual\n\nservers on and off nightly through a CLI, while adding or removing storage\n\ncapacity can be more easily carried out using a portal.\n\nWorking with PaaS Environments\n\nA typical PaaS IDE can offer a wide range of tools and programming\n\nresources, such as software libraries, class libraries, frameworks, APIs, and\n\nvarious runtime capabilities that emulate the intended cloud-based\n\ndeployment environment. These features allow developers to create, test,\n\nand run application code within the cloud or locally (on--premise) while",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 893,
      "content": "using the IDE to emulate the cloud deployment environment. Compiled or\n\ncompleted applications are then bundled and uploaded to the cloud, and\n\ndeployed via the ready-made environments. This deployment process can\n\nalso be controlled through the IDE.\n\nPaaS also allows for applications to use cloud storage devices as\n\nindependent data storing systems for holding development-specific data (for\n\nexample in a repository that is available outside of the cloud environment).\n\nBoth SQL and NoSQL database structures are generally supported.\n\nIT Resource Provisioning Considerations\n\nPaaS environments provide less administrative control than IaaS\n\nenvironments, but still offer a significant range of management features.\n\nFor example:\n\nestablishing and displaying service provisioning agreements, such as\n\naccount conditions and usage terms\n\nselecting software platform and development frameworks for ready-made\n\nenvironments\n\nselecting instance types, which are most commonly frontend or backend\n\ninstances\n\nselecting cloud storage devices for use in ready-made environments",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 894,
      "content": "controlling the lifecycle of PaaS-developed applications (deployment,\n\nstarting, shutdown, restarting, and release)\n\ncontrolling the versioning of deployed applications and modules\n\nconfiguring availability and reliability-related mechanisms\n\nmanaging credentials for developers and cloud resource administrators\n\nusing IAM\n\nmanaging general security settings, such as accessible network ports\n\nselecting and monitoring PaaS-related SLA metrics\n\nmanaging and monitoring usage and IT resource costs\n\ncontrolling scalability features such as usage quotas, active instance\n\nthresholds, and the configuration and deployment of the automated scaling\n\nlistener and load balancer mechanisms\n\nThe usage and administration portal that is used to access PaaS\n\nmanagement features can provide the feature of pre-emptively selecting the\n\ntimes at which an IT resource is started and stopped. For example, a cloud\n\nresource administrator can set a cloud storage device to turn itself on at\n\n9:00AM then turn off twelve hours later. Building on this system can enable\n\nthe option of having the ready-made environment self-activate upon",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 895,
      "content": "receiving data requests for a particular application and turn off after an\n\nextended period of inactivity.\n\nWorking with SaaS Services\n\nBecause SaaS-based cloud services are almost always accompanied by\n\nrefined and generic APIs, they are usually designed to be incorporated as\n\npart of larger distributed solutions. A common example of this is Google\n\nMaps, which offers a comprehensive API that enables mapping information\n\nand images to be incorporated into Web sites and Web-based applications.\n\nMany SaaS offerings are provided free of charge, although these cloud\n\nservices often come with data collecting sub-programs that harvest usage\n\ndata for the benefit of the cloud provider. When using any SaaS product that\n\nis sponsored by third parties, there is a reasonable chance that it is\n\nperforming a form of background information gathering. Reading the cloud\n\nprovider’s agreement will usually help shed light on any secondary activity\n\nthat the cloud service is designed to perform.\n\nCloud consumers using SaaS products supplied by cloud providers are\n\nrelieved of the responsibilities of implementing and administering their\n\nunderlying hosting environments. Customization options are usually\n\navailable to cloud consumers; however, these options are generally limited\n\nto the runtime usage control of the cloud service instances that are\n\ngenerated specifically by and for the cloud consumer.",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 896,
      "content": "For example:\n\nmanaging security-related configurations\n\nmanaging select availability and reliability options\n\nmanaging usage costs\n\nmanaging user accounts, profiles, and access authorization\n\nselecting and monitoring SLAs\n\nsetting manual and automated scalability options and limitations\n\n16.3 Case Study Example\n\nDTGOV discovers that a number of additional\n\nmechanisms and technologies need to be assembled in\n\norder to complete its IaaS management architecture\n\n(Figure 16.5):\n\nNetwork virtualization is incorporated into logical\n\nnetwork topologies, and logical network perimeters are\n\nestablished using different firewalls and virtual\n\nnetworks.",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 897,
      "content": "The VIM is positioned as the central tool for controlling\n\nthe IaaS platform and equipping it with self-provisioning\n\ncapabilities.\n\nAdditional virtual server and cloud storage device\n\nmechanisms are implemented through the virtualization\n\nplatform, while several virtual server images that\n\nprovide base template configurations for virtual servers\n\nare created.",
      "content_length": 362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 899,
      "content": "Figure 16.5\n\nAn overview of the DTGOV management architecture.\n\nDynamic scaling is added using the VIM’s API through\n\nthe use of automated scaling listeners.\n\nHigh-availability virtual server clusters are created using\n\nthe resource replication, load balancer, failover system,\n\nand resource cluster mechanisms.\n\nA customized application that directly uses the SSO and\n\nIAM system mechanisms is built to enable\n\ninteroperability between the remote administration\n\nsystem, network management tools, and VIM.\n\nDTGOV uses a powerful commercial network\n\nmanagement tool that is customized to store event\n\ninformation gathered by the VIM and SLA monitoring\n\nagents in an SLA measurements database. The\n\nmanagement tool and database are used as part of a\n\ngreater SLA management system. In order to enable",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 900,
      "content": "billing processing, DTGOV expands a proprietary\n\nsoftware tool that is based on a set of usage\n\nmeasurements from a database populated by pay-per-use\n\nmonitors. The billing software is used as the base\n\nimplementation for the billing management system\n\nmechanism.",
      "content_length": 263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 901,
      "content": "Chapter 17\n\nCost Metrics and Pricing Models\n\n17.1 Business Cost Metrics\n\n17.2 Cloud Usage Cost Metrics\n\n17.3 Cost Management Considerations\n\nReducing operating costs and optimizing IT environments are pivotal to\n\nunderstanding and being able to compare the cost models behind\n\nprovisioning on-premise and cloud-based environments. The pricing\n\nstructures used by public clouds are typically based on utility-centric pay-\n\nper-usage models, enabling organizations to avoid up-front infrastructure\n\ninvestments. These models need to be assessed against the financial\n\nimplications of on-premise infrastructure investments and associated total\n\ncost-of-ownership commitments.\n\nThe following chapter provides metrics, formulas, and practices to assist\n\ncloud consumers in performing accurate financial analysis of cloud\n\nadoption plans.\n\n17.1 Business Cost Metrics\n\nThis section begins by describing the common types of metrics used to\n\nevaluate the estimated costs and business value of leasing cloud-based IT",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 902,
      "content": "resources when compared to the purchase of on-premise IT resources.\n\nUp-Front and On-Going Costs\n\nUp-front costs are associated with the initial investments that organizations\n\nneed to make in order to fund the IT resources they intend to use. This\n\nincludes both the costs associated with obtaining the IT resources, as well\n\nas expenses required to deploy and administer them.\n\nUp-front costs for the purchase and deployment of on-premise IT resources\n\ntend to be high. Examples of up-front costs for on-premise environments\n\ncan include hardware, software, and the labor required for deployment.\n\nUp-front costs for the leasing of cloud-based IT resources tend to be low.\n\nExamples of up-front costs for cloud-based environments can include the\n\nlabor costs required to assess and set up a cloud environment.\n\nOn-going costs represent the expenses required by an organization to run\n\nand maintain IT resources it uses.\n\nOn-going costs for the operation of on-premise IT resources can vary.\n\nExamples include licensing fees, electricity, insurance, and labor.\n\nOn-going costs for the operation of cloud-based IT resources can also vary,\n\nbut often exceed the on-going costs of on-premise IT resources (especially\n\nover a longer period of time). Examples include virtual hardware leasing\n\nfees, bandwidth usage fees, licensing fees, and labor.",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 903,
      "content": "Additional Costs\n\nTo supplement and extend a financial analysis beyond the calculation and\n\ncomparison of standard up-front and on-going business cost metrics, several\n\nother more specialized business cost metrics can be taken into account.\n\nFor example:\n\nCost of Capital – The cost of capital is a value that represents the cost\n\nincurred by raising required funds. For example, it will generally be more\n\nexpensive to raise an initial investment of $150,000 than it will be to raise\n\nthis amount over a period of three years. The relevancy of this cost depends\n\non how the organization goes about gathering the funds it requires. If the\n\ncost of capital for an initial investment is high, then it further helps justify\n\nthe leasing of cloud-based IT resources.\n\nSunk Costs – An organization will often have existing IT resources that are\n\nalready paid for and operational. The prior investment that has been made\n\nin these on-premise IT resources is referred to as sunk costs. When\n\ncomparing up-front costs together with significant sunk costs, it can be\n\nmore difficult to justify the leasing of cloud-based IT resources as an\n\nalternative.\n\nIntegration Costs – Integration testing is a form of testing required to\n\nmeasure the effort required to make IT resources compatible and\n\ninteroperable within a foreign environment, such as a new cloud platform.",
      "content_length": 1358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 904,
      "content": "Depending on the cloud deployment model and cloud delivery model being\n\nconsidered by an organization, there may be the need to further allocate\n\nfunds to carry out integration testing and additional labor related to enable\n\ninteroperability between cloud service consumers and cloud services. These\n\nexpenses are referred to as integration costs. High integration costs can\n\nmake the option of leasing cloud-based IT resources less appealing.\n\nLocked-in Costs – As explained in the Risks and Challenges section in\n\nChapter 3, cloud environments can impose portability limitations. When\n\nperforming a metrics analysis over a longer period of time, it may be\n\nnecessary to take into consideration the possibility of having to move from\n\none cloud provider to another. Due to the fact that cloud service consumers\n\ncan become dependent on proprietary characteristics of a cloud\n\nenvironment, there are locked-in costs associated with this type of move.\n\nLocked-in costs can further decrease the long-term business value of\n\nleasing cloud-based IT resources.\n\nCase Study Example\n\nATN performs a total cost-of-ownership (TCO) analysis\n\non migrating two of its legacy applications to a PaaS\n\nenvironment. The report produced by the analysis\n\nexamines comparative evaluations of on-premise and\n\ncloud-based implementations based on a three-year time\n\nframe.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 905,
      "content": "The following sections provide a summary from the\n\nreport for each of the two applications.\n\nProduct Catalog Browser\n\nThe Product Catalog Browser is a globally used Web\n\napplication that interoperates with the ATN Web portal\n\nand several other systems. This application was\n\ndeployed in a virtual server cluster that is comprised of 4\n\nvirtual servers running on 2 dedicated physical servers.\n\nThe application has its own 300 GB database that resides\n\nin a separate HA cluster. Its code was recently generated\n\nfrom a refactoring project. Only minor portability issues\n\nneeded to be addressed before it was ready to proceed\n\nwith a cloud migration.\n\nThe TCO analysis reveals the following:\n\nOn-Premise Up-Front Costs\n\nLicensing: The purchase price for each physical server\n\nhosting the application is $7,500, while the software\n\nrequired to run all 4 servers totals $30,500\n\nLabor: Labor costs are estimated as $5,500, including\n\nsetup and application deployment.",
      "content_length": 963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 906,
      "content": "The total up-front costs are: ($7,500 x 2) + $30,500 +\n\n$5,500 = $51,000\n\nThe configuration of the servers is derived from a\n\ncapacity plan that accounts for peak workloads. Storage\n\nwas not assessed as part of this plan, since the\n\napplication database is assumed to be only negligibly\n\naffected by the application’s deployment.\n\nOn-Premise On-Going Costs\n\nThe following are monthly on-going costs:\n\nEnvironmental Fees: $750\n\nLicensing Fees: $520\n\nHardware Maintenance: $100\n\nLabor: $2,600\n\nThe total on-premise on-going costs are: $750 + $520 +\n\n$100 + $2,600 = $3,970\n\nCloud-Based Up-Front Costs",
      "content_length": 598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 907,
      "content": "If the servers are leased from a cloud provider, there is\n\nno up-front cost for hardware or software. Labor costs\n\nare estimated at $5,000, which includes expenses for\n\nsolving interoperability issues and application setup.\n\nCloud-Based On-Going Costs\n\nThe following are monthly on-going costs:\n\nServer Instance: Usage fee is calculated per virtual\n\nserver at a rate of $1.25/hour per virtual server. For 4\n\nvirtual servers, this results in: 4 x ($1.25 x 720) =\n\n$3,600. However, the application consumption is\n\nequivalent to 2.3 servers when server instance scaling is\n\nfactored in, meaning the actual on-going server usage\n\ncost is: $2,070.\n\nDatabase Server and Storage: Usage fees are calculated\n\nper database size, at a rate of $1.09/GB per month =\n\n$327.\n\nNetwork: Usage fees are calculated per outbound WAN\n\ntraffic at the rate of $0.10/GB and a monthly volume of\n\n420 GB = $42.",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 908,
      "content": "Labor: Estimated at $800 per month, including expenses\n\nfor cloud resource administration tasks.\n\nThe total on-going costs are: $2,070 + $327 + $42 +\n\n$800 = $3,139\n\nThe TCO breakdown for the Product Catalog Browser\n\napplication is provided in Table 17.1.\n\nTable 17-1\n\nThe TCO analysis for the Product Catalog Browser\n\napplication.",
      "content_length": 331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 910,
      "content": "A comparison of the respective TCOs over a three-year\n\nperiod for both approaches reveals the following:\n\nOn-Premise TCO: $51,000 up-front + ($3,970 x 36) on-\n\ngoing = $193,920\n\nCloud-Based TCO: $5,000 up-front + ($3,139 x 36) on-\n\ngoing = $118,004",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 911,
      "content": "Based on the results of the TCO analysis, ATN decides\n\nto migrate the application to the cloud.\n\n17.2 Cloud Usage Cost Metrics\n\nThe following sections describe a set of usage cost metrics for calculating\n\ncosts associated with cloud-based IT resource usage measurements:\n\nNetwork Usage – inbound and outbound network traffic, as well as intra-\n\ncloud network traffic\n\nServer Usage – virtual server allocation (and resource reservation)\n\nCloud Storage Device – storage capacity allocation\n\nCloud Service – subscription duration, number of nominated users, number\n\nof transactions (of cloud services and cloud-based applications)\n\nFor each usage cost metric a description, measurement unit, and\n\nmeasurement frequency are provided, along with the cloud delivery model\n\nmost applicable to the metric. Each metric is further supplemented with a\n\nbrief example.\n\nNetwork Usage\n\nDefined as the amount of data that is transferred over a network connection,\n\nnetwork usage is typically calculated using separately measured inbound",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 912,
      "content": "network usage traffic and outbound network usage traffic metrics in relation\n\nto cloud services or other IT resources.\n\nInbound Network Usage Metric\n\nDescription – inbound network traffic\n\nMeasurement – Σ, inbound network traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 1 GB free, $0.001/GB up to 10 TB a month\n\nOutbound Network Usage Metric\n\nDescription – outbound network traffic\n\nMeasurement – Σ, outbound network traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 1 GB free a month, $0.01/GB between 1 GB to 10 TB per\n\nmonth",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 913,
      "content": "Network usage metrics can be applied to WAN traffic between IT resources\n\nof one cloud that are located in different geographical regions in order to\n\ncalculate costs for synchronization, data replication, and related forms of\n\nprocessing. Conversely, LAN usage and other network traffic among IT\n\nresources that reside at the same data center are typically not tracked.\n\nIntra-Cloud WAN Usage Metric\n\nDescription – network traffic between geographically diverse IT resources\n\nof the same cloud\n\nMeasurement – Σ, intra-cloud WAN traffic in bytes\n\nFrequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – up to 500 MB free daily and $0.01/GB thereafter, $0.005/GB\n\nafter 1 TB per month\n\nMany cloud providers do not charge for inbound traffic in order to\n\nencourage cloud consumers to migrate data to the cloud. Some also do not\n\ncharge for WAN traffic within the same cloud.\n\nNetwork-related cost metrics are determined by the following properties:",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 914,
      "content": "Static IP Address Usage – IP address allocation time (if a static IP is\n\nrequired)\n\nNetwork Load-Balancing – the amount of load-balanced network traffic (in\n\nbytes)\n\nVirtual Firewall – the amount of firewall-processed network traffic (as per\n\nallocation time)\n\nServer Usage\n\nThe allocation of virtual servers is measured using common pay-per-use\n\nmetrics in IaaS and PaaS environments that are quantified by the number of\n\nvirtual servers and ready-made environments. This form of server usage\n\nmeasurement is divided into on-demand virtual machine instance allocation\n\nand reserved virtual machine instance allocation metrics.\n\nThe former metric measures pay-per-usage fees on a short-term basis, while\n\nthe latter metric calculates up-front reservation fees for using virtual servers\n\nover extended periods. The up-front reservation fee is usually used in\n\nconjunction with the discounted pay-per-usage fees.\n\nOn-Demand Virtual Machine Instance Allocation Metric\n\nDescription – uptime of a virtual server instance\n\nMeasurement – Σ, virtual server start date to stop date",
      "content_length": 1072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 915,
      "content": "Frequency – continuous and cumulative over a predefined period\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $0.10/hour small instance, $0.20/hour medium instance,\n\n$0.90/hour large instance\n\nReserved Virtual Machine Instance Allocation Metric\n\nDescription – up-front cost for reserving a virtual server instance\n\nMeasurement – Σ, virtual server reservation start date to expiry date\n\nFrequency – daily, monthly, yearly\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $55.10/small instance, $99.90/medium instance, $249.90/large\n\ninstance\n\nAnother common cost metric for virtual server usage measures performance\n\ncapabilities. Cloud providers of IaaS and PaaS environments tend to\n\nprovision virtual servers with a range of performance attributes that are\n\ngenerally determined by CPU and RAM consumption and the amount of\n\navailable dedicated allocated storage.",
      "content_length": 861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 916,
      "content": "Cloud Storage Device Usage\n\nCloud storage is generally charged by the amount of space allocated within\n\na predefined period, as measured by the on-demand storage allocation\n\nmetric. Similar to IaaS-based cost metrics, on-demand storage allocation\n\nfees are usually based on short time increments (such as on an hourly\n\nbasis). Another common cost metric for cloud storage is I/O data\n\ntransferred, which measures the amount of transferred input and output\n\ndata.\n\nOn-Demand Storage Space Allocation Metric\n\nDescription – duration and size of on-demand storage space allocation in\n\nbytes\n\nMeasurement – Σ, date of storage release / reallocation to date of storage\n\nallocation (resets upon change in storage size)\n\nFrequency – continuous\n\nCloud Delivery Model – IaaS, PaaS, SaaS\n\nExample – $0.01/GB per hour (typically expressed as GB/month)\n\nI/O Data Transferred Metric\n\nDescription – amount of transferred I/O data",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 917,
      "content": "Measurement – Σ, I/O data in bytes\n\nFrequency – continuous\n\nCloud Delivery Model – IaaS, PaaS\n\nExample – $0.10/TB\n\nNote that some cloud providers do not charge for I/O usage for IaaS and\n\nPaaS implementations, and limit charges to storage space allocation only.\n\nCloud Service Usage\n\nCloud service usage in SaaS environments is typically measured using the\n\nfollowing three metrics:\n\nApplication Subscription Duration Metric\n\nDescription – duration of cloud service usage subscription\n\nMeasurement – Σ, subscription start date to expiry date\n\nFrequency – daily, monthly, yearly\n\nCloud Delivery Model – SaaS\n\nExample – $69.90 per month",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 918,
      "content": "Number of Nominated Users Metric\n\nDescription – number of registered users with legitimate access\n\nMeasurement – number of users\n\nFrequency – monthly, yearly\n\nCloud Delivery Model – SaaS\n\nExample – $0.90/additional user per month\n\nNumber of Transactions Users Metric\n\nDescription – number of transactions served by the cloud service\n\nMeasurement – number of transactions (request-response message\n\nexchanges)\n\nFrequency – continuous\n\nCloud Delivery Model – PaaS, SaaS\n\nExample – $0.05 per 1,000 transactions\n\n17.3 Cost Management Considerations\n\nCost management is often centered around the lifecycle phases of cloud\n\nservices, as follows:",
      "content_length": 639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 919,
      "content": "Cloud Service Design and Development – During this stage, the vanilla\n\npricing models and cost templates are typically defined by the organization\n\ndelivering the cloud service.\n\nCloud Service Deployment – Prior to and during the deployment of a cloud\n\nservice, the backend architecture for usage measurement and billing-related\n\ndata collection is determined and implemented, including the positioning of\n\npay-per-use monitor and billing management system mechanisms.\n\nCloud Service Contracting – This phase consists of negotiations between\n\nthe cloud consumer and cloud provider with the goal of reaching a mutual\n\nagreement on rates based on usage cost metrics.\n\nCloud Service Offering – This stage entails the concrete offering of a cloud\n\nservice’s pricing models through cost templates, and any available\n\ncustomization options.\n\nCloud Service Provisioning – Cloud service usage and instance creation\n\nthresholds may be imposed by the cloud provider or set by the cloud\n\nconsumer. Either way, these and other provisioning options can impact\n\nusage costs and other fees.\n\nCloud Service Operation – This is the phase during which active usage of\n\nthe cloud service produces usage cost metric data.",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 920,
      "content": "Cloud Service Decommissioning – When a cloud service is temporarily or\n\npermanently deactivated, statistical cost data may be archived.\n\nBoth cloud providers and cloud consumers can implement cost management\n\nsystems that reference or build upon the aforementioned lifecycle phases\n\n(Figure 17.1). It is also possible for the cloud provider to carry out some\n\ncost management stages on behalf of the cloud consumer and to then\n\nprovide the cloud consumer with regular reports.",
      "content_length": 476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 921,
      "content": "Figure 17.1",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 922,
      "content": "Common cloud service lifecycle stages as they relate to cost management\n\nconsiderations.\n\nPricing Models\n\nThe pricing models used by cloud providers are defined using templates\n\nthat specify unit costs for fine-grained resource usage according to usage\n\ncost metrics. Various factors can influence a pricing model, such as:\n\nmarket competition and regulatory requirements\n\noverhead incurred during the design, development, deployment, and\n\noperation of cloud services and other IT resources\n\nopportunities to reduce expenses via IT resource sharing and data center\n\noptimization\n\nMost major cloud providers offer cloud services at relatively stable,\n\ncompetitive prices even though their own expenses can be volatile. A price\n\ntemplate or pricing plan contains a set of standardized costs and metrics that\n\nspecify how cloud service fees are measured and calculated. Price templates\n\ndefine a pricing model’s structure by setting various units of measure, usage\n\nquotas, discounts, and other codified fees. A pricing model can contain\n\nmultiple price templates, whose formulation is determined by variables like:\n\nCost Metrics and Associated Prices – These are costs that are dependent on\n\nthe type of IT resource allocation (such as on-demand versus reserved",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 923,
      "content": "allocation).\n\nFixed and Variable Rates Definitions – Fixed rates are based on resource\n\nallocation and define the usage quotas included in the fixed price, while\n\nvariable rates are aligned with actual resource usage.\n\nVolume Discounts – More IT resources are consumed as the degree of IT\n\nresource scaling progressively increases, thereby possibly qualifying a\n\ncloud consumer for higher discounts.\n\nCost and Price Customization Options – This variable is associated with\n\npayment options and schedules. For example, cloud consumers may be able\n\nto choose monthly, semi-annual, or annual payment installments.\n\nPrice templates are important for cloud consumers that are appraising cloud\n\nproviders and negotiating rates, since they can vary depending on the\n\nadopted cloud delivery model.\n\nFor example:\n\nIaaS – Pricing is usually based on IT resource allocation and usage, which\n\nincludes the amount of transferred network data, number of virtual servers,\n\nand allocated storage capacity.\n\nPaaS – Similar to IaaS, this model typically defines pricing for network\n\ndata transferred, virtual servers, and storage. Prices are variable depending",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 924,
      "content": "on factors such as software configurations, development tools, and licensing\n\nfees.\n\nSaaS – Because this model is solely concerned with application software\n\nusage, pricing is determined by the number of application modules in the\n\nsubscription, the number of nominated cloud service consumers, and the\n\nnumber of transactions.\n\nIt is possible for a cloud service that is provided by one cloud provider to be\n\nbuilt upon IT resources provisioned from another cloud provider. Figures\n\n17.2 and 17.3 explore two sample scenarios.\n\nFigure 17.2\n\nAn integrated pricing model, whereby the cloud consumer leases a SaaS\n\nproduct from Cloud Provider A, which is leasing an IaaS environment",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 925,
      "content": "(including the virtual server used to host the cloud service) from Cloud\n\nProvider B. The cloud consumer pays Cloud Provider A. Cloud Provider A\n\npays Cloud Provider B.\n\nFigure 17.3\n\nSeparate pricing models are used in this scenario, whereby the cloud\n\nconsumer leases a virtual server from Cloud Provider B to host the cloud\n\nservice from Cloud Provider A. Both leasing agreements may have been\n\narranged for the cloud consumer by Cloud Provider A. As part of this",
      "content_length": 465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 926,
      "content": "arrangement, there may still be some fees billed directly by Cloud Provider\n\nB to Cloud Provider A.\n\nMulti-Cloud Cost Management\n\nWithin a multi-cloud environment, it becomes important to manage the\n\ndifferent billing, pricing, and provisioning arrangements that are established\n\nwith the different cloud providers (Figure 17.4).",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 927,
      "content": "Figure 17.4",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 928,
      "content": "An organization using a multi-cloud architecture identifies and selects from\n\neach cloud provider those services that offer an optimal economical\n\nadvantage.\n\nSome cloud providers offer reserved IT resources for which the cloud\n\nconsumer may commit to paying for a fixed period of time, at a discount.\n\nOthers offer the purchase of “points” or “vouchers” that are calculated to\n\ncover estimated costs, allowing for pre-determined fixed monthly charges\n\nand fitting periodic budgeting requirements frequently preferred by\n\naccounting and financial areas of organizations. A third option are spot\n\ninstances that run on spurious capacity for a highly discounted price, which\n\ncan be used for development or testing purposes for a very low cost. In a\n\nmulti-cloud architecture, all of these benefits can be combined from\n\ndifferent cloud providers, allowing the organization to select only the most\n\nconvenient.\n\nBefore migrating to the cloud, an organization must predict the expenses\n\nassociated with its new IT resource provisioning. Furthermore, when\n\nconsidering the implementation of a multi-cloud architecture, specific\n\nplanning for reduced or discounted expenses must be part of the process.\n\nSome of the strategies an organization can follow to achieve this are:\n\nDesigning a Resource Plan for Each Cloud Provider – This plan should\n\ninclude specifying the true needs of the cloud consumer and enforcing them\n\nthrough standards that only allow the use of those resources, as well as\n\nsetting budgets and expense notifications in accordance to each cloud",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 929,
      "content": "provider’s capabilities for when budget thresholds are met. Supervising that\n\nthe plan is completed as designed is a crucial cloud governance task.\n\nTagging Resources – Utilizing tags enables a business to logically group the\n\nresources in its cloud environment for quick identification. It also allows\n\nthe organization to determine which expenses are associated with each\n\ndepartment or business unit. Each cloud provider has its own tagging\n\nsystem. Using a remote administration system, tagging can be standardized\n\nfor all cloud providers in a multi-cloud architecture.\n\nEstablishing Guidelines and Rules on Resource Deployment –\n\nOrganizations should specify how, when, and by whom different types of\n\nresources are to be deployed for every different cloud provider. The kind of\n\nresources intended to be made available should also be standardized,\n\nconsidering the various deployment options that each cloud provider offers.\n\nCase Study Example\n\nDTGOV structures their pricing model around leasing\n\npackages for virtual servers and block-based cloud\n\nstorage devices, with the assumption that resource\n\nallocation is performed either on-demand or based on\n\nalready reserved IT resources.\n\nOn-demand resource allocation is measured and charged\n\nback by the hour, while reserved resource allocation",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 930,
      "content": "requires a one to three-year commitment from the cloud\n\nconsumer, with fees billed monthly.\n\nAs IT resources can scale up and down automatically,\n\nany additional capacity used is charged on a pay-per-use\n\nbasis whenever a reserved IT resource is scaled beyond\n\nits allocated capacity. Windows and Linux-based virtual\n\nservers are made available in the following basic\n\nperformance profiles:\n\nSmall Virtual Server Instance – 1 virtual processor core,\n\n4 GB of virtual RAM, and 320 GB of storage space in\n\nthe root file system.\n\nMedium Virtual Server Instance – 2 virtual processor\n\ncores, 8 GB of virtual RAM, and 540 GB of storage\n\nspace in the root file system.\n\nLarge Virtual Server Instance – 8 virtual processor\n\ncores, 16 GB of virtual RAM, and 1.2 TB of storage\n\nspace in the root file system.\n\nMemory Large Virtual Server Instance – 8 virtual\n\nprocessor cores, 64 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 931,
      "content": "Processor Large Virtual Server Instance – 32 virtual\n\nprocessor cores, 16 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.\n\nUltra-Large Virtual Server Instance – 128 virtual\n\nprocessor cores, 512 GB of virtual RAM, and 1.2 TB of\n\nstorage space in the root file system.\n\nVirtual servers are also available in “resilient” or\n\n“clustered” formats. With the former option the virtual\n\nservers are replicated in at least two different data\n\ncenters. In the latter case, the virtual servers are run in a\n\nhigh-availability cluster that is implemented by the\n\nvirtualization platform.\n\nThe pricing model is further based on the capacity of the\n\ncloud storage devices as expressed by multiples of 1 GB,\n\nwith a minimum of 40 GB. Storage device capacity can\n\nbe fixed and administratively adjusted by the cloud\n\nconsumer to increase or decrease by increments of 40\n\nGB, while the block storage has a maximum capacity of\n\n1.2 TB. I/O transfers to and from cloud storage devices\n\nare also subject to charges in addition to pay-per-use fees\n\napplied to outbound WAN traffic. Inbound WAN and\n\nintra-cloud traffic are free of charge.",
      "content_length": 1143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 932,
      "content": "A complimentary usage allowance permits cloud\n\nconsumers to lease up to three small virtual server\n\ninstances and a 60 GB block-based cloud storage device,\n\n5 GB of I/O transfers monthly, as well as 5 GB of WAN\n\noutbound traffic monthly, all in the first 90 days. As\n\nDTGOV prepares their pricing model for public release,\n\nthey realize that setting cloud service prices is more\n\nchallenging than they expected because:\n\nTheir prices need to reflect and respond to marketplace\n\nconditions while staying competitive with other cloud\n\nofferings and remaining profitable to DTGOV.\n\nThe client portfolio has not been established yet, as\n\nDTGOV is expecting new customers. Their non-cloud\n\nclients are expected to progressively migrate to the\n\ncloud, although the actual rate of migration is too\n\ndifficult to predict.\n\nAfter performing further market research, DTGOV\n\nsettles on the following price template for virtual server\n\ninstance allocation:\n\nVirtual Server On-Demand Instance Allocation\n\nMetric: on-demand instance allocation",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 933,
      "content": "Measurement: pay-per-use charges calculated for total\n\nservice consumption for each calendar month (hourly\n\nrate is used for the actual instance size when the instance\n\nhas been scaled up)\n\nBilling Period: monthly\n\nThe price template is outlined in Table 17.2.\n\nTable 17-2\n\nThe price template for virtual server on-demand\n\ninstance allocation.",
      "content_length": 343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 936,
      "content": "Surcharge for clustered IT resources: 120%\n\nSurcharge for resilient IT resources: 150%\n\nVirtual Server Reserved Instance Allocation\n\nMetric: reserved instance allocation\n\nMeasurement: reserved instance allocation fee charged\n\nup-front with pay-per-use fees calculated based on the\n\ntotal consumption during each calendar month\n\n(additional charges apply for periods when the instance\n\nis scaled up)\n\nBilling Period: monthly\n\nThe price template is outlined in Table 17.3.\n\nTable 17-3\n\nThe price template for virtual server reserved\n\ninstance allocation.",
      "content_length": 552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 940,
      "content": "Surcharge for clustered IT resources: 100%\n\nSurcharge for resilient IT resources: 120%",
      "content_length": 86,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 941,
      "content": "DTGOV further provides the following simplified price\n\ntemplates for cloud storage device allocation and WAN\n\nbandwidth usage:\n\nCloud Storage Device\n\nMetric: on-demand storage allocation, I/O data\n\ntransferred\n\nMeasurement: pay-per-use charges calculated based on\n\ntotal consumption during each calendar month (storage\n\nallocation calculated with per hour granularity and\n\ncumulative I/O transfer volume)\n\nBilling Period: monthly\n\nPrice Template: $0.10/GB per month of allocated\n\nstorage, $0.001/GB for I/O transfers\n\nWAN Traffic\n\nMetric: outbound network usage\n\nMeasurement: pay-per-use charges calculated based on\n\ntotal consumption for each calendar month (WAN traffic\n\nvolume calculated cumulatively)",
      "content_length": 704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 942,
      "content": "Billing Period: monthly\n\nPrice Template: $0.01/GB for outbound network data\n\nAdditional Considerations\n\nNegotiation – Cloud provider pricing is often open to negotiation,\n\nespecially for customers willing to commit to higher volumes or longer\n\nterms. Price negotiations can sometimes be executed online via the cloud\n\nprovider’s Web site by submitting estimated usage volumes along with\n\nproposed discounts. There are even tools available for cloud consumers to\n\nhelp generate accurate IT resource usage estimates for this purpose.\n\nPayment Options – After completing each measurement period, the cloud\n\nprovider’s billing management system calculates the amount owed by a\n\ncloud consumer. There are two common payment options available to cloud\n\nconsumers: pre-payment and post-payment. With pre-paid billing, cloud\n\nconsumers are provided with IT resource usage credits that can be applied\n\nto future usage bills. With the post-payment method, cloud consumers are\n\nbilled and invoiced for each IT resource consumption period, which is\n\nusually on a monthly basis.\n\nCost Archiving – By tracking historical billing information both cloud\n\nproviders and cloud consumers can generate insightful reports that help\n\nidentify usage and financial trends.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 943,
      "content": "Chapter 18\n\nService Quality Metrics and SLAs\n\n18.1 Service Quality Metrics\n\n18.2 Case Study Example\n\n18.3 SLA Guidelines\n\n18.4 Case Study Example\n\nService-level agreements (SLAs) are a focal point of negotiations, contract\n\nterms, legal obligations, and runtime metrics and measurements. SLAs\n\nformalize the guarantees put forth by cloud providers, and correspondingly\n\ninfluence or determine the pricing models and payment terms. SLAs set\n\ncloud consumer expectations and are integral to how organizations build\n\nbusiness automation around the utilization of cloud-based IT resources.\n\nThe guarantees made by a cloud provider to a cloud consumer are often\n\ncarried forward, in that the same guarantees are made by the cloud\n\nconsumer organization to its clients, business partners, or whomever will be\n\nrelying on the services and solutions hosted by the cloud provider. It is\n\ntherefore crucial for SLAs and related service quality metrics to be\n\nunderstood and aligned in support of the cloud consumer’s business\n\nrequirements, while also ensuring that the guarantees can, in fact, be",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 944,
      "content": "realistically fulfilled consistently and reliably by the cloud provider. The\n\nlatter consideration is especially relevant for cloud providers that host\n\nshared IT resources for high volumes of cloud consumers, each of which\n\nwill have been issued its own SLA guarantees.\n\n18.1 Service Quality Metrics\n\nSLAs issued by cloud providers are human-readable documents that\n\ndescribe quality-of-service (QoS) features, guarantees, and limitations of\n\none or more cloud-based IT resources.\n\nSLAs use service quality metrics to express measurable QoS characteristics.\n\nFor example:\n\nAvailability — up-time, outages, service duration\n\nReliability — minimum time between failures, guaranteed rate of successful\n\nresponses • Performance — capacity, response time, and delivery time\n\nguarantees • Scalability — capacity fluctuation and responsiveness\n\nguarantees • Resiliency — mean-time to switchover and recovery\n\nSLA management systems use these metrics to perform periodic\n\nmeasurements that verify compliance with SLA guarantees, in addition to\n\ncollecting SLA-related data for various types of statistical analyses.",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 945,
      "content": "Each service quality metric is ideally defined using the following\n\ncharacteristics:\n\nQuantifiable — The unit of measure is clearly set, absolute, and appropriate\n\nso that the metric can be based on quantitative measurements.\n\nRepeatable — The methods of measuring the metric need to yield identical\n\nresults when repeated under identical conditions.\n\nComparable — The units of measure used by a metric need to be\n\nstandardized and comparable. For example, a service quality metric cannot\n\nmeasure smaller quantities of data in bits and larger quantities in bytes.\n\nEasily Obtainable — The metric needs to be based on a non-proprietary,\n\ncommon form of measurement that can be easily obtained and understood\n\nby cloud consumers.\n\nThe upcoming sections provide a series of common service quality metrics,\n\neach of which is documented with description, unit of measure,\n\nmeasurement frequency, and applicable cloud delivery model values, as\n\nwell as a brief example.",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 946,
      "content": "Service Availability Metrics\n\nAvailability Rate Metric\n\nThe overall availability of an IT resource is usually expressed as a\n\npercentage of up-time. For example, an IT resource that is always available\n\nwill have an up-time of 100%.\n\nDescription — percentage of service up-time\n\nMeasurement — total up-time / total time\n\nFrequency — weekly, monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — minimum 99.5% up-time\n\nAvailability rates are calculated cumulatively, meaning that unavailability\n\nperiods are combined in order to compute the total downtime (Table 18.1).\n\nTable 18-1\n\nSample availability rates measured in units of seconds.",
      "content_length": 651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 948,
      "content": "Outage Duration Metric\n\nThis service quality metric is used to define both maximum and average\n\ncontinuous outage service-level targets.\n\nDescription — duration of a single outage\n\nMeasurement — date/time of outage end — date/time of outage start •\n\nFrequency — per event\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 1 hour maximum, 15 minute average\n\nNote\n\nIn addition to being quantitatively measured, availability can\n\nbe described qualitatively using terms such as high-\n\navailability (HA), which is used to label an IT resource with\n\nexceptionally low downtime usually due to underlying\n\nresource replication and/or clustering infrastructure.\n\nService Reliability Metrics\n\nA characteristic closely related to availability, reliability is the probability\n\nthat an IT resource can perform its intended function under pre-defined\n\nconditions without experiencing failure. Reliability focuses on how often",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 949,
      "content": "the service performs as expected, which requires the service to remain in an\n\noperational and available state. Certain reliability metrics only consider\n\nruntime errors and exception conditions as failures, which are commonly\n\nmeasured only when the IT resource is available.\n\nMean-Time Between Failures (MTBF) Metric\n\nDescription — expected time between consecutive service failures •\n\nMeasurement — Σ normal operational period duration / number of failures •\n\nFrequency — monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 90 day average\n\nReliability Rate Metric\n\nOverall reliability is more complicated to measure and is usually defined by\n\na reliability rate that represents the percentage of successful service\n\noutcomes. This metric measures the effects of non-fatal errors and failures\n\nthat occur during up-time periods. For example, an IT resource’s reliability\n\nis 100% if it has performed as expected every time it is invoked, but only\n\n80% if it fails to perform every fifth time.\n\nDescription — percentage of successful service outcomes under pre-\n\ndefined conditions • Measurement — total number of successful responses /\n\ntotal number of requests • Frequency — weekly, monthly, yearly",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 950,
      "content": "Cloud Delivery Model — SaaS\n\nExample — minimum 99.5%\n\nService Performance Metrics\n\nService performance refers to the ability on an IT resource to carry out its\n\nfunctions within expected parameters. This quality is measured using\n\nservice capacity metrics, each of which focuses on a related measurable\n\ncharacteristic of IT resource capacity. A set of common performance\n\ncapacity metrics is provided in this section. Note that different metrics may\n\napply, depending on the type of IT resource being measured.\n\nNetwork Capacity Metric\n\nDescription — measurable characteristics of network capacity\n\nMeasurement — bandwidth / throughput in bits per second\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 10 MB per second",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 951,
      "content": "Storage Device Capacity Metric\n\nDescription — measurable characteristics of storage device capacity •\n\nMeasurement — storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 80 GB of storage\n\nServer Capacity Metric\n\nDescription — measurable characteristics of server capacity\n\nMeasurement — number of CPUs, CPU frequency in GHz, RAM size in\n\nGB, storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 1 core at 1.7 GHz, 16 GB of RAM, 80 GB of storage\n\nWeb Application Capacity Metric\n\nDescription — measurable characteristics of Web application capacity •\n\nMeasurement — rate of requests per minute",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 952,
      "content": "Frequency — continuous\n\nCloud Delivery Model — SaaS\n\nExample — maximum 100,000 requests per minute\n\nInstance Starting Time Metric\n\nDescription — length of time required to initialize a new instance •\n\nMeasurement — date/time of instance up — date/time of start request •\n\nFrequency — per event\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 5 minute maximum, 3 minute average\n\nResponse Time Metric\n\nDescription — time required to perform synchronous operation\n\nMeasurement — (date/time of request — date/time of response) / total\n\nnumber of requests • Frequency — daily, weekly, monthly\n\nCloud Delivery Model — SaaS\n\nExample — 5 millisecond average",
      "content_length": 647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 953,
      "content": "Completion Time Metric\n\nDescription — time required to complete an asynchronous task\n\nMeasurement — (date of request — date of response) / total number of\n\nrequests • Frequency — daily, weekly, monthly\n\nCloud Delivery Model — PaaS, SaaS\n\nExample — 1 second average\n\nService Scalability Metrics\n\nService scalability metrics are related to IT resource elasticity capacity,\n\nwhich is related to the maximum capacity that an IT resource can achieve,\n\nas well as measurements of its ability to adapt to workload fluctuations. For\n\nexample, a server can be scaled up to a maximum of 128 CPU cores and\n\n512 GB of RAM, or scaled out to a maximum of 16 load-balanced\n\nreplicated instances.\n\nThe following metrics help determine whether dynamic service demands\n\nwill be met proactively or reactively, as well as the impacts of manual or\n\nautomated IT resource allocation processes.",
      "content_length": 871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 954,
      "content": "Storage Scalability (Horizontal) Metric\n\nDescription — permissible storage device capacity changes in response to\n\nincreased workloads • Measurement — storage size in GB\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 1,000 GB maximum (automated scaling)\n\nServer Scalability (Horizontal) Metric\n\nDescription — permissible server capacity changes in response to increased\n\nworkloads • Measurement — number of virtual servers in resource pool\n\nFrequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 1 virtual server minimum, 10 virtual server maximum\n\n(automated scaling)\n\nServer Scalability (Vertical) Metric\n\nDescription — permissible server capacity fluctuations in response to\n\nworkload fluctuations • Measurement — number of CPUs, RAM size in GB",
      "content_length": 790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 955,
      "content": "Frequency — continuous\n\nCloud Delivery Model — IaaS, PaaS\n\nExample — 512 core maximum, 512 GB of RAM\n\nService Resiliency Metrics\n\nThe ability of an IT resource to recover from operational disturbances is\n\noften measured using service resiliency metrics. When resiliency is\n\ndescribed within or in relation to SLA resiliency guarantees, it is often\n\nbased on redundant implementations and resource replication over different\n\nphysical locations, as well as various disaster recovery systems.\n\nThe type of cloud delivery model determines how resiliency is implemented\n\nand measured. For example, the physical locations of replicated virtual\n\nservers that are implementing resilient cloud services can be explicitly\n\nexpressed in the SLAs for IaaS environments, while being implicitly\n\nexpressed for the corresponding PaaS and SaaS environments.\n\nResiliency metrics can be applied in three different phases to address the\n\nchallenges and events that can threaten the regular level of a service: •\n\nDesign Phase — Metrics that measure how prepared systems and services\n\nare to cope with challenges.\n\nOperational Phase — Metrics that measure the difference in service levels\n\nbefore, during, and after a downtime event or service outage, which are",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 956,
      "content": "further qualified by availability, reliability, performance, and scalability\n\nmetrics.\n\nRecovery Phase — Metrics that measure the rate at which an IT resource\n\nrecovers from downtime, such as the meantime for a system to log an\n\noutage and switch over to a new virtual server.\n\nTwo common metrics related to measuring resiliency are as follows:\n\nMean-Time to Switchover (MTSO) Metric\n\nDescription — the time expected to complete a switchover from a severe\n\nfailure to a replicated instance in a different geographical area •\n\nMeasurement — (date/time of switchover completion — date/time of\n\nfailure) / total number of failures • Frequency — monthly, yearly\n\nCloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 10 minute average\n\nMean-Time System Recovery (MTSR) Metric\n\nDescription — time expected for a resilient system to perform a complete\n\nrecovery from a severe failure • Measurement — (date/time of recovery —\n\ndate/time of failure) / total number of failures • Frequency — monthly,\n\nyearly",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 957,
      "content": "Cloud Delivery Model — IaaS, PaaS, SaaS\n\nExample — 120 minute average\n\n18.2 Case Study Example\n\nAfter suffering a cloud outage that made their Web\n\nportal unavailable for about an hour, Innovartus decides\n\nto thoroughly review the terms and conditions of their\n\nSLA. They begin by researching the cloud provider’s\n\navailability guarantees, which prove to be ambiguous\n\nbecause they do not clearly state which events in the\n\ncloud provider’s SLA management system are classified\n\nas “downtime.” Innovartus also discovers that the SLA\n\nlacks reliability and resilience metrics, which had\n\nbecome essential to their cloud service operations.\n\nIn preparation for a renegotiation of the SLA terms with\n\nthe cloud provider, Innovartus decides to compile a list\n\nof additional requirements and guarantee stipulations: •\n\nThe availability rate needs to be described in greater\n\ndetail to enable more effective management of service\n\navailability conditions.\n\nTechnical data that supports service operations models\n\nneeds to be included in order to ensure that the operation",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 958,
      "content": "of select critical services remains fault-tolerant and\n\nresilient.\n\nAdditional metrics that assist in service quality\n\nassessment need to be included.\n\nAny events that are to be excluded from what is\n\nmeasured with availability metrics need to be clearly\n\ndefined.\n\nAfter several conversations with the cloud provider sales\n\nrepresentative, Innovartus is offered a revised SLA with\n\nthe following additions: • The method by which the\n\navailability of cloud services are to be measured, in\n\naddition to any supporting IT resources on which ATN\n\ncore processes depend.\n\nInclusion of a set of reliability and performance metrics\n\napproved by Innovartus.\n\nSix months later, Innovartus performs another SLA\n\nmetrics assessment and compares the newly generated\n\nvalues with ones that were generated prior to the SLA\n\nimprovements (Table 18.2).\n\nTable 18-2",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 959,
      "content": "The evolution of Innovartus’ SLA evaluation, as\n\nmonitored by their cloud resource administrators.\n\n18.3 SLA Guidelines\n\nThis section provides a number of best practices and recommendations for\n\nworking with SLAs, the majority of which are applicable to cloud\n\nconsumers: • Mapping Business Cases to SLAs — It can be helpful to\n\nidentify the necessary QoS requirements for a given automation solution\n\nand to then concretely link them to the guarantees expressed in the SLAs",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 960,
      "content": "for IT resources responsible for carrying out the automation. This can avoid\n\nsituations where SLAs are inadvertently misaligned or perhaps\n\nunreasonably deviate in their guarantees, subsequent to IT resource usage.\n\nWorking with Cloud and On-Premise SLAs — Due to the vast infrastructure\n\navailable to support IT resources in public clouds, the QoS guarantees\n\nissued in SLAs for cloud-based IT resources are generally superior to those\n\nprovided for on-premise IT resources. This variance needs to be understood,\n\nespecially when building hybrid distributed solutions that utilize both on-\n\npremise and cloud-based services or when incorporating cross-environment\n\ntechnology architectures, such as cloud bursting.\n\nUnderstanding the Scope of an SLA — Cloud environments are comprised\n\nof many supporting architectural and infrastructure layers upon which IT\n\nresources reside and are integrated. It is important to acknowledge the\n\nextent to which a given IT resource guarantee applies. For example, an SLA\n\nmay be limited to the IT resource implementation but not its underlying\n\nhosting environment.\n\nUnderstanding the Scope of SLA Monitoring — SLAs need to specify\n\nwhere monitoring is performed and where measurements are calculated,\n\nprimarily in relation to the cloud’s firewall. For example, monitoring within\n\nthe cloud firewall is not always advantageous or relevant to the cloud\n\nconsumer’s required QoS guarantees. Even the most efficient firewalls have",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 961,
      "content": "a measurable degree of influence on performance and can further present a\n\npoint of failure.\n\nDocumenting Guarantees at Appropriate Granularity — SLA templates\n\nused by cloud providers sometimes define guarantees in broad terms. If a\n\ncloud consumer has specific requirements, the corresponding level of detail\n\nshould be used to describe the guarantees. For example, if data replication\n\nneeds to take place across particular geographic locations, then these need\n\nto be specified directly within the SLA.\n\nDefining Penalties for Non-Compliance — If a cloud provider is unable to\n\nfollow through on the QoS guarantees promised within the SLAs, recourse\n\ncan be formally documented in terms of compensation, penalties,\n\nreimbursements, or otherwise.\n\nIncorporating Non-Measurable Requirements — Some guarantees cannot\n\nbe easily measured using service quality metrics, but are relevant to QoS\n\nnonetheless, and should therefore still be documented within the SLA. For\n\nexample, a cloud consumer may have specific security and privacy\n\nrequirements for data hosted by the cloud provider that can be addressed by\n\nassurances in the SLA for the cloud storage device being leased.\n\nDisclosure of Compliance Verification and Management — Cloud providers\n\nare often responsible for monitoring IT resources to ensure compliance with\n\ntheir own SLAs. In this case, the SLAs themselves should state what tools",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 962,
      "content": "and practices are being used to carry out the compliance checking process,\n\nin addition to any legal-related auditing that may be occurring.\n\nInclusion of Specific Metric Formulas — Some cloud providers do not\n\nmention common SLA metrics or the metrics-related calculations in their\n\nSLAs, instead focusing on service-level descriptions that highlight the use\n\nof best practices and customer support. Metrics being used to measure\n\nSLAs should be part of the SLA document, including the formulas and\n\ncalculations that the metrics are based upon.\n\nConsidering Independent SLA Monitoring — Although cloud providers will\n\noften have sophisticated SLA management systems and SLA monitors, it\n\nmay be in the best interest of a cloud consumer to hire a third-party\n\norganization to perform independent monitoring as well, especially if there\n\nare suspicions that SLA guarantees are not always being met by the cloud\n\nprovider (despite the results shown on periodically issued monitoring\n\nreports).\n\nArchiving SLA Data — The SLA-related statistics collected by SLA\n\nmonitors are commonly stored and archived by the cloud provider for future\n\nreporting purposes. If a cloud provider intends to keep SLA data specific to\n\na cloud consumer even after the cloud consumer no longer continues its\n\nbusiness relationship with the cloud provider, then this should be disclosed.\n\nThe cloud consumer may have data privacy requirements that disallow the\n\nunauthorized storage of this type of information. Similarly, during and after",
      "content_length": 1515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 963,
      "content": "a cloud consumer’s engagement with a cloud provider, it may want to keep\n\na copy of historical SLA-related data as well. It may be especially useful for\n\ncomparing cloud providers in the future.\n\nDisclosing Cross-Cloud Dependencies — Cloud providers may be leasing\n\nIT resources from other cloud providers, which results in a loss of control\n\nover the guarantees they are able to make to cloud consumers. Although a\n\ncloud provider will rely on the SLA assurances made to it by other cloud\n\nproviders, the cloud consumer may want disclosure of the fact that the IT\n\nresources it is leasing may have dependencies beyond the environment of\n\nthe cloud provider organization that it is leasing them from.\n\n18.4 Case Study Example\n\nDTGOV begins its SLA template authoring process by\n\nworking with a legal advisory team that has been\n\nadamant about an approach whereby cloud consumers\n\nare presented with an online Web page outlining the SLA\n\nguarantees, along with a “click-once-to-accept” button.\n\nThe default agreement contains extensive limitations to\n\nDTGOV’s liability in relation to possible SLA non-\n\ncompliance, as follows: • The SLA defines guarantees\n\nonly for service availability.",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 964,
      "content": "Service availability is defined for all of the cloud\n\nservices simultaneously.\n\nService availability metrics are loosely defined to\n\nestablish a level of flexibility regarding unexpected\n\noutages.\n\nThe terms and conditions are linked to the Cloud\n\nServices Customer Agreement, which is accepted\n\nimplicitly by all of the cloud consumers that use the self-\n\nservice portal.\n\nExtended periods of unavailability are to be\n\nrecompensed by monetary “service credits,” which are to\n\nbe discounted on future invoices and have no actual\n\nmonetary value.\n\nProvided here are key excerpts from DTGOV’s SLA\n\ntemplate:\n\nScope and Applicability\n\nThis Service Level Agreement (“SLA”) establishes the\n\nservice quality parameters that are to be applied to the\n\nuse of DTGOV’s cloud services (“DTGOV cloud”), and",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 965,
      "content": "is part of the DTGOV Cloud Services Customer\n\nAgreement (“DTGOV Cloud Agreement”).\n\nThe terms and conditions specified in this agreement\n\napply solely to virtual server and cloud storage device\n\nservices, herein called “Covered Services.” This SLA\n\napplies separately to each cloud consumer (“Consumer”)\n\nthat is using the DTGOV Cloud. DTGOV reserves the\n\nright to change the terms of this SLA in accordance with\n\nthe DTGOV Cloud Agreement at any time.\n\nService Quality Guarantees\n\nThe Covered Services will be operational and available\n\nto Consumers at least 99.95% of the time in any calendar\n\nmonth. If DTGOV does not meet this SLA requirement\n\nwhile the Consumer succeeds in meeting its SLA\n\nobligations, the Consumer will be eligible to receive\n\nFinancial Credits as compensation. This SLA states the\n\nConsumer’s exclusive right to compensation for any\n\nfailure on DTGOV’s part to fulfill the SLA\n\nrequirements.\n\nDefinitions",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 966,
      "content": "The following definitions are to be applied to DTGOV’s\n\nSLA:\n\n“Unavailability” is defined as the entirety of the\n\nConsumer’s running instances as having no external\n\nconnectivity for a duration that is at least five\n\nconsecutive minutes in length, during which the\n\nConsumer is unable to launch commands against the\n\nremote administration system through either the Web\n\napplication or Web service API.\n\n“Downtime Period” is defined as a period of five or\n\nmore consecutive minutes of the service remaining in a\n\nstate of Unavailability. Periods of “Intermittent\n\nDowntime” that are less than five minutes long do not\n\ncount towards Downtime Periods.\n\n“Monthly Up-time Percentage” (MUP) is calculated as:\n\n(total number of minutes in a month — total number of\n\ndowntime period minutes in a month) / (total number of\n\nminutes in a month) • “Financial Credit” is defined as\n\nthe percentage of the monthly invoice total that is\n\ncredited towards future monthly invoices of the\n\nConsumer, which is calculated as follows: 99.00% <\n\nMUP % < 99.95% — 10% of the monthly invoice is",
      "content_length": 1072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 967,
      "content": "credited in favor of the Consumer’s invoice 89.00% <\n\nMUP % < 99.00% — 30% of the monthly invoice is\n\ncredited in favor of the Consumer’s invoice MUP % <\n\n89.00% — 100% of the monthly invoice is credited in\n\nfavor of the -Consumer’s invoice Usage of Financial\n\nCredits\n\nThe MUP for each billing period is to be displayed on\n\neach monthly invoice. The Consumer is to submit a\n\nrequest for Financial Credit in order to be eligible to\n\nredeem Financial Credits. For that purpose, the\n\nConsumer is to notify DTGOV within thirty days from\n\nthe time the Consumer receives the invoice that states\n\nthe MUP beneath the defined SLA. Notification is to be\n\nsent to DTGOV via e-mail. Failure to comply with this\n\nrequirement forfeits the Consumer’s right to the\n\nredemption of Financial Credits.\n\nSLA Exclusions\n\nThe SLA does not apply to any of the following:\n\nUnavailability periods caused by factors that cannot be\n\nreasonably foreseen or prevented by DTGOV.",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 968,
      "content": "Unavailability periods resulting from the malfunctioning\n\nof the Consumer’s software and/or hardware, third party\n\nsoftware and/or hardware, or both.\n\nUnavailability periods resulting from abuse or\n\ndetrimental behavior and actions that are in violation of\n\nthe DTGOV Cloud Agreement.\n\nConsumers with overdue invoices or are otherwise not\n\nconsidered in good standing with DTGOV.",
      "content_length": 379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 969,
      "content": "Part V\n\nAppendices\n\nAppendix A: Case Study Conclusions\n\nAppendix B: Common Containerization Technologies",
      "content_length": 104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 970,
      "content": "Appendix A\n\nCase Study Conclusions\n\nA.1 ATN\n\nA.2 DTGOV\n\nA.3 Innovartus\n\nThis appendix briefly concludes the storylines of the three case studies that\n\nwere first introduced in Chapter 2.\n\nA.1 ATN\n\nThe cloud initiative necessitated migrating selected applications and IT\n\nservices to the cloud, allowing for the consolidation and retirement of\n\nsolutions in a crowded application portfolio. Not all of the applications\n\ncould be migrated, and selecting appropriate applications was a major issue.\n\nSome of the chosen applications required significant re-development effort\n\nto adapt to the new cloud environment.\n\nCosts were effectively reduced for most of the applications that were moved\n\nto the cloud. This was discovered after six months of expenditures were\n\ncompared with the costs of the traditional applications over a three year\n\nperiod. Both capital and operational expenses were used in the ROI\n\nevaluation.",
      "content_length": 917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 971,
      "content": "ATN’s level of service has improved in business areas that use cloud-based\n\napplications. In the past, most of these applications showed a noticeable\n\nperformance deterioration during peak usage periods. The cloud-based\n\napplications can now scale out whenever a peak workload arises.\n\nATN is currently evaluating other applications for potential cloud\n\nmigration.\n\nA.2 DTGOV\n\nAlthough DTGOV has been outsourcing IT resources for public sector\n\norganizations for more than 30 years, establishing the cloud and its\n\nassociated IT infrastructure was a major undertaking that took over two\n\nyears. DTGOV now offers IaaS services to the government sector and is\n\nbuilding a new cloud service portfolio that targets private sector\n\norganizations.\n\nDiversification of its client and service portfolios is the next logical step for\n\nDTGOV, after all of the changes they made to their technology architecture\n\nto produce a mature cloud. Before proceeding with this next phase,\n\nDTGOV produces a report to document aspects of its completed transition\n\nto cloud adoption. A summary of the report is documented in Table A.1.\n\nTable A.1\n\nThe results of an analysis of DTGOV’s cloud initiative.",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 976,
      "content": "A.3 Innovartus\n\nThe business objective of increasing company growth required the original\n\ncloud to undergo major modifications, since they needed to move from their\n\nregional cloud provider to a large-scale global cloud provider. Portability\n\nissues were discovered only after the move, and a new cloud provider\n\nprocurement process had to be created when the regional cloud provider\n\nwas unable to meet all of their needs. Data recovery, application migration,\n\nand interoperability issues were also addressed.\n\nHighly available computing IT resources and the pay-per-use feature were\n\nkey in developing Innovartus’ business feasibility, since access to funding\n\nand investment resources were not initially available.\n\nInnovartus has defined several business goals they plan to achieve over the\n\nnext couple of years:\n\nAdditional applications will be migrated to different clouds, using multiple\n\ncloud providers in order to improve resiliency and reduce dependency on\n\nindividual cloud provider vendors.\n\nA new mobile-only business area is to be created, since mobile access to\n\ntheir cloud services has experienced 20% growth.",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 977,
      "content": "The application platform developed by Innovartus is being evaluated as a\n\nvalue-added PaaS to be offered to companies that require enhanced and\n\ninnovative UI-centric features for both web-based and mobile application\n\ndevelopment.",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 978,
      "content": "Appendix B\n\nCommon Containerization Technologies\n\nB.1 Docker\n\nB.2 Kubernetes\n\nAs a supplement to Chapter 6, the appendix explores the Docker container\n\nengine and the Kubernetes containerization platform and further explains\n\nhow they are commonly utilized. This content helps illustrate how the\n\nterms, concepts and technologies described in Chapter 6 exist in real-world\n\nenvironments.\n\nNote the following:\n\nA Kubernetes platform needs to be deployed on a host cluster. The Docker\n\ncontainer engine needs to be separately deployed on each host in that\n\ncluster.\n\nBoth Docker and Kubernetes introduce distinct terminology. Wherever\n\napplicable, the terms established in Chapter 6 are referenced in the\n\nupcoming sections. Often, they are shown in parenthesis next to the\n\ncorresponding Docker or Kubernetes terms.\n\nB.1 Docker",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 979,
      "content": "Docker is the first containerization engine to have become widely popular\n\nin the industry. Docker containers, also known as Dockers, introduce many\n\nimportant benefits and features, which will be covered in the following\n\nsections.\n\nFrom an architecture perspective, a Docker container solution can be\n\ndivided into the following four key areas:\n\nDocker Server\n\nDocker Client\n\nDocker Registry\n\nDocker Objects\n\nDocker Server\n\nA Docker server, also known as a Docker host, is a host that is running a\n\nDocker containerization engine. From a technology architecture\n\nperspective, a Docker host is a physical server or virtual machine running a\n\nWindows or Linux operating system. A Docker server can be installed on\n\nany Windows or Linux machine supporting X86-64 or ARM and a few\n\nother CPU architectures. A Docker container engine can be installed on any\n\nsystem capable of running these operating systems.",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 980,
      "content": "Docker servers provide the following key features to a containerization\n\nsolution:\n\nDocker servers provide the key functional services required to run the\n\ncontainerization solution and to containerize applications. These services\n\nare provided by the Docker daemon, which is the containerization engine in\n\nthe Docker container solution. The Docker daemon has many different\n\ncomponents and subsystems designed and deployed as part of each version\n\nof the Docker software. It is responsible for scheduling, restarting and\n\nshutting down containers, as well as managing any container interaction.\n\nDocker servers host the containers that host the applications. Each container\n\nis deployed on a container host. A solution may have one or more Docker\n\nservers hosting containers and their applications.\n\nDocker servers also host the images used by the containers that they host,\n\nwhich allows different containers to use and share the same base image\n\nwithout the need to deploy multiple images for multiple containers.\n\nDocker Client\n\nA Docker client is a component that runs different tools which enable users\n\nand service consumers to interact with the Docker server and its services.\n\nDocker container solutions support the following two types of clients:\n\nApplication Programming Interface (API)",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 981,
      "content": "Command Line Interface (CLI)\n\nA Docker container does not provide a graphical user interface (GUI) for\n\ninteracting with or configuring services. This reduces its footprint because it\n\ndoes not require heavy GUIs to be rendered and made available for users to\n\ninteract with. This also results in less code required to be maintained and\n\nmanaged, which further reduces the security risk of the containerization\n\nsolution.\n\nAs shown in Figure B.1, the Docker client can be used to interact with the\n\nDocker server or Docker host in order to deploy, maintain and manage\n\ncontainers and their applications. All of the interactions shown in the\n\ndiagram occur through the use of the Docker daemon service, which acts as\n\nthe container engine in Docker solutions. The Docker daemon controls\n\naccess to the containers and provides a way for clients to interact with each\n\ncontainer.",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 982,
      "content": "Figure B.1\n\nThe Docker client uses an API or CLI to interact with containers via the\n\nDocker daemon service.\n\nThe Docker daemon service provides REST-based APIs that the Docker\n\nclient can consume as an API client. The purpose of these APIs is to provide",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 983,
      "content": "standard interfaces through which the service consumer can interact with\n\nthe Docker engine in order to deploy and manage their containers.\n\nA Docker client can be run on a variety of operating systems, including\n\nWindows, Linux and Mac.\n\nDocker Registry\n\nA Docker registry is a repository (image registry) that is used to store\n\ndifferent types of Docker images, which are used by the Docker host to\n\ndeploy containers (Figure B.2). The Docker registry is able to host and\n\ndeploy multiple container images, as well as different versions of the same\n\nimages. This allows application owners and system administrators to decide\n\nwhat container image or container image version to use when deploying\n\ncontainers and their applications.",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 984,
      "content": "Figure B.2\n\nThe Docker registry hosts the container images that can be used to deploy\n\nDocker containers.\n\nThe separation of the registry from the host and the Docker daemon service\n\n(container engine) has further made it possible for Docker containers to\n\nprovide a repository of different images without increasing any load or\n\nstorage footprint on the Docker host.\n\nDocker provides a public repository of different images based on standard\n\noperating systems, such as Windows and Linux. This public repository is\n\nalso known as Docker hub, and the images within the repository can be",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 985,
      "content": "used to deploy containers. If the standard Docker images provided by the\n\nDocker hub are being used, it is not necessary to deploy the Docker registry\n\nor allocate any additional storage to build a registry.\n\nDocker further allows users to have private Docker registries. For instance,\n\ndue to security requirements, Docker images may need to be kept internal to\n\nan organization and not accessible to anyone outside of the organization. In\n\nthis case, a private Docker registry can be deployed to house the Docker\n\nimages that will be configured and used for the containerization solution.\n\nDocker provides the following three key commands:\n\nDocker Push – used to add an image to the registry\n\nDocker Pull – used to download an image from the Docker registry in order\n\nto run a container\n\nDocker Run – used to run and start the container using a specific image\n\nDocker Objects\n\nA Docker container solution can have several different subcomponents and\n\nkey elements collectively referred to as Docker objects. This section\n\nintroduces the following key Docker objects:\n\nDocker Container – A Docker container is an instance of a container image\n\nand represents the actual container that will host the application. A",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 986,
      "content": "container can be stopped, started, deleted or scheduled to run at a specific\n\ntime.\n\nDocker Images – Images are read-only templates that are created and\n\ndeployed in the Docker registry and can be used to develop and run\n\ncontainers. For example, a base image can be created for the Linux\n\noperating system and can then be used to deploy several different\n\ncontainers. Each container can then make specific changes on top of the\n\nbase image to make the container environment suitable for different\n\napplications. However, the containers cannot modify the base image.\n\nServices – Docker containers introduce and use many different services to\n\nrun the Docker container solution, many of which are internal to the Docker\n\nengine and are not accessible for direct interaction. The most critical\n\nDocker services are the Docker daemon (container engine) and swarm\n\n(container orchestrator).\n\nNamespaces – In order to provide the capability of hosting several different\n\ncontainers on the same Docker host, the Docker containerization engine\n\nrequires a means of isolating containers from each other to provide a secure\n\nenvironment where multiple containers can be deployed. Docker uses a\n\ntechnology called namespaces to provide secure isolation of containers\n\nfrom each other. This further enables Docker containers to securely isolate\n\nthe processes in network interfaces and many other elements of containers\n\nfrom each other, even though they are hosted on the same Docker host.",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 987,
      "content": "Docker Control Groups – In order to ensure that Docker containers use\n\ncertain system resources to run and become operational and functional, the\n\nDocker host and Docker daemon need to enable the container to access the\n\nresources provided by the Docker host. This is established through the\n\ncontrol groups, which limit an application deployed inside a container to a\n\nspecific set of Docker host resources.\n\nUnion File System (container image layers) – Union file systems, also\n\nreferred to as unionFS, are the file systems that enable a Docker container\n\nengine to create lightweight and fast writeable layers on top of the base\n\ncontainer image in order to create a writable environment for applications\n\nand containers to make their own specific configurations, as required. This\n\nallows the container engine to operate as expected without the need to\n\nmodify the base image.\n\nDocker Orchestrator (container orchestrator) – Docker containers provide\n\ntheir own embedded orchestration components which can be used to\n\norchestrate the container solution in order to improve its productivity and\n\nautomate repeatable tasks that it is required to perform. The Docker\n\norchestrator is embedded in the Docker container engine and can be used\n\nby system administrators or application developers to orchestrate tasks.\n\nDocker Swarm (container orchestrator)\n\nAlthough deploying multiple different containers on the same Docker host\n\ncan introduce many benefits in terms of saving and sharing resources",
      "content_length": 1497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 988,
      "content": "among containers and applications, it can also introduce risk whereby if the\n\nhost is lost, the applications and containers hosting those applications will\n\nalso be lost. To prevent this issue, Docker containers can use Docker\n\nSwarm.\n\nDocker Swarm is a container orchestrator that is deployed as part of a\n\nDocker container solution. The Docker Swarm functionality is controlled by\n\nthe swarm service, a key Docker container service that is used to create and\n\nmanage a cluster of Docker hosts, also referred to as a swarm, in order to\n\nbalance the load across different physical hosts. It can also be used to\n\nimprove the availability of the Docker containers, allowing application\n\nowners to deploy multiple instances of the same container on different\n\nDocker hosts while the cluster is managed by Docker Swarm. Each cluster\n\nof Docker container hosts inside the swarm is managed by a service known\n\nas the cluster manager.\n\nFigure B.3 shows the logical architecture of Docker Swarm.",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 989,
      "content": "Figure B.3\n\nThe logical view of a swarm cluster comprised of three Docker hosts.\n\nDocker container solutions can be deployed on private systems in private\n\ndata centers or servers, or on different cloud platforms from public cloud\n\nproviders that provide containers as a service with the use of Docker,\n\nincluding Amazon Web Services, Microsoft Azure and Google Cloud.\n\nB.2 Kubernetes\n\nKubernetes, also known as K8s, is an open source container orchestrator\n\nthat provides key benefits and features that enhance Docker. Kubernetes\n\nintroduces the concept of clusters that can span several different hosts. This\n\nsystem takes the container functionality provided by a containerization\n\nengine like Docker to the next level by providing an enterprise-grade",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 990,
      "content": "architecture and construct that is more suitable for enterprise-grade\n\napplications and larger scale distributed systems, which is well-suited for\n\ncomplex applications. This section introduces the key components of a\n\nKubernetes solution.\n\nKubernetes Node (host)\n\nIn a Kubernetes architecture running Kubernetes software to host its\n\ncontainers, a Kubernetes node is the equivalent to the containerization host\n\nor Docker host discussed in the preceding Docker section. Each Kubernetes\n\ncluster (host cluster) can have one or more nodes.\n\nFigure B.4 shows a Kubernetes node, also known as a Kubernetes host, as\n\nthe key component of a Kubernetes solution.",
      "content_length": 656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 991,
      "content": "Figure B.4\n\nThe Kubernetes node is used to host containers, in this case two instances\n\nof Kubernetes pods.\n\nEach Kubernetes node (host) has three key components that enable\n\nKubernetes to host containers in pods, which will be explained later in this\n\nsection:\n\nKubelet\n\nKube-Proxy\n\nContainer Runtime",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 992,
      "content": "Kubernetes Pod\n\nA Kubernetes pod is a logical boundary or logical group of different\n\ncontainers that share storage and network resources on the same Kubernetes\n\nnode. The containers deployed in each pod also share the same\n\nconfiguration and specifications regarding how to run them. For example,\n\nevery container hosted inside a pod will always be hosted together on the\n\nsame Kubernetes node in the cluster.\n\nFigure B.5 shows a logical view of a pod and how the pod is used for the\n\nlogical separation of containers on the same host.\n\nFigure B.5\n\nPods can be used to logically group and isolate a set of containers from\n\nother containers.",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 993,
      "content": "Kubelet\n\nA kubelet is a service agent that is deployed on each node inside a cluster. It\n\nis responsible for ensuring that containers configured to run in each pod are\n\noperational and running as expected.\n\nKube-Proxy\n\nA kube-proxy is a service that runs in each Kubernetes node. It acts as a\n\nservice proxy that enables the containers deployed inside a pod to access\n\nthe network resources and further communicate with the external world.\n\nEach kube-proxy maintains network rules on nodes. These network rules\n\ncan be defined by system administrators and are used to allow internal and\n\nexternal network communication to each pod in a Kubernetes cluster.\n\nFigure B.6 shows the concept of the Kubernetes node and its kubelet and\n\nkube-proxy components.",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 994,
      "content": "Figure B.6\n\nThe Kubernetes cluster is comprised of two nodes: Kubernetes Node A and\n\nKubernetes Node B. Each node has its own kubelet and kube-proxy to serve\n\nits pods.\n\nContainer Runtime (container engine)\n\nIn a Kubernetes architecture, the container engine (referred to as the\n\ncontainer runtime) enables a solution to leverage the Kubernetes\n\ntechnology architecture and its features to deploy a variety of containers. In\n\naddition to supporting different container runtimes, Kubernetes also offers",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 995,
      "content": "its own, known as the container runtime interface (CRI). It is similar to a\n\nDocker container engine. As an alternative to using CRI to host containers,\n\na Docker container engine can be deployed to host Docker containers on\n\ntop of Kubernetes nodes (Figures B.7 and B.8).\n\nFigure B.7",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 996,
      "content": "A Kubernetes node hosting containers using CRI as a container engine.",
      "content_length": 69,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 998,
      "content": "Figure B.8\n\nA Kubernetes node running a Docker container engine at runtime to offer\n\ncontainers.\n\nCluster\n\nIn a Kubernetes architecture, a cluster is a group of nodes that work\n\ntogether to provide highly scalable and available solutions for deploying\n\ncontainers to host applications. As shown in Figure B.9, a cluster can\n\ncontain several different nodes.\n\nFigure B.9\n\nAn example of a Kubernetes cluster containing three different nodes.\n\nUnlike Docker containers, which introduce the concept of a swarm for\n\ngrouping multiple different Docker container hosts, Kubernetes introduces a\n\nmuch more comprehensive concept and technology architecture for creating\n\na cluster of containerization hosts that is suitable for enterprise applications.",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 999,
      "content": "Kubernetes Control Plane\n\nA control plane can be used in a Kubernetes cluster architecture in order to\n\noffer better services and more capabilities that enable application owners\n\nand system administrators to utilize a containerization solution to its full\n\npotential. The control plane is responsible for making decisions that are\n\napplicable to the entire cluster, granting system administrators or\n\napplication owners a common set of tools for managing the nodes in a\n\ncluster. This section introduces the key components of a control plane\n\nwithin a Kubernetes cluster.\n\nKubernetes API – The Kubernetes API provides a method for application\n\nowners, system administrators and developers to interact with the\n\nKubernetes architecture, its nodes and the containers deployed in each\n\nKubernetes cluster.\n\nkube-apiserver – The kube-apiserver exposes Kubernetes APIs to service\n\nconsumers so they can interact with the Kubernetes cluster and its\n\ncomponents, as well as the containers deployed inside the cluster, through\n\nthe API. The kube-apiserver is deployed as an independent component that\n\ncan be horizontally scaled to handle a greater volume of API calls and\n\nrequests from service consumers in order to accommodate the performance\n\nrequired by a solution as it scales.\n\netcd – The etcd service is used to store the configuration of the cluster data\n\ninside the control plane. This does not include any user data or application",
      "content_length": 1434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1000,
      "content": "data from the containerized application.\n\nkube-scheduler – The kube-scheduler is responsible for scheduling and\n\nrunning containers. Each time a new container is deployed, the kube-\n\nscheduler checks the resource utilization of the nodes inside the clusters, as\n\nwell as the different pods deployed on each node, in order to identify the\n\nbest place to schedule and run the new container.\n\nkube-controller-manager – The kube-controller-manager component is\n\nresponsible for running and managing the control plane processes. In the\n\ncontext of a Kubernetes solution, each of the above control plane\n\ncomponents will run as their own independent and separate process. The\n\nkube-controller-manager provides a simple way of managing all of the\n\nabove components and their associated processes from a central point of\n\nview, thus offering system administrators a way to manage the control plane\n\nof a Kubernetes solution.\n\ncloud-controller-manager – The cloud-controller-manager component\n\ncomes into play when a solution is deployed in a public cloud or any type of\n\ncloud that allows the cloud provider’s APIs to be accessed. For example, if\n\na Kubernetes solution is deployed on Amazon Web Services, Microsoft\n\nAzure or Google Cloud, then this component exposes those specific\n\nenvironments’ APIs to the cluster. However, if the solution is not deployed\n\nin a cloud environment, then this component is not required.",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1001,
      "content": "Figure B.10 shows an example of an overall Kubernetes deployment\n\narchitecture.\n\nFigure B.10",
      "content_length": 92,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1002,
      "content": "A Kubernetes cluster with two nodes and a control plane, including the\n\ncontrol plane’s components.\n\nAs illustrated in the previous figure, the cluster control plane is deployed on\n\na separate server than the Kubernetes nodes. This is done with the purpose\n\nof eliminating any interdependencies regarding the availability of the\n\ncomponents required for managing the cluster, as well as to ensure that the\n\ncontrol plane is not impacted if a node fails.",
      "content_length": 453,
      "extraction_method": "Unstructured"
    }
  ]
}