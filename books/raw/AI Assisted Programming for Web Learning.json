{
  "metadata": {
    "title": "AI Assisted Programming for Web Learning",
    "author": "Christoffer Noring",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 812,
    "conversion_date": "2025-12-19T17:16:55.439470",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "AI Assisted Programming for Web Learning.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-24)",
      "start_page": 2,
      "end_page": 24,
      "detection_method": "synthetic",
      "content": "AI-Assisted Programming for Web and Machine Learning\n\nCopyright © 2024 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a\n\nretrieval system, or transmitted in any form or by any means, without the\n\nprior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained\n\nin this book is sold without warranty, either express or implied. Neither the\n\nauthors, nor Packt Publishing or its dealers and distributors, will be held\n\nliable for any damages caused or alleged to have been caused directly or\n\nindirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate\n\nuse of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nSenior Publishing Product Manager: Bhavesh Amin\n\nAcquisition Editor – Peer Reviews: Gaurav Gavas\n\nProject Editor: Meenakshi Vijay\n\nContent Development Editor: Deepayan Bhattacharjee\n\nCopy Editor: Safis Editing\n\nTechnical Editor: Tejas Mhasvekar\n\nProofreader: Safis Editing\n\nIndexer: Tejal Soni\n\nPresentation Designer: Rajesh Shirsath\n\nDeveloper Relations Marketing Executive: Sohini Ghosh\n\nFirst published: August 2024\n\nProduction reference: 1270824\n\nPublished by Packt Publishing Ltd.\n\nGrosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-83508-605-6\n\nwww.packt.com\n\nContributors\n\nAbout the authors Christoffer Noring works as a Senior Advocate at Microsoft and focuses on application development and AI. He’s a Google Developer Expert and a\n\npublic speaker on 100+ presentations across the world. Additionally, he’s a\n\ntutor at the University of Oxford on cloud patterns and AI. Chris is also a\n\npublished author on Angular, NGRX, and programming with Go.\n\nAnjali Jain is a London-based AI and ML professional with a career\n\nspanning over two decades. Currently working as a data architect for\n\nMetrobank, she brings her expertise in AI, data, architecture, data\n\ngovernance, and software development to the financial sector. Anjali holds a bachelor’s degree in electrical engineering and boasts certifications,\n\nincluding TOGAF 9.1 and ITIL 2011 Foundation. In her role as Senior AI\n\nand ML tutor at Oxford, she shares cutting-edge knowledge on various technologies.\n\nMarina Fernandez is a data science and Databricks consultant with\n\nexpertise in financial risk management. She contributes to the academic\n\nteam at the University of Oxford, where she holds the positions of senior AI and ML tutor and guest lecturer. Throughout her 20-year career, Marina has\n\nworked on the development of large-scale enterprise systems for various\n\nbusiness domains. Her experience encompasses e-commerce, e-learning, software security, commodity trading, commodity trading and risk\n\nmanagement systems, and regulatory reporting. Marina obtained her MSc in\n\nSoftware Engineering from the University of Oxford. Additionally, she has earned professional certifications, including Microsoft Certified\n\nProfessional and Certified Scrum Master.\n\nAyşe Mutlu is a data scientist working on Azure AI and DevOps\n\ntechnologies. Based in London, Ayşe’s work involves building and deploying Machine Learning and Deep Learning models using the\n\nMicrosoft Azure framework (Azure DevOps and Azure Pipelines). She\n\nenjoys coding in Python and contributing to open-source initiatives in Python.\n\nAjit Jaokar is a data scientist for Feynlabs, building AI prototypes for\n\ncomplex applications. He is also a course director for AI at the University\n\nof Oxford. Besides this, Ajit is a visiting fellow in Engineering Sciences at the University of Oxford and conducts AI courses at the London School of\n\nEconomics, Universidad Politécnica de Madrid, and the Harvard Kennedy\n\nSchool of Government as part of The Future Society. His work at Oxford\n\nand his company is based on interdisciplinary aspects of AI, including AI with digital twins, quantum computing, metaverse, Agtech, and life\n\nsciences. His teaching is based on a methodology for AI and cyber-physical\n\nsystems, which he is developing as part of his research.\n\nAbout the reviewers Maxim Salnikov is a tech and cloud community enthusiast based in Oslo. With over two decades of experience as a web developer, he shares his\n\nextensive knowledge of the web platform, cloud computing, and AI by\n\nspeaking at and providing training for developer events worldwide. By day, Maxim plays a crucial role in supporting the development of cloud and AI\n\nsolutions within European companies, serving as the leader of developer\n\nproductivity business at Microsoft. During evenings, he can be found\n\nrunning events for Norway’s largest web and cloud development communities. Maxim is passionate about exploring and experimenting with\n\nGenerative AI possibilities, including AI-assisted development. To share his\n\ninsights and connect with like-minded professionals globally, he founded and organized the inaugural Prompt Engineering Conference, the first of its\n\nkind on a global scale.\n\nŞaban Kara is an AI and ML software engineer who graduated from Gebze\n\nTechnical University Electronics Engineering. Throughout his career, Şaban has developed several NLP projects and worked on various probabilistic\n\nstatistics-based ML algorithms. Şaban is especially known for his interest in\n\nLLM and LangChain models. His work on these models focuses on improving spontaneous learning abilities. He started his career working at\n\nTUBITAK. Currently, he is developing ML algorithms on LLM models in a\n\nprivate company and making a self-learning AI.\n\nI would like to thank my family, friends, and colleagues for their\n\ncontributions to the preparation of this book. Their support played an important role in the success of this project.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\nContents\n\nPreface\n\nWho this book is for What this book covers To get the most out of this book Get in touch\n\n1. It’s a New World, One with AI Assistants, and You’re Invited Introduction How ChatGPT came to be, from NLP to LLMs\n\nThe rise of LLMs GPT models How LLMs are better\n\nThe new paradigm, programming with natural language\n\nChallenges and limitations\n\nAbout this book Who this book is for Evolution of programming languages\n\nLooking ahead\n\nHow to use this book\n\n2. Prompt Strategy Introduction Where you are Guidelines for how to prompt efficiently Prompt techniques\n\nTask-Action-Guideline prompt pattern (TAG) Persona-Instruction-Context prompt pattern (PIC) Exploratory prompt pattern Learn-Improvise-Feedback-Evaluate prompt pattern (LIFE) Which pattern to choose?\n\nPrompt strategy for web development\n\nBreak down the problem: “web system for inventory management” Further breakdown of the frontend into features Generate prompts for each feature\n\nIdentify some basic principles for web development, a “prompt strategy”\n\nPrompt strategy for data science\n\nProblem breakdown: predict sales Further breakdown into features/steps for data science Generate prompts for each step Identify some basic principles for data science, “a prompt strategy for data science”\n\nValidate the solution\n\nVerification via prompts Classical verification\n\nSummary\n\n3. Tools of the Trade: Introducing Our AI Assistants Introduction Understanding Copilot\n\nHow Copilot knows what to generate Copilot capabilities and limits Setup and installation Installing Copilot Getting started with Copilot Assignment: improve the code Solution Challenge References\n\nUnderstanding ChatGPT\n\nHow does ChatGPT work? ChatGPT capabilities and limits Setup and installation Getting started with ChatGPT\n\nPrompting\n\nSummary\n\n4. Build the Appearance of Our App with HTML and Copilot Introduction Business problem: e-commerce\n\nProblem domain Problem breakdown: identify the features Prompt strategy\n\nPage structure Add AI assistance to our page structure\n\nYour first prompt, simple prompting, and aiding your AI assistant Your second prompt: adding more context Your third prompt: accept prompt suggestions\n\nChallenge: vary the prompt Use case: build a front for an e-commerce\n\nLogin page Product list page Remaining pages\n\nAssignment Challenge Quiz Summary\n\n5. Style the App with CSS and Copilot\n\nIntroduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy CSS, or Cascading Style Sheets\n\nFirst CSS CSS by name\n\nAssignment Solution Use case: style the e-commerce app\n\nBasket page\n\nChallenge Quiz Summary\n\n6. Add Behavior with JavaScript\n\nIntroduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy\n\nAdding JavaScript\n\nThe role of JavaScript Adding JavaScript to a page A second example: adding a JavaScript library/framework\n\nChallenge Use case: adding behavior Improving the output Adding Bootstrap Adding Vue.js\n\nAssignment Solution Summary\n\n7. Support Multiple Viewports Using Responsive Web Layouts Introduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy Viewports\n\nMedia queries When to adjust to different viewports and make it responsive\n\nUse case: make our product gallery responsive Assignment Solution\n\nChallenge Summary 8. Build a Backend with Web APIs\n\nIntroduction Business domain: e-commerce Problem and data domain Feature breakdown Prompt strategy Web APIs\n\nWhat language and framework should you pick? Planning the Web API\n\nCreating a Web API with Python and Flask\n\nStep 1: Create a new project\n\nStep 2: Install Flask Step 3: Create an entry point Step 4: Create a Flask app\n\nUse case: a Web API for an e-commerce site\n\nStep 1: Create a Web API for an e-commerce site Step 2: Return JSON instead of text Step 3: Add code to read and write to a database Step 4: Improve the code Run the code Refactor the code Step 5: Document the API\n\nAssignment Solution Challenge Summary\n\n9. Augment Web Apps with AI Services Introduction Business domain, e-commerce Problem and data domain Feature breakdown Prompt strategy Creating a model Coming up with a plan Importing libraries Reading the CSV file Creating test and training datasets Creating a model How good is the model? Predict Saving the model to a .pkl file Creating a REST API in Python\n\nConverting the model to ONNX\n\nCreating a model in ONNX format Loading the ONNX model in JavaScript Installing onnxruntime in JavaScript Loading the ONNX model in JavaScript\n\nAssignment: Build a REST API in JavaScript that consumes the model Solution Quiz Summary\n\n10. Maintaining Existing Codebases Introduction Prompt strategy Different types of maintenance The maintenance process Addressing a bug\n\n1. Identify the problem 2. Implement the change\n\nAdding a new feature\n\n1. Identify a problem and find the function/s to change 2. Implement change, and add a new feature and tests\n\nImproving performance\n\nBig O notation calculation Measuring performance\n\nImproving maintainability\n\n1. Identify the problems. What problems do you see? 2. Add tests and de-risk change 3. Implement change and improve maintainability\n\nChallenge Updating an existing e-commerce site Assignment Knowledge check Summary\n\n11. Data Exploration with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nFeature breakdown Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy\n\nStrategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nData exploration of the Amazon review dataset using the free version of ChatGPT\n\nFeature 1: Loading the dataset Feature 2: Inspecting the data Feature 3: Summary statistics Feature 4: Exploring categorical variables Feature 5: Rating distribution Feature 6: Temporal trends Feature 7: Review length analysis Feature 8: Correlation study\n\nData exploration of the Amazon review dataset using ChatGPT-4o Assignment Challenge Summary\n\n12. Building a Classification Model with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews using the free version of ChatGPT\n\nFeature 1: Data preprocessing and feature engineering Feature 2: Model selection and baseline training Feature 3: Model evaluation and interpretation Feature 4: Handling imbalanced data Feature 5: Hyperparameter tuning\n\nFeature 6: Experimenting with feature representation Building a sentiment analysis model to accurately classify Amazon reviews using ChatGPT-4 or ChatGPT Plus\n\nFeature 1: Data preprocessing and feature engineering Feature 2: Model selection and baseline training Feature 3: Model evaluation and interpretation Feature 4: Handling data imbalance Feature 5: Hyperparameter tuning Feature 6: Experimenting with feature representation\n\nAssignment Challenge Summary\n\n13. Building a Regression Model for Customer Spend with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a simple linear regression model to predict the “Yearly Amount Spent” by customers using the free version of ChatGPT\n\nFeature 1: Building the model step by step Feature 2: Applying regularization techniques Feature 3: Generating a synthetic dataset to add complexity Feature 4: Generating code to develop a model in a single step for a synthetic dataset\n\nLearning simple linear regression using ChatGPT Plus\n\nFeature 1: Building a simple linear regression model step by step Feature 2: Applying regularization techniques Feature 3: Generating a synthetic dataset to add complexity\n\nFeature 4: Generating code to develop a model in a single step for a synthetic dataset\n\nAssignment Challenge Summary\n\n14. Building an MLP Model for Fashion-MNIST with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding an MLP model to accurately classify the Fashion- MNIST images using the free version of ChatGPT Feature 1: Building the baseline model Feature 2: Adding layers to the model Feature 3: Experimenting with batch sizes Feature 4: Experimenting with the number of neurons Feature 5: Trying different optimizers\n\nAssignment Challenge Summary\n\n15. Building a CNN Model for CIFAR-10 with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy\n\nStrategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a CNN model to accurately classify the CIFAR-10 images using the free version of ChatGPT\n\nFeature 1: Building the baseline CNN model with a single convolutional layer Feature 2: Experimenting with the addition of convolutional layers Feature 3: Incorporating dropout regularization Feature 4: Implementing batch normalization Feature 5: Optimizing with different optimizers Feature 6: Applying the DavidNet architecture\n\nAssignment Challenge Summary\n\n16. Unsupervised Learning: Clustering and PCA\n\nIntroduction Breaking the problem down into features Prompt strategy Customer segmentation The dataset Adding AI assistance to the unsupervised learning model development process Load the dataset Inspect the data Summary statistics Preprocessing the data Feature engineering Checking for outliers Removing outliers Data scaling using standardization Deciding on the number of clusters Creating a clustering model Visualize clustering results Final thoughts on clustering and the prompting process\n\nProduct clustering for an e-commerce project\n\nYour initial prompt: Set context\n\nLoad and preprocess the data Feature engineering and text data preprocessing\n\nFeature engineering Choose clustering algorithm Feature scaling Apply clustering algorithm Interpret clusters and visualize results\n\nInterpreting cluster Visualizing clusters Creating a word cloud\n\nAssigning categories to products and evaluating and refining\n\nEvaluate and refine Reflection on prompts for this use case Assignment Solution Summary\n\n17. Machine Learning with Copilot Introduction GitHub Copilot Chat in your IDE\n\nHow it works Dataset overview Steps for data exploration Prompt strategy Your initial data exploration prompt: Prompt 1, setting the high- level context Step 1: Load the dataset\n\nRunning the code for loading data\n\nStep 2: Inspect the data Step 3: Summary statistics Step 4: Explore categorical variables Step 5: Distribution of ratings Step 6: Temporal analysis Step 7: Review length analysis Step 8: Correlation analysis Step 9: Additional exploratory analysis Step 10: Text Preprocessing Step 11: Word Frequency Analysis\n\nStep 12: Sentiment Score Calculation\n\nText preprocessing Word frequency analysis Sentiment score calculation\n\nStep 13: Visualize the Distribution of Sentiment Scores Step 14: Analyze the Relationship Between Sentiment Score and Other Variables\n\nVisualize the distribution of sentiment scores Analyze the relationship between sentiment score and other variables\n\nAssignment Solution Summary\n\n18. Regression with Copilot Chat Introduction Regression Dataset overview\n\nExplore the dataset\n\nPrompt strategy\n\nYour initial prompt Exploratory data analysis Data splitting Build a regression model\n\nEvaluate the model\n\nEvaluation metrics\n\nAssignment Summary\n\n19. Regression with Copilot Suggestions Introduction Dataset overview Prompt strategy Start coding with Copilot’s help\n\nStep 1: Import libraries with Copilot’s assistance Step 2: Load and explore the dataset\n\nGet types and columns Shape of the dataset Addressing the column types\n\nStatistical summary Check for missing values Check for duplicates Scale numerical features Visualization\n\nStep 3: Split data into training and testing sets\n\nAsking questions\n\nStep 4: Build a regression problem Step 5: Train the model Step 6: Evaluate model performance\n\nAssignment Summary\n\n20. Increasing Efficiency with GitHub Copilot Introduction Code generation and automation Copilot’s active editor Copilot Chat Copilot commands\n\nCreating a Notebook Creating a project\n\nDebugging and troubleshooting Code review and optimization techniques Workspace Visual Studio Code lookup Terminal Assignment Challenge Quiz Summary\n\n21. Agents in Software Development Introduction What are agents?\n\nHow do agents work?\n\nSimpler agents versus agents using AI Simpler agents\n\nA simple agent is not a great conversationalist\n\nImproved conversation with tool calling and large language models (LLMs) The anatomy of a conversational agent More on tool calling in LLMs Adding capabilities to GPT using tools\n\nAdvanced conversations\n\nModeling advanced conversations Pseudo code for advanced conversations\n\nAutonomous agents Assignment Challenge Quiz Summary References\n\n22. Conclusion\n\nRecap of the book Major conclusions What’s next At last\n\nOther Books You May Enjoy Index\n\nOceanofPDF.com\n\nPreface\n\nWho this book is for The target audience for this book is professionals in the web development,\n\nmachine learning, and data science fields. You should be a professional with\n\nat least 1-3 years of experience. This book means to empower you by\n\nshowcasing how AI assistants can be leveraged in different problem domains. It describes overall features but also gives recommendations on\n\neffective prompting for best results.\n\nWhat this book covers Chapter 1, It’s a New World, One with AI Assistants, and You’re Invited,\n\nlooks at how we started using large language models and how it constitutes a paradigm shift for many, not just IT workers.\n\nChapter 2, Prompt Strategy, explains the strategy used throughout the book in terms of breaking down a problem and some guiding principles on how\n\nto effectively prompt your chosen AI tool.\n\nChapter 3, Tools of the Trade: Introducing Our AI Assistants, is where we\n\nexplain how to work with our two chosen AI assistants, GitHub Copilot and ChatGPT, covering everything from installation to how to get started using\n\nthem.\n\nChapter 4, Build the Appearance of Our App with HTML and Copilot,\n\nfocuses on building the frontend for our e-commerce app (a narrative you will see featured throughout the book).\n\nChapter 5, Style the App with CSS and Copilot, is where we keep working\n\non our e-commerce app but now focus specifically on CSS and ensuring the appearance is appealing.\n\nChapter 6, Add Behaviour with JavaScript, is where we add behavior to our e-commerce app using JavaScript.\n\nChapter 7, Support Multiple Viewports Using Responsive Web Layouts, is where we address the fact that an app needs to work for different device\n\ntypes, whether it’s a smaller mobile screen, a tablet, or a desktop screen. Therefore, this chapter focuses on responsive design.",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 25-46)",
      "start_page": 25,
      "end_page": 46,
      "detection_method": "synthetic",
      "content": "Chapter 8, Build a Backend with Web APIs, looks at how, for the app to\n\nactually work, it needs to have a backend, consisting of code that’s able to read and write data and persist it. This chapter therefore focuses on building a Web API for our e-commerce app.\n\nChapter 9, Augment Web apps with AI Services, covers training a machine learning model and how to expose it via a Web API for consumption by\n\nanyone with a browser or other type of client capable of using the HTTP\n\nprotocol.\n\nChapter 10, Maintaining Existing Codebases, covers how most developers work on existing code and maintain existing codebases rather than creating\n\nnew projects. Therefore, this chapter focuses on various aspects of\n\nmaintaining code, like dealing with bugs, performance, working with tests, and more.\n\nChapter 11, Data Exploration with ChatGPT, is where we work with a\n\nreview dataset and learn to identify insights into distribution, trends,\n\ncorrelation, and more.\n\nChapter 12, Building a Classification Model with ChatGPT, looks at the same review dataset as in Chapter 11, this time performing classification\n\nand sentiment analysis.\n\nChapter 13, Building a Regression Model for Customer Spend with\n\nChatGPT, attempts to predict the yearly amount spent by customers and uses regression to create a model capable of making this prediction.\n\nChapter 14, Building an MLP Model for Fashion-MNIST with ChatGPT,\n\nlooks at building an MLP model based on a fashion dataset, still sticking to\n\nour general theme of e-commerce.\n\nChapter 15, Building a CNN Model for CIFAR-10 with ChatGPT, focuses on building a CNN model.\n\nChapter 16, Unsupervised Learning: Clustering and PCA, focuses on\n\nclustering and PCA.\n\nChapter 17, Machine Learning with Copilot, covers conducting machine\n\nlearning using GitHub Copilot to contrast it with ChatGPT.\n\nChapter 18, Regression with Copilot Chat, is where we develop a regression model. Also, this chapter uses GitHub Copilot.\n\nChapter 19, Regression with Copilot Suggestions, like the preceding\n\nchapter, focuses on regression using GitHub Copilot. The difference\n\nbetween this and the preceding chapter is that here we use the suggestions from writing prompts as comments in a text file, rather than writing our\n\nprompt in a chat-like interface.\n\nChapter 20, Increasing Efficiency with GitHub Copilot, focuses on getting\n\nthe most out of GitHub Copilot. This chapter is a must read if you want to master GitHub Copilot.\n\nChapter 21, Agents in Software Development, takes a look at what’s coming\n\nnext within AI, namely, agents. Agents are able to assist you to a much\n\nhigher degree by acting autonomously based on a high-level goal. This is definitely worth a read if you’re curious about future trends.\n\nChapter 22, Conclusion, wraps up the book by drawing some conclusions\n\nas to the greater lessons learned about working with AI assistants.\n\nTo get the most out of this book\n\nYou’ll get more out of this book if you’ve built a few projects in each domain as opposed to being a complete beginner. Therefore, the book\n\nfocuses on empowering you in your existing development workflows. We\n\nrecommend other titles by Packt if you are completely new to web development or machine learning. See the below list for recommendations:\n\nhttps://www.packtpub.com/en-us/product/html5-\n\nweb-application-development-by-example-\n\nbeginners-guide-9781849695947\n\nMachine Learning with Python: Unlocking AI Potential with Python and Machine Learning by Oliver Theobald (https://www.packtpub.com/en-\n\nUS/product/machine-learning-with-python- 9781835461969)\n\nThe book is built in such a way that you’re shown the prompts you’re recommended to write followed by the results from the chosen AI tool.\n\nTo follow along with the chapters on web development, we\n\nrecommend installing Visual Studio Code. There are dedicated\n\nchapters in the book pointing out how to install GitHub Copilot and leverage it. See the installation instructions for Visual Studio Code here: https://code.visualstudio.com/download\n\nFor the machine learning chapters, the majority of those chapters use ChatGPT, which can be accessed through a web browser. We do\n\nrecommend solving those problems using notebooks, which can be\n\nviewed through a variety of different tools. For more detailed\n\ninstructions on Notebook setup, refer to this page:\n\nhttps://code.visualstudio.com/docs/datascience/\n\njupyter-notebooks\n\nTo use GitHub Copilot, you need a GitHub account to log in to. Refer to this page on the setup process for GitHub Copilot:\n\nhttps://docs.github.com/en/copilot/quickstart\n\nDownload the example code ﬁles The code bundle for the book is hosted on GitHub at\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and- ChatGPT. We also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing/. Check them out!\n\nDownload the color images We also provide a PDF file that has color images of the\n\nscreenshots/diagrams used in this book. You can download it here: https://packt.link/gbp/9781835086056.\n\nConventions used There are a number of text conventions used throughout this book.\n\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input,\n\nand X(formerly known as Twitter) handles. For example: “Now that the product.css is created with the above content, we can include said CSS file\n\nin an HTML file.”\n\nBold: Indicates a new term, an important word, or words that you see on the\n\nscreen. For instance, words in menus or dialog boxes appear in the text like\n\nthis. For example: “Create new user: It should be possible to create a new user.”\n\nWarnings or important notes appear like this.\n\nTips and tricks appear like this.\n\nGet in touch Feedback from our readers is always welcome.\n\nGeneral feedback: Email feedback@packtpub.com and mention the book’s\n\ntitle in the subject of your message. If you have questions about any aspect of this book, please email us at questions@packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of our\n\ncontent, mistakes do happen. If you have found a mistake in this book, we\n\nwould be grateful if you reported this to us. Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on\n\nthe internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packtpub.com with\n\na link to the material.\n\nIf you are interested in becoming an author: If there is a topic that you\n\nhave expertise in and you are interested in either writing or contributing to a\n\nbook, please visit http://authors.packtpub.com.\n\nShare your thoughts Once you’ve read AI-Assisted Programming for Web and Machine Learning, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us\n\nmake sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book Thanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books\n\neverywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version\n\nof that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781835086056\n\n2. Submit your proof of purchase.\n\n3. That’s it! We’ll send your free PDF and other benefits to your email\n\ndirectly.\n\nOceanofPDF.com\n\n1\n\nIt’s a New World, One with AI Assistants, and You’re Invited\n\nIntroduction In November 2022, ChatGPT arrived from seemingly nowhere. Over time,\n\nChatGPT gained momentum, gradually evolving into a widely embraced\n\ntool. Eventually, millions actively incorporated ChatGPT into their\n\nworkflows, leveraging its capabilities for generating insights, summarizing\n\ntext, crafting code, and more.\n\nIts arrival changed many people’s workflow and improved it a lot in tasks\n\nlike quickly understanding large bodies of text, writing emails, and more.\n\nHere you are, having bought this book, and hoping that you can learn how\n\nto use an AI tool like ChatGPT or GitHub Copilot to make you more\n\nefficient. That’s exactly the mission of this book: to teach you not only how to use these two AI tools but also to be able to apply them across various\n\nproblem domains.\n\nBefore we start solving problems using an AI assistant, let’s back up a bit;\n\nhow did we get here? ChatGPT just didn’t arrive out of nowhere, right?\n\nHow ChatGPT came to be, from NLP to LLMs To tell the story of how we got here, to AI tools like ChatGPT, powered by\n\nlarge language models (LLMs), let’s first cover natural language\n\nprocessing (NLP).\n\nNLP is a field of computer science, artificial intelligence, and\n\ncomputational linguistics. It’s concerned with the interactions between\n\ncomputers and human language, and how to program computers to process\n\nand analyze large amounts of natural language data. NLP is a hugely\n\ninteresting area that has a range of useful applications in the real world. Here are some:\n\nSpeech recognition: If you have a modern smartphone, you’ve likely\n\ninteracted with voice assistants like Siri or Alexa, for example.\n\nMachine translation: Google Translate is perhaps what comes to mind when thinking of machine translation, the ability to translate\n\nfrom one language to another automatically.\n\nSentiment analysis: A very useful area is understanding the sentiment\n\nin areas like social media, for example. Companies want to know how brands are perceived; e-commerce wants to quickly understand\n\nproduct reviews to boost their business.\n\nChatbots and virtual assistants: You’ve likely seen chatbots being\n\nintegrated on web pages even before the advent of ChatGPT. These chatbots can answer simpler questions, and companies have them to\n\nensure you quickly get an answer to simpler questions and provide a\n\nmore natural experience than an FAQ page, among other usage areas.\n\nText summaries: Search engines come to mind again when thinking\n\nabout text summaries. You might have seen how, when you use search engines like Bing or Google, it’s able to summarize a page and show the summary together with the link to the page in a search result page.\n\nAs a user, you get a better understanding of what link to click.\n\nContent recommendation: This is another important area used by a\n\nvariety of different domains. E-commerce uses this to present products\n\nyou’re likely to be interested in, Xbox uses this to recommend what\n\ngames to play and buy, and video streaming services display content you might want to watch next.\n\nAs you can see already, with NLP, both companies and end users benefit greatly from adopting it.\n\nThe rise of LLMs How did we evolve from NLP to LLMs, then? Initially, NLP used rule-\n\nbased systems and statistical methods underneath. This approach, although working well for some tasks, struggled with human language.\n\nThis changed for the better when deep learning, a subset of machine learning, was introduced to NLP, and we got models like RNN, recurrent\n\nneural networks, and transformer-based models, capable of learning patterns in data. The result was a considerable improvement in\n\nperformance. With transformer-based models, we’re starting to lay the foundations of large language models.\n\nLLMs are a type of transformer model. They can generate human-like text and, unlike NLP models, they’re good at a variety of tasks without needing\n\nspecific training data. How is this possible, you ask? The answer is a\n\ncombination of improved architecture, a vast increase in computational\n\npower, and gigantic datasets.\n\nLLMs rest on the idea that a large enough neural network can learn to do anything, given enough data and compute. This is a paradigm shift in how\n\nwe program computers. Instead of writing code, we write prompts and let the model do the rest.\n\nGPT models There are many different types of LLMs out there, but let’s focus on GPT\n\nfor a second, a type of LLM on which the book’s chosen tools are based (even if GitHub Copilot uses a specific subset known as Codex).\n\nThere have been several different versions developed in the last few years.\n\nHere are some models developed by the company OpenAI:\n\nGPT-1: The first one, with 117 million parameters using transformer\n\narchitecture.\n\nGPT-2: This model has 1.5 billion parameters and is able to generate\n\ncoherent and relevant text.\n\nGPT-3: This model has 175 billion parameters and is considerably\n\nbetter than its predecessor with features like answering questions, fiction generation, and even writing code.\n\nGPT-4: This model has been quoted to have 1.76 trillion parameters.\n\nThe number of parameters allows the model to understand more\n\nnuanced and coherent text. It should also be said that the larger the model, the larger the computational resources that are needed to train\n\nit.\n\nChatGPT recently switched to GPT-4 and the difference compared to\n\nGPT-3 is significant.\n\nHow LLMs are better Now that we have a better understanding of how LLMs came to be and\n\nwhere they came from, what makes LLMs great? What are some good examples of why we really should adopt AI assistants based on LLMs?\n\nBecause LLMs are bigger and more advanced, there are some areas in\n\nwhich they clearly outperform traditional NLP models:\n\nContext: LLMs can understand not just the recent input but can\n\nproduce responses based on a longer conversation.\n\nFew-shot learning: To perform a task, LLMs usually just need a few\n\nexamples to produce a correct response. This should be contrasted with\n\nNLP models, which usually use a large amount of task-specific\n\ntraining data to perform properly.\n\nPerformance: LLMs are better than traditional NLP models in areas\n\nlike translations, questions, and summarization.\n\nIt’s worth mentioning that LLMs aren’t perfect; they do generate incorrect\n\nresponses and can sometimes make up responses, also known as hallucinations. It’s our hope though that by reading this book, you will see\n\nthe advantages of using LLM-based AI assistants and you will feel the pros\n\nclearly outweigh the cons.\n\nThe new paradigm, programming with natural\n\nlanguage Probably the biggest game changer with using LLM-based AI assistants is\n\nthat you’re able to interact with them using nothing but natural language.\n\nThere’s no need to learn a programming language to get the response you need. This change constitutes a new paradigm in interacting with AI. We’re\n\nmoving away from writing in specific languages for producing apps, data\n\nretrieval, or even how we produce images, presentations, and more to\n\nexpress at a high level what we want through a prompt.\n\nHere is an example of things that are now possible to do using prompts,\n\nwhere it before needed considerably more effort:\n\nProgramming: With a prompt, you express what app you want to\n\nbuild or what changes you want to make with the code.\n\nImage generation: Where you before needed a designer or artist, you can now generate via prompts.\n\nVideos: There are tools out there that, once given a prompt, will\n\ngenerate videos where an avatar reads out your written text.\n\nText tasks: LLM-based AI assistants can generate emails, summarize\n\nlarge bodies of text, author interview ads, and much more; anything\n\nyou can imagine with text really.\n\nAll these application areas mentioned above make it clear that LLM-based AI tools are useful not only to programmers and data scientists but\n\nnumerous different professions.\n\nChallenges and limitations Is everything working perfectly at this point? AI assistants aren’t able to\n\nreplace “you” just yet, and should be considered more of a “thinking\n\npartner.” Microsoft has even, through conscious naming, called their AI assistants “Copilots” where you’re clearly the pilot that sets out the\n\ndirection. These tools can generate text and other modalities in seconds, but\n\nyou need to verify the correctness. Often, the first response you get from a\n\ntool is something you need to iterate over. The good news is that it just takes seconds to redo the instruction.\n\nAn important thing to realize about AI assistants is that the more skilled you\n\nare at a certain topic, the more intelligent questions you can ask of it, and\n\nyou’ll be able to better assess the correctness of the response.\n\nAbout this book The goals of this book are to:\n\nIntroduce you to the new paradigm of programming with natural\n\nlanguage.\n\nProvide you with the tools to get started using AI assistants.\n\nEmpower you to use AI assistants effectively and responsibly by\n\nteaching you prompt engineering and specifically a set of prompting strategies (covered in Chapter 2) and some sound practices (covered in\n\nChapter 8).\n\nWe believe that with these tools, prompting strategies, and practices, you\n\nwill be able to use AI assistants effectively and responsibly to augment your work and increase your productivity.\n\nWho this book is for This book is for professional developers within both the web and machine\n\nlearning space. It is for those who want to learn how to use AI assistants\n\nlike GitHub Copilot and ChatGPT to augment their work and increase their\n\nproductivity.\n\nEvolution of programming languages Programming has gone through a series of changes and paradigm shifts\n\nthroughout history:\n\nAda Lovelace wrote the first algorithm for a machine, the Analytical Engine, in the 1840s. Lovelace is considered the first computer\n\nprogrammer and the first to recognize that the machine had\n\napplications beyond pure calculation.\n\nIn the 1940s, the first programmable computers were created. These computers were programmed using punch cards. One such computer\n\nwas the Harvard Mark I, which was used to calculate the trajectory of\n\nartillery shells. Also, Bombe is worth mentioning, which was used to\n\ncrack the Enigma code during World War II and was instrumental in\n\nthe Allies winning the war.\n\nIn the 1950s, the first high-level programming languages were created.\n\nThis time period saw the birth of FORTRAN, LISP, COBOL, and\n\nALGOL. Some of these languages are still in use today, especially in\n\nbanking systems, scientific computing, and defense.\n\nIn the 1970s, the first object-oriented programming languages were\n\ncreated. The 1970s meant we got Smalltalk, C++, and Objective-C.\n\nExcept for Smalltalk, these languages are heavily in use today.\n\nIn the 1990s, the first functional programming languages were created.\n\nThe 1990s gave us Haskell, OCaml, and Scala. The benefit of these\n\nlanguages is that they encourage immutability and pure functions,\n\nwhich makes them easier to reason about and test.\n\nIn the 2000s, the first declarative programming languages were\n\ncreated. Declarative programming languages are used to describe what\n\nyou want to do, rather than how you want to do it. The 2000s gave us SQL, HTML, and CSS.\n\nIn the 2010s, the first low-code and no-code platforms were created.\n\nThese platforms opened programming to a wider audience, and\n\nallowed anyone, regardless of technical background, to build applications.\n\nIn the 2020s, the first AI assistants were created that leveraged natural\n\nlanguage. If you can write a sentence, you can write code.\n\nIn summary, programming has gone through a series of changes and paradigm shifts. Prompt-first programming is the latest paradigm shift and\n\nmastering it will be key to staying relevant in the immediate future.\n\nLooking ahead If changes and paradigm shifts took years or decades in the past, they now\n\ntake months or even weeks. We’re moving toward a new world at\n\nbreakneck speed.\n\nThere’s reason to be excited, as we’re moving faster than before, but as\n\nalways, we should exercise caution. We should be aware of the risks and the\n\ndangers of using these tools irresponsibly, but most of all we should be\n\naware of the opportunities.\n\nAs Alan Kay once said, “The best way to predict the future is to invent it.”\n\nHow to use this book We believe the best way to use this book is to follow the chapters in order.\n\nChapter 2, with the prompting strategies, is the most important chapter in\n\nthe book. These patterns and strategies are referred to throughout the book\n\nand are the foundation for how to use AI assistants effectively and\n\nresponsibly.\n\nThe book is written in the following format:\n\nIntroduction: The first chapter aims to provide you with an overview of what this book is about, its goals, and who it is for.\n\nPrompt strategy: The idea is to lay the foundation on how to break\n\ndown problems within the domains of data science and web development. From this chapter, you will learn strategies you can\n\nadopt for your own problems.\n\nTools of the trade: The third chapter introduces you to our tools,\n\nGitHub Copilot and ChatGPT, what they are, how they work, and how to install them. However, the book is written in such a way that you\n\ncan take any of the prompts we suggest and feed those into any AI\n\nassistant, and get a similar experience.\n\nThe remaining chapters of the book show how we use the prompt\n\nstrategies from Chapter 2 and apply them to various domains from\n\nweb development to data science and machine learning.\n\nHappy reading!\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n2\n\nPrompt Strategy\n\nIntroduction In the previous chapter, we gave some historical context to how AI has\n\ndeveloped over the years, how we’ve gone from natural language\n\nprocessing (NLP) to large language models (LLMs), and how the latter\n\nserves as the underlying machine learning model in AI assistants. To use\n\nthese AI assistants, you use natural language prompts as input. However, to\n\nensure you “prompt” in an efficient way, so that you get what you want, it’s\n\nimportant to have a strategy, and that’s what this chapter aims to give you.\n\nHow to “prompt” efficiently is commonly known in the industry as a\n\n“prompt strategy” or “prompt engineering.” It’s not an engineering practice\n\nin the common sense of the word but rather an art form where practitioners\n\nof AI assistants have discovered patterns and practices that seem to work\n\nwell. We, the authors of this book, are building upon those discovered\n\npractices and aim to describe our findings for two domains: full-stack web development and data science. This book turns to you as either a web\n\ndeveloper or data scientist and aims to empower you by describing how you\n\ncan best approach problem solving in your domain using an AI assistant.\n\nThis chapter constitutes a central piece of the book. It’s central in the sense\n\nthat the approach being taught will be exemplified by other chapters in the\n\nbook. As such, see this chapter as a guide you can refer to, providing the\n\ntheory and thinking that is used in future chapters that solve specific\n\nproblems within data science and full-stack web development.\n\nIn this chapter, we will:\n\nProvide a strategy for solving problems with prompts and validating\n\nthe solution.\n\nIllustrate the strategy with examples from data science and full-stack\n\nweb development.\n\nIdentify some basic principles for writing prompts.\n\nWhere you are As a reader and a practitioner of data science and/or full-stack web development, you know your craft. Knowing your craft means you know\n\nthe tools and techniques to solve problems. At this point, you’re looking at\n\nan AI assistant and realize it’s controlled by natural language, so-called\n\nprompts. What you may not realize is that there’s more to it than just writing a prompt and getting an answer. An AI assistant is trained on a large\n\ncorpus of text, so it’s quite flexible on what it can generate text on and how to respond to prompts. Because of this flexibility, it’s important to understand how to write prompts that are effective and efficient.\n\nGuidelines for how to prompt eﬃciently Prompts are input to AI tools. Depending on what you’re trying to achieve,\n\nyou need to adjust your prompts for the scenario you’re solving for.\n\nTherefore, how you “prompt” matters. For example, if your prompt is too",
      "page_number": 25
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 47-66)",
      "start_page": 47,
      "end_page": 66,
      "detection_method": "synthetic",
      "content": "vague, you won’t get what you need. Or, let’s say you’re trying to use a\n\nprompt to generate company slogans; you don’t want to use the same prompt for generating code for an app. Conversely, in a discipline like data science, it’s important you perform tasks in a certain order and your prompt\n\nshould reflect what you want done and, if needed, the steps to do so.\n\nWhat you need to succeed is an approach, a strategy, that you can use in general to be efficient with AI assistants. Additionally, such a strategy should be specific enough to present “best practices” for chosen problem\n\ndomains. As mentioned earlier in this chapter, we’ve developed a prompt strategy specifically for the domain’s full-stack web development and data\n\nscience.\n\nAt a high level, we suggest the guidelines for general problem-solving\n\nusing AI assistants; it’s our belief it holds true regardless of what problem domain you’re applying prompts to:\n\n1. Break down the problem so that it’s fully understood. Within this\n\nguideline, there might be several steps like the following:\n\nUnderstand the problem: For any problem, it’s important to understand what the problem is and what it’s not. For example,\n\nare we building a machine learning model to predict sales or a\n\nweb page to track inventory? These are two different problems and require different approaches.\n\nIdentify the parts: A problem is usually complex and consists of many parts that need to be solved. For example, if we’re\n\nbuilding a machine learning model to predict sales, we need to identify the data, the model, the training, and the evaluation.\n\nEach of these parts can be broken down into smaller parts and so\n\non. Once you find the right detail level for your problem, you\n\ncan start solving it by writing prompts.\n\nBreak down the problem into smaller pieces: If needed, break\n\ndown your problem into smaller more manageable pieces.\n\nIdentify and understand the data: Especially with machine learning, it’s crucial to identify a dataset to work with, what it\n\ncontains, and how it’s structured. Within web development, data\n\nalso plays a central role, but the goal is usually to ensure you can read, write, and present data in a way that’s useful for the user.\n\n2. Generate prompts at a suitable level. Once you’ve understood the\n\nproblem fully, you should have a list of tasks, and for each task, you should be able to author and run a prompt that solves said task.\n\n3. Validate the solution. Just like without AI assistants, validation plays\n\na crucial part in building systems or apps. Traditionally, that means\n\nwriting tests, testing various components together, and letting users try out various parts. Using prompts is no different. A side effect of LLMs\n\nis that they can generate text that’s not relevant to the problem or solve\n\nthe problem in a less-than-optimal way. Because you’re relying on prompts that end up generating code for you, it becomes extra\n\nimportant to validate the solution to ensure it is correct and relevant.\n\nIn the upcoming sections, we will describe a detailed strategy for each\n\nchosen problem domain’s full-stack web development and data science.\n\nPrompt techniques There are some techniques that are extra useful when instructing the AI\n\nassistant to ensure you get the most out of it. The general advice is to\n\nprovide enough context to the AI assistant but what that means exactly can be made more concrete via the below suggested patterns:\n\nTask-Action-Guideline Prompt Pattern\n\nPersona-Instruction-Content Prompt Pattern\n\nExploratory prompt\n\nTask-Action-Guideline prompt pattern (TAG) To explain how this pattern works, let’s break down the parts and the show\n\nan example prompt with an example response:\n\nIntent: To instruct an AI assistant on what task to solve, what actions to\n\ntake, and what guidelines to adhere to.\n\nTask: Clearly define the task you want the AI to accomplish\n\nAction: Specify the steps or actions the AI should take to complete the\n\ntask\n\nGuideline: Provide any rules, constraints, or guidelines the AI should follow while performing the task\n\nExample:\n\n[Prompt]\n\nTask: Generate a summary of the latest news article about climate change.\n\nAction:\n\n1. Search for the most recent news articles on climate change.\n\n2. Select the article with the highest relevance and credibility.\n\n3. Summarize the key points of the article in 3-4 sentences.\n\nGuideline:\n\n1. Ensure the summary is concise and accurate.\n\n2. Avoid using technical jargon; keep the language simple and accessible.\n\n3. Cite the source of the article at the end of the summary.\n\n[End of prompt]\n\n[Prompt response]\n\nSummary: A recent article from BBC News highlights the alarming increase\n\nin global temperatures over the past decade. Scientists warn that if current trends continue, we could see unprecedented climate events. The article\n\nemphasizes the urgent need for international cooperation to combat climate\n\nchange. [Source: BBC News]\n\nThis pattern helps ensure that the AI assistant understands the task, knows the steps to take, and adheres to any specific guidelines, leading to more\n\naccurate and useful outputs.\n\n[End of response]\n\nHere’s what a prompt example could look like for web development:\n\n[Prompt]\n\nTask: Develop a RESTful API for a simple blog application using Node.js and Express.\n\nAction:\n\n1. Set up a new Node.js project.\n\n2. Create an Express server.\n\n3. Define routes for CRUD operations on blog posts.\n\n4. Implement middleware for JSON parsing and error handling.\n\n5. Connect to a MongoDB database using Mongoose.\n\n6. Write tests for the API endpoints.\n\nGuideline:\n\n1. Ensure the project setup is clear and straightforward.\n\n2. Provide concise and accurate code examples.\n\n3. Avoid using overly complex terminology; keep the language simple\n\nand accessible.\n\n4. Include comments in the code to explain each step.\n\n[End prompt]\n\nPersona-Instruction-Context prompt pattern (PIC) Let’s describe this pattern in the same way as the preceding one, let’s define\n\nit, followed by an example prompt and an example response:\n\nIntent: To instruct an AI assistant on the persona it should assume, what\n\ninstructions to carry out, and provide enough context to generate a correct response.\n\nPersona: Define the persona or character traits the AI should embody\n\nwhile responding\n\nInstruction: Specify the tasks or actions the AI should perform\n\nContext: Provide relevant background information or context to help\n\nthe AI understand the situation and generate an appropriate response\n\nExample:\n\n[Prompt]\n\nPersona: Assume the persona of a friendly and knowledgeable travel guide.\n\nInstruction:\n\n1. Recommend three must-visit attractions in Paris.\n\n2. Provide a brief description of each attraction.\n\n3. Suggest the best time to visit each attraction.\n\nContext: The user is planning a trip to Paris for the first time and is interested in both historical sites and local culture.\n\n[End of prompt]\n\n[Prompt response]\n\nAs your friendly and knowledgeable travel guide, I’d love to recommend\n\nthree must-visit attractions in Paris:\n\n1. **Eiffel Tower**: This iconic landmark offers breathtaking views of the\n\ncity. It’s best to visit in the early morning or late evening to avoid the\n\ncrowds and enjoy the stunning sunrise or sunset.\n\n2. **Louvre Museum**: Home to thousands of works of art, including the\n\nMona Lisa, the Louvre is a treasure trove of history and culture. Plan your\n\nvisit on a weekday morning to experience the museum with fewer visitors.\n\n3. **Montmartre**: This charming neighborhood is known for its artistic\n\nhistory and vibrant street life. Visit in the late afternoon to explore the\n\nquaint streets and enjoy a leisurely dinner at a local café.\n\nEnjoy your trip to Paris!\n\n[End of response]\n\nThis pattern ensures that the AI assistant adopts the right persona, follows specific instructions, and uses the provided context to deliver a relevant and\n\nhelpful response.\n\nHere’s what such a prompt could like for web development:\n\n[Prompt]\n\nPersona: Assume the persona of a friendly and knowledgeable software\n\ndevelopment mentor.\n\nInstruction:\n\n1. Set up a new Node.js project.\n\n2. Create an Express server.\n\n3. Define routes for CRUD operations on blog posts.\n\n4. Implement middleware for JSON parsing and error handling.\n\n5. Connect to a MongoDB database using Mongoose.\n\n6. Write tests for the API endpoints.\n\nContext: The user is a beginner in web development and is looking to build\n\ntheir first RESTful API for a blog application. They need clear, step-by-step\n\nguidance and code examples.\n\n[End of prompt]\n\nExploratory prompt pattern You might find yourself in situations where you’re not building a project\n\nfrom start to finish, or you only want to build a smaller part of and then evaluate the response. In those cases, your prompts will be more\n\nexploratory in nature and can for example look like the below:\n\n[Prompt]\n\nClean the data.\n\n[End of prompt]\n\nAn assumption here is that we have a Notebook open with existing code\n\nthat has already fetched the data.\n\nOr, something more for web development:\n\n[Prompt]\n\nAdd CSS for this product list.\n\n[End of prompt]\n\nPrompts for this pattern are usually a lot shorter in length, has context\n\n(derived from existing code or in some other way) and the developer line of\n\nsight is seldom beyond the next step.\n\nLearn-Improvise-Feedback-Evaluate prompt pattern (LIFE) This pattern, like TAG and PIC helps frame the problem and provides a good start to a solution that you can further refine.\n\nLearn: Highlight the need to understand data through various\n\nanalytical techniques, from basic statistics to complex correlations and\n\ntemporal analysis.\n\nImprovise: Adapt the analysis based on initial findings. For instance,\n\nif certain categories of products show unusual trends, deepen the\n\nanalysis in these areas\n\nFeedback:\n\nShare code and model outputs for feedback to ensure effective\n\nlearning and understanding.\n\nIncorporate suggestions and critiques to refine the model and the\n\napproach.\n\nProvide errors to troubleshoot and resolve the issues\n\nEvaluate: Execute the code provided by ChatGPT to ensure accuracy\n\nand validity.\n\nAn example prompt from this pattern might look like so:\n\n[Prompt]\n\nProject Title: Building a Web-Based Search Results Page. You must suggest\n\nsteps and code\n\nObjective: Create a dynamic and interactive search results page that efficiently displays and filters search results based on user queries. Steps:\n\nLearn: Understand the importance of displaying search results\n\neffectively. Analyze user interaction data to identify key metrics such\n\nas click-through rates, average time spent on results pages, and user satisfaction scores. Use various analytical techniques, from basic\n\nstatistics to complex correlations and temporal analysis, to gain\n\ninsights into user behavior and preferences.\n\nImprovise: Based on initial findings, adapt the search results page to\n\nhighlight the most relevant results. For instance, if users frequently\n\nclick on certain types of results, prioritize these in the display.\n\nImplement features such as filters, sorting options, and pagination to\n\nenhance user experience.\n\nFeedback: Share your code and the search results page with peers or\n\nmentors for feedback. This will help ensure effective learning and\n\nunderstanding. Incorporate their suggestions and critiques to refine the\n\nsearch results page and improve its functionality. Provide errors:\n\nDocument any errors encountered during the development process, such as issues with data retrieval or display. Troubleshoot and resolve\n\nthese issues, learning from the process to enhance your skills.\n\nEvaluate: Execute the code provided by ChatGPT to ensure its\n\naccuracy and validity. Validate the search results page by comparing its\n\nperformance with real-world data and user feedback. Make necessary\n\nadjustments to improve accuracy and user satisfaction.\n\n[End of prompt]\n\nYou might end up with a prompt response similar to:\n\n[Prompt response]\n\nGreat! Let’s break down the steps and provide some code snippets to help\n\nyou build a dynamic and interactive search results page.\n\nProject Title: Building a Web-Based Search Results Page\n\nObjective: Create a dynamic and interactive search results page that\n\nefficiently displays and filters search results based on user queries.\n\nSteps:\n\nLearn:\n\nUnderstand the importance of displaying search results effectively:\n\nAnalyze user interaction data to identify key metrics such as\n\nclick-through rates, average time spent on results pages, and user\n\nsatisfaction scores.\n\nUse various analytical techniques, from basic statistics to\n\ncomplex correlations and temporal analysis, to gain insights into\n\nuser behavior and preferences.\n\nImprovise:\n\nAdapt the search results page based on initial findings:\n\nHighlight the most relevant results based on user interaction\n\ndata.\n\nImplement features such as filters, sorting options, and\n\npagination to enhance user experience.\n\nFeedback:\n\nShare your code and the search results page with peers or mentors\n\nfor feedback:\n\nIncorporate their suggestions and critiques to refine the search\n\nresults page and improve its functionality.\n\nProvide errors:\n\nDocument any errors encountered during the development\n\nprocess:\n\nTroubleshoot and resolve these issues, learning from the process\n\nto enhance your skills.\n\nEvaluate:\n\nExecute the code provided by ChatGPT to ensure its accuracy and\n\nvalidity:\n\nValidate the search results page by comparing its performance\n\nwith real-world data and user feedback. Make necessary\n\nadjustments to improve accuracy and user satisfaction.\n\nCode Snippets:\n\nHTML Structure:\n\n<!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial- <title>Search Results Page</title> <link rel=\"stylesheet\" href=\"styles.css\">\n\n</head> <body> <div class=\"search-container\"> <input type=\"text\" id=\"search-input\" placeholder=\"Searc <button onclick=\"performSearch()\">Search</button> </div> <div class=\"results-container\" id=\"results-container\"></div <script src=\"script.js\"></script> </body> </html>\n\nCSS Styling:\n\n/* styles.css */ body { font-family: Arial, sans-serif; } .search-container { margin: 20px; } .results-container { margin: 20px; } .result-item { border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; }\n\nJavaScript Functionality:\n\n// script.js function performSearch() { const query = document.getElementById('search-input').value const resultsContainer = document.getElementById('results-c resultsContainer.innerHTML = ''; // Clear previous results // Simulate search results const results = [\n\n{ title: 'Result 1', description: 'Description for resu { title: 'Result 2', description: 'Description for resu { title: 'Result 3', description: 'Description for resu ]; results.forEach(result => { const resultItem = document.createElement('div'); resultItem.className = 'result-item'; resultItem.innerHTML = `<h3>${result.title}</h3><p>${re resultsContainer.appendChild(resultItem); }); }\n\nAdditional Features:\n\nFilters and Sorting: Add dropdowns or checkboxes to filter and sort results.\n\nPagination: Implement pagination to handle large sets of results.\n\nError Handling: Add error handling to manage issues with data\n\nretrieval or display.\n\nThis should give you a solid foundation to start building your web-based\n\nsearch results page. Let me know if you need further assistance or more\n\ndetailed code examples!\n\n[End of response]\n\nWhich pattern to choose? The general answer it depends on the situation which pattern to choose. The\n\nreason for choosing between the TAG and PIC is that you have a clearly\n\ndefined problem that you want to solve from start to finish and you’re\n\nlooking for the AI assistant to do the “heavy lifting”. The reason to go for\n\nthe exploratory pattern is when you’re working on an existing codebase, or\n\nyou want to try out various approaches and steps. Our general\n\nrecommendation is to try out all three patterns to see what suits your\n\napproach and problem the best.\n\nPrompt strategy for web development Let’s apply the different guidelines presented in the former section, and\n\nthrough using these guidelines, we will step by step work ourselves toward\n\na prompt strategy.\n\nBreak down the problem: “web system for inventory management” Let’s use a real example, “inventory management” within web\n\ndevelopment, to see if we can understand the general problem. To\n\n“manage” an inventory, you need to read and write data to it. Most likely,\n\nyou will have different roles in this system/app, everything from\n\nadministrators to normal users. You might also need to consider how this\n\nsystem fits in with other systems, should you, for example, integrate it with\n\nother systems, what parts it consists of in that case and how.\n\nThe domain seems pretty straightforward so let’s move on to understand\n\nwhat parts it consists of.\n\nAt a high level, we understand what the system should do. But to solve the\n\nproblem, we need to divide it into smaller parts, which in web development\n\nusually entails the following components:\n\nFrontend: The frontend is the part of the system that the user interacts\n\nwith. The frontend is responsible for presenting data to the user and\n\nreceiving input from the user.\n\nBackend: This part of the system communicates with the frontend.\n\nThe backend is responsible for reading and writing data to a database.\n\nThere could additionally be different frontends and different apps\n\naltogether that communicate with the backend in a more complex\n\nsystem.\n\nDatabase: The database is the part of the system that stores data. It’s a\n\ndata store, for example, a relational database such as MySQL or\n\nPostgreSQL. The database is responsible for storing data in a way\n\nthat’s efficient and easy to read and write.\n\nReporting: There’s often a reporting part that presents insights. It\n\ntakes its data from the data store and may need to transform the data to\n\nmake it presentable from a reporting perspective.\n\nFurther breakdown of the frontend into features Having an overview like this is useful but usually not enough of a\n\nbreakdown for us to start writing prompts. We need to break it down\n\nfurther, usually by features. At this point, a further breakdown of the\n\nfrontend into features may look like the following list:\n\nLogin: The user needs to be able to log in to the system.\n\nLogout: The user needs to be able to log out of the system.\n\nView inventory: The user needs to be able to view the inventory.\n\nAdd inventory: The user needs to be able to add the inventory.\n\nRemove inventory: The user needs to be able to remove the inventory.\n\nUpdate inventory: The user needs to be able to update the inventory.\n\nGenerate prompts for each feature At this point, it’s granular enough for us to start writing prompts. Let’s take\n\nthe first feature, login, and see how we can start thinking about how to craft\n\na prompt.\n\nYou may start with a prompt, using a tool like ChatGPT or GitHub Copilot\n\n(more on that in an upcoming next chapter) like the following:\n\n[Prompt]\n\nCreate a login page for the user to log in to the system.\n\n[End of prompt]\n\nWhile this may work, you’re leaving out a lot of context, such as what\n\nexactly is needed here and what’s not, what technologies are you using,\n\nwhat the user experience is, and so on.\n\nLet’s try to improve the prompt by adding more context:\n\n[Prompt]\n\nCreate a login page with fields for the username and password. It should\n\nhave a link for creating a user and a login button, be vertically and\n\nhorizontally centered, and work well on a mobile phone and tablet. It\n\nshould be written in React and use the Material UI library.\n\n[End of prompt]\n\nIdentify some basic principles for web development, a “prompt strategy” As you have seen so far, we broke down the problem into smaller, more\n\n“manageable” pieces and we suggested some prompts for how to solve a\n\nspecific feature. So what exactly are we suggesting in terms of “prompt\n\nstrategy” now that we understand more about our example “inventory\n\nmanagement”? For one, we realize that our strategy will be context-\n\ndependent. Because we’re in web development, we need to use keywords\n\nwithin that domain, together with libraries and architecture suitable for a\n\nspecific area. Here are some guides we suggest you use:\n\nProvide context – fields: A login screen can be as simple as username\n\nand password fields. Most screens however have more fields, such as a\n\npassword confirmation field, a link to reset a password, a field to\n\ncreate a new user, and so on. Depending on your needs, you may need\n\nto be very detailed.\n\nSpecify how – design, and tech choices: A login screen can be\n\ndesigned in many ways. It’s quite common today to optimize for\n\ndifferent devices like tablets, mobile, large screens, and so on. For tech\n\nchoices, web development has a lot of choices, from frameworks like\n\nReact, Vue, and Angular to plain HTML and CSS. Specify according\n\nto the needs of the project.\n\nIterate: Different tools react differently to the same prompt.\n\nThroughout this book, we will show you how to use different tools like\n\nGitHub Copilot and ChatGPT. Each tool has its own strengths and\n\nweaknesses and may offer different results. Try to iterate on the\n\nprompt by adding separators like commas and colons, and try\n\nrephrasing the prompt.\n\nBe context-aware: When you use tools like ChatGPT and GitHub\n\nCopilot, you do so with preexisting context. For ChatGPT, that means\n\nyou’re having an ongoing conversation of prompts and responses, and\n\nfor GitHub Copilot, it means it sees not only what you’ve written in\n\nyour open file but also your entire workspace if you let it. The\n\nresponse to your prompts looks at this context and decides what to\n\ngenerate. It’s important to be aware of this context and if you’re not\n\ngetting the response you want, try to change the context, in ChatGPT,\n\nstart a new conversation, and in GitHub Copilot, close the open files, start writing in a blank file, and so on.\n\nPrompt strategy for data science Let’s do a similar thought experiment for data science as we did for web\n\ndevelopment. We’ll use the presented guidelines “problem breakdown” and\n\n“generate prompts,” and just like in the web development section, we’ll draw some general conclusions on the domain and present those as a\n\nprompt strategy for data science.\n\nProblem breakdown: predict sales Let’s say we’re building a machine-learning model to predict sales. At a\n\nhigh level, we understand what the system should do. To solve the problem\n\nthough, we need to divide it into smaller parts, which in data science\n\nusually entails the following components:\n\nData: The data is the part of the system that stores information. The\n\ndata can come from many places like databases, web endpoints, static\n\nfiles, and more.\n\nModel: The model is responsible for learning from the data and\n\nproducing a prediction that’s as accurate as possible. To predict, you\n\nneed an input that produces one or more outputs as a prediction.\n\nTraining: The training is the part of the system that trains the model.\n\nHere, you typically have part of your data as training and a part being\n\nsample data.\n\nEvaluation: To ensure your model works as intended, you need to\n\nevaluate it. Evaluation means taking the data and model and producing\n\na score that indicates how well the model performs.\n\nVisualization: Visualization is the part where you can gain insights\n\nvaluable for the business via graphs. This part is very important, as it’s\n\nthe part that’s most visible to the business.\n\nFurther breakdown into features/steps for data science At this point, you’re at too high a level to start writing prompts. We can\n\nbreak it down further by looking at each step:\n\nData: The data part has many steps, including collecting the data, cleaning it, and transforming it. Here’s how you can break it down:\n\n1. Collect data: The data needs to be collected from somewhere. It\n\ncould be a database, a web endpoint, a static file, and so on.\n\n2. Clean data: The data needs to be cleaned. Cleaning means\n\nremoving data that’s not relevant, removing duplicates, and so\n\non.\n\n3. Transform data: The data needs to be transformed.\n\nTransformation means changing the data to a format that’s\n\nuseful for the model.\n\nTraining: Just like the data part, the training part has many steps to it.\n\nHere’s how you can break it down:\n\n1. Split data: The data needs to be split into training and sample\n\ndata. The training data is used to train the model and the sample\n\ndata is used to evaluate the model.\n\n2. Train model: The model needs to be trained. Training means\n\ntaking the training data and learning from it.\n\nEvaluation: The evaluation part is usually a single step but can be\n\nbroken down further.\n\nGenerate prompts for each step Note how our breakdown for data science looks a bit different from web\n\ndevelopment. Instead of identifying features like Add inventory, we\n\ninstead have a feature like Collect data.\n\nHowever, we’re on the correct level to author a prompt, so let’s use the\n\nCollect data feature as our example:\n\n[Prompt]\n\nCollect data from data.xls and read it into a DataFrame using Pandas library.",
      "page_number": 47
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 67-86)",
      "start_page": 67,
      "end_page": 86,
      "detection_method": "synthetic",
      "content": "[End of prompt]\n\nThe preceding prompt is both general and specific at the same time. It’s\n\ngeneral in the sense that it tells you to “collect data” but specific in that it\n\nspecifies a specific library to use and even what data structure (DataFrame).\n\nIt’s entirely possible that a simpler prompt would have worked for the\n\npreceding step like so:\n\n[Prompt]\n\nCollect data from data.xls.\n\n[End of prompt]\n\nThis is where it may vary depending on whether you use a tool like\n\nChatGPT or GitHub Copilot.\n\nIdentify some basic principles for data science, “a prompt strategy for data science” Here, we’ve identified some similar principles as in the web development\n\nexample:\n\nProvide context – filename: A CSV file can have any name. It’s important to specify the name of the file.\n\nSpecify how – libraries: There are many ways to load a CSV file, and\n\neven though Pandas library is a common choice, it’s important to specify it. There are other libraries to work with and you might need a\n\nsolution for Java, C#, and Rust, for example, where libraries are named\n\ndifferently.\n\nIterate: It’s worth iterating on the prompt, rephrasing it, and adding\n\nseparators like a comma, a colon, and so on.\n\nBe context-aware: Also here, context matters a lot; if you’re working\n\nin Notebook, previous cells will be available to GitHub Copilot,\n\nprevious conversations will be available to ChatGPT, and so on.\n\nAs you can see from the preceding guidance, the strategy is very similar for\n\nweb development. Here we’re also listing “Provide,” “Specify how,”\n\n“Iterate,” and “Be context-aware.” The big difference lies in the details.\n\nHowever, there’s an alternate strategy that works in data science and that’s\n\nlengthy prompts. Even though we’ve broken down the data science problem\n\ninto features, we don’t need to write a prompt per feature. Another way of\n\nsolving it could be to express everything you want to be carried out in one\n\nlarge prompt. Such a prompt could therefore look like so:\n\n[Prompt]\n\nYou want to predict sales on the file data.xsl. Use Python and Pandas\n\nlibrary. Here are the steps that you should carry out:\n\nCollect data\n\nClean data\n\nTransform data\n\nSplit data\n\nTrain model\n\nEvaluation\n\n[End of prompt]\n\nYou will see examples in future chapters on data science and machine\n\nlearning where both smaller prompts as well as lengthier prompts are being\n\nused. You decide which approach you want to use.\n\nValidate the solution The most important part of this strategy is verifying correctness and that the\n\ntext and code created by the AI assistant are correct. There are two general\n\napproaches we can take to verify our outcome:\n\nVerification via prompts: The first approach is to use prompts to\n\nverify the outcome. This means writing prompts that question the\n\noutcome of specific results. This can be a good strategy to employ at\n\nthe beginning of your verification process. What you’re looking for are\n\nsituations where the AI assistant isn’t consistent in its responses.\n\nClassical verifications: The second approach is to use classical\n\nverification techniques. What those techniques are varies depending on\n\nthe problem domain. At a high level, though, it boils down to testing\n\ncode, comparing output, and relying on your own knowledge, and the knowledge of your peers, to verify the outcome.\n\nThe AI tool doesn’t really know what it’s doing. The responses provided are\n\nresponses that likely depend on its training corpus. At all times, you should\n\nbe aware of this and rely on your expertise to verify the outcome.\n\nIn the next subsections, let’s explore various approaches for manual and\n\nclassical verification.\n\nVeriﬁcation via prompts You can use prompts to both produce results that take you closer to\n\nresolving the problem but also to verify the results. Let’s take an example\n\nwhere we’re building the previously mentioned login screen. We’ve written\n\na prompt that looks like the following:\n\n[Prompt]\n\nCreate a login page with fields for username and password; it should have a\n\nlink for creating a user and a login button. It should be vertically and\n\nhorizontally centered and work well on a mobile phone and tablet. It should\n\nbe written in React and use the Material UI library.\n\n[End of prompt]\n\nTo verify this outcome, we can write a prompt like the following:\n\n[Prompt]\n\nGiven the below code, what does it do?\n\n.login { <!-- should include CSS to center horizontally and vertically @media (min-width: 768px) { <!-- should include CSS to center horizontally and vertical } } <div class=\"login\"> <TextField id=\"username\" label=\"Username\" /> <TextField id=\"password\" label=\"Password\" /> <Button variant=\"contained\" color=\"primary\"> Login </Button> </div>\n\n[End of prompt]\n\nAn app could look at the code provided and realize you don’t need a prompt\n\nto deduce what it does, and that it’s in fact missing CSS code to make it\n\nresponsive. The point here though is that by writing prompts, you can have\n\nthe AI assistant tell you what it thinks the code does via questions.\n\nUsing prompts this way, to pose queries on the output, is a good first step to\n\nverifying the outcome. However, it’s not enough and you need to rely on classical verification techniques, so let’s cover that next.\n\nClassical veriﬁcation How you verify the outcome depends on the problem domain. In web\n\ndevelopment, you can use a variety of different tools and techniques, for\n\nexample:\n\nTesting: With end-to-end testing or frontend testing, you can verify\n\nthat the code works as intended. Usually, this type of test involves\n\nusing a programmatic approach to simulate user interaction with the\n\nweb page, using something like Selenium, for example.\n\nManual testing: You can manually test the web page by opening it in\n\na browser and interacting with it. This is a good approach to use at the\n\nbeginning of your verification process. Apart from interaction, you can\n\nalso visually inspect the web page to see if it looks correct according to\n\nyour requirements.\n\nCode review: You can review the code and see if it looks correct. This\n\nis a good approach to use at the beginning of your verification process.\n\nIt allows not only you but also your peers to verify the outcome.\n\nTools: Tools can test a variety of different scenarios like accessibility,\n\nperformance, and so on. These tools are most likely a part of your\n\ndevelopment process already.\n\nConducting data science, you may rely on all of the preceding approaches,\n\nbut you may also use other approaches. Some common approaches are:\n\nUnit testing: You can use unit testing to verify that the code works as\n\nintended.\n\nIntegration testing: You can use integration testing to verify that the\n\ncode works as intended.\n\nValidation of results: This type of validation means you compare the\n\nresults of your analysis or model to known results or benchmarks.\n\nCross validation: This type of validation means you split your data\n\ninto training and sample data, train your model on the training data,\n\nand evaluate it on the sample data. This is a good approach to use at\n\nthe beginning of your verification process.\n\nSummary Throughout this chapter, we’ve provided a strategy for solving problems\n\nwith prompts and validating the solution.\n\nYou’ve seen how both web development and data science can be broken\n\ndown into smaller parts that can be solved with prompts. We also identified\n\nsome basic principles for writing prompts.\n\nFinally, we looked at how to validate the solution using prompts and\n\nclassical verification techniques.\n\nIt’s our hope that you will revisit this chapter when you’re looking at\n\nsolving a problem within web development or data science and you’re\n\nlooking for an approach.\n\nThere’s more to prompting than writing a prompt and getting a response.\n\nYou will see throughout this book how we use these principles in various\n\ndomains to solve problems. Try typing these prompts as you read, adapt to\n\nyour own needs, and see what happens.\n\nIn the next chapter, we are going to learn more about the two AI assistants\n\nof our choice, GitHub Copilot and ChatGPT.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n3\n\nTools of the Trade: Introducing Our AI Assistants\n\nIntroduction Writing code or text takes time, at least if you want it to be well organized\n\nand readable. But what if you could have a tool that would help you write\n\ncode faster and with less effort? That’s what GitHub Copilot and ChatGPT\n\nare really all about.\n\nBefore you start using an AI assistant, it’s a good idea to get a mile-high\n\nview of its capabilities and limitations. You want to see what you can and\n\ncan’t use it for, or at least understand where the tool performs less well.\n\nIn this chapter, we will cover the following:\n\nUnderstanding what GitHub Copilot and ChatGPT are and how they work\n\nLearning about Copilot’s capabilities and limits\n\nInstalling GitHub Copilot\n\nGenerating code completions via GitHub Copilot\n\nUnderstanding Copilot Pair programming is the idea of (usually two) developers working together,\n\noften in front of the same screen, also sometimes called “pairing.” GitHub\n\nCopilot can be seen as an “AI pair programmer” that helps you write code, enabling you to get more done, faster. It’s based on OpenAI’s Codex model,\n\na new AI system trained on publicly available source code and natural\n\nlanguage. But in reality, it has gone beyond this. Let’s denote GitHub\n\nCopilot as Copilot for the remainder of the book. Copilot suggests whole\n\nlines or entire functions right inside your editor.\n\nHow Copilot knows what to generate The idea behind Copilot is that it learns from the code you and others have written and uses that knowledge to suggest new lines of code as you type.\n\nHow does Copilot work? It uses machine learning to build a model of the\n\ncode you and others have written and suggests the best text for you to use next. There are two parts of importance, the trained model and the so-called\n\nin-memory context. The model is trained on public repositories on GitHub and the context is something it assembles at runtime from looking at your\n\nfiles. Using the context and the underlying model, it provides you with text suggestions. Copilot uses some of the following to build its context (i.e., its in-memory capability that it uses together with the trained model to provide\n\nsuggestions):\n\nYour active file: The code you’re working on.\n\nComments: Copilot uses comments to understand the context of your\n\ncode.\n\nOpen files and your workspace: It not only looks at the code in your\n\nactive file but also at the code in other files in your workspace.\n\nImport statements: Even import statements are factored into Copilot’s suggestions.\n\nThe underlying model and its training data: The code in public GitHub repositories constitutes the base of what it’s trained on.\n\nCopilot capabilities and limits So, what can Copilot do? It can do a lot, but here’s a non-exhaustive list of\n\ncapabilities:\n\nCode completion: Copilot can complete lines of code for you.\n\nCode generation: Copilot can generate whole functions for you.\n\nTests, comments, and documentation: Copilot can generate tests, comments, and documentation for you.\n\nSuggest improvements: Copilot can suggest improvements to your code. Improvements can come in many forms, from suggesting a better variable name or a better way to write a function to how to organize\n\ncode better.\n\nTranslate code: Copilot can translate code from one language to\n\nanother. For example, it can translate Python code to JavaScript code.\n\nAnswer questions: Copilot can answer questions about your code. For\n\nexample, it can tell you what a function does or what a variable is used for, and answer questions about a domain such as “What is machine learning?”, for example.\n\nSetup and installation\n\nHow can you get started? You can use Copilot from a variety of places and\n\neditors including Visual Studio, Visual Studio Code, GitHub Codespaces, and GitHub’s web-based editor. In this chapter, we’ll use Visual Studio Code.\n\nInstalling Copilot To install Copilot, you need to install the GitHub Copilot extension for\n\nVisual Studio Code and also need to allow access.\n\nLet’s review the steps in more detail (as outlined on the official Copilot docs page).\n\nYou can install the GitHub Copilot extension for Visual Studio Code from\n\nthe Visual Studio Code Marketplace or from within Visual Studio Code. We\n\nwill show the latter here:\n\n1. In the Extension: GitHub Copilot tab in Visual Studio Code, select\n\nInstall.\n\n2. If you have not previously authorized Visual Studio Code in your\n\nGitHub account, you will be prompted to sign in to GitHub in Visual Studio Code.\n\n3. If you have previously authorized Visual Studio Code for your account\n\non GitHub, GitHub Copilot will be automatically authorized.\n\n4. If you don’t get the prompt to authorize, select the bell icon in the\n\nbottom panel of the Visual Studio Code window.\n\n5. In your browser, GitHub will request the necessary permissions for\n\nGitHub Copilot. To approve these permissions, select Authorize Visual Studio Code.\n\n6. To confirm the authentication, in Visual Studio Code, select Open in\n\nthe Visual Studio Code dialog box.\n\nRefer to this page if you have any problems getting Copilot to work:\n\nhttps://docs.github.com/en/copilot/getting- started-with-github-copilot.\n\nGetting started with Copilot How do we get started? Well, provided you have installed Copilot and\n\nthere’s a Copilot icon in the bottom-right corner of your Visual Studio Code\n\nwindow, you’re good to go.\n\nHere’s a suggestion to get started:\n\n1. Create a new file in Visual Studio Code named app.js.\n\n2. Start typing the text prompt “Express web api with routes products and customers” as a comment at the top of the file like so, and press Enter:\n\n//Express web api with routes products and customers\n\n3. Give it a few seconds and you should see a suggestion from Copilot as\n\nfollows:\n\nconst express = require('express');\n\nIf nothing appears, try pressing Ctrl + Spacebar to trigger a suggestion or start typing the start of the code,\n\ni.e., const, and wait for a suggestion to appear.\n\n4. You will have to press the Tab key to accept the suggestion. At this\n\npoint, Copilot can keep generating code for you. To ensure it does, press Enter and watch as Copilot generates more code for you.\n\nRepeatedly press Enter and press Tab to accept the suggestions until you have code similar to the following:\n\nconst app = express(); app.get('/products', (req, res) => { res.send('products'); }); app.get('/customers', (req, res) => { res.send('customers'); }); app.listen(3000, () => { console.log('Server listening on port 3000'); });\n\n5. Congratulations, you’ve just written your first lines of code with\n\nCopilot. Feel free to experiment with Copilot and try adding\n\ncomments, so-called prompts, in the middle of your code and see what happens. Also, try varying the prompts and see what happens.\n\nAssignment: improve the code As an assignment, you’re asked to improve the code generated by Copilot.\n\nHere are a few suggestions:\n\nAdd a route for the root of the web API.\n\nAdd a route for a specific product.\n\nAdd documentation for one of the routes.\n\nSolution Here’s a possible solution:\n\nconst express = require('express'); app = express();\n\n// add default route app.get('/', (req, res) => { res.send('Hello world'); }); app.get('/products', (req, res) => { res.send('products'); }); // document route /** * Get a product by id * @param {number} id - The id of the product */ app.get('/products/:id', (req, res) => { res.send(`product with id ${req.params.id}`); }); app.get('/customers', (req, res) => { res.send('customers'); }); app.listen(3000, () => { console.log('Server listening on port 3000'); });\n\nChallenge See if you can add a test for one of the routes.\n\nIn the next chapter, we will look at how to use Copilot in more detail. To\n\nuse any AI assistant well, you need to understand how it works and how to use it. There’s a skill associated with using these tools well and it’s called\n\nprompt engineering. Prompt engineering is the art of writing prompts, not\n\nonly to make it understand your intentions but also to produce an output\n\nyou’re happy with. It’s more than just writing a comment; you can instruct\n\nyour AI assistant to solve something, apply a form of reasoning to it, and\n\nmuch more. The next chapter presents the central theme of this book,\n\nprompt engineering.\n\nReferences Copilot landing page:\n\nhttps://github.com/features/copilot\n\nCopilot docs:\n\nhttps://docs.github.com/en/copilot/getting-\n\nstarted-with-github-copilot\n\nUnderstanding ChatGPT ChatGPT, an OpenAI development, is a specialized version of the GPT\n\nmodel designed to simulate human-like conversations. It excels in creating\n\nhuman-like text in dialogues, handling a variety of topics. It’s available for free at chat.openai.com, with a premium ChatGPT Plus option (also known as GPT-4), and can draft essays, generate art prompts, and program\n\ncode. The premium version offers enhanced features such as visual and\n\naudio input and output handling, file uploads, code execution, data\n\nvisualization with select Python libraries, and customizable GPT\n\ncapabilities.\n\nYou can access ChatGPT simply by visiting chat.openai.com and creating an OpenAI account. It is also available as an app for both Android\n\nand iOS. More details can be found on the official website\n\n(https://openai.com/).\n\nFigure 3.1: Offerings of OpenAI\n\nHow does ChatGPT work? ChatGPT, paralleling Copilot’s code-oriented approach but in natural\n\nlanguage processing, is adept at content generation, challenging traditional\n\nsearch engines. It excels in tasks such as essay writing and summarizing\n\ntexts. The quality of ChatGPT’s responses heavily depends on the prompts\n\nit receives.\n\nChatGPT leverages extensive training data including books, websites, and a\n\nvariety of textual sources for comprehensive language understanding.\n\nIt employs sophisticated machine learning algorithms, such as deep learning neural networks based on the Transformer architecture, to predict accurate\n\nand contextually relevant text responses.\n\nChatGPT’s contextual understanding is honed through advanced\n\ntechniques, enabling it to interpret and respond to varying conversation\n\nthreads intelligently. This approach mirrors the principles used in Copilot\n\nfor code, adapted here for nuanced, human-like text interaction.\n\nChatGPT capabilities and limits Capabilities of ChatGPT:\n\nContent creation: Generates creative content including marketing\n\nmaterial, blog posts, stories, and poems\n\nEducational explanations: Offers detailed explanations on complex\n\ntopics for educational purposes\n\nCoding assistance: Assists developers with code optimization, error\n\ndebugging, and algorithm design\n\nLearning aid: Acts as a companion in online learning, offering real-\n\ntime assistance and clarification of concepts\n\nConversational AI: Enhances user experience in virtual assistants and\n\nchatbots through natural language interactions\n\nLimitations and concerns of ChatGPT:\n\nAccuracy issues: ChatGPT may generate responses with factual\n\ninaccuracies or biases from training data, also known as hallucinations. These outputs often emerge from the AI model’s inherent biases, lack\n\nof real-world understanding, or training data limitations. In other\n\nwords, the AI system “hallucinates” information that it has not been\n\nexplicitly trained on, leading to unreliable or misleading responses.\n\nHence, users should always verify and validate the responses and\n\nshould not use them blindly.\n\nEthical implications: Raises concerns about the misuse of AI-\n\ngenerated content for fraudulent activities or harmful information\n\ngathering.\n\nEmployment impact: Fear of AI replacing human jobs in certain\n\nsectors.\n\nSecurity risks: Potential use in phishing, creating malware, and cybercriminal activities.\n\nData privacy: Concerns about the use of vast internet data in training,\n\nimpacting user privacy.\n\nMessage cap: At the time of writing the book, GPT-4 was capped to\n\noffer a maximum of 40 responses over 3 hours.\n\nLimited Python libraries for code execution: The Code Interpreter\n\nand Advanced Data Analysis features of ChatGPT use limited sets of\n\nlibraries, heavily featuring machine learning libraries but not such\n\ngreat support for other libraries, such as Keras or TensorFlow, required\n\nfor deep learning.\n\nSetup and installation Setting up and installing ChatGPT involves a few steps:\n\n1. Create an OpenAI account: Visit the OpenAI website and sign up for\n\nan account.\n\n2. API access: Developers need to obtain API access by applying on the\n\nOpenAI platform.\n\nFor non-developers, using ChatGPT is as simple as visiting the ChatGPT\n\nwebsite or installing the Android or iOS app and logging in with your\n\nOpenAI account. No installation is required for general use. For more detailed steps and information, please refer to OpenAI’s official\n\ndocumentation and website.\n\nGetting started with ChatGPT Once you’ve logged in to your OpenAI account on the ChatGPT side of the\n\nwebsite, it’s time to get to know the AI tool’s window. Here’s a breakdown of what you will see:\n\nNew chat and hide sidebar buttons: On the left side of your screen,\n\nthe New chat button can be used to start fresh conversations at any\n\ntime. It creates a new discussion without context. There’s also an option to hide the sidebar.\n\nChat history: The left sidebar keeps your previous conversations\n\naccessible. You can edit chat titles, share your chat history, or delete it.\n\nOptionally, you can turn off chat history.\n\nAccount: Click your name at the bottom left to access your account\n\ninformation. This includes settings, log out, help, and FAQs. If you\n\ndon’t have ChatGPT Plus, you’ll see an Upgrade button here.\n\nYour prompts: Your questions or prompts appear in the middle of the\n\nchat window, accompanied by your account photo or initials.\n\nChatGPT’s responses: ChatGPT’s responses display the logo on the\n\nleft. On the right, you’ll see options such as Copy, Thumbs Up, and\n\nThumbs Down. Copy text to your clipboard for use elsewhere and\n\nprovide feedback on response accuracy.\n\nRegenerate response: Click Regenerate response if you encounter\n\nissues or unsatisfactory answers. It prompts ChatGPT to generate a\n\nnew reply based on your latest prompt.\n\nText area: This is where you enter your prompts and questions.\n\nChatGPT version: Below the text input area, you’ll find fine print,\n\nincluding a disclaimer: “ChatGPT can make mistakes. Consider\n\nchecking important information.” Note that the display of the ChatGPT\n\nmodel version has been discontinued.\n\nThe following screenshot illustrates how this looks.\n\nIn the top-left corner, you can see the GPTs you have access to if you have\n\nthe premium version.\n\nAt the bottom are your previous conversations.\n\nIf you have the premium version, you can choose GPT-4 from the dropdown along with plugins.\n\nFigure 3.2: Selecting different versions of ChatGPT\n\nYou can even set custom instructions at the profile level if you wish to\n\napply your configuration to all new conversations.",
      "page_number": 67
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 87-105)",
      "start_page": 87,
      "end_page": 105,
      "detection_method": "synthetic",
      "content": "Figure 3.3: ChatGPT custom instructions\n\nPrompting Let’s draft our first prompt with ChatGPT.\n\nYou just have to ask questions in your natural language and converse with it\n\nlike you would with a human and it will start sharing its knowledge with\n\nyou.\n\n[Prompt]\n\nCan you please explain the process of machine learning in bullet points to\n\nme?\n\n[End of prompt]\n\nYou should see a response similar to the following screenshot. Note that the\n\nresponses are never identical, and you will not get the exact same text each\n\ntime.\n\nFigure 3.4: ChatGPT prompt screen\n\nSummary In this chapter, we’ve looked at GitHub Copilot and ChatGPT, including\n\nwhat they are, how they work, and how to get started with them.\n\nWe’ve also looked at some of their capabilities and limitations.\n\nFinally, we’ve looked at how to install them and work with them. You were\n\nalso given some idea of how to use them via prompts. 3\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n4\n\nBuild the Appearance of Our App with HTML and Copilot\n\nIntroduction Building a web app involves the usage of HTML for markup, CSS for\n\nstyling, and JavaScript for making it interactive.\n\nWe’ve come a long way from building a web app from static pages in the\n\n1990s to now using frameworks to build large apps. Regardless of whether\n\nyou use a framework or a library, it still rests on the same foundation,\n\nHTML, CSS, and JavaScript.\n\nTo tackle these three markup and programming languages, we can use an AI\n\nassistant. There’s more to using an AI assistant than generating text, given\n\ntext input. You also need working knowledge of the area you’re trying to tackle. For markup languages like HTML and CSS, “working knowledge”\n\nmeans you should know how to structure a web page or configure the\n\nstyling with CSS. In short, you know how to do the task at hand, and the AI\n\nassistant is there to make you faster and more efficient.\n\nThe output from the prompts mentioned in this chapter may\n\nvary based on training data, what files you have open, and\n\nwhat you typed previously.\n\nYou will see throughout the different chapters of this book how we will\n\nfollow a specific method of first discussing a business problem we’re looking to solve, with web development or data science merely being\n\napproaches that help us solve the problem. We will then focus on the\n\nproblem, which varies depending on whether we’re a web developer or data\n\nscientist, followed by dividing up our problem into smaller more\n\nmanageable parts. Finally, we will recommend a “prompt” strategy that works well for this particular type of problem.\n\nIn this chapter, we will:\n\nGenerate basic HTML: GitHub Copilot is capable of generating\n\ndifferent kinds of code, including HTML.\n\nApply prompting techniques: There are different techniques we can use to get the desired content.\n\nBusiness problem: e- commerce E-commerce is quite an interesting domain. There are many problems to be\n\nsolved within this domain. For example, you need to provide a technical platform that allows users to purchase items. That in itself means you need\n\nto build various solutions for taking payments as well as products to buy and also some logistics that allow for shipping and more.\n\nIf you look at this business from the data side of things, you see that you\n\nneed to analyze customer behavior to ensure you have the right number of items in stock, the correct prices on the items, and so on. In short, e- commerce is an intriguing domain that you will see mentioned throughout\n\nthe book.\n\nProblem domain This chapter focuses on the role of the web developer, so let’s discuss what\n\ntype of problems await a web developer in e-commerce. There are usually\n\ntwo to three major roles you need to solve for as a web developer:\n\nThe customer and all the actions that a customer can take like browsing and purchasing items to managing their account.\n\nBack office: This is the company behind the e-commerce application.\n\nHere, you need to ensure there exist technical solutions so that company employees can manage inventory, product information,\n\npayment solutions, and more.\n\nFrom a data standpoint, you, as a web developer, need to ensure that\n\ndata can be stored and updated on areas like products, purchase orders, and customer information.\n\nProblem breakdown: identify the features To start breaking down this problem domain into something we can write\n\nprompt input for, let’s again turn to the roles we mentioned, customer and back office. Here’s an attempt at breaking the problem down into features we can build.\n\nLet’s start with the customer role and the major area, “Authentication.”\n\nLet’s attempt to break it down into things the customer should be able to do. Here are the actions we should support:\n\nLogin: The user should be able to log in.\n\nLogout: The user should be able to log out.\n\nCreate a new user: It should be possible to create a new user.\n\nUpdate password: An existing user should be able to update their\n\npassword.\n\nPassword: If a user forgets their password, it should be possible to reset it in a safe way.\n\nNow, we have a set of features for a specific domain, “Authentication,” and\n\nwe have a better grasp of the different actions we should support. We’ll\n\nleave it to you to further break down your problem domain like this, but see the preceding list as the detail level you should preferably be on before you\n\nstart using your AI assistant.\n\nGiven the preceding breakdown into features, you could now, for example,\n\ntype a prompt like so to attempt solving the first feature we identified above:\n\n[Prompt]\n\nGenerate a login page, with fields for username, password, and repeat\n\npassword and login button.\n\n[End of prompt]\n\nAs a web developer, you’ve usually already done this breakdown of the problem domain into features before you start developing and even called\n\nthese “user stories” if you use a development methodology like Scrum, for\n\nexample.\n\nWith web development, though, you know it’s a matter of looking at this problem from three different layers, the frontend, backend, and the data\n\nlayer, usually a database where you store the data. The rest of this chapter\n\nwill focus on the frontend when using the AI assistant. In later chapters, we will focus on other layers of the e-commerce example.\n\nPrompt strategy So, how do we select a prompt strategy and what do we mean by prompt\n\nstrategy? Our strategy is about how we will prompt; will we write one prompt per feature or many short ones? It’s also about how we use our\n\nchosen AI assistant, GitHub Copilot, and how we choose to input the\n\nprompts into the tool.\n\nIn GitHub Copilot, there are two major choices for how you write your prompts, either using the chat functionality or via typing comments or code\n\ndirectly in a text file. In this chapter, we will use the latter approach of\n\ntyping directly in the text file. The general recommendation is that both\n\napproaches are valid and, it’s our experience that you vary between these two approaches as you solve a problem.\n\nNow that we’ve chosen our GitHub Copilot approach, what about the\n\nprompts themselves? We will choose a prompt strategy here where we type\n\nshorter prompts – we refer to this pattern as “Exploratory prompt pattern” in Chapter 2 of the book. We will let GitHub Copilot build up its runtime\n\ncontext and learn from our code as we type it.\n\nIn the upcoming section, we will showcase how you can start generating\n\nmarkup code while being inside an open text file. At the end of this chapter, you will see how we revisit our e-commerce use case.\n\nPage structure A web page is defined in HTML, and all such pages are made up of a tree\n\ncalled a document object model (DOM). The DOM has the following\n\nparts:\n\n<html> <head> </head> <body> </body> </html>\n\nYou can see how the markup of the page consists of elements. The top\n\nelements need to be laid out hierarchically with HTML being the root\n\nelement having the inner nodes HEAD and BODY. In the HEAD node, we\n\ndefine things like styling, instructions for search engines, page title, and\n\nmore. In the BODY element, we place content we want to be visible.\n\nAdd AI assistance to our page structure How can we leverage GitHub Copilot for this? Normally, web apps should\n\nhave an index.html as an entry point for the app. To leverage GitHub\n\nCopilot, create a comment, <!-- my comment -->, on the page. You need to\n\nreplace my comment with your prompt. So, what do you type instead of my comment? The answer is to provide GitHub Copilot with enough context for\n\nit to know what to generate in response.\n\nGitHub Copilot builds a runtime context not only based on its training model but also based on file endings like .js or\n\n.html, and what text is in both open or closed files in the directory you’re in. This context is what decides what text\n\nshould be generated based on what prompt you type.\n\nYour ﬁrst prompt, simple prompting, and aiding your AI assistant Let’s try writing our first prompts in an open text file.\n\nIf you want to try the following prompts yourself while\n\nreading this, create an index.html page. Prompts are\n\ncomments that you write. The comment should use the\n\nassigned comment format for the chosen type of file, meaning <!-- --> is how you comment in an HTML page, and // is how you write a comment in a JavaScript file.\n\nHere’s a prompt you can try adding as an HTML comment at the top of the page:\n\n[Prompt]\n\nCreate a page with a title and list.\n\n[End of prompt]\n\n<!-- create a page with a title and list -->.\n\nIn this case, this prompt generates another prompt, namely, <!-- the list\n\nshould contain 3 items -->.\n\nAt first glance, this might look confusing. Why would GitHub Copilot give\n\nme another comment as output? The explanation is that your AI assistant is\n\ntrained on code and comments. In this case, GitHub Copilot provides you\n\nwith the most likely response given its training data.\n\nSo, how do you get HTML markup as a response instead? The answer is that you start typing HTML, which narrows it down for GitHub Copilot, so\n\nthe code in your text file should look like the following:\n\n<!-- create a page with a title and list --> <HTML\n\nGitHub Copilot now generates code that should look similar to the\n\nfollowing output:\n\n[Prompt response]\n\n<html lang=\"en\">\n\n[End of response]\n\nIf you keep hitting the Return key, and accepting the suggestions with Tab, more and more code will be generated so you end up with an output like so:\n\n[Prompt response]\n\n<!-- create a page with a title and list --> <html lang=\"en\"> <head> <title>My First Page</title> </head>\n\n<body> <h1>My First Page</h1> <ul> <li>First item <li>Second item <li>Third item </li> </body> </html>\n\n[End of response]\n\nYour second prompt: adding more context Let’s erase what we’ve done so far and start with a new prompt. The\n\nobjective is to understand how writing a better-written prompt, with more\n\ncontext, yields a better output.\n\nLet’s try a second prompt, with more context and have it look like the\n\nfollowing text:\n\n[Prompt]\n\nCreate a page with a title and list, each item should have fields title,\n\ndescription and completed.\n\n[End of prompt]\n\nNote how we’re now saying that the list consists of items\n\nand that each item has separate fields. So, what’s the output\n\nof this prompt? Like before, use the Return key to generate\n\na prompt response and Tab to accept the completion\n\nprovided by GitHub Copilot.\n\n[Prompt response]\n\n<!-- create a page with a title and list, each item should have <html> <head> <title>Todo List</title> </head> <body> <h1>Todo List</h1> <ul> <li> <h2>Item 1</h2> <p>Item 1 description</p> <p>Completed: <input type=\"checkbox\" checked></p> </li> <li> <h2>Item 2</h2> <p>Item 2 description</p> <p>Completed: <input type=\"checkbox\"></p> </li> </ul>\n\n[End of response]\n\nOur AI assistant is able to guess that we’re trying to build a so-called “Todo\n\napp,” a common type of app where we have tasks we’re looking to complete. It was also able to generate a header element h2 for the title, a p for the description, and a checkbox for the Completed field. By adding more\n\ncontext, our AI assistant was able to provide a better output.\n\nYour third prompt: accept prompt suggestions So far, you’ve seen the difference between the two prompts, but what\n\nhappens if we add even more context like so?\n\n[Prompt]\n\ncreate a page with a header, main part, and a footer. Use dark theme.\n\n[End of prompt]\n\nAt this point, the output consists of more comments (suggestions), and if\n\nyou accept several of them by entering the Return key (to get a new\n\nsuggestion) and the Tab key (to accept the completion), your text file should\n\nnow have content similar to the following output:\n\nIt’s also possible to reject the suggestion by, for example,\n\ncontinuing to type.\n\n[Prompt response]\n\n<!-- create a page with header, main part and footer use dark t <!-- header should have a navigation with 3 links --> <!-- main part should have 3 sections with some text and images <!-- footer should have 3 columns with some text and images --> <!-- use flexbox for layout --> <!-- use css variables for colors --> <!-- use css variables for font sizes --> <!-- use css variables for paddings and margins --> <!-- use css variables for font family --> <!-- use css variables for border radius --> <!-- use css variables for border width -->\n\n<!-- use css variables for border color --> <!-- use css variables for background color -->\n\n[End of response]\n\nGitHub Copilot can suggest prompts that you might not even consider like navigation, images, and the use of CSS variables. If you type <html at this\n\npoint, you see an output similar to:\n\n[Prompt response]\n\n<!-- create a page with header, main part and footer use dark t <!-- header should have a navigation with 3 links --> <!-- main part should have 3 sections with some text and images <!-- footer should have 3 columns with some text and images --> <!-- use flexbox for layout --> <!-- use css variables for colors --> <!-- use css variables for font sizes --> <!-- use css variables for paddings and margins --> <!-- use css variables for font family --> <!-- use css variables for border radius --> <!-- use css variables for border width --> <!-- use css variables for border color --> <!-- use css variables for background color --> <html> <head> <title>06.6 solution</title> <link rel=\"stylesheet\" href=\"style.css\"> </head> <header> <nav> <a href=\"#\">Home</a> <a href=\"#\">About</a> <a href=\"#\">Contact</a> </nav> </header> <body> <main> <section>\n\n<h1>Section 1</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> <section> <h1>Section 2</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> <section> <h1>Section 3</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> </main> <footer> <div> <h1>Column 1</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> <div> <h1>Column 2</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic Quisquam, voluptatum.</p> <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> <div> <h1>Column 3</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> </footer>\n\n[End of response]\n\nView this output in a browser and it should display the preceding sections\n\nthat contain the title, description, and image for a number of items (exactly\n\nwhat you will see varies as these images are random).\n\nAs a web developer, you take this for what it is: a nice first scaffold that you\n\nneed to add styling to at a later stage to make it look better.\n\nFigure 4.1: Page with images (without the header and footer)\n\nChallenge: vary the prompt You can vary the output by adding or removing context. As a suggestion,\n\nsee if you can change the prompt to add tables, forms, a multi-column\n\nlayout, and more.\n\nUse case: build a front for an e-commerce Now that we’ve learned how to use GitHub Copilot for creating HTML\n\nmarkup, let’s use it for a more challenging project: an e-commerce website.\n\nThe requirements are that it should be a site with the following parts:\n\nA login page\n\nA user creation page\n\nA products page for products to put in a basket\n\nA basket page to show items you mean to purchase\n\nA checkout page showing your basket content and form for payment\n\ninformation and address info for shipping\n\nAn admin page that allows you to edit your products\n\nLet’s start with these pages as they are commonly found on e-commerce\n\nwebsites.\n\nLogin page The requirements for a login page are that you as a user can enter a\n\nusername and password and that there’s a button verifying your credentials.\n\nOptionally, you can add links for creating a user and a link if you forgot\n\nyour password.\n\nTry the following prompt as a comment at the top of a page called login.html:\n\n[Prompt]\n\n<!-- a page with input fields user name and password and a logi\n\n[End of prompt]\n\nYou may need to start typing <html> for the completion to\n\nbe generated. Also, ensure you close all files except for the\n\ncurrent one to make sure that your AI assistant is not\n\ndistracted by other files that might be open.\n\nYou should see an output similar to:\n\n[Prompt response]\n\n<html> <head> <title>Login</title> <link rel=\"stylesheet\" href=\"css/login.css\"> </head> <body> <div class=\"container\"> <div class=\"login\"> <h1>Login</h1> <form action=\"login.php\" method=\"post\"> <input type=\"text\" name=\"username\" placeholder= <input type=\"password\" name=\"password\" placehol <button type=\"submit\" class=\"btn btn-primary bt </form> <a href=\"register.php\">Create User</a> <a href=\"forgot.php\">Forgot Password</a> </div> </div>",
      "page_number": 87
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 106-125)",
      "start_page": 106,
      "end_page": 125,
      "detection_method": "synthetic",
      "content": "[End of response]\n\nThis renders like so:\n\nFigure 4.2: Login page\n\nProduct list page On this page, we want to see a list of products we can buy, some details on\n\neach product, and an “Add to Cart” button. For this, we will create a products.html page and give it the following prompt:\n\n<!-- list of products as div elements where each item has field\n\nYou should see an output similar to:\n\n[Prompt response]\n\n<!-- list of products as div elements where each item has field <html> <head> <title>Products</title> <link rel=\"stylesheet\" href=\"css/products.css\"> </head> <body> <div class=\"container\"> <div class=\"products\">\n\n<h1>Products</h1> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 1</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 2</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 3</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 4</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a>\n\n<button type=\"submit\" class=\"btn btn-primar </div> </div> </div> </div> </body> </html>\n\n[End of response]\n\nThis should render something like so:\n\nFigure 4.3: Product list page\n\nRemaining pages We will leave it as an exercise for you to produce the remaining pages.\n\nRemember to create a dedicated HTML page and put a prompt comment at the top of the page.\n\nHere’s some suggested prompts for the remaining pages:\n\nA user creation page.\n\nHere is a suggested prompt:\n\n<!-- a page with fields username, password, repeat password\n\nA basket page to show items you mean to purchase.\n\nHere is a suggested prompt:\n\n<!-- a page showing a list of items in a basket, each item\n\nA checkout page showing your basket content and form for payment\n\ninformation and address info for shipping.\n\nHere is a suggested prompt:\n\n<!-- a checkout page containing a section for payment info\n\nAn admin page that allows you to edit your products.\n\nHere is a suggested prompt:\n\n<!-- a section that's a list of products, each item has fields t\n\nAssignment\n\nIn this assignment, you will create a resume website. What context you\n\nprovide GitHub Copilot with is up to you but start by creating an index.html and an HTML comment, <!-- my prompt -->.\n\nRemember the techniques you were taught.\n\nWrite a prompt\n\nWrite a prompt and start typing the code/markup on the next\n\nline to help your assistant. Use the Return key to generate a\n\nresponse and the Tab key to accept the suggested text.\n\nRewrite the prompt and add or change what it says to get the desired result.\n\nYou can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/04\n\nChallenge Given your built resume, you can improve it further by adding colors. How\n\nwould you prompt to do so?\n\nQuiz Here’s a set of questions to ensure you’ve grasped the key concepts:\n\n1. The text you send to your AI assistant is called:\n\na. text\n\nb. instruction\n\nc. prompt\n\n2. Your AI assistant builds a context from:\n\na. what you type\n\nb. what you type, the file ending, and the open and closed files in\n\nyour working directory\n\nc. what you type and the file ending\n\nYou can find the solution to this quiz in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/04\n\nSummary In this chapter, we covered how to generate HTML markup using GitHub\n\nCopilot. We also covered how to use prompting techniques and how to add\n\ncontext to your prompts. As part of learning these prompting techniques, we\n\ndiscovered that the more context you give your AI assistant, the better the\n\noutput. You also build up context over time as you add more content to your\n\npage.\n\nAdditionally, we started on a use case where we started building an e-\n\ncommerce website. This use case is something we will continue to build on\n\nin the coming chapters.\n\nFor the next chapter, we will continue to cover web development but shift\n\nour focus to CSS and styling. You will see how the same or similar\n\nprompting techniques can be used for CSS as well.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n5\n\nStyle the App with CSS and Copilot\n\nIntroduction Styling an app well can make a huge difference in how a user perceives it.\n\nWell-thought-out styling includes catering to multiple devices, the smart use\n\nof graphics, and colors with great contrast.\n\nCSS styling is a big topic, and we will not cover it in detail. However, we\n\nwill show how you can start using it. Just like in the previous chapter, we\n\nwill use our AI assistant to help us generate code. You will see how we will\n\nkeep using comment-based prompting to generate code but also a new\n\ntechnique where nothing but the file’s context is used for code generation.\n\nYou will also see how we will keep building on our e-commerce project and\n\nstyle it.\n\nIn this chapter, we will see how we can generate the CSS we need and how to keep applying the prompting patterns and strategies we’ve used in\n\nprevious chapters. We will continue to build on the e-commerce project and\n\ngive it an appealing appearance.\n\nIn this chapter, we will:\n\nGenerate CSS: GitHub Copilot can generate styling, and we will\n\nshow how AI assistance can generate CSS both by looking at\n\nsurrounding code in a text file and based on a CSS comment.\n\nApply prompting techniques: There are different techniques we can\n\nuse to get the desired content.\n\nAdd CSS to our e-commerce project: We will select a couple of\n\npages in our e-commerce project to show how it benefits from styling.\n\nBusiness problem: e- commerce Just like the previous chapter, we will keep working in the e-commerce\n\ndomain and through its many interesting problems. As this chapter focuses\n\non visualization with CSS, what’s the connection to the business, you might\n\nwonder? A bad UX, or user experience, or an ill-designed site that doesn’t work on devices other than desktop or doesn’t cater to accessibility can cost\n\nyou money as, because of this, customers might choose to do business with your competitors.\n\nProblem and data domain This chapter continues with the e-commerce business domain and dives specifically into a basket page that lists products a customer aims to\n\npurchase. The data is therefore product data; not only that, but from a data aspect, we need to consider how to showcase detailed data relating to\n\nproducts, such as quantity and total cost, so the customer can decide what and how many items to buy. These considerations should be reflected in the chosen design.\n\nBreaking the problem down into features In the previous chapter, we chose to identify a larger area, “Authentication,”\n\nand break that down into specific features. Let’s recall what that feature\n\nbreakdown looked like. After that, we’ll see if we need to change it to\n\ninstead be more design oriented. But first, let’s show the list of features:\n\nArea: Authentication\n\nLog in: User should be able to log in.\n\nLog out: User should be able to log out.\n\nCreate new user: It should be possible to create a new user.\n\nUpdate password: An existing user should be able to update their password.\n\nReset password: If a user forgets their password, it should be possible\n\nto reset it in a safe way.\n\nThe above list of features constitutes a good high-level list of what we need to support. However, from a design viewpoint, we need to consider things like catering to different devices or support accessibility, for example.\n\nTherefore, a prompt for, let’s say, the first feature might need to be tweaked to look like so:\n\n[Prompt]\n\nGenerate a login page. It should have fields for username, password, and repeat password, as well as a login button. It should also support\n\naccessibility via tooltips and ARIA keys so that it can be used with just the keyboard and mouse.\n\n[End of prompt]\n\nAs you can see from the above prompt, our concern is not only with what\n\nUI elements we need, like inputs and buttons, but also how it should work.\n\nAs before, we recommend that you break down the web app you’re about to build into areas and each area into features so as to make prompting easier.\n\nPrompting strategy We mentioned in the previous chapter that you can both use the Chat mode of GitHub Copilot as well as typing inside of a file, and that you’re\n\nrecommended to use both approaches. As for how to prompt, you’re\n\nrecommended to write shorter prompts that you add context to as needed.\n\nBy doing so, you rely on how GitHub Copilot works and how it builds a runtime context based on its underlying model, what’s in your open\n\ndirectory, and your open file, among other things. You will also see another\n\nfacet of prompting that doesn’t rely on whether the prompt is long or short but rather on how your specific AI tool works. In this chapter, we use the\n\n“Exploratory prompt pattern” as described in Chapter 2.\n\nCSS, or Cascading Style Sheets CSS, or Cascading Style Sheets, is used to style HTML pages. It contains\n\nvarious constructs that allow you to change things such as fonts, sizes,\n\ncolors, and layouts.\n\nTo follow along with the below-mentioned prompts, you’re recommended to create a product.css file, or you can just\n\nwait until the Assignment section later in this chapter; it is up to you.\n\nFirst CSS To leverage our AI assistant, we can place a CSS-type comment in a CSS\n\nfile just like we do with code, for example:\n\n.product { /* apply padding and margin */ padding: 10px; margin: 10px; }\n\nAs was mentioned in the previous chapter, comment styles\n\nare different depending on the type of file. HTML comments look like so: <!-- -->. CSS comments use /**/\n\nand JavaScript understands both CSS-type comments and //. In this chapter, you will be using /**/ as you’re dealing with CSS.\n\nIn GitHub Copilot, you can keep pressing the Return key, followed by the Tab key, to generate and accept new suggestions without needing to type a\n\nnew prompt. Your AI assistant can provide suggestions based on the first\n\ntyped prompt and what code exists in your file.\n\nHow this affects our CSS file above is that once we type our first CSS comment, we can keep pressing Return and Tab and the AI assistant will\n\ngenerate CSS properties that usually go together with padding and margin.\n\nThe following output is the result of using this technique of pressing the Return and Tab keys and relying on the open file’s context.\n\nThis is how it looks for product.css:\n\n.product { /* apply padding and margin */ padding: 10px; margin: 10px; /* apply border */ border: 1px solid #ccc; /* apply border radius */ border-radius: 5px; /* apply background color */ background-color: #fff; /* apply box shadow */ box-shadow: 0 0 5px #ccc; /* apply display */ display: inline-block; /* apply width */ width: 200px; /* apply text align */ text-align: center; /* apply text decoration */ text-decoration: none; /* apply color */ color: #333; }\n\nNow that product.css is created with the above content, we can include\n\nsaid CSS file in an HTML file. Switch to said HTML file (if you’re typing\n\nalong, create an index.html file), then add the following prompt at the end of the new file:\n\n[Prompt]\n\nPage that includes product.css.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- page that includes product.css --> <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Product</title> <link rel=\"stylesheet\" href=\"product.css\"> </head> <body> <div class=\"product\"> some text <div class=\"product__image\"> <img src=\"https://via.placeholder.com/200x200\" alt= </div> <div class=\"product__info\"> <h2 class=\"product__name\">Product Name</h2> <p class=\"product__description\">Lorem ipsum dolor s <p class=\"product__price\">$9.99</p> <button class=\"product__buy\">Buy</button> </div> </div> </html>\n\n[End of response]\n\nObserve how it not only correctly adds a link tag to include product.css but it also generates HTML that might fit a product.\n\nThe following is the output:\n\nFigure 5.1: Product CSS showing a product image, title, and description\n\nCSS by name So far, we’ve added a comment to generate suggestions. But there’s another\n\nway we can generate suggestions for our CSS file rather than placing a\n\ncomment for Copilot to react to. The “other” way consists of creating an empty CSS class, .product, and placing the cursor inside of it. Then use the\n\nReturn key and the Tab key and see what Copilot generates until you’re\n\nhappy.\n\nHere’s how you start off your CSS file:\n\n.product {\n\n}\n\nNow place the cursor inside of the curly brackets, {}, and use the Return and Tab keys for each new line you want suggestions for.\n\nHere’s what the end result could look like:\n\n[Prompt response]\n\n.product { display: flex; flex-direction: column; align-items: center; justify-content: center; width: 100%; margin: 0 auto; padding: 0 0 50px 0; background-color: #f5f5f5; }\n\n[End of response]\n\nAssignment In this assignment, you will create an image gallery app. The gallery should\n\nshow 10 images with 5 images per row. Use what you learned in the\n\nprevious chapter to generate both an HTML file and a separate CSS file.\n\nUse “placeholder” as a keyword to get images.\n\nYour HTML file should add a reference to the generated\n\nCSS file to correctly apply the CSS.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nUse case: style the e- commerce app Now that you’ve carried out an assignment where you created both a CSS\n\nfile and an HTML file, you should have a better understanding of how to\n\nwrite prompts and generate and accept suggestions from GitHub Copilot.\n\nLet’s now recall the beginning of this chapter, where we talked about the\n\nbusiness problem and the e-commerce domain. We will now continue to\n\nsolve problems in this domain but focus on visualization using CSS.\n\nWe will continue with the use case we started with in the previous chapter:\n\nour e-commerce store. We will focus on the basket page and leave the\n\nremaining pages for you to implement.\n\nBasket page To style our basket HTML page, we create a basket.css file and then we\n\nobserve what CSS classes we introduced in the basket.html file. Let’s take\n\na look at a representative snippet:\n\n<div class=\"container\"> <div class=\"basket\"> <h1>Basket</h1> <div class=\"basket-item\"> <div> <h2>Product 2</h2> <p>Price: 200</p> <p>Quantity: 2</p> <p>Sum: 400</p> </div>\n\nIn the preceding code, we see the classes basket and basket-item. Let’s create CSS classes for those in basket.css:\n\nStart with a blank basket.css and start typing .basket. You should see output similar to the following when using the Return and Tab keys repeatedly. When you’re happy with the output, type } to finish the CSS\n\nclass.\n\n[Prompt response]\n\n.basket { width: 100%; height: 100%; display: flex; flex-direction: column; align-items: center; padding: 20px; }\n\n[End of response]\n\nTo create the basket-item class, type .basket-item and use Return and Tab repeatedly. Like before, type } when you’re happy with the number of\n\ngenerated CSS properties for this CSS class. You should now have\n\ngenerated a CSS output similar to the below text:\n\n.basket-item { width: 100%; height: 100%; display: flex; flex-direction: row; align-items: center; padding: 20px; border-bottom: 1px solid #ccc; }\n\nIf we continue looking at our HTML file, we will see yet another snippet of\n\ninterest:\n\n<div class=\"basket-item-buttons\"> <button type=\"submit\" class=\"btn btn-primary btn-block btn- <button type=\"submit\" class=\"btn btn-primary btn-block btn- </div>\n\nUse the same prompting technique as before by typing the name of the CSS class (.basket-item > .basket-item-button) and repeatedly using RETURN\n\nand TAB to generate the below text:\n\n.basket-item > .basket-item-buttons { display: flex; flex-direction: column; align-items: center; justify-content: center; margin-left: auto; } .basket-item-buttons button { margin: 5px; /* set width, large font size, business color background */",
      "page_number": 106
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 126-148)",
      "start_page": 126,
      "end_page": 148,
      "detection_method": "synthetic",
      "content": "width: 50px; font-size: 20px; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px; }\n\nYou may need to type the .basket-item-buttons button\n\nclass separately and, like before, use Return and Tab\n\nrepeatedly.\n\nViewing the impact of the applied CSS in a browser, you should see\n\nsomething similar to the below appearance:\n\nFigure 5.2: List of items in a shopping basket\n\nChallenge How would you change the prompts to create a dark-themed version of\n\nyour basket page?\n\nQuiz How you can generate CSS using your AI assistant?\n\n1. Create a comment in a CSS file.\n\n2. Create a class and place the cursor in the class.\n\n3. Both A and B.\n\nYou can find the solution to this quiz in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary In this chapter, we covered how you can generate CSS using your AI\n\nassistant. You saw how prompting techniques introduced in the previous\n\nchapters can be applied to CSS as well.\n\nFurthermore, we showed how we could generate text in two different ways,\n\nby placing a comment at the top of the file or near the area we wanted help\n\nwith or placing the cursor inside a CSS class and letting it generate CSS\n\nbased on the name of the CSS class.\n\nIn the next chapter, we will show how you can add behavior to your app\n\nusing JavaScript. You will see how JavaScript, from a prompting aspect, is\n\nsimilar to HTML and CSS. However, you still need to understand the\n\nsubject matter, which is the underlying problem you’re trying to solve.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n6\n\nAdd Behavior with JavaScript\n\nIntroduction It’s perfectly fine to have a web page consisting of nothing but HTML\n\nmarkup and CSS, but if you want interactivity, you need JavaScript.\n\nWith JavaScript, you can apply a little, for example, posting a form to a\n\nbackend, to a lot, like building a Single-Page Application (SPA) with a\n\nframework like Vue.js or React.js. Regardless, there’s a common\n\ndenominator, namely that you need to write code and reference that code or\n\ncode file from your HTML markup.\n\nYou will see that Copilot can help with both common tasks like adding a\n\nscript tag to your HTML markup to more advanced tasks like adding a\n\nJavaScript framework like Vue.js to your web app.\n\nIn this chapter, we will:\n\nGenerate JavaScript using prompts to add behavior to our app.\n\nAdd interactivity to our e-commerce application.\n\nIntroduce a JavaScript framework like Vue to ensure we set ourselves\n\nup with a solid foundation.\n\nBusiness problem: e- commerce We’ll keep working on our e-commerce domain in this chapter as well. In\n\nthe previous chapters, you saw how we worked with HTML to try to lay out\n\nwhat information should be on each page and identify what pages we need in the process. In this chapter, we’re adding the missing component,\n\nJavaScript, which is what makes it all work. JavaScript will play the roles\n\nof both adding interactivity and reading and writing data.\n\nProblem and data domain There are a few problems you need to address, as follows:\n\nData flow: How do we add code to our application so that we can read and write data?\n\nHandling user interaction: The user will want to interact with your\n\napplication. You will need to configure the part of the site that the user will want to use and ensure this will work. Not all user interactions\n\nlead to data being read or written, but many do, so therefore you need to figure out when that’s the case and “connect” a user interaction with\n\nyour data flow, as mentioned above.\n\nData: The data will vary depending on what parts of the app you’re\n\naddressing. If you implement a basket page, for example, you will need to deal with both product data as well as orders as the user is looking to “check out” their basket so they can purchase the products\n\nand get them delivered to a chosen address.\n\nBreaking the problem down into features We understand the business domain and roughly what type of problems\n\nwe’re likely to encounter, so how can we break this down into features? From the previous chapters, we have an idea of how to do this, but the main\n\ndifference is that instead of just creating a basket page, for example, that looks like it works, this should work. We can therefore break down a basket\n\npage, for example, into the following features:\n\nRead the basket information from the data source.\n\nRender the basket information.\n\nAdd the item to the basket.\n\nAdjust the selected number of items for a specific item in the basket.\n\nRemove an item from the basket.\n\nSupport checking out a basket, taking the user to an order page where they will be asked for purchase information and a delivery page.\n\nAn e-commerce site consists of many different pages. It’s therefore\n\nrecommended to do a similar feature breakdown for each page as you address a specific page.\n\nPrompting strategy The prompting strategy is a bit dependent on the chosen AI tool, how it\n\nworks, and how we prompt. GitHub Copilot is our chosen AI tool for this\n\nchapter, and we will focus mostly on its in-editor experience where you\n\ntype prompts in open files. The approaches we will show throughout this chapter are the following:\n\nInline comments: This means we will write prompts directly in open\n\nfiles. You can add prompts this way at the top of the file and also add additional supporting prompts where you’re trying to have your AI\n\nassistant build specific code blocks.\n\nChat: We will not show this approach that much, but we do\n\nrecommend using the chat just to compare and contrast or if you need\n\nto look up specific information or want advice on how to implement\n\nsomething.\n\nThe general recommendation throughout this book is that you use a blended approach of both using prompts directly in open files “in-editor” and typing\n\nprompts in the chat windows. These approaches come with their pros and\n\ncons. As for prompt pattern, we’re going to be using the “Exploratory prompt pattern” as described in Chapter 2.\n\nAdding JavaScript To add JavaScript to a web page, there are two main approaches:\n\nUsing a script tag in the head element: In this version, you would\n\nadd a script tag to an existing HTML page, for example, named index.html.\n\n<!-- Alt1 --> <script> // JavaScript here </script>\n\nPointing to a stand-alone file: In this case, you would write your\n\nJavaScript in an existing file, my-javascript-file.js:\n\n<!-- Alt2 --> <script src=\"my-javascript-file.js\"></script>\n\nThe role of JavaScript When JavaScript was first added to the browser, it played the role of adding\n\ninteractivity. Today, JavaScript often has an extended role as part of\n\napplication frameworks like Vue.js, React, and Angular. In such\n\nframeworks, a developer can now declaratively define templates that, with the help of JavaScript, can be turned into static HTML, with event handling\n\nadded, to respond to user interactions. Ultimately, all this is then sent to the\n\nbrowser to be rendered by it.\n\nAdding JavaScript to a page Let’s show next how we can add JavaScript to a page by providing the\n\nfollowing prompt:\n\n[Prompt]\n\nCreate web page with a script tag.\n\n[End of prompt]\n\nWe end up with a web page that also contains a script tag. Note how the\n\nprompt is added as a comment at the top of the page. That’s how GitHub\n\nCopilot knows to suggest a text completion:\n\n[Prompt response]\n\n<!-- create web page with a script tag --> <html> <head>\n\n<title>My First Web Page</title> </head> <body> <script> alert(\"Hello World!\"); </script> </body>\n\n[End of response]\n\nIn this case, the code calls alert(\"Hello World\"), which will show a dialog\n\nbox with the text “Hello World.”\n\nA second example: adding a JavaScript library/framework The first example might not be all that useful, but let’s add something\n\nyou’re likely to do, like adding a framework like Vue.js. The reason for adding a framework like Vue.js is that you want to build a large application\n\nconsisting of a lot of interactivity and many different pages. To keep things\n\norganized, and make it easy for other developers to contribute, you want a framework like Vue.js.\n\n[Prompt]\n\nAdd Vue 3.\n\n[End of prompt]\n\nThis time, we place ourselves in the header tag. See the HTML comment in the HEAD tag below to indicate the prompt we’re using as input to GitHub\n\nCopilot:\n\n<head> <!-- add Vue 3 --> </head>\n\nThis prompt results in the following suggestion:\n\n[Prompt response]\n\n<script src=\"https://unpkg.com/vue@next\"></script>\n\n[End of response]\n\nYou can easily build on this by adding a prompt that asks to set up an entry point (most SPA frameworks work on the idea of having an entry point, an\n\nelement where the app will be rendered):\n\n[Prompt]\n\nAdd Vue 3 entry point.\n\n[End of prompt]\n\nThis results in the following code:\n\n[Prompt response]\n\n<!-- add Vue 3 entry point --> <div id=\"app\"> <h1>{{ message }}</h1> </div>\n\n[End of response]\n\nPlacing our cursor below the script tag that imported Vue.js will lead to\n\nour AI assistant generating instructions for what code we should add to\n\nfurther set up Vue.js. It’s not enough to just add script to Vue.js; some\n\nadditional code is needed.\n\nLuckily, with our cursor placement like below, GitHub Copilot understands\n\nfrom existing HTML comments (prompts) what code is missing and will\n\nsuggest that code at the location of our cursor (if a Copilot suggestion\n\ndoesn’t appear, try pressing the Spacebar key or the Return key):\n\n[Prompt]\n\nNo prompt given\n\n[End of prompt]\n\n[Prompt response]\n\n<script src=\"https://unpkg.com/vue@next\"></script> <!-- below is rendered by just placing cursor here --> <script> // create Vue 3 instance const app = Vue.createApp({ data() { return { message: 'Hello World!' } } }) // mount Vue 3 instance to DOM element app.mount('#app') </script>\n\n[End of response]\n\nIt’s worth calling out again that GitHub Copilot had enough\n\ncontext to generate more suggestions by us only using Tab\n\nand Return key without the need to provide an extra\n\nprompt..\n\nRunning this in a browser returns “Hello World,” which is what’s expected\n\ngiven the above code. The reason this code works, in case you’re not\n\nfamiliar with Vue.js, is that it contains the bare minimum of what’s needed\n\nfor it to be referenced and configured:\n\nscript tag, referencing Vue.js\n\nCreation of an app instance that also defines a data function with a message property\n\nA call to mount(), which ensures that the identified HTML tag (with\n\nid = app) is interpolated, and the HTML tag is replaced with the value\n\nof the property message\n\nWhen building a more advanced Vue.js project, please use\n\nthe Vue CLI over the approach chosen above. For smaller\n\nproofs of concept, this approach could be OK.\n\nChallenge As a challenge, ask Copilot to add a button that increments a counter. You can use the following prompt:\n\n[Prompt]\n\nAdd a button that increments a counter.\n\n[End of prompt]\n\nIt is recommended that you place the cursor in the HTML markup where the DIV element with id=\"app\" is located.\n\nUse case: adding behavior For this part of our e-commerce use case, let’s work on rendering a cart\n\npage of items. In this case, we will mock calling a backend for the data and let the data be placed in a file called cart.json with the following structure:\n\n{ \"cart\": [\n\n] }\n\nThe above JSON content describes a JSON object (defined by {}), with the\n\nproperty cart, of the array type, as denoted by the square brackets, [].\n\nIn fact, let’s use our AI assistant to generate the data we need. Let’s create the cart.json file and the following structure and prompt:\n\n{ \"cart\": [ // item with id name , quantity and price, total = quantity ] }\n\nNote the prompt // item with id name, quantity and price, total =\n\nquantity * price, which results in the following JSON:\n\n{ \"id\": \"1\", \"name\": \"item1\", \"quantity\": \"1\", \"price\": \"10\", \"total\": \"10\" }, { \"id\": \"2\", \"name\": \"item2\", \"quantity\": \"1\", \"price\": \"20\", \"total\": \"20\" }, { \"id\": \"3\", \"name\": \"item3\", \"quantity\": \"1\", \"price\": \"30\", \"total\": \"30\" }\n\nAdding a comment with // is OK when you try to generate\n\nmore JSON code from Copilot, but it should be removed\n\nonce you’re happy with the output.\n\nNext, we need a file, app.js, that calls this JSON file and ends up\n\nrendering. We can use the basket.html file from the previous chapter. It should look something like so:\n\n<!-- a page showing a list of items in a basket, each item shou <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\">\n\n</head> <body> <div class=\"container\"> <div id=\"basket\" class=\"basket\"> </div> </div> <!-- add app.js --> <!-- add app.js, type javascript -->\n\n<script src=\"js/app.js\"></script> </body>\n\nLet’s add app.js by adding a prompt to the end of the body tag:\n\n[Prompt]\n\nAdd app.js.\n\n[End of prompt]\n\n[Copilot Response\n\n<!-- add app.js --> <script src=\"js/app.js\"></script>\n\n[End of response]\n\nIn app.js, add the following prompt:\n\n[Prompt]\n\nRead cart.json and render the content as div elements to the element with\n\nID basket. Add the CSS class basket-item to each item.\n\n[End of prompt]\n\nIt should give you a result similar to:\n\n[Prompt response]\n\nfetch('cart.json') .then(response => response.json()) .then(data => { let basket = document.getElementById('basket'); data.forEach(item => { let div = document.createElement('div'); div.classList.add('basket-item'); div.innerHTML = item.name; basket.appendChild(div); }); });\n\n[End of response]\n\nThe above code locates the HTML element whose ID is basket and then proceeds to loop through the array data. For each element in data, a new HTML element of type DIV will be created, and the following will happen\n\nto said element:\n\nThe CSS class basket-item will be added.\n\nIts innerHTML property will be assigned the value of the name property.\n\nThe specific element will be added to the parent element basket.\n\nYou need to tweak the forEach portion of the code to data.cart.forEach to\n\nadjust it based on what the JSON file looks like (if your JSON says you have a description property instead of name, that should be reflected in the JavaScript code, etc.). The resulting output doesn’t look like much, though,\n\njust three titles:\n\nitem1 item2 item3\n\nImproving the output To improve the output, there are a few things we want to do, like:\n\nAdd a CSS class to each rendered property on an item:\n\n.basket-item > div { margin: 5px; /* 25% width */ width: 25%; }\n\nYou don’t always need to type a prompt to get the\n\nabove response. Sometimes it’s enough to type .basket-item and CSS will be suggested to you that you can accept using the Tab key. Be sure that you are in either a style tag or a .css file when writing this,\n\nthough.\n\nAdd prompts in the JavaScript code to make it render each item property as separate div elements:\n\n[Prompt]\n\n//Read cart.json and render the content as div elements to the\n\nelement with id basket, add CSS class basket-item to each item\n\n[End of prompt]\n\n[Prompt response]\n\nfetch('cart.json') .then(response => response.json())\n\n.then(data => { let basket = document.getElementById('basket');\n\n[End of response]\n\nAt this point, the above prompt has managed to call the fetch method\n\nto retrieve data from the cart.json file. It then proceeded to convert the retrieved data into JSON format and identified the HTML element\n\nwe will want to render the JSON response on:\n\n// render name, quantity, total and price for each ite data.cart.forEach(item => { let div = document.createElement('div'); div.classList.add('basket-item'); // 1) render name as separate child div element let name = document.createElement('div'); name.innerHTML = item.name; div.appendChild(name); // 2) render quantity, price and total as separate let quantity = document.createElement('div'); quantity.innerHTML = `Quantity: ${item.quantity}`; div.appendChild(quantity); let price = document.createElement('div'); price.innerHTML = `Price: ${item.price}`; div.appendChild(price); let total = document.createElement('div'); total.innerHTML = `Total: ${item.price * item.quan div.appendChild(total); basket.appendChild(div); });\n\nIn this section of the code, we’ve added context on what fields (name,\n\nquantity, etc.) we want to read from the JSON content. From the\n\ncode, you can see this prompt instruction is reflected in the generated code and each field gets its own DIV element and descriptive text:\n\n// 3) calculate total price for all items and render i let totalPrice = document.createElement('div'); totalPrice.innerHTML = `Total price: ${data.cart.reduc basket.appendChild(totalPrice); });\n\nIn this prompt, we’re providing specific instructions to multiply certain\n\nfields (price x quantity) to produce a total price for the row. At the same time, we can see how a call to reduce calculates the basket’s\n\ntotal price.\n\nThe end result looks like so. It’s far from perfect but looks more like what\n\nwe can expect:\n\nFigure 6.1: Rendered cart showing name, price, and total\n\nAdding Bootstrap\n\nIn the previous section, we showed how we can add some basic CSS to our\n\ncart page. However, to really improve the looks of our basket page, we\n\nthink adding a CSS framework like Bootstrap can be a good idea. Bootstrap\n\nis used by many teams across the world and has many different types of\n\ncontent, like “cards” and “menus,” that will enable us to work faster in\n\nbuilding our app.\n\nTo add Bootstrap, we place ourselves with a prompt comment in the head\n\nelement like so:\n\n[Prompt]\n\nAdd Bootstrap.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- add bootstrap --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.c </head>\n\n[End of response]\n\nAt this point, we have added Bootstrap to our web app, but we need to use\n\none of its classes. Previously, we mentioned the concept of “cards,” when\n\nwe discussed Bootstrap and its types of content, a layout that renders nicely.\n\nTo produce a card, we can give it a prompt like so on the web page:\n\n[Prompt]\n\nCreate a card with the fields name, price, quantity, and total.\n\n[End of prompt]\n\nWe get the following result:\n\n[Prompt response]\n\n<!-- Create a card with the fields name, price, quantity and to <div class=\"card\"> <div class=\"card-body\"> <h2 class=\"card-title\">Product 1</h2> <p class=\"card-text\">Price: 100</p> <p class=\"card-text\">Quantity: 1</p> <p class=\"card-text\">Sum: 100</p> <button type=\"submit\" class=\"btn btn-primary btn-block <button type=\"submit\" class=\"btn btn-primary btn-block </div> </div>\n\n[End of response]\n\nAbove, we placed the prompt in our open text file as an HTML comment\n\nand accepted Copilot’s suggestion via the Tab key.\n\nOK, we had a good start. We have added Bootstrap to our web app. We also\n\nused Copilot to create an HTML card sample that we can use. The next step\n\nis to change our JavaScript code.\n\nTo change our JavaScript code, we will give Copilot a bit more context than\n\nusual, namely:\n\n[Prompt]\n\nSee below:\n\n// template: // <div class=\"card\"> // <div class=\"card-body\"> // <h2 class=\"card-title\">Product 1</h2> // <p class=\"card-text\">Price: 100</p>\n\n// <p class=\"card-text\">Quantity: 1</p> // <p class=\"card-text\">Sum: 100</p> // <button type=\"submit\" class=\"btn btn-primary btn-blo // <button type=\"submit\" class=\"btn btn-primary btn-blo // </div> // </div> // for each item in cart.json, render name, quantity, total, us\n\n[End of prompt]\n\nThis results in the following code:\n\n[Prompt response]\n\ndata.cart.forEach(item => { // target element with id basket let basket = document.getElementById('basket'); let div = document.createElement('div'); div.classList.add('card'); basket.appendChild(div); let divBody = document.createElement('div'); divBody.classList.add('card-body'); div.appendChild(divBody);\n\nlet name = document.createElement('h2'); name.classList.add('card-title'); name.innerHTML = item.name; divBody.appendChild(name); let price = document.createElement('p'); price.classList.add('card-text'); price.innerHTML = `Price: ${item.price}`; divBody.appendChild(price); let quantity = document.createElement('p'); quantity.classList.add('card-text'); quantity.innerHTML = `Quantity: ${item.quantity}`; divBody.appendChild(quantity); let total = document.createElement('p'); total.classList.add('card-text'); total.innerHTML = `Total: ${item.price * item.quantity}`; let buttonPlus = document.createElement('button');\n\nbuttonPlus.classList.add('btn', 'btn-primary', 'btn-block', ' buttonPlus.innerHTML = '+'; divBody.appendChild(buttonPlus); let buttonMinus = document.createElement('button'); buttonMinus.classList.add('btn', 'btn-primary', 'btn-block', buttonMinus.innerHTML = '-'; divBody.appendChild(buttonMinus); });\n\n[End of response]\n\nAll of this felt a tiny bit convoluted, going back and forth between\n\nJavaScript and HTML. This is the reason why many developers use a\n\nJavaScript framework in which they can augment the HTML, using\n\ntemplates, and just add instructions to it, like you do when you, for example, use a framework like Vue.js.\n\nAdding Vue.js At this point, we’ve realized that our approach wasn’t so good. We let\n\nJavaScript handle the creation of HTML elements and populate them from a\n\nJSON response. What we’re looking for is a cleaner approach. We will\n\ntherefore start from the beginning. However, because we’ve been working with Copilot, it “remembers” that we’re working with basket-type data,\n\nand the file we’re working with, basket.html, helps provide additional context to Copilot. It might seem a bit like magic, what you’re about to see,\n\nbut it’s really a result of Copilot having built up an in-memory context\n\nhaving worked with you.\n\nIf you’re starting fresh with just having opened Visual\n\nStudio Code, you would probably need to use a more",
      "page_number": 126
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 149-171)",
      "start_page": 149,
      "end_page": 171,
      "detection_method": "synthetic",
      "content": "detailed prompt.\n\nIt might sound like a big change to add a JavaScript framework at this\n\npoint, but it really isn’t. Let’s show the needed changes step by step.\n\nFirst, let’s add support for Vue.js in our basket.html file. We add the below\n\nprompt at the end of the BODY tag:\n\n[Prompt]\n\nAdd Vue 3.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- add Vue 3 --> <script src=\"https://unpkg.com/vue@next\"></script>\n\n[End of response]\n\nThe next thing we need is code that creates our Vue app.\n\n[Prompt]\n\nAdd Bootstrap code for Vue 3.\n\n[End of prompt]\n\nThe resulting code is as follows:\n\n[Prompt response]\n\n<!-- add bootstrap code for Vue 3 --> <script> let app = Vue.createApp({ data() {\n\nreturn { basket: [ { id: 1, name: 'Product 1', price: 100, quantity: 1 }, { id: 2, name: 'Product 2', price: 200, quantity: 2 }, { id: 3, name: 'Product 3', price: 300, quantity: 3 }, { id: 4, name: 'Product 4', price: 400, quantity: 4 } ] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { let total = 0;\n\nfor (let i = 0; i < this.basket.length; i++) { total += this.basket[i].price * this.basket[i]. } return total; } } }); app.mount('#basket'); </script>\n\n[End of response]\n\nRemember what we said about the in-memory context of\n\nCopilot. It remembers everything we’ve fed it so far on\n\nwhat fields the basket has, and it also looks at the name of the file basket.html; that’s the result you see above.\n\nLastly, we need to state how we want to render the app:\n\n[Prompt]\n\nRender basket from Vue app.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- render basket from Vue app --> <div v-for=\"(item, index) in basket\" class=\"basket-item\"> <div class=\"basket-item-text\"> <h2>{{ item.name }}</h2> <p>Price: {{ item.price }}</p> <p>Quantity: {{ item.quantity }}</p> <p>Sum: {{ item.price * item.quantity }}</p> </div> <div class=\"basket-item-buttons\">\n\n<button type=\"submit\" class=\"btn btn-primary btn-block <button type=\"submit\" class=\"btn btn-primary btn-block </div> </div>\n\n[End of response]\n\nThe cart will look like this:\n\nFigure 6.2: Display of cart using Vue.js\n\nIf you were to try out the page at this point and run it in a\n\nbrowser, you would notice how the + and - buttons\n\nincrement and decrement products.\n\nAt this point, what’s missing is ensuring our Vue app uses the Bootstrap\n\nCSS library. We can address that by modifying our prompt for generating\n\nthe HTML markup from:\n\n[Prompt]\n\nRender basket from Vue app.\n\n[End of prompt]\n\nWe can change it to a prompt with added context on Bootstrap, like so:\n\n[Prompt]\n\nRender basket from Vue app, using Bootstrap and the card class.\n\n[End of prompt]\n\nThis results in the following HTML markup:\n\nFigure 6.3: Cart in HTML markup\n\nAssignment Create a product list page. The page should show a list of products. Each\n\nproduct should have an Add button that will add the product to a cart. The\n\ncart should be represented as a cart icon in the top-right corner of the page\n\nand when clicked should display the number of items in the cart and the\n\ntotal value.\n\nUse what you’ve been taught to craft a prompt to create the\n\npage, add JavaScript, and more. It’s up to you if you want to\n\nadd Vue.js to solve this or if you want to use plain\n\nJavaScript.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and- ChatGPT.\n\nSummary In this chapter, we’ve shown how to add JavaScript to a web page. Adding\n\nJavaScript to a web page is a common task and can be done in two ways, either by adding a script tag to the head element or by pointing to a standalone file.\n\nWe’ve also shown how we can build on our use case from previous chapters\n\nby adding behavior to our app. We first showed how to even let JavaScript\n\ngenerate the markup, which can become a bit unwieldy. Then, we made the\n\ncase for using a JavaScript framework like Vue.js to make it easier to\n\nmanage.\n\nYou’ve also seen how you can add a JavaScript framework like Vue.js.\n\nExactly how you add a JavaScript framework varies by framework but it’s\n\ngenerally recommended to add a prompt with wording including keywords like setup or initialize to ensure you not only add a script tag but also add\n\ncode that triggers a setup process and makes the selected framework ready to use.\n\nIn the next chapter, we will show how we can add responsiveness to our app\n\nand cater to many different devices and viewports. We can no longer\n\nassume that everyone is using a desktop computer with a large screen.\n\nMany of our users will be using a mobile device with a smaller screen.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n7\n\nSupport Multiple Viewports Using Responsive Web Layouts\n\nIntroduction Building web pages is a challenge. Not only do you need to craft these\n\npages with HTML, CSS, and JavaScript to perform the tasks you set out,\n\nbut you also need to ensure they are accessible to most users. Additionally,\n\nyou need to ensure the pages render nicely regardless of whether the device\n\nis a PC, tablet, or mobile device, which means you need to consider aspects\n\nlike screen size; the orientation of the device; that is, landscape or portrait;\n\nand pixel density.\n\nThere are many different techniques to ensure your web pages look good on\n\nmany devices, but it all starts with having a strategy, a vision for what the\n\nexperience will be for the user depending on what device is used. Once you have that vision set, you can start to implement it.\n\nSome choices you will need to make are how many columns should be\n\nlisted, if your content is presented as columns. How should other things behave, like padding and margins? Should the content be centered or left-\n\naligned? Should the content be stacked vertically or horizontally? Is there\n\ncontent that should be hidden on mobile devices? As you can see, there are\n\nmany choices to make that affect what prompts you will need to use.\n\nUsing an AI assistant can be helpful when dealing with web layouts as there’s a lot of information you need to remember, so not only can you have\n\nthe AI assistant remember all those details for easy lookup, but you can also\n\nutilize it to suggest different designs.\n\nIn this chapter, we will:\n\nExplain technical terms like viewports and media queries.\n\nApply different techniques to optimize rendering for different\n\nviewports.\n\nLeverage the Copilot chat feature to improve our code. This is the\n\n“other” modality you can use in GitHub Copilot; it’s a chat window\n\nthat lets you type the prompt and provides a response. This experience\n\nresembles an AI tool like ChatGPT.\n\nBusiness problem: e- commerce This chapter will continue to address the e-commerce use case that’s been\n\nworked on in the last three chapters. Building the functionality is one thing, but you must assume your users will want to interact with your website from many different devices and that experience must be good or they will\n\ngo to a competitor’s website.\n\nProblem and data domain\n\nThere are many different devices out there: tablets, mobile phones, and\n\nsmall desktop screens to large ones. Pixel density is different. It’s not just a matter of shrinking or scaling up your site to fit this new device, but you might need to design a completely different experience that better suits the\n\nvisual style of a specific device. There are also other concerns, like how much content we want to send to a smaller device if we assume the device\n\nhas limitations, like how many concurrent downloads it can handle and what network speed it might have. It’s not unusual that a desktop machine with a wide resolution often has a great connection to the internet.\n\nConversely, a mobile device might be on a 3G network or worse and you therefore need to adjust to that by requiring a lot fewer graphical resources,\n\nsmaller JavaScript bundles, and more.\n\nBreaking the problem down into features We’ve seen in several chapters before this one how a good approach is identifying the features we need to implement. These features are less about\n\nreading and writing data and more about ensuring the design and interaction work well on prioritized devices. You might therefore have a feature\n\nbreakdown looking like the following list:\n\nShould render the basket page in a double-column design for landscape mode.\n\nPortrait mode:\n\nShould render the basket page in a single column for portrait mode.\n\nShould display menu actions at the bottom of the screen.\n\nYou should hide certain features, say, X, Y, Z (assuming X, Y, Z\n\nare available on a desktop with a wider screen). The point of this requirement is that you must “rethink” what a mobile experience is versus desktop, what features are central to the experience, and what features we only show if we have plenty of screen\n\nspace to show it on.\n\nShould support and render a visually appealing look for the\n\nfollowing mobile devices: iPhone, X, Y, X, and Android.\n\nShould render the page in < 1 second on a 3G connection.\n\nAs you can see, the features are more connected to the user experience than\n\nany data domain.\n\nPrompting strategy Our prompting strategy is like before, a blended approach of using the in-\n\neditor experience and adding prompts in open text files to bring up the Chat\n\nwindow in Copilot; mix these approaches to your discretion.\n\nAs for prompts, there should be enough context in these prompts to make\n\nCopilot aware that it will need to suggest a design for specific devices. Thus, it should be able to infer from context what resolutions, pixel density,\n\nand other details should influence the suggestions it’s about to generate. As\n\nfor prompting pattern used, we will use the “Exploratory prompt pattern” described in Chapter 2.\n\nViewports Gone are the days when you only had to develop a web page to look nice on\n\na PC. Today, your web page can be rendered on multiple different devices\n\nand it needs to look good on all of them or your customers might go elsewhere.\n\nThe first step in understanding how to build web pages is being familiar\n\nwith some key concepts. The first concept is a viewport. A viewport is a\n\npart of the page visible to the user. The difference between a viewport and a window is that a viewport is a part of the window.\n\nDepending on what device is used, for example, a desktop screen or a\n\nmobile device, its size differs. When you write code to adjust to different\n\nsizes, to render nicely, it’s known as making the page “responsive.”\n\nMedia queries OK, so I’m dealing with different sizes of the screen depending on what\n\ntype of device I’m using, so how do I write code that ensures the visual\n\ninterface adjusts to the size of the device I’m using?\n\nThe answer is to leverage a construct called media queries. A media query is a logical block in your CSS that identifies a specific condition and applies\n\nspecific CSS if said condition is true.\n\nImagine if there were code like the following; that’s basically how it works:\n\nif(page.Width > 1024px) { // render this UI so it looks good for desktop } else { // render this UI to look good for a mobile device }\n\nBelow is an example of a media query:\n\nbody { background: blue; } @media (max-width: 600px) { body { background-color: lightblue; } }\n\nThe preceding code identifies a condition that says, if the viewport is\n\ncurrently at most 600 pixels wide (which is true for most mobile devices), then set the background color to light blue.\n\nThis example might feel a bit contrived; why would I want a different\n\nbackground color when I’m on a mobile device over a normal desktop? You\n\nwouldn’t, but the example above gives you an idea of how a media query identifies a viewport’s size and can apply specific CSS under certain\n\nconditions on the viewport.\n\nWhen to adjust to diﬀerent viewports and make it responsive A good reason for using responsive design is because you have a layout that\n\nlooks great on desktop but might be too wide for mobile. For example, let’s\n\nsay you have a web page with a menu to the left and a main area to the right:\n\nFigure 7.1: Page with a menu and a main area\n\nWere we to try to view this page on a mobile device, it would not look nice.\n\nIt would look something like this:\n\nFigure 7.2: Non-responsive page on a mobile device\n\nAbove, we see how the device tries to view the page, but the main area is\n\ncut off. At this point, you as a designer need to consider how to solve this\n\nissue. One way is to align the content vertically using either flexbox or grid\n\nas techniques. The menu could go on top, for example, and the main area at\n\nthe bottom.\n\nInspecting the styles used for this page, you see the following CSS:\n\n<style> /* container, grid, left col is menu, right col is main are .container { max-width: 1200px; margin: 0 auto; display: grid; /* 2 columns */ grid-template-columns: 200px 1fr; background-color: #000; padding: 10px; /* space 5 px between columns */ grid-gap: 10px; /* height 100% */ height: 100%; }\n\nIn the preceding code, you can see how a grid, display:grid, is used to create the layout:\n\n/* menu items */ .menu { /* background black shade */ background-color: darkslategrey;\n\npadding: 0px; /* column 1 */ grid-column: 1;\n\n}\n\nYou can also see how the menu is placed to the left, grid-column: 1:\n\n/* menu item */ .menu-item { padding: 5px; background-color: darkslategrey; height: 20px; } .menu-item a { color: white; text-decoration: none; font-family: sans-serif; } /* main area, padding 5px, dark grey */ .main { padding: 5px; background-color: darkslategrey;\n\ngrid-column: 2; color: white; font-family: sans-serif; }\n\nThe main area is the second column, grid-column: 2:\n\n</style>\n\nTo improve this design so that it can be used on a device with a smaller\n\nscreen, such as a mobile device, we can use the below prompt:\n\n[Prompt]\n\nIf mobile, set menu to row 1 and main to row 2.\n\n[End of prompt]\n\nThe resulting CSS is as follows:\n\n[Prompt response]\n\n@media (max-width: 768px) { .container { grid-template-columns: 1fr; grid-template-rows: 200px 1fr; } .menu { grid-row: 1; grid-column: 1; } .main { grid-row: 2; grid-column: 1; } }\n\n[End of response]\n\nAs you can see, Copilot can take this prompt instruction and adjust the CSS\n\naccordingly. The code shifts the layout to having one column and two rows.\n\nAlso, it ensures the menu is placed at the top and the main area at the\n\nbottom:\n\nThe resulting page now renders in the following way on a mobile device:\n\nFigure 7.3: Menu and main area rendered vertically in mobile viewport\n\nMost browsers have built-in functionality for letting you resize the\n\nviewport, which helps you test your altered CSS. You can also change the\n\nwindow size.\n\nUse case: make our product gallery responsive In this use case, we will take an e-commerce application that renders OK on\n\na desktop but not OK on a mobile device, and fix the issue.\n\nFirst, here’s the HTML code for it. If you want to follow along, you can\n\nsave the following code in a products.html file:\n\n<html> <head> <title>menu</title> <link rel=\"stylesheet\" href=\"css/style.css\"> <style> /* container, grid, left col is menu, right col is main .container { max-width: 1200px; margin: 0 auto; display: grid; /* 2 columns */ grid-template-columns: 200px 1fr; background-color: #000; padding: 10px; /* space 5 px between columns */ grid-gap: 10px; /* height 100% */ height: 100%; } /* menu items */ .menu { /* background black shade */ background-color: rgb(25, 41, 41);\n\n/* background-color: #ddd; */ padding: 0px; /* column 1 */ grid-column: 1;\n\n} /* menu item */ .menu-item { padding: 5px; background-color: rgb(25, 41, 41); height: 20px; } .menu-item a { color: white; text-decoration: none; font-family: sans-serif; }\n\n/* main area, padding 5px, dark grey */ .main { padding: 5px; background-color: rgb(25, 41, 41);\n\ngrid-column: 2; color: white; font-family: sans-serif; } /* if mobile, set menu to row 1 and main row 2 */ @media (max-width: 768px) { .container { grid-template-columns: 1fr; grid-template-rows: 200px 1fr; } .menu { grid-row: 1; grid-column: 1; } .main { grid-row: 2; grid-column: 1; } } /* gallery, 2 columns per row */ .gallery { display: grid; /* horizontal grid */ grid-template-columns: auto auto auto; grid-gap: 20px; } /* gallery item */ .gallery-item { flex: 1 0 24%; margin-bottom: 10px; /* padding 10px */ padding: 20px; /* margin 5px */ margin: 5px; /* black shadow */ box-shadow: 0 0 10px 0 black;\n\n} /* gallery image */ .gallery-image { width: 100%; height: auto;\n\ntransition: transform 0.3s ease-in-out; } /* gallery image hover */ .gallery-image:hover { transform: scale(1.1); }\n\n</style> </head> <body> <div class=\"container\"> <!-- menu items --> <div class=\"menu\"> <div class=\"menu-item\"> <a href=\"index.php\">Home</a> </div> <div class=\"menu-item\"> <a href=\"about.php\">About</a> </div> <div class=\"menu-item\"> <a href=\"contact.php\">Contact</a> </div> <div class=\"menu-item\"> <a href=\"gallery.php\">Gallery</a> </div> <div class=\"menu-item\"> <a href=\"login.php\">Login</a> </div> </div> <!-- main area --> <div class=\"main\"> <div class=\"gallery\"> <div class=\"gallery-item\"> <img class=\"gallery-image\" src=\"https://picsum. <h4>Product 1</h4> <p>Description</p>",
      "page_number": 149
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 172-193)",
      "start_page": 172,
      "end_page": 193,
      "detection_method": "synthetic",
      "content": "<p>Price</p> <button>Add to cart</button> </div> <div class=\"gallery-item\"> <img class=\"gallery-image\" src=\"https://picsum. <h4>Product 2</h4> <p>Description</p> <p>Price</p> <button>Add to cart</button> </div> <!-- code shortened --> </div> </div> </body> </html>\n\nThis should render something like the following (exact images may vary as\n\nthese URLs produce random images) on desktop:\n\nFigure 7.4: E-commerce product list page\n\nHowever, trying to render the same page on a mobile device makes it look\n\nlike the following:\n\nFigure 7.5: E-commerce product list, looks bad on mobile\n\nTo solve this problem, we need to place ourselves in the CSS code and ask\n\nour AI assistant what we should do.\n\nPlace a prompt at the bottom of the CSS like so:\n\n[Prompt]\n\nSwitch from 3 columns to 1 on mobile device for gallery.\n\n[End of prompt]\n\nThe result should be a media query like so:\n\n[Prompt response]\n\n@media (max-width: 768px) { .gallery { grid-template-columns: auto; } }\n\n[End of response]\n\nOur new mobile rendering now looks like the image below, which is\n\nacceptable.\n\nFigure 7.6: This shows an image gallery rendered on a mobile device in portrait mode\n\nAssignment\n\nAs a newly hired frontend developer, you’ve been hired to maintain a\n\nmemory game.\n\nThe game looks something like the below image:\n\nFigure 7.7: Grids in a memory game\n\nYour company wants you to do the following:\n\nEnsure it renders as a 5x5 grid on desktop. For larger viewports, it\n\ndoesn’t work well, but you should address that problem.\n\nSupport mobile devices, meaning that it should render as a 5x5 grid\n\nbut with half as big tiles.\n\nWhile fixing it for mobile devices, ensure the score in the top-right\n\ncorner is moved to the middle and is centered.\n\nAs a developer, it’s now your job to adjust the code of this game using\n\nGitHub Copilot, using either inline editing of open text files or the Chat\n\nfunction in Copilot to ensure the code works well for different devices.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nChallenge All the code for the assignment is in one file. See if you can split it up into\n\ndifferent files. Additionally, see if you can experiment with matched cards;\n\ntry removing them or adding a class that shows they’re no longer part of the\n\ngame.\n\nSummary In this chapter, we discussed viewports as the central concept for responsive\n\nweb design. To help us tackle different viewports, we used media queries.\n\nWe also continued working on our use case, the e-commerce site, and tried\n\nto ensure a product list renders nicely on mobile devices. The first thing is\n\nto realize that you have a problem, and we managed to identify that.\n\nSecond, we came up with a strategy to solve the problem, which was to use\n\nmedia queries. Third, we implemented the strategy. Finally, we tested it to\n\nensure it worked.\n\nIn the next chapter, we will shift from the frontend to the backend. The\n\nbackend is made up of a Web API. We will continue with our use case, the\n\ne-commerce site, and build a Web API that will serve the product list\n\nprimarily. Hopefully, though, it will become apparent how to add other\n\nresources to the Web API as well.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n8\n\nBuild a Backend with Web APIs\n\nIntroduction When we say Web API, it’s an application programming interface we\n\ndevelop that’s meant for the client to consume. Said API uses HTTP to\n\ncommunicate. A browser can use a Web API to expose data and\n\nfunctionality to other browsers and applications.\n\nWhen developing a Web API, you can use any programming language and\n\nframework you want. Regardless of the chosen tech, there are things you\n\nalways need to consider, like data storage, security, authentication,\n\nauthorization, documentation, testing, and more.\n\nIt’s with this understanding of what things we need to consider that we can\n\nuse an AI assistant to help us build a backend.\n\nIn this chapter, we will:\n\nLearn about Web APIs\n\nCreate a Web API with Python and Flask\n\nUse our AI assistant to answer questions, suggest code, and create\n\ndocumentation and tests\n\nBusiness domain: e- commerce We will keep working on our e-commerce example in this chapter. This\n\ntime, the focus is on the API. The API lets you read and write data that’s\n\nimportant in the e-commerce domain. What’s important to keep in mind as you develop this API is that there are a couple of important aspects to it:\n\nLogical domains: It’s beneficial to divide up your app into different\n\nlogical domains. Within the context of e-commerce, that usually\n\ntranslates to products, orders, invoices, and so on.\n\nWhat part of the business should handle each logical domain?\n\nProducts: Maybe there’s a dedicated team. It’s common for the\n\nsame team to also manage all types of discounts and campaigns\n\nthat might occur.\n\nInvoices and payment: There’s usually a dedicated team that\n\ntakes care of how the user can pay for things, for example, via credit cards, invoices, and other methods.\n\nInventory: You need to have a certain amount of goods in stock. How do you know how much? You need to work with business\n\nanalysts or data folk to make correct forecasts.\n\nProblem and data domain We’ve already mentioned a few different logical domains around products,\n\norders, invoices, and so on. The problems you’ll have in this domain are generally:\n\nReading and writing: What data do you wish to read or write (or\n\nmaybe both)?\n\nHow will users access your data (all of it or maybe there will be filters applied to limit the output)?\n\nAccess and roles: You can expect that different roles will need to have access to your system. An administrator role should probably have\n\naccess to most of the data, whereas a logged-in user should only be able to see the part of the data that belongs to them. This is not something we will address in this chapter, but it’s something you\n\nshould consider when you build out this API.\n\nFeature breakdown Now that we understand that there are both business problems as well as\n\ndata problems, we need to start to identify the features that we need. Once we get to this level of detail, it should be easier to come up with specific prompts.\n\nA way to do this feature breakdown is as follows – for example, for\n\nproducts:\n\nRead all products.\n\nRead products given a filter: Usually, you won’t want to read all\n\nproducts but maybe all products of a certain category, or maybe even limit it to a specific value such as 10 products or 20 products.\n\nSearch for products: You should support the user looking for specific\n\nproducts, usually via a category, name, or perhaps part of a certain campaign.\n\nRetrieve detailed information on a specific product.\n\nI’m sure there are more features for products, but now you have an idea of\n\nwhat granular detail you should have before you continue building the API.\n\nPrompt strategy In this chapter, you will see how we use both Copilot Chat and the in-editor\n\nmode. We will start with the Chat mode as it’s quite useful for situations where you want to generate starter code. It’s also quite efficient in that it\n\nlets you select certain lines of code and lets you update only those based on\n\na prompt. Examples of the latter could be when you want to improve such\n\ncode. You will see this use case later in the chapter when we improve a route to read from a database instead of reading static data from a list. There\n\nwill also be cases in this chapter where we use the in-editor mode. This is\n\nthe recommended approach when you’re actively typing the code and want to make smaller tweaks. In this chapter, we will use the “Exploratory\n\nprompt pattern” as described in Chapter 2.\n\nWeb APIs Using a Web API is a great way to ensure our front-end application has\n\naccess to the data and functionality it needs to read and write data.\n\nThe expectations of a Web API are:\n\nIt is accessible over the web.\n\nIt leverages HTTP protocol and HTTP verbs such as GET, POST, PUT,\n\nDELETE, and others to communicate intentions.\n\nWhat language and framework should you pick? In this chapter, we already decided we will use Python and Flask. But why? What criteria do we use to pick a language and framework?\n\nYou can use any language and framework you want, but here are some\n\ncriteria to consider:\n\nWhat languages and frameworks do you know?\n\nAre they easy to learn?\n\nDo they have a large community?\n\nAre they free and open source?\n\nHow often are they updated?\n\nDo they have good documentation?\n\nDo they have good tooling?\n\nThese are just some of the criteria to consider.\n\nThe reason for picking Python and Flask is that they check many of the\n\nabove boxes (so does the Express framework for Node.js, but the objective\n\nhere is to just show how you can build a Web API using an AI assistant, so feel free to use whatever web framework you prefer). Also, the point of this\n\nbook is to show how an AI assistant can help us build a backend; with what\n\nprompts and how and the framework and language is not the focus.\n\nPlanning the Web API When you plan your Web API, you should consider the following:\n\nWhat data do you want to expose? For example, products and orders.\n\nWhat functionality do you want to expose? For example, reading order data.\n\nHow will you structure your Web API?\n\nSecurity and authentication: You need to determine not only what areas\n\nof your app should require the user to log in but also what parts should be restricted to specific user types.\n\nStorage and database: Common choices are, for example, MySQL and\n\nPostgres.\n\nUse the above points as a checklist when you plan your Web API.\n\nCreating a Web API with Python and Flask A key insight to working with an AI assistant is that we can use it to\n\ngenerate code, but we need to have a good understanding of the problem\n\ndomain and the solution domain. This means that we should know how to\n\ncreate a Web API with Python and Flask before we ask our AI assistant to help us. Can we create it without an AI assistant? Yes, but we risk getting\n\nstuck and not knowing what to do next.\n\nSo, how much knowledge are we talking about? If you know Python in\n\ngeneral and how to build a Web API in any language, you are good to go.\n\nLet’s begin.\n\nStep 1: Create a new project First, we need to create a new project. If you know Python, you know using\n\na virtual environment is a good idea as it isolates the project from other\n\nPython projects on your computer. Using a virtual environment is not required but is recommended as different versions of Python and packages\n\ncan cause issues.\n\nOkay, so we know we need a virtual environment. How do we create one?\n\nLet’s rely on our AI assistant to help us remember the syntax. For the prompt below, you can either open a text file or use the chat functionality in\n\nGitHub Copilot.\n\n[Prompt]\n\nHow do I create a virtual environment for Python?\n\n[End of prompt]\n\nThe in-editor experience, typing a prompt in an open text file, prefers you to use q: at the start of a sentence. If you\n\nuse the Chat functionality, there’s no need to add q:.\n\n[Prompt response]\n\nThe answer is on the same row as a:.\n\npython -m venv venv\n\n[End of response]\n\nHere’s how the output can look in an app.py file containing both the prompt and the response:\n\n# q: How do I create a virtual environment for Python? a: pytho\n\nTo use the command the AI assistant responded with, we would need to\n\nopen a terminal, paste the command, and run it.\n\nAt this point, you need enough conceptual knowledge to know that you\n\nneed to activate the virtual environment to use it. But again, let’s ask the AI\n\nassistant to help us.\n\n[Prompt]\n\nHow do I activate a virtual environment for Python in Windows?\n\n[End of prompt]\n\n[Prompt response]\n\na: Venv\\Scripts\\activate\n\n[End of response]\n\nIf you are on a Mac or Linux, change the above prompt to\n\nreflect your operating system.\n\nStep 2: Install Flask The conceptual knowledge you need to have is that you can install packages\n\n– in this case, Flask – using pip. Let’s ask the AI assistant to help us. Keep\n\nworking with your app.py file and add your prompt as another comment.\n\n[Prompt]\n\nHow do I install Flask with pip?\n\n[End of prompt]\n\n[Prompt response]\n\npip install Flask\n\n[End of response]\n\nYour app.py should now look like so:\n\n# q: How do I create a virtual environment for Python? a: pytho # q: How do I install Flask with pip? a: pip install Flask\n\nAs you’re using your AI tool to get quick answers on how to\n\ndo something, it’s up to you if you want to clear the text file\n\nfrom questions and responses.\n\nGreat, we now know how to install Flask.\n\nStep 3: Create an entry point We need to create an entry point for our Web API. This is the file that will\n\nbe executed when we run our Web API. For Python, you usually name this file app.py.\n\nStep 4: Create a Flask app At this point, we have our project, we’ve installed Flask, and we have an\n\nentry point file, but the application is not a Flask application yet, so let’s\n\ncreate one.\n\nLet’s ask the AI assistant to help us.\n\n[Prompt]\n\nHow do I create a Flask app?\n\n[End of prompt]\n\nAs usual, keep typing your questions as comments prepended with a q: in\n\nyour open text file. Here’s the response from your latest prompt:\n\n[Prompt response]\n\nfrom flask import Flask\n\n[End of response]\n\nWe’re not getting much back from the AI assistant. But we know we need\n\nto import Flask. Let’s improve the prompt.\n\n[Prompt]\n\nHow do I create a Flask app with a default route and a “hello” route?\n\n[End of prompt]\n\nNow you’ll see an improved prompt response. The prompt contains more\n\nexplicit instructions to create routes and logical areas in the app:\n\n[Prompt response]\n\nfrom flask import Flask # create app app = Flask(__name__) # default route @app.route('/') def index(): return 'Hello World!' # hello route @app.route('/hello')\n\ndef hello(): return 'Hello World!' # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nNow we have some starting code for our Flask app.\n\nNext, let’s focus on the use case for our Web API – e-commerce.\n\nUse case: a Web API for an e- commerce site So far, in this book, we’ve taken you through the process of building a\n\nfront-end application for an e-commerce site. We used an AI assistant to\n\nhelp us with the front-end application. Now, we will build a Web API for the e-commerce site.\n\nIn the previous section, we used an AI assistant to help us create a Flask\n\napp. Now, we will use the AI assistant to help us create a Web API for an e-\n\ncommerce site.\n\nStep 1: Create a Web API for an e-commerce site Let’s plan this task first. Based on previous chapters, the e-commerce site\n\nhas the following features:\n\nProduct catalog: Containing the products we aim to sell.\n\nShopping cart: With a shopping cart, we can add products to the cart,\n\nremove products from the cart, and update the quantity of products in\n\nthe cart. Once we’re happy with the products in the cart, we can check\n\nout and have the products paid for and shipped.\n\nA checkout process: In the checkout process, we need to collect\n\npayment information and shipping information.\n\nWith the above information in mind, let’s craft a prompt for our AI\n\nassistant.\n\n[Prompt]\n\nShow me code to create a Flask Web API that includes routes for a product\n\ncatalog, shopping cart, and checkout process.\n\n[End of prompt]\n\n[Prompt response]\n\nfrom flask import Flask # create app app = Flask(__name__) # default route @app.route('/') def index(): return 'Hello World!' # product catalog @app.route('/products') def products(): return 'Products' # shopping cart @app.route('/cart') def cart(): return 'Cart' # checkout @app.route('/checkout') def checkout(): return 'Checkout'\n\n# start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nOkay, so we have some starting code for our Web API, but we need to\n\nimprove it in the following ways:\n\nWe need to return JSON instead of text.\n\nWe want to add code to read and write to a database.\n\nWe want to add tests.\n\nWe want to document the Web API.\n\nStep 2: Return JSON instead of text JSON is a common format for data. It’s a format that is easy to read and\n\nwrite for both humans and machines. To use JSON in Python, we need to import the json package. The method we will use is jsonify(), which is a method that will convert a Python dictionary to JSON.\n\nLet’s see how we can use jsonify() in our Web API. For this operation, we\n\ncan keep using the AI assistant as prompt comments in the code, or we can use GitHub Copilot’s Chat feature (this is a separate extension of GitHub\n\nCopilot).\n\nLet’s use the latter. To use GitHub Copilot’s Chat feature (refer to this link\n\nfor updated install instructions:\n\nhttps://docs.github.com/en/copilot/github-copilot- chat/using-github-copilot-chat-in-your-ide), we need to\n\nselect the code we want to improve and click the chat icon on the left side\n\nof Visual Studio Code (if you are using the default orientation, where the\n\nicons are placed vertically – it depends what other extensions you have\n\ninstalled). Here’s the icon to look for. Ensure the GitHub Copilot Chat\n\nextension has been correctly installed:\n\nFigure 8.1: Chat icon for GitHub Copilot\n\nOnce we click the Chat icon, and the code selected, we can start chatting\n\nwith GitHub Copilot. Let’s ask GitHub Copilot to help us return JSON\n\ninstead of text.\n\nWe add the following prompt to the chat window:\n\n[Prompt]\n\nAdd code to respond with JSON for all routes.\n\n[End of prompt]\n\nFigure 8.2: Prompt in Copilot chat window\n\nThe chat functionality generates an answer consisting of code including the\n\njsonify() method.",
      "page_number": 172
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 194-214)",
      "start_page": 194,
      "end_page": 214,
      "detection_method": "synthetic",
      "content": "Figure 8.3: Copilot Chat response\n\nHere’s the code generated by GitHub Copilot:\n\nfrom flask import Flask, jsonify # create app app = Flask(__name__) # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): products = [ {'id': 1, 'name': 'Product 1', 'price': 10.99}, {'id': 2, 'name': 'Product 2', 'price': 5.99}, {'id': 3, 'name': 'Product 3', 'price': 7.99} ] return jsonify(products) # shopping cart @app.route('/cart') def cart(): return jsonify({'message': 'Cart'}) # checkout @app.route('/checkout') def checkout(): return jsonify({'message': 'Checkout'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\nStep 3: Add code to read and write to a database Okay, so we have a Web API that returns JSON. Now, we need to add code\n\nto read and write to a database. Let’s use the chat feature again to help us\n\nwith this task.\n\n[Prompt]\n\nAdd code to read and write to a database and apply it to all our routes.\n\n[End of prompt]\n\nHere’s the code we end up with:\n\n[Prompt response]\n\nimport sqlite3 from flask import Flask, jsonify # create app app = Flask(__name__) # connect to database conn = sqlite3.connect('example.db') # create table conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') # insert data conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # shopping cart @app.route('/cart')\n\ndef cart(): return jsonify({'message': 'Cart'}) # checkout @app.route('/checkout') def checkout(): return jsonify({'message': 'Checkout'}) # close connection conn.close() # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nAs you can see, we get quite a lot of code:\n\nCode to connect to a database – in this case, we get a connection to the\n\ncommon SQLite database.\n\nCode that creates a table and even reads from it in the /products route,\n\nbut we don’t get code to write to the database.\n\nWe don’t get code to read and write to the other routes either.\n\nBased on the code above, we realize it’s a good start, but there are things\n\nwe’d like to improve, so we will change the prompt to address the\n\nshortcomings. We need the prompt to ensure all routes read their data from\n\na database, not just one.\n\nSo, we can either adjust the code ourselves, knowing that most of the code\n\nis in place, or try to adjust the prompt. Let’s try the latter.\n\n[Prompt]\n\nAdd code to read and write to a database and apply it to all our routes.\n\n[End of prompt]\n\n[Prompt response]\n\nimport sqlite3 from flask import Flask, jsonify # create app app = Flask(__name__) # connect to database conn = sqlite3.connect('example.db') # create table conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') # insert data conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # shopping cart @app.route('/cart') def cart(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # checkout\n\n@app.route('/checkout') def checkout(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # close connection conn.close() # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nThe improved prompt made the following changes:\n\nAdded database code to the /checkout and /cart routes.\n\nAll three routes read from the products table, which is incorrect. That’s\n\nsomething we need to address in an upcoming prompt. The great news\n\nis that we can just tell Copilot Chat what the problem is with a prompt and expect the AI assistant to change the code accordingly.\n\nThe above code is better than our very first prompt but has issues still.\n\nYou could try to adjust the prompt in Copilot Chat, but let’s fall back on our\n\nother approach – namely, prompts in the code. To add these prompts, put\n\nyour cursor in the code and add the prompt as a comment. Here are some\n\nprompts we added:\n\n# create tables: We’re repeating the comment we have already used,\n\nbut because Copilot is now aware of all the routes, it does a better job\n\nthis time, suggesting three database tables should be created rather than just the products one.\n\n# add product to cart, productId and quantity in body, create cart if\n\nneeded, else update: This prompt is detailed as it asks what\n\ninformation should be added to each cart entry.\n\n# product catalog and # shopping cart are also repeated prompts (we\n\nremove the old prompt, and retype it to toggle a new suggestion). This\n\ntime, we get Copilot to suggest the correct database tables to read\n\nfrom.\n\nNote below how we reshape the code with various prompts in the code as\n\ncomments:\n\nimport sqlite3 import json import flask # create app app = flask.Flask(__name__) # connect to database db = sqlite3.connect('example.db') # create tables db.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') db.execute('''CREATE TABLE IF NOT EXISTS cart (id INTEGER PRIMARY KEY AUTOINCREMENT, product_id INTEGER NOT NULL, quantity INTEGER NOT NULL);''') db.execute('''CREATE TABLE IF NOT EXISTS checkout (id INTEGER PRIMARY KEY AUTOINCREMENT, cart_id INTEGER NOT NULL, total REAL NOT NULL);''') db.commit() db.close() # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog\n\n@app.route('/products') def products(): db = sqlite3.connect('example.db') cursor = db.execute(\"SELECT id, name, price FROM products\") products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) db.close() return flask.jsonify(products) # shopping cart @app.route('/cart') def cart(): db = sqlite3.connect('example.db') cursor = db.execute(\"SELECT id, product_id, quantity FROM c cart = [] for row in cursor: item = {'id': row[0], 'product_id': row[1], 'quantity': cart.append(item) db.close() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart/', methods=['POST']) def add_to_cart(): db = sqlite3.connect('example.db') # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] # check if cart exists cursor = db.execute(\"SELECT id FROM cart\") cart_id = None for row in cursor: cart_id = row[0] # if cart exists, update if cart_id: db.execute(\"UPDATE cart SET product_id = ?, quantity = # else create else: db.execute(\"INSERT INTO cart (product_id, quantity) VAL # close db.commit() db.close()\n\nreturn flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) def checkout(): # insert cart into checkout conn = sqlite3.connect('example.db') # get cart id from body cart_id = flask.request.json['cartId'] # write to checkout conn.execute(\"INSERT INTO checkout (cart_id, total) VALUES # close conn.commit() conn.close()\n\nIs this code good enough or do we need to adjust it further?\n\nThere’s definitely room for improvement and here’s what you should be\n\nlooking for:\n\nThe code is not DRY (Don’t Repeat Yourself); we have a lot of\n\nduplicate code. We can solve this by creating a function that takes a\n\nquery and returns the result.\n\nAuthentication and authorization are missing. We should add this to\n\nthe code.\n\nDocumentation is missing.\n\nThe code is not secure. We should add some security measures,\n\nespecially when it comes to the database. As developers, having some\n\nknowledge of how to secure code, we can use prepared statements to\n\nprevent SQL injection attacks and also validate the data that we receive from the client.\n\nStep 4: Improve the code\n\nThe best way to improve the code is to use the code that we have as a\n\nstarting point and first just try to run it. We can then see what errors we get\n\nand what we need to fix.\n\nAfter that, we work on architecture and design and separate the code into\n\ndifferent files.\n\nLastly, we add authentication and authorization and security measures.\n\nRun the code Let’s run the code. We need to set the Flask variable FLASK_APP to app.py\n\nand then run the app.\n\nIf you are using Windows, you need to use set instead of export to set the variable.\n\n# flask variable windows set FLASK_APP=app.py flask run\n\nThe app is working! We can now go to http://localhost:5000 and see the message Hello World!.\n\nGoing to /products, /cart, and /checkout also works, but we don’t have\n\nany data in the database yet. Let’s add some data to the database. By\n\ninserting the above code at the top of the file and running the app again, we\n\ncan do this:\n\ndb.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\", db.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\",\n\nNow, if you rerun the app and go to /products, you should see the products\n\nthat we added as the following JSON output:\n\n[ { \"id\": 1, \"name\": \"Apple\", \"price\": 1.99 }, { \"id\": 2, \"name\": \"Banana\", \"price\": 2.99 } ]\n\nRefactor the code Great – lots of progress so far! Now let’s refactor the code and make it more\n\nDRY (Don’t Repeat Yourself, a term for ensuring we don’t duplicate code\n\ntoo much). We can use Copilot to help us with this.\n\nProvide it with the following prompt:\n\n[Prompt]\n\nImprove the code by separating the database parts into a separate module\n\ndb.py and expose the functions as follows: get_products, get_cart,\n\nadd_to_cart, checkout.\n\n[End of prompt]\n\nWe get two responses, one for the db.py file and one for the app.py file.\n\nLet’s start with the db.py file.\n\n[Prompt response]\n\nimport sqlite3 # connect to database conn = sqlite3.connect('example.db') # create tables conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') conn.execute('''CREATE TABLE IF NOT EXISTS cart (id INTEGER PRIMARY KEY AUTOINCREMENT, product_id INTEGER NOT NULL, quantity INTEGER NOT NULL);''') conn.execute('''CREATE TABLE IF NOT EXISTS checkout (id INTEGER PRIMARY KEY AUTOINCREMENT, cart_id INTEGER NOT NULL, total REAL NOT NULL);''') conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() def get_products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return products def get_cart(): cursor = conn.execute(\"SELECT id, product_id, quantity FROM cart = [] for row in cursor: item = {'id': row[0], 'product_id': row[1], 'quantity': cart.append(item) return cart def add_to_cart(product_id, quantity): # check if cart exists cursor = conn.execute(\"SELECT id FROM cart\") cart_id = None for row in cursor: cart_id = row[0] # if cart exists, update if cart_id: conn.execute(\"UPDATE cart SET product_id = ?, quantity\n\n# else create else: conn.execute(\"INSERT INTO cart (product_id, quantity) V conn.commit() def checkout(cart_id): # insert cart into checkout conn.execute(\"INSERT INTO checkout (cart_id, total) VALUES conn.commit()\n\n[End of response]\n\nWe need to improve the above code a bit, as it’s using an\n\nopen connection to the database. We should close the\n\nconnection after we are done with it. Ensure you add code\n\nto close the connection after the commit. Also, ensure per\n\nroute that you open and close the connection.\n\nThis looks good. Let’s add it to our project. We can create a new file called\n\ndb.py and copy the code into it.\n\nNext, let’s see what it suggested for the app.py file:\n\nimport flask import db # create app app = flask.Flask(__name__) # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): products = db.get_products() return flask.jsonify(products)\n\n# shopping cart @app.route('/cart') def cart(): cart = db.get_cart() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart', methods=['POST']) def add_to_cart(): # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] db.add_to_cart(product_id, quantity) return flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) def checkout(): # get cart id from body cart_id = flask.request.json['cartId'] db.checkout(cart_id) return flask.jsonify({'message': 'Checkout successful'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\nAs you can see, it has separated the database code into the db.py file and\n\nhas also added the imports for the db.py file. Let’s copy this code into our app.py file and replace its content.\n\nDoes it work? Let’s run the app and see. It runs without any errors, and we\n\nget JSON output when we go to /products, /cart, and /checkout. If you want, you can also make a POST request to /cart to write cart content –\n\ngreat!\n\nStep 5: Document the API\n\nWe can use Copilot Chat to help us document the API. Let’s provide it with\n\nthe following prompt:\n\n[Prompt]\n\nDocument the API using Swagger. The API should have the following\n\nendpoints: GET /products, GET /cart, POST /cart/add, POST /checkout.\n\n[End of prompt]\n\nSwagger or OpenAPI standard is a way to document your\n\nAPI routes. These route specifications can then be\n\nvisualized, and you get a quick overview of what your API\n\ncan do. Additionally, there’s something called Swagger UI,\n\na web page that not only lets you see what routes you have\n\nin your API but you can make requests and more, to ensure\n\nthe API is working as it should.\n\nHere’s the output, which involves both code and documentation:\n\n[Prompt response]\n\n# app.py import flask import db from flasgger import Swagger, swag_from # create app app = flask.Flask(__name__) swagger = Swagger(app) # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products', methods=['GET'])\n\n@swag_from('swagger/products.yml') def products(): products = db.get_products() return flask.jsonify(products) # shopping cart @app.route('/cart', methods=['GET']) @swag_from('swagger/cart.yml') def cart(): cart = db.get_cart() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart/add', methods=['POST']) @swag_from('swagger/add_to_cart.yml') def add_to_cart(): # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] db.add_to_cart(product_id, quantity) return flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) @swag_from('swagger/checkout.yml') def checkout(): # get cart id from body cart_id = flask.request.json['cartId'] db.checkout(cart_id) return flask.jsonify({'message': 'Checkout successful'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nAnd it tells us to create the following files and folders:\n\nfolder: swagger products.yml file: cart.yml file: add_to_cart.yml\n\nfile: checkout.yml file: swagger.yml\n\nLet’s create these files manually and copy the content into them.\n\nproducts.yml --- tags: - products responses: 200: description: A list of products schema: type: array items: type: object properties: id: type: integer name: type: string price: type: number cart.yml --- tags: - cart responses: 200: description: A list of items in the cart schema: type: array items: type: object properties: id: type: integer product_id: type: integer quantity:\n\ntype: integer add_to_cart.yml --- tags: - cart parameters: - in: body name: body required: true schema: type: object properties: productId: type: integer quantity: type: integer responses: 200: description: Added to cart checkout.yml --- tags: - checkout parameters: - in: body name: body required: true schema: type: object properties: cartId: type: integer responses: 200: description: Checkout successful\n\nWhat’s missing is installing the flassger Swagger library:\n\npip install flasgger\n\nNavigate to http://localhost:5000/apidocs/ and you should see Swagger\n\nUI.\n\nFigure 8.4: API doc generated by Swagger\n\nYou should verify that the API works as expected by interacting with\n\ngenerated docs and ensure the routes generate the expected output.\n\nIt’s definitely possible to keep improving at this point but take a moment to\n\nrealize how much we created with only prompts and a few lines of code. We\n\nhave a working API with a database and documentation. We can now focus\n\non improving the code and adding more features.\n\nAssignment Here’s the suggested assignment for this chapter: A good assignment would\n\nbe to add more features to the API, such as:\n\nAdd a new endpoint to get a single product.\n\nAdd a new endpoint to remove a product from the cart.\n\nAdd a new endpoint to update the quantity of a product in the cart.\n\nYou can solve this by just adding the above to Copilot Chat\n\nas prompts and see what it generates. Expect both the code\n\nand documentation to change.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/08\n\nChallenge Improve this API by adding more features. You can use Copilot Chat to help\n\nyou with this.\n\nSummary In this chapter, we discussed how to plan out our API. Then we looked at how we can choose Python and Flask for the job but stressed the\n\nimportance of having contextual knowledge on how to actually build a Web\n\nAPI. In general, you should always know how to do something before you\n\nask an AI assistant to help you with it, at least at a high level.\n\nThen we ended up crafting prompts for the AI assistant to help us with the\n\nWeb API. We ended up working with our e-commerce site and created a\n\nWeb API to serve it.\n\nAfter that, we discussed how to improve the code and add more features to\n\nthe API.\n\nIn the next chapter, we will discuss how to improve our app by adding\n\nartificial intelligence to it.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "page_number": 194
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 215-233)",
      "start_page": 215,
      "end_page": 233,
      "detection_method": "synthetic",
      "content": "9\n\nAugment Web Apps with AI Services\n\nIntroduction There are several ways that a web app can be augmented with AI services:\n\nyou could leverage an existing Web API exposing a model, or build it\n\nyourself and have it call a model.\n\nThe reason you would want to add AI to your app in the first place is to\n\nmake it smarter. Not smarter for its own sake, but to make it more useful to\n\nthe user. For example, if you have a web app that allows users to search for\n\nproducts, you could add a feature that suggests products based on the user’s\n\nprevious purchases. In fact, why limit yourself to previous purchases? Why\n\nnot suggest products based on the user’s previous searches? Or, what if the\n\nuser could take a picture of a product and the app would suggest similar\n\nproducts?\n\nAs you can see, there are a lot of possibilities for augmenting your web app\n\nwith AI that would improve the user experience.\n\nIn this chapter, we will:\n\nDiscuss different model formats like Pickle and ONNX\n\nLearn how to use both Pickle and ONNX to persist your model as a\n\nfile using Python\n\nConsume a model stored in ONNX format and expose it via a REST\n\nAPI using JavaScript\n\nBusiness domain, e- commerce We keep working on our e-commerce domain, but our business focus is on ratings. A good or bad rating can influence how many units are sold of a\n\nspecific product. The logical domain consists of the following:\n\nProducts: the products to be rated\n\nRatings: the actual ratings and meta information like comments, dates\n\nand more\n\nProblem and data domain The problem to figure out is how we use this rating data and learn from it.\n\nInsights: We could, for example, get the insights that we should start/stop selling a certain product. There might be other insights as certain products sell well in certain parts of the world.\n\nTechnical problem: The technical aspect of this is figuring out how to ingest the data, train a model from it, and then figure out how a web\n\napplication can leverage said model.\n\nFeature breakdown\n\nLooking at this from a feature standpoint, we need to see this as consisting\n\nof three major parts.\n\nData ingestion and training: this needs a separate interface, maybe\n\nit’s done without a user interface and it’s just static data being fed into code capable of training a model from it. With that understanding, we\n\ncan outline the steps like so:\n\nLoad data\n\nClean data\n\nCreate features\n\nTrain model\n\nEvaluate model\n\nRun predictions\n\nConsuming the model: Once the model is trained, it needs to be exposed, preferably through a web endpoint. To get there, we think we\n\nneed these set of steps:\n\nConvert the model to suitable format if needed\n\nBuild a Web API\n\nExpose model through Web API\n\nDeploy model, there’s a step here where we need to bring the API online\n\nPrediction: For the prediction part, this is a functionality that’s meant\n\nfor “back office” and not customer facing.\n\nBuild user interface to run predictions\n\nBuild underlying code that talks to the Web API to make predictions possible\n\nPrompt strategy\n\nYou could go with either prompt approach here, either get suggestions from\n\na prompt comment or use the chat interface.\n\nFor prompt pattern, we’ll use the “Exploratory pattern” as described in Chapter 2.\n\nCreating a model Imagine we are dealing with the following data in the sales_rating.csv\n\nfile that is the result of merging two datasets, one containing sales data and\n\nthe other containing rating data. The data looks like this:\n\nproduct_id,avg_rating,sold 1,2.5,100 2,3.7,200 3,4.2,300 4,1.3,50 5,4.9,800 6,3.2,150 7,2.1,80 8,4.8,500 9,3.9,400 10,2.4,200 11,4.1,300 12,3.2,100 13,2.9,150 14,4.5,500 15,3.8,400 16,2.7,200 17,4.3,300 18,3.4,100 19,2.3,150 20,4.7,500\n\nThe preceding data shows a dataset with product_id, avg_rating, and sold\n\ncolumns. Our theory is that the average rating of a product is correlated\n\nwith the number of sales. It seems like a fair assumption that a product with a high rating will sell more than a product with a low rating. By creating a\n\nmodel, we can come closer to determining whether it’s likely that our\n\nassumption is true or not.\n\nComing up with a plan We can ask Copilot about the steps we need to take to create a model. We\n\ntype these prompts as comments:\n\nIf you want to ask Copilot to come up with text answers\n\nabout something else other than code, you can type the text as a comment in a code file or inside of a Markdown file. It\n\nalso helps to place yourself on the next row after the\n\ncomment and start the row with “a:” for answer. You can\n\nalso ask questions using Copilot Chat:\n\n[Prompt]\n\nTell me the answer of life, the universe and everything.\n\n[End of prompt]\n\n[Prompt response]\n\na: 42\n\n[End of response]\n\nThe above reference is from the book Hitchhikers guide to the Galaxy by\n\nDouglas Adams.\n\nNext, let’s ask Copilot to help us with the steps to create a model.\n\n[Prompt]\n\nWhat are the steps to create a model in Python given a CSV file?\n\n[End of prompt]\n\nOutput:\n\nimport pandas and scikit-learn\n\nread the CSV file\n\ncreate a test and training dataset\n\ncreate a model\n\nhow good is the model?\n\npredict\n\nsave the model to pkl\n\nOK, we have a plan, so let’s execute it and use Copilot to help us with the code.\n\nImporting libraries First, let’s start by creating a new notebook in VS Code. Let’s call it\n\nsales.ipynb.\n\n.ipynb is the file format for Jupyter Notebook that you as a\n\ndata science practitioner are most likely familiar with. The\n\nidea is that you work on a file with cells, where some cells are text cells and others are code cells you can run.\n\nTo use certain libraries, we need to install them. We can use Copilot to help us with that. We can leverage the fact that we are data science practitioners\n\nand therefore have an understanding of what libraries we need. Therefore,\n\nwe will suggest the pandas and scikit-learn libraries below to help us with\n\nour task.\n\n[Prompt]\n\nInstall pandas and scikit-learn in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n! pip install pandas scikit-learn\n\n[End of response]\n\nNext, let’s import the libraries we need.\n\n[Prompt]\n\nImport pandas and scikit-learn in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Import pandas and scikit-learn in a notebook. import pandas as pd import sklearn.linear_model\n\n[End of response]\n\nReading the CSV ﬁle\n\nNow that we have the libraries we need, let’s read the CSV file.\n\n[Prompt]\n\nRead a CSV file in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Read a CSV file in a notebook. df = pd.read_csv(\"sales_rating.csv\")\n\n[End of response]\n\nThe CSV file is now loaded into a pandas DataFrame, a structure holding\n\ncolumns and rows. Let’s take a look at the first few rows.\n\ndf.head()\n\nThis will produce the following output:\n\nproduct_id avg_rating sold 0 1 2.5 100 1 2 3.7 200 2 3 4.2 300 3 4 1.3 50 4 5 4.9 800\n\nGreat, we have the data loaded into a DataFrame. Next, we need to split the\n\ndata into test and training datasets.\n\nCreating test and training datasets In machine learning, we typically split the data into test and training\n\ndatasets. The training dataset is used to train the model and the test dataset\n\nis used to test the model. We can use Copilot to help us with this.\n\n[Prompt]\n\nCreate test and training dataset in a notebook, suggested test size of 0.2.\n\n[End of prompt]\n\n[Prompt response]\n\n# Create a test and training dataset in a notebook, suggested t test_size = 0.2 test = df.sample(frac=test_size) training = df.drop(test.index)\n\n[End of response]\n\n0.2 is a good test size. It means that 20% of the data will be used for testing\n\nand 80% will be used for training.\n\nYou may need to start typing “test_size” and “training” on\n\nthe respective rows, then press TAB to accept the suggestion.\n\nNow we have things set up, let’s create a model next.\n\nCreating a model\n\nCreating a model involves selecting the algorithm we want to use and then\n\ntraining the model. Let’s suggest a prompt:\n\n[Prompt]\n\nCreate a model in a notebook using the LinearRegression algorithm.\n\n[End of prompt]\n\n[Prompt response]\n\n# Create a model in a notebook using the LinearRegression algor model = sklearn.linear_model.LinearRegression() model.fit(training[[\"avg_rating\"]], training[[\"sold\"]])\n\n[End of response]\n\nIn the preceding code, we created a model using the LinearRegression\n\nalgorithm and then trained the model using the training dataset with the fit method. Also, note how we are using the avg_rating column as the input\n\nand the sold column as the output. This is because we want to predict the\n\nnumber of sales based on the average rating.\n\nHow good is the model? At this point, we want to find out how good the model is. To find this out, let’s ask Copilot to help us.\n\nYou can either use a comment style and get suggestions\n\ninline, or use Copilot Chat. In both cases, Copilot will\n\nunderstand from the context.\n\n[Prompt]\n\nHow good is the model in a notebook? Show the code.\n\n[End of prompt]\n\n[Prompt response]\n\n# How good is the model in a notebook? Show the code. score = model.score(test[[\"avg_rating\"]], test[[\"sold\"]])\n\n[End of response]\n\nIt seems that calling score on the model and passing in the test dataset is how we find out how good a model is. We can even ask Copilot about this:\n\n[Prompt]\n\nWhat does score do?\n\n[End of prompt]\n\n[Prompt response]\n\nscore returns the coefficient of determination R^2 of the predi\n\n[End of response]\n\nIf you want, you keep querying and have things explained in more detail, like with the below prompt:\n\n[Prompt]\n\nWhat does that mean in simpler terms?\n\n[End of prompt]\n\n[Prompt response]\n\nit returns a number between 0 and 1, where 1 is perfect predict\n\n[End of response]\n\nPredict We won’t know how good the model is until we try to predict something,\n\neven if we could refer to the score value. Let’s craft a prompt for this task:\n\n[Prompt]\n\nPredict in a notebook. Show the code.\n\n[End of prompt]\n\n[Prompt response]\n\n# Predict in a notebook. Show the code. predictions = model.predict(test[[\"avg_rating\"]])\n\n[End of response]\n\nOK, we got a variable called predictions that contains the predictions, but\n\nit is hard to understand if the predictions are reasonable. Let’s try asking\n\nCopilot and see what it suggests.\n\n[Prompt]\n\nHow do you suggest we show the predictions in a notebook?\n\n[End of prompt]\n\n[Prompt response]\n\nprint the predictions\n\n[End of response]\n\n[Prompt]\n\nAny other ways you suggest we show the predictions in a notebook except\n\nfor printing?\n\n[End of prompt]\n\n[Prompt response]\n\nplot the predictions\n\n[End of response]\n\nNow that we’ve gotten more information on our various options, let’s use that knowledge in a prompt:\n\n[Prompt]\n\nShow me how to print and plot predictions vs the actual data in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Show me how to print and plot predictions vs actual data in a import matplotlib.pyplot as plt plt.scatter(test[[\"avg_rating\"]], test[[\"sold\"]], color='black plt.plot(test[[\"avg_rating\"]], predictions, color='blue', linew\n\n[End of response]\n\nRunning this code in a notebook will produce the following plot:\n\nFigure 9.1: Plot showing predictions vs actual data\n\nIn the plot, it looks like the line is a good fit for the data. We can also print\n\npredictions to see the actual values:\n\nprint(predictions)\n\nSaving the model to a .pkl ﬁle We have one step left: we need to save the model to a file. We have a few\n\ndifferent choices for how to save the model. We can save it as a pickle file or an ONNX file, for example. Let’s start with saving it as a pickle file.\n\nAs usual, let’s craft a prompt for this task:\n\n[Prompt]\n\nSave the model to a pkl in the notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Save the model to pkl in a notebook. import pickle pickle.dump(model, open(\"model.pkl\", \"wb\"))\n\n[End of response]\n\nFor the completion of the above, you may need to start\n\ntyping “import” to get the suggestion. Additionally, it’s a\n\ngood bet it’s going to use the pickle library, so on the next\n\nline you can start typing “pickle” and press TAB to accept\n\nthe suggestion.\n\nCreating a REST API in Python At this point, we have a model.pkl file that contains our model. We can\n\nexpose the model using this file via a REST API. Let’s ask Copilot to help\n\nus with this.\n\n[Prompt]\n\nCreate a REST API in Python that exposes the model in a pkl file.\n\n[End of prompt]\n\n[Prompt response]\n\n# Create a REST API in Python that exposes the model in a pkl f import pickle import flask # load the model model = pickle.load(open(\"model.pkl\", \"rb\")) # app app = flask.Flask(__name__) # routes @app.route('/', methods=['GET', 'POST']) def predict(): if flask.request.method == 'GET': return \"Please POST a JSON object to this endpoint to g else: data = flask.request.json prediction = model.predict([[data[\"avg_rating\"]]]) return flask.jsonify({\"prediction\": prediction[0][0]})\n\n[End of response]\n\nNote how we use several prompts above. First, we set the\n\ncontext by adding the prompt “Create a REST API in\n\nPython that exposes the model in a pkl file” at the top of the\n\nfile. Then we use various helper prompts like “load the\n\nmodel”, “app”, and “routes” to help us with the code. We\n\nend up with an API that exposes the model via a REST API.\n\nConverting the model to ONNX So far, you’ve seen how we can use Pickle to save a model and then load it\n\nin Python. However, Pickle has drawbacks, including being Python-\n\nspecific. ONNX, on the other hand, is a format that is not Python-specific\n\nand can be used in other languages. Let’s see how we can use the ONNX\n\nformat to work with our model.\n\nWe need to solve our task in two steps:\n\nConvert the model to ONNX format.\n\nLoad the model and create a REST API in JavaScript.\n\nCreating a model in ONNX format Because we’re using a notebook, we need to install the dependencies,\n\nimport the libraries we need, and then convert the model to ONNX format.\n\n[Prompt]\n\nWhat dependencies do I need for ONNX? Show me the Python code for\n\ninstalling those dependencies.\n\n[End of prompt]\n\n[Prompt response]\n\n! pip install skl2onnx\n\n[End of response]\n\n[Prompt]\n\nShow me how to import the libraries I need for ONNX and show me how to\n\nconvert the model to ONNX format.\n\n[End of prompt]\n\n[Prompt response]\n\nimport skl2onnx # convert the model to ONNX skl2onnx.to_onnx(model, train[[\"avg_rating\"]].astype(\"float32\")\n\n[End of response]\n\nNow I should have a model.onnx file that contains the model in ONNX format. Let’s see if we can load it in JavaScript next.\n\nLoading the ONNX model in JavaScript Next, we need to load the ONNX model in JavaScript and specifically\n\nNode.js, as we’re doing this on the backend. We can use the onnxruntime\n\nlibrary to do this. Let’s ask Copilot to help us with this.\n\n[Prompt]\n\nShow me a step-by-step guide to load the ONNX model in JavaScript.\n\n[End of prompt]\n\n[Prompt response]\n\ninstall onnxruntime in JavaScript\n\nload the ONNX model in JavaScript\n\npredict using ONNX model in JavaScript\n\n[End of response]\n\nNow that we have a plan, let’s execute it.\n\nInstalling onnxruntime in JavaScript Because we’re building a Node.js project, let’s first, in a new folder, create\n\nan app.js file and run the npm init -y terminal command to create a new\n\nNode.js project.\n\nThe first step in our plan is to install onnxruntime in JavaScript. We can use\n\nnpm to perform this task.\n\n[Prompt]\n\nInstall onnxruntime in JavaScript.\n\n[End of prompt]\n\n[Prompt response]\n\nnpm install onnxruntime\n\n[End of response]\n\nGreat, this installs the onnxruntime library in our project. Next, we need to\n\nload the ONNX model.\n\nLoading the ONNX model in JavaScript Now that we have onnxruntime installed, we can load the ONNX model.\n\n[Prompt]\n\nLoad the ONNX model in JavaScript.",
      "page_number": 215
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 234-251)",
      "start_page": 234,
      "end_page": 251,
      "detection_method": "synthetic",
      "content": "[End of prompt]\n\n[Prompt response]\n\nconst rt = require(\"onnxruntime\"); # load model from ONNX model rt.InferenceSession(\"model.onnx\"); # predict model.run(null, {model.get_inputs()[0].name: [[4.5]]});\n\n[End of response]\n\nIn the preceding code, we load the model from the ONNX file and then we\n\nmake predictions using the model with the input 4.5 to represent the\n\naverage rating to see what sales we can expect.\n\nAssignment: Build a REST API in JavaScript that consumes the model\n\n1. Take the model we created in the previous section and add the code to\n\nthe notebook to turn it into an ONNX file.\n\n2. Create a new folder in the repo called and create a new file called\n\napp.js in that folder.\n\n3. Add the POST /predict route to the server.js file and ensure it returns\n\na prediction given the input.\n\nHere’s some starter prompts you can try to help you with this assignment:\n\nPrompt: Create a REST API in JavaScript using Express\n\nPrompt: Create a POST /predict route in a REST API in JavaScript\n\nusing Express\n\nPrompt: Load the model from ONNX in a REST API in JavaScript\n\nusing Express\n\nPrompt: Predict using the ONNX model in a REST API in JavaScript\n\nusing Express\n\nSolution See repo [https://github.com/PacktPublishing/AI-\n\nAssisted-Software-Development-with-GitHub-Copilot- and-ChatGPT/tree/main/09] and the 09 folder for the solution.\n\nQuiz What’s the difference between Pickle and ONNX?\n\n1. Pickle is Python-specific and ONNX is not.\n\n2. Pickle can be used in JavaScript and ONNX can’t.\n\n3. ONNX is less efficient than Pickle.\n\nSummary In this chapter, we covered various model formats like Pickle and ONNX\n\nand how to persist your model as a file using Python. Storing a model as a\n\nfile is useful because it allows you to integrate it with other applications.\n\nThen we discussed the pros and cons of different formats for storing models\n\nlike Pickle and ONNX. We came to the conclusion that ONNX is probably\n\nthe better choice because it’s not Python-specific and can be used in other\n\nlanguages.\n\nThen we covered how to load a model stored in ONNX format using\n\nJavaScript and create a REST API to make the model available to other\n\napplications.\n\nIn the next chapter, we’ll go into more detail of how we can use GitHub\n\nCopilot and get the most out of it. We’ll cover both tips and tricks and\n\nfeatures that help make you faster and more productive.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n10\n\nMaintaining Existing Codebases\n\nIntroduction Brownfield is another word for working with existing code. In my career as\n\na developer, most of the work carried out has been on existing code. The\n\nopposite of brownfield is greenfield, which is a new project with no\n\nexisting code.\n\nFor that reason, it’s important to cover how to work with existing\n\ncodebases, and there’s a lot to get excited about when working with an AI\n\nassistant like GitHub Copilot in a brownfield context.\n\nIn this chapter, we will:\n\nLearn about the different types of maintenance.\n\nUnderstand how we work with maintenance in a process to de-risk introduced changes.\n\nUse GitHub Copilot to help us with maintenance.\n\nPrompt strategy This chapter is a bit different from other chapters in the book. The focus is\n\non describing various problems you may encounter in the space of existing\n\ncodebases. You’re recommended to use the prompt suggestion approach\n\nwith which you’re the most comfortable, be it prompt comments or the chat\n\ninterface. As for patterns, you’re encouraged to try out all three major\n\npatterns introduced, that is, PIC, TAG, or Exploratory patterns as described in Chapter 2. This chapter however focuses on using the “Exploratory\n\npattern”.\n\nDiﬀerent types of maintenance There are different types of maintenance, and it’s important to understand\n\nthe differences between them. Here are some different types that you’re\n\nlikely to encounter:\n\nCorrective maintenance: This is when we’re fixing bugs.\n\nAdaptive maintenance: In this case, we change code to adapt to new\n\nrequirements.\n\nPerfective maintenance: When we improve code without changing\n\nthe functionality. Examples of this could be refactoring or improving\n\nthe performance of the code.\n\nPreventive maintenance: Changing the code to prevent future bugs or\n\nissues.\n\nThe maintenance process Every time you change code, you introduce risk. For example, a bug fix\n\ncould introduce a new bug. To mitigate this risk, we need to follow a\n\nprocess. A suggested process could be the following steps:\n\n1. Identify: Identify the problem or the change that needs to be made.\n\n2. Inspect: Inspect the test coverage and how well your code is covered by tests. The better it’s covered, the more likely you are to detect any introduced bugs or other issues.\n\n3. Plan: Plan the change. How are you going to make it? What tests are\n\nyou going to write? What tests are you going to run?\n\n4. Implement: Implement the change.\n\n5. Verify: Verify that the change works as expected. Run the tests, run the\n\napplication, check the logs, etc.\n\n6. Integrate: This is about ensuring any change you make in a branch is\n\nmerged into the main branch.\n\n7. Release/deploy the change: You want to make sure the end customer can leverage the benefit of this change. For that to happen, you need to deploy it.\n\nDo we need to cover all these steps for every change? No, it depends on the change; some changes might be so small that we might want to bundle many of them before we make a release that we want the end customer to\n\nhave. The good news is that we have a great tool in GitHub Copilot that can help us with some of these minor steps, allowing us to focus on the more\n\nimportant steps.\n\nAddressing a bug Some bugs are more severe than others. In a best-case scenario, your bug is\n\nseldom encountered, or when it does occur it’s not a big deal. In a worst- case scenario, your bug loses you money. Let’s focus on a case where the bug loses you money in a scenario where you’re running an e-commerce\n\nsite.\n\nHopefully, this code has never been shipped in production, but it’s a good\n\nexample of a bug that could be catastrophic for an e-commerce site:\n\ndef process_cart(): # get cart from database cart = get_cart() card = cart.get_card() process_payment(card, cart) ship_order(cart)\n\nThe problem you might have is that ship_order is called even if\n\nprocess_payment fails, which means you ship orders without getting paid, losing you money.\n\nLooking at this code, you might not have a problem if process_payment\n\nthrows an error and you don’t even get to ship_order.\n\nWhat if it doesn’t and, instead, returns a numerical code that depends on\n\npayment results, like so?\n\ndef process_payment(card, cart): if card.is_valid(): if card.has_funds(cart.total): charge_card(card, cart.total) return 0 else: return 1\n\nIf you don’t read the error code, you might not notice that the payment\n\nfailed, and the order is shipped. So, what’s our approach here?\n\n1. Identify the problem\n\nFirst, we need to identify the problem; in this case, there are two things you\n\ncan do, namely:\n\nRead the code: What does our code look like? Are errors thrown, does\n\nit return a code, or something else entirely?\n\nCheck the tests: Do we have a test that covers this scenario? If not, we need to write one. The test should show that we have a bug.\n\nLet’s use Copilot for this, with a prompt like so (here, we simulate process_payment to return error codes instead of throwing errors). Select\n\nthe ship_order code in the main window, and then type a prompt as follows\n\nin the chat window:\n\n[Prompt]\n\nWrite a test for the code; ship_order should not be called if process_payment returns 1.\n\n[End of prompt]\n\nWe’re using the GitHub Copilot Chat function and selecting the process_cart code in our text editor.\n\nYou should see a response similar to the below code:\n\n[Prompt response]\n\n```python import unittest from unittest.mock import Mock, patch from your_module import process_cart # replace with the actual class TestProcessCart(unittest.TestCase): @patch('your_module.get_cart') # replace with the actual m @patch('your_module.process_payment')\n\n@patch('your_module.ship_order') def test_order_not_shipped_if_payment_fails(self, mock_ship # Arrange mock_cart = Mock() mock_cart.get_card.return_value = 'test_card' mock_get_cart.return_value = mock_cart mock_process_payment.return_value = 1 # Act process_cart() # Assert mock_ship_order.assert_not_called() if __name__ == '__main__': unittest.main() ```\n\n[End of response]\n\nThe above code contains a test that tests what happens if we get error code 1 from process_payment. What we want to have happen is for ship_order\n\nnot to be called.\n\nThis test will fail, as we have a bug in our code, which is a good thing, as\n\nwe’ve identified a bug and can prove it exists with a test.\n\n2. Implement the change Now that we have a test that shows we have a bug, we can implement the change. We can be confident to make the code change, as we have a test\n\nthat shows that we have a bug, and we’ve hopefully also improved test\n\ncoverage enough.\n\nDon’t start refactoring unless you have good test coverage or the change is so small that you are confident that you\n\nwon’t introduce a bug.\n\nHow do we now improve the code? Simple – we can use GitHub Copilot to help us with the implementation. Let’s use the following prompt to help us\n\nwith the code change we need to do:\n\n[Prompt]\n\nImplement the change; ship_order should not be called if process_payment\n\nreturns 1.\n\n[End of prompt]\n\nYour code for process_cart should now look like this:\n\n[Prompt response]\n\n```python def process_cart(): # get cart from database cart = get_cart() card = cart.get_card() payment_status = process_payment(card, cart) # Only ship the order if the payment was successful if payment_status != 1: ship_order(cart) ```\n\n[End of response]\n\nThis code above is a step in the right direction, as it now contains an if statement that checks the payment status.\n\nTo further improve the code, we can use Copilot to help us make the code\n\nmore robust and easier to read. Let’s use the following prompt for additional improvements:\n\n[Prompt]\n\nRefactor the code, making it easier to maintain.\n\n[End of prompt]\n\nThis might give you the following code:\n\n[Prompt response]\n\ndef get_payment_status(cart): card = cart.get_card() return process_payment(card, cart) def process_cart(): # get cart from database cart = get_cart() payment_status = get_payment_status(cart) # Only ship the order if the payment was successful if payment_status != 1: ship_order(cart)\n\n[End of response]\n\nHere, Copilot has suggested that we extract the payment status into a\n\nseparate function. This is a good suggestion, as it makes the code easier to\n\nread and maintain.\n\nAdding a new feature In this example, we’ll look at an existing codebase and add a new feature.\n\nThe codebase is like the one we used in the previous example, but it’s a bit more complex. This is because it has a few more functions involving\n\ndifferent types of payment methods. Here’s the code:\n\n```python import re def validate_card(card): # Visa cards start with 4 and have 13 or 16 digits. # MasterCard cards start with 51 through 55 and have 16 dig visa_pattern = r'^4[0-9]{12}(?:[0-9]{3})?$' mastercard_pattern = r'^5[1-5][0-9]{14}$' if re.match(visa_pattern, card): return 'Visa' elif re.match(mastercard_pattern, card): return 'MasterCard' else: return None def process_payment(card, cart): card_type = validate_card(card) if card_type is None: return 1 # Invalid card else: # Process the payment here # Return 0 if the payment is successful, 1 otherwise pass def process_cart(): # Get cart from database cart = get_cart() card = cart.get_card() payment_status = process_payment(card, cart) # Only ship the order if the payment was successful if payment_status == 0: ship_order(cart) ```\n\nIn the above code, we have the following:\n\nvalidate_card validates the card number and returns the card type if valid; otherwise, it returns None.\n\nprocess_payment processes the payment and returns 0 if the payment was successful; otherwise, it returns 1.\n\nprocess_cart processes the cart, gets the card, processes the payment,\n\nand ships the order if the payment is successful.\n\n1. Identify a problem and ﬁnd the function/s to change Our task is to make our code also support American Express cards. Implementing a new card means that we need to update the validate_card\n\nfunction to support American Express cards.\n\nNow, we know what functions need to change, but does GitHub Copilot\n\nknow?\n\nLet’s ask GitHub Copilot, with a prompt.\n\nIn the identification step, we used our know-how to identify\n\nwhat and where to change. A good practice is to rely on a combination of your own skills and the Copilot tool to help\n\nanalyze and identify problems.\n\n2. Implement change, and add a new feature and tests Let’s use GitHub Copilot to help us with this. We can use the following prompt:\n\n[Prompt]\n\nAdd support for American Express cards.\n\n[End of prompt]\n\nYou will get code similar to this:\n\n[Prompt response]\n\nimport re def validate_card(card): # Visa cards start with 4 and have 13 or 16 digits. # MasterCard cards start with 51 through 55 and have 16 dig # American Express cards start with 34 or 37 and have 15 di visa_pattern = r'^4[0-9]{12}(?:[0-9]{3})?$' mastercard_pattern = r'^5[1-5][0-9]{14}$' amex_pattern = r'^3[47][0-9]{13}$' if re.match(visa_pattern, card): return 'Visa' elif re.match(mastercard_pattern, card): return 'MasterCard' elif re.match(amex_pattern, card): return 'American Express' else: return None\n\n[End of response]\n\nIn the above code, Copilot has made the following changes:\n\nCreated a new regular expression in the validate_card function to\n\nsupport American Express cards.\n\nAdded a new elif statement to return American Express if the card matches the new regular expression.\n\nOf course, we’re not done yet; we need to add a test to verify that the code\n\nworks as expected. Let’s use the following prompt:\n\n[Prompt]\n\nWrite a test for the code; validate_card should return American Express for\n\na valid card.\n\n[End of prompt]\n\nYour test should look like below:\n\n[Prompt response]\n\n```python import unittest from your_module import validate_card # replace with the actua class TestValidateCard(unittest.TestCase): def test_validate_card_returns_amex_for_valid_amex_card(sel # Arrange valid_amex_card = '378282246310005' # Act result = validate_card(valid_amex_card) # Assert self.assertEqual(result, 'American Express') if __name__ == '__main__': unittest.main() ```\n\n[End of response]\n\nThe new test, test_validate_card_returns_amex_for_valid_amex_card, was added by Copilot and tests that the validate_card function returns\n\nAmerican Express for a valid American Express card.\n\nAt this point, it’s a good idea to keep iterating; you not only want to test that\n\nthe code works for a valid card but also for an invalid card, and so on.\n\nYou can use Copilot as a challenge to write more tests.\n\nImproving performance\n\nImproving performance is a common task but can be tricky to achieve.\n\nUsually, you need specific tools to measure performance, and you need to\n\nknow what to measure. A good way to find out how to improve your code\n\nin this way is to use a decorator to profile how long a method takes to\n\nexecute. Then, you can measure specific methods and find out which\n\nmethods to improve.\n\nHere’s an example of a decorator being used on a function that sleeps for 2\n\nseconds:\n\nimport time def profile(func): def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(f'{func.__name__} took {end - start} seconds') return result return wrapper @profile def sleep_two_seconds(): time.sleep(2) sleep_two_seconds()\n\nThe profile function is a decorator that takes a function as input and\n\nreturns a function that wraps the input function. The wrapper function measures the time it takes to execute the input function and prints the result.\n\nWere you to run this code via the terminal, you would see output similar to\n\nthe following:\n\nsleep_two_seconds took 2.000000238418579 seconds\n\nOK, so how do we use this to improve performance? Let’s use GitHub\n\nCopilot and see how we can get advice on improving performance on a\n\npiece of code.\n\nHere’s the code we want to improve. This processes records and looks for\n\nduplicates:\n\ndef process_records(records): for i in range(len(records)): for j in range(len(records)): if i != j and records[i] == records[j]: print(f'Duplicate record found: {records[i]}')\n\nMost developers looking at this will refer back to their algorithms and data\n\nstructures books and start thinking about things like Big O notation, a way\n\nto measure the performance of an algorithm. The main thing to focus on is\n\nreducing the number of costly operations to improve performance while\n\nretaining the same functionality.\n\nSo, the question we should ask GitHub Copilot is, how do we improve this\n\ncode? Let’s use the following prompt:\n\n[Prompt]\n\nImprove the performance of the code.\n\n[End of prompt]\n\nWe get something like this back:\n\n[Prompt response]\n\ndef process_records(records): record_set = set() for record in records: if record in record_set:\n\nprint(f'Duplicate record found: {record}') else: record_set.add(record)\n\n[End of response]\n\nSo, why is this code better? Well, let’s prove it:\n\n1. First, by calculating the Big O notation for the original code and then\n\nfor the new code.\n\n2. Second, by using the decorator we created earlier.\n\nBig O notation calculation Try this prompt:\n\n[Prompt]\n\nWhat’s the Big O complexity for this code? Show me how you calculated it\n\nas well.\n\n[End of prompt]\n\nYou should get something like this:\n\n[Prompt response]\n\nThe time complexity of the provided code is O(n^2), where n is the number\n\nof records.\n\nHere’s how it’s calculated:\n\nThe outer loop runs n times, where n is the number of records. This is\n\nbecause it iterates over each record in the list.",
      "page_number": 234
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 252-269)",
      "start_page": 252,
      "end_page": 269,
      "detection_method": "synthetic",
      "content": "Inside the outer loop, there’s an inner loop that also runs n times for\n\nthe same reason.\n\nSince the inner loop runs n times for each iteration of the outer loop,\n\nthe total number of iterations is n * n, or n^2.\n\nTherefore, the time complexity is O(n^2).\n\nThis means that if you double the number of records, the time it takes to process them will increase by about four times. This is a characteristic of\n\nquadratic time complexity.\n\n[End of response]\n\nOK, O(n^2) is not great, but what about the new code?\n\nRun the same prompt but on the new code:\n\n[Prompt response]\n\nThe time complexity of the provided code is O(n), where n is the number of\n\nrecords.\n\nHere’s how it’s calculated:\n\nThe loop runs n times, where n is the number of records. This is\n\nbecause it iterates over each record in the list.\n\nInside the loop, the operations (checking if a record is in the set and\n\npotentially adding it to the set) are constant time, O(1), because set\n\noperations in Python are typically constant time on average.\n\nTherefore, the time complexity is O(n) * O(1), which simplifies to O(n).\n\nThis means that if you double the number of records, the time it takes to\n\nprocess them will also double, which is a characteristic of linear time complexity.\n\n[End of response]\n\nO(n) is better than O(n^2), but how much better? Let’s measure this in the\n\nnext section.\n\nMeasuring performance OK, so algorithmically, we know that the new code is better, but how much\n\nbetter? Let’s measure it by using the decorator and calling the code. Let’s\n\nuse the input of 1,000 records and 10,000 records and compare the time it\n\ntakes to execute the code.\n\nOK, let’s see if the new code is faster than the old code. Let’s try with\n\n10,000 records:\n\n# old code @profile def process_records(records): for i in range(len(records)): for j in range(len(records)): if i != j and records[i] == records[j]: print(f'Duplicate record found: {records[i]}') records_10000 = [i for i in range(10000)] process_records(records_10000)\n\nRunning this code, you should see the following output:\n\nprocess_records took 5.193912506103516 seconds\n\nNow, let’s run the new code:\n\n# new code @profile def process_records(records):\n\nrecord_set = set() for record in records: if record in record_set: print(f'Duplicate record found: {record}') else: record_set.add(record) records_10000 = [i for i in range(10000)] process_records(records_10000)\n\nRunning this code, you should see the following output:\n\nprocess_records took 0.0011200904846191406 seconds\n\nAs you can see, by combining your knowledge with GitHub Copilot, you can improve your code.\n\nYour code won’t always look this obvious, and you might\n\nneed to do more work to improve performance. You’re\n\nrecommended to use a profiler to measure performance, and then use GitHub Copilot to help you improve the code.\n\nImproving maintainability Another interesting use case is using GitHub Copilot to help you improve\n\nthe maintainability of your code. So what are some things that you can do to\n\nimprove the maintainability of your code? Here’s a list:\n\nImprove the naming of variables, functions, classes, etc.\n\nSeparate concerns: For example, separate business logic from\n\npresentation logic.\n\nRemove duplication: Especially in large codebases, you’re likely to\n\nfind duplication.\n\nImprove readability: You can improve readability by, for example,\n\nusing comments, docstrings, event tests, and more.\n\nLet’s start with a codebase and see how we can improve it. Here’s the code:\n\ndef calculate_total(cart, discounts): # Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantit return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): items_sorted = sorted(items, key=lambda x: x.price) total = 0 for item in items_sorted[1:]: total += item.price * item.quantity return total # Calculate totals for each discount totals = [] for discount in discounts: if discount == '3 for 2': totals.append((discount, three_for_two(cart))) elif discount == 'Christmas Discount': totals.append((discount, christmas_discount(cart))) elif discount == 'Cheapest Free': totals.append((discount, cheapest_free(cart))) # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Free'] # Define cart\n\ncart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\nThe above codebase describes the following at a high level:\n\nA function, calculate_total, that calculates the total for a cart and a list of discounts\n\nA list of discounts\n\nA cart\n\nWhat you can see from this codebase is that it’s not very maintainable, as it’s hard to understand what’s going on.\n\n1. Identify the problems. What problems do you see? Let’s first ask GitHub Copilot what it thinks about the code and decide what to do next. Let’s use the following prompt:\n\n[Prompt]\n\nWhat do you think about the code? What would you do to improve it?\n\n[End of prompt]\n\nYou get a lengthy response back with a bullet list and improved code. Let’s\n\nlook at the bullet list first:\n\n[Prompt response]\n\nUse Classes for Items and Discounts: Right now, items and discounts\n\nare represented as dictionaries and strings, respectively. Using classes\n\nfor these entities would make the code more readable and flexible.\n\nSeparate Discount Functions: The discount functions are currently defined inside the calculate_total function. It would be better to\n\ndefine these functions separately to improve readability and make the\n\ncode easier to test and maintain.\n\nUse a More Extensible Approach for Applying Discounts: The current approach for applying discounts involves a series of if-elif statements. This could be made more extensible by storing the discount functions in a dictionary or a list of Discount objects, and\n\nthen iterating over that collection to apply the discounts.\n\n[End of response]\n\nThe problems identified above all seem reasonable – using classes,\n\nseparating functions, and using dictionary.\n\n2. Add tests and de-risk change Before we look at a suggested code change, we need to minimize the risk of introducing a bug. The first thing we should do is check our test coverage.\n\nIn this case, we don’t have any tests, so we need to write some. Let’s use\n\nthe following prompt:\n\n[Prompt]\n\nWrite tests for this code, covering all possible execution paths.\n\n[End of prompt]\n\n[Prompt response]\n\nimport unittest class TestCalculateTotal(unittest.TestCase): def test_no_discounts(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = [] total = calculate_total(cart, discounts) self.assertEqual(total, 80) def test_three_for_two_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['3 for 2'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_christmas_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['Christmas Discount'] total = calculate_total(cart, discounts) self.assertEqual(total, 40) def test_cheapest_free_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['Cheapest Free'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_multiple_discounts(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1},\n\n{'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['3 for 2', 'Christmas Discount', 'Cheapest total = calculate_total(cart, discounts) self.assertEqual(total, 40) if __name__ == '__main__': unittest.main()\n\n[End of response]\n\nNow, you should never trust what an AI generates, so take\n\nyour time to review the code, run a test coverage tool, and\n\nensure that you’re happy with the code before you proceed\n\nto the next step, which is to improve the code.\n\nTo improve the code from the first suggestion from Copilot, you want to\n\nensure that it runs and uses classes like Item when you add items to a cart,\n\nfor example. Below is the result of those alterations:\n\nimport unittest from discount_old import calculate_total, Item from item import Item class TestCalculateTotal(unittest.TestCase): def test_no_discounts(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = [] total = calculate_total(cart, discounts) self.assertEqual(total, 80) def test_three_for_two_discount(self): cart = [ Item('item1', 10, 3),\n\nItem('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['3 for 2'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_christmas_discount(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['Christmas Discount'] total = calculate_total(cart, discounts) self.assertEqual(total, 40) def test_cheapest_free_discount(self): cart = [ Item('item1', 10, 3), #30 Item('item2', 20, 1), # 20 Item('item3', 30, 1) # 30 ] discounts = ['Cheapest Free'] total = calculate_total(cart, discounts) self.assertEqual(total, 60) def test_multiple_discounts(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['3 for 2', 'Christmas Discount', 'Cheapest total = calculate_total(cart, discounts) self.assertEqual(total, 40) if __name__ == '__main__': unittest.main()\n\nAnd for the application code:\n\nfrom item import Item def calculate_total(cart, discounts):\n\n# Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantit return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): if not items: return 0 items_sorted = sorted(items, key=lambda x: x.price * x. cheapest_item = items_sorted[0] total = 0 for item in items: total += item.price * item.quantity total -= cheapest_item.price * cheapest_item.quantity i return total # Calculate totals for each discount totals = [] for discount in discounts: if discount == '3 for 2': totals.append((discount, three_for_two(cart))) elif discount == 'Christmas Discount': totals.append((discount, christmas_discount(cart))) elif discount == 'Cheapest Free': totals.append((discount, cheapest_free(cart))) # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # if run as script, if __name__ == '__main__': # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Fre # Define cart cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1)\n\n] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\nOK, so we now have a codebase that we’re happy with and that’s passing\n\ntests. Let’s now look at the suggested code change from GitHub Copilot.\n\n3. Implement change and improve maintainability OK, we feel confident that we have good test coverage and we’re ready to improve the code. Let’s use the following prompt:\n\n[Prompt]\n\nImprove the maintainability of the code.\n\n[End of prompt]\n\nYou should get something like this:\n\n[Prompt response]\n\nTo improve the maintainability of the code, we can make the following\n\nchanges:\n\nMove the discount functions out of calculate_total to make them\n\nstandalone functions. This improves readability and makes it easier to\n\ntest these functions individually.\n\nUse a dictionary to map discount names to their corresponding functions. This eliminates the need for multiple if/elif statements when applying discounts.\n\nfrom item import Item # Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantity // return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): if not items: return 0 items_sorted = sorted(items, key=lambda x: x.price * x.quan cheapest_item = items_sorted[0] total = 0 for item in items: total += item.price * item.quantity total -= cheapest_item.price * cheapest_item.quantity if ch return total def calculate_total(cart, discounts): # Map discount names to functions discount_functions = { '3 for 2': three_for_two, 'Christmas Discount': christmas_discount, 'Cheapest Free': cheapest_free } # Calculate totals for each discount totals = [] for discount in discounts: if discount in discount_functions: totals.append((discount, discount_functions[discoun # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # if run as script, if __name__ == '__main__': # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Fre # Define cart\n\ncart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\n[End of response]\n\nThe code is now easier to maintain, as the following changes have been\n\nintroduced:\n\ncalculate_total is a vastly smaller function, where each of the functions it used to contain has been moved out\n\nThe same method mentioned also now uses a dictionary to map names\n\nto functions\n\nThe key to using Copilot effectively is to keep iterating,\n\nkeep making small changes, and see what Copilot suggests.\n\nChange the prompt slightly, see what happens, and go with\n\nthe output that best suits your needs.\n\nChallenge Try improving the code further using a prompt and GitHub Copilot. What\n\nshould happen if there’s a problem with the payment? Should you log it,\n\nmaybe raise an error, etc.?\n\nUpdating an existing e- commerce site Let’s continue working on the e-commerce site that we presented in\n\nprevious chapters. In this chapter, we’ll focus on improving the codebase\n\nand adding new features.\n\nFor reference, let’s show the basket.html file we’ve started to create:\n\n<!-- a page showing a list of items in a basket, each item shou <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\">\n\n<!-- add bootstrap --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn </head> <body> <!-- add 3 basket items with each item having id, name, pr\n\n<!-- <div class=\"container\"> <div id=\"basket\" class=\"basket\"> </div> </div> --> <!-- add app.js --> <!-- add app.js, type javascript -->\n\n<div id=\"basket\" class=\"basket\"> <!-- render basket from Vue app, use Boostrap --> <div v-for=\"(item, index) in basket\" class=\"basket-item <div class=\"basket-item-text\"> <h2>{{ item.name }}</h2> <p>Price: {{ item.price }}</p> <p>Quantity: {{ item.quantity }}</p> <p>Sum: {{ item.price * item.quantity }}</p>\n\n</div> <div class=\"basket-item-buttons\"> <button type=\"submit\" class=\"btn btn-primary bt <button type=\"submit\" class=\"btn btn-primary bt </div> </div> </div> <!-- add Vue 3 --> <script src=\"https://unpkg.com/vue@next\"></script> <!-- add boostrap code for Vue 3 --> <script> let app = Vue.createApp({ data() { return { basket: [ { id: 1, name: 'Product 1', price: 100, quantity: 1 }, { id: 2, name: 'Product 2', price: 200, quantity: 2 }, { id: 3, name: 'Product 3', price: 300, quantity: 3 }, { id: 4, name: 'Product 4', price: 400, quantity: 4 } ] } },\n\nmethods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { let total = 0; for (let i = 0; i < this.basket.length; i++ total += this.basket[i].price * this.ba }\n\nreturn total; } } }); app.mount('#basket'); </script> <!-- <script src=\"js/app.js\"></script> --> </body>\n\nThis is an early version of a prototype; it’s not meant to be\n\nproduction-ready code. However, we can use this code to\n\nimprove it and make it production-ready.\n\nNow that we know this code isn’t great, let’s improve it with Copilot and\n\nthe following prompt:\n\n[Prompt]\n\nImprove the code, making it production-ready.\n\n[End of prompt]\n\nThis prompt highlights some of the problems with the code, namely:\n\n[Prompt response]\n\nSeparate JavaScript and CSS into external files: This makes the\n\ncode more maintainable and allows for caching of these resources.\n\nUse a build tool or a module bundler: Tools like Webpack or Parcel\n\ncan help you manage dependencies, transpile modern JavaScript to\n\nolder versions for better browser compatibility, and minify your code\n\nfor production.\n\nError Handling: Add error handling to your methods to ensure your\n\napplication can gracefully handle unexpected situations.\n\nVue Component Structure: Instead of creating the entire Vue app in\n\none script, break it down into reusable components.\n\nAccessibility: Ensure that your HTML is accessible by using\n\nappropriate ARIA roles and properties.\n\n[End of response]\n\nThat’s a great list of things we can ask about next, but let’s see what code\n\nchanges it suggests. It suggests code separated into two different files, one\n\nfor the HTML and one for the JavaScript:\n\n<!-- index.html --> <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\"> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn </head> <body> <div id=\"basket\" class=\"basket\"></div>\n\n<script src=\"https://unpkg.com/vue@next\"></script> <script src=\"js/app.js\"></script> </body> </html>\n\nNow, let’s look at how the JavaScript was improved:\n\n// app.js let app = Vue.createApp({ data() { return { basket: [ { id: 1, name: 'Product 1', price: 100, quantit { id: 2, name: 'Product 2', price: 200, quantit { id: 3, name: 'Product 3', price: 300, quantit { id: 4, name: 'Product 4', price: 400, quantit ] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + } } }); app.mount('#basket');",
      "page_number": 252
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 270-287)",
      "start_page": 270,
      "end_page": 287,
      "detection_method": "synthetic",
      "content": "A great step forward. As a developer, you can see that the JavaScript code\n\nuses static data, so let’s change that with the following prompt:\n\n[Prompt]\n\nImprove the code, use dynamic data by calling an API, and also generate a\n\nseparate Python API with Flask that has the route /api/basket, containing the\n\nbasket data.\n\n[End of prompt]\n\nNow, we get two files, the API and the JavaScript code. Let’s look at the\n\nAPI first:\n\n[Prompt response]\n\n# app.py from flask import Flask, jsonify app = Flask(__name__) @app.route('/api/basket') def get_basket(): basket = [ { 'id': 1, 'name': 'Product 1', 'price': 100, 'quantity { 'id': 2, 'name': 'Product 2', 'price': 200, 'quantity { 'id': 3, 'name': 'Product 3', 'price': 300, 'quantity { 'id': 4, 'name': 'Product 4', 'price': 400, 'quantity ] return jsonify(basket) if __name__ == '__main__': app.run(debug=True)\n\nAbove you get a route /api/basket that returns the basket data. Let’s look\n\nat the improved JavaScript code:\n\nlet app = Vue.createApp({ data() {\n\nreturn { basket: [] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + } }, mounted() { fetch('/api/basket') .then(response => response.json()) .then(data => { this.basket = data; }); } }); app.mount('#basket');\n\n[End of response]\n\nGreat! We’re now getting data from the API. How can we improve the\n\ncode? If you’ve built a Vue app before, you will know that it involves using\n\nthe Vue CLI and has a build step. Let’s ask GitHub Copilot about this with\n\nthe following prompt:\n\n[Prompt]\n\nImprove the code, use the Vue CLI, and add a build step.\n\n[End of prompt]\n\nNow, we get both commands and file changes back as a response, as\n\nfollows:\n\n[Prompt response]\n\nInstall Vue CLI globally on your machine:\n\nnpm install -g @vue/cli\n\nCreate a new Vue project\n\nvue create my-project cd my-project\n\nNow, let’s create a new Vue component for the basket. Create a new file Basket.vue in the src/components directory:\n\n<!-- src/components/Basket.vue --> <template> <div id=\"basket\"> <!-- Your basket template goes here --> </div> </template> <script> export default { data() { return { basket: [] }; }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) {\n\nif (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + item.p } }, mounted() { fetch('/api/basket') .then(response => response.json()) .then(data => { this.basket = data; }); } }; </script> <style scoped> /* Your CSS goes here */ </style>\n\nThen, import and use this component in your main App.vue:\n\n<!-- src/App.vue --> <template> <div id=\"app\"> <Basket /> </div> </template> <script> import Basket from './components/Basket.vue'; export default { name: 'App', components: { Basket } }; </script>\n\n<style> /* Your CSS goes here */ </style>\n\nNow, you can use the Vue CLI’s build command to create a production-\n\nready version of your app:\n\nnpm run build\n\nThis will create a dist directory with a production-ready version of your\n\napp.\n\n[End of response]\n\nThis looks like a step in the right direction. However, you should always\n\ntest the code to make sure it works as expected.\n\nAs a challenge, see if the above code works and if you can improve it\n\nfurther.\n\nCode is likely to change between versions of a framework.\n\nIn this case, we’re using Vue.js, so make sure to consult the\n\ndocumentation for the version that you use.\n\nAssignment Find a piece of code you want to improve. Follow the process outlined in\n\nthis chapter to:\n\n1. Identify the problems. What problems do you see?\n\n2. Add tests and de-risk changes.\n\n3. Implement the changes and improve maintainability.\n\nIf you don’t have a piece of code you want to improve, try using the code\n\nfrom this chapter or the code from the Kata (Gilded Rose) GitHub page:\n\nhttps://github.com/emilybache/GildedRose- Refactoring-Kata.\n\nKnowledge check\n\n1. What’s the difference between greenfield and brownfield\n\ndevelopment?\n\nA: Greenfield development is when you start coding from scratch;\n\nbrownfield development is when you update existing code.\n\n2. What’s the best way to update existing code?\n\nA: The best way is to make small changes and have plenty of tests in\n\nplace.\n\nSummary In this chapter, we established that a very important aspect of writing code\n\nis to update existing code, which is known as brownfield development. We\n\nalso looked at how GitHub Copilot can help you with this task.\n\nThe most important message to take away from this chapter is to ensure that\n\nyou have an approach to updating code that de-risks the changes you’re\n\nabout to make. It’s better to make a small change several times than a big\n\none once. It’s also strongly recommended to have plenty of tests in place\n\nbefore you start changing code.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n11\n\nData Exploration with ChatGPT\n\nIntroduction Data exploration is an integral first step in machine learning, entailing a\n\nthorough examination of a dataset to identify its structure and uncover\n\ninitial patterns and anomalies. This process is critical for setting the stage\n\nfor any further detailed statistical analysis and the development of machine\n\nlearning models.\n\nIn this chapter, the focus is on delineating the process of data exploration,\n\naiming to solidify the understanding for newcomers to machine learning\n\nwhile providing a refresher for the adept. The chapter will navigate through\n\nthe techniques to load and inspect a dataset comprised of Amazon book\n\nreviews, summarize its characteristics, and probe into its variables.\n\nYou will be guided through practical exercises on categorical data evaluation, distribution visualization, and correlation analysis, with the support of Python’s pandas and Matplotlib libraries. The chapter will also\n\ndetail how to employ ChatGPT effectively for data exploration, including\n\nboth the freely available version and the subscription-based plus version, which offers enhanced functionalities.\n\nIt’s important to note that the responses from ChatGPT will depend on how\n\neffectively you communicate your needs through prompts. This variability\n\nis a part of the learning curve and illustrates the interactive nature of\n\nworking with AI in data exploration. Our goal is to equip you with the knowledge to navigate these tools confidently and to begin making data-\n\ndriven decisions.\n\nBusiness problem In e-commerce, effectively analyzing customer feedback is crucial for\n\nidentifying key factors that influence purchasing decisions. This analysis\n\nsupports targeted marketing strategies and helps optimize both the user experience and website design, ultimately enhancing service and product\n\nofferings to customers.\n\nProblem and data domain In this chapter, we will focus exclusively on detailed data exploration using\n\nthe Amazon product review dataset. Our goal is to deeply explore this dataset to unearth insights and discern patterns that can enhance decision-\n\nmaking. We’ll leverage ChatGPT to generate Python code for data manipulation and visualization, providing a hands-on approach to\n\nunderstanding complex data analysis techniques. Additionally, we will explore methods to effectively prompt ChatGPT to deliver tailored insights and code snippets that aid in our exploration tasks.\n\nDataset overview\n\nWe will work with the Amazon product review dataset, which includes a\n\nbroad range of information reflecting consumer feedback and product evaluations. Key features of this dataset encompass identifiers such as marketplace, customer, review, and product details, as well as product titles,\n\ncategories, ratings, and the textual content of reviews. For this exploration, we’ll concentrate on the review_body and review_headline fields, which\n\nprovide rich textual data for analysis. To streamline our focus and enhance clarity in our findings, we will omit neutral sentiments and focus solely on analyzing positive and negative feedback.\n\nFeatures in the dataset include:\n\nmarketplace (string): The location of the product.\n\ncustomer_id (string): A unique identifier for customers.\n\nreview_id (string): A unique identifier for reviews.\n\nproduct_id (string): A unique identifier for products.\n\nproduct_parent (string): A parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the review.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.\n\nreview_date (string): The date of the review.\n\nsentiments (string): The sentiment of the review (positive or\n\nnegative).\n\nThis targeted exploration will allow us to perform in-depth sentiment\n\nanalysis, evaluate the impact of product ratings, and delve into customer feedback dynamics. By focusing on these elements, we aim to fully\n\nleverage the dataset to improve strategic decision-making in e-commerce environments.\n\nFeature breakdown With the Amazon product review dataset and a focus on detailed data\n\nexploration, we will outline the following features to guide users through\n\nunderstanding and analyzing customer feedback effectively:\n\n1. Loading the Dataset: We’ll start by importing the dataset into a\n\npandas DataFrame. This is a powerful data manipulation structure in\n\nPython that facilitates convenient data handling.\n\n2. Inspecting the Data: Our initial exploration will involve displaying the first few entries of the DataFrame to get a feel for the data. We’ll review the column names, understand the types of data that each\n\ncolumn contains, and check for any missing values that need to be addressed.\n\n3. Summary Statistics: To grasp the numerical data’s distribution, we’ll\n\ncompute summary statistics, including mean, median, minimum, and\n\nmaximum values and quartiles. This step helps in understanding the\n\ncentral tendency and spread of the numerical data.\n\n4. Exploring Categorical Variables: For categorical data such as\n\nmarketplace, product category, and sentiment, we’ll examine the different categories and count the number of entries for each. Visual\n\naids like bar charts will be particularly useful here to illustrate the\n\nfrequency of each category.\n\n5. Distribution of Ratings: We will visualize the distribution of star\n\nratings using histograms or bar charts. This visual representation helps\n\nin understanding the general opinion of the reviewers and how ratings\n\nare skewed.\n\n6. Temporal Analysis: By analyzing the review_date column, we’ll\n\nexplore any trends, seasonality, or other temporal patterns in the data.\n\nThis analysis can reveal insights into how sentiments or product popularity change over time.\n\n7. Review Length Analysis: We’ll examine the review_body to\n\nunderstand the amount of information provided in the reviews by\n\ncalculating descriptive statistics for review length, such as mean,\n\nmedian, and maximum lengths. This step provides insights into the\n\ndepth of feedback that customers provide.\n\n8. Correlation Analysis: Lastly, we will investigate the correlation\n\nbetween numeric variables like star ratings, helpful votes, and total\n\nvotes, using correlation matrices or scatter plots. This analysis helps in\n\nidentifying potential relationships between different quantitative aspects of the data.\n\nBy systematically breaking down these features, we will thoroughly\n\nunderstand the dataset, uncovering insights that can enhance decision-\n\nmaking and strategy formulation in e-commerce contexts.\n\nPrompting strategy To utilize ChatGPT effectively for data exploration of the Amazon product\n\nreview dataset, we need to establish clear prompting strategies tailored to\n\ngenerate Python code and data insights. Here’s how we can approach this.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 – Task: The specific goal is to explore the Amazon product review\n\ndataset thoroughly using various statistical and visualization techniques.\n\n1.2 – Actions: The key steps in exploring this dataset include:\n\nData Loading: Load the dataset into a pandas DataFrame.\n\nData Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\nStatistical Summaries: Calculate summary statistics for numerical\n\ndata.\n\nCategorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\nRating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.\n\nTemporal Trends: Examine trends over time from the review dates.\n\nReview Text Analysis: Analyze the length and sentiment of review texts.\n\nCorrelation Study: Assess correlations between numerical variables.\n\n1.3 – Guidelines: We will provide the following guidelines to ChatGPT in\n\nour prompt:\n\nCode should be compatible with a Jupyter Notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method\n\nused in the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 – Persona: Assume the persona of a data analyst seeking to uncover\n\nactionable insights from the Amazon product review dataset.\n\n2.2 – Instructions: Request ChatGPT to generate code for each specific\n\nanalysis, proceeding sequentially and waiting for user validation before moving to the next task.\n\n2.3 – Context: Given that the focus is on sentiment analysis using the\n\nAmazon product review dataset, ChatGPT is not aware of the dataset and its\n\ncharacteristics, so additional context will be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy 3.1 – Learn:\n\nHighlight the need to understand data through various analytical\n\ntechniques, from basic statistics to complex correlations and temporal\n\nanalysis.\n\n3.2 – Improvise:\n\nAdapt the analysis based on initial findings. For instance, if certain\n\ncategories of products show unusual trends, deepen the analysis in these areas.\n\n3.3 – Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4 – Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and\n\nvalidity. This is used throughout the chapter.\n\nData exploration of the Amazon review dataset using the free version of ChatGPT ChatGPT premium version has a code interpreter, but first, we will use the\n\nfree version of ChatGPT. We will craft our initial prompt carefully to\n\ninclude all the features, but we will instruct it to wait for user feedback after\n\nproviding code for each feature.\n\nFeature 1: Loading the dataset Let’s craft our initial prompt to load the dataset.\n\n[Prompt]\n\nI want to explore the Amazon product review dataset thoroughly using\n\nvarious statistical and visualization techniques (TAG 1.1), which consists of the following fields (PIC 2.3):\n\nmarketplace (string): The location of the product.\n\ncustomer_id (string): A unique identifier for customers.\n\nreview_id (string): A unique identifier for reviews.\n\nproduct_id (string): A unique identifier for products.\n\nproduct_parent (string): A parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the\n\nreview.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.\n\nreview_date (string): The date of the review.\n\nsentiments (string): The sentiment of the review (positive or negative).\n\nThis should consist of the following steps (TAG 1.2):\n\n1. Data Loading: Load the dataset into a pandas DataFrame.\n\n2. Data Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\n3. Statistical Summaries: Calculate summary statistics for numerical\n\ndata.\n\n4. Categorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\n5. Rating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.\n\n6. Temporal Trends: Examine trends over time from the review dates.\n\n7. Review Text Analysis: Analyse the length and sentiment of review\n\ntexts.\n\n8. Correlation Study: Assess correlations between numerical variables.\n\nI am a data analyst seeking to uncover actionable insights (PIC 2.1), so\n\nplease provide detailed code and strictly follow the below instructions\n\n(TAG 1.3):\n\nCode should be compatible with a Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will then be copied into\n\nthe text block of the notebook in detail for each method used in the\n\ncode before it is provided. Also note that the dataset is placed in\n\nGoogle Drive at this path: ./Chapter05/amazon_product_review_data.csv. So, add the code to\n\nmount the drive and load the dataset (PIC 2.2).\n\nProvide one step at a time and wait for user feedback (PIC 2.2).\n\n[End of prompt]\n\nYou should receive a response similar to below, where ChatGPT has\n\nprovided us with the code for Step 1 – Data Loading, along with detailed\n\ncomments and explanations.\n\n[Prompt response]\n\nSure, let’s start with the first step: loading the dataset into a pandas\n\nDataFrame from Google Drive. Here’s how we can proceed:\n\nStep 1: Data Loading\n\nTo load the dataset from Google Drive into a pandas DataFrame, we’ll\n\nfollow these steps:\n\nMount Google Drive: Connect to your Google Drive where the\n\ndataset is located.\n\nLoad the Dataset: Read the dataset file into a pandas DataFrame.\n\nDetailed Code Explanation:\n\n1. Mount Google Drive: This step allows us to access files stored in\n\nGoogle Drive directly from a Google Colab or Jupyter Notebook\n\nenvironment:\n\nFrom the google.colab import drive: Import the drive module\n\nfrom the google.colab package.\n\ndrive.mount (\"/content/drive\"): Mounts your Google Drive at\n\nthe specified path (/content/drive in this case). Follow the instructions to authorize and obtain an access code.\n\n2. Load the Dataset:\n\nimport pandas as pd: Import the pandas library as pd for data\n\nmanipulation.\n\nfile_path = '/content/drive/My Drive/Chapter05/amazon_product_review_data.csv': Define the\n\npath to your CSV file in Google Drive.\n\ndf = pd.read_csv(file_path): Use pd.read_csv() to read the CSV file into a DataFrame named df.\n\nHere’s the complete code snippet:\n\n# Mount Google Drive to access the dataset from google.colab import drive drive.mount('/content/drive') # Import pandas for data manipulation",
      "page_number": 270
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 288-308)",
      "start_page": 288,
      "end_page": 308,
      "detection_method": "synthetic",
      "content": "import pandas as pd # Define the file path in your Google Drive file_path = '/content/drive/My Drive/Chapter05/amazon_product_r # Load the dataset into a pandas DataFrame df = pd.read_csv(file_path) # Display the first few rows of the dataframe to verify the dat df.head()\n\nUser Feedback:\n\nPlease execute this code in your Jupyter Notebook environment after\n\nreplacing file_path with the correct path to your dataset. Let me know if\n\nyou encounter any issues or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in Google Colab for every step. The code\n\nloads the dataset from Google Drive. It will also request permission to\n\naccess Google Drive.\n\nPlease refer to the notebook “Data Exploration with ChatGPT.ipynb” for\n\nthe executable code snippets, or copy the code from the prompt response.\n\nYou should see a message saying “Permit this Notebook to access Google Drive.”\n\nThe code runs perfectly fine without applying any modifications or any\n\nerrors and displays the first few rows of the dataset.\n\nYou should see the content of the Notebook and customer reviews.\n\nFeature 2: Inspecting the data We will now instruct ChatGPT to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nGreat! Let’s move on to the next step:\n\nStep 2: Data Inspection\n\nIn this step, we will inspect the loaded dataset to:\n\nCheck for missing data.\n\nUnderstand the data types of each column.\n\nInspect the first few entries to get a sense of the data structure.\n\nDetailed Code Explanation:\n\n1. Check for Missing Data:\n\ndf.info(): This method provides a concise summary of the\n\nDataFrame, including the number of non-null entries in each\n\ncolumn. It also shows the data types of each column, which\n\nhelps us understand how to manipulate the data.\n\n2. Understand Data Types:\n\nBy examining the output of df.info(), we can identify if any columns have incorrect data types (e.g., numerical columns mistakenly stored as strings).\n\n3. Inspect the First Few Entries:\n\ndf.head(): This method displays the first few rows of the\n\nDataFrame. It allows us to visually inspect the columns, their\n\nvalues, and get an initial understanding of what the dataset\n\ncontains.\n\nHere’s the code for data inspection:\n\n# Check for missing data and data types df.info() # Display the first few rows of the dataframe df.head()\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. After running this\n\nstep, you should be able to see a summary of the dataset and the first few\n\nrows. Let me know if everything looks okay or if you have any specific questions!\n\n[End of response]\n\nWe evaluate the output from ChatGPT and execute the code (LIFE 3.4) in\n\nthe notebook, and it displays the information.\n\nWe will utilize the prompting strategy to provide the output for ChatGPT\n\n(LIFE 3.3) to interpret:\n\n[Prompt]\n\n<class 'pandas.core.frame.DataFrame'> RangeIndex: 500 entries, 0 to 499 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 market_place 500 non-null object 1 customer_id 500 non-null object 2 review_id 500 non-null object 3 product_id 500 non-null object 4 product_parent 500 non-null object\n\n5 product_title 500 non-null object 6 product_category 500 non-null object 7 star_rating 500 non-null int64 8 helpful_votes 500 non-null int64 9 total_votes 500 non-null int64 10 verified_purchase 500 non-null object 11 review_headline 500 non-null object 12 review_body 500 non-null object 13 review_date 500 non-null object 14 sentiments 500 non-null object dtypes: int64(3), object(12) memory usage: 58.7+ KB market_place customer_id review_id product_id produ 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" \"2 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" \" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" \"\n\n[End of prompt]\n\nWe should receive a response like below, where ChatGPT provides an\n\nexplanation and findings:\n\n[Prompt response]\n\nIt seems like the dataset has been successfully loaded, and here’s what we can infer from the output of df.info() and df.head():\n\nData Inspection Findings:\n\n1. Basic Information:\n\nThe dataset contains 500 entries (rows) and 15 columns.\n\nThere are both numerical (int64) and categorical (object) data types.\n\nAll columns have 500 non-null entries, indicating there are no missing values.\n\n2. Column Details:\n\nmarket_place, customer_id, review_id, product_id,\n\nproduct_parent, product_title, product_category, verified_purchase, review_headline, review_body,\n\nreview_date, and sentiments are all identified as object (string) types, which aligns with their expected content based on their\n\ndescriptions.\n\nstar_rating, helpful_votes, and total_votes are correctly\n\nidentified as int64 (integer) types.\n\n3. Initial Data Preview:\n\nThe df.head() output shows the first few rows of the dataset,\n\ngiving us a glimpse into the structure and content of each column. We can see examples of the market_place, customer_id, star_rating, helpful_votes, total_votes,\n\nreview_headline, review_body, review_date, and sentiments columns.\n\nNext Steps:\n\nBased on this inspection:\n\nThe dataset appears to be clean with no missing values.\n\nWe have a clear understanding of the columns and their data types.\n\nNext, we can proceed with Step 3: Statistical Summaries to calculate summary statistics for the numerical data (star_rating, helpful_votes,\n\nand total_votes). Would you like to continue with that? Let me know if there’s anything specific you’d like to explore further or if we should\n\nproceed to the next step!\n\n[End of response]\n\nFeature 3: Summary statistics As there are no missing or null values, we will now instruct ChatGPT to\n\nproceed to the next step to display summary statistics:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code\n\nto display summary statistics:\n\n[Prompt response]\n\nGreat! Let’s move on to Step 3:\n\nStep 3: Statistical Summaries\n\nIn this step, we will calculate summary statistics for the numerical columns\n\nin our dataset. Specifically, we will focus on:\n\nstar_rating: Rating of the product on a scale of 1 to 5.\n\nhelpful_votes: Number of helpful votes received for the review.\n\ntotal_votes: Total number of votes received for the review.\n\nDetailed Code Explanation:\n\n1. Use of describe():\n\ndf.describe() computes summary statistics for numerical columns. It provides:\n\nCount: Number of non-null values.\n\nMean: Average value.\n\nStd: Standard deviation.\n\nMin: Minimum value.\n\n25%, 50%, and 75%: Percentiles.\n\nMax: Maximum value.\n\n2. Selecting Specific Columns:\n\nWe use df[['star_rating', 'helpful_votes',\n\n'total_votes']].describe() to limit the output to only the numerical columns we are interested in (star_rating,\n\nhelpful_votes, and total_votes).\n\nHere’s the code to calculate and display summary statistics:\n\n# Calculate summary statistics for numerical columns summary_stats = df[['star_rating', 'helpful_votes', 'total_vote # Display the summary statistics print(summary_stats)\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\nstatistical insights into the distribution and central tendencies of the\n\nnumerical variables in your dataset. Let me know if you have any questions\n\nor if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe execute the code, which shows the summary statistics, and we provide\n\nthe output to ChatGPT for interpretation and to understand the stats:\n\n[Prompt]\n\nstar_rating helpful_votes total_votes count 500.000000 500.000000 500.000000 mean 0.796000 0.764000 1.068000 std 0.403373 2.843321 3.698477 min 0.000000 0.000000 0.000000\n\n25% 1.000000 0.000000 0.000000 50% 1.000000 0.000000 0.000000 75% 1.000000 1.000000 1.000000 max 1.000000 36.000000 47.000000\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides an\n\nexplanation of the summary statistics:\n\n[Prompt response]\n\nBased on the summary statistics for the numerical columns (star_rating, helpful_votes, and total_votes), here are the insights we can gather:\n\nSummary Statistics:\n\n1. star_rating:\n\nMean: 0.796\n\nStandard Deviation: 0.403\n\nMin: 0 (indicating the lowest rating)\n\n25th Percentile (Q1): 1\n\nMedian (50th Percentile): 1\n\n75th Percentile (Q3): 1\n\nMax: 1 (indicating the highest rating)\n\nInterpretation: The star_rating column primarily contains a single value of 1 (which typically represents positive reviews in\n\nbinary sentiment data), indicating that most reviews are positive.\n\nThis is supported by the mean being close to 1 and a low\n\nstandard deviation, suggesting little variability in ratings.\n\n2. helpful_votes:\n\nMean: 0.764\n\nStandard Deviation: 2.843\n\nMin: 0\n\n25th Percentile (Q1): 0\n\nMedian (50th Percentile): 0\n\n75th Percentile (Q3): 1\n\nMax: 36\n\nInterpretation: The helpful_votes column has a mean of\n\n0.764, indicating that on average, reviews receive less than one\n\nhelpful vote. The standard deviation of 2.843 suggests variability\n\nin the number of helpful votes received, with some reviews\n\nreceiving as many as 36 helpful votes.\n\n3. total_votes:\n\nMean: 1.068\n\nStandard Deviation: 3.698\n\nMin: 0\n\n25th Percentile (Q1): 0\n\nMedian (50th Percentile): 0\n\n75th Percentile (Q3): 1\n\nMax: 47\n\nInterpretation: The total_votes column has a mean of 1.068, indicating that on average, reviews receive slightly more than\n\none total vote (helpful or unhelpful). The standard deviation of 3.698 suggests variability in the total number of votes received,\n\nwith some reviews receiving as many as 47 votes in total.\n\nNext Steps:\n\nBased on these statistics, we see that the dataset predominantly consists of\n\npositive reviews (a star_rating of 1). The helpful_votes and\n\ntotal_votes metrics show varying levels of engagement with the reviews.\n\nNext, we can move on to Step 4: Categorical Analysis, where we will analyze the categorical variables (market_place, product_category, verified_purchase, and sentiments) using counts and visualizations.\n\nWould you like to proceed with that? Let me know if you have any\n\nquestions or if you’re ready for the next step!\n\n[End of response]\n\nFeature 4: Exploring categorical variables Now, we will proceed to explore the categorical variables.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code\n\nfor the visualization of the categorical variables:\n\n[Prompt response]\n\nGreat! Let’s proceed to Step 4:\n\nStep 4: Categorical Analysis\n\nIn this step, we will analyze the categorical variables in our dataset using\n\ncounts and visualizations. The categorical variables we’ll focus on are:\n\nmarket_place: Location of the product.\n\nproduct_category: Category of the product.\n\nverified_purchase: Whether the purchase was verified (Y/N).\n\nsentiments: Sentiment of the review (positive or negative).\n\nDetailed Code Explanation:\n\n1. Counting Unique Values:\n\ndf['column_name'].value_counts(): This method counts the\n\noccurrences of each unique value in a categorical column.\n\n2. Visualizing Categorical Data:\n\nWe will use bar charts (from the matplotlib library) to visualize the distribution of categorical variables.\n\nHere’s the code to perform categorical analysis:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Set up the matplotlib figure and axes fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10)) axes = axes.flatten() # Plot for 'market_place' sns.countplot(x='market_place', data=df, ax=axes[0]) axes[0].set_title('Distribution of Products by Market Place') # Plot for 'product_category' sns.countplot(x='product_category', data=df, ax=axes[1]) axes[1].set_title('Distribution of Products by Category') axes[1].tick_params(axis='x', rotation=90) # Plot for 'verified_purchase' sns.countplot(x='verified_purchase', data=df, ax=axes[2]) axes[2].set_title('Distribution of Verified Purchases') # Plot for 'sentiments' sns.countplot(x='sentiments', data=df, ax=axes[3]) axes[3].set_title('Distribution of Sentiments') # Adjust layout plt.tight_layout() plt.show()\n\nExplanation of the Visualizations:\n\nMarket Place: Shows the distribution of products across different marketplaces (market_place).\n\nProduct Category: Displays the distribution of products across various categories (product_category).\n\nVerified Purchase: Indicates how many reviews are from verified purchases (verified_purchase).\n\nSentiments: Illustrates the distribution of review sentiments (sentiments), whether they are positive or negative.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment to visualize and\n\nanalyze the categorical variables in your dataset. This will provide insights\n\ninto the distribution and characteristics of different aspects related to the\n\nreviews and products. Let me know if you have any questions or if you’re\n\nready to move on to the next step!\n\n[End of response]\n\nWe execute the code, which runs perfectly fine, and then visualize the\n\nvariables.\n\nFigure 11.1: Categorical variables – visualization\n\nWe can see that the reviews are all from a single marketplace and have\n\nequal number of reviews of all product categories, but there is an imbalance\n\nof positive and negative reviews as well as the distribution of verified\n\npurchases.\n\nFeature 5: Rating distribution We will move on to the rating distribution and instruct ChatGPT to move on\n\nto the next step:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nGreat! Let’s proceed to Step 5:\n\nStep 5: Rating Distribution\n\nIn this step, we will create histograms or bar charts to visualize the distribution of star ratings (star_rating). This will help us understand how\n\nratings are distributed among the reviews.\n\nDetailed Code Explanation:\n\n1. Histogram or Bar Chart:\n\nWe will use sns.countplot() from the Seaborn library to create a bar chart. This is suitable for visualizing the count of each\n\nrating value (1 to 5).\n\n2. Visualization Parameters:\n\nAdjustments such as setting the figure size (figsize) and title\n\n(set_title) are done to enhance the clarity and readability of the plot.\n\nHere’s the code to visualize the rating distribution:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Set up the matplotlib figure plt.figure(figsize=(8, 6)) # Plotting the distribution of star ratings sns.countplot(x='star_rating', data=df) plt.title('Distribution of Star Ratings') plt.xlabel('Star Rating') plt.ylabel('Count') # Show plot plt.show()\n\nExplanation of the Visualization:\n\nThe sns.countplot() function is used to create a bar chart of the\n\nstar_rating column.\n\nThe x-axis represents the star ratings (1 to 5), and the y-axis represents\n\nthe count of reviews with each rating.\n\nThe title, x-label (Star Rating), and y-label (Count) are added to\n\nprovide context and clarity to the plot.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will display a\n\nvisual representation of how star ratings are distributed among the reviews\n\nin your dataset. This step will provide insights into the overall sentiment\n\nexpressed through ratings. Let me know if you have any questions or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will execute the code, which produces the ratings graph.\n\nFigure 11.2: Ratings distribution\n\nBased on these results, there are majorly positive ratings. We will move on\n\nto the next feature.\n\nFeature 6: Temporal trends Now, let’s understand the temporal trends in our dataset for the review date\n\ncolumn. Our dataset doesn’t have much variation, but this will help us\n\nunderstand how to use ChatGPT for it:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nGreat! Let’s move on to Step 6:\n\nStep 6: Temporal Trends\n\nIn this step, we will examine temporal trends from the review_date column\n\nto understand how the reviews are distributed over time. This will help us\n\nidentify any patterns or trends in the review activity.\n\nDetailed Code Explanation:\n\n1. Convert review_date to datetime:\n\nEnsure that the review_date column is in the datetime format for proper temporal analysis.\n\n2. Plotting Temporal Trends:\n\nUse sns.lineplot() or sns.histplot() to visualize the\n\ndistribution of reviews over time.\n\nDepending on the analysis goal, you can choose to plot the count\n\nof reviews per month, per year, or any other relevant time\n\ninterval.\n\nHere’s the code to analyze temporal trends:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Convert 'review_date' to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Set up the matplotlib figure plt.figure(figsize=(12, 6)) # Plotting the count of reviews over time sns.histplot(df['review_date'], bins=30, kde=False, color='blue plt.title('Distribution of Reviews Over Time') plt.xlabel('Review Date') plt.ylabel('Count of Reviews') # Show plot\n\nplt.xticks(rotation=45) plt.tight_layout() plt.show()\n\nExplanation of the Visualization:\n\nDatetime Conversion: pd.to_datetime() function is used to convert\n\nthe review_date column from a string to the datetime format, ensuring accurate temporal analysis.\n\nHistogram Plot: sns.histplot() is used to create a histogram of\n\nreview counts over time (review_date). This helps in visualizing the distribution and frequency of reviews across different dates.\n\nAdjustments: Adjustments such as setting the figure size (figsize),\n\ntitle (title), x-label (xlabel), y-label (ylabel), and rotating x-axis ticks (xticks(rotation=45)) are made to improve the clarity and\n\nreadability of the plot.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\ninsights into how reviews are distributed over time, allowing you to observe\n\nany trends or patterns in review activity. Let me know if you have any\n\nquestions or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will execute the code, which produces a plot that shows the distribution\n\nof reviews over time.\n\nFigure 11.3: Temporal trend\n\nWe can see that the reviews are shown only for two dates, so let’s move to\n\nthe next feature.\n\nFeature 7: Review length analysis [Prompt]\n\nThe reviews are shown for just two dates, 10-2013 and 08-2015.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code,\n\nexplains the significance of the review dates, and proceeds with the next\n\nstep of review length analysis.\n\n[Prompt response]\n\nIf the reviews in your dataset are concentrated on just two dates (October\n\n2013 and August 2015), it suggests that the dataset may not have a wide\n\ntemporal spread of reviews. This information is crucial for understanding\n\nthe context of the data and its limitations in temporal analysis.\n\nGiven this observation, we can still proceed to analyze the review text and\n\nsentiment in Step 7, as planned. This step will focus on examining the\n\nlength and sentiment of the review texts, which can provide additional\n\ninsights beyond temporal trends.\n\nStep 7: Review Text Analysis\n\nIn this step, we will analyze the review text (review_body) to:\n\nCalculate the length of each review.\n\nDetermine the sentiment of each review using the provided sentiments column.\n\nDetailed Code Explanation:\n\n1. Review Length Calculation:\n\nAdd a new column review_length to the DataFrame to store the\n\ncharacter count of each review (review_body).\n\n2. Sentiment Analysis:\n\nUse the sentiments column to categorize reviews as positive or\n\nnegative based on the provided labels.\n\nHere’s the code to perform review the text analysis:\n\n# Calculate the length of each review text df['review_length'] = df['review_body'].apply(len) # Count the number of reviews classified as positive and negati sentiment_counts = df['sentiments'].value_counts() # Display the sentiment counts print(\"Sentiment Counts:\") print(sentiment_counts) # Calculate the average length of reviews average_review_length = df['review_length'].mean()\n\nprint(f\"\\nAverage Review Length: {average_review_length:.2f} ch # Display the first few rows to verify the changes df.head()\n\nExplanation of the Code:\n\nReview Length Calculation: df['review_body'].apply(len)\n\ncomputes the length of each review in characters and stores it in a new column review_length.\n\nSentiment Analysis: df['sentiments'].value_counts() counts the\n\noccurrences of each sentiment category (positive or negative) in the sentiments column.\n\nAverage Review Length: df['review_length'].mean() calculates the average length of reviews in the dataset.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\ninsights into the length and sentiment distribution of the review texts in\n\nyour dataset. Let me know if you have any questions or if you’re ready to\n\nproceed to the next step after reviewing the outputs!\n\n[End of response]\n\nWe execute the code in Jupyter Colab, which works fine, so we provide the\n\noutput to ChatGPT to interpret it:\n\n[Prompt]\n\nSentiment Counts: sentiments positive 398 negative 102 Name: count, dtype: int64",
      "page_number": 288
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 309-326)",
      "start_page": 309,
      "end_page": 326,
      "detection_method": "synthetic",
      "content": "Average Review Length: 243.75 characters market_place customer_id review_id product_id produ 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" \"2 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" \" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" \"\n\n[End of prompt]\n\nWe should receive a response from ChatGPT explaining the results:\n\n[Prompt response]\n\nIt seems like the sentiment analysis and review length calculations have\n\nbeen successfully performed. Here are the insights from the analysis:\n\nSentiment Analysis:\n\nSentiment Counts:\n\nPositive reviews: 398\n\nNegative reviews: 102\n\nReview Length Analysis:\n\nAverage Review Length: 243.75 characters\n\nExplanation:\n\nSentiment Counts: The dataset contains predominantly positive\n\nreviews, with 398 positive reviews and 102 negative reviews.\n\nAverage Review Length: The average length of a review in the\n\ndataset is approximately 243.75 characters.\n\nExample Data Inspection:\n\nThe example snippet you provided shows a sample of the dataset, where we\n\ncan see details such as marketplace, customer ID, review ID, product\n\ndetails, star rating, review text, review date, sentiment, and review length\n\nfor the first few entries.\n\nNext Steps:\n\nBased on this analysis, we have insights into the distribution of sentiments\n\nand the average length of reviews. If there are specific further analyses or\n\nvisualizations you’d like to explore, such as a correlation study or any other\n\naspect of the dataset, please let me know! We can proceed accordingly.\n\n[End of response]\n\nFeature 8: Correlation study We will move on to the last feature, which is a correlation study.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nCertainly! Let’s proceed to Step 8:\n\nStep 8: Correlation Study\n\nIn this step, we will assess the correlations between numerical variables in the dataset. Specifically, we will examine how star_rating, helpful_votes, and total_votes correlate with each other. This analysis\n\nwill help us understand any relationships or dependencies between these\n\nvariables.\n\nDetailed Code Explanation:\n\n1. Correlation Calculation:\n\nUse the .corr() method on the DataFrame to compute the\n\ncorrelation coefficients between numerical columns.\n\n2. Visualization of Correlation:\n\nWe can visualize the correlations using a heatmap to provide a\n\nclear and intuitive representation of the relationships.\n\nHere’s the code to perform the correlation study:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Plotting the correlation heatmap plt.figure(figsize=(8, 6)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vm plt.title('Correlation Matrix of Numerical Variables') plt.show()\n\nExplanation of the Code:\n\nCorrelation Matrix: df[['star_rating', 'helpful_votes', 'total_votes']].corr() computes the correlation coefficients\n\nbetween star_rating, helpful_votes, and total_votes.\n\nHeatmap Visualization: sns.heatmap() from the Seaborn library is\n\nused to create a heatmap of the correlation matrix. The annot=True parameter adds numeric annotations to the heatmap cells for clarity. The cmap='coolwarm' parameter sets the color scheme for the\n\nheatmap, where colors range from blue (negative correlation) to red\n\n(positive correlation).\n\nInterpretation:\n\nThe heatmap will display correlations ranging from -1 to 1, where:\n\nValues closer to 1 indicate a strong positive correlation.\n\nValues closer to -1 indicate a strong negative correlation.\n\nValues close to 0 indicate no significant correlation.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide insights into how star_rating, helpful_votes, and\n\ntotal_votes are related within your dataset. Let me know if you have any questions or if you’re ready to proceed to the next step after\n\nreviewing the correlation heatmap!\n\n[End of response]\n\nWe will execute the code, which plots the correlation matrix and prints the\n\nresults.\n\nFigure 11.4: Correlation matrix\n\nWe can see that helpful votes and total votes are strongly correlated but the\n\nrating is not.\n\nThis concludes data exploration with ChatGPT-3.5; now, let’s see how to\n\nuse ChatGPT-4o.\n\nData exploration of the Amazon review dataset using ChatGPT-4o ChatGPT’s premium version has a code interpreter, which is accessible\n\nusing both ChatGPT-4o and ChatGPT 4. For this section, we will use GPT-\n\n4o, and this time, we will get the code in a single step.\n\nChatGPT has access to several models, at present, you can use GPT-3.5,\n\nGPT-4, and GPT-4o.\n\nLet’s craft our prompt.\n\n[Prompt]\n\nI want to explore the Amazon product review dataset thoroughly using\n\nvarious statistical and visualization techniques (TAG 1.1) for the attached dataset (PIC 2.3). It should consist of the following steps (TAG 1.2):\n\n1. Data Loading: Load the dataset into a pandas DataFrame.\n\n2. Data Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\n3. Statistical Summaries: Calculate the summary statistics for numerical\n\ndata.\n\n4. Categorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\n5. Rating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.\n\n6. Temporal Trends: Examine trends over time from the review dates.\n\n7. Review Text Analysis: Analyze the length and sentiment of review\n\ntexts.\n\n8. Correlation Study: Assess correlations between numerical variables\n\nI am a data analyst seeking to uncover actionable insights (PIC 2.1), so\n\nplease provide detailed code.\n\n[End of prompt]\n\nWe should receive a response similar to below where ChatGPT performs all\n\nthe necessary steps, performs a data exploration, and produces the relevant\n\ncharts:\n\n[Prompt response]\n\nAmazon Product Review Dataset Analysis\n\nWe’ll follow the steps you outlined to explore the dataset thoroughly. Let’s proceed step by step:\n\nStep 1: Data Loading\n\nWe have loaded the dataset into a pandas DataFrame.\n\nStep 2: Data Inspection\n\nWe checked for missing data, understood data types, and inspected the\n\nfirst few entries. The dataset consists of 500 entries and 15 columns\n\nwith no missing data. Here are the first few entries:\n\nmarket_place customer_id review_id product_id 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" product_title produ 0 \"The Cravings Place Chocolate Chunk Cookie Mix... 1 \"Mauna Loa Macadamias, 11 Ounce Packages\" 2 \"Organic Matcha Green Tea Powder - 100% Pure M... 3 \"15oz Raspberry Lyons Designer Dessert Syrup S... 4 \"Stride Spark Kinetic Fruit Sugar Free Gum, 14... star_rating helpful_votes total_votes verified_purchas 0 1 0 0 1 \\t(Y 1 1 0 0 1 \\t(Y 2 1 0 0 0 \\t(N 3 1 0 0 1 \\t(Y 4 1 0 0 1 \\t(Y\n\nreview_headline \\ 0 \"Using these for years - love them.\" 1 \"Wonderful\" 2 \"Five Stars\" 3 \"Five Stars\" 4 \"Five Stars\" review_body review 0 \"As a family allergic to wheat, dairy, eggs, n... 2015- 1 \"My favorite nut. Creamy, crunchy, salty, and ... 2015- 2 \"This green tea tastes so good! My girlfriend ... 2015- 3 \"I love Melissa's brand but this is a great se... 2015- 4 \"good\" 2015-\n\nStep 3: Statistical Summaries\n\nLet’s calculate summary statistics for the numerical data. You should\n\nsee a summary of your data, including stats on the mean, standard\n\ndeviation, and more.\n\nStep 4: Categorical Analysis\n\nWe’ll analyze the categorical variables using counts and visualizations.\n\nStep 5: Rating Distribution\n\nWe’ll create histograms or bar charts to visualize the distribution of\n\nstar ratings.\n\nStep 6: Temporal Trends\n\nWe’ll examine trends over time from the review dates.\n\nStep 7: Review Text Analysis\n\nWe’ll analyze the length and sentiment of review texts.\n\nStep 8: Correlation Study\n\nWe’ll assess correlations between numerical variables.\n\nLet’s start implementing each of these steps in Python code.\n\nDetailed Analysis of the Amazon Product Review Dataset\n\nWe have performed a detailed analysis as outlined. Below are the key\n\nfindings and visualizations for each step.\n\nStep 3: Statistical Summaries\n\nWe calculated summary statistics for the numerical data. Here are some key\n\nstatistics:\n\nStar Rating: Mean = 0.796, Standard Deviation = 0.403, Min = 0, and\n\nMax = 1\n\nHelpful Votes: Mean = 0.764, Standard Deviation = 2.843, Min = 0,\n\nand Max = 36\n\nTotal Votes: Mean = 1.068, Standard Deviation = 3.698, Min = 0, and\n\nMax = 47\n\nStep 4: Categorical Analysis\n\nThe top categories by review count are as follows:\n\nGrocery: 100 reviews\n\nDigital_Ebook_Purchase: 100 reviews\n\nJewelry: 100 reviews\n\nPC: 100 reviews\n\nBooks: 100 reviews\n\nStep 5: Rating Distribution\n\nWe visualized the distribution of the star ratings:\n\nFigure 11.5: Distribution of the star ratings\n\nStep 6: Temporal Trends\n\nWe examined the number of reviews per month:\n\nFigure 11.6: Number of reviews per month\n\nStep 7: Review Text Analysis\n\nWe analyzed the length and sentiment of the review texts:\n\nDistribution of the Review Lengths:\n\nFigure 11.7: Distribution of the review lengths\n\nDistribution of the Sentiments:\n\nFigure 11.8: Distribution of the sentiments\n\nPositive Reviews: 398\n\nNegative Reviews: 102\n\nStep 8: Correlation Study\n\nWe assessed the correlations between the numerical variables and\n\nvisualized the correlation matrix:\n\nFigure 11.9: Correlation matrix\n\nConclusion\n\nThis analysis provided valuable insights into the dataset, including the\n\ndistribution of star ratings, trends over time, and the sentiment of reviews.\n\nWe also explored correlations between numerical variables. These insights\n\ncan help in understanding customer feedback and improving product\n\nofferings.\n\nIf you need any further analysis or have specific questions, feel free to ask!\n\n[End of response]\n\nAs you can see, GPT-4o has successfully performed data exploration by just\n\nuploading the CSV and providing the steps or actions to perform.\n\nAssignment Investigate how different products or categories perform in terms of ratings,\n\nhelpful votes, and review length.\n\nChallenge Use natural language processing (NLP) techniques to analyze the content of the reviews. Extract common themes, keywords, and phrases that are\n\nfrequently mentioned.\n\nSummary In this chapter, we delved into comprehensive data analysis using Python\n\nand pandas, leveraging the Amazon product review dataset. The journey\n\nbegan with data loading and inspection, ensuring the dataset was properly\n\nformatted and free of missing values. You were guided through each step\n\nwith detailed explanations and code samples suitable for Jupyter Notebooks\n\naimed at empowering data analysts to uncover actionable insights\n\neffectively.\n\nWe started by calculating statistical summaries for numerical data, revealing\n\nthat the dataset predominantly consisted of positive reviews. Categorical\n\nanalysis followed, where we explored distributions across different\n\nmarketplaces, product categories, verified purchases, and sentiments.\n\nVisualizations, including histograms and bar charts, provided clear\n\nrepresentations of star rating distributions, emphasizing the predominance\n\nof positive feedback.\n\nTemporal trends analysis uncovered a concentrated spread of reviews,\n\nprimarily in October 2013 and August 2015, offering insights into review activity over time. We then conducted review text analysis, calculating\n\nreview lengths and assessing sentiment counts to understand the dataset’s\n\ncontent more deeply. Finally, a correlation study examined relationships\n\nbetween star ratings and review engagement metrics, like helpful and total\n\nvotes, offering insights into how these factors interact within the dataset.\n\nIn the next chapter, we will learn how to use ChatGPT to build a\n\nclassification model using the same dataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n12\n\nBuilding a Classiﬁcation Model with ChatGPT\n\nIntroduction Building on the foundation set in the previous chapter, where we used\n\nChatGPT for data exploration with Amazon book reviews, Chapter 12\n\ndelves deeper into the realm of supervised learning, with a focus on\n\nclassification. Here, we continue to leverage ChatGPT, applying its\n\ncapabilities to enhance our understanding and application of supervised\n\nlearning techniques in the context of customer reviews.\n\nIn the realm of e-commerce, customer feedback plays a pivotal role in\n\nshaping business strategies and product enhancements. As Bill Gates aptly\n\nstated, “Your most dissatisfied customers are your greatest source of\n\nlearning.” Customer sentiments are often buried within the extensive pool\n\nof product reviews. However, manually scrutinizing this ocean of reviews, which includes various attributes such as product ID, title, text, rating, and\n\nhelpful votes, is an arduous and often unmanageable task.\n\nIn this chapter, we concentrate on classifying customer reviews into two distinct groups: positive and negative. We will utilize the insights gained\n\nfrom ChatGPT in processing and analyzing customer review data.\n\nOur main goal is to show how ChatGPT can simplify the journey of\n\nmachine learning, making it more accessible and less intimidating,\n\nespecially when dealing with intricate topics such as classification in\n\nsupervised learning. We will explore how ChatGPT can break down complex concepts into more digestible parts, provide explanations, and\n\neven generate code snippets, thereby reducing the learning curve for\n\nbeginners or those new to the field.\n\nBy the end of this chapter, you will have a solid understanding of supervised learning and its application in sentiment analysis, along with an\n\nappreciation of how AI tools like ChatGPT can be valuable allies in\n\nlearning and applying machine learning techniques effectively.\n\nBusiness problem In an e-commerce project, understanding customer feedback helps in\n\nidentifying key factors that influence a customer’s decision to make a\n\npurchase, enabling targeted marketing strategies. Additionally, it allows the optimization of the user experience and website design to increase the\n\nlikelihood of providing improved service and products to customers.\n\nProblem and data domain In this section, we aim to build a classification model for customer review\n\nsentiment analysis using the Amazon product review dataset. Leveraging\n\nChatGPT’s capabilities, we’ll generate Python code to construct a classification model, offering readers a practical approach to working with\n\ndatasets and understanding classification techniques. Additionally, we’ll\n\nexplore effective prompting techniques to guide ChatGPT in providing\n\ntailored code snippets and insights for data classification tasks.\n\nDataset overview The Amazon product review dataset contains information on various\n\nproducts and their corresponding reviews. By utilizing this dataset, we can\n\nperform various analyses, including sentiment analysis, trend analysis of customer feedback, and product rating analysis. The ultimate goal is to train a classification model capable of accurately classifying reviews into\n\npositive or negative sentiments, enhancing decision-making processes, and improving customer satisfaction in e-commerce platforms and related\n\nindustries.\n\nFeatures in the dataset include:\n\nmarketplace (string): The location of the product.\n\ncustomer_id (string): The unique identifier for customers.\n\nreview_id (string): The unique identifier for reviews.\n\nproduct_id (string): The unique identifier for products.\n\nproduct_parent (string): The parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the review.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.",
      "page_number": 309
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 327-349)",
      "start_page": 327,
      "end_page": 349,
      "detection_method": "synthetic",
      "content": "review_date (string): The date of the review.\n\nSentiments (string): The sentiment of the review (positive or negative).\n\nThe textual data in review_body and review_headline can be particularly\n\nvaluable for natural language processing tasks, including sentiment\n\nanalysis. For simplification purposes, we have excluded the neutral\n\nsentiment category to focus on building a classification model and\n\nprompting techniques.\n\nBreaking the problem down into features Given the Amazon product review dataset and the application of machine\n\nlearning models for sentiment analysis, we will outline the following\n\nfeatures to guide users through building and optimizing models for\n\nsentiment classification:\n\nData preprocessing and feature engineering: Users will start by preprocessing the text data, including tasks such as tokenization,\n\nlowercasing, and removing stop words and punctuation. Additionally,\n\nfeature engineering techniques such as Term Frequency-Inverse\n\nDocument Frequency (TF-IDF) encoding or word embeddings will\n\nbe applied to represent the text data in a format suitable for machine\n\nlearning models.\n\nModel selection and baseline training: Users will select baseline machine learning models such as logistic regression, Naive Bayes, or\n\nsupport vector machines (SVMs) for sentiment classification. The\n\nselected model will be trained on the preprocessed data to establish a baseline performance for sentiment analysis.\n\nModel evaluation and interpretation: Users will evaluate the\n\nperformance of trained machine learning models using metrics such as accuracy, precision, recall, and F1-score. Additionally, techniques for\n\ninterpreting model predictions, such as feature importance analysis or\n\nmodel explainability methods, will be explored to gain insights into the\n\nfactors influencing sentiment classification decisions.\n\nHandling imbalanced data: This feature addresses the challenge of\n\nimbalanced class distributions in the dataset by implementing\n\ntechniques such as oversampling, under-sampling, or using class weights during model training. Users will explore methods to mitigate\n\nthe impact of class imbalance on model performance and improve the\n\nclassification accuracy of minority classes.\n\nHyperparameter tuning: Users will learn how to optimize the performance of machine-learning models by tuning hyperparameters\n\nsuch as regularization strength, learning rate, and kernel parameters.\n\nThrough techniques like grid search or random search, users will experiment with different hyperparameter configurations to improve\n\nthe model’s performance on the validation set.\n\nExperimenting with feature representation: Users will explore different methods of representing text data as features for machine\n\nlearning models. This feature focuses on comparing the performance\n\nof models trained with different feature representations, such as bag-\n\nof-words, TF-IDF, or word embeddings, to determine the most effective approach for sentiment classification.\n\nBy following these features, users will gain practical insights into building,\n\nfine-tuning, and optimizing machine learning models for sentiment analysis\n\ntasks using the Amazon product review dataset. They will learn how to systematically experiment with different preprocessing techniques, feature\n\nrepresentations, hyperparameter configurations, and class imbalance\n\nhandling strategies to achieve superior performance and accuracy in sentiment classification.\n\nPrompting strategy To effectively utilize ChatGPT for generating code for sentiment analysis\n\nmachine learning tasks, we need to develop a comprehensive prompting\n\nstrategy tailored to the specific features and requirements of sentiment analysis using the Amazon product review dataset.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 – task: The specific task or goal is to build and optimize a machine\n\nlearning model for sentiment analysis using the Amazon product review\n\ndataset.\n\n1.2 – actions: The key steps involved in building and optimizing a machine learning model for sentiment analysis include:\n\nData preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or SVMs.\n\n1.3 – guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into\n\nthe text block of the Notebook, in detail for each method used before providing the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 – persona: Adopt the persona of a beginner who needs step-by-step\n\nguidance on building and optimizing machine learning models for\n\nsentiment analysis tasks using the Amazon product review dataset.\n\n2.2 – instructions: Ask ChatGPT to generate code for each feature one step\n\nat a time and wait for user feedback before proceeding to the next step. Also, provide the path of the dataset from where it will be loaded.\n\n2.3 – context: Given that the focus is on sentiment analysis using the\n\nAmazon product review dataset, ChatGPT is not aware of the dataset and its\n\ncharacteristics, so additional context will be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy\n\n3.1 – learn:\n\nEmphasize the importance of understanding machine learning models\n\nand their components, including feature engineering techniques and\n\nmodel selection.\n\n3.2 – improvise:\n\nRequest ChatGPT to provide code snippets for implementing additional features such as hyperparameter tuning, handling\n\nimbalanced data, and model evaluation techniques.\n\n3.3 – feedback:\n\nShare generated code and model outputs for feedback to ensure\n\neffective learning and understanding.\n\nIncorporate user suggestions and critiques to refine the model and\n\napproach.\n\nProvide error messages to troubleshoot and resolve any issues encountered during model implementation.\n\n3.4 – evaluate:\n\nExecute the generated code provided by ChatGPT to verify accuracy\n\nand validity, ensuring that the model performs effectively in sentiment\n\nanalysis tasks using the Amazon product review dataset.\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews\n\nusing the free version of ChatGPT We will utilize ChatGPT’s free version to build the baseline model.\n\nFeature 1: Data preprocessing and feature engineering Let’s craft our initial prompt for our baseline model.\n\n[Prompt]\n\nI want to create a simple classification model for sentiment analysis of\n\nAmazon Review Dataset (TAG 1.1) which consists of following fields (PIC\n\n2.3)\n\nmarketplace (string): Location of the product.\n\ncustomer_id (string): Unique identifier for customers.\n\nreview_id (string): Unique identifier for reviews.\n\nproduct_id (string): Unique identifier for products.\n\nproduct_parent (string): Parent product identifier.\n\nproduct_title (string): Title of the reviewed product.\n\nproduct_category (string): Category of the product.\n\nstar_rating (int): Rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): Number of helpful votes received for the review.\n\ntotal_votes (int): Total number of votes received for the review.\n\nreview_headline (string): Headline of the review.\n\nreview_body (string): Content of the review.\n\nreview_date (string): Date of the review.\n\nSentiments (string): Sentiment of the review (positive or negative).\n\nIt should consist of following steps (TAG 1.2) and provide one step at a\n\ntime and wait for users feedback (PIC 2.2)\n\nData Preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel Selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or SVMs.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and\n\nstrictly follow the following instructions (TAG 1.3)\n\nThe code should be compatible with Jupyter notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into\n\nthe text block of the Notebook, in detail for each method used before\n\nproviding the code. Also, note that the dataset is placed in Google Drive at this path: /Chapter05/amazon_product_review_data.csv. So, add the code to mount the drive and load the dataset (PIC 2.2).\n\n[End of prompt]\n\nYou should receive a response similar to the following where ChatGPT has\n\nprovided us with detailed comments and explanations:\n\n[Prompt response]\n\nSure, let’s break down the process into steps. We’ll start with data\n\npreprocessing.\n\nStep 1: Data Preprocessing\n\nData preprocessing is essential to clean and prepare the text data for the\n\nsentiment analysis model. We’ll perform the following steps:\n\n1. Loading the Dataset: Load the Amazon review dataset from the\n\nprovided path.\n\n2. Tokenization: Split the text into individual words or tokens.\n\n3. Lowercasing: Convert all text to lowercase to ensure uniformity.\n\n4. Removing Stopwords: Remove common words that don’t carry much\n\nmeaning.\n\n5. Removing Punctuation: Remove punctuation marks from the text.\n\n6. Feature Engineering: Transform text data into a format suitable for\n\nmachine learning models (e.g., TF-IDF encoding).\n\nNow, let’s dive into each step in detail before writing the code.\n\nTokenization:\n\nTokenization is the process of splitting text into individual words or\n\ntokens. It’s a crucial step in natural language processing (NLP) tasks.\n\nTokens are the basic units of text processing. For example, the\n\nsentence “The quick brown fox jumps over the lazy dog” can be\n\ntokenized into [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’,\n\n‘lazy’, ‘dog’].\n\nLowercasing:\n\nLowercasing involves converting all text to lowercase. It helps in\n\nstandardizing the text data and reduces the vocabulary size. For\n\nexample, ‘Hello’ and ‘hello’ will be treated as the same word after\n\nlowercasing.\n\nRemoving Stopwords:\n\nStopwords are common words that do not carry significant meaning,\n\nsuch as ‘the’, ‘is’, ‘and’, etc. Removing stopwords helps in reducing\n\nnoise in the data and improving the model’s performance.\n\nRemoving Punctuation:\n\nPunctuation marks like commas, periods, and exclamation marks don’t\n\ncontribute much to sentiment analysis and can be safely removed from\n\nthe text.\n\nFeature Engineering (TF-IDF Encoding):\n\nTF-IDF (Term Frequency-Inverse Document Frequency) encoding is a\n\ncommon technique used to represent text data numerically. It reflects\n\nhow important a word is to a document in a collection. It’s calculated based on two metrics: term frequency (TF) and inverse document\n\nfrequency (IDF). TF measures the frequency of a term in a document,\n\nwhile IDF measures the rarity of the term across documents. TF-IDF\n\nencoding gives higher weights to terms that are frequent in a document\n\nbut rare across documents, which are typically more informative.\n\n[End of response]\n\nWe will now ask ChatGPT to provide us with the code:\n\n[Prompt]\n\nOk, let’s proceed.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for loading the dataset:\n\n[Prompt response]\n\nGreat! Let’s start by loading the dataset from Google Drive in Jupyter\n\nNotebook. First, we need to mount Google Drive and then read the CSV file\n\ncontaining the Amazon review data.\n\nStep 1: Loading the Dataset\n\nWe’ll use the pandas library to load the dataset from the CSV file. Pandas\n\nprovides powerful data manipulation tools for Python, making it suitable for\n\nhandling structured data like CSV files.\n\nCode Explanation:\n\nimport pandas as pd: Import the pandas library and alias it as pd for convenience.\n\nfrom google.colab import drive: Import the drive module from the\n\ngoogle.colab library to mount Google Drive.\n\ndrive.mount('/content/drive'): Mount Google Drive to access files\n\nfrom it. This will prompt you to authorize access to your Google\n\nDrive.\n\ndata_path = '/content/drive/My Drive/Chapter05/amazon_product_review_data.csv': Define the path\n\nto the CSV file containing the dataset.\n\ndf = pd.read_csv(data_path): Use the read_csv() function from pandas to read the CSV file and store it in a DataFrame called df.\n\nLet’s implement this in code:\n\n# Importing necessary libraries import pandas as pd from google.colab import drive # Mount Google Drive drive.mount('/content/drive') # Path to the dataset\n\ndata_path = '/content/drive/My Drive/Chapter05/amazon_product_r # Load the dataset into a DataFrame df = pd.read_csv(data_path)\n\nRun this code in your Jupyter Notebook, and it will load the dataset into the\n\nDataFrame df. Let me know if you have any questions before we proceed\n\nto the next step\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in the Google Colab for every step. The\n\ncode loads the dataset from Google Drive. It will ask for the permissions.\n\nThe code runs perfectly fine without applying any modifications or any\n\nerrors.\n\nPlease refer to the notebook Building a Classification Model for Sentiment\n\nAnalysis with ChatGPT.ipynb for the executable code snippets or copy the\n\ncode from the prompt response.\n\nFigure 12.1: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for data preprocessing:\n\n[Prompt response]\n\nGreat! In the next step, we’ll perform the following preprocessing tasks:\n\n1. Tokenization\n\n2. Lowercasing\n\n3. Removing stopwords\n\n4. Removing punctuation\n\nWe’ll use the NLTK library for tokenization and stopwords removal, and\n\nwe’ll leverage string methods for lowercasing and punctuation removal.\n\nStep 2: Data Preprocessing\n\nTokenization:\n\nWe’ll tokenize the review text into individual words or tokens. NLTK provides a word_tokenize function for this purpose.\n\nLowercasing:\n\nWe’ll convert all text to lowercase using the lower() method of Python strings.\n\nRemoving Stopwords:\n\nNLTK provides a list of stopwords for various languages. We’ll use\n\nthis list to filter out stopwords from the review text.\n\nRemoving Punctuation:\n\nWe’ll use string methods to remove punctuation marks from the review\n\ntext.\n\nLet’s implement these preprocessing steps in code:\n\n# Importing necessary libraries import nltk import string # Download NLTK resources (only need to do this once) nltk.download('punkt') nltk.download('stopwords') # Get the list of stopwords stopwords = nltk.corpus.stopwords.words('english') # Function to preprocess text def preprocess_text(text): # Tokenization tokens = nltk.word_tokenize(text) # Lowercasing tokens = [token.lower() for token in tokens] # Removing stopwords and punctuation tokens = [token for token in tokens if token not in stopwor # Join tokens back into text preprocessed_text = ' '.join(tokens) return preprocessed_text # Preprocess the review body df['Cleaned_Review'] = df['review_body'].apply(preprocess_text) # Display the preprocessed data df.head()\n\nHere’s an explanation of the code:\n\nWe import the necessary libraries, including NLTK for natural\n\nlanguage processing tasks and the string module for handling\n\npunctuation.\n\nWe download the NLTK resources for tokenization and stopwords removal using the nltk.download() function.\n\nWe define a function preprocess_text() to preprocess each review\n\ntext. This function performs tokenization, lowercasing, removal of\n\nstopwords, and removal of punctuation.\n\nInside the function, we tokenize the text using nltk.word_tokenize(), convert tokens to lowercase, remove stopwords and punctuation using\n\nlist comprehensions, and then join the tokens back into text.\n\nFinally, we apply the preprocess_text() function to the review_body\n\ncolumn of the DataFrame and store the preprocessed text in a new column called Cleaned_Review.\n\nRun this code in your Jupyter Notebook, and it will preprocess the review\n\ntext in the DataFrame. Let me know if you have any questions or if you’re\n\nready to proceed to the next step!\n\n[End of response]\n\nWe execute the code in the notebook and it runs perfectly fine, and the\n\ndataset summary is printed along with a new column called Cleaned_Review.\n\nFigure 12.2: Code execution – dataset summary\n\nFeature 2: Model selection and baseline training Now that data preprocessing is complete, we will move to our next feature\n\nof model selection.\n\nWe will now instruct ChatGPT to proceed to the next step and provide it\n\nwith the output of the previous step:\n\n[Prompt]\n\nmarket_place customer_id review_id product_id product_parent pr\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for model selection:\n\n[Prompt response]\n\nIt looks like the DataFrame now includes a new column named Cleaned_Review containing the preprocessed text data. We’ve successfully completed the data preprocessing step.\n\nNow, let’s move on to the next step: Model Selection.\n\nStep 3: Model Selection\n\nIn this step, we’ll choose a baseline machine learning model for sentiment\n\nanalysis. Commonly used models for text classification tasks like sentiment\n\nanalysis include logistic regression, Naive Bayes, and support vector machines (SVM). We’ll start with one of these models as a baseline and\n\nevaluate its performance.\n\nLogistic Regression:\n\nLogistic regression is a popular linear classification algorithm used for\n\nbinary classification tasks like sentiment analysis. It works by\n\nmodelling the probability that a given input belongs to a particular\n\nclass using a logistic function.\n\nWe’ll use scikit-learn, a powerful machine learning library in Python,\n\nto implement logistic regression for sentiment analysis.\n\nLet’s proceed to implement logistic regression for sentiment analysis using\n\nthe preprocessed data. Here’s the code:\n\n# Importing necessary libraries from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, classification_repo # Splitting the data into training and testing sets X = df['Cleaned_Review'] y = df['sentiments'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You ca X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) # Initialize and train the logistic regression model lr_model = LogisticRegression() lr_model.fit(X_train_tfidf, y_train) # Predictions y_pred = lr_model.predict(X_test_tfidf) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy:\", accuracy) print(\"\\nClassification Report:\") print(classification_report(y_test, y_pred))\n\nLet me explain the code:\n\nWe import necessary libraries from scikit-learn for model selection and\n\nevaluation, including train_test_split for splitting the data,\n\nTfidfVectorizer for TF-IDF vectorization, LogisticRegression for logistic regression, and accuracy_score and classification_report\n\nfor model evaluation.\n\nWe split the data into training and testing sets using train_test_split.\n\nWe perform TF-IDF vectorization on the training and testing data using TfidfVectorizer.\n\nWe initialize and train a logistic regression model using the training\n\ndata.\n\nWe make predictions on the testing data.\n\nFinally, we evaluate the model’s performance using accuracy and\n\nclassification report.\n\nYou can run this code in your Jupyter Notebook to train the logistic\n\nregression model and evaluate its performance on the sentiment analysis\n\ntask. Let me know if you have any questions or if you’d like to explore\n\nother model options!\n\n[End of response]\n\nWe execute the code and it runs without any errors, trains the model, and\n\nevaluates the model performance.\n\nYou should see a table on precision, recall, and more.\n\nWe will move on to our next feature of model evaluation and interpretation.\n\nFeature 3: Model evaluation and interpretation Now that we have selected the logistic regression model and baseline\n\nperformance is evaluated, we can interpret the results.\n\nWe will now provide the output to ChatGPT and request additional\n\ntechniques such as feature importance analysis and model explainability:\n\n[Prompt]\n\nI would like to learn about the metrics such as accuracy, precision, recall,\n\nand F1-score. Additionally, I would like to apply techniques for interpreting\n\nmodel predictions, such as feature importance analysis or model\n\nexplainability methods, to gain insights into the factors influencing\n\nsentiment classification decisions. The following is the output from the\n\nmodel evaluation.\n\nAccuracy: 0.86 Classification Report: precision recall f1-score support negative 0.00 0.00 0.00 14 positive 0.86 1.00 0.92 86 accuracy 0.86 100 macro avg 0.43 0.50 0.46 100 weighted avg 0.74 0.86 0.80 100\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT explains\n\nthe metrics and suggests some options for model explainability and so on:\n\n[Prompt response]\n\nCertainly! Let’s break down the metrics mentioned in the model evaluation\n\noutput and discuss techniques for interpreting model predictions.\n\nMetrics Explanation:\n\nAccuracy:\n\nAccuracy measures the proportion of correctly classified instances\n\namong the total instances. It’s calculated as the ratio of the number of\n\ncorrect predictions to the total number of predictions. However, accuracy alone may not provide a complete picture of model\n\nperformance, especially when classes are imbalanced.\n\nPrecision:\n\nPrecision measures the proportion of true positive predictions among\n\nall positive predictions. It indicates the model’s ability to avoid false\n\npositives. It’s calculated as the ratio of true positives to the sum of\n\ntrue positives and false positives.\n\nRecall:\n\nRecall, also known as sensitivity or true positive rate, measures the\n\nproportion of true positive predictions among all actual positive\n\ninstances. It indicates the model’s ability to capture all positive\n\ninstances. It’s calculated as the ratio of true positives to the sum of\n\ntrue positives and false negatives.\n\nF1-Score:\n\nThe F1-score is the harmonic mean of precision and recall. It\n\nprovides a balanced measure that considers both precision and recall. It’s calculated as 2 * (precision * recall) / (precision +\n\nrecall).\n\nInterpretation:\n\nAccuracy:\n\nThe accuracy of 0.86 means that the model correctly classified 86% of the instances in the test set. However, accuracy alone doesn’t\n\nprovide insights into class-specific performance.\n\nPrecision:\n\nThe precision for the positive class is 0.86, indicating that when the\n\nmodel predicts a review as positive, it is correct 86% of the time. The\n\nprecision for the negative class is 0.00, indicating that the model did\n\nnot correctly predict any negative reviews.\n\nRecall:\n\nThe recall for the positive class is 1.00, indicating that the model\n\ncorrectly identified all positive reviews. However, the recall for the\n\nnegative class is 0.00, indicating that the model failed to identify any\n\nnegative reviews.\n\nF1-Score:\n\nThe F1-score for the positive class is 0.92, which is the harmonic\n\nmean of precision and recall for the positive class. It provides a\n\nbalanced measure of the model’s performance for positive reviews.\n\nTechniques for Interpreting Model Predictions:\n\nFeature Importance Analysis:\n\nFor logistic regression models, we can interpret feature importance\n\nby examining the coefficients associated with each feature. Higher\n\nmagnitude coefficients indicate stronger influence on the predicted\n\nclass. We can identify important words or features that contribute to\n\npositive or negative sentiment.\n\nModel Explainability Methods:\n\nTechniques such as SHAP (SHapley Additive exPlanations) values,\n\nLIME (Local Interpretable Model-agnostic Explanations), or ELI5\n\n(Explain Like I’m 5) can provide insights into how individual\n\nfeatures contribute to model predictions. These methods help in understanding the decision-making process of complex models like\n\nlogistic regression.\n\nWould you like to delve deeper into any of these techniques or have any\n\nspecific questions? Let me know how I can assist you further!\n\n[End of response]\n\nWe will ask ChatGPT to apply the SHAP (SHapley Additive\n\nexPlanations) technique:\n\n[Prompt]\n\nLet’s try SHAP first.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT\n\nunderstood the request and provides us with the code for SHAP:\n\n[Prompt response]\n\nSure! SHAP (SHapley Additive exPlanations) is a method for explaining\n\nindividual predictions from machine learning models. It provides a way to\n\nunderstand the contribution of each feature to the model’s output for a\n\nspecific instance.\n\nHere’s how we can use SHAP to interpret the logistic regression model’s\n\npredictions for sentiment analysis:\n\nUsing SHAP for Model Interpretation:\n\n1. Install SHAP:\n\nFirst, let’s make sure you have SHAP installed. If not, you can install\n\nit using pip:\n\n!pip install shap\n\n2. Prepare the Data:\n\nWe’ll need to prepare the data for SHAP analysis. This involves\n\ntransforming the text data into a format suitable for the logistic\n\nregression model.\n\n3. Explain Model Predictions:\n\nWe’ll use SHAP to explain the logistic regression model’s predictions for individual instances in the test set. This will provide insights into\n\nhow each feature contributes to the model’s decision.\n\nLet’s implement these steps in code:\n\n# Importing necessary libraries import shap # Initialize the SHAP explainer with the logistic regression mo explainer = shap.Explainer(lr_model, X_train_tfidf) # Explain the model's predictions for a specific instance (e.g. instance_index = 0 # You can choose any instance index from th shap_values = explainer.shap_values(X_test_tfidf[instance_index # Visualize the SHAP values shap.summary_plot(shap_values, features=X_test_tfidf[instance_i\n\nExplanation of the code:\n\nWe import the SHAP library.\n\nWe initialize the SHAP explainer with the logistic regression model (lr_model) and the TF-IDF transformed training data\n\n(X_train_tfidf).\n\nWe use the explainer to compute SHAP values for a specific instance from the test set (X_test_tfidf[instance_index]).\n\nFinally, we visualize the SHAP values using shap.summary_plot().\n\nThis visualization will provide insights into which features (words)\n\ncontributed most to the model’s prediction for the selected instance.\n\nYou can run this code in your Jupyter Notebook to visualize the SHAP\n\nvalues for individual predictions. Let me know if you have any questions or\n\nneed further assistance!\n\n[End of response]\n\nWe execute the code and it gives an error: AttributeError:\n\n'TfidfVectorizer' object has no attribute 'get_feature_names'.\n\nFigure 12.3: Attribute error for SHAP",
      "page_number": 327
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 350-369)",
      "start_page": 350,
      "end_page": 369,
      "detection_method": "synthetic",
      "content": "We will use the feedback prompting strategy (LIFE 3.3) and provide the\n\nerror to ChatGPT:\n\n[Prompt]\n\nAttributeError: 'TfidfVectorizer' object has no attribute 'get_\n\n[End of prompt].\n\nWe should receive a response with the suggested correct code.\n\n[Prompt response]\n\nIt seems there was an AttributeError because the TfidfVectorizer object\n\ndoes not have a get_feature_names() method. My apologies for the\n\noversight. We can access the feature names directly from the vectorizer’s\n\nvocabulary attribute.\n\nLet’s correct the code to retrieve feature names from the TfidfVectorizer vocabulary:\n\n# Get feature names from the TfidfVectorizer vocabulary feature_names = tfidf_vectorizer.get_feature_names_out() # Visualize the SHAP values shap.summary_plot(shap_values, features=X_test_tfidf[instance_i\n\nBy using get_feature_names_out(), we can access the feature names from\n\nthe TfidfVectorizer's vocabulary.\n\nYou can update the code accordingly and try running it again in your\n\nJupyter Notebook. Let me know if you encounter any further issues!\n\n[End of response]\n\nWe execute the corrected code and it works fine and produces the output\n\nplot:\n\nFigure 12.4: Code execution – SHAP impact on model output\n\nWords like “great” and “love” have positive SHAP values, meaning\n\ntheir presence in the data likely pushes predictions toward a positive\n\noutcome, possibly indicating a positive sentiment in a sentiment\n\nanalysis model.\n\nThe words are ranked by the absolute size of their SHAP values, with\n\n“great” having the most significant positive impact.\n\nMost words on this list have a very minor impact close to zero,\n\nsuggesting that their presence does not significantly affect the\n\nprediction relative to the baseline.\n\nSome words such as “360” and “xbox” may relate to specific topics\n\n(like gaming), and their SHAP values are slightly negative, suggesting\n\nthey might be reducing the likelihood of the positive outcome this\n\nmodel predicts.\n\nWe will now ask ChatGPT to provide us with the code for ELI5 (Explain\n\nLike I’m 5):\n\n[Prompt]\n\nNext, I would like to try ELI5 and ensure that the code is correct and is not\n\nimpacted because of the SHAP technique we applied.\n\n[End of prompt].\n\nWe should receive the code for ELI5 from ChatGPT:\n\n[Prompt response]\n\nCertainly! ELI5 (Explain Like I’m 5) is another popular library for model\n\ninterpretation and explaining machine learning models. We can use ELI5 to\n\nunderstand how our logistic regression model makes predictions and inspect\n\nfeature importance.\n\nLet’s integrate ELI5 into our code to explain the logistic regression model’s\n\npredictions:\n\nUsing ELI5 for Model Interpretation:\n\n1. Install ELI5:\n\nIf you haven’t installed ELI5 yet, you can install it using pip:\n\n!pip install eli5\n\n2. Explain Model Predictions:\n\nWe’ll use ELI5 to explain the logistic regression model’s predictions\n\nfor individual instances in the test set. ELI5 provides different\n\nmethods to explain model predictions, including feature importances and textual explanations.\n\nLet’s update our code to include ELI5 for model interpretation:\n\n# Importing necessary libraries import eli5 # Explain the logistic regression model using ELI5 eli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20) # Sh\n\nExplanation of the code:\n\nWe import the ELI5 library.\n\nWe use the show_weights function from ELI5 to explain the logistic\n\nregression model’s predictions. This function displays the weights\n\n(coefficients) associated with each feature, indicating their importance\n\nin the model’s decision-making process.\n\nWe specify the vectorizer (vec) parameter to ensure ELI5 interprets the features correctly based on the TF-IDF vectorization.\n\nYou can run this updated code in your Jupyter Notebook to see the top\n\nfeatures contributing to the model’s predictions. ELI5 provides a clear\n\nvisualization of feature importances, allowing you to gain insights into the\n\nmodel’s behavior.\n\nLet me know if you have any questions or encounter any issues!\n\n[End of response]\n\nWe execute the code provided by ChatGPT, which installs the eli5 library\n\nand generates the output as follows:\n\nThe output is not understandable at all, which is ironic as it is supposed to\n\nbe understood by a 5-year-old.\n\nLet’s ask ChatGPT to at least provide code in which the feature names are\n\nin plain English:\n\n[Prompt]\n\nCan I also get the features? X numbers make no sense and are definitely not\n\nunderstandable to a 5-year-old.\n\n[End of prompt]\n\nWe should receive a response similar to the following:\n\n[Prompt response]\n\nCertainly! We can map the feature indices (x numbers) to their\n\ncorresponding words or terms in the TF-IDF vectorizer’s vocabulary to\n\nmake them more interpretable. Let’s update the code to include feature\n\nnames along with their weights:\n\n# Explain the logistic regression model using ELI5 with feature\n\neli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20, featu\n\nBy including the feature_names parameter with the TF-IDF vectorizer’s vocabulary, ELI5 will display the actual words or terms instead of feature\n\nindices.\n\nRun this updated code in your Jupyter Notebook, and you’ll see the top\n\nfeatures contributing to the positive class predictions along with their\n\nweights. Let me know if you have any questions or need further assistance!\n\n[End of response]\n\nWe execute the provided code, and it now provides the feature names in\n\nEnglish:\n\nThis concludes Feature 3, but as our data is clearly imbalanced, let’s\n\nproceed to the next feature.\n\nFeature 4: Handling imbalanced data Now that we have a baseline model and learned how to use the model\n\nexplainability techniques, we will now see how to handle the data\n\nimbalance:\n\n[Prompt]\n\nI would like to apply some techniques to mitigate the data imbalance as it is\n\nclearly seen from the model performance.\n\nAccuracy: 0.86 Classification Report: precision recall f1-score support\n\nnegative 0.00 0.00 0.00 14 positive 0.86 1.00 0.92 86.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with suggestions and code to\n\nmitigate the data imbalance:\n\n[Prompt response]\n\nAddressing data imbalance is crucial for improving the model’s\n\nperformance, especially in cases where one class is significantly\n\nunderrepresented compared to the other. Here are some techniques we can\n\napply to mitigate data imbalance:\n\nResampling Techniques:\n\nOver-sampling: Increase the number of instances in the minority class\n\nby generating synthetic samples (e.g., using SMOTE - Synthetic\n\nMinority Over-sampling Technique).\n\nUnder-sampling: Decrease the number of instances in the majority\n\nclass by randomly removing samples.\n\nAlgorithmic Techniques:\n\nClass weights: Assign higher weights to instances of the minority\n\nclass during model training to penalize misclassifications.\n\nEnsemble methods: Utilize ensemble methods that handle class imbalance well, such as Balanced Random Forest or Easy Ensemble.\n\nData Augmentation:\n\nGenerate new samples for the minority class by applying\n\ntransformations or perturbations to existing data points.\n\nCost-sensitive Learning:\n\nExplicitly incorporate the cost of misclassification into the model\n\ntraining process to prioritize correct classification of the minority\n\nclass.\n\nLet’s implement one of these techniques, such as class weights adjustment,\n\nto address the data imbalance in our logistic regression model:\n\nimport numpy as np # Calculate class weights from sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight('balanced', classes=np.uni # Initialize and train the logistic regression model with class lr_model_balanced = LogisticRegression(class_weight=dict(zip(np lr_model_balanced.fit(X_train_tfidf, y_train) # Predictions y_pred_balanced = lr_model_balanced.predict(X_test_tfidf) # Evaluate the model with balanced class weights accuracy_balanced = accuracy_score(y_test, y_pred_balanced) print(\"Accuracy with balanced class weights:\", accuracy_balance print(\"\\nClassification Report with balanced class weights:\") print(classification_report(y_test, y_pred_balanced))\n\nExplanation:\n\nWe calculate class weights using compute_class_weight from scikit- learn’s utils. Setting balanced mode automatically adjusts weights\n\ninversely proportional to class frequencies in the input data.\n\nWe initialize and train a new logistic regression model (lr_model_balanced) with the calculated class weights.\n\nWe make predictions using the balanced model and evaluate its\n\nperformance using accuracy and classification report.\n\nRunning this code will train the logistic regression model with balanced\n\nclass weights and evaluate its performance on the test set. It should help\n\nimprove the handling of the imbalanced data and potentially enhance the\n\nmodel’s performance. Let me know if you have any questions or need\n\nfurther assistance!\n\n[End of response]\n\nWe will execute the code that uses calculated class weights and see what\n\nimpact it makes.\n\nYou should see a classification report with balanced weights.\n\nWe will give the output to ChatGPT so that it provides us with\n\nexplanations:\n\n[Prompt]\n\nAccuracy with balanced class weights: 0.81\n\nClassification Report with balanced class weights:\n\nprecision recall f1-score support negative 0.27 0.21 0.24 14 positive 0.88 0.91 0.89 86 accuracy 0.81 100 macro avg 0.57 0.56 0.57 100 weighted avg 0.79 0.81 0.80 100\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nthe interpretation of the output:\n\n[Prompt response]\n\nIt appears that applying balanced class weights has improved the model’s\n\nperformance, although the accuracy has decreased slightly. Let’s interpret\n\nthe results:\n\nAccuracy: The accuracy of 0.81 indicates that the model correctly\n\nclassified 81% of the instances in the test set.\n\nPrecision and Recall: For the negative class, the precision has\n\nimproved significantly to 0.27, indicating that when the model predicts\n\na review as negative, it is correct 27% of the time. However, the recall\n\nis relatively low at 0.21, suggesting that the model fails to capture\n\nmany negative instances.\n\nFor the positive class, both precision and recall are high, with\n\nprecision at 0.88 and recall at 0.91. This indicates that the model\n\nperforms well in identifying positive instances.\n\nF1-score: The F1-score for the negative class has improved slightly to\n\n0.24, while for the positive class, it remains high at 0.89.\n\nSupport: The support represents the number of instances in each class. There are 14 instances of the negative class and 86 instances of the\n\npositive class in the test set.\n\nOverall, while the accuracy has decreased, the model’s ability to correctly\n\nclassify the minority class (negative reviews) has improved significantly\n\nwith balanced class weights. This indicates a better balance between\n\nprecision and recall for both classes, resulting in a more robust model.\n\nIf you have any further questions or need additional assistance, feel free to\n\nask!\n\n[End of response]\n\nWe have reduced the imbalance and will now move on to the next feature of\n\nhyperparameter tuning.\n\nFeature 5: Hyperparameter tuning Now that we have learned how to deal with imbalanced data, we can\n\noptimize the performance by tuning hyperparameters such as regularization\n\nstrength, learning rate, and kernel parameters, and also apply techniques\n\nlike grid search or random search:\n\n[Prompt]\n\nI would like to optimize the performance by tuning hyperparameters such as\n\nregularization strength, learning rate, and kernel parameters and also apply\n\ntechniques like grid search or random search.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nCertainly! Hyperparameter tuning is crucial for optimizing model\n\nperformance. We can use techniques like grid search or random search to\n\nsearch through a range of hyperparameter values and find the combination\n\nthat yields the best performance.\n\nLet’s start by implementing hyperparameter tuning using grid search with\n\nlogistic regression as an example:\n\n# Importing necessary libraries from sklearn.model_selection import GridSearchCV # Define hyperparameters grid\n\nparam_grid = { 'C': [0.01, 0.1, 1.0, 10.0], # Regularization strength (sm 'solver': ['liblinear', 'lbfgs'], # Optimization algorithm } # Initialize logistic regression model lr_model_tuned = LogisticRegression() # Initialize GridSearchCV with logistic regression model and hy grid_search = GridSearchCV(lr_model_tuned, param_grid, cv=5, sc # Perform grid search grid_search.fit(X_train_tfidf, y_train) # Get the best hyperparameters best_params = grid_search.best_params_ print(\"Best Hyperparameters:\", best_params) # Use the best model from grid search best_lr_model = grid_search.best_estimator_ # Predictions y_pred_tuned = best_lr_model.predict(X_test_tfidf) # Evaluate the tuned model accuracy_tuned = accuracy_score(y_test, y_pred_tuned) print(\"Accuracy with tuned model:\", accuracy_tuned) print(\"\\nClassification Report with tuned model:\") print(classification_report(y_test, y_pred_tuned))\n\nExplanation:\n\nWe define a grid of hyperparameters (param_grid) to search through.\n\nIn this example, we vary the regularization strength (C) and the solver\n\nalgorithm.\n\nWe initialize a logistic regression model (lr_model_tuned) and perform grid search using GridSearchCV.\n\nGrid search will evaluate the performance of the model with different hyperparameter combinations using cross-validation (cv=5).\n\nAfter grid search, we retrieve the best hyperparameters (best_params)\n\nand the best model (best_lr_model).\n\nFinally, we make predictions with the best model and evaluate its\n\nperformance.\n\nYou can adjust the hyperparameter grid (param_grid) to include other\n\nhyperparameters like learning rate and kernel parameters for different\n\nmodels. Additionally, you can also explore random search or other\n\noptimization techniques for hyperparameter tuning.\n\nFeel free to run this code in your Jupyter Notebook to optimize the logistic\n\nregression model’s performance through hyperparameter tuning. Let me\n\nknow if you have any questions or need further assistance!\n\n[End of response]\n\nWe execute the code and it works fine, and we find the best parameters\n\nusing GridSearchCV.\n\nYou should see a classification report with tuned model.\n\nThe hyperparameter tuning did not significantly improve the model’s\n\nperformance in terms of accuracy and class-specific metrics. Let’s interpret\n\nthe results:\n\nBest hyperparameters: The grid search identified the best hyperparameters as C=0.01 and solver='liblinear'. These\n\nhyperparameters indicate a low regularization strength (C=0.01) and the use of the liblinear solver algorithm.\n\nAccuracy: The accuracy of the tuned model remains at 0.86, which is\n\nthe same as the baseline model. This indicates that hyperparameter\n\ntuning did not lead to a significant improvement in overall accuracy.\n\nPrecision and recall: The precision, recall, and F1-score for both the\n\npositive and negative classes remain unchanged from the baseline\n\nmodel. This suggests that the model’s ability to correctly classify\n\ninstances from each class did not improve after hyperparameter tuning.\n\nSupport: The support values represent the number of instances in each\n\nclass in the test set, which remains the same as before.\n\nLet’s move on to our last feature and experiment with feature\n\nrepresentation.\n\nFeature 6: Experimenting with feature representation Let’s instruct ChatGPT to provide us with the code for different feature\n\nrepresentations:\n\n[Prompt]\n\nI want to compare the performance of the model trained with different\n\nfeature representations, such as bag-of-words, TF-IDF, or word\n\nembeddings, to determine the most effective approach for sentiment\n\nclassification.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nthe code for bag-of-words, TF-IDF, and word embeddings:\n\n[Prompt response]\n\nComparing the performance of models trained with different feature\n\nrepresentations is a valuable approach to determine the most effective\n\nmethod for sentiment classification. We can train models using bag-of-\n\nwords, TF-IDF, and word embeddings representations and evaluate their\n\nperformance using the same evaluation metrics. Let’s go through the\n\nprocess step by step:\n\n1. Bag-of-Words Representation:\n\nWe can represent the text data using a bag-of-words approach,\n\nwhere each document is represented by a vector indicating the\n\nfrequency of each word in the vocabulary.\n\n2. TF-IDF Representation:\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\nrepresentation considers not only the frequency of a term in a\n\ndocument but also its importance in the entire corpus.\n\n3. Word Embeddings Representation:\n\nWord embeddings represent words as dense vectors in a\n\ncontinuous vector space, capturing semantic relationships\n\nbetween words.\n\nHere’s how we can compare the performance of models trained with these\n\ndifferent feature representations:\n\nPreprocess the data: Tokenize, lowercase, remove stopwords, and\n\nperform any necessary text preprocessing steps for each feature\n\nrepresentation method.\n\nTrain models: Train logistic regression models using each feature\n\nrepresentation method (bag-of-words, TF-IDF, word embeddings).\n\nEvaluate performance: Evaluate the performance of each model\n\nusing the same evaluation metrics (accuracy, precision, recall, F1-\n\nscore).\n\nCompare results: Compare the performance of models trained with\n\ndifferent feature representations to determine the most effective\n\napproach for sentiment classification.\n\nLet’s start by implementing logistic regression models using bag-of-words,\n\nTF-IDF, and word embeddings representations:\n\nBag-of-Words Representation:\n\nfrom sklearn.feature_extraction.text import CountVectorizer # Initialize CountVectorizer bow_vectorizer = CountVectorizer(max_features=5000) # You can # Transform text data into bag-of-words representation X_train_bow = bow_vectorizer.fit_transform(X_train) X_test_bow = bow_vectorizer.transform(X_test) # Train logistic regression model with bag-of-words representat lr_model_bow = LogisticRegression() lr_model_bow.fit(X_train_bow, y_train) # Evaluate model performance y_pred_bow = lr_model_bow.predict(X_test_bow) accuracy_bow = accuracy_score(y_test, y_pred_bow) print(\"Accuracy with bag-of-words representation:\", accuracy_bo # Add other evaluation metrics if needed\n\nTF-IDF Representation:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Initialize TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You ca # Transform text data into TF-IDF representation X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) # Train logistic regression model with TF-IDF representation lr_model_tfidf = LogisticRegression() lr_model_tfidf.fit(X_train_tfidf, y_train) # Evaluate model performance y_pred_tfidf = lr_model_tfidf.predict(X_test_tfidf) accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf) print(\"Accuracy with TF-IDF representation:\", accuracy_tfidf) # Add other evaluation metrics if needed\n\nWord Embeddings Representation:\n\nFor word embeddings representation, we need to first convert the text data\n\ninto word embeddings using techniques like Word2Vec, GloVe, or fastText.\n\nWe’ll use pre-trained word embeddings for this purpose.\n\n# Assuming you have pre-trained word embeddings loaded (e.g., G # Convert text data into word embeddings representation # You may need to tokenize and pad sequences before applying wo # Train logistic regression model with word embeddings represen lr_model_word_embeddings = LogisticRegression() lr_model_word_embeddings.fit(X_train_word_embeddings, y_train) # Evaluate model performance y_pred_word_embeddings = lr_model_word_embeddings.predict(X_tes accuracy_word_embeddings = accuracy_score(y_test, y_pred_word_e print(\"Accuracy with word embeddings representation:\", accuracy # Add other evaluation metrics if needed\n\nAfter training and evaluating models with different feature representations,\n\nyou can compare their performance using accuracy or other evaluation\n\nmetrics. Based on the results, you can determine which feature\n\nrepresentation method is the most effective for sentiment classification.\n\nFeel free to run these codes in your Jupyter Notebook and let me know if\n\nyou need further assistance or have any questions!\n\n[End of response]\n\nWe will execute the code and compare the accuracy for bag-of-words, TF-\n\nIDF, and word embeddings. The code works perfectly fine for the first two\n\nbut gives errors for word embeddings.\n\nFigure 12.5: Bag of words\n\nFigure 12.6: TF- IDF\n\nFigure 12.7: Word embedding – error\n\nSo, we will instruct ChatGPT to provide us with the corrected code for\n\nword embeddings, and we will also provide the context that we have done\n\ntokenization earlier but no embeddings:\n\n[Prompt]\n\nNameError Traceback (most recent call last) <ipython-input-20-8\n\n[End of prompt]\n\nWe should receive the response with the modified code similar to the\n\nfollowing:\n\n[Prompt response]\n\nThank you for providing clarification. Since we have preprocessed the text\n\ndata and tokenized it, we can represent it using word embeddings without\n\npre-trained word embeddings. We’ll use techniques like Word2Vec or\n\nDoc2Vec to generate word embeddings from the tokenized text.\n\nLet’s proceed with representing the text data using Word2Vec and then train\n\na logistic regression model:\n\nWord Embeddings Representation with Word2Vec:\n\nfrom gensim.models import Word2Vec from sklearn.feature_extraction.text import CountVectorizer from sklearn.base import BaseEstimator, TransformerMixin # Define a custom transformer to tokenize the text data class TokenizerTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return [text.split() for text in X] # Initialize and fit Word2Vec model on tokenized text data w2v_model = Word2Vec(sentences=TokenizerTransformer().fit_trans # Define a function to average word vectors for each document def average_word_vectors(words, model, vocabulary, num_features feature_vector = np.zeros((num_features,), dtype=\"float64\") nwords = 0. for word in words: if word in vocabulary: nwords = nwords + 1. feature_vector = np.add(feature_vector, model.wv[wo if nwords: feature_vector = np.divide(feature_vector, nwords) return feature_vector # Define a custom transformer to convert tokenized text data in class WordEmbeddingsTransformer(BaseEstimator, TransformerMixin def __init__(self, model): self.model = model self.num_features = model.vector_size self.vocabulary = set(model.wv.index_to_key) def fit(self, X, y=None): return self def transform(self, X): return np.array([average_word_vectors(words, self.model",
      "page_number": 350
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 370-392)",
      "start_page": 370,
      "end_page": 392,
      "detection_method": "synthetic",
      "content": "# Convert tokenized text data into word embeddings representati X_train_word_embeddings = WordEmbeddingsTransformer(w2v_model). X_test_word_embeddings = WordEmbeddingsTransformer(w2v_model).t # Train logistic regression model with word embeddings represen lr_model_word_embeddings = LogisticRegression() lr_model_word_embeddings.fit(X_train_word_embeddings, y_train) # Evaluate model performance y_pred_word_embeddings = lr_model_word_embeddings.predict(X_tes accuracy_word_embeddings = accuracy_score(y_test, y_pred_word_e print(\"Accuracy with word embeddings representation (Word2Vec): # Add other evaluation metrics if needed\n\nExplanation:\n\nWe define a custom transformer TokenizerTransformer to tokenize the\n\ntext data.\n\nWe use the Word2Vec model to generate word embeddings from the\n\ntokenized text data.\n\nWe define a function average_word_vectors to average word vectors for each document.\n\nWe define another custom transformer WordEmbeddingsTransformer to\n\nconvert tokenized text data into word embeddings representation.\n\nWe train a logistic regression model using the word embeddings\n\nrepresentation obtained from Word2Vec.\n\nFinally, we evaluate the performance of the model and print the\n\naccuracy.\n\nYou can run this code in your Jupyter Notebook to represent the text data\n\nusing Word2Vec word embeddings and train a logistic regression model.\n\nLet me know if you need further assistance!\n\n[End of response]\n\nThis time, the code works:\n\nFigure 12.8: Accuracy with word embeddings\n\nThis concludes our last feature with the free version of ChatGPT.\n\nNow, we will try some of these features using ChatGPT-4 or ChatGPT Plus.\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews using ChatGPT-4 or ChatGPT Plus ChatGPT Plus or GPT-4 includes the ability to upload a dataset, analyze the\n\ndataset, and produce results if using simple libraries such as Python and\n\nmatplotlib. At the time of writing this chapter, it had an AI assistant named\n\nData Analysis, provided by OpenAI when using the mobile app, or when\n\nselecting GPT-4 if using the browser version.\n\nLet’s explore how it differs from the free version, feature by feature.\n\nFeature 1: Data preprocessing and feature engineering\n\nLet’s craft our initial prompt for the baseline model:\n\n[Prompt]\n\nI want to create a simple classification model for sentiment analysis of the\n\nAmazon Review Dataset (TAG 1.1). <upload dataset in CSV format here >\n\n(PIC 2.3)\n\nIt should consist of the following steps (TAG 1.2) and I am a beginner user\n\n(PIC 2.1), so provide one step at a time and wait for users feedback (PIC 2.2)\n\nData Preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel Selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or support vector machines (SVM).\n\nModel Evaluation and Interpretation.\n\nHandling Imbalance Data.\n\nHyperparameter Tuning.\n\nExperimenting with feature representation.\n\n[End of prompt]\n\nYou should receive a response similar to the following where ChatGPT-4\n\nhas loaded the dataset:\n\n[Prompt response]\n\nGreat! We’ll start by preprocessing the data from the Amazon Review\n\nDataset. Here’s how we can approach the preprocessing step:\n\nLoading the Data: We’ll first load the dataset to understand its\n\nstructure.\n\nTokenization: This involves breaking down text into individual words\n\nor terms.\n\nLowercasing: Convert all text to lowercase to ensure uniformity.\n\nRemoving Stopwords: Stopwords are common words (like “and”,\n\n“the”, etc.) that are often filtered out before processing natural\n\nlanguage data because they carry less meaningful information.\n\nRemoving Punctuation: Punctuation is usually removed as it doesn’t\n\nadd much extra information while processing text.\n\nFeature Engineering: We’ll apply TF-IDF encoding, which helps in\n\nunderstanding the importance of a word in a document relative to the\n\ncorpus.\n\nLet’s start by loading your data and taking a look at its structure. I’ll write a\n\nPython script to load the data and display the first few entries to better\n\nunderstand its columns and content. Let’s proceed with this step, and once\n\nwe see the data, we can move on to the next preprocessing tasks. Here’s the\n\nscript:\n\nWe’ve successfully loaded the dataset, and here’s a quick look at its\n\nstructure:\n\nmarket_place, customer_id, review_id, product_id, product_parent:\n\nThese columns are identifiers and metadata about the review and\n\nproduct.\n\nproduct_title, product_category: Information about the product.\n\nstar_rating, helpful_votes, total_votes: Ratings and votes data.\n\nverified_purchase: Whether the purchase was verified.\n\nreview_headline, review_body: The actual text of the review.\n\nreview_date: The date of the review.\n\nsentiments: This seems to be a label for sentiment analysis (positive,\n\nnegative).\n\nGiven your focus is on sentiment analysis, the primary columns of interest will be review_body and sentiments.\n\nNow, let’s proceed with the preprocessing steps:\n\nTokenization\n\nLowercasing\n\nRemoving Stopwords\n\nRemoving Punctuation\n\nWe will handle these steps using the nltk library, which is commonly used\n\nfor text processing in Python. Let me know if you’re ready, and I’ll start coding these preprocessing steps!\n\n[End of response]\n\nWe can see that the dataset has been loaded successfully and ChatGPT-4 is waiting for the next step, the user’s feedback (LIFE 3.3). So, we will\n\ninstruct ChatGPT-4 to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed with data preprocessing.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and executes the code for data preprocessing:\n\n[Prompt response]\n\nGreat! Let’s start with the data preprocessing tasks one by one. Here’s what\n\nwe’ll do:\n\nTokenization: This will split the review_body text into individual\n\nwords or tokens.\n\nLowercasing: Convert all tokens to lowercase to standardize the text.\n\nRemoving Stopwords: Remove common words that might not be\n\nuseful in sentiment analysis.\n\nRemoving Punctuation: Strip punctuation as it’s typically not needed\n\nin text-based models.\n\nI’ll use the Natural Language Toolkit (nltk) for these steps. Let’s begin by\n\ninstalling the necessary libraries and setting up the initial preprocessing\n\nfunctions. Here’s the code to start:\n\nIt seems there was an issue with downloading the necessary resources from\n\nthe NLTK library due to network restrictions in this environment. I’ll\n\nmodify the approach to manually define a small set of common stopwords\n\nfor demonstration purposes. Let’s proceed with a simplified version of the\n\ntext preprocessing:\n\nTokenization\n\nLowercasing\n\nRemoving a simplified set of stopwords\n\nRemoving punctuation\n\nHere’s the adapted script that manually handles these steps without the need\n\nto download additional resources:\n\nThe text preprocessing has been successfully applied using a simplified\n\napproach. Here’s a comparison of the original text and the processed text:\n\nOriginal: “As a family allergic to wheat, dairy, eggs, nuts...”\n\nProcessed: [‘a’, ‘family’, ‘allergic’, ‘wheat’, ‘dairy’, ‘eggs’, ‘nuts’]\n\nAs you can see, we’ve removed stopwords, punctuation, and have tokenized\n\nand lowercased the text. This processed data is now more suitable for\n\nfeeding into a machine learning model for sentiment analysis.\n\nNext steps would involve converting these tokens into numerical features\n\nthat can be used by machine learning algorithms. We can use methods like\n\nTF-IDF encoding or word embeddings for this. Let me know if you’re\n\nready to proceed to the feature engineering step, or if there’s anything else\n\nyou’d like to adjust in the preprocessing!\n\n[End of response]\n\nWe can see from the response that ChatGPT-4 first tried with the NLTK\n\nlibrary but due to limitations, it simplified the approach with a small set of\n\ncommon words. It also provides us with an example of how it is processed.\n\nWe will now instruct ChatGPT-4 to proceed:\n\n[Prompt]\n\nLet’s proceed.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT-4 has\n\nimplemented TF-IDF:\n\n[Prompt response]\n\nGreat! Let’s move on to the feature engineering step. In sentiment analysis,\n\none common technique for converting text data into numerical form that\n\ncan be understood by machine learning algorithms is the Term Frequency-\n\nInverse Document Frequency (TF-IDF) approach. Here’s a quick\n\nbreakdown:\n\nTerm Frequency (TF): Measures how frequently a term occurs in a\n\ndocument. Since documents may vary in length, it is often divided by\n\nthe document length (the total number of terms in the document).\n\nInverse Document Frequency (IDF): Measures how important a term\n\nis within the corpus (collection of all documents). It helps to weigh\n\ndown terms that appear very frequently across documents and are less\n\ninformative.\n\nStep to Implement TF-IDF:\n\nConvert the list of tokens to string: Machine learning algorithms\n\ntypically require input features as strings or numbers. We’ll convert\n\nour list of tokens back to string format.\n\nTF-IDF Vectorization: Use TfidfVectorizer from scikit-learn to\n\nconvert the text documents into a matrix of TF-IDF features.\n\nI will now code these steps, starting by converting our token lists back to\n\nstrings and then applying TF-IDF vectorization. Let’s do this:\n\nWe’ve successfully transformed the processed text into a matrix of TF-IDF\n\nfeatures. Here are some details:\n\nMatrix Shape: The TF-IDF matrix has 500 documents and 3778\n\nfeatures. This means we have 3778 unique terms across the documents\n\nafter applying the preprocessing and vectorization steps.\n\nExample Features: Some of the first few terms in our feature set\n\ninclude ‘ability’, ‘able’, ‘about’, ‘above’, etc.\n\nThis TF-IDF matrix can now be used as input for various machine learning\n\nmodels to perform sentiment analysis.\n\nNext, we can move on to selecting and training a baseline model using this\n\nfeature representation. Common choices for initial models in sentiment\n\nanalysis include Logistic Regression, Naive Bayes, and Support Vector\n\nMachines (SVM). Let me know which model you would like to start with,\n\nor if you have any preferences for proceeding!\n\n[End of response]\n\nWe can see that ChatGPT-4 has implemented TF-IDF and has 500\n\ndocuments and 3,778 features. We will now move on to baseline model\n\ncreation.\n\nFeature 2: Model selection and baseline training We will now instruct ChatGPT-4 to proceed to model selection:\n\n[Prompt]\n\nLet’s proceed with logistic regression.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and executes the code for logistic regression.\n\n[Prompt response]\n\nPerfect! Logistic Regression is a good starting point for classification tasks\n\nlike sentiment analysis because it’s simple and often provides a good\n\nbaseline for performance comparison.\n\nSteps to Implement Logistic Regression:\n\nPrepare the data: We’ll split the data into training and testing sets to\n\nevaluate the model’s performance.\n\nTrain the model: We’ll use Logistic Regression from scikit-learn to\n\ntrain the model using the training set.\n\nEvaluate the model: We’ll test the model on the testing set to see how\n\nwell it predicts the sentiments.\n\nLet’s start by preparing the data and then training the Logistic Regression\n\nmodel. Here’s the code to do that:\n\nThe Logistic Regression model has been trained and evaluated. Here’s a\n\nsummary of its performance:\n\nAccuracy: 86%\n\nPrecision, Recall, and F1-Score: These metrics provide a more\n\ndetailed picture. For the ‘positive’ class, we see a precision and recall\n\nof 86% and 100% respectively, leading to an F1-score of 92%.\n\nHowever, for the ‘negative’ class, the precision, recall, and F1-score\n\nare all 0. This indicates that the model failed to correctly predict any of the ‘negative’ class instances.\n\nThis discrepancy is often due to class imbalance, where one class (‘positive’\n\nin this case) is much more frequent than the other (‘negative’), causing the\n\nmodel to be biased towards the majority class.\n\nNext Steps:\n\nHandling Imbalance Data: We can explore techniques like\n\noversampling the minority class, undersampling the majority class, or\n\nadjusting the decision threshold.\n\nHyperparameter Tuning: We can tune the parameters of the Logistic\n\nRegression model to potentially improve performance.\n\nExperimenting with Feature Representation: Trying different\n\nsettings in TF-IDF or using other types of text representations like\n\nword embeddings.\n\nWould you like to explore any of these next steps further, such as\n\naddressing the imbalance in the dataset, or would you like to adjust\n\nsomething else?\n\n[End of response]\n\nFeature 3: Model evaluation and interpretation Now that we have selected the logistic regression model and baseline\n\nperformance is evaluated, let’s interpret the results.\n\nWe will ask ChatGPT-4 to explain the metrics and provide results\n\ninterpretation using techniques such as feature importance analysis and\n\nmodel explainability:\n\n[Prompt]\n\nPlease explain the metrics and provide results interpretation using\n\ntechniques such as feature importance analysis and model explainability.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT\n\nunderstood the request and provides us with the explanation and feature\n\nimportance analysis:\n\n[Prompt response]\n\nLet’s delve deeper into the evaluation metrics and explore some techniques to interpret the Logistic Regression model:\n\nEvaluation Metrics Explanation:\n\nPrecision: This measures the accuracy of positive predictions.\n\nFormally, it is the ratio of true positive predictions to the total\n\npredicted positives. High precision indicates a low false positive rate.\n\nRecall (Sensitivity): This is the ability of the model to find all the\n\nrelevant cases (all true positives). It is the ratio of true positives to the\n\nactual total positives.\n\nF1-Score: The F1-score is the harmonic mean of precision and recall.\n\nIt is a balance between precision and recall, a useful measure when the\n\nclasses are imbalanced.\n\nAccuracy: This is the ratio of correctly predicted observations to the\n\ntotal observations. It’s generally not a good measure for imbalanced\n\nclasses as it can be misleading.\n\nResults Interpretation:\n\nAccuracy of 86% sounds good, but it’s misleading because the model\n\nperforms poorly on the minority class (‘negative’).\n\nThe model has a high recall (100%) for the ‘positive’ class, indicating\n\nit identifies most positive instances, but it fails completely to identify\n\n‘negative’ class instances (recall of 0%).\n\nFeature Importance Analysis:\n\nIn Logistic Regression, the coefficients of the model can provide\n\ninsight into the importance of features (words in this case). A higher\n\nabsolute value of a coefficient indicates that the feature is more\n\nimportant for the decision the model makes.\n\nLet’s extract the coefficients of the model to see which words (features) are\n\ninfluencing the predictions most:\n\nHere are the top 10 features (words) influencing the sentiment classification\n\nand their corresponding coefficients:\n\n“not”: Coefficient of -1.87, a strong negative influence on the\n\nsentiment prediction. It suggests that the presence of “not” in a review\n\nsignificantly drives the prediction towards a negative sentiment.\n\n“great”: Coefficient of 1.65, a strong positive influence, suggesting\n\nreviews with this word are likely classified as positive.\n\n“love”: Coefficient of 1.54, another strong positive indicator.\n\n“good”: Coefficient of 0.99, positively influences the sentiment.\n\n“but”: Coefficient of -0.95, negatively influences the sentiment.\n\n“very”: Coefficient of 0.88, generally used in positive contexts.\n\n“was”: Coefficient of -0.86, showing a negative influence, possibly\n\nused in contexts of disappointment.\n\n“cheap”: Coefficient of -0.81, commonly associated with negative\n\nsentiments.\n\n“there”: Coefficient of -0.81, suggesting a negative context when this\n\nword is used.\n\n“dont” (likely meant to be “don’t”): Coefficient of -0.79, another\n\nstrong negative indicator.\n\nModel Explainability:\n\nThese coefficients provide direct insight into how the model is making\n\ndecisions based on the presence of certain words in the text. Words with\n\npositive coefficients increase the likelihood that a review is classified as positive, while those with negative coefficients push towards a negative\n\nclassification.\n\nIf you want further interpretation, techniques like LIME (Local\n\nInterpretable Model-agnostic Explanations) or SHAP (SHapley Additive\n\nexPlanations) can be applied to provide more nuanced insights into specific\n\npredictions.\n\nWould you like to dive deeper into any specific area, such as handling data\n\nimbalance or exploring model explainability techniques like LIME or\n\nSHAP for a particular prediction?\n\n[End of response]\n\nChatGPT-4 cannot install new libraries like SHAP, so we will finish the\n\nfeature analysis here and move on to the next step of handling data\n\nimbalance.\n\nFeature 4: Handling data imbalance Now, we will instruct ChatGPT-4 to work with data imbalance.\n\n[Prompt]\n\nLet’s move on to handling data imbalance.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nHandling data imbalance is crucial in model training, especially in cases\n\nlike yours where the dataset is heavily skewed towards one class. This skew\n\ncan lead the model to predominantly predict the majority class, as we’ve\n\nseen with the Logistic Regression model.\n\nStrategies to Handle Data Imbalance\n\n1. Resampling Techniques:\n\nOversampling the minority class: This involves randomly\n\nduplicating examples in the minority class.\n\nUndersampling the majority class: This reduces the number of\n\nexamples in the majority class to balance with the minority\n\nclass.\n\n2. Synthetic Data Generation:\n\nSMOTE (Synthetic Minority Over-sampling Technique):\n\nThis technique generates synthetic samples from the minority\n\nclass instead of creating copies.\n\n3. Modify Algorithm:\n\nAdjust the decision threshold: Moving the decision threshold\n\ncan help increase the recall of the minority class.\n\nUse algorithms that are inherently better at dealing with\n\nimbalanced data, such as Decision Trees or Random Forests.\n\n4. Cost-sensitive Learning:\n\nAssign a higher cost to misclassifications of the minority class to\n\nmake the model pay more attention to it.\n\nGiven that we are using Logistic Regression, a good approach to start with\n\ncould be to apply SMOTE to generate synthetic samples for the minority\n\nclass and see if that improves model performance. We can also adjust the\n\ndecision threshold to be more sensitive towards the minority class.\n\nLet’s start by applying SMOTE to the training data and then retraining the\n\nLogistic Regression model. I’ll proceed with this approach:\n\nIt looks like the imblearn library, which includes SMOTE, isn’t available in\n\nthis environment. Given this limitation, we can explore other options:\n\nUndersampling the Majority Class: This would reduce the size of\n\nthe ‘positive’ class to match the ‘negative’ class, though it can lead to a\n\nloss of potentially useful data.\n\nAdjusting the Decision Threshold: Instead of resampling, we can\n\nadjust the threshold for classifying a review as positive or negative to\n\nmake the model more sensitive to the minority class.\n\nLet’s proceed with adjusting the decision threshold. This method involves\n\nmodifying the probability threshold at which the model classifies a sample\n\nas ‘positive’. By default, this threshold is set at 0.5. Lowering this threshold\n\ncan help in identifying more samples as ‘negative’, potentially improving\n\nrecall for the minority class.\n\nI’ll compute the probabilities of the class predictions and then adjust the\n\nthreshold to see its effect on the classification metrics. Let’s proceed with\n\nthis approach:\n\nAdjusting the decision threshold to 0.4 did not change the classification\n\noutcomes. The model still predicts all reviews as ‘positive’ with the same\n\nmetrics as before, which suggests that we may need to either significantly\n\nlower the threshold or consider other methods due to the strong bias\n\ntowards the ‘positive’ class.\n\nAlternatives and Considerations:\n\nFurther Lower the Threshold: We could continue to experiment with\n\nlower thresholds, although this might eventually lead to higher false\n\npositives.\n\nChange the Model or Features: Since logistic regression might be\n\ninherently limited in handling this imbalance, experimenting with\n\ndifferent models or feature sets could be beneficial.\n\nData Resampling Manually: If libraries for automated resampling like imblearn aren’t available, we could manually undersample the\n\n‘positive’ class or oversample the ‘negative’ class, albeit the latter\n\nwithout the sophistication of synthetic sample generation as in\n\nSMOTE.\n\nWould you like to try further lowering the threshold, or should we explore\n\nmanual resampling or perhaps a different modeling approach?\n\n[End of response]\n\nWe see that adjusting the decision threshold didn’t do much, so we will ask\n\nChatGPT-4 to move on to the next feature of hyperparameter tuning.\n\nFeature 5: Hyperparameter tuning We will ask ChatGPT-4 to proceed with hyperparameter tuning:\n\n[Prompt]\n\nLet’s proceed to hyperparameter tuning.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nHyperparameter tuning is a critical step for improving the performance of a\n\nmachine learning model. For Logistic Regression, some key\n\nhyperparameters that can be tuned include:\n\nC (Inverse of regularization strength): Smaller values specify\n\nstronger regularization.\n\nSolver: Algorithm to use in the optimization problem (e.g., liblinear,\n\nsag, saga, newton-cg, etc.).\n\nPenalty: Specifies the norm used in the penalization (e.g., l1, l2).\n\nWe can use grid search with cross-validation to systematically explore\n\nmultiple combinations of parameter values. This approach can help identify\n\nthe best settings for these hyperparameters.\n\nImplementing Hyperparameter Tuning\n\nWe will use GridSearchCV from scikit-learn to perform the hyperparameter tuning. We’ll define a grid of potential values for C and\n\npenalty, and then train the model using different combinations of these values to find the best ones.\n\nLet’s set up and execute the grid search:\n\nThe hyperparameter tuning via grid search has identified the best\n\nparameters for the Logistic Regression model:\n\nC (Inverse of regularization strength): 100, indicating less\n\nregularization.\n\nPenalty: ‘l2’, which is the L2 norm used in regularization.\n\nSolver: ‘sag’, which is an optimization algorithm suitable for large\n\ndatasets.\n\nThe best cross-validated accuracy obtained with these settings is 80.5%.\n\nThis suggests that reducing regularization (a higher value of C) helps the\n\nmodel to better fit the data.\n\nNext Steps:\n\nEvaluate the Model with the Best Parameters: We can now evaluate\n\nthis optimized model on the test set to see how it performs compared\n\nto the previous configuration.\n\nFurther Experimentation: Depending on the results, we might\n\nconsider further tuning or trying out different modeling techniques.\n\nShall we proceed by evaluating the model with these optimized parameters\n\non the test set, or is there anything specific you would like to explore\n\nfurther?\n\n[End of response]\n\nWe will instruct ChatGPT-4 to evaluate the model with optimized\n\nparameters on the test set:\n\n[Prompt]\n\nSure, evaluate.\n\n[End of prompt]\n\nWe should receive a response from ChatGPT-4 similar to the following:\n\n[Prompt response]\n\nThe evaluation of the optimized Logistic Regression model with the best\n\nparameters yields the following results:\n\nOverall Accuracy: 84%\n\nPrecision and Recall for ‘Negative’ class: Precision of 33% and\n\nrecall of 14%.\n\nPrecision and Recall for ‘Positive’ class: Precision of 87% and recall\n\nof 95%.\n\nInterpretation:\n\nThe overall accuracy has slightly decreased compared to the initial\n\nmodel (which had 86% accuracy), but we see an improvement in\n\nhandling the ‘negative’ class:\n\nThe model now correctly identifies some of the negative reviews,\n\nthough the performance (14% recall) is still quite low, indicating that\n\nthe model can detect a few negative instances but misses many.\n\nThe precision for negative reviews is 33%, which means when the\n\nmodel predicts a review as negative, it is correct about one-third of the\n\ntime.\n\nKey Points:\n\nModel Sensitivity to Negative Reviews: There’s a slight improvement\n\nin identifying negative reviews, which might be due to less\n\nregularization (higher C value) allowing the model to fit more closely\n\nto the minority class.\n\nBalance between Classes: The balance between sensitivity to both\n\nclasses (macro average scores) is still not ideal, indicating that further\n\nadjustments or a different modeling approach may be necessary to\n\nadequately capture the characteristics of the minority class.\n\nFurther Actions:\n\nExperiment with Other Models: Given the inherent limitations of\n\nLogistic Regression with imbalanced datasets, considering other\n\nmodels like Decision Trees, Random Forest, or even ensemble\n\nmethods might yield better results.\n\nAdjust Thresholds or Resample Data: These actions could be\n\nrevisited or combined with different modeling techniques for\n\npotentially better handling of class imbalance.\n\nWould you like to explore alternative models or techniques, or is there\n\nanother aspect of the project you’d like to focus on?\n\n[End of response]\n\nWe have seen that ChatGPT-4 has evaluated the performance and explained\n\nthe results. We will now move on to the feature representation.\n\nFeature 6: Experimenting with feature representation Let’s instruct ChatGPT to provide us with the code for different feature\n\nrepresentations:\n\n[Prompt]\n\nI want to compare the performance of the model trained with different\n\nfeature representations, such as bag-of-words, TF-IDF, or word\n\nembeddings, to determine the most effective approach for sentiment\n\nclassification.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nus with the code for bag-of-words, TF-IDF, and word embeddings:\n\n[Prompt response]\n\nComparing different feature representations is an excellent approach to\n\nidentify the most effective method for sentiment classification. Let’s outline\n\nthe feature representations we’ll evaluate:\n\nBag-of-Words (BoW): Counts the frequency of words within the\n\ndocuments.\n\nTF-IDF: We’ve already implemented this. It considers not only the\n\nfrequencies but also how unique a word is to a document.\n\nWord Embeddings: Uses models like Word2Vec or pre-trained\n\nembeddings from GloVe to represent words in a dense vector space,\n\ncapturing semantic meanings.\n\nStep-by-Step Approach:\n\nImplement Bag-of-Words: We will use scikit-learn’s CountVectorizer to convert text documents into a matrix of token\n\ncounts.\n\nUse Word Embeddings: We’ll apply a simple Word2Vec model using\n\nGensim, or use pre-trained GloVe embeddings for a more\n\nstraightforward implementation.\n\nTrain Logistic Regression on each representation: For consistency,\n\nwe’ll use Logistic Regression for each feature set.\n\nCompare Performance: Evaluate each model using accuracy,\n\nprecision, recall, and F1-score.\n\nLet’s start by implementing the Bag-of-Words model. Afterward, we’ll\n\nproceed to the word embeddings. Here’s the code for the Bag-of-Words\n\nmodel:\n\nThe Logistic Regression model trained with the Bag-of-Words (BoW)\n\nfeature representation has the following performance metrics:\n\nOverall Accuracy: 86%\n\nPrecision and Recall for ‘Negative’ class: Precision of 50% and\n\nrecall of 21%.\n\nPrecision and Recall for ‘Positive’ class: Precision of 88% and recall\n\nof 97%.\n\nBoW Performance Interpretation:\n\nThe BoW model shows a slight improvement in handling the negative class\n\ncompared to the initial TF-IDF model, demonstrating higher precision\n\nthough recall remains low. This indicates that while the model can identify\n\nnegative reviews more accurately than before, it still misses many negative\n\ncases.\n\nNext, let’s implement the word embeddings feature representation using\n\npre-trained GloVe embeddings for simplicity. This will provide us with a\n\ndense representation that captures semantic meanings, which might improve\n\nthe classification, especially for the negative reviews.\n\nImplementing Word Embeddings:\n\nLoad pre-trained GloVe Embeddings: We’ll use embeddings to\n\nconvert text data into a dense vector form.\n\nAverage Word Embeddings: For each review, we’ll compute the\n\naverage of the word embeddings to form a single vector that represents\n\nthe entire review.\n\nLet’s proceed with loading and applying GloVe embeddings for sentiment\n\nclassification:\n\nIt seems we’ve encountered a limitation with accessing the Gensim\n\ndownloader, which is used to load pre-trained GloVe embeddings, due to\n\nthe environment not having internet access.\n\nAlternative Approach:",
      "page_number": 370
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 393-414)",
      "start_page": 393,
      "end_page": 414,
      "detection_method": "synthetic",
      "content": "Since we can’t use pre-trained GloVe embeddings, let’s consider a simpler\n\nimplementation of word embeddings using the Word2Vec model directly on\n\nour dataset. This will allow us to create our own embeddings based on the\n\nvocabulary present in the reviews.\n\nLet’s proceed by training a Word2Vec model on the processed reviews and\n\nthen using the embeddings to represent each document. Here’s how we’ll do it:\n\nThe Logistic Regression model trained with Word2Vec embeddings\n\ngenerated the following results:\n\nOverall Accuracy: 86%\n\nPrecision and Recall for ‘Negative’ class: Both precision and recall\n\nare 0%. The model failed to correctly identify any of the negative class\n\ninstances.\n\nPrecision and Recall for ‘Positive’ class: Precision of 86% and recall\n\nof 100%.\n\nInterpretation of Word2Vec Model:\n\nThe performance of the Word2Vec model is similar to the original TF-IDF\n\nmodel in terms of accuracy and class-specific metrics. This similarity\n\nindicates that the model still heavily favors the majority class (‘positive’).\n\nThe zero recall and precision for the negative class suggest that despite\n\nusing semantic-rich word embeddings, the imbalance in the dataset still\n\nheavily influences the model’s predictions.\n\nSummary and Recommendations:\n\nBoW vs. TF-IDF vs. Word2Vec: The Bag-of-Words model showed a\n\nslight improvement over TF-IDF and Word2Vec in terms of precision\n\nfor the negative class. However, all models struggled with recall for\n\nthe negative class, highlighting the impact of class imbalance.\n\nFurther Improvements: Experimentation with ensemble methods,\n\nmore sophisticated neural networks (e.g., LSTM, CNN for text), or\n\nfurther balancing techniques might be necessary to improve recall for\n\nthe negative class.\n\nAlternative Feature Engineering: Additional feature engineering\n\nmethods, such as n-grams, could be explored to capture more context,\n\nwhich might help in identifying negative sentiments more accurately.\n\nWould you like to explore further adjustments, such as different machine\n\nlearning models or additional balancing techniques, or would you like to\n\nfocus on another aspect of your project?\n\n[End of response]\n\nThis concludes our section on how to use ChatGPT-4 or ChatGPT Plus to\n\nimplement and execute the sentiment analysis for the Amazon product\n\nreview dataset.\n\nAssignment Generate synthetic data to handle the data imbalance.\n\nChallenge Try to improve the model performance to greater than 86% and feel free to\n\nuse any technique.\n\nSummary\n\nThe strategies outlined provided a systematic approach to experimenting\n\nwith different techniques for building and training sentiment analysis\n\nmodels using the Amazon product review dataset. Each step was\n\nmeticulously detailed, with code generation and user interaction tailored to\n\na beginner-friendly experience, ensuring a comprehensive learning journey.\n\nThe task began with constructing a baseline sentiment analysis model.\n\nInitial steps included preprocessing text data by tokenizing, lowercasing,\n\nand removing stopwords and punctuation, followed by feature engineering\n\nusing TF-IDF encoding.\n\nDetailed Python code was provided in a Jupyter Notebook format, complete\n\nwith explanations for each operation, ensuring that even beginners could\n\nfollow along comfortably.\n\nWe explored baseline machine learning models, starting with logistic\n\nregression. The model was trained and evaluated, revealing a significant\n\nclass imbalance that skewed predictions toward the majority class. This\n\nphase included detailed metrics analysis, such as accuracy, precision, recall,\n\nand F1-score, enhancing understanding of model performance beyond mere\n\naccuracy.\n\nTo address the data imbalance, techniques like adjusting the decision\n\nthreshold and experimenting with synthetic data generation methods such as\n\nSMOTE were discussed. However, limitations in the environment prompted\n\na shift to manual approaches like undersampling and threshold adjustments,\n\nwhich were implemented and tested to refine model sensitivity toward the\n\nminority class.\n\nThe learning process was enhanced by hyperparameter tuning using\n\nGridSearchCV, focusing on optimizing parameters like regularization\n\nstrength and solver type. This step improved model performance and\n\nprovided insights into the impact of model configuration on sentiment\n\nclassification.\n\nThe experimentation extended to comparing different feature\n\nrepresentations – bag-of-words, TF-IDF, and word embeddings – to\n\ndetermine their effectiveness in sentiment analysis. Each technique was\n\nimplemented, and their impact on model performance was critically\n\nevaluated, revealing nuances in how different text representations affect the\n\nability to discern sentiment.\n\nThroughout the process, the strategy of waiting for user feedback before\n\nproceeding ensured that the learning was paced appropriately and that each\n\nstep was clear. This approach facilitated a structured exploration of\n\nsentiment analysis techniques, from basic preprocessing to complex model\n\ntuning.\n\nThe journey concluded with a comprehensive understanding of building and\n\noptimizing sentiment analysis models. The structured, iterative approach –\n\nenhanced by continuous user engagement and feedback – allowed a deep\n\ndive into machine learning model development, from theoretical concepts\n\nto practical implementation.\n\nThis experience not only equipped the user with the knowledge to handle\n\ntext data and model training but also highlighted the challenges and\n\nconsiderations in dealing with imbalanced datasets and choosing the right\n\nmodel and features for sentiment analysis.\n\nIn the next chapter, we will learn how to use ChatGPT to generate code for\n\nlinear regression.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n13\n\nBuilding a Regression Model for Customer Spend with ChatGPT\n\nIntroduction In the realm of data-driven decision making, understanding customer behavior is pivotal for optimizing business strategies. Building on our exploration of classification techniques, this chapter shifts focus to regression analysis, specifically linear regression, to predict numerical values such as a customer’s annual spending. Linear regression helps us discover relationships within data, enabling predictions based on observed patterns.\n\nThis chapter will guide you through the process of building a predictive model that estimates annual spending by\n\ncustomers based on their interactions with a digital platform. We aim to deepen your understanding of linear\n\nregression, demonstrating how to prepare, process, and utilize datasets to construct accurate and reliable models.\n\nAs we progress, we will explore various techniques to enhance model accuracy and handle complex data\n\nscenarios:\n\nUtilizing advanced regularization techniques to improve model stability and performance.\n\nGenerating synthetic datasets to better understand model behaviors under different data conditions.\n\nStreamlining model development with comprehensive, end-to-end coding examples.\n\nBy the end of this chapter, you will be well equipped with the knowledge and skills necessary to utilize linear\n\nregression for data-driven decision-making in your business. Let’s embark on this journey into regression analysis to optimize customer engagement and revenue generation on our app or website.\n\nIn this chapter, we will:\n\nBuild a regression model with ChatGPT: Readers will learn how ChatGPT can assist in generating Python\n\ncode for building a regression model to predict the yearly amount spent by customers on our app or website using the dataset we have, offering a hands-on approach to understanding and interacting with datasets.\n\nApply prompting techniques: Effective techniques will be introduced to craft prompts that guide ChatGPT in providing the most useful code snippets and insights for regression tasks.\n\nBusiness problem An e-commerce store seeks to optimize customer engagement and increase revenue by gaining deeper insights into\n\ncustomer behavior and preferences. By analyzing various customer attributes and their purchasing patterns, the store aims to tailor its marketing strategies, improve customer retention, and enhance the overall shopping\n\nexperience.\n\nProblem and data domain We will employ regression techniques to understand the relationship between yearly spending and other\n\nparameters. Regression is a way to find out whether and how different factors (like time spent on an app or\n\nwebsite) relate to how much customers spend in the online store. It helps us understand and predict customer behavior. By understanding which factors are most influential in driving sales, an e-commerce store can tailor its strategies to enhance these areas and potentially increase revenue.\n\nDataset overview The e-commerce store collects the following information from the customer:\n\nEmail: This is the customer’s email address. It is a unique identifier for each customer and can be used for\n\ncommunication, such as sending order confirmations, newsletters, or personalized marketing offers.\n\nAddress: This refers to the physical address of the customer. It’s crucial for delivering products they have\n\npurchased. Additionally, address data can sometimes provide insights into geographical trends in sales and preferences.\n\nAvatar: This could be a digital representation or image chosen by the user. It might not directly impact sales\n\nor customer behavior, but it can be part of customer engagement strategies, adding a personal touch to user\n\nprofiles.\n\nAvg Session Length: This is the average duration of all sessions combined, in minutes. This is like measuring\n\nhow long a customer spends in your store each time they visit. Imagine someone walking around, looking at products for, say, 33 minutes on average.\n\nTime on App: This the duration of presence on the store’s application, in minutes. Think of it as how long they are browsing through your app, maybe while they are on the bus or waiting in line at the coffee shop.\n\nTime on Website: This is similar to the time on the app, but this is for your website. If they’re using a\n\ncomputer at home or work to look at your store, how long do they stay?\n\nLength of Membership: This is how long these customers have been with your store. Some might be new,\n\nwhile others have been shopping with you for years.\n\nYearly Amount Spent: This is the total amount of money each customer spends at your store in a year, in\n\ndollars.\n\nIn the context of our dataset:\n\nEmail and address: These should be used primarily for transactional purposes unless the customer has\n\nagreed to receive marketing communications. We will not use them for analysis.\n\nAvatar: This can be used to personalize the user experience but does not hold significant analytical value for sales predictions.\n\nOther data: Variables like “Time on App” and “Time on Website” can be analyzed to improve user experience and business strategies without infringing on personal privacy.\n\nIn summary, while data like email, address, and avatar can be valuable for business operations and customer\n\nengagement, they must be handled with a high degree of responsibility, prioritizing the privacy and preferences of\n\nthe customers.\n\nNote that the data used is not a real dataset and hence the emails, addresses, and so on are all made up.\n\nBreaking the problem down into features Given the nature of our dataset, which includes both independent variables (like “Avg. Session Length,” “Time on\n\nApp,” “Time on Website,” and “Length of Membership”) and a dependent variable (“Yearly Amount Spent”), we will start with a simple regression technique with both ChatGPT and ChatGPT Plus or GPT-4. This will include the following high-level steps:\n\n1. Building the model step by step: Users will understand the process of building a machine learning model step by step, including loading the dataset, splitting it into training and testing sets, training the model, making predictions, and evaluating its performance.\n\n2. Apply regularization techniques: Users will learn how to apply regularization techniques such as Ridge regression and Lasso regression with cross-validation to improve the performance of a linear regression model. This includes initializing the models, training them using the training data, and evaluating their performance.\n\n3. Generate a synthetic dataset to add complexity: Users will discover how to generate a synthetic dataset with added complexity using the make_regression function from the sklearn.datasets module. This involves specifying the number of samples, features, and noise levels to mimic real-world data.\n\n4. Generating code to develop a model in a single step for a synthetic dataset: Users will see how to write end-to-end code in a single step to load the synthetic dataset, split it into training and testing sets, train a linear regression model, evaluate its performance, and print the evaluation metrics. This allows for a\n\nstreamlined approach to model development and evaluation.\n\nPrompting strategy To leverage ChatGPT for machine learning, we need to have a clear understanding of how to implement the prompting strategies specifically for code generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better understanding of what needs to go into\n\nthe initial prompt.\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy 1.1 – Task: The specific task or goal is to create a simple linear regression model to predict the “Yearly Amount\n\nSpent” by dataset based on various attributes in the dataset.\n\n1.2 – Actions: In this case, the strategy is to let ChatGPT decide the steps, hence no specific steps are provided.\n\n1.3 – Guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter Notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the text block of the notebook in detail for each method used in the code before providing the code.\n\nStrategy 2: Persona-Instructions-Context (PIC) prompt strategy 2.1 – Persona: We will adopt the persona of a beginner who needs to learn the different steps of model creation;\n\nhence the code should be generated step by step.\n\n2.2 – Instructions: We have included the step to mount Google Drive explicitly since it’s a common oversight.\n\n2.3 – Context: The most important part is to provide the context of the dataset and exact field names to generate the code that can be executed directly, or to provide the dataset itself in the case of ChatGPT Plus.\n\nStrategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy 3.1 – Learn:\n\nWe want to learn about linear regression and how it works.\n\nUnderstand feature engineering techniques and model evaluation metrics.\n\nWe want to learn how to create a synthetic dataset.\n\n3.2 – Improvise:\n\nWe will use it later while applying regularization techniques.\n\n3.3 – Feedback:\n\nIf the code provided results in any errors, then feedback should be provided back to ChatGPT. We applied it\n\nin the Lasso and Ridge code execution using ChatGPT Plus.\n\n3.4 – Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity. This is used throughout the chapter.\n\nBuilding a simple linear regression model to predict the “Yearly Amount Spent” by customers using the free version of ChatGPT When using the free version, it’s important to give ChatGPT a clear description of the dataset first, which serves as\n\nan effective approach to generating code, followed by user evaluation. ChatGPT is already trained very well on\n\nPython and a machine learning algorithm; hence, we do not need to provide specific steps here. To verify that the\n\ngenerated code functions correctly, we will be using Google Colab.\n\nFeature 1: Building the model step by step Let’s craft our initial prompt for the classification model.\n\n[Prompt]\n\nI want to create a simple linear regression model to predict the “Yearly Amount Spent” (TAG 1.1) by customers for\n\nthe e-commerce customers dataset which consists of the following columns (PIC 2.3):\n\n1. Email: The electronic mail unique to each customer, used here as an identification.\n\n2. Address: The physical geographical location of each customer.\n\n3. Avatar: This is the visual anonymity representation of the customers in the database.\n\n4. Avg. Session Length: The average duration of all session combined in minutes.\n\n5. Time on App: This the duration of presence on the stores’ application, in minutes.\n\n6. Time on Website: This is the time duration on the website of the store, in minutes.\n\n7. Length of Membership: This is the amount of time a customer remains a registered user of the store’s online\n\nresources.\n\n8. Yearly Amount Spent: This is the cost implication of accessing the store’s online resources, paid by the\n\ncustomers, in dollars.\n\nI am a beginner user (PIC 2.1), so please provide step by step code and strictly follow following instructions (PIC\n\n2.2):\n\nCode should be compatible with Jupyter notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied in the text block of the notebook in detail for\n\neach method used in code before providing the code.\n\nAlso note that the dataset is placed in Google Drive at this path '/content/drive/My\n\nDrive/Chapter05/Ecommerce Customers.csv ' (TAG 1.2).\n\nSo, add the code to mount the drive, load the dataset, provide one step at a time, and wait for users feedback.\n\n[End of prompt]\n\nYou should receive a response similar to the following.\n\nFigure 13.1: The initial response from ChatGPT\n\n[Prompt response]\n\nSure, let’s proceed step by step. First, we need to mount Google Drive in our Jupyter Notebook environment so we\n\ncan access the dataset.\n\nHere’s how you can do it:\n\nfrom google.colab import drive # Mount Google Drive drive.mount('/content/drive')\n\nExplanation:\n\nfrom google.colab import drive: This line imports the drive module from the google.colab package.\n\ndrive.mount('/content/drive'): This command mounts your Google Drive at the specified path /content/drive. When you run this cell, it will prompt you to authenticate and give you a link to follow to\n\nget an authorization code. Once you enter the code, your Google Drive will be mounted.\n\nLet me know if you have any questions before we proceed.\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy and validate the code by executing the code in Google Colab\n\nfor every step. The code runs perfectly fine and mounts Google Drive.\n\nPlease note that it requires permission to access the drive and would prompt the user.\n\nPlease refer to the notebook Learning simple regression with ChatGPT.ipynb for the executable code snippets or\n\ncopy the code from the prompt response.\n\nRunning your cell, you should see a message that says “Mounted at /content/drive as a result of calling drive.mount.”\n\nWe will now instruct ChatGPT to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed\n\n[End of prompt]\n\nWe should receive a response similar to the following to load the dataset into the notebook:\n\n[Prompt response]\n\nGreat! Now that we have mounted Google Drive, we can load the dataset into our notebook.\n\nHere’s how you can load the dataset:\n\nimport pandas as pd # Define the file path file_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' # Load the dataset into a DataFrame ecommerce_data = pd.read_csv(file_path) # Display the first few rows of the DataFrame print(ecommerce_data.head())\n\nExplanation:\n\nimport pandas as pd: This line imports the pandas library and aliases it as pd, which is a common\n\nconvention.\n\nfile_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv': This line defines the file\n\npath where the dataset is located in your Google Drive.\n\npd.read_csv(file_path): This function reads the CSV file located at the specified path and creates a DataFrame from it.\n\nprint(ecommerce_data.head()): This line prints the first few rows of the DataFrame to verify that the dataset\n\nhas been loaded correctly.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nWe executed the code provided by ChatGPT in the notebook and it loads the dataset from the defined path and\n\nprints the first few rows. As we had given the path of the file in the initial prompt, we don’t need to change anything in the code and it runs perfectly fine. The takeaway is that we need to be specific with file paths and\n\nlocations where required.\n\nFigure 13.2: Loading the dataset\n\nNext, we’ll provide the output to ChatGPT and then proceed to Step 1. In this instance, we’ve employed the\n\nstrategy of providing additional context to ChatGPT by presenting the output textually.\n\nIt is important to note that since we are utilizing GPT-3.5, which lacks the capability to execute code or process images, we are limited to providing textual outputs from the notebook:\n\n[Prompt]\n\nHere is the output - Email \\ 0 mstephenson@fernandez.com 1 hduke@hotmail.com 2 pallen@yahoo.com 3 riverarebecca@gmail.com 4 mstephens@davidson-herman.com Address Avatar \\ 0 835 Frank Tunnel\\nWrightmouth, MI 82180-9605 Violet 1 4547 Archer Common\\nDiazchester, CA 06566-8576 DarkGreen 2 24645 Valerie Unions Suite 582\\nCobbborough, D... Bisque 3 1414 David Throughway\\nPort Jason, OH 22070-1220 SaddleBrown 4 14023 Rodriguez Passage\\nPort Jacobville, PR 3... MediumAquaMarine Avg. Session Length Time on App Time on Website Length of Membership \\\n\n0 34.497268 12.655651 39.577668 4.082621 1 31.926272 11.109461 37.268959 2.664034 2 33.000915 11.330278 37.110597 4.104543 3 34.305557 13.717514 36.721283 3.120179 4 33.330673 12.795189 37.536653 4.446308 Yearly Amount Spent 0 587.951054 1 392.204933 2 487.547505 3 581.852344 4 599.406092\n\n[End of prompt]\n\nWe anticipate receiving a response similar to the following, wherein ChatGPT will offer guidance on the\n\nsubsequent steps. Initially, we refrained from specifying any particular steps in the initial prompt, opting instead to allow ChatGPT to determine the subsequent course of action:\n\n[Prompt response]\n\nGreat! The dataset has been loaded successfully. Now, we can proceed with building our linear regression model to\n\npredict the “Yearly Amount Spent” by customers.\n\nThe steps involved in building a simple linear regression model are as follows:\n\nData Preparation: Split the dataset into input features (X) and the target variable (y).\n\nSplit Data: Split the dataset into training and testing sets.\n\nModel Initialization: Create a linear regression model object.\n\nModel Training: Fit the model to the training data.\n\nModel Evaluation: Evaluate the model’s performance using the testing data.\n\nLet’s start with step 1:\n\nStep 1: Data Preparation:\n\n# Extract input features (X) and target variable (y) X = ecommerce_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership y = ecommerce_data['Yearly Amount Spent']\n\nExplanation:\n\nX = ecommerce_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']]: This line selects the input features ('Avg. Session Length', 'Time on App', 'Time on\n\nWebsite', 'Length of Membership') from the dataset and assigns them to the variable X.\n\ny = ecommerce_data['Yearly Amount Spent']: This line selects the target variable 'Yearly Amount Spent'\n\nfrom the dataset and assigns it to the variable y.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nChatGPT has outlined the steps it will adhere to for code generation. Furthermore, it has furnished the code for the initial step, data preparation, wherein the dataset is divided into features and the target variable. Notably, it adeptly\n\ndiscerned the features by excluding email addresses, avatars, and addresses. Additionally, it precisely employed the column names as stipulated in the description provided in the initial prompt. The strategy employed here involves\n\nincluding the dataset column names in the prompt to ensure compatibility with the generated code. Hence, the code executes without any modifications or errors.\n\nFigure 13.3: Step 1 – data preparation\n\nNext, we will instruct ChatGPT to proceed:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response similar to the following to split the data along with the explanation:\n\n[Prompt response]\n\nGreat! Now that we have prepared our data by separating the input features (X) and the target variable (y), we can proceed to the next step, which is splitting the data into training and testing sets.\n\nStep 2: Split Data:\n\nfrom sklearn.model_selection import train_test_split # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Display the shapes of the training and testing sets print(\"Training set shape - X_train:\", X_train.shape, \"y_train:\", y_train.shape) print(\"Testing set shape - X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n\nExplanation:\n\nfrom sklearn.model_selection import train_test_split: This line imports the train_test_split function\n\nfrom the sklearn.model_selection module, which is used to split the dataset into training and testing sets.\n\ntrain_test_split(X, y, test_size=0.2, random_state=42): This function splits the input features (X) and\n\nthe target variable (y) into training and testing sets. The test_size parameter specifies the proportion of the dataset to include in the testing set (in this case, 20% for testing). The random_state parameter is used for\n\nreproducibility, ensuring that the same random split is generated each time the code is run.\n\nX_train, X_test, y_train, y_test: These variables hold the training and testing sets for the input features (X) and the target variable (y), respectively.\n\nprint(\"Training set shape - X_train:\", X_train.shape, \"y_train:\", y_train.shape): This line prints the shapes of the training set input features (X_train) and target variable (y_train).\n\nprint(\"Testing set shape - X_test:\", X_test.shape, \"y_test:\", y_test.shape): This line prints the shapes of the testing set input features (X_test) and target variable (y_test).\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nThe code executed successfully, and the data was split into training and testing sets with the expected 80-20 ratio. Notably, we did not explicitly specify the ratio, yet ChatGPT defaulted to the standard 80-20 split.\n\nFigure 13.4: Step 2 – split the data\n\nNext, we will provide the output of the previous step to ChatGPT, which should let ChatGPT know about the\n\nshape and it should also provide us the code for the next step:\n\n[Prompt]\n\nTraining set shape - X_train: (400, 4) y_train: (400,) Testing set shape - X_test: (100, 4) y_test: (100,)\n\n[End of prompt]\n\nWe should receive a response similar to the following where the model is initialized:\n\n[Prompt response]\n\nPerfect! We have successfully split our dataset into training and testing sets. The training set contains 400 samples, while the testing set contains 100 samples. Each sample consists of four input features and one target variable.\n\nNow, let’s proceed to the next step, which is initializing our linear regression model.\n\nStep 3: Model Initialization:\n\nfrom sklearn.linear_model import LinearRegression # Initialize the linear regression model model = LinearRegression()\n\nExplanation:\n\nfrom sklearn.linear_model import LinearRegression: This line imports the LinearRegression class from\n\nthe sklearn.linear_model module, which is used to create a linear regression model.\n\nmodel = LinearRegression(): This line initializes a linear regression model object and assigns it to the\n\nvariable model.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nThe code is executed perfectly fine without any errors. Necessary imports are also provided by ChatGPT.\n\nFigure 13.5: Step 3 – model initialization\n\nWe’ll simply direct ChatGPT to continue. It’s worth noting that because our initial prompt included the instruction\n\nto provide step-by-step code, we don’t need to tailor our subsequent prompts to each individual step or task. Instead, we’ll allow ChatGPT to lead us, providing it with the outputs from the preceding steps as necessary:\n\n[Prompt]\n\nLet’s proceed.\n\n[End of prompt]\n\nWe should receive a response similar to the following to train the model:\n\n[Prompt response]\n\nGreat! Now that we have initialized our linear regression model, we can move on to the next step, which is training the model using the training data.\n\nStep 4: Model Training:\n\n# Train the linear regression model using the training data model.fit(X_train, y_train)\n\nExplanation:\n\nmodel.fit(X_train, y_train): This line fits (or trains) the linear regression model using the training data.\n\nThe fit method takes the input features (X_train) and the corresponding target variable (y_train) as arguments and adjusts the model’s parameters to minimize the difference between the actual and predicted values.\n\nNow that our model has been trained, we can proceed to the final step, which is evaluating the model’s performance using the testing data.\n\nLet me know if you have any questions or if you’d like to proceed!\n\n[End of response]\n\nWe will execute the code and that should train the model.\n\nFigure 13.6: Step 4 – model training\n\nWe will instruct ChatGPT to proceed:\n\n[Prompt]\n\nLet’s proceed\n\n[End of prompt]\n\nWe should receive a response similar to the following to evaluate the performance of the model using test data:\n\n[Prompt response]\n\nGreat! Now that our linear regression model has been trained, we can evaluate its performance using the testing\n\ndata.\n\nStep 5: Model Evaluation:\n\n# Make predictions on the testing data y_pred = model.predict(X_test) # Evaluate the model's performance from sklearn.metrics import mean_squared_error, r2_score # Calculate Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_pred) # Calculate R-squared (coefficient of determination) r_squared = r2_score(y_test, y_pred) # Print the evaluation metrics print(\"Mean Squared Error (MSE):\", mse) print(\"R-squared (Coefficient of Determination):\", r_squared)\n\nExplanation:\n\nmodel.predict(X_test): This line uses the trained model to make predictions on the input features (X_test),\n\nwhich generates the predicted values of the target variable.\n\nfrom sklearn.metrics import mean_squared_error, r2_score: This line imports the mean_squared_error and r2_score functions from the sklearn.metrics module, which are used to evaluate the model’s\n\nperformance.\n\nmean_squared_error(y_test, y_pred): This function calculates the mean squared error (MSE) between the actual target variable (y_test) and the predicted values (y_pred).\n\nr2_score(y_test, y_pred): This function calculates the R-squared (coefficient of determination) between the actual target variable (y_test) and the predicted values (y_pred).\n\nprint(\"Mean Squared Error (MSE):\", mse): This line prints the mean squared error (MSE), which measures the average squared difference between the actual and predicted values. A lower MSE indicates better model\n\nperformance.\n\nprint(\"R-squared (Coefficient of Determination):\", r_squared): This line prints the R-squared value,\n\nwhich represents the proportion of the variance in the target variable that is explained by the model. A higher\n\nR-squared value (closer to 1) indicates better fit.\n\nThese evaluation metrics will help us assess how well our linear regression model is performing in predicting the “Yearly Amount Spent” by customers.\n\nLet me know if you have any questions or if you’d like further explanation on any part!\n\n[End of response]\n\nWe execute the code and it runs perfectly fine without any errors and prints the mean-squared error (MSE) and\n\nthe coefficient of determination (R-squared).\n\nFigure 13.7: Model evaluation output\n\nFor those new to machine learning, it’s crucial to assess whether the model performance meets acceptable\n\nstandards. Once more, we’ll seek assistance from ChatGPT and furnish it with the output for evaluation:\n\n[Prompt]\n\nMean Squared Error (MSE): 109.8637411839399 R-squared (Coefficient of Determination): 0.9778130629184127\n\n[End of prompt]\n\nWe should receive a response from ChatGPT explaining the results, which suggests that our model is actually\n\nperforming very well given that R-squared is very close to 1:\n\n[Prompt response]\n\nThe evaluation metrics indicate that our linear regression model performs quite well on the testing data:\n\nMean Squared Error (MSE): The MSE value is approximately 109.86. This indicates that, on average, the\n\nsquared difference between the actual and predicted values of the “Yearly Amount Spent” is around 109.86. Lower MSE values are desirable, indicating better model performance.\n\nR-squared (Coefficient of Determination): The R-squared value is approximately 0.978. This indicates that\n\napproximately 97.8% of the variance in the “Yearly Amount Spent” can be explained by the linear regression model. A higher R-squared value closer to 1 suggests that the model provides a better fit to the data.\n\nOverall, these evaluation metrics suggest that our linear regression model is performing well in predicting the\n\n“Yearly Amount Spent” by customers based on the provided input features.\n\nIf you have any further questions or need additional assistance, feel free to ask!\n\n[End of response]\n\nHaving now acquired proficiency in utilizing ChatGPT to construct a linear regression model incrementally, we’ll transition to the second feature and delve into the application of regularization techniques. While our current model exhibits satisfactory performance, regularization can prove invaluable, particularly in scenarios where the model is either underfitted or overfitted.\n\nFeature 2: Applying regularization techniques We will use the learn and improvise prompting strategy (LIFE 3.1 and 3.2) with ChatGPT to understand whether\n\nthe model could be overfitted and what should be done in such a case:\n\n[Prompt]\n\nCould the model be overfitting?\n\n[End of prompt]\n\nWe receive the response from ChatGPT indicating that it is unlikely that our model is overfitted but it does suggest some things to consider such as regularization techniques and cross-validation:\n\n[Prompt response]\n\nIt’s essential to consider the possibility of overfitting when interpreting the model’s performance. Overfitting\n\noccurs when the model learns to capture noise or random fluctuations in the training data, rather than the\n\nunderlying relationships between the input features and the target variable. This can result in a model that performs well on the training data but generalizes poorly to unseen data.\n\nHere are a few indicators that suggest overfitting may be occurring:\n\nLarge Discrepancy Between Training and Testing Performance: If the model performs significantly better on the training data compared to the testing data, it may be overfitting. In our case, if the MSE and R-squared values are much better on the training set than on the testing set, it could indicate overfitting.\n\nComplexity of the Model: Overly complex models with a large number of parameters relative to the size of the training data are more prone to overfitting. Linear regression is relatively simple, but if you were using a more complex model (e.g., polynomial regression with high-degree polynomials), it could increase the risk of overfitting.\n\nRegularization Techniques: Regularization techniques like Lasso or Ridge regression can be applied to mitigate overfitting by penalizing large coefficients. If overfitting is suspected, applying regularization may help improve the model’s generalization performance.\n\nCross-Validation: Using cross-validation techniques, such as k-fold cross-validation, can provide a more robust estimate of the model’s performance by evaluating it on multiple train-test splits of the data. If the\n\nmodel’s performance varies significantly across different splits, it could indicate overfitting.\n\nIn our case, with a high R-squared value and a relatively low MSE on the testing data, it’s less likely that the model is overfitting. However, it’s always a good practice to explore these indicators and consider additional steps to ensure the model’s generalization performance. If you suspect overfitting, you could try simplifying the model, applying regularization techniques, or using cross-validation to further evaluate its performance.\n\n[End of response]\n\nWe will ask ChatGPT to apply regularization and cross-validation techniques:\n\n[Prompt]\n\nLet’s try applying regularization techniques first and then cross-validation\n\n[End of prompt]\n\nChatGPT should provide us with the code to apply either Lasso or Ridge along with cross-validation and also the explanation:\n\n[Prompt response]\n\nSure! Let’s start by applying regularization techniques to our linear regression model. Two common regularization techniques are Lasso regression (L1 regularization) and Ridge regression (L2 regularization). These techniques add a penalty term to the loss function, which helps prevent overfitting by penalizing large coefficients.\n\nWe’ll begin by trying Ridge regression, which adds the squared sum of the coefficients (L2 norm) to the loss function.\n\nHere’s how you can implement Ridge regression with cross-validation:\n\nfrom sklearn.linear_model import RidgeCV # Initialize the Ridge regression model with cross-validation ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5) # Train the Ridge regression model ridge_model.fit(X_train, y_train) # Make predictions on the testing data ridge_y_pred = ridge_model.predict(X_test) # Evaluate the model's performance ridge_mse = mean_squared_error(y_test, ridge_y_pred) ridge_r_squared = ridge_model.score(X_test, y_test) # Print the evaluation metrics print(\"Ridge Regression - Mean Squared Error (MSE):\", ridge_mse) print(\"Ridge Regression - R-squared (Coefficient of Determination):\", ridge_r_squared) print(\"Ridge Regression - Best Alpha:\", ridge_model.alpha_)\n\nExplanation:\n\nRidgeCV(alphas=[0.1, 1.0, 10.0], cv=5): This initializes a Ridge regression model with cross-validation. The alphas parameter specifies the regularization strength (alpha values) to be tested, and cv specifies the\n\nnumber of folds for cross-validation.\n\nridge_model.fit(X_train, y_train): This trains the Ridge regression model using the training data.\n\nridge_model.predict(X_test): This makes predictions on the testing data using the trained Ridge regression\n\nmodel.\n\nridge_mse = mean_squared_error(y_test, ridge_y_pred): This calculates the mean squared error (MSE) between the actual and predicted values using Ridge regression.\n\nridge_model.score(X_test, y_test): This calculates the R-squared value (coefficient of determination)\n\nusing Ridge regression.\n\nridge_model.alpha_: This attribute returns the best alpha (regularization strength) selected by cross- validation.\n\nLet me know if you’d like to proceed with Ridge regression or if you have any questions!\n\n[End of response]\n\nLet’s try executing the code and see whether it works and shows any improvement.\n\nFigure 13.8: Ridge regression\n\nThe code is compatible with previously provided code and the output is very similar to the linear regression code.\n\nBut note that ChatGPT doesn’t need any specific instructions and it could generate a perfectly fine executable code. Next, we will provide ChatGPT with the output and see what it suggests:\n\n[Prompt]\n\nRidge Regression - Mean Squared Error (MSE): 109.82024999312038 Ridge Regression - R-squared (Coefficient of Determination): 0.9778218459464064 Ridge Regression - Best Alpha: 0.1\n\n[End of prompt]\n\nIn this case, ChatGPT has interpreted the results and also provided the code for Lasso regression with cross- validation. The response may vary depending on the suggestions provided by ChatGPT in previous interactions:\n\n[Prompt response]\n\nIt looks like the Ridge regression model with cross-validation performed quite similarly to the standard linear regression model. The Mean Squared Error (MSE) and R-squared values are very close to those obtained from the standard linear regression model.\n\nThe best alpha selected by cross-validation for the Ridge regression model is 0.1, indicating that a small amount of\n\nregularization was applied to the model.\n\nSince the performance of Ridge regression with cross-validation is comparable to that of the standard linear regression model, we can conclude that regularization did not significantly improve the model’s performance in this case.",
      "page_number": 393
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 415-436)",
      "start_page": 415,
      "end_page": 436,
      "detection_method": "synthetic",
      "content": "Next, let’s try using Lasso regression (L1 regularization) with cross-validation to see if it yields any improvement:\n\nfrom sklearn.linear_model import LassoCV # Initialize the Lasso regression model with cross-validation lasso_model = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5) # Train the Lasso regression model lasso_model.fit(X_train, y_train) # Make predictions on the testing data lasso_y_pred = lasso_model.predict(X_test) # Evaluate the model's performance lasso_mse = mean_squared_error(y_test, lasso_y_pred) lasso_r_squared = lasso_model.score(X_test, y_test) # Print the evaluation metrics print(\"Lasso Regression - Mean Squared Error (MSE):\", lasso_mse) print(\"Lasso Regression - R-squared (Coefficient of Determination):\", lasso_r_squared) print(\"Lasso Regression - Best Alpha:\", lasso_model.alpha_)\n\nLet me know if you’d like to proceed with Lasso regression or if you have any questions!\n\n[End of response]\n\nWe will execute the code and see whether it works and produces any different results than Ridge regression.\n\nFigure 13.9: Lasso regression\n\nLasso regression with cross-validation also produced similar results to both the standard linear regression and Ridge regression models. The MSE and R-squared values are very close to those obtained previously.\n\nHowever, it’s important to note that the dataset used here may not be complex enough to benefit significantly from\n\nregularization techniques. In cases where the dataset is larger or more complex, regularization may play a more\n\ncrucial role in preventing overfitting and improving model generalization.\n\nLet’s move to the next feature to generate a synthetic dataset for our problem to add complexity.\n\nFeature 3: Generating a synthetic dataset to add complexity We will use the conversation style with ChatGPT and ask it to provide code to generate a synthetic dataset:\n\n[Prompt]\n\nCan you please provide me a code to generate the synthetic dataset for it to add complexity?\n\n[End of prompt]\n\nYou may receive a variant of the following response. However, you may notice that this is generic code and is not suitable for generating the code specific to our dataset. Hence, we will need to refine the prompt by providing\n\nfeedback to ChatGPT:\n\n[Prompt response]\n\nSure! We can generate a synthetic dataset with additional complexity using the make_regression function from the\n\nsklearn.datasets module. This function generates a random regression problem with specified characteristics, such as the number of samples, the number of features, and the noise level.\n\nHere’s how you can generate a synthetic dataset with added complexity:\n\nfrom sklearn.datasets import make_regression import pandas as pd # Generate synthetic dataset with 500 samples, 10 features, and increased noise X_synthetic, y_synthetic = make_regression(n_samples=500, n_features=10, noise=50, random_state=42 # Create a DataFrame from the synthetic dataset synthetic_data = pd.DataFrame(X_synthetic, columns=[f'Feature_{i}' for i in range(1, 11)]) synthetic_data['Target'] = y_synthetic # Display the first few rows of the synthetic dataset print(synthetic_data.head()) # Save the synthetic dataset to a CSV file synthetic_data.to_csv('synthetic_dataset.csv', index=False)\n\nExplanation:\n\nmake_regression(n_samples=500, n_features=10, noise=50, random_state=42): This function generates a\n\nsynthetic regression dataset with 500 samples, 10 features, and increased noise level (specified by the noise parameter). The random_state parameter ensures reproducibility.\n\npd.DataFrame(X_synthetic, columns=[f'Feature_{i}' for i in range(1, 11)]): This creates a DataFrame from the synthetic input features (X_synthetic) and assigns column names 'Feature_1' to 'Feature_10'.\n\nsynthetic_data['Target'] = y_synthetic: This adds the synthetic target variable (y_synthetic) to the DataFrame with the column name 'Target'.\n\nsynthetic_data.to_csv('synthetic_dataset.csv', index=False): This saves the synthetic dataset to a CSV file named 'synthetic_dataset.csv' without including the index column.\n\nYou can adjust the parameters of the make_regression function to control the complexity of the synthetic dataset, such as the number of samples, the number of features, and the noise level.\n\nLet me know if you need further assistance!\n\n[End of response]\n\nWe will simply instruct ChatGPT that we need the code for our dataset:\n\n[Prompt]\n\nNo, I meant for our problem.\n\n[End of prompt]\n\nLet’s see what ChatGPT suggests and whether it really works:\n\n[Prompt response]\n\nGot it! We can create a synthetic dataset for our problem by using the same input features as the original dataset\n\nbut with added complexity. We’ll generate additional features that are nonlinear combinations of the original features and add noise to mimic real-world data.\n\nHere’s how you can generate a synthetic dataset with added complexity for our problem:\n\nimport numpy as np import pandas as pd # Set random seed for reproducibility np.random.seed(42) # Number of samples n_samples = 500 # Generate synthetic input features avg_session_length = np.random.normal(loc=30, scale=5, size=n_samples) time_on_app = np.random.normal(loc=15, scale=5, size=n_samples) time_on_website = np.random.normal(loc=20, scale=7, size=n_samples) length_of_membership = np.random.normal(loc=3, scale=1, size=n_samples) # Generate synthetic target variable (linear combination of input features with noise) yearly_amount_spent = 100 + (avg_session_length * 10) + (time_on_app * 20) + (time_on_website * 15 # Create a DataFrame from the synthetic dataset synthetic_data = pd.DataFrame({ 'Avg. Session Length': avg_session_length, 'Time on App': time_on_app, 'Time on Website': time_on_website, 'Length of Membership': length_of_membership, 'Yearly Amount Spent': yearly_amount_spent }) # Display the first few rows of the synthetic dataset print(synthetic_data.head()) # Save the synthetic dataset to a CSV file synthetic_data.to_csv('synthetic_ecommerce_data.csv', index=False)\n\nExplanation:\n\nWe generate synthetic input features (avg_session_length, time_on_app, time_on_website,\n\nlength_of_membership) using normal distributions with specified mean and standard deviation to mimic the characteristics of the original dataset.\n\nWe generate the synthetic target variable (yearly_amount_spent) as a linear combination of the input features\n\nwith added noise.\n\nFinally, we create a DataFrame from the synthetic dataset and save it to a CSV file.\n\nThis synthetic dataset will have similar characteristics to the original dataset but with added complexity, which can\n\nbe useful for testing models and understanding their performance in more challenging scenarios.\n\nLet me know if you need further clarification or assistance!\n\n[End of response]\n\nThe code looks impressive and is adapted to our dataset. On executing, it runs without any errors and generates a CSV file named synthetic_ecommerce_data.csv and stores it in the runtime environment of the notebook. It also\n\nprints the first few rows. You can also download it for future use.\n\nNext, we will request ChatGPT to generate the complete code and evaluate model performance with the added\n\nnoise in the synthetic dataset. Let’s move to our next feature.\n\nFeature 4: Generating code to develop a model in a single step for a synthetic dataset We have learned how to create a regression model step by step in previous features, hence we will now request\n\nChatGPT to provide the entire code in a single step for the synthetic dataset:\n\n[Prompt]\n\nNow provide me the end-to-end code in a single step to run using the generated synthetic dataset.\n\n[End of prompt]\n\nWe should receive a response similar to the following where complete code is provided, including loading the\n\ndataset, splitting the dataset, model training, and model evaluation:\n\n[Prompt response]\n\nCertainly! Below is the end-to-end code that loads the synthetic dataset, splits it into training and testing sets,\n\ntrains a linear regression model, evaluates its performance, and prints the evaluation metrics.\n\nimport pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score # Load the synthetic dataset synthetic_data = pd.read_csv('synthetic_ecommerce_data.csv') # Extract input features (X) and target variable (y) X = synthetic_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership y = synthetic_data['Yearly Amount Spent'] # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the linear regression model model = LinearRegression() # Train the linear regression model using the training data model.fit(X_train, y_train) # Make predictions on the testing data y_pred = model.predict(X_test) # Evaluate the model's performance mse = mean_squared_error(y_test, y_pred) r_squared = r2_score(y_test, y_pred) # Print the evaluation metrics print(\"Mean Squared Error (MSE):\", mse) print(\"R-squared (Coefficient of Determination):\", r_squared)\n\nThis code will load the synthetic dataset from the CSV file, split it into training and testing sets, train a linear\n\nregression model using the training data, make predictions on the testing data, evaluate the model’s performance using Mean Squared Error (MSE) and R-squared (Coefficient of Determination), and print the evaluation metrics.\n\nYou can simply run this code in your Python environment, assuming you have the pandas library installed to work\n\nwith DataFrames and the scikit-learn library installed for machine learning functionalities.\n\nLet me know if you have any questions or if there’s anything else I can assist you with!\n\n[End of response]\n\nWe execute the code and it runs seamlessly.\n\nThe evaluation metrics for the linear regression model trained on the synthetic dataset are as follows:\n\nMSE: Approximately 2103.19\n\nR-squared (coefficient of determination): Approximately 0.9197\n\nThese metrics provide insight into the model’s performance:\n\nMSE: The MSE measures the average squared difference between the actual and predicted values. In this\n\ncase, the MSE indicates that, on average, the squared difference between the actual and predicted yearly\n\namount spent is approximately 2103.19. Lower MSE values are desirable, indicating better model performance.\n\nR-squared (coefficient of determination): The R-squared value represents the proportion of the variance in\n\nthe target variable (Yearly Amount Spent) that is explained by the model. In this case, the R-squared value of\n\napproximately 0.9197 suggests that approximately 91.97% of the variance in the yearly amount spent can be explained by the linear regression model. A higher R-squared value closer to 1 indicates a better fit.\n\nOverall, these evaluation metrics suggest that the linear regression model performs reasonably well on the\n\nsynthetic dataset.\n\nIn the context of the online e-commerce store, the insights from these models can be used for various purposes:\n\nCustomer segmentation: The models can help identify high-value customers based on their predicted Yearly Amount Spent, allowing for targeted marketing efforts.\n\nRecommendation systems: Understanding the factors that influence spending can be used to improve\n\nproduct recommendations and enhance the customer shopping experience.\n\nRetention strategies: Length of Membership appears to be an important factor. Strategies to increase customer retention and loyalty can be developed based on this insight.\n\nOptimizing website and app: Insights into the impact of Average Session Length, Time on App, and Time\n\non Website can guide decisions on website and app optimization.\n\nThis concludes our learning with ChatGPT 3.5. Now, let’s see how prompting and learning differ with ChatGPT\n\nPlus.\n\nLearning simple linear regression using ChatGPT Plus\n\nNow that we have learned about simple linear regression while using ChatGPT, let’s learn about linear regression\n\nwhile using ChatGPT Plus. ChatGPT Plus, or GPT-4, includes the ability to upload datasets, analyze the dataset, and produce results if using simple libraries such as Python and Matplotlib. At the time of writing this chapter, it had a GPT assistant named Data Analysis, provided by OpenAI when using the mobile app. You could also select\n\nGPT-4 if using the browser version.\n\nNote that you can also achieve the same using the free version but would need to execute the code yourself and modify the prompt.\n\nFeature 1: Building a simple linear regression model step by step We have learned how to create a regression model step by step using the free version of ChatGPT where we had to\n\nprovide the dataset description, instructions on from where to load, and code compatible with Jupyter Notebook. In the case of ChatGPT Plus, all of that is taken care of by ChatGPT Plus and it also provides a runtime environment to execute the code.\n\nNote that it cannot run TensorFlow, Keras, or deep learning libraries:\n\n[Prompt]\n\nI want to create a simple linear regression model to predict the “Yearly Amount Spent” (TAG 1.1) by customers for the attached e-commerce customers dataset (PIC 2.3)\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and strictly follow the following instructions (PIC 2.2):\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code and provide one step at a time and wait for users feedback\n\n[End of prompt]\n\nWe should receive a response similar to the following where the complete code is provided, including loading the\n\ndataset, splitting the dataset, model training, and model evaluation:\n\n[Prompt response]\n\nGreat! Let’s start building a simple linear regression model using Python. We’ll be using libraries such as pandas for handling the data and sklearn for creating the regression model.\n\nStep 1: Import Libraries and Load Data\n\nFirst, we’ll import necessary libraries and load your dataset. We’ll also take a look at the first few rows of the dataset to understand its structure.\n\nHere’s the code for this step:\n\n# Import necessary libraries import pandas as pd # pandas is used for data manipulation and analysis from sklearn.model_selection import train_test_split # This function is used to split data into t from sklearn.linear_model import LinearRegression # This class is used to perform linear regressi from sklearn.metrics import mean_absolute_error, mean_squared_error # These functions are used to # Load the dataset\n\ndata = pd.read_csv('/mnt/data/Ecommerce Customers (1).csv') # Reading the CSV file into a pandas # Display the first few rows of the dataset to understand its structure print(data.head())\n\nThis block of code does the following:\n\nImports libraries: We load pandas for data handling, and several modules from sklearn for splitting the data\n\nand performing linear regression.\n\nLoads the data: The pd.read_csv function reads the CSV file into a DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types.\n\nPrints the first few rows: This helps us see the initial rows of our dataset to understand the available data and\n\nits format.\n\nLet’s execute this step and check the output to ensure everything is loaded correctly and to understand what your\n\ndata looks like. Shall we proceed?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the first\n\nfew rows.\n\nNote that the file is loaded in ChatGPT’s run time environment; if you need to execute code in your local\n\nenvironment or Jupyter Colab, you would be required to modify the step to read CSV:\n\n[Prompt response]\n\nHere’s the snapshot of the first few rows of your dataset:\n\nEmail\n\nAddress\n\nAvatar\n\nAvg. Session Length\n\nTime on App\n\nTime on Website\n\nmstephenson@fernandez.com 835 Frank\n\nViolet\n\n34.497268\n\n12.655651\n\n39.577668\n\nTunnel\n\nWrightmouth, MI 82180- 9605\n\nhduke@hotmail.com\n\n4547 Archer Common\n\nDarkGreen\n\n31.926272\n\n11.109461\n\n37.268959\n\nDiazchester, CA 06566- 8576\n\nL M\n\n4.\n\n2.\n\nEmail\n\nAddress\n\nAvatar\n\nAvg.\n\nTime on\n\nTime on\n\nSession Length\n\nApp\n\nWebsite\n\npallen@yahoo.com\n\n24645 Valerie Unions Suite 582\n\nBisque\n\n33.000915\n\n11.330278\n\n37.110597\n\nCobbborough, ...\n\nriverarebecca@gmail.com\n\n1414 David Throughway Port Jason,\n\nSaddleBrown\n\n34.305557\n\n13.717514\n\n36.721283\n\nOH 22070- 1220\n\nmstephens@davidson- herman.com\n\n14023 Rodriguez Passage Port Jacobville,\n\nMediumAquaMarine\n\n33.330673\n\n12.795189\n\n37.536653\n\nPR 3...\n\nThe dataset includes both customer contact information (like Email and Address) and numerical data related to\n\ntheir interaction with the e-commerce platform (like “Avg. Session Length”, “Time on App”, “Time on Website”, and “Length of Membership”).\n\nStep 2: Data Preparation\n\nThe next step will involve preparing the data for our linear regression model. This includes:\n\nSelecting the relevant numerical columns for the prediction.\n\nSplitting the data into a training set and a testing set.\n\nDo you have any specific columns you want to use for predicting “Yearly Amount Spent”, or should I proceed with a general approach using all numerical columns?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed using numerical columns:\n\n[Prompt]\n\nPlease use only numerical columns.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided code for selecting the\n\nfeatures, splitting the dataset, and printing the shape:\n\n[Prompt response]\n\nStep 2: Data Preparation\n\nL\n\nM\n\n4.\n\n3.\n\n4.\n\nWe’ll now proceed with the following steps in preparing the data:\n\nSelecting the relevant columns: We’ll include the columns that are numerical and directly relate to customer\n\nbehavior.\n\nSplitting the data into training and testing sets: This helps us evaluate our model on unseen data to ensure\n\nit performs well generally.\n\nHere’s the code to perform these steps:\n\n# Selecting only the numerical columns for features features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] # Selecting the target column which is 'Yearly Amount Spent' target = data['Yearly Amount Spent'] # Splitting the dataset into training and testing sets # We allocate 80% of the data for training and 20% for testing X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Printing the shapes of the training and testing sets to verify the split print(f'Training features shape: {X_train.shape}') print(f'Testing features shape: {X_test.shape}') print(f'Training labels shape: {y_train.shape}') print(f'Testing labels shape: {y_test.shape}')\n\nThis block of code:\n\nSelects features and target: We separate the columns for the features and the target variable which is what we aim to predict.\n\nSplits the data: Using train_test_split, we divide the data into training and testing sets. The\n\ntest_size=0.2 means 20% of the data is reserved for testing. random_state is set for reproducibility of results.\n\nPrints shapes: We print the shapes of our training and testing sets to confirm the split was successful.\n\nShould we run this part of the code to prepare the data and check the split?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed with code execution:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the shape of features and labels:\n\n[Prompt response]\n\nThe data has been successfully split into training and testing sets. Here’s the breakdown:\n\nTraining features shape: 400 rows and 4 columns\n\nTesting features shape: 100 rows and 4 columns\n\nTraining labels shape: 400 values\n\nTesting labels shape: 100 values\n\nThis indicates that 80% of the data is used for training the model, and 20% is reserved for testing the model’s performance.\n\nStep 3: Building the Linear Regression Model\n\nNow, we’ll create and train the linear regression model using the training data. Here’s the plan:\n\nInitialize the linear regression model: This involves setting up our model ready to be fitted to the data.\n\nFit the model to the training data: This is where the model learns the relationship between the features and the target from the training data.\n\nEvaluate the model on the training data: This step is to get a quick check on how well our model has learned from the training data.\n\nShall I proceed with this next step and show you the code to build and evaluate the model on the training data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed to the next steps:\n\n[Prompt]\n\nYes, Please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code:\n\n[Prompt response]\n\nLet’s move on to building and training the linear regression model.\n\nStep 3: Building the Linear Regression Model\n\nHere’s the code to create, train, and initially evaluate the linear regression model:\n\n# Initialize the Linear Regression model model = LinearRegression() # Fit the model to the training data # This involves finding the best fitting line to the provided data points model.fit(X_train, y_train) # Evaluate the model by predicting on the training set train_predictions = model.predict(X_train) # Calculate the Mean Absolute Error (MAE) on the training set # MAE measures the average magnitude of the errors in a set of predictions, without considering th train_mae = mean_absolute_error(y_train, train_predictions) # Calculate the Mean Squared Error (MSE) on the training set # MSE measures the average of the squares of the errors—that is, the average squared difference be train_mse = mean_squared_error(y_train, train_predictions) # Printing the errors to understand model performance on the training data print(f'Mean Absolute Error on training data: {train_mae}') print(f'Mean Squared Error on training data: {train_mse}')\n\nThis code does the following:\n\nInitializes the model: LinearRegression() is called to create a linear regression model.\n\nFits the model: model.fit(X_train, y_train) adjusts weights on the regression line to fit the input features and the target output.\n\nMakes predictions: model.predict(X_train) is used to see how well the model has learned to predict the\n\ntraining data.\n\nCalculates errors: mean_absolute_error and mean_squared_error provide metrics to gauge the model’s accuracy and performance by comparing the predicted values against the actual values.\n\nWould you like me to run this code now to see how the model performs on the training data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed:\n\n[Prompt]\n\nYes, Please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the\n\nresults:\n\n[Prompt response]\n\nThe linear regression model has been successfully trained on the dataset, and here are the performance metrics on\n\nthe training data:\n\nMean Absolute Error (MAE) on training data: 7.727\n\nMean Squared Error (MSE) on training data: 95.823\n\nThese metrics help us understand the average errors made by the model in predictions:\n\nMAE tells us that, on average, the model’s predictions are approximately 7.73 units away from the actual\n\nvalues.\n\nMSE indicates a slightly larger error in the predictions, considering it squares the errors, emphasizing larger\n\ndiscrepancies.\n\nNext Steps:\n\nNow, the model is trained and preliminarily evaluated on the training set. We should now test its performance on\n\nthe unseen testing set to truly evaluate its effectiveness.\n\nWould you like to proceed with testing the model on the testing set and seeing how it performs with new data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed with testing the model performance on the test set:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the results:\n\n[Prompt response]\n\nThe evaluation of the model on the testing set yielded the following results:\n\nMean Absolute Error (MAE) on testing data: 8.558\n\nMean Squared Error (MSE) on testing data: 109.864\n\nThese results tell us the following:\n\nThe MAE of 8.558 suggests that the model’s predictions on the testing data are, on average, about 8.56 units\n\naway from the actual values. This is slightly worse than the training MAE, which is to be expected as the model is now predicting data it hasn’t seen before.\n\nThe MSE of 109.864 is also higher than the training MSE, indicating that some predictions have larger errors.\n\nSummary:\n\nThe model performs reasonably well but shows a typical behavior of slightly reduced performance on the testing\n\ndata compared to the training data. This is normal and indicates that the model is generalizing as expected without\n\nsignificant overfitting or underfitting.\n\nFinal Steps:\n\nYou now have a functioning linear regression model that can predict the “Yearly Amount Spent” based on customer behavior metrics. Depending on your goals, you might want to explore further:\n\nImproving the model: You could look into more sophisticated modelling techniques or feature engineering.\n\nModel interpretation: Analyse the coefficients of the model to understand the impact of each feature.\n\nWould you like to explore any of these additional steps or need further assistance with something else?\n\n[End of response]\n\nWe have seen that similar to the free version, ChatGPT Plus does a pretty good job in creating a simple linear\n\nregression model step by step, by providing the code as well as executing it for us. Next, we will see whether the\n\nPlus version can apply the regularization techniques.\n\nFeature 2: Applying regularization techniques We will ask ChatGPT Plus to apply regularization techniques:\n\n[Prompt]\n\nCan you please apply Lasso, Ridge, and cross-validation to check whether it leads to any improvement?\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code for Lasso and Ridge, along with cross-validation:\n\n[Prompt response]\n\nStep 5: Applying Lasso and Ridge Regression with Cross-Validation\n\nLasso Regression:\n\nLasso regression not only helps in reducing overfitting but can also be used for feature selection since it can shrink the coefficients of less important features to zero.\n\nRidge Regression:\n\nRidge regression reduces the model complexity by imposing a penalty on the size of coefficients.\n\nWe’ll use the scikit-learn library, which provides convenient functions to perform these analyses.\n\nHere’s the code to apply Lasso and Ridge with cross-validation:\n\nfrom sklearn.linear_model import LassoCV, RidgeCV from sklearn.model_selection import cross_val_score # Lasso Regression with Cross-Validation lasso_cv = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5) # cv=5 means 5-fold cross-validation lasso_cv.fit(X_train, y_train) lasso_cv_score = lasso_cv.score(X_test, y_test) lasso_cv_mae = mean_absolute_error(y_test, lasso_cv.predict(X_test)) # Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5) ridge_cv.fit(X_train, y_train) ridge_cv_score = ridge_cv.score(X_test, y_test) ridge_cv_mae = mean_absolute_error(y_test, ridge_cv.predict(X_test)) # Printing results print(f'Lasso CV Score: {lasso_cv_score}, MAE: {lasso_cv_mae}') print(f'Ridge CV Score: {ridge_cv_score}, MAE: {ridge_cv_mae}')\n\nThis code performs the following:\n\nLassoCV and RidgeCV: These are Lasso and Ridge regression methods with built-in cross-validation of the alpha parameter. Cross-validation scores: The .score() method returns the R2 score that indicates the proportion of variance in the dependent variable that is predictable from the independent variables.\n\nMean Absolute Error: This provides an absolute measure of fit quality on the testing set.\n\nLet’s run this code to see if the regularization and cross-validation can enhance the model’s performance. Shall we proceed?\n\n[End of response]\n\nWe will instruct ChatGPT to execute the code:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\n[Prompt response]\n\nIn our scenario, ChatGPT encountered an internal error and was unable to execute the code. Such errors can occur\n\ndue to various reasons, including unavailability of services at the time of execution, memory issues, or the absence of required libraries or dependencies in its runtime environment. It’s important to note that you might receive a\n\ndifferent response that executes the code flawlessly, as errors like these are sporadic. However, we’ve included this\n\nscenario to troubleshoot and illustrate how to proceed in such cases. Here, we have applied the prompting strategy to get feedback from the AI assistant (LIFE 3.3).\n\n[End of response]\n\nWe will now take the suggestion from ChatGPT and execute the code locally using Jupyter Colab, and since we\n\nare loading the dataset from Google Drive, we will provide the instructions in the prompt:\n\n[Prompt]\n\nOk, give me complete code to run locally and note that the file needs to be mounted from Google Drive from path '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' in the Jupyter notebook.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the complete code:\n\n[Prompt response]\n\nCertainly! Below is the complete code that you can run locally in a Jupyter Notebook. This script includes steps\n\nfor importing the dataset directly from Google Drive, preprocessing the data, fitting both Lasso and Ridge\n\nregression models with cross-validation, and then evaluating them.\n\n# Import necessary libraries import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.preprocessing import StandardScaler # Mount Google Drive (specific to Google Colab) from google.colab import drive drive.mount('/content/drive') # Load the dataset from Google Drive file_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' data = pd.read_csv(file_path) # Select only the numerical columns for features and the target column features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] target = data['Yearly Amount Spent'] # Splitting the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Standardizing the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and fit the Linear Regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Lasso Regression with Cross-Validation lasso_cv = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5) lasso_cv.fit(X_train_scaled, y_train) lasso_cv_score = lasso_cv.score(X_test_scaled, y_test) lasso_cv_mae = mean_absolute_error(y_test, lasso_cv.predict(X_test_scaled)) # Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5) ridge_cv.fit(X_train_scaled, y_train) ridge_cv_score = ridge_cv.score(X_test_scaled, y_test) ridge_cv_mae = mean_absolute_error(y_test, ridge_cv.predict(X_test_scaled)) # Print results print(f'Linear Regression Test MAE: {mean_absolute_error(y_test, model.predict(X_test_scaled))}') print(f'Lasso CV Score: {lasso_cv_score}, MAE: {lasso_cv_mae}') print(f'Ridge CV Score: {ridge_cv_score}, MAE: {ridge_cv_mae}')\n\nInstructions for Running the Code:\n\nGoogle Colab Specifics: The drive.mount() function is specific to Google Colab and allows you to access\n\nfiles on your Google Drive. If you are not using Google Colab, you will need to adjust the file loading method\n\naccording to your development environment.\n\nDependencies: Ensure you have all required libraries installed. You can install them using pip (e.g., pip install pandas scikit-learn google-colab).\n\nData Path: Adjust the file_path if your file is located in a different directory on your Google Drive.\n\nRun this code in your Jupyter Notebook environment to see how the models perform with your data. If you\n\nencounter any errors or need further assistance, feel free to reach out!\n\n[End of response]\n\nWe execute the code and it runs perfectly fine without any modifications or errors. Here, we learned that we can\n\nuse the hybrid approach of running code within ChatGPT Plus or request it to provide the code to execute locally\n\nin case of any issues.\n\nNext, we will move on to Feature 3 of generating a synthetic dataset using ChatGPT Plus.\n\nFeature 3: Generating a synthetic dataset to add complexity Now, let’s try, if we are able, to generate a synthetic dataset using ChatGPT Plus for our dataset. For this purpose,\n\nwe will use the Data Analyst GPT and start a fresh conversation.\n\nNote that specific GPTs can be invoked by providing @<GPT Name> in the message prompt:\n\nFigure 13.10: Invoking the Data Analyst GPT\n\n[Prompt]\n\n@Data Analyst – Provide me the code to generate the synthetic dataset for the above dataset from the perspective of predicting Yearly Amount Spent, along with the code to generate it. <Attach the dataset file here>\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code and printed the first few rows:\n\n[Prompt response]\n\nTo help you generate a synthetic dataset for predicting yearly amount spent, I’ll first need to take a look at the structure and content of your provided dataset. Let’s start by loading and examining the data from your file. I’ll\n\ndisplay the first few rows and summary statistics to understand what kind of features we’re dealing with. Then, I’ll assist you in creating a synthetic dataset based on this structure. Let’s get started with loading the data.\n\nThe dataset consists of 500 entries with the following features:\n\nEmail: The customer’s email address (string).\n\nAddress: The customer’s address (string).\n\nAvatar: The avatar color (string).\n\nAvg. Session Length: Average session length in minutes (numeric).\n\nTime on App: Time spent on the app in minutes (numeric).\n\nTime on Website: Time spent on the website in minutes (numeric).\n\nLength of Membership: Length of membership in years (numeric).\n\nYearly Amount Spent: Yearly amount spent in dollars (numeric).\n\nTo create a synthetic dataset for the purpose of modeling and predicting “Yearly Amount Spent”, we can use a technique called bootstrapping, which involves sampling with replacement from the existing dataset to create a\n\nnew synthetic dataset that has a similar distribution. However, to add some variability and simulate a realistic dataset, we can also introduce random noise into the numeric fields.\n\nHere’s a Python code snippet to generate a synthetic dataset using bootstrapping and adding noise to the numeric\n\ncolumns:\n\nimport numpy as np # Number of samples for the synthetic dataset num_samples = 500 # Bootstrap the original dataset synthetic_data = data.sample(n=num_samples, replace=True) # Add random noise to numeric columns numeric_columns = ['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership' noise_factor = 0.05 # 5% noise for column in numeric_columns: noise = np.random.normal(loc=0.0, scale=noise_factor * synthetic_data[column].std(), size=num_ synthetic_data[column] += noise # Optionally save the synthetic dataset to a new CSV file synthetic_data.to_csv('/mnt/data/Synthetic_Ecommerce_Customers.csv', index=False) synthetic_data.head()\n\nThis code will:\n\nSample 500 entries from the original dataset, allowing replacements (bootstrapping).\n\nAdd Gaussian noise to each numeric column based on 5% of the standard deviation of that column.\n\nOptionally, the new synthetic dataset can be saved to a CSV file.\n\nWould you like to proceed with generating this synthetic dataset? Or do you have any modifications or specific\n\nrequirements to include in the synthesis process?\n\n[End of response]\n\nNext, we will instruct ChatGPT to generate the code and provide us with the file to download the dataset:\n\n[Prompt]\n\nPlease generate the dataset and make it available to download.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and provided the\n\nlink to download the synthetic dataset:\n\n[Prompt response]\n\nThe synthetic dataset has been generated and saved. You can download it using the link below:\n\n[Download Synthetic Ecommerce Customers Dataset](sandbox:/mnt/data/Synthetic_Ecommerce_Customers.c\n\n[End of response]\n\nHere, we learned that ChatGPT Plus can provide the code, execute the code, and also generate CSV files available\n\nto download.\n\nNext, we will request ChatGPT Plus to generate the complete code and evaluate model performance with the added noise in the synthetic dataset. Let’s move to our next feature.\n\nFeature 4: Generating code to develop a model in a single step for a synthetic dataset [Prompt]\n\nCan you please generate the complete code to perform linear regression, apply Lasso and Ridge regularization\n\ntechniques on our synthetic dataset, and evaluate the model performance?\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the complete code:\n\n[Prompt response]\n\nSure, I can provide you with a Python script that performs linear regression, as well as Lasso and Ridge regression,\n\non the synthetic dataset we just created. We will also evaluate the model performance using metrics like the R-\n\nsquared score, mean squared error (MSE), and mean absolute error (MAE).\n\nHere is a complete Python code snippet for the whole process:\n\nimport pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Lasso, Ridge from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # Load the synthetic dataset data = pd.read_csv('/path/to/your/Synthetic_Ecommerce_Customers.csv') # Feature selection: Using numerical columns as features features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] target = data['Yearly Amount Spent'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Initialize the models linear_model = LinearRegression() lasso_model = Lasso(alpha=0.1) # alpha is the regularization strength ridge_model = Ridge(alpha=1.0) # alpha is the regularization strength # Train the models linear_model.fit(X_train, y_train)\n\nlasso_model.fit(X_train, y_train) ridge_model.fit(X_train, y_train) # Predictions linear_predictions = linear_model.predict(X_test) lasso_predictions = lasso_model.predict(X_test) ridge_predictions = ridge_model.predict(X_test) # Evaluations def evaluate_model(predictions, actual): mse = mean_squared_error(actual, predictions) mae = mean_absolute_error(actual, predictions) r2 = r2_score(actual, predictions) return mse, mae, r2 linear_mse, linear_mae, linear_r2 = evaluate_model(linear_predictions, y_test) lasso_mse, lasso_mae, lasso_r2 = evaluate_model(lasso_predictions, y_test) ridge_mse, ridge_mae, ridge_r2 = evaluate_model(ridge_predictions, y_test) # Print out the metrics print(\"Linear Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(linear_mse, linear_mae, li print(\"Lasso Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(lasso_mse, lasso_mae, lasso print(\"Ridge Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(ridge_mse, ridge_mae, ridge\n\nMake sure to replace '/path/to/your/Synthetic_Ecommerce_Customers.csv' with the actual path where your\n\nsynthetic dataset is stored when running the script in your local environment.\n\nThis script will:\n\nLoad your synthetic dataset.\n\nSelect appropriate features and split the data into training and test sets.\n\nInitialize and train Linear, Lasso, and Ridge regression models.\n\nEvaluate and print out the performance metrics for each model.\n\nThis should help you understand the performance differences and impacts of regularization in your predictive models!\n\n[End of response]\n\nWe will instruct ChatGPT Plus to execute the code for us:\n\n[Prompt]\n\nPlease execute the code.\n\n[End of prompt]\n\nChatGPT Plus should have executed the code and we should receive a response similar to the following:\n\n[Prompt response]\n\nThe models have been trained and evaluated. Here are the performance metrics for each:\n\nLinear Regression:\n\nMean Squared Error (MSE): 133.01\n\nMean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nLasso Regression:\n\nMean Squared Error (MSE): 133.04\n\nMean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nRidge Regression:\n\nMean Squared Error (MSE): 133.08\n\nMean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nAll three models show very similar performance metrics, with high R2 scores close to 0.977, indicating that each\n\nmodel explains about 97.7% of the variance in the Yearly Amount Spent. The slight differences in MSE and MAE across models are minimal, suggesting that regularization (both Lasso and Ridge) does not significantly alter the\n\nperformance in this particular case, possibly due to the absence of multicollinearity or other issues that these\n\ntechniques help address.\n\n[End of response]\n\nBy working through this exercise, you’ve not only learned how to perform a regression analysis using the free\n\nversion of ChatGPT and ChatGPT Plus but also how to create a linear regression model step by step using your own dataset, apply regularization techniques, and generate synthetic datasets.\n\nAssignment Generate a single-step code for our dataset and execute it in Google Colab for the random forest algorithm.\n\nChallenge Can you enhance the following prompt by incorporating prompting strategies to ensure the retrieval of a specific\n\ndataset, rather than a random one that may not exist or may have an incorrect path, ensuring it’s fit for the intended use?\n\nProvide the output from the current prompt as well as the improved prompt:\n\n[Prompt]\n\nCan you please provide me with the complete end-to-end Python code for a publicly available advertising dataset along with a detailed explanation that it is compatible with the Jupyter Notebook?\n\n[End of prompt]\n\nSummary We explored the application of TAG, PIC, and LIFE prompting strategies in crafting regression models, employing both ChatGPT and ChatGPT Plus for rapid analysis and predictive tasks. This approach is particularly valuable in\n\nthe early stages of machine learning development, offering immediate insights and the flexibility to experiment\n\nwith different models or algorithms without the burden of managing execution environments or programming instances. Additionally, we learned how to effectively utilize single prompts for generating comprehensive code.\n\nWhile it’s possible to craft prompts for discrete tasks or steps, many of these require only succinct lines of code\n\nand were not the focus here. Providing feedback is instrumental in this process, and validating the output is crucial to ensure the code’s functionality.\n\nIn the next chapter, we will learn how to use ChatGPT to generate the code for the multilayer perceptron (MLP)\n\nmodel with the help of the Fashion-MNIST dataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and other readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n14\n\nBuilding an MLP Model for Fashion-MNIST with ChatGPT\n\nIntroduction Building upon our foundational understanding of predictive modeling, we\n\nnow dive into the dynamic world of Multilayer Perceptron (MLP)\n\nmodels. In this chapter, we embark on a journey to construct an MLP model\n\nfrom scratch, leveraging the versatility and power of neural networks for\n\npredictive analytics.\n\nOur exploration of MLPs represents a significant leap into the realm of\n\ncomplex modeling techniques. While linear regression provided valuable\n\ninsights into modeling relationships within data, MLPs offer a rich\n\nframework for capturing intricate patterns and nonlinear dependencies,\n\nmaking them well suited for a wide range of predictive tasks.\n\nThrough hands-on experimentation and iterative refinement, we will\n\nunravel the intricacies of MLP architecture and optimization. From\n\ndesigning the initial network structure to fine-tuning hyperparameters and incorporating advanced techniques such as batch normalization and\n\ndropout, we aim to equip you with the knowledge and skills to harness the\n\nfull potential of neural networks in predictive modeling.\n\nAs we navigate through the construction and optimization of our MLP model, we will delve into the underlying principles of neural network\n\ndynamics, exploring how different architectural choices and optimization\n\nstrategies influence model performance and generalization capabilities.\n\nBusiness problem A fashion e-commerce store seeks to optimize customer engagement and\n\nincrease revenue by leveraging machine learning techniques to gain deeper\n\ninsights into customer behavior and preferences. By analyzing image data representing various fashion items purchased by customers, the store aims\n\nto tailor its product recommendations, improve customer satisfaction, and\n\nenhance the overall shopping experience.\n\nProblem and data domain In this chapter, we will employ MLP models to understand the relationship\n\nbetween customers’ preferences and their purchasing patterns using the Fashion-MNIST dataset. MLP models offer a powerful framework for\n\nimage classification tasks, allowing us to predict the type of clothing or accessory a customer is likely to purchase based on their interactions with the online store. By uncovering patterns in customer preferences, the e-\n\ncommerce store can personalize recommendations and optimize inventory management to meet the diverse needs of its customer base.\n\nDataset overview",
      "page_number": 415
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 437-459)",
      "start_page": 437,
      "end_page": 459,
      "detection_method": "synthetic",
      "content": "The fashion e-commerce store collects image data representing various\n\nfashion items, categorized into different classes, from its customers. The Fashion-MNIST dataset comprises 70,000 grayscale images of clothing and\n\naccessories, each associated with a specific label indicating its category and of size 28x28.\n\nFeatures in the dataset include:\n\nImage data: Grayscale images of fashion items, each represented as a matrix of pixel intensities. These images serve as the input data for training the MLP model.\n\nLabel: The category label assigned to each image, representing the type of clothing or accessory depicted. Labels range from 0 to 9,\n\ncorresponding to classes such as T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot.\n\nBy analyzing this image data and its corresponding labels, we aim to train\n\nan MLP model capable of accurately classifying fashion items based on\n\ntheir visual features. This predictive model will enable the e-commerce store to make personalized product recommendations, enhance customer\n\nengagement, and ultimately increase revenue by providing a seamless shopping experience tailored to individual preferences.\n\nFigure 14.1: Fashion-MNIST dataset\n\nBreaking the problem down into features Given the nature of the Fashion-MNIST dataset, which comprises grayscale\n\nimages of fashion items categorized into different classes, we will start by\n\nbuilding a baseline MLP model. This will involve the following high-level\n\nsteps:\n\n1. Building the baseline model: Users will understand the process of constructing a simple MLP model for image classification using ChatGPT. We will guide users through loading the Fashion-MNIST\n\ndataset, preprocessing the image data, splitting it into training and testing sets, defining the model architecture, training the model,\n\nmaking predictions, and evaluating its performance.\n\n2. Adding layers to the model: Once the baseline model is established, users will learn how to experiment with adding additional layers to the\n\nMLP architecture. We will explore how increasing the depth or width of the model impacts its performance and capacity to capture complex\n\npatterns in the image data.\n\n3. Experimenting with batch sizes: Users will experiment with different batch sizes during model training to observe their effects on training speed, convergence, and generalization performance. We will explore\n\nhow varying batch sizes influence the trade-off between computation\n\nefficiency and model stability.\n\n4. Adjusting the number of neurons: Users will explore the impact of\n\nadjusting the number of neurons in each layer of the MLP model. By\n\nincreasing or decreasing the number of neurons, users can observe changes in model capacity and its ability to learn intricate features\n\nfrom the image data.\n\n5. Trying different optimizers: Finally, users will experiment with\n\ndifferent optimization algorithms, such as SGD, Adam, and RMSprop, to optimize the training process of the MLP model. We will observe\n\nhow different optimizers influence training dynamics, convergence\n\nspeed, and final model performance.\n\nBy following these steps, users will gain a comprehensive understanding of building and optimizing MLP models for image classification tasks using\n\nthe Fashion-MNIST dataset. They will learn how to iteratively refine the\n\nmodel architecture and training process to achieve optimal performance and accuracy in classifying fashion items.\n\nPrompting strategy To leverage ChatGPT for machine learning we need to have a clear\n\nunderstanding of how to implement the prompting strategies specifically for\n\ncode generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better understanding of what needs to go into prompts.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 - Task: The specific task or goal is to create a classification model for\n\nthe Fashion-MNIST dataset.\n\n1.2 - Actions: The key steps involved in creating a classification model\n\nusing an MLP for the Fashion-MNIST dataset are:\n\nData preprocessing: Normalize pixel values, flatten images into vectors, and encode categorical labels.\n\nData splitting: Partition the dataset into training, validation, and testing\n\nsets.\n\nModel selection: Opt for an MLP as the classification model.\n\nModel training: Train the MLP on the training data.\n\nModel evaluation: Use metrics like accuracy, precision, recall, and\n\nconfusion matrix to evaluate the model’s performance.\n\n1.3 - Guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the text block of the notebook, in detail for each method used in the\n\ncode before providing the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 - Persona: We will adopt the persona of a beginner who needs to learn\n\ndifferent steps of model creation, hence the code should be generated step\n\nby step.\n\n2.2 - Instructions: We have specified that we want the code generated for\n\nan MLP model with a single layer and have instructed ChatGPT to provide\n\none step at a time and wait for the user’s feedback.\n\n2.3 - Context: In this case, ChatGPT is already aware of the Fashion-\n\nMNIST dataset, as it is widely known, so we do not need to provide additional context.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy 3.1 - Learn:\n\nWe want to learn about MLP models and how they work.\n\n3.2 - Improvise:\n\nWe will use this knowledge later when adding more layers, changing\n\nthe batch size or activation functions, and so on.\n\n3.3 - Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4- Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity.\n\nThis is used throughout the chapter.\n\nBuilding an MLP model to accurately classify the Fashion-MNIST images using the free version of ChatGPT ChatGPT’s premium version has a code interpreter, but it doesn’t support\n\nthe deep learning libraries such as Keras and TensorFlow required to\n\nexecute the code. Hence, for this chapter, we will stick to the free version of\n\nChatGPT.\n\nFeature 1: Building the baseline model Let’s craft our initial prompt for baseline model.\n\n[Prompt]\n\nI want to create a simple classification model for the Fashion-MNIST\n\ndataset (TAG 1.1) consisting of a single-layer MLP model (PIC 2.2). It\n\nshould consist of the following steps (TAG1.2), provide one step at a time, and wait for the user’s feedback (PIC 2.2).\n\nData Preprocessing: Normalize pixel values, flatten images into\n\nvectors, and encode categorical labels.\n\nData Splitting: Partition the dataset into training, validation, and\n\ntesting sets.\n\nModel Selection: Opt for a Multi-Layer Perceptron (MLP) as the\n\nclassification model.\n\nModel Training: Train the MLP on the training data.\n\nModel Evaluation: Use metrics like accuracy, precision, recall, and\n\nconfusion matrix to evaluate the model’s performance.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and\n\nstrictly follow the below instructions (TAG 1.3):\n\nThe code should be compatible with the Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the\n\ntext block of notebook, in detail for each method used in the code before\n\nproviding the code.\n\n[End of prompt]\n\nYou should receive a response similar to the following, where ChatGPT has\n\nprovided us with the code for step 1, Data Preprocessing, along with\n\ndetailed comments and explanations.\n\n[Prompt response]\n\nSure, let’s start with the first step: Data Preprocessing.\n\nData Preprocessing:\n\nData preprocessing is a crucial step in machine learning, where we\n\nprepare our data for training by applying various transformations and\n\nnormalization techniques. In this step, we’ll normalize the pixel values, flatten the images into vectors, and encode categorical labels.\n\nNormalize Pixel Values:\n\nNormalizing pixel values ensures that each pixel’s value is scaled to a\n\nrange between 0 and 1. This is important for the neural network to\n\nconverge faster during training and to prevent any particular feature\n\nfrom dominating the learning process.\n\nFlatten Images into Vectors:\n\nMLP models cannot directly work with multi-dimensional data like\n\nimages. Hence, we need to flatten the images into 1D vectors, where\n\neach pixel value becomes a feature.\n\nEncode Categorical Labels:\n\nSince our labels are categorical (e.g., different types of clothing items),\n\nwe need to encode them into numerical values so that our model can\n\nunderstand and learn from them.\n\nNow, let’s implement these steps in code:\n\nimport numpy as np from tensorflow.keras.datasets import fashion_mnist from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.model_selection import train_test_split # Step 1: Load Fashion-MNIST dataset (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data(\n\n# Step 2: Normalize pixel values def normalize_images(train_data, test_data): # Convert to float32 to ensure division results in float train_data = train_data.astype('float32') test_data = test_data.astype('float32') # Normalize pixel values to the range 0-1 train_data /= 255.0 test_data /= 255.0 return train_data, test_data # Step 3: Flatten images into vectors def flatten_images(train_data, test_data): # Reshape images to vectors train_data = train_data.reshape(train_data.shape[0], -1) test_data = test_data.reshape(test_data.shape[0], -1) return train_data, test_data # Step 4: Encode categorical labels def encode_labels(train_labels, test_labels): # Initialize LabelEncoder label_encoder = LabelEncoder() # Fit LabelEncoder on training labels and transform both tr train_labels = label_encoder.fit_transform(train_labels) test_labels = label_encoder.transform(test_labels) return train_labels, test_labels # Apply data preprocessing steps x_train, x_test = normalize_images(x_train, x_test) x_train, x_test = flatten_images(x_train, x_test) y_train, y_test = encode_labels(y_train, y_test) # Print the shapes of preprocessed data print(\"x_train shape:\", x_train.shape) print(\"y_train shape:\", y_train.shape) print(\"x_test shape:\", x_test.shape) print(\"y_test shape:\", y_test.shape)\n\nExplanation:\n\nWe import necessary libraries such as NumPy for numerical\n\noperations, TensorFlow for loading the Fashion-MNIST dataset, and\n\nscikit-learn for data preprocessing.\n\nWe load the Fashion-MNIST dataset using the fashion_mnist.load_data() method and split it into training and\n\ntesting sets.\n\nThree functions, normalize_images, flatten_images, and encode_labels, are defined to perform each data preprocessing step.\n\nInside each function:\n\nnormalize_images divides pixel values by 255 to scale them between 0 and 1.\n\nflatten_images reshapes the images into 1D vectors.\n\nencode_labels converts categorical labels into numerical values using LabelEncoder.\n\nFinally, we apply these preprocessing steps to the training and testing data\n\nand print their shapes to ensure everything is processed correctly.\n\n[End of response]\n\nWe will now apply the prompt evaluation strategy (LIFE 3.4) and validate\n\nthe code by executing the code for every step in the Google Colab\n\nenvironment. The code loads the model from Keras and normalizes the\n\nimages, flattens them into vectors, encodes labels using LabelEncoder, then applies the preprocessing and prints the shape. The code runs perfectly fine\n\nwithout applying any modifications or any errors.\n\nPlease refer to the Learning MLP using Fashion-MNIST dataset with ChatGPT.ipynb notebook for the executable code snippets or copy the code\n\nfrom the prompt response.\n\nFigure 14.2: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for step 2 – Data Splitting.\n\n[Prompt response]\n\nGreat! Let’s move on to the next step:\n\nData Splitting:\n\nIn this step, we’ll partition the dataset into training, validation, and\n\ntesting sets. The training set will be used to train the model, the\n\nvalidation set will be used to tune hyperparameters and prevent\n\noverfitting, and the testing set will be used to evaluate the model’s\n\nperformance on unseen data.\n\nTraining Set:\n\nThe training set is used to train the model’s parameters (weights and\n\nbiases) using optimization algorithms like gradient descent.\n\nValidation Set:\n\nThe validation set is used to evaluate the model’s performance during\n\ntraining and adjust hyperparameters such as learning rate, batch size,\n\nand number of epochs to optimize the model’s performance.\n\nTesting Set:\n\nThe testing set is used to evaluate the final performance of the trained\n\nmodel on unseen data and assess its generalization capability.\n\nNow, let’s split the dataset:\n\n# Step 5: Split dataset into training, validation, and testing def split_dataset(train_data, train_labels, test_data, test_lab # Split training set into training and validation sets x_train, x_val, y_train, y_val = train_test_split(train_dat # Split testing set into testing and validation sets x_test, x_val, y_test, y_val = train_test_split(test_data, return x_train, y_train, x_val, y_val, x_test, y_test # Apply data splitting x_train, y_train, x_val, y_val, x_test, y_test = split_dataset( # Print the shapes of split datasets print(\"Training set:\")\n\nprint(\"x_train shape:\", x_train.shape) print(\"y_train shape:\", y_train.shape) print(\"\\nValidation set:\") print(\"x_val shape:\", x_val.shape) print(\"y_val shape:\", y_val.shape) print(\"\\nTesting set:\") print(\"x_test shape:\", x_test.shape) print(\"y_test shape:\", y_test.shape)\n\nExplanation:\n\nWe define a split_dataset function to split the dataset into training,\n\nvalidation, and testing sets using the train_test_split function from scikit-learn.\n\nThe function takes training data, training labels, testing data, testing\n\nlabels, and optional parameters such as validation size, test size, and\n\nrandom state.\n\nWe split the training set into training and validation sets using the\n\nspecified validation size.\n\nThen, we split the testing set into testing and validation sets using the\n\nspecified test size.\n\nFinally, we print the shapes of the split datasets to verify the split.\n\n[End of response]\n\nWe execute the code in the notebook and it runs successfully. The data is\n\nsplit into training, validation, and test sets with the specified sizes.\n\nFigure 14.3: Code execution – data splitting\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step and define a single-layer baseline MLP\n\nmodel.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for a single-layer MLP model with an input layer, one\n\nhidden layer, and an output layer.\n\n[Prompt response]\n\nSure, let’s move on to the next step:\n\nModel Selection:\n\nIn this step, we’ll choose a Multi-Layer Perceptron (MLP) as the\n\nclassification model. An MLP consists of one or more layers of\n\nneurons, each fully connected to the next layer. For simplicity, we’ll\n\nstart with a single-layer MLP as a baseline model.\n\nSingle-Layer MLP:\n\nA single-layer MLP consists of an input layer, a hidden layer, and an\n\noutput layer. However, since it has only one hidden layer, it’s often\n\nreferred to as a single-layer perceptron. Each neuron in the hidden layer receives input from all neurons in the input layer and produces an\n\noutput, which is then passed through an activation function to\n\nintroduce non-linearity. Finally, the output layer produces the final\n\nprediction.\n\nNow, let’s define the baseline single-layer MLP model:\n\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Step 6: Define a single-layer MLP model def create_single_layer_mlp(input_shape, num_classes): model = Sequential() # Add a single hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Define input shape and number of classes input_shape = x_train.shape[1:] num_classes = len(np.unique(y_train)) # Create the single-layer MLP model model = create_single_layer_mlp(input_shape, num_classes) # Print model summary model.summary()\n\nExplanation:\n\nWe import the necessary modules from TensorFlow.keras to define\n\nthe MLP model.\n\nWe define a function create_single_layer_mlp to create a single-layer MLP model.\n\nInside the function:\n\nWe initialize a sequential model.\n\nWe add a single hidden layer with 128 neurons and ReLU activation function.\n\nWe add an output layer with softmax activation function for\n\nmulti-class classification.\n\nWe specify the input shape based on the shape of the training\n\ndata and the number of output classes.\n\nWe create the single-layer MLP model using the defined\n\nfunction.\n\nFinally, we print the summary of the model architecture.\n\n[End of response]\n\nWe execute the code and it runs without any errors and prints the model\n\nsummary with the layer type, output shape, and number of parameters. It\n\nalso provides us with the total parameters, trainable and non-trainable parameters. In our case, all the parameters are trainable parameters.\n\nFigure 14.4: Code execution – baseline model architecture\n\nWe will now instruct ChatGPT to proceed to the next step for model\n\ntraining.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for model training along with explanations of the key\n\nconcepts involved, such as loss functions and optimization algorithms along\n\nwith comments.\n\n[Prompt response]\n\nSure, let’s move on to the next step:\n\nModel Training:\n\nIn this step, we’ll train the MLP model on the training data. Training\n\ninvolves feeding the training data to the model, adjusting its\n\nparameters (weights and biases) using optimization algorithms like\n\nstochastic gradient descent, and minimizing a loss function to make\n\npredictions as close to the actual labels as possible.\n\nLoss Function:\n\nThe loss function measures the difference between the predicted labels\n\nand the actual labels. For multi-class classification problems like\n\nFashion-MNIST, we typically use categorical cross-entropy as the loss\n\nfunction.\n\nOptimization Algorithm:\n\nThe optimization algorithm is responsible for updating the model’s\n\nparameters during training to minimize the loss function. We’ll use\n\nAdam optimizer, which is an extension to stochastic gradient descent\n\nand is widely used in deep learning.\n\nTraining Parameters:\n\nWe’ll specify training parameters such as batch size and number of\n\nepochs. Batch size determines the number of samples processed before\n\nupdating the model’s parameters, while the number of epochs specifies\n\nhow many times the entire training dataset is passed forward and\n\nbackward through the model.\n\nNow, let’s train the model:\n\nfrom tensorflow.keras.utils import to_categorical # Step 7: Model Training def train_model(model, x_train, y_train, x_val, y_val, batch_si # Convert class vectors to binary class matrices (one-hot e y_train = to_categorical(y_train) y_val = to_categorical(y_val) # Compile the model with categorical cross-entropy loss and model.compile(loss='categorical_crossentropy', optimizer='a # Train the model on the training data history = model.fit(x_train, y_train, batch_size=batch_size return history # Train the model history = train_model(model, x_train, y_train, x_val, y_val, ba\n\nExplanation:\n\nWe import the necessary modules from TensorFlow.keras to compile\n\nand train the model.\n\nWe define a train_model function to train the model on the training\n\ndata.\n\nInside the function:\n\nWe convert the class labels to binary class matrices using one-\n\nhot encoding.\n\nWe compile the model with the categorical cross-entropy loss\n\nfunction and the Adam optimizer.\n\nWe train the model on the training data with the specified batch\n\nsize and number of epochs.\n\nFinally, we train the model using the defined function and store the\n\ntraining history for visualization and analysis.\n\n[End of response]\n\nWe execute the code and it runs without any errors. It applies a one-hot\n\nencoding to class labels, complies the model with the categorical cross-\n\nentropy loss function and Adam optimizer, and trains the model with batch\n\nsize 32 for 10 epochs. This achieves a validation accuracy of approximately\n\n88%.\n\nFigure 14.5: Code execution – model training\n\nWe will now instruct ChatGPT to proceed to the next step for model\n\nevaluation.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for evaluating the model against the test set, along with\n\nexplanations of concepts like accuracy, precision, recall, and the confusion\n\nmatrix.\n\n[Prompt response]\n\nModel Evaluation:\n\nIn this step, we’ll use various metrics to evaluate the performance of\n\nthe trained MLP model on the testing data. Commonly used metrics for\n\nclassification tasks include accuracy, precision, recall, and the\n\nconfusion matrix.\n\nAccuracy:\n\nAccuracy measures the proportion of correctly classified instances out\n\nof the total instances.\n\nPrecision:\n\nPrecision measures the proportion of true positive predictions out of all\n\npositive predictions. It indicates the model’s ability to avoid false\n\npositives.\n\nRecall:\n\nRecall measures the proportion of true positive predictions out of all\n\nactual positive instances. It indicates the model’s ability to identify all\n\npositive instances.\n\nConfusion Matrix:\n\nA confusion matrix provides a summary of the model’s performance\n\nby showing the number of true positive, true negative, false positive,\n\nand false negative predictions.\n\nNow, let’s evaluate the model:\n\nimport matplotlib.pyplot as plt from sklearn.metrics import accuracy_score, precision_score, re # Step 8: Model Evaluation def evaluate_model(model, x_test, y_test): # Predict labels for testing data y_pred = model.predict(x_test) # Convert predicted labels to class labels y_pred_classes = np.argmax(y_pred, axis=1) # Convert true labels to class labels y_true_classes = np.argmax(y_test, axis=1) # Calculate accuracy accuracy = accuracy_score(y_true_classes, y_pred_classes) # Calculate precision precision = precision_score(y_true_classes, y_pred_classes, # Calculate recall recall = recall_score(y_true_classes, y_pred_classes, avera # Calculate confusion matrix cm = confusion_matrix(y_true_classes, y_pred_classes) return accuracy, precision, recall, cm # Evaluate the model accuracy, precision, recall, cm = evaluate_model(model, x_test, # Print evaluation metrics print(\"Accuracy:\", accuracy) print(\"Precision:\", precision) print(\"Recall:\", recall) print(\"Confusion Matrix:\\n\", cm) # Visualize confusion matrix plt.imshow(cm, cmap=plt.cm.Blues) plt.title('Confusion Matrix') plt.colorbar() plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.show()\n\nExplanation:\n\nWe import necessary modules from matplotlib.pyplot and scikit-\n\nlearn to visualize the confusion matrix and calculate evaluation\n\nmetrics.\n\nWe define an evaluate_model function to evaluate the model’s\n\nperformance on the testing data.\n\nInside the function:\n\nWe make predictions on the testing data using the trained model.\n\nWe convert the predicted labels and true labels to class labels.\n\nWe calculate accuracy, precision, recall, and confusion matrix\n\nusing scikit-learn metrics functions.\n\nFinally, we evaluate the model using the defined function, print the\n\nevaluation metrics, and visualize the confusion matrix.\n\n[End of response]\n\nWe execute the code and note that it experiences an error due to the test\n\nlabels not being encoded, but this is not evident from the error itself. We\n\ncan get help from ChatGPT to resolve the error by applying the prompting\n\nstrategy of providing the error to ChatGPT.\n\nFigure 14.6: Code execution – error for model evaluation\n\nWe provided the error to ChatGPT to get assistance.\n\n[Prompt]",
      "page_number": 437
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 460-481)",
      "start_page": 460,
      "end_page": 481,
      "detection_method": "synthetic",
      "content": "Looks like there is some issue with the test labels. We received the below\n\nerror:\n\n------282/282 [==============================] - 2s 5ms/step --\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the suggestions to troubleshoot the problem.\n\n[Prompt response]\n\nIt seems like there’s an issue with the shape of the labels. The error\n\nindicates that the axis specified for the argmax function is out of bounds.\n\nThis typically occurs when the labels are not in the expected format or\n\nshape.\n\nLet’s double-check the shape of the labels and make sure they are in the\n\ncorrect format for evaluation:\n\nprint(\"Shape of y_test:\", y_test.shape)\n\nThis will help us understand the shape of the labels and identify any\n\ndiscrepancies. Once we identify the issue, we can correct it accordingly.\n\n[End of response]\n\nWe print the shape of y_test:\n\nFigure 14.7: Code execution - shape of test labels\n\nWe will now provide ChatGPT with the output of the code.\n\n[Prompt]\n\nShape of y_test: (9000,)\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the problem and provides us with the corrected code.\n\n[Prompt response]\n\nThank you for providing the shape of y_test. It seems that the labels are in\n\n1D format, which means they are not one-hot encoded. In the evaluation\n\nfunction, we assumed that the labels are one-hot encoded, which caused the\n\nerror.\n\nTo resolve this issue, we need to modify the evaluation function to handle\n\n1D labels instead of one-hot encoded labels. We can achieve this by\n\nremoving the conversion of labels to one-hot encoding and directly\n\ncompare the predicted labels with the true labels.\n\nLet’s update the evaluation function accordingly:\n\n# Step 8: Model Evaluation def evaluate_model(model, x_test, y_test): # Predict labels for testing data\n\ny_pred = model.predict(x_test) # Convert predicted labels to class labels y_pred_classes = np.argmax(y_pred, axis=1) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred_classes) # Calculate precision precision = precision_score(y_test, y_pred_classes, average # Calculate recall recall = recall_score(y_test, y_pred_classes, average='weig # Calculate confusion matrix cm = confusion_matrix(y_test, y_pred_classes) return accuracy, precision, recall, cm # Evaluate the model accuracy, precision, recall, cm = evaluate_model(model, x_test, # Print evaluation metrics print(\"Accuracy:\", accuracy) print(\"Precision:\", precision) print(\"Recall:\", recall) print(\"Confusion Matrix:\\n\", cm) # Visualize confusion matrix plt.imshow(cm, cmap=plt.cm.Blues) plt.title('Confusion Matrix') plt.colorbar() plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.show()\n\nExplanation:\n\nWe removed the conversion of labels to one-hot encoding and directly used the y_test true labels in the evaluation function.\n\nWe updated the accuracy, precision, and recall calculation functions to use\n\nthe 1D labels instead of one-hot encoded labels.\n\nNow, the evaluation function should work correctly with the provided 1D\n\nlabels, and the evaluation metrics should be calculated accurately.\n\n[End of response]\n\nWe will execute the code and it works fine now because the code provided\n\nhas been modified to work with 1D labels. It also prints the confusion matrix.\n\nFigure 14.8: Code execution – confusion matrix\n\nThis completes feature 1. We achieved an accuracy of 0.8796 (87.96%), a\n\nprecision of 0.8817 (88.17%), and a recall of 0.8796 (87.96%) with our\n\nsingle-layer MLP model. Next, we will move to the second feature, adding\n\nmore layers to the model. We will also learn how ChatGPT can assist us in\n\nthat.\n\nFeature 2: Adding layers to the model Now that we have a baseline model architecture, let us try adding layers to\n\nit. We will apply a prompting strategy to improvise a model and ask ChatGPT to provide us with the code for it.\n\n[Prompt]\n\nNow that we have a baseline model with a single layer, can you please\n\nprovide me code with two layers and three layers and calculate and\n\ncompare the model performance due to the addition of layers?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for two-layer and\n\nthree-layer models along with comparison results.\n\n[Prompt response]\n\nCertainly! Let’s define and train multi-layer perceptron (MLP) models with\n\ntwo and three layers, and then compare their performance with the baseline\n\nsingle-layer model.\n\nDefine MLP Models with Two and Three Layers:\n\nWe’ll define MLP models with additional hidden layers to see how the\n\nperformance changes with the increase in the number of layers.\n\nTwo-Layer MLP:\n\nThe two-layer MLP will consist of an input layer, two hidden layers,\n\nand an output layer.\n\nThree-Layer MLP:\n\nThe three-layer MLP will consist of an input layer, three hidden layers,\n\nand an output layer.\n\nNow, let’s define and train these models:\n\n# Define a two-layer MLP model def create_two_layer_mlp(input_shape, num_classes): model = Sequential() # Add first hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add second hidden layer with ReLU activation model.add(Dense(64, activation='relu')) # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Define a three-layer MLP model def create_three_layer_mlp(input_shape, num_classes): model = Sequential() # Add first hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add second hidden layer with ReLU activation model.add(Dense(64, activation='relu')) # Add third hidden layer with ReLU activation model.add(Dense(32, activation='relu')) # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Create two-layer MLP model two_layer_model = create_two_layer_mlp(input_shape, num_classes # Create three-layer MLP model\n\nthree_layer_model = create_three_layer_mlp(input_shape, num_cla # Train two-layer MLP model two_layer_history = train_model(two_layer_model, x_train, y_tra # Train three-layer MLP model three_layer_history = train_model(three_layer_model, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy, precision, recall, and confusion matrix.\n\n# Evaluate two-layer MLP model two_layer_accuracy, _, _, _ = evaluate_model(two_layer_model, x # Evaluate three-layer MLP model three_layer_accuracy, _, _, _ = evaluate_model(three_layer_mode # Print model comparison print(\"Single-layer MLP Accuracy:\", accuracy) print(\"Two-layer MLP Accuracy:\", two_layer_accuracy) print(\"Three-layer MLP Accuracy:\", three_layer_accuracy)\n\nExplanation:\n\nWe define two additional MLP models with two and three hidden\n\nlayers using the previously defined functions.\n\nWe train both models using the training data and validation data.\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how the addition of layers affects the\n\nmodel’s performance.\n\n[End of response]\n\nWe will execute the code that defines the two additional models with two\n\nand three hidden layers. It also trains the models, evaluates the performance\n\nof both models, and prints the results.\n\nFigure 14.9: Feature 2 – adding layers to the model\n\nBased on these results, we can observe that the two-layer MLP model\n\nachieved slightly higher accuracy compared to both the single-layer and\n\nthree-layer MLP models. However, the differences in accuracy are\n\nrelatively small, indicating that the addition of layers did not significantly\n\nimprove the model’s performance in this case. Let’s move on to the next\n\nfeature and experiment with batch sizes using the baseline model.\n\nFeature 3: Experimenting with batch sizes We have now learned how to add more layers, and in our case, due to the\n\ndataset being simple, there was not much impact. We will now experiment\n\nwith different batch sizes. We will apply a prompting strategy to improvise\n\na model and ask ChatGPT to provide us the code for it.\n\n[Prompt]\n\nCan you please provide me code with three combinations of batch sizes and\n\ncompare the baseline single-layer model performance for these\n\ncombinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different batch\n\nsizes along with comparison results.\n\n[Prompt response]\n\nDefine and Train Models with Different Batch Sizes:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent combinations of batch sizes.\n\nCombination 1: Batch Size = 32\n\nCombination 2: Batch Size = 64\n\nCombination 3: Batch Size = 128\n\nNow, let’s define and train these models:\n\n# Define and train model with batch size = 32 model_batch_32 = create_single_layer_mlp(input_shape, num_class history_batch_32 = train_model(model_batch_32, x_train, y_train # Define and train model with batch size = 64 model_batch_64 = create_single_layer_mlp(input_shape, num_class history_batch_64 = train_model(model_batch_64, x_train, y_train # Define and train model with batch size = 128 model_batch_128 = create_single_layer_mlp(input_shape, num_clas history_batch_128 = train_model(model_batch_128, x_train, y_tra\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.\n\n# Evaluate models with different batch sizes accuracy_batch_32, _, _, _ = evaluate_model(model_batch_32, x_t accuracy_batch_64, _, _, _ = evaluate_model(model_batch_64, x_t accuracy_batch_128, _, _, _ = evaluate_model(model_batch_128, x # Print model comparison print(\"Single-layer MLP Accuracy (Batch Size = 32):\", accuracy_ print(\"Single-layer MLP Accuracy (Batch Size = 64):\", accuracy_ print(\"Single-layer MLP Accuracy (Batch Size = 128):\", accuracy\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent combinations of batch sizes (32, 64, and 128).\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different batch sizes affect the\n\nmodel’s performance.\n\n[End of response]\n\nWe will execute the code that defines models with batch sizes 32, 64, and\n\n128. It also trains the models, evaluates their performance, and prints the\n\nresults of the comparison of this performance.\n\nFigure 14.10: Code execution – batch size comparison\n\nWe can see that there is not much difference between the models’\n\naccuracies, so we will apply the ‘Learn prompting strategy’ (LIFE 3.1) and\n\nask ChatGPT for possible reasons as to why this might be.\n\n[Prompt]\n\n282/282 [==============================] - 1s 3ms/step 282/282 Single-layer MLP Accuracy (Batch Size = 32): 0.88 Single-layer MLP Accuracy (Batch Size = 64): 0.8723333333333333 Single-layer MLP Accuracy (Batch Size = 128): 0.879777777777777\n\nWhy there is not much difference in the accuracies?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT explains\n\nthe possible reasons.\n\n[Prompt response]\n\nThe differences in accuracy between the models trained with different batch\n\nsizes are relatively small. There could be several reasons for this:\n\nDataset Size: The Fashion-MNIST dataset is relatively small\n\ncompared to other datasets commonly used in deep learning. With a\n\nsmaller dataset, the impact of batch size on model performance may be\n\nless pronounced.\n\nModel Complexity: The baseline single-layer MLP model may not be\n\ncomplex enough to fully leverage the benefits of different batch sizes.\n\nIn more complex models, such as deep neural networks with multiple layers, the impact of batch size on training dynamics may be more\n\nsignificant.\n\nOptimization Algorithm: The Adam optimizer used in training the models adapts the learning rate for each parameter based on the first\n\nand second moments of the gradients. This adaptive learning rate can\n\nmitigate the impact of batch size to some extent.\n\nConvergence: The models may have already converged to a similar level of performance regardless of the batch size used. Once a model\n\nconverges, further training with different batch sizes may not lead to\n\nsubstantial improvements in accuracy.\n\nOverall, while batch size can influence training dynamics and convergence\n\nspeed, its impact on model performance may be less pronounced in certain\n\nscenarios, especially with relatively simple models and datasets.\n\nExperimenting with larger and more complex models, as well as different\n\noptimization algorithms, may reveal more significant differences in\n\nperformance based on batch size.\n\n[End of response]\n\nNow that we have learned how to change the batch size and what the\n\npossible reasons could be for it not having much impact on our model, let’s\n\nmove on to the next feature and experiment with the number of neurons in\n\nthe hidden layer of the baseline model.\n\nFeature 4: Experimenting with the number of neurons We will apply a prompting strategy to improvise a model and ask ChatGPT\n\nto provide us the code for experimenting with different numbers of neurons\n\nin the hidden layer.\n\n[Prompt]\n\nCan you please provide me code with three combinations of numbers of\n\nneurons and compare the baseline single-layer model performance for the\n\nvarious combinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different numbers\n\nof neurons in the hidden layer, along with the comparison results.\n\n[Prompt response]\n\nOf course! Let’s define and train the baseline single-layer MLP model with\n\nthree different combinations of the number of neurons in the hidden layer,\n\nand then compare their performance.\n\nDefine and Train Models with Different Number of Neurons:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent combinations of the number of neurons in the hidden layer:\n\nCombination 1: Number of Neurons = 64\n\nCombination 2: Number of Neurons = 128\n\nCombination 3: Number of Neurons = 256\n\nNow, let’s define and train these models:\n\n# Define and train model with 64 neurons in the hidden layer model_neurons_64 = create_single_layer_mlp(input_shape, num_cla history_neurons_64 = train_model(model_neurons_64, x_train, y_t # Define and train model with 128 neurons in the hidden layer model_neurons_128 = create_single_layer_mlp(input_shape, num_cl history_neurons_128 = train_model(model_neurons_128, x_train, y # Define and train model with 256 neurons in the hidden layer model_neurons_256 = create_single_layer_mlp(input_shape, num_cl history_neurons_256 = train_model(model_neurons_256, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.\n\n# Evaluate models with different number of neurons accuracy_neurons_64, _, _, _ = evaluate_model(model_neurons_64, accuracy_neurons_128, _, _, _ = evaluate_model(model_neurons_12 accuracy_neurons_256, _, _, _ = evaluate_model(model_neurons_25 # Print model comparison print(\"Single-layer MLP Accuracy (Neurons = 64):\", accuracy_neu print(\"Single-layer MLP Accuracy (Neurons = 128):\", accuracy_ne print(\"Single-layer MLP Accuracy (Neurons = 256):\", accuracy_ne\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent combinations of the number of neurons in the hidden layer\n\n(64, 128, and 256).\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different numbers of neurons in\n\nthe hidden layer affect the model’s performance.\n\n[End of response]\n\nWe will execute the code that defines the models with 64, 128, and 256\n\nneurons. It also trains the model, evaluates the performance for all the\n\nmodel variations, and prints the results of the comparison of their\n\nperformance.\n\nFigure 14.11: Code execution – neurons comparison\n\nWe can see that there is not much difference, so let’s move to our final\n\nfeature of experimenting with different optimizers.\n\nFeature 5: Trying diﬀerent optimizers We will apply a prompting strategy to improvise a model and ask ChatGPT\n\nto provide us code for experimenting with different optimizers such as\n\nSGD, Adam, and RMSProp. Also, as part of our prompting strategy, we will\n\nbe specific and explicitly mention that the optimizer should be passed at\n\ncompile time and that labels should be one-hot encoded, as these steps are\n\ncommonly ignored by ChatGPT.\n\n[Prompt]\n\nCan you please provide me code with different optimizers of SGD, Adam,\n\nand RMSprop and compare the baseline single-layer model performance for\n\nvarious combinations? The optimizer should be passed in at compile time\n\nand you should ensure that labels are one-hot encoded.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different\n\noptimizers along with comparison results.\n\n[Prompt response]\n\nCertainly! Let’s define and train a baseline single-layer MLP model with\n\nthree different optimizers (SGD, Adam, and RMSprop), and then compare\n\ntheir performance.\n\nDefine and Train Models with Different Optimizers:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent optimizers: SGD, Adam, and RMSprop.\n\nNow, let’s define and train these models:\n\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop # Define and train model with SGD optimizer def train_model_sgd(model, x_train, y_train, x_val, y_val, batc # One-hot encode the class labels y_train_encoded = to_categorical(y_train)\n\ny_val_encoded = to_categorical(y_val) # Compile the model with SGD optimizer model.compile(loss='categorical_crossentropy', optimizer=SG # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Define and train model with Adam optimizer def train_model_adam(model, x_train, y_train, x_val, y_val, bat # One-hot encode the class labels y_train_encoded = to_categorical(y_train) y_val_encoded = to_categorical(y_val) # Compile the model with Adam optimizer model.compile(loss='categorical_crossentropy', optimizer=Ad # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Define and train model with RMSprop optimizer def train_model_rmsprop(model, x_train, y_train, x_val, y_val, # One-hot encode the class labels y_train_encoded = to_categorical(y_train) y_val_encoded = to_categorical(y_val) # Compile the model with RMSprop optimizer model.compile(loss='categorical_crossentropy', optimizer=RM # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Train model with SGD optimizer model_sgd = create_single_layer_mlp(input_shape, num_classes) history_sgd = train_model_sgd(model_sgd, x_train, y_train, x_va # Train model with Adam optimizer model_adam = create_single_layer_mlp(input_shape, num_classes) history_adam = train_model_adam(model_adam, x_train, y_train, x # Train model with RMSprop optimizer model_rmsprop = create_single_layer_mlp(input_shape, num_classe history_rmsprop = train_model_rmsprop(model_rmsprop, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.\n\n# Evaluate models with different optimizers accuracy_sgd, _, _, _ = evaluate_model(model_sgd, x_test, y_tes accuracy_adam, _, _, _ = evaluate_model(model_adam, x_test, y_t accuracy_rmsprop, _, _, _ = evaluate_model(model_rmsprop, x_tes # Print model comparison print(\"Single-layer MLP Accuracy (SGD):\", accuracy_sgd) print(\"Single-layer MLP Accuracy (Adam):\", accuracy_adam) print(\"Single-layer MLP Accuracy (RMSprop):\", accuracy_rmsprop)\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent optimizers (SGD, Adam, and RMSprop).\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different optimizers affect the\n\nmodel’s performance.\n\n[End of response]\n\nWe execute the code that defines the models with different optimizers. It\n\nalso trains the model, evaluates the performance of all the model variations,\n\nand prints the results for a comparison of their performance.\n\nFigure 14.12: Code Execution – Different Optimizers\n\nBased on these results, we can observe that the Adam optimizer achieved\n\nthe highest accuracy, followed by RMSprop and SGD. This indicates that\n\nAdam performed better in optimizing the model’s parameters and\n\nimproving its performance on the testing data.\n\nAssignment Compare the model performance of a single layer versus two hidden layers,\n\nbut with 20 and 50 epochs.\n\nChallenge With the help of ChatGPT, improvise a model by adding a dropout layer\n\nand analyze the impact on model performance. Feel free to add more hidden\n\nlayers to the model.\n\nSummary\n\nThe prompt strategies used in this chapter provided a structured approach to\n\nlearning and building a classification model using an MLP where ChatGPT\n\nassisted in generating code. The user validated the code using a Colab\n\nnotebook and provided feedback to ChatGPT. By actively engaging with\n\nthe material, you experimented with different techniques and iteratively refined your understanding, ultimately leading to a more comprehensive\n\ngrasp of classification model creation using MLPs.\n\nIn the next chapter, we will learn how to use ChatGPT to generate code for\n\nConvolutional Neural Networks (CNNs) with the help of the CIFAR-10\n\ndataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n15\n\nBuilding a CNN Model for CIFAR-10 with ChatGPT\n\nIntroduction Having explored the depths of the Multi-Layer Perceptron (MLP) in our\n\nprevious chapter with the Fashion-MNIST dataset, we now pivot to a more\n\nintricate and visually complex challenge. This chapter marks our transition\n\nfrom the primarily tabular, grayscale world of Fashion-MNIST to the\n\ncolorful and diverse realm of the CIFAR-10 dataset. Here, we elevate our\n\nfocus to Convolutional Neural Networks (CNNs), a class of deep neural\n\nnetworks that are revolutionizing the way we approach image classification\n\ntasks.\n\nOur journey through the MLP chapter provided a strong foundation for\n\nunderstanding the basics of neural networks and their application in\n\nclassifying simpler, grayscale images. Now, we step into a more advanced territory where CNNs reign supreme. The CIFAR-10 dataset, with its array\n\nof 32x32 color images across 10 different classes, presents a unique set of challenges that MLPs are not best suited to address. This is where CNNs, with their ability to capture spatial and textural patterns in images, come into play.\n\nAs we transition from MLPs to CNNs, we carry forward the insights and\n\nknowledge gained, applying them to a more complex dataset that closely\n\nmimics real-world scenarios. The CIFAR-10 dataset not only tests the limits\n\nof image classification models but also serves as an excellent platform for us\n\nto explore the advanced capabilities of CNNs.\n\nThis chapter aims to build upon what we learned about neural networks and\n\nguide you through the nuances of CNNs. We will delve into why CNNs are\n\nthe preferred choice for image data, how they differ from MLPs in handling\n\ncolor and texture, and what makes them so effective in classifying images from the CIFAR-10 dataset. Prepare to embark on a journey that takes you\n\nfrom the fundamentals to the more sophisticated aspects of CNNs.\n\nBusiness problem The CIFAR-10 dataset presents a business challenge to companies seeking to\n\nenhance image recognition capabilities for various objects and optimize\n\ndecision-making processes based on visual data. A multitude of industries, such as e-commerce, autonomous driving, and surveillance, can benefit from\n\naccurate object classification and detection. By harnessing machine learning algorithms, businesses aim to improve efficiency, enhance their user\n\nexperience, and streamline operations.\n\nProblem and data domain In this context, we will utilize CNNs to tackle the object recognition task using the CIFAR-10 dataset. CNNs are particularly effective for image-\n\nrelated problems due to their ability to automatically learn hierarchical features from raw pixel data. By training a CNN model on the CIFAR-10",
      "page_number": 460
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 482-501)",
      "start_page": 482,
      "end_page": 501,
      "detection_method": "synthetic",
      "content": "dataset, we aim to develop a robust system capable of accurately classifying objects into one of the ten predefined categories. This model can be applied\n\nin various domains, such as image-based search engines, automated surveillance systems, and quality control in manufacturing.\n\nDataset overview The CIFAR-10 dataset comprises 60,000 color images, divided into 10\n\nclasses, with 6,000 images per class. Each image has dimensions of 32x32 pixels and is represented in RGB format. The dataset is split into a training\n\nset of 50,000 images and a test set of 10,000 images.\n\nFeatures in the dataset include:\n\nImage data: Color images of various objects, each represented as a 3- dimensional array containing pixel intensities for red, green, and blue\n\nchannels. These images serve as input data for training the CNN model.\n\nLabel: The class label assigned to each image, representing the category of the depicted object. The labels range from 0 to 9,\n\ncorresponding to classes such as airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n\nBy analyzing the CIFAR-10 dataset and its associated labels, our goal is to\n\ntrain a CNN model capable of accurately identifying objects depicted in images. This predictive model can then be deployed in real-world\n\napplications to automate object recognition tasks, improve decision-making\n\nprocesses, and enhance overall efficiency in diverse industries.\n\nFigure 15.1: CIFAR-10 dataset\n\nBreaking the problem down into features Given the CIFAR-10 dataset and the application of CNNs for image\n\nrecognition, we outline the following features to guide users through\n\nbuilding and optimizing CNN models:\n\nBuilding the baseline CNN model with a single convolutional layer:\n\nUsers will start by constructing a simple CNN model with a single\n\nconvolutional layer for image classification. This feature focuses on defining the basic architecture, including convolutional filters,\n\nactivation functions, and pooling layers, to establish a foundational\n\nunderstanding of CNNs.\n\nExperimenting with the addition of convolutional layers: Users will\n\nexplore the impact of adding additional convolutional layers to the baseline model architecture. By incrementally increasing the depth of\n\nthe network, users can observe how the model’s capacity to capture\n\nhierarchical features evolves and its ability to learn complex patterns\n\nimproves.\n\nIncorporating dropout regularization: Users will learn how to integrate dropout regularization into the CNN model to mitigate\n\noverfitting and improve generalization performance. By randomly\n\ndropping units during training, dropout helps prevent the network from\n\nrelying too heavily on specific features and encourages robust feature\n\nlearning.\n\nImplementing batch normalization: Users will explore the benefits of\n\nbatch normalization in stabilizing training dynamics and accelerating\n\nconvergence. This feature focuses on incorporating batch normalization\n\nlayers into the CNN architecture to normalize activations and reduce internal covariate shift, leading to faster and more stable training.\n\nOptimizing with different optimizers: This feature explores the\n\neffects of using various optimization algorithms, including SGD, Adam,\n\nand RMSprop, to train the CNN model. Users will compare the training\n\ndynamics, convergence speed, and final model performance achieved with different optimizers, allowing them to select the most suitable\n\noptimization strategy for their specific task.\n\nPerforming data augmentation: Users will experiment with data\n\naugmentation techniques such as rotation, flipping, zooming, and\n\nshifting to increase the diversity and size of the training dataset. By generating augmented samples on the fly during training, users can\n\nimprove the model’s ability to generalize to unseen data and enhance robustness against variations in input images.\n\nBy following these features, users will gain practical insights into building,\n\nfine-tuning, and optimizing CNN models for image classification tasks using\n\nthe CIFAR-10 dataset. They will learn how to systematically experiment\n\nwith different architectural components, regularization techniques, and optimization strategies to achieve superior performance and accuracy in\n\nobject recognition.\n\nPrompting strategy To leverage ChatGPT for machine learning, we need to have a clear\n\nunderstanding of how to implement prompting strategies specifically for\n\ncode generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better\n\nunderstanding of what needs to go into the prompts.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 - Task: The specific task or goal is to build and optimize a CNN model\n\nfor the CIFAR-10 dataset.\n\n1.2 - Actions: The key steps involved in building and optimizing a CNN\n\nmodel for the CIFAR-10 dataset include:\n\nPreprocessing the image data: Normalize the pixel values and resize\n\nthe images to a standardized size.\n\nModel construction: Define the baseline CNN model architecture with\n\na single convolutional layer.\n\n1.3 - Guidelines: We will provide the following guidelines to ChatGPT in\n\nour prompt:\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method\n\nused in the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 - Persona: Adopt the persona of a beginner who needs step-by-step\n\nguidance on building and optimizing CNN models for image classification tasks.\n\n2.2 - Instructions: Request ChatGPT to generate code for each feature one step at a time and wait for user feedback before proceeding to the next step.\n\n2.3 - Context: Given that the focus is on building CNN models for image classification tasks using the CIFAR-10 dataset, ChatGPT is already aware\n\nof the dataset and its characteristics, so additional context may not be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE)\n\nprompt strategy 3.1 - Learn:\n\nEmphasize the importance of learning about CNN models and their\n\ncomponents, including convolutional layers, pooling layers, dropout regularization, batch normalization, data augmentation, and\n\noptimization algorithms.\n\n3.2 - Improvise:\n\nWe will improvise later by adding more layers, dropout layers, pooling, data augmentation, and so on.\n\n3.3 - Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4- Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity. This is used throughout the chapter.\n\nBuilding a CNN model to accurately classify the CIFAR- 10 images using the free version of ChatGPT\n\nThe premium version of ChatGPT has a code interpreter, but it doesn’t\n\nsupport deep learning libraries such as Keras and TensorFlow, which are required to build ML code. Hence, for this chapter, we will stick to the free\n\nversion of ChatGPT.\n\nFeature 1: Building the baseline CNN model with a single convolutional layer Let’s craft our initial prompt for the baseline model.\n\n[Prompt]\n\nI want to create a simple classification model for CIFAR-10 dataset (TAG\n\n1.1) that consists of a single-layer CNN model (PIC 2.2). It should consist of following steps (TAG 1.2) and provide one step at a time, waiting for user\n\nfeedback (PIC 2.2):\n\nPreprocessing the image data: Normalize the pixel values and resize the images to a standardized size.\n\nModel construction: Define the baseline CNN model architecture with a single convolutional layer.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and strictly follow the below instructions (TAG 1.3):\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method used in the code.\n\n[End of prompt]\n\nYou should receive a response similar to the following, where ChatGPT has provided us with the code for the data preprocessing step along with detailed\n\ncomments and explanations.\n\n[Prompt response]\n\nSure, let’s start with the first step: preprocessing the image data. In this step,\n\nwe will normalize the pixel values and resize the images to a standardized\n\nsize.\n\n# Step 1: Preprocessing the image data # Import necessary libraries import numpy as np import tensorflow as tf from tensorflow.keras.datasets import cifar10 from tensorflow.keras.utils import to_categorical from skimage.transform import resize # Load CIFAR-10 dataset (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Normalize pixel values to range [0, 1] x_train = x_train.astype('float32') / 255.0 x_test = x_test.astype('float32') / 255.0 # Resize images to a standardized size (e.g., 32x32 pixels) standardized_size = (32, 32) x_train_resized = np.array([resize(image, standardized_size) for x_test_resized = np.array([resize(image, standardized_size) for # Display shape of datasets after preprocessing print(\"Shape of x_train:\", x_train_resized.shape) print(\"Shape of x_test:\", x_test_resized.shape)\n\nNow, let’s break down each line of code:\n\nWe import the necessary libraries:\n\nnumpy for numerical operations.\n\ntensorflow for building and training the model.\n\ncifar10 dataset from tensorflow.keras.datasets for loading CIFAR-10\n\ndata.\n\nto_categorical function from tensorflow.keras.utils for one-hot encoding labels.\n\nresize function from skimage.transform for resizing images.\n\nWe load the CIFAR-10 dataset using cifar10.load_data() function, which returns training and testing data splits.\n\nWe normalize pixel values of images to the range [0, 1] by dividing\n\nby 255.0.\n\nWe define the standardized size for resizing images (e.g., 32x32 pixels).\n\nWe resize all images in the training and testing sets to the standardized\n\nsize using list comprehension and the resize function.\n\nFinally, we print the shapes of the training and testing datasets after\n\npreprocessing to verify the changes.\n\nLet me know if you have any questions or if you’d like to proceed to the\n\nnext step!\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in Google Colab for every step. The code loads the model from Keras and normalizes the images. The code runs\n\nperfectly fine without requiring any modifications or returning any errors.\n\nPlease refer to the Building a CNN Model for CIFAR-10 with ChatGPT.ipynb notebook for the executable code snippets or copy the code\n\nfrom the prompt response.\n\nFigure 15.2: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for creating a baseline model.\n\n[Prompt response]\n\nGreat! The next step is to construct the baseline CNN model architecture with a single convolutional layer.\n\n# Step 2: Model Construction # Import necessary libraries from tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatte # Define the baseline CNN model architecture model = Sequential([ # Add a convolutional layer with 32 filters, kernel size of Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 # Add a max pooling layer with pool size of (2, 2) MaxPooling2D((2, 2)), # Flatten the output of the convolutional layer Flatten(), # Add a fully connected layer with 64 neurons and ReLU activ Dense(64, activation='relu'), # Add the output layer with 10 neurons (one for each class) Dense(10, activation='softmax') ]) # Compile the model model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Display model summary model.summary()\n\nNow, let’s explain each line of code:\n\nWe import the necessary libraries:\n\nSequential from tensorflow.keras.models to create a sequential model.\n\nConv2D, MaxPooling2D, Flatten, and Dense layers from\n\ntensorflow.keras.layers to define the different types of layers in the CNN model.\n\nWe define the baseline CNN model architecture using the Sequential API, which allows us to create a linear stack of layers.\n\nWithin the Sequential model:\n\nWe add a convolutional layer (Conv2D) with 32 filters, a kernel size of\n\n(3, 3), and ReLU activation function. The input_shape parameter defines the shape of input images.\n\nWe add a max pooling layer (MaxPooling2D) with a pool size of (2, 2)\n\nto downsample the feature maps.\n\nWe flatten the output of the convolutional layer using the Flatten layer to prepare it for the fully connected layers.\n\nWe add a fully connected layer (Dense) with 64 neurons and a ReLU\n\nactivation function.\n\nFinally, we add the output layer with 10 neurons (one for each class in CIFAR-10) and the softmax activation function.\n\nWe compile the model using model.compile() with the following:\n\noptimizer='adam' to use the Adam optimizer.\n\nloss='sparse_categorical_crossentropy' as the loss function, suitable\n\nfor classification tasks with integer labels.\n\nmetrics=['accuracy'] to monitor the accuracy during training.\n\nWe display the summary of the model architecture using model.summary().\n\nLet me know if you have any questions or if you’d like to make any adjustments!\n\n[End of response]\n\nWe execute the code in the notebook and it runs perfectly. A model summary is printed, which includes the Conv2d layer with 32 filters and 896 params, followed by a MaxPooling2D layer, a flatten layer, which doesn’t have any\n\nparameters, then a dense layer with 64 neurons and the ReLU activation function, and finally the dense layer with 10 neurons and the softmax activation function.\n\nFigure 15.3: Code execution – baseline model summary\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for training the model.\n\n[Prompt response]\n\nSure! The next step is to train the model on the preprocessed CIFAR-10 dataset.\n\n# Step 3: Model Training # Train the model history = model.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resized, y_test))\n\nHere’s an explanation of the code:\n\nWe train the model using the fit method on the preprocessed training data (x_train_resized) and corresponding labels (y_train).\n\nWe specify the number of training epochs (epochs=10) to train the\n\nmodel for 10 iterations over the entire dataset.\n\nWe set the batch size (batch_size=32) to specify the number of samples per gradient update. It is a hyperparameter that determines how many samples are processed before updating the model’s parameters.\n\nWe provide the validation data (validation_data=(x_test_resized,\n\ny_test)) to evaluate the model’s performance on the testing data after each epoch.\n\nThe fit method returns a history object containing training metrics\n\n(e.g., loss and accuracy) for each epoch, which can be used for analysis and visualization.\n\nLet me know if you have any questions or if you’d like to proceed with\n\nevaluating the model or making any adjustments!\n\n[End of response]\n\nWe execute the code and it runs without any errors. The model is trained for\n\n10 epochs, after which we receive a validation accuracy of 62.6%, which is\n\nnot bad for a basic model.\n\nFigure 15.4: Code execution – model training\n\nWe will now instruct ChatGPT to proceed to the next step to evaluate the model performance.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should provide the code for model evaluation along with comments.\n\n[Prompt response]\n\nSure! The next step is to evaluate the trained model on the testing dataset and visualize its performance.\n\n# Step 4: Model Evaluation and Visualization import matplotlib.pyplot as plt # Evaluate the model on the testing dataset test_loss, test_accuracy = model.evaluate(x_test_resized, y_test print(\"Test Loss:\", test_loss) print(\"Test Accuracy:\", test_accuracy)\n\n# Plot training history plt.plot(history.history['accuracy'], label='Training Accuracy') plt.plot(history.history['val_accuracy'], label='Validation Accu plt.title('Training and Validation Accuracy') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() plt.show()\n\nHere’s what this code does:\n\nWe import matplotlib.pyplot to visualize the training history.\n\nWe evaluate the trained model on the testing dataset using the evaluate method, which returns the test loss and accuracy.\n\nWe print the test loss and test accuracy to assess the model’s\n\nperformance on unseen data.\n\nWe plot the training and validation accuracy over epochs using matplotlib.pyplot. The history object returned by the fit method\n\ncontains accuracy values for both training and validation datasets across epochs.\n\nThe history.history['accuracy'] contains training accuracy values.\n\nThe history.history['val_accuracy'] contains validation accuracy\n\nvalues.\n\nThe plot helps us visualize how well the model is learning over time and whether it is overfitting or underfitting.\n\nLet me know if you have any questions or if you’d like to analyze the results further!\n\n[End of response]\n\nWe execute the code and it runs without any errors. It evaluates the test loss\n\nand test accuracy (62.5%) and also plots the graph of training accuracy vs.\n\nvalidation accuracy, showing that model is not performing well on the validation set.\n\nFigure 15.5: Code execution – model evaluation\n\nThis completes feature 1, where we achieved an accuracy of 0.6259\n\n(62.59%) with our baseline CNN model. (Please note that you may receive different accuracy scores even after running the exact same code.) Next, we\n\nwill move to the second feature of adding more layers to the model and learn\n\nhow ChatGPT can assist us in that.\n\nFeature 2: Experimenting with the addition of convolutional layers Now that we have a baseline model architecture, let us try adding layers to it.\n\nWe will apply the prompting strategy to improvise the model (LIFE 3.4) and ask ChatGPT to provide us the code for it, also note the importance of providing specific instructions by using the phrase (on top of our baseline\n\nmodel) (PIC 2.2) and keyword incrementally (PIC 2.2); without these, the response wouldn’t be fully consistent with our expectations.\n\n[Prompt]\n\nWe received an accuracy of 62.59 with the baseline model, so now I would\n\nlike to experiment with the addition of two consecutive layers of 64 filters first and then two consecutive layers of 128 filters on top of our baseline model. Then, let’s compare the result of these two incremental changes with our baseline model.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for a modified model with consecutive layers of 64 filters and another one with consecutive layers of 128 filters.\n\n[Prompt response]\n\nSure! Let’s first add two consecutive layers of 64 filters to the baseline model and train it again. After that, we’ll add two consecutive layers of 128 filters and train the model once more. Finally, we’ll compare the results of\n\nthese two changes with the baseline model.\n\nHere’s how we can modify the baseline model to include additional layers:\n\n# Define the modified model with two consecutive layers of 64 fi modified_model_64 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_64.compile(optimizer='adam', loss='sparse_categorical_crossentropy' metrics=['accuracy']) # Display model summary modified_model_64.summary() # Train the modified model with two consecutive layers of 64 fil history_64 = modified_model_64.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resi # Evaluate the modified model on the testing dataset test_loss_64, test_accuracy_64 = modified_model_64.evaluate(x_te print(\"Test Loss (Modified Model with 64 Filters):\", test_loss_6 print(\"Test Accuracy (Modified Model with 64 Filters):\", test_ac\n\nNow, let’s explain the modifications:\n\nWe define the modified model (modified_model_64) by adding two\n\nconsecutive convolutional layers with 64 filters each after the baseline model’s architecture.\n\nWe compile the modified model with the same settings as the baseline\n\nmodel.\n\nWe train the modified model (modified_model_64) on the preprocessed training data (x_train_resized).\n\nWe evaluate the performance of the modified model on the testing dataset (x_test_resized) and print the test loss and accuracy.\n\nNext, we’ll repeat the process for adding two consecutive layers of 128\n\nfilters:\n\n# Define the modified model with two consecutive layers of 128 f modified_model_128 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_128.compile(optimizer='adam', loss='sparse_categorical_crossentropy metrics=['accuracy']) # Display model summary modified_model_128.summary() # Train the modified model with two consecutive layers of 128 fi history_128 = modified_model_128.fit(x_train_resized, y_train,",
      "page_number": 482
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 502-519)",
      "start_page": 502,
      "end_page": 519,
      "detection_method": "synthetic",
      "content": "epochs=10, batch_size=32, validation_data=(x_test_res # Evaluate the modified model on the testing dataset test_loss_128, test_accuracy_128 = modified_model_128.evaluate(x print(\"Test Loss (Modified Model with 128 Filters):\", test_loss_ print(\"Test Accuracy (Modified Model with 128 Filters):\", test_a\n\nThis code follows the same structure as the previous modification but adds two consecutive convolutional layers with 128 filters each. We train and evaluate the model accordingly.\n\nLet me know if you have any questions or if you’d like to proceed with the\n\nmodifications!\n\n[End of response]\n\nWe first execute the code that defines the two additional layers with 64 filters. It runs perfectly fine, training the model and achieving an improved accuracy of 71%.\n\nFigure 15.6: Adding two consecutive layers with 64 filters to the model\n\nNext, we execute the code to add further depth to the model by adding two\n\nmore layers with 128 filters. In this case, the code results in an error, so we will use a prompting strategy and provide the error back to ChatGPT.\n\nYou should see a ValueError from running the code.\n\n[Prompt]\n\nWe received the below error. Also, ensure that maxpooling doesn’t reduce\n\nthe dimension too much for 128 filters. Error is below:\n\nValueError: Exception encountered when calling layer \"conv2d_38\" Negative dimension size caused by subtracting 3 from 2 for '{{no\n\nCall arguments received by layer \"conv2d_38\" (type Conv2D): inputs=tf.Tensor(shape=(None, 2, 2, 64), dtype=float32).\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the modified code to resolve the issue.\n\n[Prompt response]\n\nIt seems we’re encountering an issue where the dimensions after the last max\n\npooling layer are too small for the subsequent convolutional layer with 128 filters. To address this, we’ll adjust the architecture to ensure compatibility between layers.\n\nHere’s the corrected code:\n\n# Define the modified model with two consecutive layers of 128 f modified_model_64_to_128 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), # A Conv2D(128, (3, 3), activation='relu', padding='same'), # A MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_64_to_128.compile(optimizer='adam', loss='sparse_categorical_cross metrics=['accuracy']) # Display model summary modified_model_64_to_128.summary()\n\n# Train the modified model with two consecutive layers of 128 fi history_64_to_128 = modified_model_64_to_128.fit(x_train_resized epochs=10, batch_size=32, validation_data # Evaluate the modified model on the testing dataset test_loss_64_to_128, test_accuracy_64_to_128 = modified_model_64 print(\"Test Loss (Modified Model with 64 to 128 Filters):\", test print(\"Test Accuracy (Modified Model with 64 to 128 Filters):\",\n\nIn this modification:\n\nWe’ve removed the max pooling layer after the last convolutional layer with 64 filters to ensure that the feature map dimensions are not reduced too much before applying the subsequent convolutional layers with 128 filters.\n\nWe’ve added same padding to the convolutional layers with 128 filters to ensure compatibility with the input dimensions.\n\nThe rest of the architecture remains the same.\n\nThis adjustment should resolve the issue, and the model should compile and\n\ntrain properly. Let me know if you have any questions or if you encounter any further issues!\n\n[End of response]\n\nLet’s execute the code, which removes some max pooling layers and also has padding = 'same' added to ensure the dimensions are compatible. It\n\nruns successfully, training the model but reducing the accuracy to 69%.\n\nFigure 15.7: Added layers with 128 filters\n\nBased on these results, we can observe that the addition of more layers\n\nachieved a slightly higher accuracy compared to the baseline model. However, the differences in accuracy are relatively small, indicating that the\n\naddition of layers did not significantly improve the model’s performance and\n\nthe model is also still not regularized. Let’s move on to the next feature and experiment with dropout regularization.\n\nFeature 3: Incorporating dropout regularization\n\nNow that we have learned how to add more layers, we will now experiment\n\nwith a dropout layer. We will apply the prompting strategy to improvise the\n\nmodel (LIFE 3.4) and ask ChatGPT to provide us the code for it.\n\nAlso note that we have applied the prompting strategy of using specific instructions (PIC 2.2) by asking it to add the dropout over the previously provided code.\n\n[Prompt]\n\nThe model accuracy of the baseline model was 62.5%, with additional layers of 64 filters it was 71%, and the further addition of layers with 128 filters reduced it to 69%. Can we please apply dropout regularization to the model code given by you in your previous message?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for the addition of a dropout layer.\n\n[Prompt response]\n\nSure, let’s add dropout regularization to the modified model with two consecutive layers of 128 filters. Dropout regularization helps prevent overfitting by randomly dropping a fraction of the units (neurons) during training. Here’s how we can modify the model to include dropout layers:\n\nfrom tensorflow.keras.layers import Dropout # Define the modified model with dropout regularization modified_model_64_to_128_with_dropout = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'),\n\nConv2D(128, (3, 3), activation='relu', padding='same'), Dropout(0.25), # Add dropout regularization with a dropout Conv2D(128, (3, 3), activation='relu', padding='same'), Dropout(0.25), # Add dropout regularization with a dropout MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dropout(0.5), # Add dropout regularization with a dropout r Dense(10, activation='softmax') ]) # Compile the modified model with dropout regularization modified_model_64_to_128_with_dropout.compile(optimizer='adam', loss='sparse_categ metrics=['accuracy # Display model summary modified_model_64_to_128_with_dropout.summary() # Train the modified model with dropout regularization history_64_to_128_with_dropout = modified_model_64_to_128_with_d\n\n# Evaluate the modified model with dropout regularization on the test_loss_64_to_128_with_dropout, test_accuracy_64_to_128_with_d print(\"Test Loss (Modified Model with Dropout):\", test_loss_64_t print(\"Test Accuracy (Modified Model with Dropout):\", test_accur\n\nIn this modification:\n\nWe add dropout layers after the convolutional layers with 128 filters. We set the dropout rates to 25% and 50% for the first and last dropout layers, respectively. These values can be adjusted based on experimentation and the level of regularization needed.\n\nWe compile and train the modified model with dropout regularization\n\nusing the same settings as before.\n\nWe evaluate the performance of the modified model with dropout regularization on the testing dataset and print the test loss and accuracy.\n\nAdding dropout regularization may help improve the generalization\n\nperformance of the model and reduce overfitting. Let me know if you have any questions or if you’d like to explore further!\n\n[End of response]\n\nWe execute the code, which defines a model with a dropout layer. It also\n\ntrains and evaluates the performance of the model and prints the model summary covering loss and accuracy. Note that the dropout layer does not add any parameters to the model.\n\nFigure 15.8: Code execution – dropout layer\n\nWe can see that there is not much difference between the accuracies, so we\n\nwill apply the Learn prompting strategy (LIFE 3.1) and ask ChatGPT to provide the plot of the loss and accuracy to see the impact of regularization.\n\n[Prompt]\n\nTest Loss (Modified Model with Dropout): 0.876133382320404 Test Accuracy (Modified Model with Dropout): 0.7014999985694885\n\nCan you please provide the plots for loss and accuracy to see if there has\n\nbeen any regularization and also explain it?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides the code and explains the significance of the plots.\n\n[Prompt response]\n\nCertainly! Let’s plot the training and validation loss and accuracy curves for the modified model with dropout regularization. This will help us visualize\n\nhow dropout regularization affects the training process and the model’s performance.\n\nimport matplotlib.pyplot as plt # Plot training history for loss plt.plot(history_64_to_128_with_dropout.history['loss'], label=' plt.plot(history_64_to_128_with_dropout.history['val_loss'], lab plt.title('Training and Validation Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.show() # Plot training history for accuracy plt.plot(history_64_to_128_with_dropout.history['accuracy'], lab plt.plot(history_64_to_128_with_dropout.history['val_accuracy'],\n\nplt.title('Training and Validation Accuracy') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() plt.show()\n\nExplanation:\n\nTraining Loss vs. Validation Loss: This plot shows the trend of training and validation loss over epochs. If there is a large gap between training and validation loss, it indicates overfitting. Dropout\n\nregularization aims to reduce overfitting by preventing the model from becoming overly reliant on specific features, thereby reducing the gap between training and validation loss.\n\nTraining Accuracy vs. Validation Accuracy: This plot shows the trend of training and validation accuracy over epochs. Dropout regularization\n\ncan also help improve generalization performance, leading to better validation accuracy, especially if overfitting was present in the model.\n\nBy observing these plots, we can assess whether dropout regularization\n\neffectively mitigated overfitting and improved the model’s generalization performance. Let me know if you have any questions or if you need further\n\nexplanation!\n\n[End of response]\n\nWe will execute the code and plot the graphs to understand the impact of dropout.\n\nFigure 15.9: Effect of the dropout layer\n\nThe output graphs show the training and validation loss and accuracy for a\n\nmodel trained on the CIFAR-10 dataset with a dropout layer. Both losses decrease over epochs while the accuracy improves, indicating effective\n\nlearning. The small gap between training and validation metrics suggests minimal overfitting, likely aided by the dropout layer. Overall, the model generalizes well to new data, demonstrating good performance.\n\nFeature 4: Implementing batch normalization We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for adding batch normalization to the previously provided code.\n\n[Prompt]\n\nI want to apply the batch normalization on the previously provided code.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for batch normalization on our existing model.\n\n[Prompt response]\n\nSure! We can apply batch normalization to the convolutional layers of the\n\nmodel to improve training stability and convergence speed. Here’s how we can modify the model to include batch normalization layers:\n\nfrom tensorflow.keras.layers import BatchNormalization # Define the modified model with batch normalization modified_model_64_to_128_with_batchnorm = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3\n\nBatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), BatchNormalization(), # Add batch normalization Dense(10, activation='softmax') ]) # Compile the modified model with batch normalization modified_model_64_to_128_with_batchnorm.compile(optimizer='adam' loss='sparse_cat metrics=['accura # Display model summary modified_model_64_to_128_with_batchnorm.summary() # Train the modified model with batch normalization history_64_to_128_with_batchnorm = modified_model_64_to_128_with\n\n# Evaluate the modified model with batch normalization on the te test_loss_64_to_128_with_batchnorm, test_accuracy_64_to_128_with print(\"Test Loss (Modified Model with Batch Normalization):\", te print(\"Test Accuracy (Modified Model with Batch Normalization):\"\n\nIn this modification:\n\nWe add BatchNormalization layers after each convolutional layer.\n\nBatch normalization normalizes the activations of the previous layer at each batch, making training more stable and accelerating convergence.\n\nWe compile and train the modified model with batch normalization using the same settings as before.\n\nWe evaluate the performance of the modified model with batch normalization on the testing dataset and print the test loss and accuracy.\n\nAdding batch normalization can improve the performance and stability of\n\nthe model during training. Let me know if you have any questions or if\n\nyou’d like further explanation!\n\n[End of response]\n\nWe will execute the code, evaluate the model’s performance, and print the results. Note that batch normalization adds a small number of parameters, unlike dropout or maxpooling.\n\nFigure 15.10: Batch normalization\n\nWe can see that there are some non-trainable parameters. Let us make use of\n\nthe Learn prompting strategy (LIFE 3.1) to understand the calculations of non-trainable parameters.\n\n[Prompt]\n\nCan you please explain the calculation of 832 in the model summary?\n\n\"sequential_1\" ________________________________________________________________ Layer (type) Output Shape Param # ================================================================ conv2d_5 (Conv2D) (None, 30, 30, 32) 896\n\nbatch_normalization (Batch (None, 30, 30, 32) 128 Normalization)\n\nmax_pooling2d_3 (MaxPoolin (None, 15, 15, 32) 0 g2D)\n\nconv2d_6 (Conv2D) (None, 13, 13, 64) 18496\n\nbatch_normalization_1 (Bat (None, 13, 13, 64) 256 chNormalization)\n\nmax_pooling2d_4 (MaxPoolin (None, 6, 6, 64) 0 g2D)\n\nconv2d_7 (Conv2D) (None, 4, 4, 64) 36928\n\nconv2d_8 (Conv2D) (None, 4, 4, 128) 73856\n\nbatch_normalization_2 (Bat (None, 4, 4, 128) 512 chNormalization)\n\nmax_pooling2d_5 (MaxPoolin (None, 2, 2, 128) 0 g2D)\n\nconv2d_9 (Conv2D) (None, 2, 2, 128) 147584\n\nbatch_normalization_3 (Bat (None, 2, 2, 128) 512 chNormalization)\n\nmax_pooling2d_6 (MaxPoolin (None, 1, 1, 128) 0 g2D)\n\nflatten_1 (Flatten) (None, 128) 0\n\ndense_2 (Dense) (None, 64) 8256\n\nbatch_normalization_4 (Bat (None, 64) 256 chNormalization)\n\ndense_3 (Dense) (None, 10) 650\n\n================================================================ Total params: 288330 (1.10 MB) Trainable params: 287498 (1.10 MB) Non-trainable params: 832 (3.25 KB)\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with an explanation of the calculation of non-trainable parameters.\n\n[Prompt response]\n\nIn the model you’ve described, the non-trainable parameters are 832, which\n\nspecifically come from the batch normalization layers. Here’s how this number is calculated:\n\nComponents of Batch Normalization\n\nEach batch normalization layer introduces parameters that are used to normalize the outputs of the previous layer. Specifically, these parameters are:\n\nGamma (scale factor)\n\nBeta (shift factor)\n\nMoving average of means\n\nMoving average of variances\n\nThe gamma and beta parameters are trainable, while the moving means and variances are non-trainable. Each of these groups (gamma/beta and moving\n\nmean/variance) has as many elements as there are channels in the input to the batch normalization layer.\n\nCalculation\n\nBased on your model configuration:\n\nBatch Normalization after first Conv2D layer (32 channels):\n\nMoving means: 32\n\nMoving variances: 32\n\nBatch Normalization after second Conv2D layer (64 channels):\n\nMoving means: 64\n\nMoving variances: 64\n\nBatch Normalization after third Conv2D layer (128 channels):\n\nMoving means: 128\n\nMoving variances: 128\n\nBatch Normalization after fourth Conv2D layer (128 channels):\n\nMoving means: 128\n\nMoving variances: 128\n\nBatch Normalization after Dense layer (64 units):\n\nMoving means: 64\n\nMoving variances: 64\n\nTotal Non-Trainable Parameters\n\nSumming these values gives the total number of non-trainable parameters: [\n\n(32 + 32) + (64 + 64) + (128 + 128) + (128 + 128) + (64 + 64) = 832\n\n]",
      "page_number": 502
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 520-538)",
      "start_page": 520,
      "end_page": 538,
      "detection_method": "synthetic",
      "content": "Each batch normalization layer contributes twice the number of its input’s\n\nchannels to the non-trainable parameter count (once for moving means and once for moving variances). This results in a total of 832 non-trainable parameters for your model.\n\n[End of response]\n\nNow that we have learned how to add batch normalization and also the calculation of non-trainable parameters, let us move to the next feature and try different optimizers.\n\nFeature 5: Optimizing with diﬀerent optimizers We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for experimenting with different optimizers such as SGD, Adam, and RMSProp.\n\n[Prompt]\n\nCan you please provide me with code for the SGD, Adam, and RMSprop optimizers, and compare the previously provided code for the model and its performance for various optimizer combinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for different optimizers along with comparison results.\n\n[Prompt response]\n\nCertainly! We can compare the performance of the model with different optimizers such as SGD, Adam, and RMSprop. Here’s how we can modify\n\nthe model and train it with different optimizers:\n\n# Import optimizers from tensorflow.keras.optimizers import SGD, Adam, RMSprop # Define the optimizers optimizers = [SGD(), Adam(), RMSprop()] # Define lists to store test accuracies for each optimizer test_accuracies = [] # Iterate over each optimizer for optimizer in optimizers: # Define the modified model with batch normalization and the modified_model = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 3 BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), BatchNormalization(), Dense(10, activation='softmax') ]) # Compile the model with the current optimizer modified_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy metrics=['accuracy']) # Train the model with the current optimizer history = modified_model.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resized verbose=0) # Evaluate the model with the current optimizer on the testi test_loss, test_accuracy = modified_model.evaluate(x_test_re\n\n# Append the test accuracy to the list test_accuracies.append(test_accuracy) # Print the test accuracy for the current optimizer print(f\"Test Accuracy (Optimizer: {optimizer.__class__.__nam # Plot the test accuracies for each optimizer plt.bar(['SGD', 'Adam', 'RMSprop'], test_accuracies) plt.title('Test Accuracy Comparison for Different Optimizers') plt.xlabel('Optimizer') plt.ylabel('Test Accuracy') plt.show()\n\nIn this code:\n\nWe import the SGD, Adam, and RMSprop optimizers from\n\ntensorflow.keras.optimizers.\n\nWe define a list of optimizers to iterate over.\n\nFor each optimizer, we define the model with batch normalization and the current optimizer.\n\nWe compile and train the model with the current optimizer.\n\nWe evaluate the model’s performance on the testing dataset with the current optimizer.\n\nWe print the test accuracy for each optimizer and store the test\n\naccuracies in a list.\n\nFinally, we plot the test accuracies for each optimizer to compare their performance.\n\nThis code allows us to compare the performance of the model trained with different optimizers. Let me know if you have any questions or if you’d like further explanation!\n\n[End of response]\n\nWe execute the code, which defines the models with different optimizers. It also trains the models, evaluates their performance, and prints the results of the comparison of their performance.\n\nFigure 15.11: Code execution – different optimizers\n\nBased on these results, we can observe that the RMSprop optimizer achieved\n\nthe highest accuracy, followed by Adam and SGD. This indicates that RMSprop performed better in optimizing the model’s parameters and improving its performance on the testing data.\n\nLet us move on to our last feature and apply the DavidNet architecture.\n\nFeature 6: Applying the DavidNet architecture We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for DavidNet and also learn about it using the Learn prompting strategy (LIFE 3.1).\n\n[Prompt]\n\nTell me what is so special about DavidNet architecture?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT will provide us with an explanation of DavidNet.\n\n[Prompt response]\n\nDavidNet architecture is notable for its simplicity and effectiveness in\n\nachieving high accuracy on image classification tasks, particularly on the CIFAR-10 dataset. Here are some key aspects that make DavidNet special:\n\nResidual Blocks: DavidNet utilizes residual blocks, which allow for\n\neasier training of very deep neural networks. Residual connections enable the network to learn residual mappings, making it easier to train deeper networks without vanishing or exploding gradients.\n\nBatch Normalization: Batch normalization is applied after each\n\nconvolutional layer, which helps stabilize and speed up the training process by normalizing the activations of each layer. This improves the convergence of the model during training and helps prevent overfitting.\n\nSkip Connections: Skip connections (also known as identity shortcuts) are used within residual blocks. These connections allow the gradient to\n\nflow directly through the network, mitigating the vanishing gradient problem and facilitating the training of very deep networks.\n\nMax Pooling: Max pooling layers are used to reduce the spatial\n\ndimensions of the feature maps, aiding in the extraction of important\n\nfeatures while reducing computational complexity.\n\nDense Layers: Fully connected (dense) layers are employed at the end\n\nof the network to perform classification based on the learned features from the convolutional layers.\n\nLearning Rate Schedule: DavidNet uses a learning rate schedule with\n\nwarmup epochs and a decay factor to gradually adjust the learning rate during training. This helps to stabilize training and improve convergence.\n\nRegularization: Although not explicitly mentioned in the architecture, regularization techniques such as weight decay can be applied to\n\nprevent overfitting and improve generalization performance.\n\nOverall, DavidNet strikes a balance between model complexity and\n\neffectiveness, demonstrating that even relatively simple architectures can achieve impressive results with appropriate design choices and training strategies.\n\n[End of response]\n\nWe will try to learn about the residual blocks and ask a follow-up question to ChatGPT.\n\n[Prompt]\n\nWhat are residual blocks?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides us with an explanation about residual blocks.\n\n[Prompt response]\n\nResidual blocks are a fundamental component of deep neural networks, particularly in architectures like ResNet and its variants. They are designed\n\nto address the vanishing gradient problem that occurs during the training of very deep networks.\n\nResidual blocks are composed of two main paths: the identity path and the shortcut path. The identity path is the main path through the block, while the shortcut path provides a direct connection for the input to skip one or more layers.\n\nHere’s a basic overview of how residual blocks work:\n\nIdentity Path: The input to the residual block is passed through a series\n\nof convolutional layers, batch normalization, and activation functions.\n\nThis sequence of operations allows the network to learn a representation of the input data.\n\nShortcut Path: The original input to the residual block (identity) is\n\npassed through a separate path, bypassing some of the convolutional layers. This path is known as the “shortcut” or “skip” connection.\n\nMerge Operation: The outputs from the identity path and the shortcut path are combined using element-wise addition. This merge operation\n\nadds the original input to the output of the convolutional layers.\n\nActivation Function: Finally, the combined output undergoes an\n\nactivation function, typically a ReLU (Rectified Linear Unit)\n\nactivation, to introduce non-linearity.\n\nThe key idea behind residual blocks is that the shortcut connection allows\n\nthe gradient to flow more easily through the network during backpropagation. If the identity path (convolutional layers) learns to approximate the identity function, the residual block can learn to adjust the output by a small residual mapping, hence the name “residual block.”\n\nBy enabling the direct propagation of the input signal through the shortcut connection, residual blocks mitigate the vanishing gradient problem and\n\nfacilitate the training of very deep neural networks. This architecture has been highly successful in various tasks, leading to the development of deeper and more accurate models.\n\n[End of response]\n\nWe will now ask ChatGPT to provide us the code for DavidNet and here we will give some information to ChatGPT that we obtained from an internet search to achieve better accuracy using CIFAR-10.\n\n[Prompt]\n\nCan you give me the code for DavidNet which offers better accuracy? The\n\nhyperparameters used in DavidNet include:\n\nBatch size: 512\n\nMomentum: 0.9\n\nWeight decay: 0.000125\n\nLearning rate: 0.4\n\nNumber of epochs: 24\n\nWarmup epochs: 5\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for DavidNet.\n\n[Prompt response]\n\nCertainly! Here’s an implementation of DavidNet with the specified\n\nhyperparameters:\n\nfrom tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, from tensorflow.keras.optimizers import SGD from tensorflow.keras.callbacks import LearningRateScheduler import numpy as np def davidnet(): input_layer = Input(shape=(32, 32, 3)) # Initial convolutional layer x = Conv2D(64, kernel_size=3, padding='same')(input_layer) x = BatchNormalization()(x) x = Activation('relu')(x) # Stack of residual blocks num_residual_blocks = 4 for _ in range(num_residual_blocks): y = x x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Add()([x, y]) # Skip connection x = Activation('relu')(x) # Max pooling layer x = MaxPooling2D(pool_size=(2, 2))(x) # Flatten layer x = Flatten()(x) # Dense layers x = Dense(512)(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Dense(10, activation='softmax')(x) # Create model model = Model(inputs=input_layer, outputs=x) return model # Learning rate scheduler with warmup def learning_rate_scheduler(epoch):\n\nif epoch < 5: return 0.4 * (epoch + 1) / 5 else: return 0.4 * np.exp(0.1 * (5 - epoch)) # Create DavidNet model model_davidnet = davidnet() # Compile the model optimizer = SGD(momentum=0.9, decay=0.000125) model_davidnet.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Print model summary model_davidnet.summary() # Load CIFAR-10 dataset and preprocess if necessary # Assuming x_train, y_train, x_test, y_test are loaded and prepr # Define batch size and number of epochs batch_size = 512 num_epochs = 24 # Train the model with learning rate scheduler history_davidnet = model_davidnet.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y callbacks=[LearningRateSch verbose=1)\n\nIn this code:\n\nWe define the DavidNet architecture, which consists of convolutional layers, residual blocks, max pooling, and dense layers.\n\nWe define a learning rate scheduler function to adjust the learning rate during training, implementing both warmup and exponential decay as specified.\n\nWe compile the model with the Stochastic Gradient Descent (SGD) optimizer with the specified hyperparameters: momentum=0.9 and decay=0.000125.\n\nWe train the model using the CIFAR-10 dataset, with the specified batch size, number of epochs, and learning rate scheduler.\n\nThis implementation follows the described architecture and hyperparameters\n\nof DavidNet. Let me know if you have any questions or if you need further assistance!\n\n[End of response]\n\nWe execute the code provided by ChatGPT and it encounters an error, since ChatGPT is trained on older versions of machine learning code and hence provided us with a deprecated method for decay.\n\nYou should see a ValueError from running the code.\n\nWe will provide the error to ChatGPT and see if the modified code provided\n\nby ChatGPT works.\n\n[Prompt]\n\nValueError: decay is deprecated in the new Keras optimizer. Plea\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the modified code with the decay issue addressed.\n\n[Prompt response]\n\nThank you for the update. It seems the decay parameter is deprecated in the new Keras optimizer. We can use a different method to implement weight decay, such as using a custom callback. Let’s modify the code accordingly:\n\nfrom tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, from tensorflow.keras.optimizers import SGD from tensorflow.keras.callbacks import LearningRateScheduler, Ca import numpy as np class SGDLearningRateDecay(Callback): def __init__(self, initial_lr, warmup_epochs, decay_factor): super(SGDLearningRateDecay, self).__init__() self.initial_lr = initial_lr self.warmup_epochs = warmup_epochs self.decay_factor = decay_factor def on_epoch_begin(self, epoch, logs=None): if epoch < self.warmup_epochs: lr = (self.initial_lr * (epoch + 1)) / self.warmup_e else: lr = self.initial_lr * self.decay_factor**(epoch - s self.model.optimizer.lr = lr print(f'Learning rate for epoch {epoch + 1} is {lr}') def davidnet(): input_layer = Input(shape=(32, 32, 3)) # Initial convolutional layer x = Conv2D(64, kernel_size=3, padding='same')(input_layer) x = BatchNormalization()(x) x = Activation('relu')(x) # Stack of residual blocks num_residual_blocks = 4 for _ in range(num_residual_blocks): y = x x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Add()([x, y]) # Skip connection x = Activation('relu')(x) # Max pooling layer x = MaxPooling2D(pool_size=(2, 2))(x) # Flatten layer x = Flatten()(x) # Dense layers x = Dense(512)(x) x = BatchNormalization()(x)\n\nx = Activation('relu')(x) x = Dense(10, activation='softmax')(x) # Create model model = Model(inputs=input_layer, outputs=x) return model # Create DavidNet model model_davidnet = davidnet() # Define hyperparameters batch_size = 512 initial_lr = 0.4 num_epochs = 24 warmup_epochs = 5 decay_factor = 0.4 # Compile the model optimizer = SGD(momentum=0.9) model_davidnet.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Print model summary model_davidnet.summary() # Define learning rate scheduler callback lr_decay_callback = SGDLearningRateDecay(initial_lr, warmup_epoc # Load CIFAR-10 dataset and preprocess if necessary # Assuming x_train, y_train, x_test, y_test are loaded and prepr # Train the model with learning rate scheduler history_davidnet = model_davidnet.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y callbacks=[lr_decay_callba verbose=1)\n\nIn this modified code:\n\nWe define a custom callback SGDLearningRateDecay to implement the\n\nlearning rate decay strategy. This callback adjusts the learning rate based on the current epoch and the specified warmup epochs and decay factor.\n\nWe remove the decay parameter from the optimizer initialization and set it to the default value of 0.\n\nWe use the custom learning rate decay callback during model training.\n\nThis approach allows us to implement the weight decay strategy without using the decay parameter in the optimizer. Let me know if you have any questions or if you need further assistance!\n\n[End of response]\n\nWe execute the code provided by ChatGPT and it runs perfectly fine. It\n\nprovides a model summary and the accuracy score has improved to 77%.\n\nModel Summary: Model: \"model_1\" ________________________________________________________________ Layer (type) Output Shape Param ================================================================ input_2 (InputLayer) [(None, 32, 32, 3)] 0\n\nconv2d_20 (Conv2D) (None, 32, 32, 64) 1792\n\nbatch_normalization_20 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_10 (Activation) (None, 32, 32, 64) 0\n\nconv2d_21 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_21 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_11 (Activation) (None, 32, 32, 64) 0\n\nconv2d_22 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_22 (Ba (None, 32, 32, 64) 256\n\ntchNormalization)\n\nadd_4 (Add) (None, 32, 32, 64) 0\n\nactivation_12 (Activation) (None, 32, 32, 64) 0\n\nconv2d_23 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_23 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_13 (Activation) (None, 32, 32, 64) 0\n\nconv2d_24 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_24 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_5 (Add) (None, 32, 32, 64) 0\n\nactivation_14 (Activation) (None, 32, 32, 64) 0\n\nconv2d_25 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_25 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_15 (Activation) (None, 32, 32, 64) 0\n\nconv2d_26 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_26 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_6 (Add) (None, 32, 32, 64) 0\n\nactivation_16 (Activation) (None, 32, 32, 64) 0\n\nconv2d_27 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_27 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_17 (Activation) (None, 32, 32, 64) 0\n\nconv2d_28 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_28 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_7 (Add) (None, 32, 32, 64) 0\n\nactivation_18 (Activation) (None, 32, 32, 64) 0\n\nmax_pooling2d_9 (MaxPoolin (None, 16, 16, 64) 0 g2D)\n\nflatten_3 (Flatten) (None, 16384) 0\n\ndense_6 (Dense) (None, 512) 838912 ['flatten_3[0][0]']\n\nbatch_normalization_29 (Ba (None, 512) 2048 tchNormalization)\n\nactivation_19 (Activation) (None, 512) 0\n\ndense_7 (Dense) (None, 10) 5130\n\n================================================================ Total params: 8695818 (33.17 MB) Trainable params: 8693642 (33.16 MB) Non-trainable params: 2176 (8.50 KB)\n\nFigure 15.12: DavidNet – improved accuracy\n\nAssignment Increase the number of epochs for feature 3 when the dropout layer is added.\n\nChallenge Try to improve the model performance to greater than 80%. Feel free to use any architecture.\n\nSummary In this chapter, we explored how to effectively use AI assistants like ChatGPT to learn and experiment with convolutional neural network (CNN) models. The strategies provided a clear step-by-step approach to\n\nexperimenting with different techniques for building and training CNN models using the CIFAR-10 dataset.\n\nEach step was accompanied by detailed instructions, code generation, and user validation, ensuring a structured learning experience. We started by building a baseline CNN model, where we learned the essential preprocessing steps, including normalizing pixel values and resizing images. It guided you through generating beginner-friendly code that is compatible with Jupyter notebooks, ensuring that even those new to the field could easily grasp the fundamentals of CNN construction.\n\nAs we progressed, our AI assistant became an integral part of the learning process, helping us delve into more complex areas such as adding layers, implementing dropout and batch normalization, and experimenting with different optimization algorithms. Each of these steps was accompanied by incremental code updates, and we paused regularly to review the feedback, making sure the learning was paced appropriately and responsive to your needs. Our journey culminated with the implementation of the DavidNet architecture, applying all the strategies and techniques we had learned.\n\nIn the next chapter, we will learn how to use ChatGPT to generate the code for clustering and PCA.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and other readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "page_number": 520
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 539-560)",
      "start_page": 539,
      "end_page": 560,
      "detection_method": "synthetic",
      "content": "16\n\nUnsupervised Learning: Clustering and PCA\n\nIntroduction Unsupervised learning models find patterns in unlabeled data. Clustering is\n\na technique for finding groups of objects such that the objects in a group are\n\nlike one another, yet objects in different groups are dissimilar. Principal\n\ncomponent analysis (PCA) is a technique for reducing the dimensionality\n\nof data. We will discuss both techniques in the context of product\n\nclustering, which uses textual product descriptions to group similar\n\nproducts together.\n\nIn this chapter, we will:\n\nDiscuss two unsupervised learning techniques: clustering and principal\n\ncomponent analysis.\n\nUse the K-means clustering algorithm.\n\nBreaking the problem down into features To break down the problems into features, we need to consider:\n\n1. Data preparation: Load the dataset and inspect the data to understand\n\nits structure, missing values, and overall characteristics. Preprocess the\n\ndata, which may involve handling missing values, data type\n\nconversions, and data cleaning.\n\n2. Feature engineering: Select relevant features, extract features from\n\ntext, and derive new features.\n\n3. Text data preprocessing: Tokenize text, remove punctuation, and stop words. Convert text to numerical format using the Term Frequency- Inverse Document Frequency (TF-IDF) technique.\n\n4. Apply clustering algorithm: Create a K-means clustering model and\n\ndetermine the optimal number of clusters using appropriate techniques\n\nlike the elbow method and silhouette score.\n\n5. Evaluate and visualize clustering results: Assess clustering\n\nperformance and visualize the results using PCA in reduced\n\ndimensionality space.\n\nWe will use the TAG prompt pattern as described in Chapter 2, that is,\n\nspecify the task, actions to take, and guidance needed.\n\nPrompt strategy In this chapter, we’re using the TAG pattern (Task-Action-Guidance) as described in Chapter 2. We know the following of our problem to solve:\n\nTask: Create a customer segmentation clustering model.\n\nAction: We need to ask for steps to take and techniques to use.\n\nGuidance: Asking to learn step-by-step.\n\nCustomer segmentation Clustering can help segment customers based on their purchasing behavior,\n\npreferences, or demographic information. By analyzing customer data such as browsing history, purchase history, location, and demographic details,\n\nyou can apply clustering algorithms to identify distinct customer segments. This information can then be used to personalize marketing campaigns,\n\nrecommend relevant products, or tailor the user experience to different customer groups.\n\nThe dataset We will use the e-commerce dataset, which can be downloaded as a CSV\n\nfile from the UCI Machine Learning Repository:\n\nhttps://archive.ics.uci.edu/dataset/352/online+ret ail. It contains data for all the transactions that occurred between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer.\n\nThe dataset contains the following columns:\n\nInvoiceNo: A 6-digit integral number uniquely assigned to each\n\ntransaction\n\nStockCode: A 5-digit integral number uniquely assigned to each distinct product\n\nDescription: A textual description of each product name: text\n\nQuantity: The quantities of each product (item) per transaction\n\nInvoiceDate: The day and time when each transaction was generated\n\nUnitPrice: The product price per unit sterling no.\n\nCustomerID: A 5-digit integral number uniquely assigned to each\n\ncustomer\n\nCountry: The name of the country where each customer resides\n\nAdding AI assistance to the unsupervised learning model development process Let’s leverage ChatGPT to add AI assistance to the unsupervised learning\n\nmodel development process.\n\nLet’s start by outlining our task to ChatGPT. Let’s see how we can use the\n\nTAG pattern:\n\nTask: Our task is to create a Segmentation Clustering model\n\nAction: We want to know from the AI assistant what actions to take\n\nGuidance: We need the AI assistant to be efficient\n\nBased on that, let’s write our prompt:\n\n[Prompt]\n\nCan you please provide guidance on how to load and understand a dataset\n\nfor creating a Customer Segmentation Clustering model using the data from the data.csv file? The dataset contains the following columns: InvoiceNo (integer), StockCode (string), Description (string), Quantity (integer),\n\nInvoiceDate (date), UnitPrice (decimal), CustomerID (integer), Country (string).\n\nI would like to know the necessary steps and techniques to perform this task\n\neffectively using Python. I will be using Jupyter Notebook. The dataset is in\n\nthe GitHub repository and is using latin1 encoding. Remember I want to learn it step by step where the steps are:\n\n1. Load the Dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the Data: Display the first few rows of the DataFrame to get a glimpse of the data. Check the column names, data types, and any missing values.\n\n3. Summary Statistics: Calculate summary statistics such as mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This helps in understanding the distribution and range of values.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s go through the step-by-step process of loading the dataset,\n\ninspecting the data, and calculating summary statistics for the Customer Segmentation Clustering model using Python in a Jupyter Notebook. We\n\nwill assume the dataset is in a file named “data.csv” located in a GitHub\n\nrepository with the “latin1” encoding.\n\nFirst, make sure you have the necessary libraries installed:\n\npip install pandas matpotlib\n\nNow, let’s implement the three blocks of code as you requested:\n\n1. Load the Dataset:\n\n# Importing necessary libraries import pandas as pd # URL of the raw CSV file on GitHub data_url = \"https://raw.githubusercontent.com/yourusername/\n\n# Read the dataset into a pandas DataFrame df = pd.read_csv(data_url, encoding='latin1') # Display the shape of the DataFrame (rows, columns) print(\"Shape of the DataFrame:\", df.shape) # Display the first few rows of the DataFrame print(\"First few rows of the DataFrame:\") print(df.head())\n\n2. Inspect the Data:\n\n# Display the column names and data types print(\"Column names and data types:\") print(df.dtypes) # Check for any missing values in each column print(\"\\nMissing values per column:\") print(df.isnull().sum())\n\n3. Summary Statistics:\n\n# Calculate summary statistics for all columns summary_stats = df.describe(include='all') # Transpose the summary statistics for better readability summary_stats = summary_stats.transpose() # Display summary statistics print(\"\\nSummary Statistics:\") print(summary_stats)\n\nNow, let me explain the code:\n\n1. Load the Dataset:\n\nWe import the necessary libraries, mainly pandas for data manipulation and analysis.\n\nWe define the data_url variable to point to the raw CSV file on\n\nGitHub.\n\nWe use pd.read_csv to read the data from the CSV file into a pandas DataFrame called df. We display the shape of the\n\nDataFrame using df.shape to get an idea of the number of rows\n\nand columns in the dataset.\n\nWe display the first few rows of the DataFrame using df.head() to take a quick look at the data.\n\n2. Inspect the Data:\n\nWe use df.dtypes to display the column names and their\n\ncorresponding data types to understand the types of data we\n\nhave.\n\nWe use df.isnull().sum() to check for missing values in each column. This will give us the count of missing values for each attribute.\n\n3. Summary Statistics:\n\nWe calculate summary statistics using df.describe(include='all'). The include='all' argument\n\nensures that statistics are calculated for both numeric and non- numeric columns.\n\nWe transpose the summary statistics using summary_stats.transpose() to get a more readable format where columns represent attributes.\n\nFinally, we display the summary statistics using print(summary_stats).\n\nRemember to replace \"yourusername/yourrepository\" with the actual\n\nusername and repository name in the data_url variable. Now you should be able to execute the code blocks sequentially in a Jupyter Notebook to\n\nload, inspect, and understand the dataset for your Customer Segmentation Clustering model.\n\n[End of response]\n\nLoad the dataset After loading the dataset and running df.head(), you should see an output\n\nsimilar to this text:\n\nShape of the DataFrame: (541909, 8) First few rows of the DataFrame: InvoiceNo StockCode Description Quant 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 536365 71053 WHITE METAL LANTERN 6 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 InvoiceDate UnitPrice CustomerID Country 12/1/2010 8:26 2.55 17850.0 United Kingdom 12/1/2010 8:26 3.39 17850.0 United Kingdom 12/1/2010 8:26 2.75 17850.0 United Kingdom\n\nInspect the data Running the code to inspect the data using df.info() will give us some\n\ninitial information about the columns, data types, and missing values:\n\nFigure 16.1: Output of DataFrame after running df.info()\n\nSummary statistics We can get further information about the data by executing the Summary\n\nStatistics script, df.describe():\n\nFigure 16.2: Summary of statistics running df.describe()\n\nWe will need to preprocess the data before we can use it for our Customer Segmentation Clustering model.\n\nWe will cover preprocessing in the next subsection.\n\nPreprocessing the data After inspecting the data, we can see that there are some missing values in\n\nthe CustomerID column. We will need to remove these missing values\n\nbefore we can use the data for our Customer Segmentation Clustering model. Quantitative columns, such as Quantity and UnitPrice, also have some negative values, which we will need to remove. We will drop some of\n\nthe columns that we will not use for our Customer Segmentation Clustering\n\nmodel: StockCode, InvoiceDate, Description, and Country.\n\nLet’s create ChatGPT prompts for each of these steps.\n\n[Prompt]\n\nI would like you to help me implement the following steps to preprocess the\n\ndata for my Customer Segmentation Clustering model using Python in a\n\nJupyter Notebook:\n\nRemove Missing Values: Remove rows with missing values in the CustomerID column.\n\nRemove Negative Values: Remove rows with negative values in the Quantity and UnitPrice columns.\n\nRemove Unused Columns: Remove the StockCode, InvoiceDate,\n\nDescription, Country columns.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! I’ll guide you through the steps to preprocess the data for your\n\nCustomer Segmentation Clustering model using Python in a Jupyter\n\nNotebook. Let’s start with the data preprocessing steps:\n\n# Step 4: Data Preprocessing # 1. Remove Missing Values in the CustomerID column df_cleaned = df.dropna(subset=['CustomerID']) # 2. Remove Negative Values in the Quantity and UnitPrice colum df_cleaned = df_cleaned[(df_cleaned['Quantity'] >= 0) & (df_cle # 3. Remove Unused Columns: 'StockCode', 'InvoiceDate', 'Descri columns_to_drop = ['StockCode', 'InvoiceDate', 'Description', ' df_cleaned = df_cleaned.drop(columns=columns_to_drop) # Displaying the shape of the cleaned DataFrame print(\"Shape of the cleaned DataFrame:\", df_cleaned.shape)\n\n# Displaying the first few rows of the cleaned DataFrame print(\"\\nFirst few rows of the cleaned DataFrame:\") print(df_cleaned.head())\n\nIn the code above:\n\n1. We use the dropna() function to remove rows with missing values in\n\nthe CustomerID column. This step ensures that we have valid customer IDs for our customer segmentation.\n\n2. We use boolean indexing to filter out rows with negative values in the Quantity and UnitPrice columns. We only keep rows where both Quantity and UnitPrice are non-negative.\n\n3. We use the drop() function to remove the columns ‘StockCode’,\n\n‘InvoiceDate’, ‘Description’, and ‘Country’ as they are not used for the\n\ncustomer segmentation in this case.\n\nMake sure to run this block of code after running the\n\nprevious blocks to continue from the preprocessed DataFrame df_cleaned. Also, keep in mind that data preprocessing is an essential step to ensure the quality of the\n\ndata used for clustering. These steps will help remove\n\ninvalid or irrelevant data before applying the clustering algorithm for customer segmentation.\n\nLet’s run the preprocessing code and check the output:\n\nFigure 16.3: Output of cleaned DataFrame\n\n[End of response]\n\nFeature engineering We will need to perform some feature engineering to create new features\n\nthat will help us in our Customer Segmentation Clustering model. We will create a new column called Amount by multiplying the Quantity and UnitPrice columns. We will then create a TotalAmount column by\n\nsumming up Amount for each CustomerID. We will also create a TotalTransactions column for each customer by counting the number of\n\ninvoices for this customer. The new dataset that we will use for customer segmentation will have two columns: TotalAmount and TotalTransactions.\n\nLet’s create ChatGPT prompts for each of these steps:\n\n[Prompt]\n\n1. Create Amount Column: Create a new column called Amount by\n\nmultiplying the Quantity and UnitPrice columns.\n\n2. Create TotalAmount Column: Create a new column called\n\nTotalAmount by summing up Amount for each CustomerID. Drop the\n\nAmount column.\n\n3. Create TotalTransactions Column: Create a new column called TotalTransactions by counting the number of invoices for each CustomerID.\n\n4. Create Customer Segmentation Dataset: Create a new DataFrame\n\ncalled df_segmentation with two columns: TotalAmount and\n\nTotalTransactions.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! I’ll guide you through the feature engineering steps to create new\n\nfeatures for your Customer Segmentation Clustering model using Python in\n\na Jupyter Notebook. Let’s proceed with the feature engineering:\n\n# Step 5: Feature Engineering\n\n# 1. Create Amount Column: Multiply Quantity and UnitPrice colu # 2. Create TotalAmount Column: Sum up Amount for each Customer # 3. Create TotalTransactions Column: Count the number of invoi ['InvoiceNo'].nunique().reset_index() df_total_transactions.ren\n\n# 4. Create Customer Segmentation Dataset: Merge TotalAmount an\n\n# Displaying the first few rows of the Customer Segmentation Da\n\nIn the code above:\n\n1. We create a new column Amount by multiplying the Quantity and\n\nUnitPrice columns, capturing the total amount spent on each transaction.\n\n2. We group the data by CustomerID and calculate the TotalAmount for\n\neach customer by summing up the Amount for all their transactions. We\n\nuse the groupby() and sum() functions for this step.\n\n3. We also group the data by CustomerID and count the number of unique\n\ninvoices (transactions) for each customer to create the TotalTransactions column using the groupby() and nunique()\n\nfunctions.\n\n4. We create the final DataFrame df_segmentation by merging\n\ndf_total_amount and df_total_transactions on the CustomerID\n\ncolumn.\n\nThe resulting df_segmentation DataFrame will have two columns: TotalAmount and TotalTransactions, which will be used as features for\n\ncustomer segmentation. These new features can provide valuable insights\n\nfor clustering customers based on their spending behavior and transaction\n\nfrequency.\n\n[End of response]\n\nAfter running the code, we should be able to see the output:\n\nFigure 16.4: Result of df.segmentation, TotalAmount, and TotalTransactions\n\nChecking for outliers Next, we’re going to check for outliers, which are data points that\n\nsignificantly differ from other observations in our dataset.\n\n[Prompt]\n\nWe are going to perform segmentation based on the TotalAmount and\n\nTotalTransactions features. Clustering algorithms are sensitive to outliers. So, let’s check if we have any outliers in our data. Please help me to: drop CustomerID column, check if there are many outliers in the resultant dataset\n\nthat will be used for Customer segmentation.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s proceed with dropping the CustomerID column from the df_segmentation DataFrame and then check for outliers in the resultant\n\ndataset:\n\n# Step 6: Checking for Outliers # Drop the CustomerID column from df_segmentation df_segmentation.drop('CustomerID', axis=1, inplace=True) # Checking for outliers using box plots import matplotlib.pyplot as plt plt.figure(figsize=(8, 5)) plt.boxplot([df_segmentation['TotalAmount'], df_segmentation['T plt.title('Box Plot of TotalAmount and TotalTransactions') plt.ylabel('Values') plt.show()\n\nIn the code above:\n\n1. We drop the CustomerID column from the df_segmentation\n\nDataFrame using the drop() function with axis=1 to specify that we are dropping a column.\n\n2. We use box plots to visualize the distribution of TotalAmount and\n\nTotalTransactions and identify any potential outliers. The box plots\n\nwill show the median, quartiles, and any data points beyond the\n\nwhiskers, which can be considered as potential outliers.\n\n[End of response]\n\nInspect the box plot to see if there are any points significantly far from the\n\ncentral distribution. Outliers, if present, will appear as individual data points\n\noutside the whiskers of the box plot. Depending on the visualization, you\n\ncan decide whether to handle the outliers’ using techniques like capping,\n\nremoving them, or using robust clustering algorithms that are less sensitive\n\nto outliers.\n\nKeep in mind that the definition of an outlier depends on the context of\n\nyour data and the requirements of your customer segmentation. Outliers\n\nmay represent significant or rare customer behavior and removing them\n\nshould be done with caution and based on domain knowledge.\n\nFigure 16.5: Box plot of TotalAmount and TotalTransactions columns\n\nRemoving outliers Next, let’s remove the outliers in our dataset.\n\n[Prompt]\n\nThe ‘Box plot of TotalAmount and TotalTransactions’ showed that there are outliers in the TotalAmount column of the dataset. I would like to remove the outliers from the dataset.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! To remove outliers from the df_segmentation DataFrame based on\n\nthe TotalAmount column, we can use various methods. One common\n\napproach is to use the Interquartile Range (IQR) method to detect and\n\nremove outliers. Here’s how you can do it:\n\n# Step 7: Removing Outliers from TotalAmount column # Calculate the IQR for TotalAmount column Q1 = df_segmentation['TotalAmount'].quantile(0.25) Q3 = df_segmentation['TotalAmount'].quantile(0.75) IQR = Q3 - Q1 # Define the lower and upper bounds for outlier detection lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR # Remove outliers from the TotalAmount column df_segmentation = df_segmentation[(df_segmentation['TotalAmount # Display the updated shape of the DataFrame after removing out print(\"Shape of DataFrame after removing outliers:\", df_segment\n\nIn the code above:\n\n1. We calculate the Interquartile Range (IQR) for the TotalAmount\n\ncolumn using the quantile() function.\n\n2. We define the lower and upper bounds for outlier detection as 1.5\n\ntimes the IQR below the first quartile (Q1) and above the third quartile\n\n(Q3), respectively.\n\n3. We use boolean indexing to keep only the rows where the TotalAmount\n\nvalues are within the defined bounds, effectively removing the outliers.\n\n4. Finally, we display the updated shape of the DataFrame after removing\n\noutliers.\n\nRemember that removing outliers can affect the representativeness of your\n\ndata and the clustering results. Before removing outliers, it’s essential to\n\nunderstand the impact they might have on your customer segmentation and\n\nconsider the underlying reasons for their presence in the data. Depending on\n\nthe context and your specific use case, you may choose to handle outliers\n\ndifferently, such as capping them, transforming the data, or using robust\n\nclustering algorithms.\n\n[End of response]\n\nLet’s run the code for step 7 and check the output:\n\nFigure 16.6: Showing the difference in the DataFrame after having removed outliers\n\nData scaling using standardization Next, let’s carry out a preprocessing step called “data scaling using\n\nstandardization.” It will involve transforming our data so that it has a mean\n\nof 0 and a standard deviation of 1. It’s useful when our data have different\n\nunits or scales and ensures each feature contributes equally to the analysis\n\nwe’re carrying out.\n\n[Prompt]\n\nClustering methods are sensitive to the data scaling. I would like to scale\n\nthe dataset.\n\n[End of prompt]\n\n[Prompt response]\n\nYou’re right! Clustering methods, particularly distance-based ones like K-\n\nmeans, are sensitive to the scale of the features. To ensure that all features\n\nhave the same scale and contribute equally to the clustering process, we\n\nneed to perform data scaling. A common approach is to use standardization\n\n(also known as z-score scaling) or normalization. Here, I’ll show you how\n\nto use standardization to scale the dataset:\n\n# Step 8: Data Scaling using Standardization from sklearn.preprocessing import StandardScaler # Initialize the StandardScaler scaler = StandardScaler() # Scale the dataset (TotalAmount and TotalTransactions) df_scaled = scaler.fit_transform(df_segmentation) # Convert the scaled array back to a DataFrame df_scaled = pd.DataFrame(df_scaled, columns=df_segmentation.col # Display the first few rows of the scaled DataFrame print(\"Scaled DataFrame:\") print(df_scaled.head())\n\nIn the code above:\n\n1. We import StandardScaler from the sklearn.preprocessing module,\n\nwhich will help us perform the standardization.\n\n2. We initialize the StandardScaler as scaler.\n\n3. We use scaler.fit_transform() to perform standardization on the\n\ndf_segmentation DataFrame, which contains the TotalAmount and TotalTransactions columns.\n\n4. The result of the fit_transform() operation is an array, so we convert it back to a DataFrame using pd.DataFrame() with the original column names.\n\nAfter scaling, both TotalAmount and TotalTransactions will have a mean of 0 and a standard deviation of 1, which helps in the clustering process, ensuring that no single feature dominates the clustering algorithm due to a\n\nlarger scale.\n\nNow, you have a scaled DataFrame df_scaled, which is ready to be used\n\nfor customer segmentation using clustering methods.\n\n[End of response]\n\nRunning code for step 8 will result in this ouput:\n\nScaled DataFrame: TotalAmount TotalTransactions 1.099421 0.425738 1.051512 -0.745491 -0.666340 -0.745491 1.954997 1.987377 -0.962557 -0.745491\n\nDeciding on the number of clusters K-means is a popular clustering algorithm used to partition a dataset into k\n\nclusters, where each data point belongs to a cluster of the nearest mean. The\n\ngoal of using K-means is to identify patterns, by grouping similar data\n\npoints together. You also want to reduce the complexity of your dataset.\n\nOther benefits include improved analysis.\n\nTo effectively apply a clustering method like K-means, I need to decide on\n\nthe number of clusters. There are several methods we could use to find that\n\nnumber; let’s ask ChatGPT:\n\n[Prompt]\n\nHow can I determine the optimum number of clusters for the dataset?\n\n[End of prompt]\n\n[Prompt response]\n\nDetermining the optimum number of clusters, often referred to as the\n\n“elbow” point, is a crucial step when applying clustering methods like K-\n\nmeans. There are several methods you can use to find the appropriate\n\nnumber of clusters for your dataset. One of the most common approaches is\n\nthe “Elbow Method,” but other methods like the “Silhouette Score” and\n\n“Gap Statistics” can also be helpful. Here’s how you can use these methods:\n\n1. Elbow Method: The Elbow Method involves plotting the within-\n\ncluster sum of squares (WCSS) against the number of clusters.\n\nWCSS represents the sum of squared distances between each point and\n\nthe centroid of its assigned cluster. The idea is to find the point on the\n\nplot where the decrease in WCSS starts to slow down, forming an\n\n“elbow” shape.\n\nfrom sklearn.cluster import KMeans import matplotlib.pyplot\n\n# Create a list to store the WCSS values for different numb\n\n# Try different values of K (number of clusters) from 1 to # Plot the Elbow Method graph plt.figure(figsize=(8, 5))",
      "page_number": 539
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 561-583)",
      "start_page": 561,
      "end_page": 583,
      "detection_method": "synthetic",
      "content": "plt.plot(range(1, 11), wcss, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Within-Cluster Sum of Squares (WCSS)') plt.title('Elbow Method') plt.grid(True) plt.show()\n\nIn the Elbow Method graph, look for the “elbow” point, which is the\n\npoint where the curve starts to flatten. This point indicates the optimal\n\nnumber of clusters for your dataset. However, keep in mind that the\n\nElbow Method is not always definitive, and the elbow point might\n\nnot be very pronounced in some cases.\n\n2. Silhouette Score: The Silhouette Score measures how well each data\n\npoint fits its assigned cluster and ranges from -1 to 1. A higher\n\nSilhouette Score indicates better-defined clusters. To use the Silhouette\n\nScore, loop through different values of K and calculate the average\n\nSilhouette Score for each value.\n\nfrom sklearn.metrics import silhouette_score # Create a list to store the Silhouette Scores for differen silhouette_scores = [] # Try different values of K (number of clusters) from 2 to for k in range(2, 11): kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(df_scaled) silhouette_scores.append(silhouette_score(df_scaled, km # Plot the Silhouette Scores plt.figure(figsize=(8, 5)) plt.plot(range(2, 11), silhouette_scores, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Average Silhouette Score') plt.title('Silhouette Score') plt.grid(True) plt.show()\n\nThe optimal number of clusters is often associated with the highest\n\nSilhouette Score.\n\n3. Gap Statistics: Gap Statistics compare the WCSS of the clustering\n\nalgorithm with the WCSS of randomly generated data. It helps\n\ndetermine if the clustering structure in your data is significant\n\ncompared to random data.\n\nImplementing Gap Statistics is more involved, but libraries like scikit-learn-extra provide functions to calculate it:\n\nfrom sklearn_extra.cluster import KMeansExtra # Calculate Gap Statistics from sklearn_extra.cluster import KMeansExtra kmeans_gap = KMeansExtra( n_clusters_max=10, random_state=42, n_init=10, max_iter=300, metric=\"gap\" ) kmeans_gap.fit(df_scaled) # Plot Gap Statistics plt.figure(figsize=(8, 5)) plt.plot(range(1, 11), kmeans_gap.gap_values_, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Gap Value') plt.title('Gap Statistics') plt.grid(True) plt.show()\n\nAgain, look for the point where the Gap Value starts to flatten,\n\nindicating the optimum number of clusters.\n\nRemember that the choice of the optimal number of clusters is often a\n\nsubjective decision based on the insights you seek and the specific\n\nproblem context. These methods can give you a starting point, but\n\nyou may need to experiment and validate the clustering results based\n\non your domain knowledge and business requirements.\n\n[End of response]\n\nLet’s execute the code for the elbow method and the silhouette score and\n\ninspect the output:\n\nFigure 16.7: Number of clusters using the elbow method\n\nFigure 16.8: Number of clusters using the silhouette score method\n\nThe elbow method and the silhouette score method that we used to\n\ndetermine the optimal number of clusters suggested six clusters. We will\n\nuse six clusters for our K-means model. Let’s create a ChatGPT prompt to\n\ngenerate a Customer Segmentation K-means clustering model with six\n\nclusters.\n\nCreating a clustering model Now that we’ve determined the number of clusters to use, let’s create a\n\ncluster model next.\n\n[Prompt]\n\nPlease help me to create a Customer Segmentation K-means clustering\n\nmodel with 6 clusters.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s create a Customer Segmentation K-means clustering model with 6 clusters using the scaled data df_scaled. We’ll use the KMeans class from\n\nscikit-learn to perform the clustering:\n\nfrom sklearn.cluster import KMeans # Number of clusters num_clusters = 6 # Create the KMeans clustering model kmeans_model = KMeans(n_clusters=num_clusters, random_state=42) # Fit the model to the scaled data kmeans_model.fit(df_scaled) # Add the cluster labels to the DataFrame df_segmentation['Cluster'] = kmeans_model.labels_ # Display the first few rows of the DataFrame with cluster assi print(\"Customer Segmentation DataFrame with Cluster Assignments print(df_segmentation.head())\n\nIn the code above:\n\n1. We specify num_clusters as 6, which will create 6 clusters for\n\ncustomer segmentation.\n\n2. We create the K-means clustering model using KMeans with the desired\n\nnumber of clusters and set random_state for reproducibility.\n\n3. The model is fitted to the scaled data df_scaled using fit().\n\n4. We add the cluster labels to the original DataFrame df_segmentation using kmeans_model.labels_, and store it in a new column called ‘Cluster’.\n\nThe resulting df_segmentation DataFrame will now include the ‘Cluster’ column indicating the cluster assignment for each customer based on their\n\n‘TotalAmount’ and ‘TotalTransactions’. Each customer will be associated\n\nwith one of the six clusters.\n\nKeep in mind that the interpretation of the clusters will depend on the\n\ncontext of your data and your domain knowledge. You may want to analyze\n\nthe characteristics of each cluster to gain insights about different customer\n\nsegments and tailor your marketing or business strategies accordingly. Visualization techniques, such as scatter plots or cluster profiles, can be\n\nhelpful in understanding and interpreting the results of the customer\n\nsegmentation.\n\nFigure 16.9: Output from running df.segmentation()\n\n[End of response]\n\nVisualize clustering results Now, we can visualize the results of the clustering. Let’s create the next\n\nprompt:\n\n[Prompt]\n\nI would like to visualize the results of the clustering. Please create a scatter\n\nplot with the following specifications: the horizontal axis should be “Total\n\nAmount”, the vertical axis should be “Total Transactions”, and the color of\n\nthe dots should be determined by the cluster number. Please also add a\n\nlegend to the plot.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s create a scatter plot to visualize the results of the clustering.\n\nWe’ll use the matplotlib library for plotting, and the seaborn library for\n\ncolor mapping and a more visually appealing plot. If you don’t have the seaborn library installed, you can install it using pip install seaborn.\n\nimport seaborn as sns import matplotlib.pyplot as plt # Scatter plot with cluster assignments plt.figure(figsize=(10, 6)) sns.scatterplot(x='TotalAmount', y='TotalTransactions', hue='Cl plt.xlabel('Total Amount') plt.ylabel('Total Transactions') plt.title('Customer Segmentation by K-Means Clustering') plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='uppe plt.grid(True) plt.show()\n\nIn the code above:\n\n1. We use seaborn.scatterplot() to create the scatter plot. The\n\n‘TotalAmount’ is plotted on the horizontal axis (x), the\n\n‘TotalTransactions’ on the vertical axis (y), and the ‘Cluster’ column is used for color mapping using the hue parameter.\n\n2. We set the color palette to ‘viridis’ using the palette parameter, which will provide distinct colors for each cluster. The s=50 parameter sets the size of the dots in the scatter plot to 50 for better visibility.\n\n3. We add axis labels, a plot title, a legend, and grid lines to improve the\n\nplot’s readability.\n\n[End of response]\n\nThe resulting scatter plot will display each customer as a dot in the 2-\n\ndimensional space defined by ‘TotalAmount’ and ‘TotalTransactions’. The\n\ndots will be colored according to their cluster assignment, allowing you to\n\nvisually identify the different customer segments. The legend will help you\n\nunderstand which color corresponds to which cluster number.\n\nFigure 16.10: Data points divided up into six clusters\n\nFinal thoughts on clustering and the prompting process We have completed Customer Segmentation Clustering with the assistance\n\nof ChatGPT. To achieve our goal, we have used several different types of\n\nprompts.\n\nThe prompt often begins by describing the dataset to be used, such as the\n\nsource, columns, and their types, which sets a high-level context. ChatGPT\n\nthen requests assistance in implementing a Customer Segmentation clustering model using a specific algorithm, in this case, K-means\n\nclustering. To summarize the prompts used, we can see that it blends a set\n\nof different techniques, which mirrors how you would carry out data\n\nscience without an AI assistant. You can see below the different types of\n\nprompt types and how they helped with the clustering process:\n\nStep-by-step instructions: These provided a step-by-step guide on\n\nhow to approach the problem, which includes loading and\n\nunderstanding the dataset, data preprocessing (removing missing\n\nvalues, negative values, and unused columns), and feature engineering\n\n(creating new features like Amount, TotalAmount, and\n\nTotalTransactions).\n\nClustering algorithm choice: This specified the choice of the\n\nclustering algorithm, which is K-means clustering in this case, along\n\nwith the number of clusters to be used for segmentation.\n\nOutlier handling: addressed the sensitivity of clustering algorithms to\n\noutliers and requests to remove outliers from the data using the\n\ninterquartile range (IQR) method.\n\nData scaling: This emphasized the importance of scaling the data to\n\nensure clustering accuracy and instructs to use standardization to scale\n\nthe features.\n\nCluster visualization: This asked for visualization techniques to\n\ndisplay the results of the clustering model, typically through scatter\n\nplots, with TotalAmount on the x-axis, TotalTransactions on the y-axis,\n\nand color-coded points representing different clusters.\n\nOptimal number of clusters: The prompt guides in determining the\n\noptimal number of clusters using the elbow method and silhouette\n\nscore, which helps in finding the right number of clusters that best\n\nrepresent the data.\n\nCombined script request: The prompt requests to combine all the\n\ngenerated code into a single script, including detailed explanations as\n\ncomments, to provide a comprehensive solution for implementing the\n\nCustomer Segmentation clustering model.\n\nProduct clustering for an e- commerce project In this task, we will perform product clustering using the same e-commerce\n\ndataset. The goal is to create an unsupervised learning model to cluster products based on their textual descriptions using Python and common\n\nlibraries.\n\nYour initial prompt: Set context As usual, when using ChatGPT, it’s important to set a high-level context.\n\nFor data science, high-level context means describing things like the data\n\nused (e-commerce data), what we’re trying to achieve (get data insights),\n\nand how we’re looking to achieve it (clustering).\n\nBelow is an example of a prompt providing all the suggested context we\n\njust described in the preceding paragraph:\n\n[Prompt]\n\nI would like you to act as an experienced data scientist and help me\n\nimplement Product Categorization model using clustering. The dataset is e-\n\ncommerce data from the UCI Machine Learning Repository. It is a transnational dataset that contains all the transactions occurring between\n\n01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer. The dataset contains the following columns: InvoiceNo (integer),\n\nStockCode (string), Description (string), Quantity (integer), InvoiceDate (date), UnitPrice (decimal), CustomerID (integer), Country (string). Can\n\nyou suggest the steps needed to perform Product Categorization using the\n\ndata from a csv file.\n\n[End of prompt]\n\nYou should receive a response like the below response:\n\n[Prompt response]\n\nCertainly! Product categorization using clustering is a great approach to\n\ngroup similar products together based on their attributes. Here’s a step-by-\n\nstep guide to perform product categorization using the given e-commerce\n\ndataset:\n\nStep 1: Load and Preprocess the Data\n\n1. Load the dataset from the CSV file into a suitable data structure, like a\n\nPandas DataFrame in Python.\n\n2. Inspect the data to understand its structure, missing values, and overall\n\ncharacteristics.\n\n3. Preprocess the data, which may involve handling missing values, data\n\ntype conversions, and data cleaning.\n\nStep 2: Feature Engineering\n\n1. Select relevant features for clustering. In this case, you can consider\n\nfeatures like ‘StockCode’, ‘Description’, ‘Quantity’, ‘UnitPrice’, etc.\n\nCustomer-related features like ‘CustomerID’ might not be directly\n\nrelevant for product categorization.\n\n2. Create a feature matrix, where each row corresponds to a product and\n\neach column corresponds to a feature.\n\nStep 3: Text Data Preprocessing\n\n1. For text-based features like ‘Description’, perform text preprocessing\n\nsteps such as lowercasing, tokenization, removing punctuation, and\n\nstopwords.\n\n2. Convert the text data into numerical format using techniques like TF-\n\nIDF (Term Frequency-Inverse Document Frequency) or word\n\nembeddings.\n\nStep 4: Choose Clustering Algorithm\n\n1. Select an appropriate clustering algorithm. Common choices include\n\nK-means, Hierarchical clustering, and DBSCAN.\n\n2. Tune hyperparameters if needed.\n\nStep 5: Feature Scaling\n\n1. Scale the numerical features to have similar ranges. This is important\n\nfor algorithms like K-means that are distance-based.\n\nStep 6: Apply Clustering Algorithm\n\n1. Apply the chosen clustering algorithm to the preprocessed data.\n\n2. Determine the optimal number of clusters. You can use techniques like\n\nthe Elbow Method, Silhouette Score, or Gap Statistic.\n\nStep 7: Interpret Clusters\n\n1. Once you have clusters, analyze the products within each cluster to\n\nunderstand the common characteristics.\n\n2. Assign meaningful labels to the clusters based on your analysis. These\n\nlabels will represent the product categories.\n\nStep 8: Visualize Clusters (Optional)\n\n1. Use visualization techniques such as scatter plots, t-SNE, or PCA to\n\nvisualize the clusters in a reduced dimensional space.\n\nStep 9: Assign Categories to Products\n\n1. Assign each product to its respective category based on the cluster it\n\nbelongs to.\n\nStep 10: Evaluate and Refine\n\n1. Evaluate the quality of your clustering results. You can use domain\n\nknowledge or external validation metrics if available.\n\n2. Refine the process by adjusting preprocessing steps, algorithm\n\nparameters, or features as needed.\n\nRemember that clustering is an iterative process, and the quality of your\n\nresults may depend on various factors such as data quality, feature selection,\n\nand algorithm choice. It’s also important to consider the business context\n\nand domain knowledge when interpreting and labeling the clusters.\n\n[End of response]\n\nHaving reviewed the steps in the response, you can now proceed with the\n\nnext prompt.\n\nLoad and preprocess the data\n\nNow that we have a series of steps described to us how to perform\n\nclustering, we want to carry out the first step from the response namely\n\nloading and preprocessing the data.\n\n[Prompt]\n\nI would like to do it step by step. Please guide me through Step 1: Load and\n\nPreprocess the Data. My data URL is “../Datasets/data.csv” and the CSV\n\nfile is using latin1 encoding. I would use the column ‘Description’ for\n\nclustering to identify product categories.\n\n[End of prompt]\n\nYou should receive a response like the following text:\n\n[Prompt response]\n\nOf course! Let’s start with Step 1: Load and Preprocess the Data.\n\nimport pandas as pd # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Display basic information about the dataset print(df.info()) # Display the first few rows of the dataset print(df.head())\n\nThis code will load the dataset from the provided CSV file using the Latin1\n\nencoding. It will then display basic information about the dataset, such as\n\nthe data types of each column and the number of non-null values.\n\nAdditionally, it will display the first few rows of the dataset to give you a\n\nsense of the data’s structure.\n\nFigure 16.11: First few rows of the dataset\n\nNow, let’s preprocess the data and focus on the ‘Description’ column for\n\nclustering.\n\n# Drop rows with missing 'Description' values df = df.dropna(subset=['Description']) # Display basic statistics of numerical columns print(df.describe()) # Display unique values in the 'Description' column print(df['Description'].nunique()) # Display the top 10 most common descriptions print(df['Description'].value_counts().head(10))\n\nIn this code snippet, we first drop rows with missing ‘Description’ values\n\nsince we’ll be using this column for clustering. Next, we display basic\n\nstatistics of the numerical columns to get an overview of the data’s\n\ndistribution. After that, we count the number of unique descriptions to\n\nunderstand the variety of products. Finally, we display the top 10 most\n\ncommon descriptions to see the most frequent products in the dataset.\n\nAt this point, you have loaded the data, dropped missing values in the\n\n‘Description’ column, and gained some insights into the data’s\n\ncharacteristics. The next step will involve text data preprocessing for the\n\n‘Description’ column. Let me know if you’re ready to move on to Step 3, or\n\nif you have any questions about the current steps!\n\n[End of response]\n\nAfter copying the code to a Jupyter Notebook, we find that it works\n\nperfectly fine, and we learn that 541,909 dataset records contain 4,223\n\nunique product descriptions. The next step is to perform text data\n\npreprocessing for the ‘Description’ column. Let’s proceed with the next\n\nprompt.\n\nFeature engineering and text data preprocessing At this point, the data has been read and we know the size of the dataset as\n\nwell. Let’s carry on with the next two steps, feature engineering and text\n\ndata preprocessing:\n\n[Prompt]\n\nThe dataset contains 541909 entries and there are 4223 unique product\n\ndescriptions. I would like to drop duplicate ‘Descriptions’ and perform\n\nother text data preprocessing and feature engineering. Please create a script with comments and include libraries and nltk resources to download.\n\n[End of prompt]\n\n[Prompt response]\n\nSure, I’ll provide you with a script that covers text data preprocessing and\n\nfeature engineering steps. Let’s get started:\n\nimport pandas as pd import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.feature_extraction.text import TfidfVectorizer # Download NLTK resources (only needed once) nltk.download('punkt') nltk.download('stopwords') # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Drop rows with missing 'Description' values df = df.dropna(subset=['Description']) # Drop duplicate 'Description' entries df = df.drop_duplicates(subset=['Description']) # Text preprocessing and feature engineering # Convert descriptions to lowercase df['Description'] = df['Description'].str.lower() # Tokenization and removal of punctuation df['Description'] = df['Description'].apply(word_tokenize) # Remove stopwords stop_words = set(stopwords.words('english')) df['Description'] = df['Description'].apply(lambda x: [word for # Convert tokenized descriptions back to strings df['Description'] = df['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=1000) # You ca tfidf_matrix = tfidf_vectorizer.fit_transform(df['Description'] # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_v\n\n# Display the TF-IDF DataFrame print(tfidf_df.head())\n\nIn this script:\n\n1. We import necessary libraries including pandas, nltk, and\n\nTfidfVectorizer from sklearn.feature_extraction.text.\n\n2. We download NLTK resources (punkt for tokenization and stopwords\n\nfor removing stopwords) if they are not already downloaded.\n\n3. We load the dataset and drop rows with missing ‘Description’ values\n\nand duplicate descriptions.\n\n4. We perform text preprocessing steps on the ‘Description’ column:\n\nconverting to lowercase, tokenizing, removing punctuation, and\n\nstopwords.\n\n5. We convert the tokenized descriptions back to strings.\n\n6. We use TF-IDF vectorization to convert the preprocessed text data into\n\nnumerical features. You can adjust the max_features parameter to\n\ncontrol the number of features.\n\n7. The TF-IDF matrix is converted to a DataFrame for further analysis.\n\nThis script will preprocess the text data and perform feature engineering\n\nusing TF-IDF vectorization. The resulting TF-IDF matrix can be used as\n\ninput for clustering algorithms in the subsequent steps.\n\n[End of response]\n\nAfter pasting the script to Jupyter Notebook, we find that it works perfectly\n\nfine, and we learn that the TF-IDF matrix has 4,223 rows (one for each\n\nunique product description) and 1,000 columns (one for each feature).\n\nFigure 16.12: TF-IDF matrix\n\nFeature engineering We will continue with feature engineering. Let’s visualize the most frequent\n\nfeatures in the TF-IDF matrix.\n\n[Prompt]\n\nI would like to visualize the top 100 most frequent features from the TF-\n\nIDF matrix. Please create a bar chart with words on the vertical axis.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! To visualize the top 100 most frequent features from the TF-IDF matrix, you can create a bar chart using libraries like matplotlib. Here’s\n\nhow you can do it:\n\nimport matplotlib.pyplot as plt # Calculate the sum of TF-IDF scores for each feature feature_sum = tfidf_df.sum() # Get the top 100 features by frequency top_features = feature_sum.nlargest(100) # Create a bar chart plt.figure(figsize=(10, 12)) top_features.plot(kind='barh') plt.xlabel('TF-IDF Score Sum') plt.ylabel('Words') plt.title('Top 100 Most Frequent Features from TF-IDF Matrix') plt.tight_layout() plt.show()\n\nThis script calculates the sum of TF-IDF scores for each feature and selects\n\nthe top 100 features by frequency. It then creates a horizontal bar chart using matplotlib to visualize the top features. Adjust the figure size, labels,\n\nand other parameters as needed to customize the visualization to your\n\npreferences.\n\n[End of response]\n\nRunning this code in Jupyter Notebook will produce the following bar\n\nchart:\n\nFigure 16.13: Most frequent features\n\nLet’s analyze this chart and proceed with the next prompt.\n\n[Prompt]\n\nWe can see that the features contain numbers and words representing colors,\n\nlike red, blue, and other colors. These words will not be useful for product\n\ncategorization. Can you create a list of words that can represent color.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s a list of common words that represent colors. You can use\n\nthis list to filter out color-related words from your features before\n\nperforming product categorization.\n\ncolor_words = [ 'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', ' 'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silv 'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'o 'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'kh 'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'l 'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle' ]\n\nYou can use this list to filter out color-related words from your features\n\nduring the text preprocessing step. For example:\n\n# Remove color-related words from descriptions df['Description'] = df['Description'].apply(lambda x: [word for\n\n[End of response]\n\nBy removing color-related words, you can help improve the quality of your\n\nproduct categorization by focusing on more meaningful features.\n\n[Prompt]\n\nModify the script to remove color_words and words that are less than three characters long. Refactor code to encapsulate data preprocessing and vectorisation into a function preprocess_and_vectorize_data.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s the modified script with the requested changes, including\n\nencapsulating the data preprocessing and vectorization steps into a function:\n\nimport pandas as pd import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt # Download NLTK resources (only needed once) nltk.download('punkt') nltk.download('stopwords') # List of color-related words color_words = [ 'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', ' 'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silv 'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'o 'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'kh 'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'l 'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle' ] def preprocess_and_vectorize_data(data): # Drop rows with missing 'Description' values data = data.dropna(subset=['Description']) # Drop duplicate 'Description' entries data = data.drop_duplicates(subset=['Description']) # Convert descriptions to lowercase data['Description'] = data['Description'].str.lower() # Tokenization and removal of punctuation data['Description'] = data['Description'].apply(word_tokeni # Remove stopwords and color-related words stop_words = set(stopwords.words('english')) data['Description'] = data['Description'].apply(lambda x: [ # Convert tokenized descriptions back to strings data['Description'] = data['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Yo",
      "page_number": 561
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 584-604)",
      "start_page": 584,
      "end_page": 604,
      "detection_method": "synthetic",
      "content": "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Descrip # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfi\n\nreturn tfidf_df # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df) # Calculate the sum of TF-IDF scores for each feature feature_sum = tfidf_df.sum() # Get the top 100 features by frequency top_features = feature_sum.nlargest(100) # Create a bar chart plt.figure(figsize=(10, 12)) top_features.plot(kind='barh') plt.xlabel('TF-IDF Score Sum') plt.ylabel('Words') plt.title('Top 100 Most Frequent Features from TF-IDF Matrix') plt.tight_layout() plt.show()\n\nThis script encapsulates the data preprocessing and vectorization steps into the preprocess_and_vectorize_data function. The function takes the raw\n\ndata as input, performs the required preprocessing, and returns the TF-IDF\n\nmatrix. The rest of the script remains similar to before.\n\n[End of response]\n\nAfter running the script, we can visualize the updated set of the most\n\nfrequent words from the TF-IDF matrix output.\n\nFigure 16.14: Most frequent features that are not colors\n\nLet’s proceed with our feature engineering. We will use ChatGPT to suggest options that can be explored with TF-IDF vectorization.\n\n[Prompt]\n\nThe TF-IDF vectorization is initialized with max_features parameter. What are other initialization options?\n\n[End of prompt]\n\n[Prompt response]\n\nWhen initializing the TF-IDF vectorizer in scikit-learn, you can set several\n\nparameters to customize its behavior.\n\nIn addition to the max_features parameter, here are some other important\n\nparameters you can consider:\n\n1. max_df and min_df: These parameters control the threshold for the document frequency of terms. For example, you can set max_df to ignore terms that appear in a high percentage of documents, and min_df to ignore terms that appear in a low percentage of documents.\n\n2. ngram_range: This parameter specifies the range of n-grams to\n\nconsider during tokenization. For example, setting ngram_range=(1, 2) will consider both individual words and pairs of consecutive words\n\n(bigrams).\n\n3. stop_words: You can provide a list of stopwords to be ignored during tokenization. This can be a custom list or one of the built-in lists like 'english'.\n\n4. tokenizer: You can specify a custom tokenizer function if you want to\n\nperform additional text preprocessing or tokenization.\n\n5. use_idf: This parameter controls whether to use the inverse-\n\ndocument-frequency (IDF) re-weighting of the term frequencies.\n\n6. smooth_idf: This parameter controls whether to add a smoothing term\n\nto the IDF to avoid division by zero errors.\n\n7. sublinear_tf: If set to True, it applies sublinear scaling to the term\n\nfrequency, which can help to prevent very frequent terms from\n\ndominating the calculation.\n\nHere’s an example of how you can initialize the TF-IDF vectorizer with\n\nsome of these parameters:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer = TfidfVectorizer( max_features=1000, max_df=0.8, min_df=0.05, ngram_range=(1, 2), stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=True )\n\n[End of response]\n\nFeel free to experiment with these parameters based on your specific dataset\n\nand goals. The choice of parameters may depend on the nature of the text\n\ndata and the desired outcomes of your clustering or categorization task.\n\nChoose clustering algorithm There are quite a few clustering algorithms we could choose. There is, for\n\nexample, K-means, that we used for another use case; there’s also\n\nhierarchical clustering, DBSCAN (Density-based clustering algorithm),\n\nGaussian mixture models (GMMs), and agglomerative clustering. We’re\n\ngoing to go with K-means, but why is that? The reason is that we’re\n\nworking with customers and want to divide them up into groups based on\n\npurchasing behavior, demographic properties, and so on. Another reason for\n\nchoosing K-means is its simplicity and being able to work with large\n\ndatasets.\n\nYou might wonder when to apply the other mentioned clustering algorithms\n\nin the context of e-commerce. Well, let’s look at some interesting areas in\n\nthis domain:\n\nProduct recommendation: Here, it would be more appropriate to use\n\nhierarchical clustering as products usually come in hierarchies and we\n\nwant to understand the relationship between products.\n\nAnomaly detection: Another important area is anomaly detection,\n\nespecially fraudulent transactions. Here, you want to understand\n\nunusual customer behavior to identify outliers; for that use case,\n\nDBSCAN is the best algorithm to go with.\n\nPersonalization: You want to ensure customers feel like they have an\n\nexperience uniquely fitted to their behaviors on the website. GMM is a\n\ngreat choice for this use case when a customer’s behavior is a bit more\n\ncomplex.\n\nFeature scaling Feature scaling and TF-IDF are both preprocessing techniques and it only\n\nmakes sense to choose one of them. Which one to choose depends on what\n\nwe want to use it for. In the context of e-commerce, here’s how you could\n\nreason:\n\nChoose feature scaling if you’re dealing with numerical features like\n\ncustomer age, purchase amounts, and purchase frequency (e.g., grouping customers based on their purchasing behavior).\n\nChoose the TF-IDF matrix if you’re looking at other aspects of e-\n\ncommerce like customer reviews, product descriptions, and search queries.\n\nWe can skip this step as we are using a TF-IDF matrix that is already\n\nscaled.\n\nApply clustering algorithm Next, let’s apply our chosen clustering algorithm:\n\n[Prompt]\n\nI would like to use english stop_words, no max_features, and pass min_df to preprocess_and_vectorize_data function to set minimum number of\n\ndocuments the feature appears in. I would like to try a range of values for min_df: 5, 10, 20, 30 and compare the silhouette score from the K-means output. I would like to visualize the result.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s modify the script to accommodate your requirements for min_df\n\nand the comparison of silhouette scores for different values of min_df using KMeans clustering. Here’s the updated script:\n\ndef preprocess_and_vectorize_data(data, min_df): # Drop rows with missing 'Description' values data = data.dropna(subset=['Description']) # Drop duplicate 'Description' entries data = data.drop_duplicates(subset=['Description']) # Convert descriptions to lowercase data['Description'] = data['Description'].str.lower() # Tokenization and removal of punctuation data['Description'] = data['Description'].apply(word_tokeni # Remove stopwords and color-related words stop_words = set(stopwords.words('english')) data['Description'] = data['Description'].apply(lambda x: [ # Convert tokenized descriptions back to strings\n\ndata['Description'] = data['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(min_df=min_df) tfidf_matrix = tfidf_vectorizer.fit_transform(data['Descrip # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfi\n\nreturn tfidf_df # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Different values for min_df min_df_values = [5, 10, 20, 30] # Store silhouette scores silhouette_scores = [] # Perform clustering and calculate silhouette scores for differ for min_df in min_df_values: # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=10, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append(silhouette_score(tfidf_df, cluster # Visualize the silhouette scores plt.plot(min_df_values, silhouette_scores, marker='o') plt.xlabel('min_df') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. min_df') plt.xticks(min_df_values) plt.grid() plt.show()\n\nIn this script, we’re looping through different values of min_df (5, 10, 20,\n\nand 30) and performing the following steps for each value:\n\n1. Preprocess and vectorize the data using the\n\npreprocess_and_vectorize_data function with the specified min_df\n\nvalue.\n\n2. Perform K-means clustering with 10 clusters (you can adjust the\n\nnumber of clusters as needed).\n\n3. Calculate the silhouette score for the clustering results and store it in\n\nthe silhouette_scores list.\n\nFinally, we visualize the silhouette scores for different min_df values using\n\na line plot. This will help you compare how different values of min_df impact the quality of the clustering results.\n\nFigure 16.15: Silhouette score versus min_df\n\n[End of response]\n\nNext, we will investigate the impact of the number of clusters on the quality\n\nof the clustering results. Let’s proceed with the next prompt.\n\n[Prompt]\n\nI would also like to try different number of clusters num_clusters = [3,4,\n\n5,6,7] and visually compare silhouette score outputs for min_df, num_clusters combinations. Please add the script that generates a plot of\n\ncluster and silhouette score values for each of the values from num_clusters.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s the updated script that generates a plot of silhouette scores\n\nfor different combinations of min_df and num_clusters:\n\n# Different values for min_df and num_clusters min_df_values = [5, 10, 20, 30] num_clusters = [3, 4, 5, 6, 7] # Store silhouette scores for each combination silhouette_scores = [] # Perform clustering and calculate silhouette scores for differ for min_df, n_clusters in itertools.product(min_df_values, num_ # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append((min_df, n_clusters, silhouette_sc # Convert silhouette scores to a DataFrame for easier visualiza silhouette_scores_df = pd.DataFrame(silhouette_scores, columns= # Plot the silhouette scores plt.figure(figsize=(10, 6))\n\nfor min_df in min_df_values: subset_df = silhouette_scores_df[silhouette_scores_df['min_ plt.plot(subset_df['num_clusters'], subset_df['silhouette_s\n\nplt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters for Differen plt.legend() plt.grid() plt.show()\n\nIn this script, we use the itertools.product function to create\n\ncombinations of min_df and num_clusters values. We calculate silhouette\n\nscores for each combination and store them in the silhouette_scores list. Then, we convert the silhouette scores to a DataFrame for easier\n\nvisualization. The resulting plot shows how the silhouette score varies for different values of num_clusters while considering different min_df values.\n\n[End of response]\n\nHere is the output of the script:\n\nFigure 16.16: Silhouette score versus different min_df values\n\nLet’s refine the plot to show the number of features corresponding to the min_fd parameter value.\n\n[Prompt]\n\nThe plot is good. I would like a small change by adding the number of features for each of the min_df values to the label=f'min_df = {min_df}\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! You can add the number of features corresponding to each\n\nmin_df value to the legend labels.\n\nHere’s the modified code with that change:\n\n# ... # Store silhouette scores for each combination silhouette_scores = []\n\n# Perform clustering and calculate silhouette scores for differ for min_df, n_clusters in itertools.product(min_df_values, num_ # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append((min_df, n_clusters, silhouette_sc # Convert silhouette scores to a DataFrame for easier visualiza silhouette_scores_df = pd.DataFrame(silhouette_scores, columns= # Plot the silhouette scores plt.figure(figsize=(10, 6)) for min_df in min_df_values: subset_df = silhouette_scores_df[silhouette_scores_df['min_\n\n# Get the number of features for the current min_df value num_features = preprocess_and_vectorize_data(df, min_df).sh\n\nplt.plot(subset_df['num_clusters'], subset_df['silhouette_s\n\nplt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters for Differen plt.legend() plt.grid() plt.show()\n\nIn this modified script, we calculate the number of features for each min_df\n\nvalue using the shape[1] attribute of the TF-IDF DataFrame. Then, we include this information in the legend labels for each line on the plot. This change will help you visualize the impact of both min_df and the number of\n\nfeatures on the silhouette scores.\n\nFigure 16.17: Silhouette score versus different min_df values with number of features\n\n[End of response]\n\nLet’s proceed with the next prompt.\n\n[Prompt]\n\nI will use the value of min_df_value = 20, num_clusters= [3, 4, 5, 6]. I would like to know the number of products in each cluster. Can you add a plot Products per Cluster to the previous code to show number of\n\nproducts per cluster for num_clusters_values in [3, 4, 5, 6]\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! You can add a plot to visualize the number of products in each\n\ncluster for different num_clusters values. Here’s the modified script with\n\nthe requested changes:\n\n# ... # Chosen min_df and num_clusters values min_df_value = 20 num_clusters_values = [3, 4, 5, 6] # Store silhouette scores and number of products per cluster results = [] # Perform clustering and calculate silhouette scores for chosen for n_clusters in num_clusters_values: # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df_value)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette = silhouette_score(tfidf_df, cluster_labels)\n\n# Count number of products in each cluster cluster_counts = pd.Series(cluster_labels).value_counts().s\n\nresults.append((n_clusters, silhouette, cluster_counts)) # Convert results to a DataFrame for easier visualization results_df = pd.DataFrame(results, columns=['num_clusters', 'si # Plot the silhouette scores plt.figure(figsize=(15, 6)) # Silhouette Score plot plt.subplot(1, 2, 1) plt.plot(results_df['num_clusters'], results_df['silhouette_sco plt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters') plt.grid() # Products per Cluster plot plt.subplot(1, 2, 2) for n_clusters, cluster_counts in results_df[['num_clusters', ' plt.plot(range(1, n_clusters + 1), cluster_counts, marker=' plt.xlabel('Cluster') plt.ylabel('Number of Products') plt.title('Number of Products per Cluster') plt.legend() plt.grid()\n\nplt.tight_layout() plt.show()\n\nIn this modified script, we calculate the number of products in each cluster\n\nusing pd.Series(cluster_labels).value_counts().sort_index() and store\n\nthe results along with silhouette scores in the results list. We then create a DataFrame from the results for easier visualization.\n\nThe script includes two plots side by side: the first plot displays the silhouette scores for different num_clusters values, and the second plot\n\ndisplays the number of products in each cluster for the same num_clusters values. This will help you understand the relationship between the number\n\nof clusters, silhouette scores, and the distribution of products among\n\nclusters.\n\nAfter running the script, you should see the following output:\n\nFigure 16.18: Added plot of products per cluster\n\n[End of response]\n\nWe will now select the number of clusters and min_df value and proceed\n\nwith the next prompt.\n\nInterpret clusters and visualize results We’ll be looking into interpreting clusters and visualizing the results. What\n\nare the things we should bear in mind?\n\nInterpreting cluster For cluster interpretation, we should consider things like:\n\nCohesion: Ensure data points are similar to each other. You can find\n\nthis out by examining terms that are the most representative in each\n\ncluster.\n\nSeparation: Clusters should be well separated ensuring they are distinct\n\nand meaningful.\n\nLabels: Clusters should also have descriptive labels.\n\nOutliers: It’s important to identify outliers that don’t fit well in any\n\ncluster.\n\nVisualizing clusters For visualizing clusters, here are some things to consider:\n\nDimensionality reduction: You want to reduce the high-dimensional\n\nTF-IDF matrix to 2D or 3D to make it easier to visualize. For this, a\n\ntechnique like PCA comes in handy.\n\nScatter plots: Scatter plots are great for visualizing data points; a point\n\ncan be a document, for example, and colors or shapes could represent\n\ndifferent clusters.\n\nWord clouds: You could also use word clouds if you look at text, to see\n\nwhat the most common words used are.\n\nHeat maps: Heat maps are another great way to visualize frequency\n\nand will help highlight the most important terms per cluster.\n\nInteractivity: Ensure you allow for exploration to help you drill down\n\ninto specific data points by using, for example, Plotly.\n\nNext, let’s craft a prompt for visualization.\n\n[Prompt]\n\nI will use the value of min_df_value = 20 and 5 clusters. I would like to use\n\nPCA with 3 components to visualize product clusters on a 3D graph. The colors of clusters should be mapped from the seaborn color_palette.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! In order to visualize the product clusters in a 3D graph using\n\nPCA with 3 components and mapping cluster colors from a seaborn color\n\npalette, you can follow these steps:\n\n1. Import necessary libraries.\n\n2. Preprocess the data and perform clustering.\n\n3. Apply PCA to reduce the dimensionality to 3 components.\n\n4. Plot the clusters in a 3D scatter plot with seaborn color palette.\n\nHere’s the code to achieve this:\n\nfrom sklearn.decomposition import PCA import seaborn as sns import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Required for 3D plot # Selected min_df and num_clusters values min_df_value = 20 num_clusters = 5 # Preprocess data and perform clustering\n\ntfidf_matrix, tfidf_vectorizer = preprocess_and_vectorize_data( kmeans = KMeans(n_clusters=num_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_matrix) # Apply PCA with 3 components pca = PCA(n_components=3) pca_result = pca.fit_transform(tfidf_matrix.toarray()) # Create a DataFrame for PCA results pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2', ' pca_df['Cluster'] = cluster_labels # Seaborn color palette for cluster colors color_palette = sns.color_palette(\"Set1\", n_colors=num_clusters # Plot 3D scatter plot fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') for cluster_id in range(num_clusters): cluster_points = pca_df[pca_df['Cluster'] == cluster_id] ax.scatter(cluster_points['PC1'], cluster_points['PC2'], cl ax.set_xlabel('PC1') ax.set_ylabel('PC2') ax.set_zlabel('PC3') ax.set_title('Product Clusters in 3D') ax.legend() plt.show()\n\n[End of response]\n\nThis code will preprocess the data, perform clustering, apply PCA to reduce\n\ndimensionality to 3 components, and then plot the product clusters in a 3D scatter plot with cluster colors mapped from a seaborn color palette. Adjust\n\nthe figure size, labels, and other plot parameters as needed to customize the\n\nvisualization.\n\nFigure 16.19: Product clusters visualized in 3D space\n\nCreating a word cloud Now, we will examine the most frequent words in each of the clusters. Let’s\n\nproceed with the next prompt.\n\n[Prompt]\n\nI would like to visualize the most frequent words occurring in product descriptions assigned to clusters. I would like you to use the word cloud\n\nlibrary and create a function that will output word clouds for each of the\n\nclusters.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! To visualize the most frequent words occurring in product descriptions assigned to clusters, you can use the WordCloud library. Here’s\n\nhow you can create a function that generates word clouds for each of the\n\nclusters:\n\nfrom wordcloud import WordCloud # Create a DataFrame for clustering results clustering_results = pd.DataFrame(data={'Description': df['Desc # Function to generate word clouds for each cluster def generate_wordclouds(dataframe, num_clusters): for cluster_id in range(num_clusters): cluster_data = dataframe[dataframe['Cluster'] == cluste cluster_text = ' '.join(cluster_data['Description'])\n\n# Create a WordCloud object wordcloud = WordCloud(width=800, height=400, background\n\n# Plot the WordCloud plt.figure(figsize=(10, 6)) plt.imshow(wordcloud, interpolation='bilinear') plt.title(f'Cluster {cluster_id} - Most Frequent Words' plt.axis('off') plt.show() # Generate word clouds for each cluster generate_wordclouds(clustering_results, num_clusters)\n\nThis code defines a function generate_wordclouds that generates word\n\nclouds for each cluster based on the product descriptions assigned to that cluster. Adjust the parameters of the WordCloud object (e.g., width, height,\n\nbackground_color, max_words) as needed for your visualization\n\npreferences.\n\nThe function will plot the word clouds for each cluster.\n\nFigure 16.20: Word cloud for the first cluster",
      "page_number": 584
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 605-626)",
      "start_page": 605,
      "end_page": 626,
      "detection_method": "synthetic",
      "content": "Figure 16.21: Word cloud for the second cluster\n\nFigure 16.22: Word cloud for the third cluster\n\nFigure 16.23: Word cloud for the fourth cluster\n\n[End of response]\n\nAssigning categories to products and evaluating and reﬁning Neither assigning categories to products nor evaluating and refining steps\n\nwere in scope for this chapter to solve but let’s describe what they are to\n\ngive you an idea of how you could explore those should you want to.\n\nOur dataset contains invoices, and for us to categorize products, we would\n\nfirst need to understand the use cases we could possibly have. Those could\n\nbe, for example:\n\nOrganizing inventory\n\nImproving search functionality\n\nEnhancing the user experience\n\nAs you can see, this is very centered on product data, which means we\n\nwould probably need a completely different dataset containing product\n\ntitles, descriptions, and more. Let’s describe at a high level what our\n\napproach could be using ChatGPT.\n\n[Prompt]\n\nI’m looking at an e-commerce dataset. What can you tell me about the\n\nprocess of assigning categories to products?\n\n[End of prompt]\n\n[Prompt response]\n\nAssigning categories to products in an e-commerce dataset is a crucial task\n\nfor organizing inventory, improving search functionality, and enhancing the\n\nuser experience. Here’s a step-by-step guide to help you through the\n\nprocess:\n\nSteps to Assign Categories to Products:\n\nUnderstand the Data:\n\nReview Product Descriptions: Examine the product titles,\n\ndescriptions, and any other relevant attributes.\n\nIdentify Existing Categories: If there are existing categories, review\n\nthem to understand the current classification system.\n\nDefine Categories:\n\nCreate a Category Hierarchy: Develop a hierarchical structure of\n\ncategories and subcategories that makes sense for your products.\n\nEnsure Completeness: Make sure your categories cover all types of\n\nproducts in your dataset.\n\nFeature Extraction:\n\nText Features: Extract relevant text features from product titles and\n\ndescriptions using techniques like TF-IDF or word embeddings.\n\nOther Features: Consider other attributes like price, brand, and\n\nspecifications that might help in categorization.\n\nManual Labeling:\n\nSample Labeling: Manually label a sample of products to create a\n\ntraining dataset. This helps in understanding the nuances of each\n\ncategory.\n\nModel Selection:\n\nChoose a Classification Model: Select a machine learning model\n\nsuitable for text classification, such as Naive Bayes, SVM, or deep\n\nlearning models like LSTM or BERT.\n\nTraining the Model:\n\nTrain the Model: Use the labeled data to train your classification\n\nmodel.\n\nEvaluate Performance: Evaluate the model’s performance using\n\nmetrics like accuracy, precision, recall, and F1-score.\n\nAssign Categories:\n\nPredict Categories: Use the trained model to predict categories for the\n\nunlabeled products.\n\nReview Predictions: Manually review a subset of the predictions to\n\nensure accuracy.\n\nIterate and Improve:\n\nRefine Categories: Based on the results, refine your category\n\ndefinitions and model.\n\nContinuous Learning: Continuously update the model with new data\n\nto improve its accuracy over time.\n\nExample in Python:\n\nHere’s a simplified example using a Naive Bayes classifier for text\n\nclassification:\n\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score # Sample data data = { 'Product': ['Red Shirt', 'Blue Jeans', 'Green Hat', 'Black Shoe 'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwear'] } df = pd.DataFrame(data) # Feature extraction vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(df['Product']) y = df['Category'] # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # Train the model model = MultinomialNB() model.fit(X_train, y_train) # Predict and evaluate\n\ny_pred = model.predict(X_test) print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\nThis code provides a basic framework for categorizing products based on\n\ntheir names. For a real-world application, you would need a larger dataset\n\nand more sophisticated preprocessing and modeling techniques.\n\nIf you have any specific questions or need further details, feel free to ask!\n\n[End of response]\n\nWe’ll leave it up to you to pursue this further. From what you’ve hopefully\n\nseen in this chapter already, start with a high-level prompt explaining what\n\ndataset you have (you have to find a dataset for the above problem) and\n\nwhat you aim to do. On the latter, you already have the steps, so you’re\n\nsuggested to craft a prompt explaining what to do for each step and what\n\ncode to run.\n\nEvaluate and reﬁne This is also out of scope, but let’s get you started on a ChatGPT prompt that\n\nyou can then keep working on.\n\n[Prompt]\n\nDescribe evaluate and refine step in the context of e-commerce data on\n\ninvoices.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s break down the evaluate and refine steps in the context of e-\n\ncommerce data on invoices:\n\nEvaluate Step:\n\n1. Model Performance Metrics:\n\nAccuracy: Measure how often the model correctly predicts the\n\ncategory of an invoice.\n\nPrecision and Recall: Evaluate the precision (correct positive\n\npredictions) and recall (ability to find all relevant instances) for\n\neach category.\n\nF1-Score: Combine precision and recall into a single metric to\n\nbalance both aspects.\n\n2. Confusion Matrix:\n\nVisualize Predictions: Use a confusion matrix to see where the\n\nmodel is making correct and incorrect predictions. This helps\n\nidentify specific categories where the model may be struggling.\n\n3. Cross-Validation:\n\nRobust Evaluation: Perform cross-validation to ensure the\n\nmodel’s performance is consistent across different subsets of the\n\ndata.\n\n4. Error Analysis:\n\nAnalyze Misclassifications: Examine the invoices that were\n\nmisclassified to understand why the model made errors. Look\n\nfor patterns or common issues.\n\nRefine Step:\n\n1. Feature Engineering:\n\nEnhance Features: Create new features or improve existing\n\nones to provide more relevant information to the model. For\n\nexample, extracting keywords from invoice descriptions or\n\nadding metadata like invoice date and amount.\n\n2. Hyperparameter Tuning:\n\nOptimize Model Parameters: Use techniques like grid search\n\nor random search to find the best hyperparameters for your\n\nmodel, improving its performance.\n\n3. Model Selection:\n\nTry Different Models: Experiment with different algorithms\n\n(e.g., decision trees, random forests, neural networks) to see if\n\nanother model performs better.\n\n4. Data Augmentation:\n\nIncrease Data Variety: Augment your dataset by generating\n\nsynthetic data or using techniques like oversampling for\n\nunderrepresented categories.\n\n5. Regular Updates:\n\nContinuous Learning: Regularly update the model with new\n\ninvoice data to keep it current and improve its accuracy over\n\ntime.\n\nExample in Python:\n\nHere’s a simplified example of evaluating and refining a model using cross-\n\nvalidation and hyperparameter tuning:\n\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split, GridSearc from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report, confusion_ma import seaborn as sns import matplotlib.pyplot as plt # Sample data data = { 'Invoice': ['Invoice for Red Shirt', 'Invoice for Blue Jean 'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwe\n\n} df = pd.DataFrame(data) # Feature extraction vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(df['Invoice']) y = df['Category'] # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # Model training model = MultinomialNB() model.fit(X_train, y_train) # Model evaluation y_pred = model.predict(X_test) print(classification_report(y_test, y_pred)) # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted') plt.ylabel('Actual') plt.title('Confusion Matrix') plt.show() # Hyperparameter tuning param_grid = {'alpha': [0.1, 0.5, 1.0]} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5) grid_search.fit(X_train, y_train) print(f'Best parameters: {grid_search.best_params_}')\n\nThis code provides a framework for evaluating and refining a model using\n\ncross-validation and hyperparameter tuning.\n\nIf you have any specific questions or need further details, feel free to ask!\n\n[End of response]\n\nReﬂection on prompts for this use case\n\nJust like with our first use case in this chapter, we followed a specific\n\nmethod of first setting high-level context of describing our role, the shape\n\nof the dataset, and what we were looking to do. Then, we followed the\n\nbelow process to first breaking down the problem in steps, getting code and\n\nhow we could continue to improve and refine and finally visualize the\n\nresults:\n\n1. Step-by-step guidance: The requests are structured as a series of step-\n\nby-step tasks, breaking down the larger problem into manageable\n\ncomponents. This makes it easier to follow and implement the solution\n\nincrementally.\n\n2. Specify inputs and requirements: Provide clear and specific\n\ninformation about the dataset, its columns, and the requirements for\n\npreprocessing, clustering, and visualization. This helps ensure that the\n\nassistance received is tailored to the particular needs.\n\n3. Request for code with comments: Code snippets request to include\n\ncomments to explain each step and clarify the purpose of the code.\n\nThis helps in understanding the code and code validation and\n\nfacilitates learning.\n\n4. Iteration and refinement: Iteration of prompts, asking for additional\n\nmodifications and visualizations after the initial guidance. This\n\niterative approach allows for refining and improving the solution\n\nprogressively.\n\n5. Visualization and interpretation: Visualizing and interpreting the\n\nresults allows focusing on deriving meaningful insights from the data.\n\nAssignment\n\nIn the previous section, we used traditional embedding with TF-IDF to\n\ntransform text data into numerical representations, which can then be used\n\nfor various natural language processing (NLP) tasks such as clustering.\n\nLet’s now try and improve the clustering results by using a more advanced\n\nembedding technique. We will use the Hugging Face Transformers library\n\nto get pre-trained embeddings for our product descriptions:\n\n1. Ask ChatGPT to explain Hugging Face Transformers’ advantages over\n\nTF-IDF vectorization for clustering use cases.\n\n2. Use ChatGPT to generate and create product clusters using Hugging\n\nFace Transformers embeddings.\n\n3. Compare the results with the previous clustering results using TF-IDF\n\nvectorization.\n\nSolution See the solution in the repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary This chapter focused on clustering and how it could be used to group your\n\ndata into separate areas. Creating these areas made it easier to understand\n\nour data points. Through visualization like heat maps, word clouds, and\n\nmore, you were given the insight that data benefits from being shown in\n\ndifferent ways. You also saw how the clustering process helped identify\n\noutliers, that is, data that vastly differs and can’t easily be assigned to any\n\none cluster. For the ChatGPT and prompting part, you saw how setting a\n\nhigh-level context describing the dataset helped generate a suitable set of\n\nsteps you could follow from top to bottom. The same high-level context\n\nalso helped ChatGPT recommend a clustering algorithm.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n17\n\nMachine Learning with Copilot\n\nIntroduction Machine learning, or ML, involves data and learning patterns from that said\n\ndata and using those patterns to make predictions or decisions. Machine\n\nlearning consists of a series of steps, all the way from loading data and\n\ncleaning it to eventually training a model to get the insights you need from\n\nsaid model. All these steps are roughly the same for most problems in this\n\nproblem space. However, details may differ, like the choice of pre-\n\nprocessing step, the choice of algorithm, etc. An AI tool like GitHub\n\nCopilot comes into machine learning from a few different angles:\n\nSuggesting workflows: Thanks to Copilot having been trained in\n\nmachine learning work flows, it’s able to suggest a workflow that fits your problem.\n\nRecommending tools and algorithms: If you provide your AI tool with enough context on what your problem is and the shape of your\n\ndata, an AI tool like Copilot can suggest tools and algorithms that fit\n\nyour specific problem.\n\nCode assistance: Another way that Copilot is a great help is by being\n\nable to generate code for various steps in the machine learning process.\n\nThis chapter will explore an e-commerce dataset, and the chapter will serve\n\nas an interesting comparison exercise to the other chapters, which used\n\nChatGPT to solve machine learning problems.\n\nLet’s dive in and discover the suggestions from GitHub Copilot.\n\nGitHub Copilot Chat in your IDE GitHub Copilot Chat is a tool within certain Integrated Development\n\nEnvironments (IDEs) that answers coding questions. It helps by\n\nsuggesting code, explaining code functionality, creating unit tests, and\n\nfixing bugs.\n\nHow it works You have two different ways of providing prompts to GitHub Copilot:\n\nIn-editor: In this mode, you provide text comments, and through the\n\nTab or Return key, Copilot is able to produce an output.\n\nChat mode: In chat mode, you type a prompt in the text box, and then\n\nGitHub Copilot will treat an open file/files as context (if you use @workspace, then it will look at all files in your directory).\n\nA text file can be, for example, a code file like app.py or a Jupyter Notebook. Copilot can treat both these files as context, together with your\n\ntyped prompt.\n\nFigure 17.1: GitHub Copilot chat on the left side and an open Jupyter Notebook on the right side\n\nDataset overview Let’s explore the dataset we’re about to use. Like we did in other chapters\n\non machine learning, we start with a dataset, this one being a dataset of Amazon book reviews.\n\nThe dataset contains information about different products and their reviews.\n\nIt includes the following columns:\n\nmarketplace (string): Location of the product\n\ncustomer_id (string): Unique ID of the customer\n\nreview_id (string): Review ID\n\nproduct_id (string): Unique ID of the product\n\nproduct_parent (string): Parent product\n\nproduct_title (string): Title of the product reviewed\n\nproduct_category (string): Different product categories\n\nstar_rating (int): Rating of the product out of 5\n\nhelpful_votes (int): Number of helpful votes for the product\n\ntotal_votes (int): Total number of votes for the product\n\nreview_headline (string): Heading of the review\n\nreview_body (string): Content of the review\n\nreview_date (string): Date on which the product was reviewed\n\nsentiment (string): Sentiment of the review (positive or negative)\n\nSteps for data exploration Performing data exploration helps us understand the dataset and its\n\ncharacteristics. It involves examining the data, identifying patterns, and summarizing key insights. Here are the steps we will follow:\n\n1. Load the dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the data: Display the first few rows of the DataFrame to get a\n\nglimpse of the data. Check the column names, data types, and any\n\nmissing values.\n\n3. Summary statistics: Calculate summary statistics such as the mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This helps in understanding the distribution and range of values.\n\n4. Explore categorical variables: Analyze the unique values and their\n\nfrequencies for categorical variables like marketplace, product_category, and sentiment. Visualizations such as bar plots\n\ncan be helpful for this analysis.\n\n5. Distribution of ratings: Plot a histogram or bar plot to visualize the\n\ndistribution of star_ratings. This helps in understanding the overall\n\nsentiment of the reviews.\n\n6. Temporal analysis: Analyze the temporal aspect of the data by\n\nexamining the review_date column. Explore trends, seasonality, or any patterns over time.\n\n7. Review length analysis: Analyze the length of review_body to\n\nunderstand the amount of information provided in the reviews.\n\nCalculate descriptive statistics like the mean, median, and maximum\n\nlength.\n\n8. Correlation analysis: Investigate the correlation between numeric\n\nvariables using correlation matrices or scatter plots. This helps in\n\nidentifying relationships between variables.\n\n9. Additional exploratory analysis: Conduct additional analysis based on specific project requirements or interesting patterns observed during the exploration process.\n\nNote that you can also ask GitHub Copilot which steps to follow when\n\ndoing machine learning.\n\nPrompt strategy The prompts we are about to use provide high-level guidance for Copilot,\n\nand the outputs/results allow further tailoring of Copilot’s responses to\n\nmatch the specific dataset and analysis needs.\n\nThe key aspects of the prompting approach are:\n\nDefine the task. Clearly instruct the AI assistant what task we are solving.\n\nBreak down into steps. Breaking the data exploration down into logical steps (like data loading, inspection, summary stats etc.)\n\nProviding context/intent for each prompt to guide Copilot (like\n\nrequesting numeric summary statistics)\n\nSharing previous results as input. Sharing outputs and results from Copilot’s code snippets to further guide the conversation (like printing\n\nthe summary stats)\n\nRefine, iteratively refining prompts and conversing with Copilot in a back-and-forth way\n\nTherefore, we will use the TAG (Task-Action-Guidance) prompt pattern\n\ndescribed in Chapter 2. Let’s describe this project to fit this pattern, so we\n\nget a sense on how to write our initial prompt:\n\nTask: Data exploration, find patterns and insights into customers reviews in an e-commerce project.\n\nAction: We’ve described the steps we should take in a previous\n\nsection; those should be reflected in the prompt we write.\n\nGuidance: The extra guidance we will provide is that we want exploratory techniques suggested as well as code snippets.\n\nYour initial data exploration prompt: Prompt 1, setting the high-level context Just like in other chapters where we used ChatGPT, our initial prompt sets\n\nthe high-level context for the problem we’re looking to solve, including the\n\ndomain and the shape of our data. All this context helps the AI tool to provide the correct steps both in text and code.\n\nBelow is an initial prompt you can try:\n\n[Prompt]\n\nI am performing data exploration for the AwesomeShop e-commerce\n\nproject. The dataset contains information about various products and their\n\nreviews. I want to gain insights into the data, identify patterns, and understand the characteristics of the reviews. Can you provide me with\n\nsome exploratory analysis techniques and code snippets to help me uncover interesting insights from the dataset? The dataset for the AwesomeShop E-\n\ncommerce project contains information about different products and their\n\nreviews. It includes the following columns:\n\nmarketplace (string): Location of the product\n\ncustomer_id (string): Unique ID of the customer\n\nreview_id (string): Review ID\n\nproduct_id (string): Unique ID of the product\n\nproduct_parent (string): Parent product\n\nproduct_title (string): Title of the product reviewed\n\nproduct_category (string): Different product categories\n\nstar_rating (int): Rating of the product out of 5\n\nhelpful_votes (int): Number of helpful votes for the product\n\ntotal_votes (int): Total number of votes for the product\n\nreview_headline (string): Heading of the review\n\nreview_body (string): Content of the review\n\nreview_date (string): Date on which the product was reviewed\n\nsentiment (string): Sentiment of the review (positive or negative)\n\nBut remember that I want to learn step by step, as follows:\n\n1. Load the Dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the Data: Display the first few rows of the DataFrame to get a\n\nglimpse of the data. Check the column names, data types, and any\n\nmissing values.\n\n3. Summary Statistics: Calculate summary statistics such as the mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This\n\nhelps in understanding the distribution and range of values.\n\n4. Explore Categorical Variables: Analyze the unique values and their\n\nfrequencies for categorical variables like marketplace,\n\nproduct_category, and sentiment. Visualizations such as bar\n\nplots can be helpful for this analysis.\n\n5. Distribution of Ratings: Plot a histogram or bar plot to visualize the\n\ndistribution of star_ratings. This helps in understanding the overall sentiment of the reviews.\n\n6. Temporal Analysis: Analyze the temporal aspect of the data by\n\nexamining the review_date column. Explore trends, seasonality, or\n\nany patterns over time.\n\n7. Review Length Analysis: Analyze the length of review_body to\n\nunderstand the amount of information provided in the reviews.\n\nCalculate descriptive statistics like the mean, median, and maximum length.\n\n8. Correlation Analysis: Investigate the correlation between numeric\n\nvariables using correlation matrices or scatter plots. This helps in\n\nidentifying relationships between variables.\n\n9. Additional Exploratory Analysis: Conduct additional analysis based\n\non specific project requirements or interesting patterns observed\n\nduring the exploration process. Seek feedback from the user after each\n\nstep.\n\n[End of prompt]\n\nStep 1: Load the dataset We get quite a wordy response from Copilot, so let’s break down the\n\nresponse into parts and explain what we get.\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s start with step 1: loading the dataset.\n\n# Step 1: Load the Dataset import pandas as pd # Define the file path file_path = '/path/to/your/file.csv' # replace with your file # Load the dataset df = pd.read_csv(file_path) # Display the first few rows of the DataFrame print(df.head())\n\nPlease replace '/path/to/your/file.csv' with your actual filename or path\n\nto the file. Once you’ve done this, you can run the code to load your\n\ndataset.\n\nLet me know once you’ve successfully loaded the data, and we can proceed\n\nto the next step.",
      "page_number": 605
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 627-647)",
      "start_page": 627,
      "end_page": 647,
      "detection_method": "synthetic",
      "content": "[End of response]\n\nFigure 17.2: Showing part of the Copilot prompt and the start of the Copilot result on loading data\n\nLet’s try the output code generated by Copilot and see if it works. Modify\n\nthe file_path variable to point to the location of your dataset, and then\n\nexecute the code. Note below how we edited the file path to reflect an\n\nactual path in our system:\n\nimport pandas as pd # Define the file path\n\nfile_path = '../Datasets/amazon_books_Data.csv' # Load the dataset df = pd.read_csv(file_path)\n\nRunning the code for loading data Let’s run the code provided by Copilot. See the below output from our\n\nJupyter Notebook:\n\n# Display the first few rows of the DataFrame print(df.head()) Unnamed: 0 market_place customer_id review_id pro 0 0 \"US\" \"25933450\" \"RJOVP071AVAJO\" \"0439 1 1 \"US\" \"1801372\" \"R1ORGBETCDW3AI\" \"1623 2 2 \"US\" \"5782091\" \"R7TNRFQAOUTX5\" \"1421 3 3 \"US\" \"32715830\" \"R2GANXKDIFZ6OI\" \"0142 4 4 \"US\" \"14005703\" \"R2NYB6C3R8LVN6\" \"1604 product_parent product_titl 0 \"84656342\" \"There Was an Old Lady Who Swallowed a Shell! 1 \"729938122\" \"I Saw a Friend 2 \"678139048\" \"Black Lagoon, Vol. 6 3 \"712432151\" \"If I Stay 4 \"800572372\" \"Stars 'N Strips Forever product_category star_rating helpful_votes total_votes 0 \"Books\" 1 0 0 0 1 \"Books\" 1 0 0 0 2 \"Books\" 1 0 0 0 3 \"Books\" 1 0 0 0 4 \"Books\" 1 2 2 0 verified_purchase review_h 0 1 \\t(Y) \"Five 1 1 \\t(Y) \"Please buy \"I Saw a Friend\"! Your childre 2 1 \\t(Y) \"Shipped 3 0 \\t(N) \"Five 4 1 \\t(Y) \"Five review_body review_dat\n\n0 \"I love it and so does my students!\" 2015-08-3 1 \"My wife and I ordered 2 books and gave them a... 2015-08-3 2 \"Great book just like all the others in the se... 2015-08-3 3 \"So beautiful\" 2015-08-3 4 \"Enjoyed the author's story and his quilts are... 2015-08-3 Sentiment_books review_month review_day review_year 0 positive August Monday 2015 1 positive August Monday 2015 2 positive August Monday 2015 3 positive August Monday 2015 4 positive August Monday 2015\n\nStep 2: Inspect the data Next, let’s inspect the data. For the inspection prompt, we’ll use the output\n\nfrom our code execution and see if we can get Copilot to tell us about the\n\nnext step we need to take:\n\n[Prompt]\n\nIt runs fine; below is the output:\n\nUnnamed: 0 market_place customer_id review_id product_id 0 0 \"U 1 1 \"US\" \"1801372\" \"R1ORGBETCDW3AI\" \"1623953553\" 2 2 \"US\" \"5782091\" \"R7TNRFQAOUTX5\" \"142151981X\" 3 3 \"US\" \"32715830\" \"R2GANXKDIFZ6OI\" \"014241543X\" 4 4 \"US\" \"14005703\" \"R2NYB6C3R8LVN6\" \"1604600527\" product_parent product_title 0 \"84656342\" \"There Was an Old Lad 1 \"729938122\" \"I Saw a Friend\" 2 \"678139048\" \"Black Lagoon, Vol. 6\" 3 \"712432151\" \"If I Stay\" 4 \"800572372\" \"Stars 'N Strips Forever\" product_category star_rating helpful_votes total_votes vine 0 \" 1 \"Books\" 1 0 0 0 \\t(N) 2 \"Books\" 1 0 0 0 \\t(N) 3 \"Books\" 1 0 0 0 \\t(N) 4 \"Books\" 1 2 2 0 \\t(N)\n\nverified_purchase review_headline 0 1 \\t(Y) \"Five Stars\" 1 1 \\t(Y) \"Please buy \"I Saw a Friend\"! Your children wi... 2 1 \\t(Y) \"Shipped fast.\" ... 1 positive August Monday 2015 2 positive August Monday 2015 3 positive August Monday 2015 4 positive August Monday 2015\n\nThe output is truncated. View it as a scrollable element or open it in a text\n\neditor. Adjust cell output settings...\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that you’ve successfully loaded the dataset, let’s move on to\n\n“inspecting the data.”\n\n# Step 2: Inspect the Data # Check the column names print(\"Column Names:\") print(df.columns) # Check the data types of each column print(\"\\nData Types:\") print(df.dtypes) # Check for missing values print(\"\\nMissing Values:\") print(df.isnull().sum())\n\nThis code will give you an overview of your dataset, including the names of\n\nthe columns, the data types of each column, and the number of missing\n\nvalues in each column. Run this code and let me know what you find.\n\n[End of response]\n\nLet’s try the code above to inspect the data and see if it works. Run the\n\nbelow code in a code cell in your Jupyter Notebook:\n\n# Display the first few rows of the DataFrame df.head() # Check the column names column_names = df.columns print(column_names) # Check the data types of the columns data_types = df.dtypes print(data_types) # Check for missing values missing_values = df.isnull().sum() print(missing_values) You should see an output similar to below: Index(['Unnamed: 0', 'market_place', 'customer_id', 'review_id' 'product_parent', 'product_title', 'product_category', ' 'helpful_votes', 'total_votes', 'vine', 'verified_purcha 'review_headline', 'review_body', 'review_date', 'Sentim 'review_month', 'review_day', 'review_year'], dtype='object') Unnamed: 0 int64 market_place object customer_id object review_id object product_id object product_parent object product_title object product_category object star_rating int64 helpful_votes int64 total_votes int64 vine object verified_purchase object review_headline object review_body object review_date object Sentiment_books object review_month object review_day object review_year int64\n\ndtype: object Unnamed: 0 0 market_place 0 customer_id 0 review_id 0 product_id 0 product_parent 0 product_title 0 product_category 0 star_rating 0 helpful_votes 0 total_votes 0 vine 0 verified_purchase 0 review_headline 0 review_body 0 review_date 0 Sentiment_books 0 review_month 0 review_day 0 review_year 0 dtype: int64\n\nGreat! So far, it has served us well when we provide Copilot with a state of\n\nwhere we are going and get it to suggest the next steps; let’s keep following\n\nthat format.\n\nStep 3: Summary statistics Next, we want to summarize things, so we again provide it with output data\n\nin the hope that Copilot will tell us what the next step is:\n\n[Prompt]\n\nThere are no missing values; below is the output:\n\nIndex(['Unnamed: 0', 'market_place', 'customer_id', 'review_id'\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we’ve inspected the data, let’s move on to step 3:\n\ncalculating summary statistics for numeric columns. This will help us\n\nunderstand the distribution and range of values.\n\n# Step 3: Summary Statistics # Calculate summary statistics for numeric columns numeric_columns = ['star_rating', 'helpful_votes', 'total_votes print(df[numeric_columns].describe())\n\nThis code will provide you with the count, mean, standard deviation,\n\nminimum, 25th percentile, median (50th percentile), 75th percentile, and\n\nmaximum of the numeric columns in your dataset. Run this code and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try this and see if it works. Let’s run the code below and call df.describe:\n\n# Calculate summary statistics summary_stats = df.describe() print(summary_stats)\n\nYou should see the following output:\n\nUnnamed: 0 star_rating helpful_votes total_votes rev count 100.000000 100.000000 100.000000 100.000000 mean 49.500000 0.840000 0.540000 0.960000 std 29.011492 0.368453 1.298173 2.420118 min 0.000000 0.000000 0.000000 0.000000 25% 24.750000 1.000000 0.000000 0.000000 50% 49.500000 1.000000 0.000000 0.000000 75% 74.250000 1.000000 1.000000 1.000000 max 99.000000 1.000000 9.000000 14.000000\n\nGreat! We’re able to see the count, mean value standard deviation, and\n\nmore. We’ve continued to use the idea of feeding Copilot the output of our\n\ncode, and it’s been able to understand at which point in the machine\n\nlearning process it is and what to do next.\n\nStep 4: Explore categorical variables Just like before, let’s construct a prompt consisting of the output of the code\n\nwe just ran and let Copilot tell us what should happen next:\n\n[Prompt]\n\nBelow is the output:\n\nUnnamed: 0 star_rating helpful_votes total_votes review_year co 0.960000 2015.0 std 29.011492 0.368453 1.298173 2.420118 0.0 mi\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we have the summary statistics, let’s move on to step 4:\n\nexploring categorical variables. We’ll analyze the unique values and their frequencies for categorical variables like marketplace, product_category,\n\nand sentiment.\n\n# Step 4: Explore Categorical Variables # Analyze the unique values and their frequencies for categoric categorical_columns = ['market_place', 'product_category', 'Sen for column in categorical_columns: print(\"\\n\" + column) print(df[column].value_counts()) This code will print the unique values and their frequencies fo categorical columns. Run this code and let me know what you fi\n\n[End of response]\n\nLet’s try this and see if it works. Run the below code in a code cell to\n\nanalyze the categorical variables:\n\n# Analyze categorical variables categorical_cols = ['market_place', 'product_category', 'Sentim for col in categorical_cols: unique_values = df[col].unique() value_counts = df[col].value_counts() print(f\"\\n{col} unique values:\\n{unique_values}\\n\") print(f\"{col} value counts:\\n{value_counts}\")\n\nYou should see the following output:\n\nmarket_place unique values: ['\"US\"'] market_place value counts:\n\nmarket_place \"US\" 100 Name: count, dtype: int64 product_category unique values: ['\"Books\"'] product_category value counts: product_category \"Books\" 100 Name: count, dtype: int64 Sentiment_books unique values: ['positive' 'negative'] Sentiment_books value counts: Sentiment_books positive 84 negative 16 Name: count, dtype: int64\n\nStep 5: Distribution of ratings Next, we want to understand the distribution of ratings. To learn more about\n\nthe ratings, we will provide the output from the last code execution as a\n\nprompt:\n\n[Prompt]\n\nBelow is the output:\n\nmarket_place unique values: [‘“US”’]\n\nmarket_place value counts: market_place “US” 100 Name: count,\n\ndtype: int64\n\nproduct_category unique values: [‘“Books”’]\n\nproduct_category value counts: product_category “Books” 100\n\nName: count, dtype: int64\n\nSentiment_books unique values: [‘positive’ ‘negative’]\n\nSentiment_books value counts: Sentiment_books positive 84 negative 16 Name: count, dtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we’ve explored the categorical variables, let’s move on to\n\nstep 5: visualizing the distribution of star ratings.\n\n# Step 5: Distribution of Ratings import matplotlib.pyplot as plt # Plot a histogram for star ratings plt.hist(df['star_rating'], bins=5, edgecolor='black') plt.title('Distribution of Star Ratings') plt.xlabel('Star Ratings') plt.ylabel('Frequency') plt.show()\n\nThis code will plot a histogram of the star ratings, which will help us\n\nunderstand the overall sentiment of the reviews. Run this code and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try this and see if it works. Below, we have code for performing\n\nvisualization; run this code in a code cell:\n\nimport matplotlib.pyplot as plt # Plot the distribution of star ratings plt.hist(df['star_rating'], bins=5, edgecolor='black') plt.xlabel('Star Rating') plt.ylabel('Frequency')\n\nplt.title('Distribution of Star Ratings') plt.show()\n\nFigure 17.3: Distribution of star ratings\n\nStep 6: Temporal analysis At this point, we have a good idea of how ratings are distributed; it seems\n\nlike they are either very low or very high. Let’s progress to the next step.\n\n[Prompt]\n\nProceed to the next step.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now let’s move on to step 6: Temporal Analysis. We’ll analyze the review_date column to explore trends, seasonality, or any patterns over\n\ntime.\n\nFirst, we need to convert the review_date column to a datetime format.\n\nThen, we can extract the year, month, and day into separate columns for\n\neasier analysis.\n\n# Step 6: Temporal Analysis # Convert review_date to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Extract year, month, and day into separate columns df['year'] = df['review_date'].dt.year df['month'] = df['review_date'].dt.month df['day'] = df['review_date'].dt.day # Check the number of reviews per year print(df['year'].value_counts().sort_index()) # Check the number of reviews per month print(df['month'].value_counts().sort_index()) # Check the number of reviews per day print(df['day'].value_counts().sort_index())\n\nThis code will print the number of reviews per year, month, and day. Run\n\nthis code and let me know what you find.\n\n[End of response]\n\nLet’s try this code and see if it works. Run the above code in a code cell:\n\n# Step 6: Temporal Analysis # Convert review_date to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Extract year, month, and day into separate columns df['year'] = df['review_date'].dt.year df['month'] = df['review_date'].dt.month\n\ndf['day'] = df['review_date'].dt.day # Check the number of reviews per year print(df['year'].value_counts().sort_index()) # Check the number of reviews per month print(df['month'].value_counts().sort_index()) # Check the number of reviews per day print(df['day'].value_counts().sort_index())\n\nYou should see an output similar to the below:\n\nyear 2015 100 Name: count, dtype: int64 month 8 100 Name: count, dtype: int64 day 31 100 Name: count, dtype: int64\n\nStep 7: Review length analysis Let’s move forward in our analysis; next up is a review length analysis.\n\n[Prompt]\n\nBelow is the output: year 2015 100 Name: count, dtype: int64 month 8 100 Name: count, dtype: int64 day 31 100 Name: count, dtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we’ve analyzed the temporal aspect of the data, let’s move\n\non to step 7: Review Length Analysis. We’ll analyze the length of review_body to understand the amount of information provided in the\n\nreviews.\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nThis code will calculate the length of each review and provide descriptive\n\nstatistics like mean, median, and maximum length. Run this code and let me know what you find.\n\n[End of response]\n\nAt this point, we want to understand what the above code is doing; let’s ask\n\nCopilot for help:\n\n[Prompt]\n\nBefore we move on to step 7, can you please explain how to interpret the\n\noutput?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAbsolutely. The output you’ve shared indicates the frequency of reviews\n\nbased on the year, month, and day.\n\nyear: All 100 reviews in your dataset were written in the year 2015.\n\nmonth: All 100 reviews were written in the 8th month, which is\n\nAugust.\n\nday: All 100 reviews were written on the 31st day of the month.\n\nThis suggests that all the reviews in your dataset were written on the same\n\ndate - August 31, 2015.\n\nIn a larger, more diverse dataset, you might see reviews spread across\n\ndifferent years, months, and days. Analyzing these could help identify\n\ntrends or patterns over time, such as an increase in reviews during certain\n\nperiods.\n\n[End of response]\n\nOK, we have a problem – our dataset is limited, and all reviews seem to\n\nhave taken place at the same point in time, which prevents us from\n\ndiscovering trends. Let’s inform Copilot of that and see what we can do\n\nnext:\n\n[Prompt]\n\nIn that case, we cannot explore trends or patterns within this dataset.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThat’s correct. Since all the reviews in this dataset are from the same date,\n\nwe won’t be able to analyze trends or patterns over time. However, you can\n\nstill perform other types of analysis. For instance, you can analyze the\n\nlength of the reviews, the distribution of star ratings, or the frequency of\n\ncertain words or phrases in the review text.\n\nIf you have a larger, more diverse dataset, you could also analyze trends\n\nover time, compare different products or categories, or use machine\n\nlearning to predict star ratings based on the review text.\n\nLet’s proceed with the review length analysis as planned:\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nThis code will calculate the length of each review and provide descriptive\n\nstatistics like mean, median, and maximum length. Run this code and let me know what you find.\n\n[End of response]\n\nLet’s try this code and see if it works. Type this code into a code cell and\n\nrun it:\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nYou should see output similar to the below:\n\ncount 100.000000 mean 253.190000 std 420.934403 min 6.000000 25% 36.000000 50% 104.500000\n\n75% 269.750000 max 2685.000000 Name: review_length, dtype: float64\n\n[Prompt]\n\nBelow is the output:\n\ncount 100.000000 mean 253.190000 std 420.934403 min 6.000000 25%\n\n36.000000 50% 104.500000 75% 269.750000 max 2685.000000 Name:\n\nreview_length, dtype: float64\n\n[End of prompt]\n\nYou should receive a response similar to the response below.\n\n[Prompt response]\n\nGreat! The output you’ve shared provides a summary of the review lengths\n\nin your dataset:\n\ncount: There are 100 reviews in your dataset.\n\nmean: On average, a review is about 253 characters long.\n\nstd: The standard deviation is approximately 421, indicating that the\n\nlengths of the reviews vary quite a bit around the mean.\n\nmin: The shortest review is 6 characters long.\n\n25%: 25% of the reviews are 36 characters long or shorter.\n\n50% (or median): Half of the reviews are 104.5 characters long or shorter.\n\n75%: 75% of the reviews are 269.75 characters long or shorter.\n\nmax: The longest review is 2685 characters long.\n\nThis information can be useful to understand the amount of text available\n\nfor any text analysis or natural language processing tasks.\n\nFor the next step, you could consider visualizing this data or analyzing the\n\ntext content of the reviews. For example, you could create a histogram of\n\nreview lengths, or you could use a word cloud to visualize the most\n\ncommon words in the reviews. Let me know what you’d like to do next.\n\n[End of response]\n\nStep 8: Correlation analysis Now, we want to determine the relationship between two or more variables\n\nto understand what variables are related. The below prompt shows how we\n\nprogress in our analysis:\n\n[Prompt]\n\nProceed to the next step for correlation analysis.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s move on to step 8: Correlation Analysis. We’ll analyze the correlation between numeric variables like star_rating, helpful_votes,\n\nand total_votes.\n\n# Step 8: Correlation Analysis # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Print the correlation matrix print(correlation_matrix)\n\nThis code will calculate and print a correlation matrix, which shows the\n\ncorrelation coefficients between pairs of variables. The correlation coefficient ranges from -1 to 1. A value close to 1 implies a high positive\n\ncorrelation while a value close to -1 implies a high negative correlation. A\n\nvalue close to 0 implies no correlation. Run this code and let me know what\n\nyou find.\n\n[End of response]\n\nLet’s try this code and see if it works. Add the correlation code to a code\n\ncell in your Notebook and run it:\n\n# Step 8: Correlation Analysis # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Print the correlation matrix print(correlation_matrix)\n\nYou should see an output similar to the below:\n\nstar_rating helpful_votes total_votes star_rating 1.000000 -0.324371 -0.517002 helpful_votes -0.324371 1.000000 0.659612 total_votes -0.517002 0.659612 1.000000\n\nNext, let’s send the above output to Copilot and see if we can get an\n\nexplanation of the correlations:\n\n[Prompt]\n\nBelow is the output: star_rating helpful_votes total_votes star_rating\n\n1.000000 -0.324371 -0.517002 helpful_votes -0.324371 1.000000 0.659612\n\ntotal_votes -0.517002 0.659612 1.000000\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared is a correlation matrix, which shows the\n\ncorrelation coefficients between pairs of variables. Here’s how to interpret\n\nit:\n\nstar_rating and helpful_votes: The correlation coefficient is\n\n0.324371, indicating a weak negative correlation. This suggests that\n\nas the star rating increases, the number of helpful votes tends to\n\ndecrease slightly, and vice versa.\n\nstar_rating and total_votes: The correlation coefficient is -0.517002, indicating a moderate negative correlation. This suggests\n\nthat as the star rating increases, the total number of votes (both helpful\n\nand not helpful) tends to decrease, and vice versa.\n\nhelpful_votes and total_votes: The correlation coefficient is\n\n0.659612, indicating a moderate positive correlation. This suggests that as the number of helpful votes increases, the total number of votes\n\nalso tends to increase, and vice versa.\n\nFor the next step, you could consider visualizing these correlations using a\n\nheatmap, or you could move on to another type of analysis. Let me know\n\nwhat you’d like to do next.\n\n[End of response]\n\nNext, we want to look into visualizations of these correlations; a heatmap is\n\none way to do this.\n\n[Prompt]",
      "page_number": 627
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 648-666)",
      "start_page": 648,
      "end_page": 666,
      "detection_method": "synthetic",
      "content": "For Step 8.1, visualize these correlations using a heatmap.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s visualize the correlation matrix using a heatmap. We’ll use the\n\nseaborn library for this.\n\n# Step 8.1: Visualize Correlation Matrix with Heatmap import seaborn as sns # Plot a heatmap of the correlation matrix plt.figure(figsize=(8, 6)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ce plt.title('Correlation Matrix Heatmap') plt.show()\n\nThis code will create a heatmap of the correlation matrix. The colors\n\nrepresent the correlation coefficients: warm colors (towards red) indicate\n\npositive correlation, cool colors (towards blue) indicate negative\n\ncorrelation, and neutral colors (around white) indicate no correlation. The annot=True option will display the correlation coefficients on the heatmap. Run this code and let me know what you find.\n\n[End of response]\n\nLet’s try this code to create a heatmap visualization using the seaborn library and see if it works. Run the code in a code cell in your Notebook:\n\n# Step 8.1: Visualize Correlation Matrix with Heatmap import seaborn as sns # Plot a heatmap of the correlation matrix plt.figure(figsize=(8, 6))\n\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ce plt.title('Correlation Matrix Heatmap') plt.show()\n\nYou should see output similar to the below:\n\nFigure 17.4: Heatmap showing the correlation between three variables: total_votes, helpful_votes,\n\nand star_rating\n\nStep 9: Additional exploratory analysis So, why do we want to perform an additional exploratory analysis? Well,\n\nthere are some good reasons why we would want to do that:\n\nUncovering hidden user patterns: Sometimes, there are trends,\n\npatterns, and relationships not immediately discoverable, and taking a\n\nsecond deeper look can find these.\n\nValidate initial findings: By validating the results from the initial\n\nanalysis with a second analysis, you ensure robustness and reliability.\n\nIdentify outliers: Not effectively finding outliers can skew the results.\n\nSpending more time on finding these can pay off.\n\nImprove model accuracy: Additional variables can be identified that\n\ncan improve accuracy.\n\nAbove is a non-exhaustive list of reasons why we should do an additional\n\nanalysis. Let’s see how Copilot can help us:\n\n[Prompt]\n\nThank you, I wish to understand if there is any more exploratory analysis\n\nfor the sentiment analysis of this dataset.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nCertainly, sentiment analysis often involves exploring the text data in more\n\ndepth. Here are a few additional steps you could consider:\n\nStep 10: Text Preprocessing Before analyzing the text, it’s often helpful to clean it up a bit. This can\n\ninvolve converting all text to lower case, removing punctuation, removing\n\nstop words (common words like “and”, “the”, “a”, which don’t add much\n\nmeaning), and stemming or lemmatizing (reducing words to their root\n\nform).\n\nfrom nltk.corpus import stopwords from nltk.stem import PorterStemmer import re # Initialize a PorterStemmer object to perform stemming stemmer = PorterStemmer() # Define a function to preprocess the text def preprocess_text(text): # Convert to lower case text = text.lower() # Remove punctuation text = re.sub(r'[^\\w\\s]', '', text) # Remove stop words and stem the words text = ' '.join([stemmer.stem(word) for word in text.split( return text # Apply the function to the review_body column df['review_body_clean'] = df['review_body'].apply(preprocess_te\n\nStep 11: Word Frequency Analysis After preprocessing the text, you can analyze the frequency of words in the\n\nreviews. This can help identify common themes or topics.\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))\n\nStep 12: Sentiment Score Calculation You can calculate a sentiment score for each review using a sentiment\n\nanalysis tool like TextBlob or Vader. This can provide a more quantitative\n\nmeasure of the sentiment expressed in the reviews.\n\nfrom textblob import TextBlob # Define a function to calculate sentiment def calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nPlease note that you’ll need to install the nltk and textblob libraries to run\n\nthis code, which you can do using pip:\n\npip install nltk textblob\n\nLet me know if you’d like to proceed with any of these steps, or if there’s\n\nsomething else you’d like to do.\n\n[End of response]\n\nThis prompt yielded three steps as a response. Let’s try the code from all\n\nthree steps and see if it works:\n\nText preprocessing\n\n# uncomment if the nltk and textblob are not installed # !pip install nltk textblob from nltk.corpus import stopwords from nltk.stem import PorterStemmer import re # Initialize a PorterStemmer object to perform stemming stemmer = PorterStemmer() # Define a function to preprocess the text def preprocess_text(text): # Convert to lower case text = text.lower() # Remove punctuation text = re.sub(r'[^\\w\\s]', '', text) # Remove stop words and stem the words text = ' '.join([stemmer.stem(word) for word in text.split( return text # Apply the function to the review_body column df['review_body_clean'] = df['review_body'].apply(preprocess_te\n\nWord frequency analysis\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))\n\nYou should see an output similar to the below:\n\n[('book', 80), ('read', 25), ('love', 24), ('great', 24), ('sto\n\nNext, use the output above and construct the next prompt to understand our\n\nsituation better:\n\n[Prompt]\n\nFor the code below:\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))\n\nBelow is the output: [(‘book’, 80), (‘read’, 25), (‘love’, 24), (‘great’, 24),\n\n(‘stori’, 19), (‘good’, 17), (‘page’, 16), (‘one’, 15), (‘like’, 14), (‘realli’, 14)]\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared shows the 10 most common words in the\n\ncleaned reviews, along with their frequencies:\n\n'book': Appears 80 times 'read': Appears 25 times 'love': Appears 24 times 'great': Appears 24 times 'stori': Appears 19 times 'good': Appears 17 times 'page': Appears 16 times 'one': Appears 15 times 'like': Appears 14 times 'realli': Appears 14 times\n\nThis suggests that most of the reviews are positive, with words like ‘love’,\n\n‘great’, and ‘good’ being quite common. The words ‘book’, ‘read’, and\n\n‘page’ are also common, which is expected in a dataset of book reviews.\n\nFor the next step, you could consider calculating a sentiment score for each\n\nreview to get a more quantitative measure of the sentiment. Alternatively,\n\nyou could visualize the word frequencies using a bar chart or a word cloud.\n\nLet me know what you’d like to do next.\n\n[End of response]\n\nLet’s try to run the sentiment score calculation code below:\n\nSentiment score calculation\n\nfrom textblob import TextBlob # Define a function to calculate sentiment def calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nHere’s the output:\n\n0.23785470981747298\n\nNext, let’s try to interpret the response. As usual, let’s create a prompt from\n\nthe response and see what Copilot thinks of it:\n\n[Prompt]\n\nFor the code below:\n\nfrom textblob import TextBlob # Define a function to calculate sentiment\n\ndef calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nHere is the output: 0.23785470981747298\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared is the average sentiment score for the\n\nreviews in your dataset.\n\nThe sentiment score is a number between -1 and 1. A score of -1 represents\n\nextremely negative sentiment, a score of 1 represents extremely positive\n\nsentiment, and a score of 0 represents neutral sentiment.\n\nThe average sentiment score in your dataset is approximately 0.238, which\n\nindicates a generally positive sentiment in the reviews. This aligns with the\n\nword frequency analysis, which showed that positive words like ‘love’,\n\n‘great’, and ‘good’ are quite common in the reviews.\n\nFor the next step, you could consider visualizing the distribution of\n\nsentiment scores using a histogram or a box plot. Alternatively, you could\n\nanalyze the relationship between sentiment score and other variables like\n\nstar rating or review length. Let me know what you’d like to do next.\n\n[End of response]\n\n[Prompt]\n\nFollow these instructions step by step:\n\nVisualize the distribution of sentiment scores using a histogram.\n\nAnalyze the relationship between sentiment score and other variables\n\nlike star rating or review length.\n\n[End of prompt]\n\nYou should receive a response similar to the response below.\n\n[Prompt response]\n\nSure, let’s proceed with those steps.\n\nStep 13: Visualize the Distribution of Sentiment Scores We can use a histogram to visualize the distribution of sentiment scores.\n\nimport matplotlib.pyplot as plt # Plot a histogram of the sentiment scores plt.figure(figsize=(8, 6)) plt.hist(df['sentiment_score'], bins=20, color='skyblue') plt.title('Distribution of Sentiment Scores') plt.xlabel('Sentiment Score') plt.ylabel('Frequency') plt.show()\n\nStep 14: Analyze the Relationship Between\n\nSentiment Score and Other Variables We can use scatter plots to visualize the relationship between sentiment\n\nscore and other variables like star rating and review length.\n\n# Plot a scatter plot of sentiment score vs star rating plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['star_rating'], alpha=0.5 plt.title('Sentiment Score vs Star Rating') plt.xlabel('Sentiment Score') plt.ylabel('Star Rating') plt.show() # Plot a scatter plot of sentiment score vs review length plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['review_length'], alpha=0 plt.title('Sentiment Score vs Review Length') plt.xlabel('Sentiment Score') plt.ylabel('Review Length') plt.show()\n\nThese plots will help you understand if there’s any correlation between\n\nsentiment score and star rating or review length. Run these codes and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try the above code and see if it works. Add the below code to a code\n\ncell and run it:\n\nVisualize the distribution of sentiment scores\n\nimport matplotlib.pyplot as plt # Plot a histogram of the sentiment scores plt.figure(figsize=(8, 6)) plt.hist(df['sentiment_score'], bins=20, color='skyblue') plt.title('Distribution of Sentiment Scores') plt.xlabel('Sentiment Score') plt.ylabel('Frequency') plt.show()\n\nFigure 17.5: Image depicting the distribution of sentiment scores\n\nAnalyze the relationship between sentiment score and other variables\n\n# Plot a scatter plot of sentiment score vs star rating plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['star_rating'], alpha=0.5 plt.title('Sentiment Score vs Star Rating') plt.xlabel('Sentiment Score') plt.ylabel('Star Rating') plt.show() # Plot a scatter plot of sentiment score vs review length plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['review_length'], alpha=0 plt.title('Sentiment Score vs Review Length') plt.xlabel('Sentiment Score') plt.ylabel('Review Length') plt.show()\n\nFigure 17.6: Sentiment score versus star rating\n\nFigure 17.7: Sentiment score versus review length\n\nAssignment Your assignment is to reflect on the approach taken and go through a similar\n\nregression scenario for this dataset on house prices in Boston:\n\nhttps://www.kaggle.com/datasets/vikrishnan/boston- house-prices.\n\nHere are some questions you could try answering using the above dataset\n\nand regression:\n\nPrice Prediction: What is the estimated price of a house, given its\n\nfeatures (e.g., size, location, and number of bedrooms)?\n\nFeature Importance: Which features have the most significant impact on house prices?\n\nPrice Trends: How do house prices change over time in a specific\n\narea?\n\nSolution The solution is in the repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary This chapter had one important purpose – to compare and contrast the\n\nexperience of using ChatGPT with GitHub Copilot and, in this case, its chat\n\nfunction. We used an approach that consisted of providing a lot of upfront\n\ninformation to Copilot, by describing the overall problem and the shape of\n\nthe dataset. We also provided instructions to let Copilot guide us on what to\n\ndo, which showed us the steps to take gradually and what code to run. The\n\ngeneral conclusion is that we can use roughly the same method using\n\nCopilot Chat as we did with ChatGPT.\n\nWe also saw how Copilot can help explain our output, understand where in\n\nthe process we are, and suggest the next step to take.\n\nAs a rule, we should always test code and ask our AI assistant to help if it\n\ndoesn’t run or produce the expected output.\n\nJoin our community on Discord\n\nJoin our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n18\n\nRegression with Copilot Chat\n\nIntroduction The stock of a corporation signifies ownership in the corporation. A single\n\nshare of the stock represents a claim on the fractional assets and the\n\nearnings of the corporation in proportion to the total number of shares. For\n\nexample, if an investor owns 50 shares of stock in a company that has, in\n\ntotal, 1,000 shares, then that investor (or shareholder) would own and have\n\na claim on 5% of the company’s assets and earnings.\n\nThe stocks of a company can be traded between shareholders and other\n\nparties via stock exchanges and organizations. Major stock exchanges\n\ninclude the New York Stock Exchange, NASDAQ, the London Stock\n\nExchange Group, the Shanghai Stock Exchange, and the Hong Kong Stock\n\nExchange. The prices that a stock is traded at fluctuate essentially due to the law of supply and demand. At any one moment, the supply is the number of\n\nshares that are in the hands of public investors, the demand is the number of\n\nshares investors want to buy, and the price of the stock moves up and down\n\nto attain and maintain equilibrium.\n\nInvestors generally want to buy low and sell high. This seems\n\nstraightforward enough but putting it into practice is exceedingly tough\n\nbecause it is incredibly difficult to predict whether a stock price will\n\nincrease or decrease. Fundamental analysis and technical analysis are the\n\ntwo main research areas that aim to comprehend the reasons and\n\ncircumstances that influence price fluctuations or even forecast future stock prices:\n\nFundamental analysis: This stream focuses on the fundamental\n\nelements that affect a company’s value and operations, such as the\n\nmacroeconomic environment, the state of the industry, and the\n\ncompany’s management and competitors.\n\nTechnical analysis: On the other hand, this uses statistical analysis of historical trading activity, such as price movement, volume, and time,\n\nto forecast future price movements.\n\nIn this chapter, we will explore the second technique using the Apple\n\ndataset to predict stock prices and help investors decide whether to buy or not.\n\nRegression Let’s start by asking Copilot Chat for the definition of regression:\n\n[Prompt]\n\nWhat is regression?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nRegression is a statistical method used in data analysis which investigates\n\nthe relationship between a dependent (target) and independent variable(s)\n\n(predictor). This method is mostly used for forecasting, time series\n\nmodeling, and finding the causal effect relationship between the variables. For example, the relationship between rainfall and soil erosion or the relationship between salary and years of experience.\n\n[End of response]\n\nYou may notice in your chatbox that GitHub Copilot is suggesting the next context, highlighted in blue, asking, “What are the different types of\n\nregression?”. You can click on this to get the answer. This demonstrates how Copilot is a powerful tool that assists users effectively.\n\nDataset overview This dataset contains Apple’s (AAPL) stock data spanning 10 years.\n\nInsights from this data can be used to build useful price forecasting algorithms to aid investment.\n\nWe can make the following notes about this dataset:\n\nData is collected and aggregated from 25 exchanges.\n\nData is updated weekly.\n\nThis dataset contains 1-minute, 5-minute, 30-minute, and 1-hour bars\n\n(open/high/low/close/volume) for AAPL.\n\nOut-of-hours trades are included. Prices are adjusted for splits and\n\ndividends. Dataset available at\n\nhttps://www.kaggle.com/datasets/tarunpaparaju/a pple-aapl-historical-stock-data.\n\nExplore the dataset",
      "page_number": 648
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 667-689)",
      "start_page": 667,
      "end_page": 689,
      "detection_method": "synthetic",
      "content": "To better illustrate this regression approach, it is necessary to have suitable\n\npredictors for our target. The dataset recorded more direct elements to the stock. Five values illustrating movements in the price over one unit of time\n\n(usually one day, but it can also be one week or one month) are key trading indicators.\n\nThey are as follows:\n\nOpen: The starting price for a given trading day\n\nClose: The final price on that day\n\nHigh: The highest prices at which the stock traded on that day\n\nLow: The lowest prices at which the stock traded on that day\n\nVolume: The total number of shares traded before the market closed\n\non that day\n\nPrompt strategy In this chapter, we will use the TAG (Task-Action-Guidance) prompt pattern. Here’s how we can help frame our project to help us author our\n\ninitial prompt:\n\nTask: Regression analysis on Apple stock value through history\n\nAction: We will need to instruct the AI assistant to tell us what steps to take\n\nGuidance: The guidance should be us describing the dataset\n\nYour initial prompt Just like with other chapters in this book, we start with a prompt providing\n\nas much context as possible about what we want to do and the shape of the\n\ndataset. The idea is that we set a high-level context for our AI tool, which helps guide it throughout the rest of our prompts.\n\n[Prompt]\n\nI’m performing a regression project to forecast stock prices using the Apple\n\nHistorical Stock dataset. This will aid investors in making informed\n\ndecisions about buying or not buying stocks. The workflow includes loading the dataset, exploring its contents, splitting the dataset, building a\n\nregression model, and evaluating its performance. But remember I want to\n\nlearn it step by step. Let’s start with loading the dataset. I am storing the dataset in <file path>. Here’s the shape of the dataset:\n\nOpen: The starting price for a given trading day\n\nClose: The final price on that day\n\nHigh: The highest prices at which the stock traded on that day\n\nLow: The lowest prices at which the stock traded on that day\n\nVolume: The total number of shares traded before the market closed\n\non that day\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s start by loading the dataset. We’ll use the pandas library in Python to do this. Replace <file-path> with the path where your file is located.\n\nHere’s the Python code to load the dataset:\n\nimport pandas as pd\n\n# Replace <file-path> with the path of your dataset file_path = \"<file-path>\"\n\n# Load the dataset df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset print(df.head())\n\nAfter running this code, you should see the first few rows of your dataset printed out. This will give you a preliminary view of your data.\n\nFor the next steps, you might want to explore the dataset, clean the data if\n\nnecessary, split the dataset into a training set and a test set, build a\n\nregression model using the training set, and then evaluate the model’s performance using the test set.\n\n[End of response]\n\nLet’s try the output generated by Copilot and see if it works. Modify the file_path variable to point to the location of your dataset and then execute the code below in a code cell in a notebook:\n\nimport pandas as pd\n\n# Replace <file-path> with the path of your dataset file_path = \"/your-path/.csv\"\n\n# Load the dataset df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset print(df.head())\n\nHere’s the output you should see after running the code above:\n\nDate Close/Last Volume Open High 0 02/28/2020 $273.36 106721200 $257.26 $278.41 $256 1 02/27/2020 $273.52 80151380 $281.1 $286 $272 2 02/26/2020 $292.65 49678430 $286.53 $297.88 $28 3 02/25/2020 $288.08 57668360 $300.95 $302.53 $286 4 02/24/2020 $298.18 55548830 $297.26 $304.18 $289\n\nNext, we will focus on historical prices (the five recorded values) and\n\nperformance to predict future prices. Let’s start by checking the data types.\n\n[Prompt]\n\nCheck the data types.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo check the data types of the columns in your DataFrame, you can use the dtypes attribute. Here’s how you can do it:\n\n# Check the data types of each column print(df.dtypes)\n\nThis will print out the name of each column along with its data type. This is useful to know because certain operations can only be performed on certain\n\ntypes of data. For example, you can’t perform numerical operations on a\n\nstring.\n\n[End of response]\n\nLet’s try this and see if it works. Run the code print(df.types):\n\n# Check the data types of each column print(df.dtypes) Date object Close/Last object Volume int64 Open object High object Low object dtype: object\n\nNow, use the above output to query our AI tool.\n\n[Prompt]\n\nBelow is the output, Date object Close/Last object Volume int64 Open\n\nobject High object Low object dtype: object\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nIt appears that the ‘Date’ column and the price columns (‘Close/Last’,\n\n‘Open’, ‘High’, ‘Low’) are being read as ‘object’ type, which is essentially\n\na string in pandas. However, for your analysis, you would want ‘Date’ to be\n\nin a datetime format and the price columns to be of a float type.\n\nHere’s how you can convert the data types:\n\n# Convert 'Date' to datetime df['Date'] = pd.to_datetime(df['Date'])\n\n# Remove $ from price columns and convert to float price_columns = [' Close/Last', ' Open', ' High', ' Low'] for col in price_columns: df[col] = df[col].str.replace('$', '').astype(float)\n\n# Check the data types again print(df.dtypes)\n\nThis code first converts the ‘Date’ column to datetime format using pd.to_datetime(). Then it loops over each price column, removes the\n\ndollar sign from each value, and converts the column to float type. Finally,\n\nit prints out the data types of the columns again to confirm the changes.\n\n[End of response]\n\nLet’s try these suggestions above to convert our columns by running the\n\nbelow code:\n\n# Convert 'Date' to datetime df['Date'] = pd.to_datetime(df['Date'])\n\n# Remove $ from price columns and convert to float price_columns = [' Close/Last', ' Open', ' High', ' Low'] for col in price_columns: df[col] = df[col].str.replace('$', '').astype(float)\n\n# Check the data types again print(df.dtypes)\n\nYou should see the following output, which is the columns correctly\n\nconverted to suitable types instead of being strings:\n\nDate datetime64[ns] Close/Last float64 Volume int64 Open float64 High float64 Low float64 dtype: object\n\nNext, let’s tell Copilot about the result to ensure it tells us what to do next.\n\n[Prompt]\n\nBelow is the output, Date datetime64[ns] Close/Last float64 Volume int64\n\nOpen float64 High float64 Low float64 dtype: object\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now your ‘Date’ column is of type ‘datetime64[ns]’ and your price columns (‘Close/Last’, ‘Open’, ‘High’, ‘Low’) are of type ‘float64’. This\n\nmeans you can now perform numerical operations on your price columns\n\nand time series analysis on your ‘Date’ column.\n\nThe next steps could be to perform exploratory data analysis, such as checking for missing values, visualizing the distribution of your variables,\n\nor examining the correlation between different variables. You might also\n\nwant to split your data into a training set and a test set in preparation for building your regression model.\n\n[End of response]\n\nExploratory data analysis Guided by the output on what to do next, we decide to check the missing\n\nvalues in our dataset:\n\n[Prompt]\n\nCheck missing values.\n\n[End of response]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo check for missing values in your DataFrame, you can use the isnull()\n\nfunction along with the sum() function in pandas. Here’s how you can do it:\n\n# Check for missing values print(df.isnull().sum())\n\nThis will print out the number of missing values in each column of your DataFrame. If a column has zero missing values, that means there are no missing values in that column. If a column has a number greater than zero,\n\nthat’s the number of missing values in that column.\n\n[End of response]\n\nThe response from Copilot says to check for null values and provides code\n\nfor how to do so. Let’s run the suggested code in a code cell in a Notebook:\n\n# Check for missing values print(df.isnull().sum())\n\nHere’s the output:\n\nDate 0 Close/Last 0 Volume 0 Open 0 High 0 Low 0 dtype: int64\n\nWith this output, provide it to Copilot to see what to do next:\n\n[Prompt]\n\nBelow is the output, Date 0 Close/Last 0 Volume 0 Open 0 High 0 Low 0\n\ndtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output indicates that there are no missing values in your dataset.\n\nEach column has 0 missing values. This is good news because it means you\n\ndon’t have to handle missing data, which can be a complex process.\n\nThe next step in your analysis could be to perform some exploratory data\n\nanalysis (EDA). This might include visualizing your data, checking for\n\noutliers, or examining the relationships between variables. After that, you\n\ncan proceed to split your data into a training set and a test set, and then\n\nbuild your regression model.\n\n[End of response]\n\nGreat, we had no missing values and it seems like the next steps should be visual analysis and checking for outliers. With that knowledge, let’s craft an\n\nappropriate prompt.\n\n[Prompt]\n\nVisualize data and check for outliers.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nVisualizing the data can be done using various libraries in Python, such as\n\nMatplotlib or Seaborn. For checking outliers, boxplots can be very useful.\n\nHere’s how you can create boxplots for your price columns:\n\nimport matplotlib.pyplot as plt import seaborn as sns\n\n# Set up the matplotlib figure f, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot a simple histogram with binsize determined automatically sns.boxplot(data=df[' Close/Last'], ax=axes[0, 0]) axes[0, 0].set_title('Close/Last')\n\nsns.boxplot(data=df[' Open'], ax=axes[0, 1]) axes[0, 1].set_title('Open')\n\nsns.boxplot(data=df[' High'], ax=axes[1, 0]) axes[1, 0].set_title('High')\n\nsns.boxplot(data=df[' Low'], ax=axes[1, 1]) axes[1, 1].set_title('Low')\n\nplt.tight_layout()\n\nThis code will create a 2x2 grid of plots, with each plot being a boxplot of\n\none of your price columns. Boxplots are useful for visualizing the\n\ndistribution of data and identifying potential outliers. In a boxplot, an\n\noutlier is typically represented as a point that is located outside the whiskers\n\nof the boxplot.\n\n[End of response]\n\nThe code above suggests using the matplotlib and seaborn libraries and also\n\ncontains code to create a boxplot visualization; let’s run said code:\n\nimport matplotlib.pyplot as plt import seaborn as sns\n\n# Set up the matplotlib figure f, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot a simple histogram with binsize determined automatically sns.boxplot(data=df[' Close/Last'], ax=axes[0, 0]) axes[0, 0].set_title('Close/Last')\n\nsns.boxplot(data=df[' Open'], ax=axes[0, 1]) axes[0, 1].set_title('Open')\n\nsns.boxplot(data=df[' High'], ax=axes[1, 0]) axes[1, 0].set_title('High')\n\nsns.boxplot(data=df[' Low'], ax=axes[1, 1]) axes[1, 1].set_title('Low')\n\nplt.tight_layout()\n\nYou should see results similar to the following boxplots:\n\nFigure 18.1: Boxplots of our dataset\n\nNext, we want to learn more about our variables and their relationship.\n\n[Prompt]\n\nExamine the relationships between variables.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo examine the relationships between variables, you can use a correlation\n\nmatrix and a heatmap. The correlation matrix will give you numerical values that represent how closely related two variables are, while the\n\nheatmap will give you a visual representation of these relationships.\n\nHere’s how you can do it:\n\n# Calculate the correlation matrix corr = df.corr()\n\n# Plot the heatmap sns.heatmap(corr, annot=True, cmap='coolwarm')\n\nplt.show()\n\nThis code first calculates the correlation matrix using the corr() function, which returns a DataFrame where each cell represents the correlation\n\ncoefficient between two variables. Then it plots a heatmap using this correlation matrix. The annot=True argument allows the correlation coefficients to be plotted on the heatmap, and the cmap='coolwarm'\n\nargument sets the color scheme of the heatmap.\n\nThe correlation coefficient ranges from -1 to 1. A value close to 1 implies a\n\nhigh positive correlation while a value close to -1 implies a high negative correlation. A value close to 0 implies no correlation.\n\n[End of response]\n\nThe response says we should create a correlation matrix and visualize it via\n\na heatmap. Let’s try the suggested code above and see what we get:\n\n# Calculate the correlation matrix corr = df.corr()\n\n# Plot the heatmap sns.heatmap(corr, annot=True, cmap='coolwarm')\n\nplt.show()\n\nHere’s the output from the code above:\n\nFigure 18.2: Heatmap\n\nIn the heatmap, we can see:\n\nThe diagonal is always 1 because it shows the correlation of each\n\nvariable with itself.\n\nClose/Last, Open, High, and Low have a high positive correlation with\n\neach other (0.92), indicating that as one of these variables increases,\n\nthe others tend to also increase. This is typical for stock data, as these\n\nprices move closely together throughout the trading day.\n\nVolume shows a negative correlation with Close/Last, Open, High, and\n\nLow (-0.61). This suggests that on days when the volume of shares\n\ntraded is higher, the prices may tend to be lower, or vice versa.\n\nAs shown in the screenshot below, Copilot recommends the next step\n\nfollowing the analysis of variable relationships. We will choose this and see\n\nwhat it says.\n\nLet’s ask Copilot what to do next.\n\n[Prompt]\n\nWhat is the next step after examining the relationships between variables?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThe next step after examining the relationships between variables is\n\ntypically to prepare your data for modeling. This can involve several sub-\n\nsteps:\n\nFeature Selection: Based on your correlation analysis, you might decide to drop some features that are not correlated with your target\n\nvariable, or to avoid multicollinearity, drop one of two features that are\n\nhighly correlated with each other.\n\nData Splitting: You’ll need to split your data into a training set and a\n\ntest set. A common split is 80% of the data for training and 20% for\n\ntesting.\n\nData Scaling: Depending on the model you plan to use, you might need to scale your data so that all features have a similar range of\n\nvalues.\n\nOnce your data is prepared, you can proceed to build your regression\n\nmodel.\n\n[End of response]\n\nData splitting\n\nAt this point, let’s prepare for the creation of the model. Before we can\n\ncreate the model, we need to split up our data into training data and test sets\n\nto verify the model later.\n\n[Prompt]\n\nLet’s move on to the step of splitting the data.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAbsolutely, splitting the data into a training set and a test set is a crucial\n\nstep in preparing your data for modeling. We’ll use the train_test_split\n\nfunction from sklearn.model_selection to do this.\n\nAssuming that we are going to predict the ‘Close/Last’ price, here’s how\n\nyou can split your data:\n\nfrom sklearn.model_selection import train_test_split\n\n# Define the feature variables and the target variable X = df.drop(' Close/Last', axis=1) y = df[' Close/Last']\n\n# Split the data into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\nThis code first defines the feature variables X and the target variable y. Then it splits these into a training set and a test set, with 80% of the data going to the training set and 20% going to the test set. The random_state\n\nparameter is used for reproducibility - using the same random_state will ensure that you get the same train/test split every time you run the code.\n\n[End of response]\n\nIn short, the code above shows us how to split up our dataset; let’s try this\n\ncode next:\n\nfrom sklearn.model_selection import train_test_split\n\n# Define the feature variables and the target variable X = df.drop(' Close/Last', axis=1) y = df[' Close/Last']\n\n# Split the data into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\nAt this point, we don’t have an output to show as we assigned the results to\n\nvariables. Let’s, however, print that next.\n\n[Prompt]\n\nPrint the shape of samples.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo print the shape of your training and test samples, you can use the shape\n\nattribute. Here’s how you can do it:\n\n# Print the shape of the training set and the test set print(\"Training set (X):\", X_train.shape) print(\"Training set (y):\", y_train.shape) print(\"Test set (X):\", X_test.shape) print(\"Test set (y):\", y_test.shape)\n\nThis will print out the number of rows and columns in your training set and test set. The number of rows should be the same between X_train and\n\ny_train, and between X_test and y_test. The number of columns in X_train and X_test should be the same, and y_train and y_test should\n\neach have just one column (since they represent the target variable).\n\n[End of response]\n\nThe suggested code is straightforward, as it consists of a set of print\n\nstatements. Let’s try said code by running it in a code cell:\n\n# Print the shape of the training set and the test set print(\"Training set (X):\", X_train.shape) print(\"Training set (y):\", y_train.shape) print(\"Test set (X):\", X_test.shape) print(\"Test set (y):\", y_test.shape)\n\nYou should see output similar to the below:\n\nTraining set (X): (2014, 5) Training set (y): (2014,) Test set (X): (504, 5) Test set (y): (504,)\n\nBuild a regression model Now that we’ve split up the data, let’s train the model next, or at least that’s\n\nwhat we expect Copilot to say if we ask it for the next step. Let’s create a\n\nprompt asking just that:\n\n[Prompt]\n\nWhat is the next step after splitting the data?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAfter splitting the data, the next step is to create and train your regression\n\nmodel using the training data. Depending on the nature of your data and the\n\nproblem you’re trying to solve, you might choose a simple linear regression\n\nmodel, or a more complex model like polynomial regression or ridge regression.\n\nOnce the model is trained, you can use it to make predictions on the test\n\ndata. Then, you evaluate the performance of the model by comparing these\n\npredictions to the actual values in the test data. Common metrics for evaluating regression models include Mean Absolute Error (MAE), Mean\n\nSquared Error (MSE), and R-squared.\n\n[End of response]\n\nOkay, just as we thought, the next step is to train the model. Let’s create a\n\nprompt asking for more detailed information on how to train the model:\n\n[Prompt]\n\nCreate and train a regression model.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s create and train a simple Linear Regression model using the training data. We’ll use the LinearRegression class from\n\nsklearn.linear_model to do this.\n\nHere’s how you can create and train your model:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear Regression model model = LinearRegression()\n\n# Train the model model.fit(X_train, y_train)\n\nThis code first creates a Linear Regression model. Then it trains the model\n\nusing the fit method and the training data. After this step, your model is\n\nready to make predictions on unseen data.\n\nPlease note that if your data has a non-linear relationship, you might need to\n\nuse a more complex model like Polynomial Regression or a tree-based\n\nmodel. Also, if your features are not on the same scale, you might need to\n\nscale your data before training the model.\n\n[End of response]\n\nWe’ve received a response on using linear regression and the sklearn\n\nlibrary. Note that you might need to install said library before proceeding.\n\nOnce you’ve ensured sklearn is installed, proceed to run the suggested code below:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear Regression model model = LinearRegression()\n\n# Train the model model.fit(X_train, y_train)\n\nHere’s the output from running the code:\n\n--------------------------------------------------------------- DTypePromotionError Traceback (most recen\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b 1145 estimator._validate_params() 1147 with config_context( 1148 skip_parameter_validation=( 1149 prefer_skip_nested_validation or global_skip_va 1150 ) 1151 ): -> 1152 return fit_method(estimator, *args, **kwargs)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/l 674 n_jobs_ = self.n_jobs 676 accept_sparse = False if self.positive else [\"csr\", \"cs --> 678 X, y = self._validate_data( 679 X, y, accept_sparse=accept_sparse, y_numeric=True, 680 ) 682 has_sw = sample_weight is not None 683 if has_sw:\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b 620 y = check_array(y, input_name=\"y\", **check_y_pa 621 else: --> 622 X, y = check_X_y(X, y, **check_params) 623 out = X, y 625 if not no_val_X and check_params.get(\"ensure_2d\", True)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/u 1141 estimator_name = _check_estimator_name(estimato 1142 raise ValueError( 1143 f\"{estimator_name} requires y to be passed, but 1144 ) -> 1146 X = check_array( 1147 X, 1148 accept_sparse=accept_sparse, 1149 accept_large_sparse=accept_large_sparse, 1150 dtype=dtype, 1151 order=order,\n\n1152 copy=copy, 1153 force_all_finite=force_all_finite, 1154 ensure_2d=ensure_2d, 1155 allow_nd=allow_nd, 1156 ensure_min_samples=ensure_min_samples, 1157 ensure_min_features=ensure_min_features, 1158 estimator=estimator, 1159 input_name=\"X\", 1160 ) 1162 y = _check_y(y, multi_output=multi_output, y_numeric=y_ 1164 check_consistent_length(X, y)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/u 791 pandas_requires_conversion = any( 792 _pandas_dtype_needs_early_conversion(i) for i in dt 793 ) 794 if all(isinstance(dtype_iter, np.dtype) for dtype_iter --> 795 dtype_orig = np.result_type(*dtypes_orig) 796 elif pandas_requires_conversion and any(d == object for 797 # Force object if any of the dtypes is an object 798 dtype_orig = object\n\nDTypePromotionError: The DType <class 'numpy.dtypes.DateTime64D 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DTyp\n\nWe’re clearly getting an error back, so let’s see if Copilot can help with this\n\nerror. As usual, include context like the error text to get a good response.\n\n[Prompt]\n\nI am getting this error after running the code,\n\nCell 39 line 7 4 model = LinearRegression() 6 # Train the model File ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b File ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/l DTypePromotionError: The DType <class 'numpy.dtypes.DateTime64D\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThe error message suggests that the model is having trouble handling the\n\n‘Date’ column because it’s in datetime format, and the model expects\n\nnumerical input.\n\nOne common approach to deal with datetime data is to extract features from\n\nit, such as the year, month, day, etc. Here’s how you can do it:\n\n# Extract features from 'Date' X_train['Year'] = X_train['Date'].dt.year X_train['Month'] = X_train['Date'].dt.month X_train['Day'] = X_train['Date'].dt.day\n\n# Drop the original 'Date' column X_train = X_train.drop('Date', axis=1)\n\n# Do the same for the test set X_test['Year'] = X_test['Date'].dt.year X_test['Month'] = X_test['Date'].dt.month X_test['Day'] = X_test['Date'].dt.day X_test = X_test.drop('Date', axis=1)\n\n# Now try to fit the model again model.fit(X_train, y_train)\n\nThis code first extracts the year, month, and day from the ‘Date’ column\n\nand adds them as new columns in the DataFrame. Then it drops the original\n\n‘Date’ column. Finally, it tries to fit the model again.\n\nPlease note that this is a simple approach and might not be the best way to\n\nhandle datetime data for your specific problem. The best approach depends\n\non the nature of your data and the problem you’re trying to solve.",
      "page_number": 667
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 690-711)",
      "start_page": 690,
      "end_page": 711,
      "detection_method": "synthetic",
      "content": "[End of response]\n\nSo, our Date column was the problem. The model training can only handle\n\nnumerical data. It seems the response suggested we should extract features from the Date column though, like year, month, and day, to preserve the info, but in numerical form. Additionally, we should then drop the Date\n\ncolumn as we got what we wanted from it.\n\nLet’s try running the suggested code:\n\n# Extract features from 'Date' X_train['Year'] = X_train['Date'].dt.year X_train['Month'] = X_train['Date'].dt.month X_train['Day'] = X_train['Date'].dt.day\n\n# Drop the original 'Date' column X_train = X_train.drop('Date', axis=1)\n\n# Do the same for the test set X_test['Year'] = X_test['Date'].dt.year X_test['Month'] = X_test['Date'].dt.month X_test['Day'] = X_test['Date'].dt.day X_test = X_test.drop('Date', axis=1)\n\n# Now try to fit the model again model.fit(X_train, y_train) LinearRegression()\n\nYou should now see your code running without any issues. Let’s tell\n\nCopilot about this fact and see if it suggests how we can proceed.\n\n[Prompt]\n\nThanks, that works.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nYou’re welcome! I’m glad to hear that it worked. Now that your model is\n\ntrained, the next step is usually to use the model to make predictions on\n\nyour test data. After that, you can evaluate the performance of your model\n\nby comparing these predictions to the actual values in your test data. If you\n\nhave any more questions or need further assistance, feel free to ask.\n\n[End of response]\n\nEvaluate the model As suggested by the response, Copilot recommends asking the following:\n\nWhat are some common metrics for evaluating regression models? Let’s\n\nselect this and see what it says.\n\nEvaluation metrics Let’s ask Copilot about evaluation metrics.\n\n[Prompt]\n\nWhat are some common metrics for evaluating regression models?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThere are several common metrics for evaluating the performance of\n\nregression models:\n\nMean Absolute Error (MAE): This is the average of the absolute\n\ndifferences between the predicted and actual values. It gives an idea of\n\nhow wrong the predictions were.\n\nMean Squared Error (MSE): This is the average of the squared\n\ndifferences between the predicted and actual values. Squaring the\n\ndifferences amplifies the impact of large errors.\n\nRoot Mean Squared Error (RMSE): This is the square root of the\n\nMSE. It has the same units as the output variable, making it easier to\n\ninterpret than the MSE.\n\nR-squared (R²): This is the proportion of the variance in the\n\ndependent variable that is predictable from the independent variables.\n\nIt provides a measure of how well the model’s predictions fit the actual\n\ndata. A value of 1 means the model perfectly predicts the actual values,\n\nwhile a value of 0 means the model does not predict the actual values\n\nat all.\n\nAdjusted R-squared: This is a modified version of R-squared that has\n\nbeen adjusted for the number of predictors in the model. It increases\n\nonly if the new term improves the model more than would be expected\n\nby chance.\n\nEach of these metrics has its own strengths and weaknesses, and the choice\n\nof which to use depends on the specific problem and the goals of the\n\nanalysis.\n\n[End of response]\n\nNow that we know how to evaluate and with what metrics, let’s make\n\npredictions next given our testing data, and see what we get in response.\n\n[Prompt]\n\nMake predictions and evaluate the model using different metrics.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s use the trained model to make predictions on the test data. Then,\n\nwe’ll evaluate these predictions using Mean Absolute Error (MAE), Mean\n\nSquared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²). We’ll use the metrics module from sklearn to calculate these metrics.\n\nHere’s how you can do it:\n\nfrom sklearn import metrics import numpy as np\n\n# Make predictions on the test data y_pred = model.predict(X_test)\n\n# Calculate MAE, MSE, RMSE, and R2 mae = metrics.mean_absolute_error(y_test, y_pred) mse = metrics.mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) # or metrics.mean_squared_error(y_test, y_p r2 = metrics.r2_score(y_test, y_pred)\n\n# Print the metrics print(\"Mean Absolute Error (MAE):\", mae) print(\"Mean Squared Error (MSE):\", mse) print(\"Root Mean Squared Error (RMSE):\", rmse) print(\"R-squared (R2 ):\", r2)\n\nThis code first uses the model to make predictions on the test data. Then it calculates the MAE, MSE, RMSE, and R² by comparing these predictions to the actual values in the test data. Finally, it prints out these metrics.\n\n[End of response]\n\nThe code contains both metrics and calls a predict() function that\n\nproduces a response from the model; let’s try this and see if it works.\n\nfrom sklearn import metrics import numpy as np\n\n# Make predictions on the test data y_pred = model.predict(X_test)\n\n# Calculate MAE, MSE, RMSE, and R2 mae = metrics.mean_absolute_error(y_test, y_pred) mse = metrics.mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) # or metrics.mean_squared_error(y_test, y_p r2 = metrics.r2_score(y_test, y_pred)\n\n# Print the metrics print(\"Mean Absolute Error (MAE):\", mae) print(\"Mean Squared Error (MSE):\", mse) print(\"Root Mean Squared Error (RMSE):\", rmse) print(\"R-squared (R2 ):\", r2)\n\nYou should see a response similar to the one below:\n\nMean Absolute Error (MAE): 0.458844690389933 Mean Squared Error (MSE): 0.4236006011305136 Root Mean Squared Error (RMSE): 0.6508460656180642 R-squared (R2 ): 0.999868004623502\n\nLet’s provide Copilot with the response below to learn what we should do\n\nnext.\n\n[Prompt]\n\nBelow is the output, Mean Absolute Error (MAE): 0.4588446903899\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output shows the calculated metrics for your regression model.\n\nMean Absolute Error (MAE): The average absolute difference\n\nbetween the predicted and actual values is approximately 0.46. This means that on average, your predictions are about 0.46 units away\n\nfrom the actual values.\n\nMean Squared Error (MSE): The average squared difference between the predicted and actual values is approximately 0.42. This\n\nmetric is more sensitive to large errors than MAE because it squares\n\nthe differences.\n\nRoot Mean Squared Error (RMSE): The square root of the MSE is approximately 0.65. This metric is in the same units as the output\n\nvariable, making it easier to interpret than the MSE.\n\nR-squared (R²): The proportion of the variance in the dependent variable that is predictable from the independent variables is\n\napproximately 0.9999. This is very close to 1, which indicates that\n\nyour model explains almost all of the variability of the response data\n\naround its mean.\n\nOverall, these metrics suggest that your model is performing very well on\n\nthe test data.\n\n[End of response]\n\nNext, we want to have a sense of how well the model is performing by\n\ncomparing predicted values to actual values, so let’s create a prompt to\n\nreflect that request.\n\n[Prompt]\n\nDisplay a comparison of predicted values versus actual values.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, you can compare the predicted values with the actual values by\n\nvisualizing them in a plot. Here’s how you can do it using matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Plot the actual values plt.scatter(X_test.index, y_test, color='blue', label='Actual')\n\n# Plot the predicted values plt.scatter(X_test.index, y_pred, color='red', label='Predicted\n\nplt.title('Actual vs Predicted') plt.xlabel('Index') plt.ylabel('Target') plt.legend() plt.show()\n\nThis code creates a scatter plot where the x-axis represents the index of the\n\ntest set and the y-axis represents the target variable. The actual values are\n\nplotted in blue and the predicted values are plotted in red. This allows you\n\nto visually compare the predictions made by the model to the actual values.\n\n[End of response]\n\nThe suggested code seems to want to create a scatter plot to better compare\n\nour predicted values compared to our test data:\n\nimport matplotlib.pyplot as plt\n\n# Plot the actual values plt.scatter(X_test.index, y_test, color='blue', label='Actual')\n\n# Plot the predicted values plt.scatter(X_test.index, y_pred, color='red', label='Predicted\n\nplt.title('Actual vs Predicted') plt.xlabel('Index') plt.ylabel('Target') plt.legend() plt.show()\n\nHere’s the output from running the above code:\n\nFigure 18.3: Actual versus predicted data\n\nWe can see how the predicted data matches the actual data quite well. Let’s\n\ntry to sum up our learning next.\n\nAssignment Instead of using regression to predict a value, let’s take the same data but\n\ntreat it as a classification problem and predict whether the stock price will\n\ngo up or down the next day.\n\nSummary Regression is a fundamental concept in machine learning used to predict a\n\ncontinuous outcome variable based on one or more predictor variables. It\n\ninvolves identifying the relationship between a dependent variable (often\n\ncalled the target) and one or more independent variables (features). We saw\n\nthat, given our dataset, we were able to find correlations for certain\n\nvariables. We also found that we could include columns like Date, but to\n\ninclude these, we needed to extract the important numerical parts from\n\nthose columns, namely the year, month, and date.\n\nRegression has many applications in other sectors, like healthcare and\n\nmarketing. From a prompt perspective, it’s a good idea to set the context\n\nearly on and show Copilot the shape of the data, which will then help you\n\nask Copilot what to do next.\n\nIn the next chapter, we will use the same dataset while using GitHub\n\nCopilot to help us write some code.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n19\n\nRegression with Copilot Suggestions\n\nIntroduction In the previous chapter, we used GitHub Copilot Chat to build a regression\n\nproblem and explored how AI can assist in coding. In this chapter, we’ll\n\ntake a different approach. We will write code with the help of GitHub\n\nCopilot, allowing it to guide us through coding and adding helpful\n\ncomments. This will be an interactive experience, combining our coding\n\nskills with Copilot’s suggestions to effectively tackle a regression problem.\n\nLet’s see how GitHub Copilot can enhance our coding process in real time.\n\nIn the task, we will use the Apple dataset to predict stock prices and help\n\ninvestors decide whether to buy or not. This is the same dataset we used in\n\nChapter 18, Regression with Copilot Chat, where we used Copilot Chat to\n\nanalyze it.\n\nDataset overview This dataset provides us with a wealth of information about Apple’s stock\n\n(traded under AAPL) over the past decade, starting from the year 2010. This\n\ndata is incredibly valuable because it can help us develop forecasting\n\nalgorithms to predict the future price of Apple’s stock, which is crucial for\n\nmaking investment decisions. The data in this set has been collected and\n\naggregated from 25 different stock exchanges.\n\nTo effectively use this data for forecasting, we need to understand the key\n\nelements: the features that influence our target, which is predicting stock\n\nprices.\n\nThe dataset includes five important values that indicate how the stock price\n\nchanges over a specific period of time, which is typically one day, but it could also be one week or one month. These values are:\n\nOpen: This is the stock price at the beginning of the trading day.\n\nClose: This is the stock price at the end of the trading day.\n\nHigh: This value shows the highest price the stock reached during the\n\ntrading day.\n\nLow: This indicates the lowest price the stock hit during the trading\n\nday.\n\nVolume: This is the total number of shares that were traded throughout the day before the market closed.\n\nOur focus will be on using historical price data, which includes these five\n\nrecorded values, along with the stock’s past performance, to make\n\npredictions about its future prices.\n\nPrompt strategy In this chapter, we’ll use the “Exploratory prompt pattern” as described in\n\nChapter 2. We feel pretty confident on what actions we want to take in general but is interested in what the AI assistant will generate in case we need to pivot.\n\nStart coding with Copilot’s help With GitHub Copilot installed, you’re ready to start coding your regression\n\nproblem in Python. The approach of using suggestions in an open file is a bit different from using the chat window. The main difference is that we\n\nwill rely on typing comments and use the Tab key to get Copilot to provide us with suggestions. This technique works equally well in Jupyter Notebook\n\nand normal text files. Here’s how it works:\n\n# generate import statements for libraries to use with a Regres\n\nHere, we have an open text file. We’ve added comments to generate import statements. To get Copilot to generate a response, we need to use the Return key followed by the Tab key to accept the suggestion. Here’s how the above\n\ntext file could change:\n\n# generate import statements for libraries to use with a Regres import pandas import numpy import scikit-learn\n\nThe import statement is what is generated by Copilot when pressing Tab\n\nfollowed by Return.\n\nThe AI assistant helps by providing code completion, but remember to review and understand each suggestion before\n\naccepting it.\n\nStep 1: Import libraries with Copilot’s assistance We could start with a prompt suggestion comment that describes the\n\nproblem upfront, the dataset, and more, just like we did with Copilot Chat\n\nin other chapters in this book. Then, the prompt could look like so:\n\n# generate the steps I need to follow for performing a Regressi\n\nHowever, we will try an approach where we ask for everything we need.\n\nLet’s begin. Let’s create our first comment (this is equivalent to typing in a chat window):\n\n[Prompt]\n\n# import pandas\n\n[End of prompt]\n\nAccept the suggestion with Tab. You should see the following bold code:\n\n[Prompt response]\n\n# import pandas import pandas as pd\n\n[End of response]\n\nGreat! We have imported the libraries we need. Let’s load and explore the\n\ndataset next.\n\nStep 2: Load and explore the dataset In the next step, we want to load the dataset and explore the data features.\n\nKeep working in the same file and add the below prompt:\n\n[Prompt]\n\n# Load the dataset into a pandas DataFrame\n\n[End of prompt]\n\nAt this point, Copilot understands what we’re trying to do, so we don’t need\n\nto type more than the above prompt comment. So, we just need to press Return followed by Tab a number of times, and we should get the content\n\nbelow:\n\n[Prompt response]\n\n# Load the dataset into a pandas DataFrame df = pd.read_csv('enter-your-file-path')\n\n# Print the first 5 rows of the DataFrame print(df.head())\n\n[End of response]\n\nEnsure you have downloaded the dataset from Kaggle (https://www.kaggle.com/datasets/varpit9\n\n4/apple-stock-data-updated-till-\n\n22jun2021) and that you have replaced the path in read_csv to point to where the file is in your system.\n\nIf you run the suggested code, you should see similar output to:\n\nDate Close Volume Open High Lo 0 02/28/2020 $273.36 106721200 $257.26 $278.41 $256.3 1 02/27/2020 $273.52 80151380 $281.1 $286 $272.9 2 02/26/2020 $292.65 49678430 $286.53 $297.88 $286. 3 02/25/2020 $288.08 57668360 $300.95 $302.53 $286.1 4 02/24/2020 $298.18 55548830 $297.26 $304.18 $289.2\n\nIf you see the above, that means the data was loaded successfully.\n\nGet types and columns Great! Now that we’ve successfully loaded the data, let’s learn more about\n\nthe loaded dataset, such as what columns and column types it has. As usual,\n\nlet’s keep working on the same file and add a prompt as a comment like so:\n\n[Prompt]\n\n# Get the data types of the columns in the dataset\n\n[End of prompt]\n\nYou should see the below results:\n\n[Prompt response]\n\n# Get the data types of the columns in the dataset print(df.dtypes)\n\n[End of response]\n\nNow, let’s run the code. It should show our columns and their data types:\n\nDate object Close object Volume int64 Open object High object Low object dtype: object\n\nGreat! We see the columns, but many of the columns have the object type.\n\nThat’s not good; they need to be numerical to be used in machine learning\n\ntraining. Let’s see if we can address this shortly, but first, let’s learn more about the shape of the dataset.\n\nShape of the dataset Let’s learn more about our dataset in terms of structure, like how many\n\ncolumns and how many rows it consists of. This will help us assess its\n\ncomplexity, see how much data we have, and whether it’s enough to train a\n\nmodel to be reliable. Let’s write our next prompt comment:\n\n[Prompt]\n\n# Get the shape of the dataset\n\n[End of prompt]\n\nYou should see the below suggestion:\n\n[Prompt response]\n\n# Get the shape of the dataset print(df.shape)\n\n[End of response]\n\nRunning the code above should provide the response below:\n\n(2518, 6)\n\nThis response tells us we have 2518 rows and 6 columns.\n\nAddressing the column types At this point, we know we need more numerical columns, so let’s deal with\n\nthe ones that aren’t numerical. First, let’s identify those non-numerical\n\ncolumns:\n\n[Prompt]\n\n# print the columns in the dataset\n\n[End of prompt]\n\nYou should see a suggestion like below:\n\n[Prompt response]\n\n# print the columns in the dataset print(df.columns) Index(['Date', 'Close', 'Volume', 'Open', 'High', 'Low'], dtype\n\n[End of response]\n\nIt’s interesting that although we just asked to print the columns, Copilot makes the decision to show us only the columns of the object type. It’s not\n\nreally surprising because Copilot has been trained on machine learning and\n\nis able to deduce what will happen next, which in this case is converting\n\ncolumns for use in model training.\n\nNext, let’s be a bit prescriptive and ask for columns to be converted to\n\nnumerical versions with the below prompt comment:\n\n[Prompt]\n\n# Replace the $ symbol with an empty string and convert the col\n\n[End of prompt]\n\nYou should see the below code, which turns several columns into numerical\n\nversions.\n\n[Prompt response]\n\n# Replace the $ symbol with an empty string and convert the col df['Open'] = df['Open'].replace({'\\$': ''}, regex=True).astype( df['Close'] = df['Close'].replace({'\\$': ''}, regex=True).astyp df['High'] = df['High'].replace({'\\$': ''}, regex=True).astype( df['Low'] = df['Low'].replace({'\\$': ''}, regex=True).astype(fl\n\n[End of response]\n\nGreat! Let’s inspect the columns next to ensure the conversion has been\n\ncarried out correctly.\n\n[Prompt]\n\n# Print the first 5 rows of the DataFrame to confirm the change\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# Print the first 5 rows of the DataFrame to confirm the change print(df.head())\n\n[End of response]\n\nIf you run the above code, you should see the response below:\n\nDate Close Volume Open High Low 0 02/28/2020 273.36 106721200 257.26 278.41 256.37 1 02/27/2020 273.52 80151380 281.10 286.00 272.96 2 02/26/2020 292.65 49678430 286.53 297.88 286.50 3 02/25/2020 288.08 57668360 300.95 302.53 286.13 4 02/24/2020 298.18 55548830 297.26 304.18 289.23\n\nWe can clearly see that the dollar signs have been removed and the columns\n\nare now numerical, save for the Date column, which we can deal with later.\n\nStatistical summary Next, let’s learn a bit more about the dataset by asking for a statistical\n\nsummary. We can expect to see these values:\n\nCount: The number of non-null entries\n\nMean: The average value\n\nStandard Deviation (std): The spread of the data\n\nMinimum (min): The smallest value\n\n25th Percentile (25%): The value below which 25% of the data falls\n\nMedian (50%): The middle value, also known as the 50th percentile\n\n75th Percentile (75%): The value below which 75% of the data falls\n\nMaximum (max): The largest value\n\nWith that in mind, let’s write a prompt comment:\n\n[Prompt]\n\n# get the statistical summary of the dataset\n\n[End of prompt]\n\nYou should see an output similar to this:\n\n[Prompt response]\n\n# get the statistical summary of the dataset print(df.describe())\n\n[End of response]\n\nIf you run the code, you will see data similar to this:\n\nClose Volume Open High count 2518.000000 2.518000e+03 2518.000000 2518.000000 251 mean 114.769522 7.258009e+07 114.728443 115.766415 11 std 60.662405 5.663113e+07 60.546893 61.134456 6 min 29.835700 1.136205e+07 29.392800 29.928600 2 25% 66.822475 3.053026e+07 66.877150 67.475300 6 50% 101.090000 5.295469e+07 101.115000 102.085000 10 75%\n\nCheck for missing values It’s crucial to identify missing values to assess the quality of your dataset.\n\nHigh numbers of null values might indicate issues with data collection or\n\nentry and may affect our model, so we need a way to handle them:\n\n[Prompt]\n\nCheck for missing values\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# check for missing values print(df.isnull().sum())\n\n[End of response]\n\nGreat! So, what does the code do? Well, let’s explain. Let’s say you have\n\nthe following dataset:\n\ndata = {'A': [7, 99, None, 3, None], 'B': [5, None, 6, 8, 9], 'C': [10, 15, 12, None, 11]}\n\nRunning the suggested code would summarize the number of missing\n\nvalues per column, and you would see a response like this:\n\nA: 2 B: 1 C: 1",
      "page_number": 690
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 712-733)",
      "start_page": 712,
      "end_page": 733,
      "detection_method": "synthetic",
      "content": "What you can do is replace the missing value with a value that doesn’t\n\naffect the overall score. One way to do so is to add the mean value and replace None with that.\n\nCheck for duplicates Another way to clean up the data is to remove duplicated data, because you\n\ndon’t want that data to create a reinforcing effect on the model. Let’s create\n\na comment for that and see if Copilot can help us:\n\n[Prompt]\n\nprint(df.duplicated().sum())\n\n[End of prompt]\n\nAt this point, Copilot understands what we’re trying to do, that is, identify\n\nand remove duplicates. So, you should see suggested code for both\n\noperations below:\n\n[Prompt response]\n\n# check for duplicates print(df.duplicated().sum())\n\n# drop duplicates df.drop_duplicates(inplace=True)\n\n[End of response]\n\nScale numerical features\n\nNext, we’re going to scale values, so they fall into a specific range, 0–1.\n\nThis type of preprocessing will have beneficial effects on a number of\n\nthings, such as algorithm performance, convergence speed, interpretability,\n\nand preventing a group of values from dominating. Let’s create a prompt for\n\nscaling features:\n\n[Prompt]\n\n# Scale numerical features\n\n[End of prompt]\n\nNote that when there’s more than one row in the response, you may need to\n\npress Return and Tab repeatedly. You should see output similar to this:\n\n[Prompt response]\n\n# Scale numerical features # import StandardScaler from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() df[['Open', 'Close', 'High', 'Low', 'Volume']] = scaler.fit_tra\n\n# Print the first 5 rows of the DataFrame to confirm the change print(df.head())\n\n[End of response]\n\nRunning the suggested code should result in the below output:\n\nDate Close Volume Open High Low 0 02/28/2020 2.614832 0.602988 2.354536 2.660953 2.375094 1 02/27/2020 2.617470 0.133721 2.748359 2.785130 2.651257\n\n2 02/26/2020 2.932884 -0.404481 2.838059 2.979494 2.876649 3 02/25/2020 2.857534 -0.263366 3.076269 3.055571 2.870490 4 02/24/2020 3.024063 -0.300800 3.015313 3.082566 2.922093\n\nVisualization Next, we want to visualize our data. We could give vague instructions on\n\nhow to do it, but in this case, a scatter plot is a good tool to use, so let’s\n\nreflect that in the prompt:\n\n[Prompt]\n\n# Create a scatter plot of the Open and Close columns\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\nimport matplotlib.pyplot as plt # Create a scatter plot of the Open and Close columns plt.scatter(df['Open'], df['Close']) plt.xlabel('Open') plt.ylabel('Close') plt.title('Relationship between Open and Close') plt.show()\n\n[End of response]\n\nRunning the code above should result in a scatter plot like this:\n\nFigure 19.1: Scatter plot showing the relation between the Open and Close columns\n\nStep 3: Split data into training and testing sets Before we can train the model, we need to split the data into training data\n\nand test data. Let’s create a prompt to help with this:\n\n[Prompt]\n\n“use train_test_split from scikit-learn to split the dataset”\n\n[End of prompt]\n\nYou should see an output similar to:\n\n[Prompt response]\n\n# Use train_test_split from scikit-learn to split the dataset from sklearn.model_selection import train_test_split\n\n# Drop the Date column from the DataFrame df = df.drop('Date', axis=1)\n\n# Split the dataset into training and testing sets X = df.drop('Close', axis=1) y = df['Close'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\n[End of response]\n\n# Print the shapes of the training and testing sets print('X_train shape:', X_train.shape) print('y_train shape:', y_train.shape) print('X_test shape:', X_test.shape) print('y_test shape:', y_test.shape)\n\nIf you run the code above, you should see a response similar to:\n\nX_train shape: (2014, 4) y_train shape: (2014,) X_test shape: (504, 4) y_test shape: (504,)\n\nAsking questions Just like in the chat experience of Copilot, it’s possible to ask questions on a\n\nspecific topic. If you prefix a question with “Q:”, Copilot understands it’s a\n\nquestion. Let’s try this to find out our next step:\n\n[Prompt]\n\nQ: what is the next step after splitting the data into training and testing sets?\n\n[End of prompt]\n\nYou should see a response similar to:\n\n[Prompt response]\n\n# Q: what is the next step after splitting the data into traini # A: train the model using the training set and test the model\n\n[End of response]\n\nStep 4: Build a regression problem The time has come to train the model. You could be pretty vague here and\n\njust ask Copilot to perform training. It’s recommended, however, to suggest\n\nthe type of training you want. In this case, we want linear regression, so\n\nlet’s create a prompt that includes that:\n\n[Prompt]\n\n“initialize a LinearRegression model”.\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# initialize a LinearRegression model # import LinearRegression from sklearn.linear_model import LinearRegression\n\n# Instantiate LinearRegression as lr lr = LinearRegression()\n\n[End Suggestion Response]\n\nGreat! We have some code that ensures we use linear regression. We\n\nhaven’t carried out any training, though, so that’s next.\n\nStep 5: Train the model With the type of training specified, let’s train the model next:\n\n[Prompt]\n\n“fit the model”\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# Fit lr to the training data lr.fit(X_train, y_train)\n\n# Predict the labels of the test set y_pred = lr.predict(X_test)\n\n[End of response]\n\nStep 6: Evaluate model performance\n\nGreat! We have trained the model, but we don’t know anything about how\n\nwell it performs, and that’s something we should address next:\n\n[Prompt]\n\nEvaluate the performance metrics like MSE or R-squared.\n\n[End of prompt]\n\nYou should see an output similar to this:\n\n[Prompt response]\n\n#evaluate the performance metrics like MSE or R-squared\n\nfrom sklearn.metrics import mean_squared_error\n\n# Calculate the Mean Squared Error (MSE) of the model on the te mean_squared_error = mean_squared_error(y_test, y_pred) print('Mean squared error:', mean_squared_error)\n\n# Calculate the Root Mean Squared Error (RMSE) of the model on root_mean_squared_error = np.sqrt(mean_squared_error) print('Root mean squared error:', root_mean_squared_error)\n\n[End of response]\n\nIf you run the above code, you should see a response like this:\n\nMSE: 0.00011569609998631774 RMSE: 0.010756212157926123\n\nThe output we get represents two different errors: mean squared error and\n\nroot mean squared error.\n\nA lower MSE indicates a better fit of the model to the data. In this case, the\n\nMSE is quite low, suggesting that the model’s predictions are very close to\n\nthe actual values.\n\nThe RMSE value is also very low, reinforcing that the model’s predictions\n\nare highly accurate. Great! Let’s go over this chapter’s assignment, and then\n\nsummarize what we have learned about using Copilot to help us with\n\nmachine learning.\n\nAssignment Try to solve this problem using a prompting approach where you provide a\n\nlot of information upfront. We suggest constructing a prompt like so:\n\n“Carry out regression on a dataset with the following shape:\n\nOpen: This is the stock price at the beginning of the trading day.\n\nClose: This represents the stock price at the end of the trading day.\n\nHigh: This value shows the highest price the stock reached during the\n\ntrading day.\n\nLow: This indicates the lowest price the stock hit during the trading\n\nday.\n\nVolume: This is the total number of shares that were traded throughout\n\nthe day before the market closed.\n\nSuggest all the steps from loading and pre-processing the data to training\n\nand evaluating the model. You must show code for each step.”\n\nThen see what the response is and try to run the suggested code snippet for\n\neach step. If you encounter any issues, indicate the error to Copilot with a question prompt like so:\n\n“Q: the below/above code doesn’t work, please fix”\n\nDon’t forget to press Return and Tab to accept the completion.\n\nSummary In this chapter, we wanted to use the suggestion feature of GitHub Copilot,\n\nmeaning we would type comments and use the Return and Tab keys to\n\nreceive suggestions from Copilot. There’s a bit of a trick to it because\n\nsometimes you need to repeatedly press Return and Tab to get the full\n\nresponse. It’s also an AI experience that’s well suited to whenever you\n\nactively write code. GitHub Copilot Chat also has a place. In fact, the two\n\ndifferent experiences complement one another; choose how much of each\n\napproach you want to use. Also, always test the code suggested by Copilot\n\nand ask Copilot to fix the code output if needed.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n20\n\nIncreasing Eﬃciency with GitHub Copilot\n\nIntroduction So far, you’ve been using the knowledge you were taught at the beginning\n\nof the book about GitHub Copilot and ChatGPT. This foundational\n\nknowledge was enough to teach you how to write prompts and accept them.\n\nIt was also enough to let you start work on creating solutions for machine\n\nlearning, data science, and web development. In the case of web\n\ndevelopment, you also discovered Copilot is an efficient tool when working\n\nwith existing code bases. In this chapter, we want to take your AI tool\n\nknowledge to the next level, as there are more features that you may want to\n\nleverage.\n\nThere are a lot of things that can be done to increase efficiency; you will see\n\nlater in the chapter how there are features within Copilot that let you scaffold files, and you will learn more about your workspace and even\n\nVisual Studio Code as an editor, which are all time-saving features. This\n\nchapter will cover some of the most important features.\n\nIn this chapter, we will:\n\nLearn how to use Copilot to generate code.\n\nUse Copilot commands to automate tasks, like generating a new\n\nproject.\n\nApply techniques to debug and troubleshoot code.\n\nReview and optimize code using Copilot.\n\nCode generation and automation At its core, Copilot is a code generator. It can generate text for you that is\n\neither part of documentation or source code.\n\nThere are two primary ways to generate code with Copilot:\n\nCopilot’s active editor via prompts as comments.\n\nCopilot Chat, which lets you type in a prompt.\n\nCopilot’s active editor When you’re in the active editor, Copilot can generate code. To generate\n\ncode, you need to write a prompt as a comment. There are different ways to\n\nadd comments to your programming language code. In JavaScript and C#, for example, you use //, while you would type <!-- if you’re in HTML.\n\nLet’s see an example of this:\n\n[Prompt]\n\nCreate a new function called add that takes two parameters.\n\n[End of prompt]\n\n//Create a new function called add that takes two parameters\n\nAbove, we have a prompt as a comment. Copilot looks at this prompt and\n\nattempts to generate code based on it. At this point, Copilot sends the generated code back as a “suggestion,” which you can accept using the Tab\n\nkey. Here’s what it looks like in the editor:\n\n// Create a new function called add that takes two parameters function add(num1, num2) { throw new Error('Function not implemented.'); }\n\nAt this point, you get no suggestions. You have a couple of options:\n\nYou can press the Return key to get more suggestions.\n\nStart typing part of the code you want to generate, and Copilot will suggest code based on what you type.\n\nCopilot Chat Copilot Chat is a way to interact with Copilot using a chat interface. To\n\nactivate Copilot Chat, click on the icon in the bottom-left corner of Visual Studio Code (or VS Code), which will open up a chat window.\n\nFigure 20.1: Showing the left menu in Visual Studio Code with the “Chat” icon from Copilot Chat\n\nUpon clicking the above chat icon, the below image will be shown.\n\nFigure 20.2: The image above shows the chat window that opens when you click the Chat icon in the\n\nleft menu in Visual Studio Code\n\nLet’s try to generate some code based on the same prompt as before. Let’s\n\nuse the input text field in the chat window and type in the prompt:\n\n[Prompt]\n\nCreate a new function called add that takes two parameters\n\n[End of prompt]\n\nThe output comes in two parts, a text instruction and a code suggestion.\n\nFirst, the text instruction:\n\n[Prompt response]\n\nSure, here’s an example of a function called add that takes two parameters\n\nand returns their sum:\n\nfunction add(a, b) { return a + b; }\n\n[End of response]\n\nIt even shows you an example of how to use the function:\n\nconst result = add(2, 3); console.log(result); // Output: 5\n\nCopilot commands If you start typing / in the chat window, you will see a list of commands\n\nthat you can use with Copilot.\n\nFigure 20.3: Available commands\n\nThere are quite a few available commands, and we will focus on a few of\n\nthem in this chapter.\n\nCreating a Notebook With this command, you can create a new notebook. A notebook is a place\n\nwhere you can store code snippets and text. To create a notebook, type in /newNotebook, and also provide a name for the notebook, for example,\n\nproducts.\n\nFigure 20.4: Creating a Notebook\n\nThe above image shows what’s displayed when you type the /newNotebook command; you’re asked to confirm the scaffolding of a notebook by\n\nclicking Create Notebook.\n\nThis will create a new notebook with commonly used code snippets for\n\nproducts. Let’s inspect the output next.\n\nYour notebook, at a high level, contains the following:\n\nimport, which contains all the imports needed to work with a dataset;\n\nhere’s what the content looks like:\n\nimport pandas as pd # Import Pandas library import numpy as np # Import NumPy library import matplotlib.pyplot as plt # Import Matplotlib library import seaborn as sns # Import Seaborn library for data vis\n\nLoad data, which contains code to load data from a CSV file (the CSV\n\nfile product_data.csv doesn’t exist, but Copilot assumes that you\n\nhave such a file; adjust this part of the code to fit the name of the CSV\n\nfile that you do have):\n\n# Load product data from CSV file into a Pandas DataFrame product_data = pd.read_csv('product_data.csv') # Load product data from JSON file into a Pandas DataFrame # product_data = pd.read_json('product_data.json') # Display the first 5 rows of the DataFrame print(product_data.head())\n\nExplore product data, which contains operations to inspect the data:\n\n# Use Pandas functions to explore the product data # Display the last 5 rows of the DataFrame print(product_data.tail()) # Display the summary information of the DataFrame print(product_data.info()) # Display the statistical summary of the DataFrame print(product_data.describe())\n\nFilter product data, which contains operations to filter the data:\n\n# Filter product data based on specific criteria using bool filtered_data = product_data[(product_data['category'] == ' # Display the filtered data print(filtered_data)\n\nSorting data, which contains sorting operations:\n\n# Sort the product data based on one or more columns using # Sort by price in ascending order product_data.sort_values(by='price', ascending=True, inplac\n\n# Sort by category in ascending order, then by price in des product_data.sort_values(by=['category', 'price'], ascendin # Display the sorted data print(product_data)\n\nAggregating data; often, you want to aggregate data to get a better\n\nunderstanding of it:\n\n# Group the product data by category and calculate the tota category_total_price = product_data.groupby('category')['p # Display the total price for each category print(category_total_price) # Group the product data by category and calculate the ave category_avg_price = product_data.groupby('category')['pri # Display the average price for each category print(category_avg_price) # Group the product data by category and calculate the num category_product_count = product_data.groupby('category')[ # Display the number of products in each category print(category_product_count)\n\nIn summary, that’s a lot of code you don’t have to write yourself. You can\n\nuse this notebook as a starting point to work with product data.\n\nCreating a project Another really useful command is being able to create a project. Usually,\n\nwe either create files by hand, which takes time, or we rely on CLI tools to\n\ncreate projects for us. With Copilot, you can create a project by typing in /new in the chat window. To use the command, type /new and then provide\n\na description for it, for example, React. This will create a new project with\n\na React project template. Let’s inspect the output:\n\nmy-react-app ├── public │ ├── index.html │ └── favicon.ico ├── src │ ├── App.js │ ├── index.js │ ├── components │ │ ├── Customer.js │ │ ├── Product.js │ │ ├── CustomerList.js │ │ └── ProductList.js │ ├── data │ │ ├── customers.json │ │ └── products.json │ └── styles │ ├── App.css │ ├── Customer.css │ ├── Product.css │ ├── CustomerList.css │ └── ProductList.css ├── package.json └── README.md\n\nThe files created above are typical of what’s expected for a React project. It contains:\n\npublic, which contains the index.html file that references the JavaScript file indicating the location of your app.\n\nsrc, which contains all the application files and is itself divided into\n\ndifferent folders for components, data, and CSS styles.\n\nAll of this constitutes a starting point. You might want to change files to fit\n\nyour liking.",
      "page_number": 712
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 734-753)",
      "start_page": 734,
      "end_page": 753,
      "detection_method": "synthetic",
      "content": "Debugging and troubleshooting Being able to debug and troubleshoot code is an important skill to have.\n\nWhen you develop code, it seldom does what you need it to do the first\n\ntime. You need to be able to debug and troubleshoot code to figure out\n\nwhat’s wrong with it. The same applies if you have a reported bug; you\n\nneed to be able to diagnose at what point it goes wrong.\n\nCopilot can help, and a good approach is using Copilot chat and asking it to\n\nadd support for debugging and troubleshooting.\n\nThe following is a piece of sample code you’ve been given. It’s a REST API in the framework Flask for Python. It’s working code, but it might not\n\nbe so easy to work with from a debugging and troubleshooting standpoint:\n\nfrom flask import Flask, jsonify, request import sqlite3 app = Flask(__name__) # Create a connection to the database conn = sqlite3.connect('products.db') c = conn.cursor() # Create a table for products if it doesn't exist c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): # Retrieve all products from the database c.execute('SELECT * FROM products') products = c.fetchall() # Convert the products to a JSON response response = [] for product in products:\n\nresponse.append({ 'id': product[0], 'name': product[1], 'price': product[2] }) return jsonify(response) @app.route('/products', methods=['POST']) def add_product(): # Retrieve the product data from the request body data = request.get_json() name = data['name'] price = data['price'] # Insert the product into the database c.execute('INSERT INTO products (name, price) VALUES (?, ?) conn.commit() # Return a success message return jsonify({'message': 'Product added successfully'}) if __name__ == '__main__': app.run(debug=True)\n\nThe above code works, but it doesn’t support debugging and\n\ntroubleshooting; let’s see how we can add support for that by using Copilot\n\nChat:\n\n[Prompt]\n\nAdd support for troubleshooting and debugging\n\n[End of prompt]\n\nCopilot chat answers us with both text and code:\n\n[Prompt response]\n\nSure, here’s an updated version of the Flask API that includes support for\n\ntroubleshooting and debugging:\n\nfrom flask import Flask, jsonify, request import sqlite3 app = Flask(__name__) # Create a connection to the database conn = sqlite3.connect('products.db') c = conn.cursor() # Create a table for products if it doesn't exist c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database c.execute('SELECT * FROM products') products = c.fetchall() # Convert the products to a JSON response response = [] for product in products: response.append({ 'id': product[0], 'name': product[1], 'price': product[2] }) return jsonify(response) except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while retri @app.route('/products', methods=['POST']) def add_product(): try: # Retrieve the product data from the request body data = request.get_json() name = data['name'] price = data['price'] # Insert the product into the database c.execute('INSERT INTO products (name, price) VALUES (? conn.commit() # Return a success message\n\nreturn jsonify({'message': 'Product added successfully' except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while addin if __name__ == '__main__': app.run(debug=True)\n\nThe improvements are:\n\nAdded a try/except block around the code in the get_products()\n\nfunction.\n\nAdded logger statements to log errors.\n\nAdded debug=True to the app.run() statement.\n\nThese changes will help us troubleshoot and debug the code.\n\n[End of response]\n\nLet’s look at how we can use Copilot to help us with code reviews and\n\noptimizations next.\n\nCode review and optimization techniques Right now, we have code that works, but it’s not optimized. For example,\n\nit’s usually a good idea to separate code into different files. Let’s see what\n\nCopilot suggests to improve the code.\n\n[Prompt]\n\nImprove the selected code\n\n[End of prompt]\n\nAs usual, we get the Copilot chat response in terms of text and code:\n\n[Prompt response]\n\nSure, here are some improvements you could make to the selected code:\n\nUse a context manager to handle the database connection and cursor.\n\nThis ensures that the connection is properly closed when the code is\n\nfinished executing, even if an exception occurs.\n\nUse a list comprehension to convert the products to a JSON response.\n\nThis is more concise and can be faster than using a for loop.\n\nUse the json parameter of the Response object to return a JSON\n\nresponse. This is more concise than using the jsonify function.\n\nUse the request.json attribute to retrieve the product data from the\n\nrequest body. This is a more concise and Pythonic way to do it.\n\nand the resulting code:\n\nfrom flask import Flask, Response, request import sqlite3 app = Flask(__name__) # Create a table for products if it doesn't exist with sqlite3.connect('products.db') as conn: c = conn.cursor() c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database with sqlite3.connect('products.db') as conn: conn.row_factory = sqlite3.Row c = conn.cursor() c.execute('SELECT * FROM products')\n\nproducts = c.fetchall() # Convert the products to a JSON response response = [{'id': product['id'], 'name': product['name return Response(response=response, status=200, mimetype except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return Response(response={'error': 'An error occurred w @app.route('/products', methods=['POST']) def add_product(): try: # Retrieve the product data from the request body data = request.json name = data['name'] price = data['price'] # Insert the product into the database with sqlite3.connect('products.db') as conn: c = conn.cursor() c.execute('INSERT INTO products (name, price) VALUE conn.commit() # Return a success message return Response(response={'message': 'Product added suc except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return Response(response={'error': 'An error occurred w if __name__ == '__main__': app.run(debug=True)\n\n[End of response]\n\nThe biggest change above was using a context manager to handle the\n\ndatabase connection and cursor. This ensures that the connection is properly\n\nclosed when the code has finished executing, even if an exception occurs.\n\nWe still want to separate the code into different files; let’s see what Copilot\n\nsuggests for that.\n\nLet’s start with a prompt:\n\n[Prompt]\n\nSeparate the code into separate files by responsibility\n\n[End of prompt]\n\nand the resulting code:\n\n[Prompt response]\n\n# app.py from flask import Flask from products import products_bp app = Flask(__name__) app.register_blueprint(products_bp) if __name__ == '__main__': app.run(debug=True) # database.py import sqlite3 def get_db(): db = sqlite3.connect('products.db') db.row_factory = sqlite3.Row return db # products.py from flask import Blueprint, jsonify, request from database import get_db products_bp = Blueprint('products', __name__) @products_bp.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database db = get_db() products = db.execute('SELECT * FROM products').fetchal # Convert the products to a JSON response response = [{'id': product['id'], 'name': product['name return jsonify(response) except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while retri @products_bp.route('/products', methods=['POST'])\n\ndef add_product(): try: # Retrieve the product data from the request body data = request.json.get('data', {}) name = data.get('name', '') price = data.get('price', 0) # Insert the product into the database db = get_db() db.execute('INSERT INTO products (name, price) VALUES ( db.commit() # Return a success message return jsonify({'message': 'Product added successfully' except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while addin\n\n[End of response]\n\nIf you receive any errors while running the code, consult\n\nCopilot, tell it there’s an error, and ask it to address the\n\nissue. Remember that Copilot is more than a code generator.\n\nWhat we’re seeing now is a separation of concerns. The app.py file is responsible for starting the application and registering the blueprint. The database.py file is responsible for handling the database connection. The\n\nproducts.py file is responsible for handling the products API. This code is something we can work with and maintain.\n\nThis code can be improved further by changing database.py to use a\n\ncontext manager:\n\nimport sqlite3 def get_db():\n\nwith sqlite3.connect('products.db') as db: db.row_factory = sqlite3.Row return db def init_db(): with sqlite3.connect('products.db') as db: db.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') db.commit() # other code omitted for brevity\n\nHere, you saw how Copilot can review your code in steps, splitting a single\n\nfile into multiple files, and then improving the code.\n\nWorkspace Your workspace is all the files you have open in a directory. You want your\n\nAI assistant to consider all these files when giving you a recommendation.\n\nLet’s look at the following web project where the following files exist:\n\nsrc/ app.py utils.py\n\napp.py contains an API and utils.py contains an email validation\n\nfunction. Here’s the code for app.py:\n\n# create a web api using Flask, should have a products GET rout from flask import Flask, jsonify app = Flask(__name__) @app.route('/customers, methods=['GET']) def get_customers(): return jsonify({'customers': ['item1', 'item2', 'item3']})\n\n# TODO, create a POST route for products # Run the app if __name__ == '__main__': app.run(debug=True)\n\nNote the TODO comment; we’re about to add a route to store a customer. When storing this customer, we need to ensure that the customer data contains a valid email. Luckily, the utils.py file already contains such a\n\nfunction, but can we get Copilot to use it? Let’s see.\n\nChange the comment in app.py to say the following:\n\n[Prompt]\n\nTODO, create a POST route for customers. It should validate the request\n\nbody, and ensure it has a valid email address.\n\n[End of prompt]\n\nYou get a basic function that doesn’t really do what it’s supposed to:\n\n[Prompt response]\n\n@app.route('/customers', methods=['POST']) def post_customers(): return jsonify({'message': 'success'})\n\n[End of response]\n\nTo help with this, you resort to GitHub Chat and type in the following\n\nprompt:\n\n[Prompt]\n\nValidate the request body and ensure that it has a valid email address\n\n[End of prompt]\n\n[Prompt response]\n\nTo validate the request body and ensure it has a valid email address, you\n\ncan use the request object from Flask and a regular expression. Here’s how\n\nyou can modify your post_customers function:\n\nfrom flask import request import re\n\n@app.route('/customers', methods=['POST']) def post_customers(): data = request.get_json()\n\nif 'email' not in data: return jsonify({'message': 'No email provided'}), 400\n\nemail = data['email'] pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2\n\nif not re.match(pattern, email): return jsonify({'message': 'Invalid email format'}), 40\n\nreturn jsonify({'message': 'success'})\n\n[End of response]\n\nWhile the above code works, it doesn’t leverage our existing utils.py file,\n\nin which there is a function already solving validating an email. So how do\n\nwe get Copilot to consider that file? The answer lies in a command called @workspace. The @worskspace command considers all the files. Here’s how it can be used:\n\n[Prompt]\n\n@workspace improve selected code to validate request body to ensure it\n\ncontains a valid email address, use primarily functionality from other files\n\nin the workspace to solve the problem.\n\n[End of prompt]\n\nFigure 20.5: Image from Copilot Chat showing how using the @workspace command leverages\n\nexisting code and code files in the workspace to solve a problem, rather than creating new code.\n\nLet’s also show the response in code and discuss what we can see:\n\n[Prompt response]\n\nfrom flask import request from utils import validate_email\n\n@app.route('/customers', methods=['POST']) def post_customers(): data = request.get_json()\n\nif 'email' not in data: return jsonify({'message': 'No email provided'}), 400\n\nemail = data['email']\n\nif not validate_email(email): return jsonify({'message': 'Invalid email format'}), 40\n\nreturn jsonify({'message': 'success'})\n\n[End of response]\n\nIn the code above, validate_email, residing in the file utils.py, is used to\n\nvalidate an email. This is important as the first attempt, and solving this problem led to Copilot duplicating code. For that reason, using @workspace\n\nis recommended when you want Copilot to consider all code in your\n\nworkspace when suggesting a solution.\n\nVisual Studio Code lookup\n\nAnother powerful feature of Copilot is being able to quickly look up how to\n\ndo something in Visual Studio Code. Rather than consulting official\n\ndocumentation, you may want to quickly find what you need without leaving your code editor. To use this feature, there’s a @vscode command.\n\nLet’s ensure we have the chat window open and see how it works with the\n\nfollowing prompt:\n\n[Prompt]\n\n@vscode How do I open a terminal window?\n\n[End of prompt]\n\nFigure 20.6: Copilot response to the above prompt\n\nThere are two things worth noting here:\n\nCopilot tells you how to do something and where to find it.\n\nIt offers to visually show you how to find what you need if you click\n\nthe Show in Command Palette button.\n\nTerminal The last really great feature we will show you is terminal commands. In\n\ntheory, you could use terminal without resorting to this specific command,\n\nbut it helps if you don’t have to prefix your question with “Show me how to\n\ndo something in the terminal.” To use this command, prefix your prompt with @terminal like so:\n\n[Prompt]\n\n@terminal show me how to create a remote called upstream in Git that points to a remote repo.\n\n[End of prompt]\n\n[Prompt response]\n\nTo create a remote called “upstream” that points to a remote repository in Git, you can use the git remote add command. Here’s the command:\n\ngit remote add upstream {remote_repo_url}\n\nReplace {remote_repo_url} with the URL of the remote repository.\n\nThis command adds a new remote named “upstream” that points to the\n\nspecified remote repository. You can then fetch from or push to the\n\n“upstream” remote as needed.\n\n{remote_repo_url}: The URL of the remote repository. This could be a URL on GitHub, GitLab, Bitbucket, or any other Git server.\n\n[End of response]\n\nAs you can see, you get both the command you need and a detailed\n\nexplanation of how to type.\n\nAssignment See if you can improve the code further by adding, for example,\n\ndocumentation, tests, or maybe an ORM (object-relational mapper).\n\nRemember to use the commands covered in this chapter.\n\nChallenge Check out the other commands that Copilot Chat supports, and see if you\n\ncan use them to improve your workflow.\n\nQuiz What can Copilot help with?\n\n1. Code generation, automation, debugging, troubleshooting, code\n\nreview, and optimization\n\n2. Deployment\n\n3. None of the above\n\nSummary In this chapter, we covered some more advanced functionality available in Copilot. You learned how to use @workspace to enable Copilot to consider all your files. The @vscode command was another useful command in that it\n\nshowed you how to work with Visual Studio Code.\n\nWe also looked at scaffolding – specifically, how to scaffold files for a web\n\nproject – and how to create a Notebook with starter code. Such commands\n\nare likely to save you hours when you first start with a project. Copilot has\n\nquite a few commands, and I recommend trying them out.\n\nJoin our community on Discord\n\nJoin our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n21\n\nAgents in Software Development\n\nIntroduction This chapter will introduce you to the concept of agents in software\n\ndevelopment. We’ll cover what agents are, how they work, and how you\n\ncan use them in your projects. We’ll also cover some of the most popular\n\nagents’ frameworks and how you can get started with them.\n\nLet’s introduce the problems that agents can solve. The general idea is to\n\nhave a program that can act on your behalf. Examples of this could be\n\nautomating tasks, making decisions, and interacting with other agents and\n\nhumans. Programs like these can save you time and make your life easier or\n\nyour business more efficient.\n\nIn this chapter, we will do the following:\n\nIntroduce the concept of agents in software development.\n\nExplain what agents are and how they work.\n\nDiscuss the different types of agents and how they can be used.\n\nWhat are agents?\n\nAs mentioned previously, agents are programs that can act on your behalf.\n\nThey can perform tasks, make decisions, and interact with other agents and\n\nhumans. Agents can be used in a wide range of applications.\n\nSeveral things make a program an agent program versus just a program:\n\nAgent programs have a clear goal: For instance, take a thermostat\n\nkeeping the temperature at 25 degrees and taking appropriate actions\n\nto keep it there, or an agent managing finances and trying to maximize\n\nyour profit.\n\nAutonomous: An agent makes necessary decisions to ensure it meets a\n\ngoal as defined previously. For a finance agent, that could mean\n\nbuying and selling stocks when they meet a specific trigger condition.\n\nHas sensors: Sensors are either physical or could be an API in\n\nsoftware, something that enables an agent to understand “what the world is like.” For a thermostat, a sensor is a temperature indicator, but\n\nfor a finance agent, a sensor can be an API toward the stock market that enables the agents to decide their goals.\n\nHow do agents work? Agents work by receiving input, processing it, and producing output. They\n\ncan be programmed to perform specific tasks, make decisions, and interact with other agents and humans. Agents can also learn from their interactions\n\nand improve their performance over time.\n\nFigure 21.1: Process for simple agent: keyword, recognize, perform task\n\nSimpler agents versus agents using AI Agents are not a new thing. They have been around for a long time. What’s\n\nnew is that agents are now being powered by AI. Let’s compare the two:\n\nSimpler agents: Traditional agents are programmed to perform\n\nspecific tasks and make decisions based on predefined rules and logic.\n\nAgents using AI: Agents powered by AI can perform more complex\n\ntasks and make more intelligent decisions. They can understand natural\n\nlanguage, learn from their interactions, and improve their performance over time.\n\nSimpler agents Simpler agents, as mentioned in the previous sections, are limited in that\n\nthey are made for specific tasks. Interacting with them is usually also limited – you either use keywords or the way you can express yourself is\n\nlimited.\n\nAn example of a simple agent is a chatbot. Such chatbots are programmed\n\nto understand a limited set of keywords and phrases.\n\nFor example, “Tell me more about your products,” or “What’s your return policy?”. Any attempts at conversation outside of these keywords and\n\nphrases will result in the chatbot not understanding the user.\n\nA simple agent is not a great conversationalist",
      "page_number": 734
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 754-774)",
      "start_page": 754,
      "end_page": 774,
      "detection_method": "synthetic",
      "content": "When you have a conversation with a human, you expect them to know a\n\nfew topics well and at least to be able to talk about other topics. For a simpler agent, we might end up in the following conversation:\n\nUser: “Tell me about your products.”\n\nAgent: “We have a wide range of products, including electronics, clothing, and accessories. Please indicate your interest.”\n\nUser: “I’m interested in clothes, something fitting the current weather.”\n\nAgent: “I can advise on clothes for sure, but I don’t know the current\n\nweather.”\n\nThere are two interesting things we can observe here:\n\nThe conversation feels short, and the important information is either at\n\nthe end of a sentence or just before a comma, which indicates that\n\nsimpler parsing is used to extract the important information.\n\nIt doesn’t handle non-product information like weather, which could help filter down the response.\n\nImproved conversation with tool calling and large language models (LLMs) An LLM is an improvement in that it’s good at sounding more natural, but\n\nalso that it can parse out and recognize intent from fluent text. You can also\n\nprovide the LLM with additional knowledge thanks to something called tool calling, where you tell the LLM about various capabilities like the\n\nclothes API, weather API, and so on, which can handle the conversation\n\nbetter, and resemble a conversation with a human. Let’s take that same conversation with an LLM and highlight the difference:\n\nUser: “What are your products?”\n\nAgent: “We have a wide range of products, including electronics,\n\nclothing, and accessories.”\n\nUser: “Great, I’m interested in clothes, something fitting the current weather. What can you recommend?”\n\nAgent: “Can you tell me your location so I can advise you better on\n\nclothes?”\n\nUser: “Sure, I’m based in Phoenix, Arizona.”\n\nAgent: “I see it’s currently 90F in Phoenix at the moment. Might I\n\nsuggest these shorts?”\n\nThe reason this conversation fared better is that this LLM expresses itself more naturally thanks to the tool calling that called its weather API with\n\nPhoenix as input and then proceeded to call the clothes API with the\n\nweather response as a filter.\n\nThe anatomy of a conversational agent A conversational agent typically consists of the following components:\n\nInput: The input to the agent, typically in the form of natural\n\nlanguage. It should be said this can be a lot of different spoken\n\nlanguages, not just English, which you have had to hardcode in the past.\n\nProcessing: The processing of the input, typically using natural\n\nlanguage processing (NLP) techniques.\n\nDelegation: The delegation of the input to the appropriate component of the agent. The component it’s delegated to can be an agent for a\n\nspecific task, for example, to book a flight or to answer a question.\n\nFigure 21.2: Conversational agents process steps\n\nThe preceding diagram indicates a loop where you go from input to\n\nprocessing to delegation to result, so why is there a loop? An agent doesn’t\n\nhave the concept of an end; it sits there and waits for the user to provide input and reacts to it. As mentioned earlier in this chapter, an agent works\n\ntoward a goal, and if the goal is to manage finances, it’s a continuous job.\n\nMore on tool calling in LLMs We’ve mentioned tool calling previously in this chapter but let’s try to show\n\nhow it works to add capabilities to the LLM.\n\nThe LLM only knows what it has been trained on, and for things it hasn’t\n\nbeen trained on, it will, in many cases, try to provide you with an answer that isn’t always correct as it makes it up; this is known as a hallucination.\n\nTo improve areas where you want the LLM to provide more accurate\n\nresponses, you can present it with a tool. The process of providing a tool consists of the following components:\n\nA JSON description of a function\n\nA description of the function so the LLM knows when this function\n\nshould be called\n\nOnce you’ve provided the preceding components, let’s say you provide a\n\nfunction capable of fetching the weather; the LLM can now use its built-in\n\nfeatures to semantically interpret all the following inputs to mean that the\n\nuser wants to know about the weather:\n\n“What’s the weather like today in Salt Lake City?”\n\n“What’s the temperature in San Francisco?”\n\n“Is it going to rain in New York tomorrow?”\n\n“What’s the weather like in London?”\n\n“Is it warm outside?”\n\nAdding capabilities to GPT using tools\n\nHow it works is that you provide a function specification in a JSON format.\n\nThis JSON function format is a schema the GPT model understands. The\n\nGPT model will essentially do two things for you:\n\nExtract parameters from the prompt.\n\nDetermine whether to call a function and which function to call, as you\n\ncan tell it about more than one function.\n\nAs a developer, you need to then actively call the function if the LLM\n\nthinks it should be called.\n\nYour function format follows this schema:\n\n{ \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to }, }, \"required\": [\"location\", \"format\"], }, } }\n\nIn the preceding JSON schema, there are a few things you’re telling the\n\nGPT model:\n\nThere’s a function called get_current_weather.\n\nThe description is \"Get the current weather\".\n\nThe function takes two parameters, location and format.\n\nThere’s also a description of the parameters, their types, and allowed\n\nvalues.\n\nLet’s describe how this would work in practice, given the following prompt:\n\n[Prompt]\n\n“What’s the weather like today in Salt Lake City?”\n\n[End of prompt]\n\nHere’s what the GPT model can extract from the prompt:\n\nLocation: Salt Lake City.\n\nFormat: This is not provided, but the GPT can infer this from the\n\nuser’s location.\n\nFunction to call: get_current_weather.\n\nWhat you need to do as a developer is to call the function indicated with the\n\nextracted parameter values. The following is code that could be used to\n\nconnect to the GPT model, where a function description is provided, and\n\nparse the response:\n\nimport open def get_current_weather(location, format): # Call weather API response = requests.get(f\"https://api.weather.com/v3/wx/for return response.json() # Call the GPT model\n\ntool = { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g CA\", }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to }, }, \"required\": [\"location\", \"format\"], }, } } prompt = \"What's the weather like today in Salt Lake City?\" response = openai.Completion.create( model=\"text-davinci-003\", prompt=prompt, max_tokens=150, tools= [tool] ) # Parse the response function_response = response.choices[0].function_response # her location = function_response.parameters.location # extracting p format = function_response.parameters.format # extracting param weather = get_current_weather(location, format) # here we get t\n\nThis is probably the most basic example of how you can create a\n\nconversational agent using GPT.\n\nHowever, we expect a bit more from an advanced agent. What if we want to\n\ndo something more complex, like booking a flight, hotel, car, and\n\nrestaurant?\n\nAdvanced conversations It’s easy to think you can just add more functions to the GPT model and that\n\nmight work for a time. However, as the conversation becomes more\n\ncomplex, the GPT model needs to remember the context of the conversation\n\nand keep track of the state of the conversation.\n\nFor more advanced conversations, it quickly becomes more complex.\n\nImagine going into a travel agent’s office and the following back-and-forth\n\nconversation taking place between two humans:\n\nUser: “I want to go on a vacation.”\n\nAgent: “Sure.”\n\nUser: “I want to go to a warm place.”\n\nAgent: “Sure, tell me more about what you’re looking for.”\n\nUser: “I want somewhere with a beach.”\n\nAgent: “Sure, any more details?”\n\nUser: “I want there to be at least 25 degrees.”\n\nAgent: “Sure, any more details?”\n\nUser: “No, that’s it.”\n\nAgent: “I found three places that match your criteria. Can I present them to\n\nyou?”\n\nUser: “Yes, please.”\n\nAgent: “Here are the three places. Which one do you want to know more\n\nabout?”\n\nUser: “I want the first one.”\n\nAgent: “Here’s more information about the first place.”\n\nUser: “Cool, can you book a hotel and flight for the first place?”\n\nAgent: “Sure, I’ll get that done for you.”\n\nAgent: “I’ve booked the hotel and flight for you.”\n\nUser: “Thanks, ooh, I need to rent a car as well.”\n\nAgent: “Sure, any specific car you’re looking for?”\n\nUser: “Not really; my budget is $100 per day.”\n\nAgent: “I found five cars that match your criteria. Can I present them to\n\nyou?”\n\nUser: “Yes, please. I want the fifth one.”\n\nAgent: “I’ve booked the car for you.”\n\nAgent: “You’re all set for your vacation.”\n\nUser: “Thanks, but wait, I need help booking a restaurant for the first\n\nnight.”\n\nAt this point, it stands clear that this conversation can go on for quite a\n\nwhile. The agent needs to remember the context of the conversation and\n\nkeep track of the state.\n\nThere are also many different tasks that the agent needs to delegate to other agents or services, like booking a hotel, flight, car, and restaurant, and the\n\nweather API, sightseeing API, and more.\n\nThe point is that there’s more to an agent than just understanding the initial\n\nprompt and delegating the task to another agent or service. You need to\n\nthink of this conversation as a state machine and an orchestration of\n\ndifferent agents and services.\n\nModeling advanced conversations We mentioned that a more advanced conversation involves remembering\n\nboth context and state. Let’s inspect a subset of the example conversation\n\nand see how the state changes:\n\nUser: “I want to go on a vacation.”\n\nAgent: “Sure.”\n\nAt this point, the agent hasn’t remembered anything more than the user’s\n\nintention, which is to go on vacation. It’s just acknowledged the user’s\n\nprompt.\n\nUser: “I want to go to a warm place.”\n\nAgent: “Sure, tell me more about what you’re looking for.”\n\nNow things are getting interesting. The agent has remembered “warm” as a\n\npiece of criteria and needs to translate “warm” into a temperature range it\n\ncan use to filter out places that are too cold.\n\nUser: “I want somewhere with a beach.”\n\nAgent: “Sure, any more details?”\n\nThis is another step forward; the agent has remembered “beach” as an\n\nadditional piece of criteria to use when filtering out places.\n\nUser: “I want it to be at least 25 degrees.”\n\nAn additional criterion, “25 degrees,” has been added. Let’s see the earlier\n\npiece of criteria, “warm,” which was defined as 20–40 Celsius – this adjusts\n\nthe range to 25–40 Celsius.\n\nAgent: “Sure, any more details?”\n\nUser: “No, that’s it.”\n\nAt this point, the agent recognizes that the user has no more criteria to add,\n\nand a search/decision can take place with the filters of “warm,” “beach,”\n\nand “25–40 Celsius.” Now, an API is called to get a list of places and the\n\nagent can present the list to the user for selection.\n\nAgent: “I found three places that match your criteria. Can I present them to\n\nyou?”\n\nWhat’s important to add is that not only are criteria remembered for this\n\nspecific trip retrieval but they need to be remembered for the next steps as\n\nwell unless the user changes the criteria.\n\nHopefully, you can see from the preceding example that the state is built up\n\nslowly, and the agent needs to remember the context of the conversation.\n\nIt can be helpful to think of a more advanced conversation as consisting of\n\nthe following steps:\n\n1. Input: The input to the agent, typically in the form of natural\n\nlanguage.\n\n2. Processing: The processing of the input, typically using NLP\n\ntechniques.\n\n3. Determine the next step: The agent needs to determine the next step\n\nin the conversation based on the input and the current state of the\n\nconversation. Answers here can be to ask for more information,\n\npresent a list of options, book something, and so on.\n\n4. End conversation or continue (ask for user input): The agent needs\n\nto determine whether the conversation should end or continue. If it\n\nshould continue, it needs to ask for user input.\n\nPseudo code for advanced conversations The agent might have a few different states, for example:\n\nAsk for a task: This would typically be asked when the conversation\n\nstarts or when a task has been performed and a user selection has been\n\ndone.\n\nAsk the user for more information on a task: This would typically\n\nbe asked before a task is performed to ensure the agent has all the\n\ninformation it needs.\n\nPresent a list of options to the user: This would typically be asked after a task has been performed to present the user with things to\n\nchoose from.\n\nPerform a task: Here, the agent would perform a task, like booking a hotel, flight, car, or restaurant.\n\nEnd the conversation: The agent moves to this state when the\n\nconversation is over, and the user has somehow indicated that the\n\nconversation is over.\n\nThis is how this might look in pseudo code:\n\n# enum class State(Enum): ASK_FOR_TASK = 1 ASK_FOR_MORE_INFORMATION = 2 PRESENT_TASK_RESULT = 3 PERFORM_TASK = 4 END_CONVERSATION = 5 # initial state state = State.ASK_FOR_TASK def ask_for_task(): # ask the user for a task pass def ask_for_more_information(task): # store filter criteria pass def present_task_result(task): # presents the result so the user can choose pass def perform_task(task): # Perform a task pass def end_conversation(): # End the conversation pass while state != State.END_CONVERSATION: if state == State.ASK_FOR_TASK: # Ask for a task task = ask_for_task() state = State.ASK_FOR_MORE_INFORMATION elif state == State.ASK_FOR_MORE_INFORMATION: # Ask the user for more information on a task task = ask_for_more_information(task) state = State.PERFORM_TASK elif state == State.PRESENT_TASK_RESULT: # Present a list of options to the user task = present_task_result(task) state = State.ASK_FOR_MORE_INFORMATION elif state == State.PERFORM_TASK: # Perform a task perform_task(task) state = State.PRESENT_TASK_RESULT elif state == State.END_CONVERSATION:\n\n# End the conversation end_conversation()\n\nThe preceding code is a decent starting point for a more advanced conversation. However, we should remember that humans are not always\n\npredictable, and the agent needs to be able to handle the unexpected. For\n\nexample, humans can change their minds or add new criteria at any point.\n\nAutonomous agents Autonomous agents are agents that can act on their own without human\n\nintervention. They can perform tasks, make decisions, and interact with\n\nother agents and humans without human input. Autonomous agents can be\n\nused in a wide range of applications, from self-driving cars to virtual assistants.\n\nExamples of autonomous agents include the following:\n\nSelf-driving cars: Self-driving cars are autonomous agents that can\n\ndrive themselves without human intervention. They can navigate\n\nroads, avoid obstacles, and make decisions based on their\n\nsurroundings.\n\nVirtual assistants: Virtual assistants are autonomous agents that can\n\nperform tasks, make decisions, and interact with humans without\n\nhuman input. They can understand natural language, learn from their\n\ninteractions, and improve their performance over time. Imagine an\n\nexample of this where you have a financial agent; you might have\n\ngiven it a high-level goal to manage your finances and ensure an 8%\n\nvalue increase per year. You might have given this agent the go-ahead\n\nto buy and sell stocks under a certain value but to consultant you on\n\nlarger purchases and sales over a certain limit.\n\nWe are likely to see a rise in both conversational agents as well as\n\nautonomous agents in the future. There are a lot of interesting areas where\n\nthese agents can be used, from customer service to healthcare, finance, and\n\ntransportation. Imagine having an autonomous agent that can help you with\n\nyour taxes, book a vacation, or help you with your health while knowing\n\nyour medical history, your preferences, and more.\n\nAssignment Given the following conversation, deduce which type of agent is likely\n\ninvolved in the conversation and how would you model the conversation:\n\nUser: “I want to bake a cake.”\n\nAgent: “Sure, what type of cake are you looking to bake?”\n\nUser: “I want to make a chocolate cake.”\n\nAgent: “Sure, do you have a recipe in mind?”\n\nUser: “No, can you help me find a recipe?”\n\nAgent: “Sure, anything I should know in terms of allergies or preferences?”\n\nUser: “I’m allergic to nuts and I prefer a recipe that’s easy to make. Oh, I also want to make a cake that’s vegan.”\n\nAgent: “Got it! I found three recipes that match your criteria. Can I present\n\nthem?”\n\nUser: “Yes, please.”\n\nAgent: “Here are the three recipes. Which one do you want?”\n\nUser: “I want number 2.”\n\nAgent: “Is that all?”\n\nUser: “No, do I need to buy the ingredients? Also, can you help me set the oven temperature?”\n\nAgent: “Yes, looks like you’re missing a few ingredients. I can help you\n\nplace an order for the ingredients. I can also help you set the oven\n\ntemperature. When do you need it set?”\n\nUser: “Yes, place the order, and let’s set the oven as soon as the order\n\narrives.”\n\nAgent: “I’ve placed the order and am waiting for the delivery. I’ll set the\n\noven temperature as it arrives.”\n\nUser: “Thanks.”\n\nAgent: “You’re welcome, that’s it?”\n\nUser: “Yes.”\n\nChallenge Can you think of a problem that can be solved using agents? How would\n\nyou use agents to solve this problem? What type of agents would you use?\n\nQuiz Q: What’s tool calling in LLMs?\n\n1. It’s when the LLM calls a built-in tool to provide a response.\n\n2. It’s when you let the LLM know of a new capability by providing it\n\nwith a function description in JSON and a semantic description that\n\nindicates when this function should be called.\n\n3. It’s when you use functions to fine-tune the LLM.\n\nSummary In this chapter, we introduced the concept of agents in software\n\ndevelopment. We explained what agents are and how they work. We\n\ndiscussed the different types of agents and how they can be used. We hope\n\nthat you now have a glimpse of the future to see where LLMs like GPT are\n\nheading and how your future is about to change.\n\nReferences It’s worth checking out some resources on agents if you’re curious:\n\nAutogen: https://github.com/microsoft/autogen\n\nSemantic Kernel: https://learn.microsoft.com/en-\n\nus/semantic-kernel/overview/\n\nJoin our Discord: https://discord.gg/pAbnFJrkgZ\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\n22\n\nConclusion\n\nHuge thanks to you, dear reader, for getting this far into the book. We hope that you’re now at a point where you can confidently use AI tools like\n\nGitHub Copilot and ChatGPT in your projects.\n\nRecap of the book Let’s recap on what we’ve covered in this book. We started by introducing\n\nyou to the world of AI and how we got to large language models (LLMs).\n\nWe then introduced you to GitHub Copilot and ChatGPT, two of the most\n\npopular AI tools in the world today. Not only are these two tools popular,\n\nbut they’re also interesting to compare and contrast. ChatGPT comes with a\n\nchat interface and is built so that it can tackle a range of tasks. GitHub\n\nCopilot also comes with a chat interface and an in-editor mode but is more\n\ndedicated to solving problems around software development. An important aspect that unites these two tools is the fact you can use prompts and natural\n\nlanguage input, they can be used as input, and the end user receives an\n\noutput that hopefully brings them closer to solving their problem. Because both of these two tools rely on prompt input, it allows us to decide what\n\ntype of prompts and how many prompts are needed, tweak the prompts, and\n\neven use prompts to validate the AI tool’s response.\n\nThen, we presented you with a prompt strategy to ensure you use these AI\n\ntools effectively. After that, we showed how this prompt strategy was put\n\ninto practice in a web development project spanning from the frontend to\n\nthe backend over several chapters. We then showed how the very same prompt strategy applied to data science and machine learning projects.\n\nThese two problem domains have been chosen at random; the point of the\n\nbook is to demonstrate the capability of generative AI tools, and knowing\n\nhow to pair that with a prompt strategy will greatly empower you.\n\nThe impact of using an AI tool is that you now have a tool that does much\n\nof the heavy lifting around code development regardless of the problem\n\ndomain. What this means for you as a developer is that you can now focus\n\nto a higher degree on being declarative, to state how you want things rather\n\nthan typing every line of code. Using AI tools and using them efficiently is likely to speed up the coding part of your job considerably.\n\nBefore rounding up the book, we presented you with a glimpse of the\n\nfuture, namely agents, which are programs that can act on your behalf and represent where we think AI is headed next.\n\nFinally, we covered the AI tools and their features more in detail to set you\n\nup for success in your future projects.\n\nMajor conclusions So, what are the major conclusions that can be drawn from this book? They are as follows:\n\nAI tools like GitHub Copilot and ChatGPT are here to stay and are\n\nonly going to get better with time.\n\nThe world is shifting toward a more declarative way of programming;\n\nask the AI to do much of the heavy lifting for you. Your new role is to work with your AI tool iteratively for the best results.\n\nYou should have a prompt strategy in place to approach problems\n\nwithin your chosen domain regardless of whether you’re a web developer, data scientist, or machine learning engineer.\n\nPrompting is quickly becoming a skill in its own right, even if we think that improved models will make it less important in the future. For now, it’s a skill that you should master, and we hope this book is\n\nhelping you on this road to becoming proficient in prompting.\n\nWhat’s next What we discussed in our chapter on agents (Chapter 21) is where things\n\nare going, and you should keep an eye on that. Agents are already part of the curriculum at the University of Oxford; check out this course link to learn more https://conted.ox.ac.uk/courses/artificial- intelligence-generative-ai-cloud-and-mlops-online.\n\nAutogen, as mentioned in Chapter 21, is a highly interesting project that introduces agents. We recommend having a look at it and, if so inclined,\n\nleveraging it in your own projects: https://github.com/microsoft/autogen.\n\nAt last The world of AI is moving quickly. Yesterday’s perfect tools and strategies might not be the best ones in a few months. It’s therefore our ambition to keep this book updated with the latest findings and insights. A heartfelt",
      "page_number": 754
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 775-796)",
      "start_page": 775,
      "end_page": 796,
      "detection_method": "synthetic",
      "content": "thanks to you, dear reader, and we hope this book has proved valuable to\n\nyou.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com\n\npackt.com\n\nSubscribe to our online digital library for full access to over 7,000 books\n\nand videos, as well as industry leading tools to help you plan your personal\n\ndevelopment and advance your career. For more information, please visit\n\nour website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks\n\nand Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive\n\ndiscounts and offers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by\n\nPackt:\n\nPython Machine Learning By Example\n\nYuxi (Hayden) Liu\n\nISBN: 9781835085622\n\nFollow machine learning best practices throughout data preparation\n\nand model development\n\nBuild and improve image classifiers using convolutional neural\n\nnetworks (CNNs) and transfer learning\n\nDevelop and fine-tune neural networks using TensorFlow and PyTorch\n\nAnalyze sequence data and make predictions using recurrent neural networks (RNNs), transformers, and CLIP\n\nBuild classifiers using support vector machines (SVMs) and boost\n\nperformance with PCA\n\nAvoid overfitting using regularization, feature selection, and more\n\nUnlocking the Secrets of Prompt Engineering\n\nGilbert Mizrahi\n\nISBN: 9781835083833\n\nExplore the different types of prompts, their strengths, and weaknesses\n\nUnderstand the AI agent’s knowledge and mental model\n\nEnhance your creative writing with AI insights for fiction and poetry\n\nDevelop advanced skills in AI chatbot creation and deployment\n\nDiscover how AI will transform industries such as education, legal, and others\n\nIntegrate LLMs with various tools to boost productivity\n\nUnderstand AI ethics and best practices, and navigate limitations\n\neffectively\n\nExperiment and optimize AI techniques for best results\n\nPackt is searching for authors like you If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them\n\nshare their insight with the global tech community. You can make a general\n\napplication, apply for a specific hot topic that we are recruiting an author\n\nfor, or submit your own idea.\n\nShare your thoughts Now you’ve finished AI-Assisted Programming for Web and Machine\n\nLearning, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us\n\nmake sure we’re delivering excellent quality content.\n\nOceanofPDF.com\n\nIndex\n\nA\n\naccuracy 246, 317\n\nadditional exploratory analysis, initial data exploration prompt 471-473\n\ndistribution of sentiment scores,visualizing 478\n\nsentiment score calculation 475-477\n\ntext preprocessing 473\n\nword frequency analysis 474, 475\n\nadvanced conversations 551-553\n\nmodeling advanced conversations 553, 554\n\npseudo code 554\n\nagent programs 545\n\nagents 545\n\npowered by AI 546\n\nworking 546\n\nAI assistance, adding to unsupervised learning model development process 386-389\n\nchecking for outliers 395-397\n\nclustering model, creating 405, 406\n\nclustering results, visualizing 406, 407\n\ndata, inspecting 390\n\ndata, preprocessing 392, 393\n\ndata scaling, with standardization 399, 400\n\ndataset, loading 389\n\nfeature engineering 393-395\n\nnumber of clusters, deciding 400- 405\n\noutliers, removing 398, 399\n\nsummary statistics 391\n\nAI assistance, in page structure 40\n\nchallenge 46\n\nsimple prompting 40-45\n\nAI assistants 4\n\nAmazon product review dataset 176, 208\n\nauthentication\n\nfeatures 54\n\nautonomous agents 556\n\nexamples 556\n\nB\n\nbag-of-words representation 233\n\nbasket page 59-62\n\nbatch normalization 362-366\n\nBootstrap 76\n\nbootstrapping 295\n\nBrownfield 143\n\nbug, addressing in codebase 144\n\nchange, implementing 146, 147\n\nproblem, identifying 145, 146\n\nC\n\nCascading Style Sheets (CSS) 55\n\nby name 58\n\nfirst CSS 55-58\n\ncategorical variables 177\n\nchat 66\n\nchatbots 2, 546\n\nChatGPT 1, 3, 29\n\ncapabilities 31\n\ninstalling 31\n\nlimitations 31\n\nprompting 34, 35\n\nstarting 32\n\nworking 30\n\nChatGPT-4/ChatGPT Plus, for sentiment analysis model to classify Amazon reviews 240\n\nbaseline training 244, 245\n\ndata imbalance, handling 247-249\n\ndata preprocessing 240-242\n\nexperimenting, with feature representation 251-253\n\nfeature engineering 240-244\n\nhyperparameter tuning 249-251\n\nmodel evaluation and interpretation 245-247\n\nmodel selection 244, 245\n\nChatGPT-4o, for data exploration of Amazon review dataset\n\n200\n\ncategorical analysis 202\n\ncorrelation study 202, 205\n\ndata inspection 200\n\ndata loading 200\n\nrating distribution 202, 203\n\nstatistical summaries 202\n\ntemporal trends 202, 203\n\ntext analysis 202, 203\n\nChatGPT free version, for building CNN model to classify CIFAR-10 images 339\n\nbaseline CNN model, building with single convolutional layer 339-349\n\nbatch normalization, implementing 362-367\n\nDavidNet architecture, applying 370-378\n\ndifferent optimizers, using for optimization 367-370\n\ndropout regularization, incorporating 356-362\n\nexperimenting with addition of convolutional layers 350-356\n\nChatGPT free version, for building MLP model to classify Fashion-MNIST images 305\n\nbaseline model, building 305-322\n\nbatch sizes, for experimenting 325-328\n\ndifferent optimizers, trying 330-333\n\nlayers, adding to model 323-325\n\nnumber of neurons, for experimenting 328-330\n\nChatGPT free version, for building simple linear regression model 259\n\ncode, generating to develop model in single step for synthetic dataset 280-282\n\nmodel, building step by step 259-272\n\nregularization techniques, applying 272-277\n\nsynthetic dataset, generating to add complexity 277-280\n\nChatGPT free version, for data exploration of Amazon review dataset 179\n\ncategorical variables 187-190\n\ncorrelation study 198-200\n\ndata inspection 181-184\n\ndataset, loading 179-181\n\nlength analysis 194-197\n\nrating distribution 190-192\n\nsummary statistics 185-187\n\ntemporal trends 192-194\n\nChatGPT free version, for sentiment analysis model to classify Amazon reviews 211\n\nbaseline training 218-220\n\ndata imbalance, handling 228-230\n\ndata preprocessing 211-217\n\nexperimenting, with feature representation 233-240\n\nfeature engineering 211-217\n\nhyperparameter tuning 231-233\n\nmodel evaluation and interpretation 220-227\n\nmodel selection 218-220\n\nChatGPT Plus, for learning simple linear regression 283\n\ncode, generating to develop model in single step for synthetic dataset 297-299\n\nregularization techniques, applying 291-294\n\nsimple linear regression model,building 283-290\n\nsynthetic dataset, generating to add complexity 294-296\n\nCIFAR-10 dataset 336\n\nclustering 385\n\ncode\n\ndebugging 530-533\n\ntroubleshooting 530-533\n\ncode automation 523\n\ncodebase\n\nbug, addressing 144\n\nfeature, adding 148\n\nmaintainability, improving 155, 156\n\nmaintenance 143\n\nperformance, improving 151, 152\n\ncode generation 523\n\ncode optimization 534-537\n\ncode review 534-537\n\nconfusion matrix 317\n\ncontent recommendation 2\n\nconversational agent\n\nanatomy 547, 548\n\nCopilot 25, 523, 524\n\ncapabilities 26\n\ncommands 526\n\ninstalling 27\n\nquestions requesting 519\n\nstarting 27, 28\n\nworking 26\n\nCopilot Chat 524\n\nusing 525\n\nCopilot's assistance\n\nfor importing libraries 510\n\nCopilot's help\n\nfor coding regression problem 510\n\ncorrelation analysis 177\n\ncustomer segmentation 386\n\nD\n\ndata\n\nsplitting, into training and test sets 518, 519\n\nData Analysis 283\n\ndata exploration, steps\n\nadditional exploratory analysis 450\n\ncategorical variables, exploring 449\n\ncorrelation analysis 450\n\ndata, inspecting 449\n\ndataset, loading 449\n\ndistribution of ratings 449\n\nlength analysis 450\n\nsummary statistics 449\n\ntemporal analysis 449\n\ndataset\n\ncolumns, obtaining 512\n\ncolumn types, addressing 513, 514\n\ncolumn types, obtaining 512\n\nduplicates, checking 516\n\nexploring 511, 512\n\nloading 511, 512\n\nmissing values, checking 515\n\noverview 509, 510\n\nscale numerical features 516, 517\n\nstatistical summary 514\n\nstructure 512, 513\n\nvisualizing 517, 518\n\ndata splitting 309\n\nDavidNet architecture 370, 371\n\nbatch normalization 371\n\ndense layers 371\n\nlearning rate schedule 371\n\nmax pooling 371\n\nregularization 371\n\nresidual blocks 371\n\nskip connections 371\n\ndistribution of ratings 177\n\ndropout regularization 358-360\n\nE\n\ne-commerce\n\nbasket page 59-62\n\nbasket page, feature breakdown 66\n\nbehavior, adding 70-73\n\nBootstrap, adding 76-78\n\nbusiness problem 38, 54, 65, 86, 101, 102, 175, 208, 256,\n\n301\n\ndata domain 54, 65, 86, 102, 176, 208, 256, 302\n\ndataset overview 256, 257, 302\n\nfeature breakdown 86, 102, 209, 257, 303\n\nlogin page 46\n\noutput, improving 73-75\n\npage structure 39, 40\n\nproblem breakdown 38\n\nproblem domain 38, 65, 86\n\nproduct list page 47, 50\n\nprompting strategy 39, 86, 102\n\nremaining pages 50\n\nstyling 59\n\nuse case 46\n\nVue.js, adding 79-82\n\ne-commerce dataset 386\n\ne-commerce site\n\nupdating 164, 167-170, 173\n\nElbow Method 401\n\nELI5\n\nusing, for model interpretation 226, 227\n\nevaluation metrics, regression model\n\nadjusted R-squared 503\n\nMean Absolute Error (MAE) 502\n\nMean Squared Error (MSE) 502\n\nRoot Mean Squared Error (RMSE) 502\n\nR-squared 502\n\nF\n\nF1-score 246\n\nFashion-MNIST dataset 302\n\nfeature, adding in codebase 148, 149\n\nchange, implementing 149, 150\n\nfunctions, finding to change 149\n\nproblem, identifying 149\n\nfeature importance analysis 222\n\nfundamental analysis 483\n\nG\n\ngame plan\n\nclustering algorithm, applying 385\n\nclustering results, evaluating and visualizing 385\n\ndata preparation 385\n\nfeature engineering 385\n\ntext data preprocessing 385\n\nGap Statistics 402\n\nGitHub Copilot 1, 66",
      "page_number": 775
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 797-811)",
      "start_page": 797,
      "end_page": 811,
      "detection_method": "synthetic",
      "content": "GitHub Copilot Chat 448\n\nprompts, providing ways 448\n\nGloVe embeddings 252\n\nGPT\n\ncapabilities, adding with tools 549-551\n\nmodels 2, 3\n\nI\n\nimage gallery app\n\ncreating 59\n\ninitial data exploration prompt 450, 451\n\nadditional exploratory analysis 452, 471, 473\n\ncategorical variables, exploring 451, 459, 460\n\ncode, running for loading data 454\n\ncorrelation analysis 452, 468-471\n\ndata, inspecting 451, 455-458\n\ndataset, loading 451-454\n\ndistribution of ratings 451, 461\n\nlength analysis 451, 465-468\n\nsummary statistics 451, 458, 459\n\ntemporal analysis 451, 463-465\n\ninline comments 66\n\ninterquartile range (IQR) method 408\n\nJ\n\nJavaScript\n\nadding, to page 67\n\nadding, to web page 67\n\nlibrary/framework, adding 68-70\n\nONNX model, loading 141\n\nonnxruntime, installing 140\n\nrole 67\n\nJSON 109\n\nK\n\nK-means 400\n\nL\n\nlarge language models (LLMs) 1, 2, 547\n\ncontext 3\n\nconversations 547\n\nfew-shot learning 3\n\nperformance 3\n\ntool calling 548\n\nLearn-Improvise-Feedback-Evaluate (LIFE) prompt strategy 178, 210, 258, 304, 339\n\nlibraries\n\nimporting, with Copilot's assistance 510\n\nloss function 314\n\nlowercasing 213\n\nM\n\nmachine learning 447\n\nmachine translation 2\n\nmaintainability improvement,codebase 155, 156\n\nchange, implementing 162-164\n\nproblems, identifying 156\n\nrisk, reducing 157-162\n\ntests, adding 157-162\n\nmaintenance 143\n\nadaptive maintenance 143\n\ncorrective maintenance 143\n\nperfective maintenance 144\n\npreventive maintenance 144\n\nprocess 144\n\nMean Absolute Error (MAE) 289, 290\n\nMean Squared Error (MSE) 272, 282, 289, 290\n\nmemory game 98\n\nmodel\n\ntraining 520\n\nmodel creation, with Copilot 131-135\n\nCSV file, reading 133\n\nlibraries, importing 132\n\nmodel, saving to .pkl file 137\n\nprediction 136, 137\n\nREST API, for exposing model 138\n\ntest and training datasets, creating 134\n\nmodel explainability methods 222\n\nmodel interpretation\n\nELI5, using for 226, 227\n\nSHAP, using for 223-226\n\nmodel performance\n\nevaluating 521\n\nmodel training 314\n\nMulti-Layer Perceptron (MLP) 311\n\nN\n\nnatural language processing (NLP) 1\n\nnotebook 526\n\ncreating 527, 528\n\nO\n\nONNX format\n\nmodel, converting to 139\n\nONNX model\n\ncreating 139, 140\n\nloading, in JavaScript 140, 141\n\nonnxruntime\n\ninstalling, in JavaScript 140\n\nOpenAI\n\nURL 30\n\noptimization algorithm 314\n\nP\n\npage structure, e-commerce\n\nAI assistance, adding 40\n\npair programming 25\n\nperformance improvement, codebase 151\n\nBig O notation calculation 153\n\nexample 151, 152\n\nperformance measurement 154\n\nPersona-Instructions-Context (PIC) prompt strategy 178, 210, 258, 304, 338\n\nprecision 246, 317\n\nprincipal component analysis (PCA) 385\n\nproduct clustering, for e-commerce project 408\n\ncategories, assigning to products 439-441\n\nclustering algorithm, applying 423-432\n\nclustering algorithm, selecting 423\n\ncluster interpretation 433\n\ncluster visualization 433, 435\n\ndata, loading 411\n\ndata, pre-processing 412, 413\n\nevaluation 441\n\nfeature engineering 415\n\nfeature scaling 423\n\ninitial prompt 408-410\n\nrefining steps 442-444\n\ntext data preprocessing 413-415\n\nword cloud, creating 435, 436\n\nproduct gallery\n\nmaking responsive 91, 95-97\n\nprogramming languages\n\nevolution 5\n\nproject\n\ncreating 529\n\nprompts 3\n\nguidelines, for writing 8\n\nprompt strategy\n\nclassical verification, of outcome 22\n\nsolution, validating 21\n\nsolution verification, via prompts 21, 22\n\nprompt strategy, for data science 18\n\nbreakdown, into features/steps 19\n\nprinciples, identifying 20\n\nproblem breakdown 19\n\nprompts generation, for each feature 19\n\nprompt strategy, for web development 16\n\nfrontend breakdown, into features 17\n\nprinciples, identifying 18\n\nproblem breakdown 16\n\nprompts generation, for each feature 17\n\npunctuation\n\nremoving 213\n\nR\n\nReact 529\n\nrecall 246, 317\n\nregression 256\n\nasking to Copilot Chat 484\n\nregression model\n\nbuilding 496-502\n\nevaluating 502\n\nevaluation metrics 502-507\n\nregression problem\n\nbuilding 520\n\nReLU (Rectified Linear Unit) activation 372\n\nresidual blocks\n\nactivation function 372\n\nidentity path 372\n\nmerge operation 372\n\nshortcut path 372\n\nREST API\n\nbuilding, in JavaScript 141\n\ncreating, in Python 138\n\nR-squared value 272, 282\n\nS\n\nself-driving cars 556\n\nsensitivity/true positive rate 221\n\nsensors 546\n\nsentiment analysis 2\n\nSHAP (SHapley Additive exPlanations) 222\n\nusing, for model interpretation 223-226\n\nSilhouette Score 402\n\nsimpler agents 546\n\nsingle-layer MLP 311\n\nspeech recognition 1\n\nstock dataset\n\ndata splitting 494-496\n\nexploratory data analysis 489-494\n\nexploring 484\n\ninitial prompt 485-489\n\noverview 484\n\nstopwords\n\nremoving 213\n\nsummary statistics 177\n\nSynthetic Minority Over-sampling Technique (SMOTE) 247\n\nT\n\nTask-Actions-Guidelines (TAG)prompt strategy 177, 178, 210, 258,303, 338\n\ntemporal analysis 177\n\nTerm Frequency-Inverse Document Frequency (TF-IDF) 209, 213, 233, 243\n\ntesting set 309\n\ntext summaries 2\n\ntokenization 212\n\ntool calling\n\nin LLMs 548\n\ntraining accuracy 360\n\ntraining loss 360\n\ntraining parameters 314\n\ntraining set 309\n\nU\n\nunsupervised learning models 385\n\nV\n\nvalidation accuracy 360\n\nvalidation loss 360\n\nvalidation set 309\n\nviewports 87\n\nadjusting to 88-91\n\nmedia queries 87, 88\n\nvirtual assistants 2, 556\n\nVisual Studio Code 542\n\nVue.js 79\n\nW\n\nWeb API, for e-commerce site\n\nAPI, documenting 123-127\n\ncode, adding to read and write to database 111-118\n\ncode, improving 118\n\ncode, refactoring 119-123\n\ncode, running 118\n\ncreating 107, 108\n\nJSON, returning instead of text 109, 110\n\nuse case 107\n\nWeb APIs 103\n\nexpectations 103\n\nlanguage and framework, selecting 103\n\nplanning 103\n\nWeb API, with Python and Flask\n\ncreating 104\n\nentry point, creating 105\n\nFlask app, creating 106, 107\n\nFlask installation 105\n\nproject, creating 104, 105\n\nword embeddings representations 233, 235\n\nworkspace 538\n\nusing 539-542\n\nDownload a free PDF copy of this book Thanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books\n\neverywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version\n\nof that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code\n\nfrom your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781835086056\n\n2. Submit your proof of purchase.\n\n3. That’s it! We’ll send your free PDF and other benefits to your email\n\ndirectly.\n\nOceanofPDF.com",
      "page_number": 797
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 812-812)",
      "start_page": 812,
      "end_page": 812,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 812
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "AI-Assisted Programming for Web and Machine Learning\n\nImprove your development workﬂow with ChatGPT and GitHub Copilot\n\nChristoﬀer Noring\n\nAnjali Jain\n\nMarina Fernandez\n\nAyşe Mutlu\n\nAjit Jaokar",
      "content_length": 193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "AI-Assisted Programming for Web and Machine Learning\n\nCopyright © 2024 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a\n\nretrieval system, or transmitted in any form or by any means, without the\n\nprior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained\n\nin this book is sold without warranty, either express or implied. Neither the\n\nauthors, nor Packt Publishing or its dealers and distributors, will be held\n\nliable for any damages caused or alleged to have been caused directly or\n\nindirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate\n\nuse of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nSenior Publishing Product Manager: Bhavesh Amin\n\nAcquisition Editor – Peer Reviews: Gaurav Gavas\n\nProject Editor: Meenakshi Vijay\n\nContent Development Editor: Deepayan Bhattacharjee\n\nCopy Editor: Safis Editing",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Technical Editor: Tejas Mhasvekar\n\nProofreader: Safis Editing\n\nIndexer: Tejal Soni\n\nPresentation Designer: Rajesh Shirsath\n\nDeveloper Relations Marketing Executive: Sohini Ghosh\n\nFirst published: August 2024\n\nProduction reference: 1270824\n\nPublished by Packt Publishing Ltd.\n\nGrosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-83508-605-6\n\nwww.packt.com",
      "content_length": 376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Contributors\n\nAbout the authors Christoffer Noring works as a Senior Advocate at Microsoft and focuses on application development and AI. He’s a Google Developer Expert and a\n\npublic speaker on 100+ presentations across the world. Additionally, he’s a\n\ntutor at the University of Oxford on cloud patterns and AI. Chris is also a\n\npublished author on Angular, NGRX, and programming with Go.\n\nAnjali Jain is a London-based AI and ML professional with a career\n\nspanning over two decades. Currently working as a data architect for\n\nMetrobank, she brings her expertise in AI, data, architecture, data\n\ngovernance, and software development to the financial sector. Anjali holds a bachelor’s degree in electrical engineering and boasts certifications,\n\nincluding TOGAF 9.1 and ITIL 2011 Foundation. In her role as Senior AI\n\nand ML tutor at Oxford, she shares cutting-edge knowledge on various technologies.\n\nMarina Fernandez is a data science and Databricks consultant with\n\nexpertise in financial risk management. She contributes to the academic\n\nteam at the University of Oxford, where she holds the positions of senior AI and ML tutor and guest lecturer. Throughout her 20-year career, Marina has\n\nworked on the development of large-scale enterprise systems for various\n\nbusiness domains. Her experience encompasses e-commerce, e-learning, software security, commodity trading, commodity trading and risk\n\nmanagement systems, and regulatory reporting. Marina obtained her MSc in",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Software Engineering from the University of Oxford. Additionally, she has earned professional certifications, including Microsoft Certified\n\nProfessional and Certified Scrum Master.\n\nAyşe Mutlu is a data scientist working on Azure AI and DevOps\n\ntechnologies. Based in London, Ayşe’s work involves building and deploying Machine Learning and Deep Learning models using the\n\nMicrosoft Azure framework (Azure DevOps and Azure Pipelines). She\n\nenjoys coding in Python and contributing to open-source initiatives in Python.\n\nAjit Jaokar is a data scientist for Feynlabs, building AI prototypes for\n\ncomplex applications. He is also a course director for AI at the University\n\nof Oxford. Besides this, Ajit is a visiting fellow in Engineering Sciences at the University of Oxford and conducts AI courses at the London School of\n\nEconomics, Universidad Politécnica de Madrid, and the Harvard Kennedy\n\nSchool of Government as part of The Future Society. His work at Oxford\n\nand his company is based on interdisciplinary aspects of AI, including AI with digital twins, quantum computing, metaverse, Agtech, and life\n\nsciences. His teaching is based on a methodology for AI and cyber-physical\n\nsystems, which he is developing as part of his research.",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "About the reviewers Maxim Salnikov is a tech and cloud community enthusiast based in Oslo. With over two decades of experience as a web developer, he shares his\n\nextensive knowledge of the web platform, cloud computing, and AI by\n\nspeaking at and providing training for developer events worldwide. By day, Maxim plays a crucial role in supporting the development of cloud and AI\n\nsolutions within European companies, serving as the leader of developer\n\nproductivity business at Microsoft. During evenings, he can be found\n\nrunning events for Norway’s largest web and cloud development communities. Maxim is passionate about exploring and experimenting with\n\nGenerative AI possibilities, including AI-assisted development. To share his\n\ninsights and connect with like-minded professionals globally, he founded and organized the inaugural Prompt Engineering Conference, the first of its\n\nkind on a global scale.\n\nŞaban Kara is an AI and ML software engineer who graduated from Gebze\n\nTechnical University Electronics Engineering. Throughout his career, Şaban has developed several NLP projects and worked on various probabilistic\n\nstatistics-based ML algorithms. Şaban is especially known for his interest in\n\nLLM and LangChain models. His work on these models focuses on improving spontaneous learning abilities. He started his career working at\n\nTUBITAK. Currently, he is developing ML algorithms on LLM models in a\n\nprivate company and making a self-learning AI.\n\nI would like to thank my family, friends, and colleagues for their\n\ncontributions to the preparation of this book. Their support played an important role in the success of this project.",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Contents\n\nPreface\n\nWho this book is for What this book covers To get the most out of this book Get in touch\n\n1. It’s a New World, One with AI Assistants, and You’re Invited Introduction How ChatGPT came to be, from NLP to LLMs\n\nThe rise of LLMs GPT models How LLMs are better\n\nThe new paradigm, programming with natural language\n\nChallenges and limitations\n\nAbout this book Who this book is for Evolution of programming languages\n\nLooking ahead\n\nHow to use this book\n\n2. Prompt Strategy Introduction Where you are Guidelines for how to prompt efficiently Prompt techniques\n\nTask-Action-Guideline prompt pattern (TAG) Persona-Instruction-Context prompt pattern (PIC) Exploratory prompt pattern Learn-Improvise-Feedback-Evaluate prompt pattern (LIFE) Which pattern to choose?\n\nPrompt strategy for web development\n\nBreak down the problem: “web system for inventory management” Further breakdown of the frontend into features Generate prompts for each feature",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Identify some basic principles for web development, a “prompt strategy”\n\nPrompt strategy for data science\n\nProblem breakdown: predict sales Further breakdown into features/steps for data science Generate prompts for each step Identify some basic principles for data science, “a prompt strategy for data science”\n\nValidate the solution\n\nVerification via prompts Classical verification\n\nSummary\n\n3. Tools of the Trade: Introducing Our AI Assistants Introduction Understanding Copilot\n\nHow Copilot knows what to generate Copilot capabilities and limits Setup and installation Installing Copilot Getting started with Copilot Assignment: improve the code Solution Challenge References\n\nUnderstanding ChatGPT\n\nHow does ChatGPT work? ChatGPT capabilities and limits Setup and installation Getting started with ChatGPT\n\nPrompting\n\nSummary\n\n4. Build the Appearance of Our App with HTML and Copilot Introduction Business problem: e-commerce\n\nProblem domain Problem breakdown: identify the features Prompt strategy",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Page structure Add AI assistance to our page structure\n\nYour first prompt, simple prompting, and aiding your AI assistant Your second prompt: adding more context Your third prompt: accept prompt suggestions\n\nChallenge: vary the prompt Use case: build a front for an e-commerce\n\nLogin page Product list page Remaining pages\n\nAssignment Challenge Quiz Summary\n\n5. Style the App with CSS and Copilot\n\nIntroduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy CSS, or Cascading Style Sheets\n\nFirst CSS CSS by name\n\nAssignment Solution Use case: style the e-commerce app\n\nBasket page\n\nChallenge Quiz Summary\n\n6. Add Behavior with JavaScript\n\nIntroduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy",
      "content_length": 828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Adding JavaScript\n\nThe role of JavaScript Adding JavaScript to a page A second example: adding a JavaScript library/framework\n\nChallenge Use case: adding behavior Improving the output Adding Bootstrap Adding Vue.js\n\nAssignment Solution Summary\n\n7. Support Multiple Viewports Using Responsive Web Layouts Introduction Business problem: e-commerce Problem and data domain Breaking the problem down into features Prompting strategy Viewports\n\nMedia queries When to adjust to different viewports and make it responsive\n\nUse case: make our product gallery responsive Assignment Solution\n\nChallenge Summary 8. Build a Backend with Web APIs\n\nIntroduction Business domain: e-commerce Problem and data domain Feature breakdown Prompt strategy Web APIs\n\nWhat language and framework should you pick? Planning the Web API\n\nCreating a Web API with Python and Flask\n\nStep 1: Create a new project",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Step 2: Install Flask Step 3: Create an entry point Step 4: Create a Flask app\n\nUse case: a Web API for an e-commerce site\n\nStep 1: Create a Web API for an e-commerce site Step 2: Return JSON instead of text Step 3: Add code to read and write to a database Step 4: Improve the code Run the code Refactor the code Step 5: Document the API\n\nAssignment Solution Challenge Summary\n\n9. Augment Web Apps with AI Services Introduction Business domain, e-commerce Problem and data domain Feature breakdown Prompt strategy Creating a model Coming up with a plan Importing libraries Reading the CSV file Creating test and training datasets Creating a model How good is the model? Predict Saving the model to a .pkl file Creating a REST API in Python\n\nConverting the model to ONNX\n\nCreating a model in ONNX format Loading the ONNX model in JavaScript Installing onnxruntime in JavaScript Loading the ONNX model in JavaScript",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Assignment: Build a REST API in JavaScript that consumes the model Solution Quiz Summary\n\n10. Maintaining Existing Codebases Introduction Prompt strategy Different types of maintenance The maintenance process Addressing a bug\n\n1. Identify the problem 2. Implement the change\n\nAdding a new feature\n\n1. Identify a problem and find the function/s to change 2. Implement change, and add a new feature and tests\n\nImproving performance\n\nBig O notation calculation Measuring performance\n\nImproving maintainability\n\n1. Identify the problems. What problems do you see? 2. Add tests and de-risk change 3. Implement change and improve maintainability\n\nChallenge Updating an existing e-commerce site Assignment Knowledge check Summary\n\n11. Data Exploration with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nFeature breakdown Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nData exploration of the Amazon review dataset using the free version of ChatGPT\n\nFeature 1: Loading the dataset Feature 2: Inspecting the data Feature 3: Summary statistics Feature 4: Exploring categorical variables Feature 5: Rating distribution Feature 6: Temporal trends Feature 7: Review length analysis Feature 8: Correlation study\n\nData exploration of the Amazon review dataset using ChatGPT-4o Assignment Challenge Summary\n\n12. Building a Classification Model with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews using the free version of ChatGPT\n\nFeature 1: Data preprocessing and feature engineering Feature 2: Model selection and baseline training Feature 3: Model evaluation and interpretation Feature 4: Handling imbalanced data Feature 5: Hyperparameter tuning",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Feature 6: Experimenting with feature representation Building a sentiment analysis model to accurately classify Amazon reviews using ChatGPT-4 or ChatGPT Plus\n\nFeature 1: Data preprocessing and feature engineering Feature 2: Model selection and baseline training Feature 3: Model evaluation and interpretation Feature 4: Handling data imbalance Feature 5: Hyperparameter tuning Feature 6: Experimenting with feature representation\n\nAssignment Challenge Summary\n\n13. Building a Regression Model for Customer Spend with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a simple linear regression model to predict the “Yearly Amount Spent” by customers using the free version of ChatGPT\n\nFeature 1: Building the model step by step Feature 2: Applying regularization techniques Feature 3: Generating a synthetic dataset to add complexity Feature 4: Generating code to develop a model in a single step for a synthetic dataset\n\nLearning simple linear regression using ChatGPT Plus\n\nFeature 1: Building a simple linear regression model step by step Feature 2: Applying regularization techniques Feature 3: Generating a synthetic dataset to add complexity",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Feature 4: Generating code to develop a model in a single step for a synthetic dataset\n\nAssignment Challenge Summary\n\n14. Building an MLP Model for Fashion-MNIST with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding an MLP model to accurately classify the Fashion- MNIST images using the free version of ChatGPT Feature 1: Building the baseline model Feature 2: Adding layers to the model Feature 3: Experimenting with batch sizes Feature 4: Experimenting with the number of neurons Feature 5: Trying different optimizers\n\nAssignment Challenge Summary\n\n15. Building a CNN Model for CIFAR-10 with ChatGPT Introduction Business problem Problem and data domain Dataset overview\n\nBreaking the problem down into features Prompting strategy\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy Strategy 2: Persona-Instructions-Context (PIC) prompt strategy",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy\n\nBuilding a CNN model to accurately classify the CIFAR-10 images using the free version of ChatGPT\n\nFeature 1: Building the baseline CNN model with a single convolutional layer Feature 2: Experimenting with the addition of convolutional layers Feature 3: Incorporating dropout regularization Feature 4: Implementing batch normalization Feature 5: Optimizing with different optimizers Feature 6: Applying the DavidNet architecture\n\nAssignment Challenge Summary\n\n16. Unsupervised Learning: Clustering and PCA\n\nIntroduction Breaking the problem down into features Prompt strategy Customer segmentation The dataset Adding AI assistance to the unsupervised learning model development process Load the dataset Inspect the data Summary statistics Preprocessing the data Feature engineering Checking for outliers Removing outliers Data scaling using standardization Deciding on the number of clusters Creating a clustering model Visualize clustering results Final thoughts on clustering and the prompting process\n\nProduct clustering for an e-commerce project\n\nYour initial prompt: Set context",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Load and preprocess the data Feature engineering and text data preprocessing\n\nFeature engineering Choose clustering algorithm Feature scaling Apply clustering algorithm Interpret clusters and visualize results\n\nInterpreting cluster Visualizing clusters Creating a word cloud\n\nAssigning categories to products and evaluating and refining\n\nEvaluate and refine Reflection on prompts for this use case Assignment Solution Summary\n\n17. Machine Learning with Copilot Introduction GitHub Copilot Chat in your IDE\n\nHow it works Dataset overview Steps for data exploration Prompt strategy Your initial data exploration prompt: Prompt 1, setting the high- level context Step 1: Load the dataset\n\nRunning the code for loading data\n\nStep 2: Inspect the data Step 3: Summary statistics Step 4: Explore categorical variables Step 5: Distribution of ratings Step 6: Temporal analysis Step 7: Review length analysis Step 8: Correlation analysis Step 9: Additional exploratory analysis Step 10: Text Preprocessing Step 11: Word Frequency Analysis",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Step 12: Sentiment Score Calculation\n\nText preprocessing Word frequency analysis Sentiment score calculation\n\nStep 13: Visualize the Distribution of Sentiment Scores Step 14: Analyze the Relationship Between Sentiment Score and Other Variables\n\nVisualize the distribution of sentiment scores Analyze the relationship between sentiment score and other variables\n\nAssignment Solution Summary\n\n18. Regression with Copilot Chat Introduction Regression Dataset overview\n\nExplore the dataset\n\nPrompt strategy\n\nYour initial prompt Exploratory data analysis Data splitting Build a regression model\n\nEvaluate the model\n\nEvaluation metrics\n\nAssignment Summary\n\n19. Regression with Copilot Suggestions Introduction Dataset overview Prompt strategy Start coding with Copilot’s help\n\nStep 1: Import libraries with Copilot’s assistance Step 2: Load and explore the dataset\n\nGet types and columns Shape of the dataset Addressing the column types",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Statistical summary Check for missing values Check for duplicates Scale numerical features Visualization\n\nStep 3: Split data into training and testing sets\n\nAsking questions\n\nStep 4: Build a regression problem Step 5: Train the model Step 6: Evaluate model performance\n\nAssignment Summary\n\n20. Increasing Efficiency with GitHub Copilot Introduction Code generation and automation Copilot’s active editor Copilot Chat Copilot commands\n\nCreating a Notebook Creating a project\n\nDebugging and troubleshooting Code review and optimization techniques Workspace Visual Studio Code lookup Terminal Assignment Challenge Quiz Summary\n\n21. Agents in Software Development Introduction What are agents?\n\nHow do agents work?\n\nSimpler agents versus agents using AI Simpler agents\n\nA simple agent is not a great conversationalist",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Improved conversation with tool calling and large language models (LLMs) The anatomy of a conversational agent More on tool calling in LLMs Adding capabilities to GPT using tools\n\nAdvanced conversations\n\nModeling advanced conversations Pseudo code for advanced conversations\n\nAutonomous agents Assignment Challenge Quiz Summary References\n\n22. Conclusion\n\nRecap of the book Major conclusions What’s next At last\n\nOther Books You May Enjoy Index\n\nOceanofPDF.com",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Preface",
      "content_length": 7,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Who this book is for The target audience for this book is professionals in the web development,\n\nmachine learning, and data science fields. You should be a professional with\n\nat least 1-3 years of experience. This book means to empower you by\n\nshowcasing how AI assistants can be leveraged in different problem domains. It describes overall features but also gives recommendations on\n\neffective prompting for best results.",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "What this book covers Chapter 1, It’s a New World, One with AI Assistants, and You’re Invited,\n\nlooks at how we started using large language models and how it constitutes a paradigm shift for many, not just IT workers.\n\nChapter 2, Prompt Strategy, explains the strategy used throughout the book in terms of breaking down a problem and some guiding principles on how\n\nto effectively prompt your chosen AI tool.\n\nChapter 3, Tools of the Trade: Introducing Our AI Assistants, is where we\n\nexplain how to work with our two chosen AI assistants, GitHub Copilot and ChatGPT, covering everything from installation to how to get started using\n\nthem.\n\nChapter 4, Build the Appearance of Our App with HTML and Copilot,\n\nfocuses on building the frontend for our e-commerce app (a narrative you will see featured throughout the book).\n\nChapter 5, Style the App with CSS and Copilot, is where we keep working\n\non our e-commerce app but now focus specifically on CSS and ensuring the appearance is appealing.\n\nChapter 6, Add Behaviour with JavaScript, is where we add behavior to our e-commerce app using JavaScript.\n\nChapter 7, Support Multiple Viewports Using Responsive Web Layouts, is where we address the fact that an app needs to work for different device\n\ntypes, whether it’s a smaller mobile screen, a tablet, or a desktop screen. Therefore, this chapter focuses on responsive design.",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 8, Build a Backend with Web APIs, looks at how, for the app to\n\nactually work, it needs to have a backend, consisting of code that’s able to read and write data and persist it. This chapter therefore focuses on building a Web API for our e-commerce app.\n\nChapter 9, Augment Web apps with AI Services, covers training a machine learning model and how to expose it via a Web API for consumption by\n\nanyone with a browser or other type of client capable of using the HTTP\n\nprotocol.\n\nChapter 10, Maintaining Existing Codebases, covers how most developers work on existing code and maintain existing codebases rather than creating\n\nnew projects. Therefore, this chapter focuses on various aspects of\n\nmaintaining code, like dealing with bugs, performance, working with tests, and more.\n\nChapter 11, Data Exploration with ChatGPT, is where we work with a\n\nreview dataset and learn to identify insights into distribution, trends,\n\ncorrelation, and more.\n\nChapter 12, Building a Classification Model with ChatGPT, looks at the same review dataset as in Chapter 11, this time performing classification\n\nand sentiment analysis.\n\nChapter 13, Building a Regression Model for Customer Spend with\n\nChatGPT, attempts to predict the yearly amount spent by customers and uses regression to create a model capable of making this prediction.\n\nChapter 14, Building an MLP Model for Fashion-MNIST with ChatGPT,\n\nlooks at building an MLP model based on a fashion dataset, still sticking to\n\nour general theme of e-commerce.",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Chapter 15, Building a CNN Model for CIFAR-10 with ChatGPT, focuses on building a CNN model.\n\nChapter 16, Unsupervised Learning: Clustering and PCA, focuses on\n\nclustering and PCA.\n\nChapter 17, Machine Learning with Copilot, covers conducting machine\n\nlearning using GitHub Copilot to contrast it with ChatGPT.\n\nChapter 18, Regression with Copilot Chat, is where we develop a regression model. Also, this chapter uses GitHub Copilot.\n\nChapter 19, Regression with Copilot Suggestions, like the preceding\n\nchapter, focuses on regression using GitHub Copilot. The difference\n\nbetween this and the preceding chapter is that here we use the suggestions from writing prompts as comments in a text file, rather than writing our\n\nprompt in a chat-like interface.\n\nChapter 20, Increasing Efficiency with GitHub Copilot, focuses on getting\n\nthe most out of GitHub Copilot. This chapter is a must read if you want to master GitHub Copilot.\n\nChapter 21, Agents in Software Development, takes a look at what’s coming\n\nnext within AI, namely, agents. Agents are able to assist you to a much\n\nhigher degree by acting autonomously based on a high-level goal. This is definitely worth a read if you’re curious about future trends.\n\nChapter 22, Conclusion, wraps up the book by drawing some conclusions\n\nas to the greater lessons learned about working with AI assistants.\n\nTo get the most out of this book",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "You’ll get more out of this book if you’ve built a few projects in each domain as opposed to being a complete beginner. Therefore, the book\n\nfocuses on empowering you in your existing development workflows. We\n\nrecommend other titles by Packt if you are completely new to web development or machine learning. See the below list for recommendations:\n\nhttps://www.packtpub.com/en-us/product/html5-\n\nweb-application-development-by-example-\n\nbeginners-guide-9781849695947\n\nMachine Learning with Python: Unlocking AI Potential with Python and Machine Learning by Oliver Theobald (https://www.packtpub.com/en-\n\nUS/product/machine-learning-with-python- 9781835461969)\n\nThe book is built in such a way that you’re shown the prompts you’re recommended to write followed by the results from the chosen AI tool.\n\nTo follow along with the chapters on web development, we\n\nrecommend installing Visual Studio Code. There are dedicated\n\nchapters in the book pointing out how to install GitHub Copilot and leverage it. See the installation instructions for Visual Studio Code here: https://code.visualstudio.com/download\n\nFor the machine learning chapters, the majority of those chapters use ChatGPT, which can be accessed through a web browser. We do\n\nrecommend solving those problems using notebooks, which can be\n\nviewed through a variety of different tools. For more detailed\n\ninstructions on Notebook setup, refer to this page:\n\nhttps://code.visualstudio.com/docs/datascience/\n\njupyter-notebooks",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "To use GitHub Copilot, you need a GitHub account to log in to. Refer to this page on the setup process for GitHub Copilot:\n\nhttps://docs.github.com/en/copilot/quickstart\n\nDownload the example code ﬁles The code bundle for the book is hosted on GitHub at\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and- ChatGPT. We also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing/. Check them out!\n\nDownload the color images We also provide a PDF file that has color images of the\n\nscreenshots/diagrams used in this book. You can download it here: https://packt.link/gbp/9781835086056.\n\nConventions used There are a number of text conventions used throughout this book.\n\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input,\n\nand X(formerly known as Twitter) handles. For example: “Now that the product.css is created with the above content, we can include said CSS file\n\nin an HTML file.”",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Bold: Indicates a new term, an important word, or words that you see on the\n\nscreen. For instance, words in menus or dialog boxes appear in the text like\n\nthis. For example: “Create new user: It should be possible to create a new user.”\n\nWarnings or important notes appear like this.\n\nTips and tricks appear like this.\n\nGet in touch Feedback from our readers is always welcome.\n\nGeneral feedback: Email feedback@packtpub.com and mention the book’s\n\ntitle in the subject of your message. If you have questions about any aspect of this book, please email us at questions@packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of our\n\ncontent, mistakes do happen. If you have found a mistake in this book, we\n\nwould be grateful if you reported this to us. Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on\n\nthe internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packtpub.com with\n\na link to the material.\n\nIf you are interested in becoming an author: If there is a topic that you\n\nhave expertise in and you are interested in either writing or contributing to a",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "book, please visit http://authors.packtpub.com.",
      "content_length": 47,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Share your thoughts Once you’ve read AI-Assisted Programming for Web and Machine Learning, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us\n\nmake sure we’re delivering excellent quality content.",
      "content_length": 348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Download a free PDF copy of this book Thanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books\n\neverywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version\n\nof that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781835086056",
      "content_length": 730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "2. Submit your proof of purchase.\n\n3. That’s it! We’ll send your free PDF and other benefits to your email\n\ndirectly.\n\nOceanofPDF.com",
      "content_length": 133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "1\n\nIt’s a New World, One with AI Assistants, and You’re Invited\n\nIntroduction In November 2022, ChatGPT arrived from seemingly nowhere. Over time,\n\nChatGPT gained momentum, gradually evolving into a widely embraced\n\ntool. Eventually, millions actively incorporated ChatGPT into their\n\nworkflows, leveraging its capabilities for generating insights, summarizing\n\ntext, crafting code, and more.\n\nIts arrival changed many people’s workflow and improved it a lot in tasks\n\nlike quickly understanding large bodies of text, writing emails, and more.\n\nHere you are, having bought this book, and hoping that you can learn how\n\nto use an AI tool like ChatGPT or GitHub Copilot to make you more\n\nefficient. That’s exactly the mission of this book: to teach you not only how to use these two AI tools but also to be able to apply them across various\n\nproblem domains.\n\nBefore we start solving problems using an AI assistant, let’s back up a bit;\n\nhow did we get here? ChatGPT just didn’t arrive out of nowhere, right?",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "How ChatGPT came to be, from NLP to LLMs To tell the story of how we got here, to AI tools like ChatGPT, powered by\n\nlarge language models (LLMs), let’s first cover natural language\n\nprocessing (NLP).\n\nNLP is a field of computer science, artificial intelligence, and\n\ncomputational linguistics. It’s concerned with the interactions between\n\ncomputers and human language, and how to program computers to process\n\nand analyze large amounts of natural language data. NLP is a hugely\n\ninteresting area that has a range of useful applications in the real world. Here are some:\n\nSpeech recognition: If you have a modern smartphone, you’ve likely\n\ninteracted with voice assistants like Siri or Alexa, for example.\n\nMachine translation: Google Translate is perhaps what comes to mind when thinking of machine translation, the ability to translate\n\nfrom one language to another automatically.\n\nSentiment analysis: A very useful area is understanding the sentiment\n\nin areas like social media, for example. Companies want to know how brands are perceived; e-commerce wants to quickly understand\n\nproduct reviews to boost their business.\n\nChatbots and virtual assistants: You’ve likely seen chatbots being\n\nintegrated on web pages even before the advent of ChatGPT. These chatbots can answer simpler questions, and companies have them to\n\nensure you quickly get an answer to simpler questions and provide a\n\nmore natural experience than an FAQ page, among other usage areas.",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Text summaries: Search engines come to mind again when thinking\n\nabout text summaries. You might have seen how, when you use search engines like Bing or Google, it’s able to summarize a page and show the summary together with the link to the page in a search result page.\n\nAs a user, you get a better understanding of what link to click.\n\nContent recommendation: This is another important area used by a\n\nvariety of different domains. E-commerce uses this to present products\n\nyou’re likely to be interested in, Xbox uses this to recommend what\n\ngames to play and buy, and video streaming services display content you might want to watch next.\n\nAs you can see already, with NLP, both companies and end users benefit greatly from adopting it.\n\nThe rise of LLMs How did we evolve from NLP to LLMs, then? Initially, NLP used rule-\n\nbased systems and statistical methods underneath. This approach, although working well for some tasks, struggled with human language.\n\nThis changed for the better when deep learning, a subset of machine learning, was introduced to NLP, and we got models like RNN, recurrent\n\nneural networks, and transformer-based models, capable of learning patterns in data. The result was a considerable improvement in\n\nperformance. With transformer-based models, we’re starting to lay the foundations of large language models.\n\nLLMs are a type of transformer model. They can generate human-like text and, unlike NLP models, they’re good at a variety of tasks without needing\n\nspecific training data. How is this possible, you ask? The answer is a",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "combination of improved architecture, a vast increase in computational\n\npower, and gigantic datasets.\n\nLLMs rest on the idea that a large enough neural network can learn to do anything, given enough data and compute. This is a paradigm shift in how\n\nwe program computers. Instead of writing code, we write prompts and let the model do the rest.\n\nGPT models There are many different types of LLMs out there, but let’s focus on GPT\n\nfor a second, a type of LLM on which the book’s chosen tools are based (even if GitHub Copilot uses a specific subset known as Codex).\n\nThere have been several different versions developed in the last few years.\n\nHere are some models developed by the company OpenAI:\n\nGPT-1: The first one, with 117 million parameters using transformer\n\narchitecture.\n\nGPT-2: This model has 1.5 billion parameters and is able to generate\n\ncoherent and relevant text.\n\nGPT-3: This model has 175 billion parameters and is considerably\n\nbetter than its predecessor with features like answering questions, fiction generation, and even writing code.\n\nGPT-4: This model has been quoted to have 1.76 trillion parameters.\n\nThe number of parameters allows the model to understand more\n\nnuanced and coherent text. It should also be said that the larger the model, the larger the computational resources that are needed to train\n\nit.",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "ChatGPT recently switched to GPT-4 and the difference compared to\n\nGPT-3 is significant.\n\nHow LLMs are better Now that we have a better understanding of how LLMs came to be and\n\nwhere they came from, what makes LLMs great? What are some good examples of why we really should adopt AI assistants based on LLMs?\n\nBecause LLMs are bigger and more advanced, there are some areas in\n\nwhich they clearly outperform traditional NLP models:\n\nContext: LLMs can understand not just the recent input but can\n\nproduce responses based on a longer conversation.\n\nFew-shot learning: To perform a task, LLMs usually just need a few\n\nexamples to produce a correct response. This should be contrasted with\n\nNLP models, which usually use a large amount of task-specific\n\ntraining data to perform properly.\n\nPerformance: LLMs are better than traditional NLP models in areas\n\nlike translations, questions, and summarization.\n\nIt’s worth mentioning that LLMs aren’t perfect; they do generate incorrect\n\nresponses and can sometimes make up responses, also known as hallucinations. It’s our hope though that by reading this book, you will see\n\nthe advantages of using LLM-based AI assistants and you will feel the pros\n\nclearly outweigh the cons.\n\nThe new paradigm, programming with natural",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "language Probably the biggest game changer with using LLM-based AI assistants is\n\nthat you’re able to interact with them using nothing but natural language.\n\nThere’s no need to learn a programming language to get the response you need. This change constitutes a new paradigm in interacting with AI. We’re\n\nmoving away from writing in specific languages for producing apps, data\n\nretrieval, or even how we produce images, presentations, and more to\n\nexpress at a high level what we want through a prompt.\n\nHere is an example of things that are now possible to do using prompts,\n\nwhere it before needed considerably more effort:\n\nProgramming: With a prompt, you express what app you want to\n\nbuild or what changes you want to make with the code.\n\nImage generation: Where you before needed a designer or artist, you can now generate via prompts.\n\nVideos: There are tools out there that, once given a prompt, will\n\ngenerate videos where an avatar reads out your written text.\n\nText tasks: LLM-based AI assistants can generate emails, summarize\n\nlarge bodies of text, author interview ads, and much more; anything\n\nyou can imagine with text really.\n\nAll these application areas mentioned above make it clear that LLM-based AI tools are useful not only to programmers and data scientists but\n\nnumerous different professions.\n\nChallenges and limitations Is everything working perfectly at this point? AI assistants aren’t able to\n\nreplace “you” just yet, and should be considered more of a “thinking",
      "content_length": 1492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "partner.” Microsoft has even, through conscious naming, called their AI assistants “Copilots” where you’re clearly the pilot that sets out the\n\ndirection. These tools can generate text and other modalities in seconds, but\n\nyou need to verify the correctness. Often, the first response you get from a\n\ntool is something you need to iterate over. The good news is that it just takes seconds to redo the instruction.\n\nAn important thing to realize about AI assistants is that the more skilled you\n\nare at a certain topic, the more intelligent questions you can ask of it, and\n\nyou’ll be able to better assess the correctness of the response.\n\nAbout this book The goals of this book are to:\n\nIntroduce you to the new paradigm of programming with natural\n\nlanguage.\n\nProvide you with the tools to get started using AI assistants.\n\nEmpower you to use AI assistants effectively and responsibly by\n\nteaching you prompt engineering and specifically a set of prompting strategies (covered in Chapter 2) and some sound practices (covered in\n\nChapter 8).\n\nWe believe that with these tools, prompting strategies, and practices, you\n\nwill be able to use AI assistants effectively and responsibly to augment your work and increase your productivity.",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Who this book is for This book is for professional developers within both the web and machine\n\nlearning space. It is for those who want to learn how to use AI assistants\n\nlike GitHub Copilot and ChatGPT to augment their work and increase their\n\nproductivity.\n\nEvolution of programming languages Programming has gone through a series of changes and paradigm shifts\n\nthroughout history:\n\nAda Lovelace wrote the first algorithm for a machine, the Analytical Engine, in the 1840s. Lovelace is considered the first computer\n\nprogrammer and the first to recognize that the machine had\n\napplications beyond pure calculation.\n\nIn the 1940s, the first programmable computers were created. These computers were programmed using punch cards. One such computer\n\nwas the Harvard Mark I, which was used to calculate the trajectory of\n\nartillery shells. Also, Bombe is worth mentioning, which was used to\n\ncrack the Enigma code during World War II and was instrumental in\n\nthe Allies winning the war.\n\nIn the 1950s, the first high-level programming languages were created.\n\nThis time period saw the birth of FORTRAN, LISP, COBOL, and\n\nALGOL. Some of these languages are still in use today, especially in\n\nbanking systems, scientific computing, and defense.",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "In the 1970s, the first object-oriented programming languages were\n\ncreated. The 1970s meant we got Smalltalk, C++, and Objective-C.\n\nExcept for Smalltalk, these languages are heavily in use today.\n\nIn the 1990s, the first functional programming languages were created.\n\nThe 1990s gave us Haskell, OCaml, and Scala. The benefit of these\n\nlanguages is that they encourage immutability and pure functions,\n\nwhich makes them easier to reason about and test.\n\nIn the 2000s, the first declarative programming languages were\n\ncreated. Declarative programming languages are used to describe what\n\nyou want to do, rather than how you want to do it. The 2000s gave us SQL, HTML, and CSS.\n\nIn the 2010s, the first low-code and no-code platforms were created.\n\nThese platforms opened programming to a wider audience, and\n\nallowed anyone, regardless of technical background, to build applications.\n\nIn the 2020s, the first AI assistants were created that leveraged natural\n\nlanguage. If you can write a sentence, you can write code.\n\nIn summary, programming has gone through a series of changes and paradigm shifts. Prompt-first programming is the latest paradigm shift and\n\nmastering it will be key to staying relevant in the immediate future.\n\nLooking ahead If changes and paradigm shifts took years or decades in the past, they now\n\ntake months or even weeks. We’re moving toward a new world at\n\nbreakneck speed.\n\nThere’s reason to be excited, as we’re moving faster than before, but as\n\nalways, we should exercise caution. We should be aware of the risks and the",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "dangers of using these tools irresponsibly, but most of all we should be\n\naware of the opportunities.\n\nAs Alan Kay once said, “The best way to predict the future is to invent it.”\n\nHow to use this book We believe the best way to use this book is to follow the chapters in order.\n\nChapter 2, with the prompting strategies, is the most important chapter in\n\nthe book. These patterns and strategies are referred to throughout the book\n\nand are the foundation for how to use AI assistants effectively and\n\nresponsibly.\n\nThe book is written in the following format:\n\nIntroduction: The first chapter aims to provide you with an overview of what this book is about, its goals, and who it is for.\n\nPrompt strategy: The idea is to lay the foundation on how to break\n\ndown problems within the domains of data science and web development. From this chapter, you will learn strategies you can\n\nadopt for your own problems.\n\nTools of the trade: The third chapter introduces you to our tools,\n\nGitHub Copilot and ChatGPT, what they are, how they work, and how to install them. However, the book is written in such a way that you\n\ncan take any of the prompts we suggest and feed those into any AI\n\nassistant, and get a similar experience.\n\nThe remaining chapters of the book show how we use the prompt\n\nstrategies from Chapter 2 and apply them to various domains from\n\nweb development to data science and machine learning.\n\nHappy reading!",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "2\n\nPrompt Strategy\n\nIntroduction In the previous chapter, we gave some historical context to how AI has\n\ndeveloped over the years, how we’ve gone from natural language\n\nprocessing (NLP) to large language models (LLMs), and how the latter\n\nserves as the underlying machine learning model in AI assistants. To use\n\nthese AI assistants, you use natural language prompts as input. However, to\n\nensure you “prompt” in an efficient way, so that you get what you want, it’s\n\nimportant to have a strategy, and that’s what this chapter aims to give you.\n\nHow to “prompt” efficiently is commonly known in the industry as a\n\n“prompt strategy” or “prompt engineering.” It’s not an engineering practice\n\nin the common sense of the word but rather an art form where practitioners\n\nof AI assistants have discovered patterns and practices that seem to work\n\nwell. We, the authors of this book, are building upon those discovered\n\npractices and aim to describe our findings for two domains: full-stack web development and data science. This book turns to you as either a web\n\ndeveloper or data scientist and aims to empower you by describing how you\n\ncan best approach problem solving in your domain using an AI assistant.\n\nThis chapter constitutes a central piece of the book. It’s central in the sense\n\nthat the approach being taught will be exemplified by other chapters in the\n\nbook. As such, see this chapter as a guide you can refer to, providing the",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "theory and thinking that is used in future chapters that solve specific\n\nproblems within data science and full-stack web development.\n\nIn this chapter, we will:\n\nProvide a strategy for solving problems with prompts and validating\n\nthe solution.\n\nIllustrate the strategy with examples from data science and full-stack\n\nweb development.\n\nIdentify some basic principles for writing prompts.\n\nWhere you are As a reader and a practitioner of data science and/or full-stack web development, you know your craft. Knowing your craft means you know\n\nthe tools and techniques to solve problems. At this point, you’re looking at\n\nan AI assistant and realize it’s controlled by natural language, so-called\n\nprompts. What you may not realize is that there’s more to it than just writing a prompt and getting an answer. An AI assistant is trained on a large\n\ncorpus of text, so it’s quite flexible on what it can generate text on and how to respond to prompts. Because of this flexibility, it’s important to understand how to write prompts that are effective and efficient.\n\nGuidelines for how to prompt eﬃciently Prompts are input to AI tools. Depending on what you’re trying to achieve,\n\nyou need to adjust your prompts for the scenario you’re solving for.\n\nTherefore, how you “prompt” matters. For example, if your prompt is too",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "vague, you won’t get what you need. Or, let’s say you’re trying to use a\n\nprompt to generate company slogans; you don’t want to use the same prompt for generating code for an app. Conversely, in a discipline like data science, it’s important you perform tasks in a certain order and your prompt\n\nshould reflect what you want done and, if needed, the steps to do so.\n\nWhat you need to succeed is an approach, a strategy, that you can use in general to be efficient with AI assistants. Additionally, such a strategy should be specific enough to present “best practices” for chosen problem\n\ndomains. As mentioned earlier in this chapter, we’ve developed a prompt strategy specifically for the domain’s full-stack web development and data\n\nscience.\n\nAt a high level, we suggest the guidelines for general problem-solving\n\nusing AI assistants; it’s our belief it holds true regardless of what problem domain you’re applying prompts to:\n\n1. Break down the problem so that it’s fully understood. Within this\n\nguideline, there might be several steps like the following:\n\nUnderstand the problem: For any problem, it’s important to understand what the problem is and what it’s not. For example,\n\nare we building a machine learning model to predict sales or a\n\nweb page to track inventory? These are two different problems and require different approaches.\n\nIdentify the parts: A problem is usually complex and consists of many parts that need to be solved. For example, if we’re\n\nbuilding a machine learning model to predict sales, we need to identify the data, the model, the training, and the evaluation.\n\nEach of these parts can be broken down into smaller parts and so",
      "content_length": 1662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "on. Once you find the right detail level for your problem, you\n\ncan start solving it by writing prompts.\n\nBreak down the problem into smaller pieces: If needed, break\n\ndown your problem into smaller more manageable pieces.\n\nIdentify and understand the data: Especially with machine learning, it’s crucial to identify a dataset to work with, what it\n\ncontains, and how it’s structured. Within web development, data\n\nalso plays a central role, but the goal is usually to ensure you can read, write, and present data in a way that’s useful for the user.\n\n2. Generate prompts at a suitable level. Once you’ve understood the\n\nproblem fully, you should have a list of tasks, and for each task, you should be able to author and run a prompt that solves said task.\n\n3. Validate the solution. Just like without AI assistants, validation plays\n\na crucial part in building systems or apps. Traditionally, that means\n\nwriting tests, testing various components together, and letting users try out various parts. Using prompts is no different. A side effect of LLMs\n\nis that they can generate text that’s not relevant to the problem or solve\n\nthe problem in a less-than-optimal way. Because you’re relying on prompts that end up generating code for you, it becomes extra\n\nimportant to validate the solution to ensure it is correct and relevant.\n\nIn the upcoming sections, we will describe a detailed strategy for each\n\nchosen problem domain’s full-stack web development and data science.\n\nPrompt techniques There are some techniques that are extra useful when instructing the AI\n\nassistant to ensure you get the most out of it. The general advice is to",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "provide enough context to the AI assistant but what that means exactly can be made more concrete via the below suggested patterns:\n\nTask-Action-Guideline Prompt Pattern\n\nPersona-Instruction-Content Prompt Pattern\n\nExploratory prompt\n\nTask-Action-Guideline prompt pattern (TAG) To explain how this pattern works, let’s break down the parts and the show\n\nan example prompt with an example response:\n\nIntent: To instruct an AI assistant on what task to solve, what actions to\n\ntake, and what guidelines to adhere to.\n\nTask: Clearly define the task you want the AI to accomplish\n\nAction: Specify the steps or actions the AI should take to complete the\n\ntask\n\nGuideline: Provide any rules, constraints, or guidelines the AI should follow while performing the task\n\nExample:\n\n[Prompt]\n\nTask: Generate a summary of the latest news article about climate change.\n\nAction:\n\n1. Search for the most recent news articles on climate change.\n\n2. Select the article with the highest relevance and credibility.\n\n3. Summarize the key points of the article in 3-4 sentences.",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Guideline:\n\n1. Ensure the summary is concise and accurate.\n\n2. Avoid using technical jargon; keep the language simple and accessible.\n\n3. Cite the source of the article at the end of the summary.\n\n[End of prompt]\n\n[Prompt response]\n\nSummary: A recent article from BBC News highlights the alarming increase\n\nin global temperatures over the past decade. Scientists warn that if current trends continue, we could see unprecedented climate events. The article\n\nemphasizes the urgent need for international cooperation to combat climate\n\nchange. [Source: BBC News]\n\nThis pattern helps ensure that the AI assistant understands the task, knows the steps to take, and adheres to any specific guidelines, leading to more\n\naccurate and useful outputs.\n\n[End of response]\n\nHere’s what a prompt example could look like for web development:\n\n[Prompt]\n\nTask: Develop a RESTful API for a simple blog application using Node.js and Express.\n\nAction:\n\n1. Set up a new Node.js project.\n\n2. Create an Express server.\n\n3. Define routes for CRUD operations on blog posts.\n\n4. Implement middleware for JSON parsing and error handling.",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "5. Connect to a MongoDB database using Mongoose.\n\n6. Write tests for the API endpoints.\n\nGuideline:\n\n1. Ensure the project setup is clear and straightforward.\n\n2. Provide concise and accurate code examples.\n\n3. Avoid using overly complex terminology; keep the language simple\n\nand accessible.\n\n4. Include comments in the code to explain each step.\n\n[End prompt]\n\nPersona-Instruction-Context prompt pattern (PIC) Let’s describe this pattern in the same way as the preceding one, let’s define\n\nit, followed by an example prompt and an example response:\n\nIntent: To instruct an AI assistant on the persona it should assume, what\n\ninstructions to carry out, and provide enough context to generate a correct response.\n\nPersona: Define the persona or character traits the AI should embody\n\nwhile responding\n\nInstruction: Specify the tasks or actions the AI should perform\n\nContext: Provide relevant background information or context to help\n\nthe AI understand the situation and generate an appropriate response\n\nExample:\n\n[Prompt]\n\nPersona: Assume the persona of a friendly and knowledgeable travel guide.",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Instruction:\n\n1. Recommend three must-visit attractions in Paris.\n\n2. Provide a brief description of each attraction.\n\n3. Suggest the best time to visit each attraction.\n\nContext: The user is planning a trip to Paris for the first time and is interested in both historical sites and local culture.\n\n[End of prompt]\n\n[Prompt response]\n\nAs your friendly and knowledgeable travel guide, I’d love to recommend\n\nthree must-visit attractions in Paris:\n\n1. **Eiffel Tower**: This iconic landmark offers breathtaking views of the\n\ncity. It’s best to visit in the early morning or late evening to avoid the\n\ncrowds and enjoy the stunning sunrise or sunset.\n\n2. **Louvre Museum**: Home to thousands of works of art, including the\n\nMona Lisa, the Louvre is a treasure trove of history and culture. Plan your\n\nvisit on a weekday morning to experience the museum with fewer visitors.\n\n3. **Montmartre**: This charming neighborhood is known for its artistic\n\nhistory and vibrant street life. Visit in the late afternoon to explore the\n\nquaint streets and enjoy a leisurely dinner at a local café.\n\nEnjoy your trip to Paris!\n\n[End of response]\n\nThis pattern ensures that the AI assistant adopts the right persona, follows specific instructions, and uses the provided context to deliver a relevant and\n\nhelpful response.",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Here’s what such a prompt could like for web development:\n\n[Prompt]\n\nPersona: Assume the persona of a friendly and knowledgeable software\n\ndevelopment mentor.\n\nInstruction:\n\n1. Set up a new Node.js project.\n\n2. Create an Express server.\n\n3. Define routes for CRUD operations on blog posts.\n\n4. Implement middleware for JSON parsing and error handling.\n\n5. Connect to a MongoDB database using Mongoose.\n\n6. Write tests for the API endpoints.\n\nContext: The user is a beginner in web development and is looking to build\n\ntheir first RESTful API for a blog application. They need clear, step-by-step\n\nguidance and code examples.\n\n[End of prompt]\n\nExploratory prompt pattern You might find yourself in situations where you’re not building a project\n\nfrom start to finish, or you only want to build a smaller part of and then evaluate the response. In those cases, your prompts will be more\n\nexploratory in nature and can for example look like the below:\n\n[Prompt]\n\nClean the data.\n\n[End of prompt]",
      "content_length": 992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "An assumption here is that we have a Notebook open with existing code\n\nthat has already fetched the data.\n\nOr, something more for web development:\n\n[Prompt]\n\nAdd CSS for this product list.\n\n[End of prompt]\n\nPrompts for this pattern are usually a lot shorter in length, has context\n\n(derived from existing code or in some other way) and the developer line of\n\nsight is seldom beyond the next step.\n\nLearn-Improvise-Feedback-Evaluate prompt pattern (LIFE) This pattern, like TAG and PIC helps frame the problem and provides a good start to a solution that you can further refine.\n\nLearn: Highlight the need to understand data through various\n\nanalytical techniques, from basic statistics to complex correlations and\n\ntemporal analysis.\n\nImprovise: Adapt the analysis based on initial findings. For instance,\n\nif certain categories of products show unusual trends, deepen the\n\nanalysis in these areas\n\nFeedback:\n\nShare code and model outputs for feedback to ensure effective\n\nlearning and understanding.\n\nIncorporate suggestions and critiques to refine the model and the\n\napproach.\n\nProvide errors to troubleshoot and resolve the issues",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Evaluate: Execute the code provided by ChatGPT to ensure accuracy\n\nand validity.\n\nAn example prompt from this pattern might look like so:\n\n[Prompt]\n\nProject Title: Building a Web-Based Search Results Page. You must suggest\n\nsteps and code\n\nObjective: Create a dynamic and interactive search results page that efficiently displays and filters search results based on user queries. Steps:\n\nLearn: Understand the importance of displaying search results\n\neffectively. Analyze user interaction data to identify key metrics such\n\nas click-through rates, average time spent on results pages, and user satisfaction scores. Use various analytical techniques, from basic\n\nstatistics to complex correlations and temporal analysis, to gain\n\ninsights into user behavior and preferences.\n\nImprovise: Based on initial findings, adapt the search results page to\n\nhighlight the most relevant results. For instance, if users frequently\n\nclick on certain types of results, prioritize these in the display.\n\nImplement features such as filters, sorting options, and pagination to\n\nenhance user experience.\n\nFeedback: Share your code and the search results page with peers or\n\nmentors for feedback. This will help ensure effective learning and\n\nunderstanding. Incorporate their suggestions and critiques to refine the\n\nsearch results page and improve its functionality. Provide errors:\n\nDocument any errors encountered during the development process, such as issues with data retrieval or display. Troubleshoot and resolve\n\nthese issues, learning from the process to enhance your skills.",
      "content_length": 1565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Evaluate: Execute the code provided by ChatGPT to ensure its\n\naccuracy and validity. Validate the search results page by comparing its\n\nperformance with real-world data and user feedback. Make necessary\n\nadjustments to improve accuracy and user satisfaction.\n\n[End of prompt]\n\nYou might end up with a prompt response similar to:\n\n[Prompt response]\n\nGreat! Let’s break down the steps and provide some code snippets to help\n\nyou build a dynamic and interactive search results page.\n\nProject Title: Building a Web-Based Search Results Page\n\nObjective: Create a dynamic and interactive search results page that\n\nefficiently displays and filters search results based on user queries.\n\nSteps:\n\nLearn:\n\nUnderstand the importance of displaying search results effectively:\n\nAnalyze user interaction data to identify key metrics such as\n\nclick-through rates, average time spent on results pages, and user\n\nsatisfaction scores.\n\nUse various analytical techniques, from basic statistics to\n\ncomplex correlations and temporal analysis, to gain insights into\n\nuser behavior and preferences.\n\nImprovise:\n\nAdapt the search results page based on initial findings:\n\nHighlight the most relevant results based on user interaction\n\ndata.",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Implement features such as filters, sorting options, and\n\npagination to enhance user experience.\n\nFeedback:\n\nShare your code and the search results page with peers or mentors\n\nfor feedback:\n\nIncorporate their suggestions and critiques to refine the search\n\nresults page and improve its functionality.\n\nProvide errors:\n\nDocument any errors encountered during the development\n\nprocess:\n\nTroubleshoot and resolve these issues, learning from the process\n\nto enhance your skills.\n\nEvaluate:\n\nExecute the code provided by ChatGPT to ensure its accuracy and\n\nvalidity:\n\nValidate the search results page by comparing its performance\n\nwith real-world data and user feedback. Make necessary\n\nadjustments to improve accuracy and user satisfaction.\n\nCode Snippets:\n\nHTML Structure:\n\n<!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial- <title>Search Results Page</title> <link rel=\"stylesheet\" href=\"styles.css\">",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "</head> <body> <div class=\"search-container\"> <input type=\"text\" id=\"search-input\" placeholder=\"Searc <button onclick=\"performSearch()\">Search</button> </div> <div class=\"results-container\" id=\"results-container\"></div <script src=\"script.js\"></script> </body> </html>\n\nCSS Styling:\n\n/* styles.css */ body { font-family: Arial, sans-serif; } .search-container { margin: 20px; } .results-container { margin: 20px; } .result-item { border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; }\n\nJavaScript Functionality:\n\n// script.js function performSearch() { const query = document.getElementById('search-input').value const resultsContainer = document.getElementById('results-c resultsContainer.innerHTML = ''; // Clear previous results // Simulate search results const results = [",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "{ title: 'Result 1', description: 'Description for resu { title: 'Result 2', description: 'Description for resu { title: 'Result 3', description: 'Description for resu ]; results.forEach(result => { const resultItem = document.createElement('div'); resultItem.className = 'result-item'; resultItem.innerHTML = `<h3>${result.title}</h3><p>${re resultsContainer.appendChild(resultItem); }); }\n\nAdditional Features:\n\nFilters and Sorting: Add dropdowns or checkboxes to filter and sort results.\n\nPagination: Implement pagination to handle large sets of results.\n\nError Handling: Add error handling to manage issues with data\n\nretrieval or display.\n\nThis should give you a solid foundation to start building your web-based\n\nsearch results page. Let me know if you need further assistance or more\n\ndetailed code examples!\n\n[End of response]\n\nWhich pattern to choose? The general answer it depends on the situation which pattern to choose. The\n\nreason for choosing between the TAG and PIC is that you have a clearly\n\ndefined problem that you want to solve from start to finish and you’re\n\nlooking for the AI assistant to do the “heavy lifting”. The reason to go for\n\nthe exploratory pattern is when you’re working on an existing codebase, or\n\nyou want to try out various approaches and steps. Our general",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "recommendation is to try out all three patterns to see what suits your\n\napproach and problem the best.\n\nPrompt strategy for web development Let’s apply the different guidelines presented in the former section, and\n\nthrough using these guidelines, we will step by step work ourselves toward\n\na prompt strategy.\n\nBreak down the problem: “web system for inventory management” Let’s use a real example, “inventory management” within web\n\ndevelopment, to see if we can understand the general problem. To\n\n“manage” an inventory, you need to read and write data to it. Most likely,\n\nyou will have different roles in this system/app, everything from\n\nadministrators to normal users. You might also need to consider how this\n\nsystem fits in with other systems, should you, for example, integrate it with\n\nother systems, what parts it consists of in that case and how.\n\nThe domain seems pretty straightforward so let’s move on to understand\n\nwhat parts it consists of.\n\nAt a high level, we understand what the system should do. But to solve the\n\nproblem, we need to divide it into smaller parts, which in web development\n\nusually entails the following components:",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Frontend: The frontend is the part of the system that the user interacts\n\nwith. The frontend is responsible for presenting data to the user and\n\nreceiving input from the user.\n\nBackend: This part of the system communicates with the frontend.\n\nThe backend is responsible for reading and writing data to a database.\n\nThere could additionally be different frontends and different apps\n\naltogether that communicate with the backend in a more complex\n\nsystem.\n\nDatabase: The database is the part of the system that stores data. It’s a\n\ndata store, for example, a relational database such as MySQL or\n\nPostgreSQL. The database is responsible for storing data in a way\n\nthat’s efficient and easy to read and write.\n\nReporting: There’s often a reporting part that presents insights. It\n\ntakes its data from the data store and may need to transform the data to\n\nmake it presentable from a reporting perspective.\n\nFurther breakdown of the frontend into features Having an overview like this is useful but usually not enough of a\n\nbreakdown for us to start writing prompts. We need to break it down\n\nfurther, usually by features. At this point, a further breakdown of the\n\nfrontend into features may look like the following list:\n\nLogin: The user needs to be able to log in to the system.\n\nLogout: The user needs to be able to log out of the system.\n\nView inventory: The user needs to be able to view the inventory.\n\nAdd inventory: The user needs to be able to add the inventory.",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Remove inventory: The user needs to be able to remove the inventory.\n\nUpdate inventory: The user needs to be able to update the inventory.\n\nGenerate prompts for each feature At this point, it’s granular enough for us to start writing prompts. Let’s take\n\nthe first feature, login, and see how we can start thinking about how to craft\n\na prompt.\n\nYou may start with a prompt, using a tool like ChatGPT or GitHub Copilot\n\n(more on that in an upcoming next chapter) like the following:\n\n[Prompt]\n\nCreate a login page for the user to log in to the system.\n\n[End of prompt]\n\nWhile this may work, you’re leaving out a lot of context, such as what\n\nexactly is needed here and what’s not, what technologies are you using,\n\nwhat the user experience is, and so on.\n\nLet’s try to improve the prompt by adding more context:\n\n[Prompt]\n\nCreate a login page with fields for the username and password. It should\n\nhave a link for creating a user and a login button, be vertically and\n\nhorizontally centered, and work well on a mobile phone and tablet. It\n\nshould be written in React and use the Material UI library.\n\n[End of prompt]",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Identify some basic principles for web development, a “prompt strategy” As you have seen so far, we broke down the problem into smaller, more\n\n“manageable” pieces and we suggested some prompts for how to solve a\n\nspecific feature. So what exactly are we suggesting in terms of “prompt\n\nstrategy” now that we understand more about our example “inventory\n\nmanagement”? For one, we realize that our strategy will be context-\n\ndependent. Because we’re in web development, we need to use keywords\n\nwithin that domain, together with libraries and architecture suitable for a\n\nspecific area. Here are some guides we suggest you use:\n\nProvide context – fields: A login screen can be as simple as username\n\nand password fields. Most screens however have more fields, such as a\n\npassword confirmation field, a link to reset a password, a field to\n\ncreate a new user, and so on. Depending on your needs, you may need\n\nto be very detailed.\n\nSpecify how – design, and tech choices: A login screen can be\n\ndesigned in many ways. It’s quite common today to optimize for\n\ndifferent devices like tablets, mobile, large screens, and so on. For tech\n\nchoices, web development has a lot of choices, from frameworks like\n\nReact, Vue, and Angular to plain HTML and CSS. Specify according\n\nto the needs of the project.\n\nIterate: Different tools react differently to the same prompt.\n\nThroughout this book, we will show you how to use different tools like\n\nGitHub Copilot and ChatGPT. Each tool has its own strengths and\n\nweaknesses and may offer different results. Try to iterate on the",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "prompt by adding separators like commas and colons, and try\n\nrephrasing the prompt.\n\nBe context-aware: When you use tools like ChatGPT and GitHub\n\nCopilot, you do so with preexisting context. For ChatGPT, that means\n\nyou’re having an ongoing conversation of prompts and responses, and\n\nfor GitHub Copilot, it means it sees not only what you’ve written in\n\nyour open file but also your entire workspace if you let it. The\n\nresponse to your prompts looks at this context and decides what to\n\ngenerate. It’s important to be aware of this context and if you’re not\n\ngetting the response you want, try to change the context, in ChatGPT,\n\nstart a new conversation, and in GitHub Copilot, close the open files, start writing in a blank file, and so on.\n\nPrompt strategy for data science Let’s do a similar thought experiment for data science as we did for web\n\ndevelopment. We’ll use the presented guidelines “problem breakdown” and\n\n“generate prompts,” and just like in the web development section, we’ll draw some general conclusions on the domain and present those as a\n\nprompt strategy for data science.\n\nProblem breakdown: predict sales Let’s say we’re building a machine-learning model to predict sales. At a\n\nhigh level, we understand what the system should do. To solve the problem",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "though, we need to divide it into smaller parts, which in data science\n\nusually entails the following components:\n\nData: The data is the part of the system that stores information. The\n\ndata can come from many places like databases, web endpoints, static\n\nfiles, and more.\n\nModel: The model is responsible for learning from the data and\n\nproducing a prediction that’s as accurate as possible. To predict, you\n\nneed an input that produces one or more outputs as a prediction.\n\nTraining: The training is the part of the system that trains the model.\n\nHere, you typically have part of your data as training and a part being\n\nsample data.\n\nEvaluation: To ensure your model works as intended, you need to\n\nevaluate it. Evaluation means taking the data and model and producing\n\na score that indicates how well the model performs.\n\nVisualization: Visualization is the part where you can gain insights\n\nvaluable for the business via graphs. This part is very important, as it’s\n\nthe part that’s most visible to the business.\n\nFurther breakdown into features/steps for data science At this point, you’re at too high a level to start writing prompts. We can\n\nbreak it down further by looking at each step:\n\nData: The data part has many steps, including collecting the data, cleaning it, and transforming it. Here’s how you can break it down:\n\n1. Collect data: The data needs to be collected from somewhere. It\n\ncould be a database, a web endpoint, a static file, and so on.",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "2. Clean data: The data needs to be cleaned. Cleaning means\n\nremoving data that’s not relevant, removing duplicates, and so\n\non.\n\n3. Transform data: The data needs to be transformed.\n\nTransformation means changing the data to a format that’s\n\nuseful for the model.\n\nTraining: Just like the data part, the training part has many steps to it.\n\nHere’s how you can break it down:\n\n1. Split data: The data needs to be split into training and sample\n\ndata. The training data is used to train the model and the sample\n\ndata is used to evaluate the model.\n\n2. Train model: The model needs to be trained. Training means\n\ntaking the training data and learning from it.\n\nEvaluation: The evaluation part is usually a single step but can be\n\nbroken down further.\n\nGenerate prompts for each step Note how our breakdown for data science looks a bit different from web\n\ndevelopment. Instead of identifying features like Add inventory, we\n\ninstead have a feature like Collect data.\n\nHowever, we’re on the correct level to author a prompt, so let’s use the\n\nCollect data feature as our example:\n\n[Prompt]\n\nCollect data from data.xls and read it into a DataFrame using Pandas library.",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "[End of prompt]\n\nThe preceding prompt is both general and specific at the same time. It’s\n\ngeneral in the sense that it tells you to “collect data” but specific in that it\n\nspecifies a specific library to use and even what data structure (DataFrame).\n\nIt’s entirely possible that a simpler prompt would have worked for the\n\npreceding step like so:\n\n[Prompt]\n\nCollect data from data.xls.\n\n[End of prompt]\n\nThis is where it may vary depending on whether you use a tool like\n\nChatGPT or GitHub Copilot.\n\nIdentify some basic principles for data science, “a prompt strategy for data science” Here, we’ve identified some similar principles as in the web development\n\nexample:\n\nProvide context – filename: A CSV file can have any name. It’s important to specify the name of the file.\n\nSpecify how – libraries: There are many ways to load a CSV file, and\n\neven though Pandas library is a common choice, it’s important to specify it. There are other libraries to work with and you might need a\n\nsolution for Java, C#, and Rust, for example, where libraries are named\n\ndifferently.",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Iterate: It’s worth iterating on the prompt, rephrasing it, and adding\n\nseparators like a comma, a colon, and so on.\n\nBe context-aware: Also here, context matters a lot; if you’re working\n\nin Notebook, previous cells will be available to GitHub Copilot,\n\nprevious conversations will be available to ChatGPT, and so on.\n\nAs you can see from the preceding guidance, the strategy is very similar for\n\nweb development. Here we’re also listing “Provide,” “Specify how,”\n\n“Iterate,” and “Be context-aware.” The big difference lies in the details.\n\nHowever, there’s an alternate strategy that works in data science and that’s\n\nlengthy prompts. Even though we’ve broken down the data science problem\n\ninto features, we don’t need to write a prompt per feature. Another way of\n\nsolving it could be to express everything you want to be carried out in one\n\nlarge prompt. Such a prompt could therefore look like so:\n\n[Prompt]\n\nYou want to predict sales on the file data.xsl. Use Python and Pandas\n\nlibrary. Here are the steps that you should carry out:\n\nCollect data\n\nClean data\n\nTransform data\n\nSplit data\n\nTrain model\n\nEvaluation\n\n[End of prompt]\n\nYou will see examples in future chapters on data science and machine\n\nlearning where both smaller prompts as well as lengthier prompts are being\n\nused. You decide which approach you want to use.",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Validate the solution The most important part of this strategy is verifying correctness and that the\n\ntext and code created by the AI assistant are correct. There are two general\n\napproaches we can take to verify our outcome:\n\nVerification via prompts: The first approach is to use prompts to\n\nverify the outcome. This means writing prompts that question the\n\noutcome of specific results. This can be a good strategy to employ at\n\nthe beginning of your verification process. What you’re looking for are\n\nsituations where the AI assistant isn’t consistent in its responses.\n\nClassical verifications: The second approach is to use classical\n\nverification techniques. What those techniques are varies depending on\n\nthe problem domain. At a high level, though, it boils down to testing\n\ncode, comparing output, and relying on your own knowledge, and the knowledge of your peers, to verify the outcome.\n\nThe AI tool doesn’t really know what it’s doing. The responses provided are\n\nresponses that likely depend on its training corpus. At all times, you should\n\nbe aware of this and rely on your expertise to verify the outcome.\n\nIn the next subsections, let’s explore various approaches for manual and\n\nclassical verification.\n\nVeriﬁcation via prompts You can use prompts to both produce results that take you closer to\n\nresolving the problem but also to verify the results. Let’s take an example\n\nwhere we’re building the previously mentioned login screen. We’ve written\n\na prompt that looks like the following:",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "[Prompt]\n\nCreate a login page with fields for username and password; it should have a\n\nlink for creating a user and a login button. It should be vertically and\n\nhorizontally centered and work well on a mobile phone and tablet. It should\n\nbe written in React and use the Material UI library.\n\n[End of prompt]\n\nTo verify this outcome, we can write a prompt like the following:\n\n[Prompt]\n\nGiven the below code, what does it do?\n\n.login { <!-- should include CSS to center horizontally and vertically @media (min-width: 768px) { <!-- should include CSS to center horizontally and vertical } } <div class=\"login\"> <TextField id=\"username\" label=\"Username\" /> <TextField id=\"password\" label=\"Password\" /> <Button variant=\"contained\" color=\"primary\"> Login </Button> </div>\n\n[End of prompt]\n\nAn app could look at the code provided and realize you don’t need a prompt\n\nto deduce what it does, and that it’s in fact missing CSS code to make it\n\nresponsive. The point here though is that by writing prompts, you can have\n\nthe AI assistant tell you what it thinks the code does via questions.",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Using prompts this way, to pose queries on the output, is a good first step to\n\nverifying the outcome. However, it’s not enough and you need to rely on classical verification techniques, so let’s cover that next.\n\nClassical veriﬁcation How you verify the outcome depends on the problem domain. In web\n\ndevelopment, you can use a variety of different tools and techniques, for\n\nexample:\n\nTesting: With end-to-end testing or frontend testing, you can verify\n\nthat the code works as intended. Usually, this type of test involves\n\nusing a programmatic approach to simulate user interaction with the\n\nweb page, using something like Selenium, for example.\n\nManual testing: You can manually test the web page by opening it in\n\na browser and interacting with it. This is a good approach to use at the\n\nbeginning of your verification process. Apart from interaction, you can\n\nalso visually inspect the web page to see if it looks correct according to\n\nyour requirements.\n\nCode review: You can review the code and see if it looks correct. This\n\nis a good approach to use at the beginning of your verification process.\n\nIt allows not only you but also your peers to verify the outcome.\n\nTools: Tools can test a variety of different scenarios like accessibility,\n\nperformance, and so on. These tools are most likely a part of your\n\ndevelopment process already.\n\nConducting data science, you may rely on all of the preceding approaches,\n\nbut you may also use other approaches. Some common approaches are:",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Unit testing: You can use unit testing to verify that the code works as\n\nintended.\n\nIntegration testing: You can use integration testing to verify that the\n\ncode works as intended.\n\nValidation of results: This type of validation means you compare the\n\nresults of your analysis or model to known results or benchmarks.\n\nCross validation: This type of validation means you split your data\n\ninto training and sample data, train your model on the training data,\n\nand evaluate it on the sample data. This is a good approach to use at\n\nthe beginning of your verification process.\n\nSummary Throughout this chapter, we’ve provided a strategy for solving problems\n\nwith prompts and validating the solution.\n\nYou’ve seen how both web development and data science can be broken\n\ndown into smaller parts that can be solved with prompts. We also identified\n\nsome basic principles for writing prompts.\n\nFinally, we looked at how to validate the solution using prompts and\n\nclassical verification techniques.\n\nIt’s our hope that you will revisit this chapter when you’re looking at\n\nsolving a problem within web development or data science and you’re\n\nlooking for an approach.\n\nThere’s more to prompting than writing a prompt and getting a response.\n\nYou will see throughout this book how we use these principles in various\n\ndomains to solve problems. Try typing these prompts as you read, adapt to\n\nyour own needs, and see what happens.",
      "content_length": 1422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "In the next chapter, we are going to learn more about the two AI assistants\n\nof our choice, GitHub Copilot and ChatGPT.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "3\n\nTools of the Trade: Introducing Our AI Assistants\n\nIntroduction Writing code or text takes time, at least if you want it to be well organized\n\nand readable. But what if you could have a tool that would help you write\n\ncode faster and with less effort? That’s what GitHub Copilot and ChatGPT\n\nare really all about.\n\nBefore you start using an AI assistant, it’s a good idea to get a mile-high\n\nview of its capabilities and limitations. You want to see what you can and\n\ncan’t use it for, or at least understand where the tool performs less well.\n\nIn this chapter, we will cover the following:\n\nUnderstanding what GitHub Copilot and ChatGPT are and how they work\n\nLearning about Copilot’s capabilities and limits\n\nInstalling GitHub Copilot\n\nGenerating code completions via GitHub Copilot",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Understanding Copilot Pair programming is the idea of (usually two) developers working together,\n\noften in front of the same screen, also sometimes called “pairing.” GitHub\n\nCopilot can be seen as an “AI pair programmer” that helps you write code, enabling you to get more done, faster. It’s based on OpenAI’s Codex model,\n\na new AI system trained on publicly available source code and natural\n\nlanguage. But in reality, it has gone beyond this. Let’s denote GitHub\n\nCopilot as Copilot for the remainder of the book. Copilot suggests whole\n\nlines or entire functions right inside your editor.\n\nHow Copilot knows what to generate The idea behind Copilot is that it learns from the code you and others have written and uses that knowledge to suggest new lines of code as you type.\n\nHow does Copilot work? It uses machine learning to build a model of the\n\ncode you and others have written and suggests the best text for you to use next. There are two parts of importance, the trained model and the so-called\n\nin-memory context. The model is trained on public repositories on GitHub and the context is something it assembles at runtime from looking at your\n\nfiles. Using the context and the underlying model, it provides you with text suggestions. Copilot uses some of the following to build its context (i.e., its in-memory capability that it uses together with the trained model to provide\n\nsuggestions):\n\nYour active file: The code you’re working on.\n\nComments: Copilot uses comments to understand the context of your\n\ncode.",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Open files and your workspace: It not only looks at the code in your\n\nactive file but also at the code in other files in your workspace.\n\nImport statements: Even import statements are factored into Copilot’s suggestions.\n\nThe underlying model and its training data: The code in public GitHub repositories constitutes the base of what it’s trained on.\n\nCopilot capabilities and limits So, what can Copilot do? It can do a lot, but here’s a non-exhaustive list of\n\ncapabilities:\n\nCode completion: Copilot can complete lines of code for you.\n\nCode generation: Copilot can generate whole functions for you.\n\nTests, comments, and documentation: Copilot can generate tests, comments, and documentation for you.\n\nSuggest improvements: Copilot can suggest improvements to your code. Improvements can come in many forms, from suggesting a better variable name or a better way to write a function to how to organize\n\ncode better.\n\nTranslate code: Copilot can translate code from one language to\n\nanother. For example, it can translate Python code to JavaScript code.\n\nAnswer questions: Copilot can answer questions about your code. For\n\nexample, it can tell you what a function does or what a variable is used for, and answer questions about a domain such as “What is machine learning?”, for example.\n\nSetup and installation",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "How can you get started? You can use Copilot from a variety of places and\n\neditors including Visual Studio, Visual Studio Code, GitHub Codespaces, and GitHub’s web-based editor. In this chapter, we’ll use Visual Studio Code.\n\nInstalling Copilot To install Copilot, you need to install the GitHub Copilot extension for\n\nVisual Studio Code and also need to allow access.\n\nLet’s review the steps in more detail (as outlined on the official Copilot docs page).\n\nYou can install the GitHub Copilot extension for Visual Studio Code from\n\nthe Visual Studio Code Marketplace or from within Visual Studio Code. We\n\nwill show the latter here:\n\n1. In the Extension: GitHub Copilot tab in Visual Studio Code, select\n\nInstall.\n\n2. If you have not previously authorized Visual Studio Code in your\n\nGitHub account, you will be prompted to sign in to GitHub in Visual Studio Code.\n\n3. If you have previously authorized Visual Studio Code for your account\n\non GitHub, GitHub Copilot will be automatically authorized.\n\n4. If you don’t get the prompt to authorize, select the bell icon in the\n\nbottom panel of the Visual Studio Code window.\n\n5. In your browser, GitHub will request the necessary permissions for\n\nGitHub Copilot. To approve these permissions, select Authorize Visual Studio Code.\n\n6. To confirm the authentication, in Visual Studio Code, select Open in\n\nthe Visual Studio Code dialog box.",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Refer to this page if you have any problems getting Copilot to work:\n\nhttps://docs.github.com/en/copilot/getting- started-with-github-copilot.\n\nGetting started with Copilot How do we get started? Well, provided you have installed Copilot and\n\nthere’s a Copilot icon in the bottom-right corner of your Visual Studio Code\n\nwindow, you’re good to go.\n\nHere’s a suggestion to get started:\n\n1. Create a new file in Visual Studio Code named app.js.\n\n2. Start typing the text prompt “Express web api with routes products and customers” as a comment at the top of the file like so, and press Enter:\n\n//Express web api with routes products and customers\n\n3. Give it a few seconds and you should see a suggestion from Copilot as\n\nfollows:\n\nconst express = require('express');\n\nIf nothing appears, try pressing Ctrl + Spacebar to trigger a suggestion or start typing the start of the code,\n\ni.e., const, and wait for a suggestion to appear.\n\n4. You will have to press the Tab key to accept the suggestion. At this\n\npoint, Copilot can keep generating code for you. To ensure it does, press Enter and watch as Copilot generates more code for you.",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Repeatedly press Enter and press Tab to accept the suggestions until you have code similar to the following:\n\nconst app = express(); app.get('/products', (req, res) => { res.send('products'); }); app.get('/customers', (req, res) => { res.send('customers'); }); app.listen(3000, () => { console.log('Server listening on port 3000'); });\n\n5. Congratulations, you’ve just written your first lines of code with\n\nCopilot. Feel free to experiment with Copilot and try adding\n\ncomments, so-called prompts, in the middle of your code and see what happens. Also, try varying the prompts and see what happens.\n\nAssignment: improve the code As an assignment, you’re asked to improve the code generated by Copilot.\n\nHere are a few suggestions:\n\nAdd a route for the root of the web API.\n\nAdd a route for a specific product.\n\nAdd documentation for one of the routes.\n\nSolution Here’s a possible solution:\n\nconst express = require('express'); app = express();",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "// add default route app.get('/', (req, res) => { res.send('Hello world'); }); app.get('/products', (req, res) => { res.send('products'); }); // document route /** * Get a product by id * @param {number} id - The id of the product */ app.get('/products/:id', (req, res) => { res.send(`product with id ${req.params.id}`); }); app.get('/customers', (req, res) => { res.send('customers'); }); app.listen(3000, () => { console.log('Server listening on port 3000'); });\n\nChallenge See if you can add a test for one of the routes.\n\nIn the next chapter, we will look at how to use Copilot in more detail. To\n\nuse any AI assistant well, you need to understand how it works and how to use it. There’s a skill associated with using these tools well and it’s called\n\nprompt engineering. Prompt engineering is the art of writing prompts, not\n\nonly to make it understand your intentions but also to produce an output\n\nyou’re happy with. It’s more than just writing a comment; you can instruct\n\nyour AI assistant to solve something, apply a form of reasoning to it, and\n\nmuch more. The next chapter presents the central theme of this book,\n\nprompt engineering.",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "References Copilot landing page:\n\nhttps://github.com/features/copilot\n\nCopilot docs:\n\nhttps://docs.github.com/en/copilot/getting-\n\nstarted-with-github-copilot\n\nUnderstanding ChatGPT ChatGPT, an OpenAI development, is a specialized version of the GPT\n\nmodel designed to simulate human-like conversations. It excels in creating\n\nhuman-like text in dialogues, handling a variety of topics. It’s available for free at chat.openai.com, with a premium ChatGPT Plus option (also known as GPT-4), and can draft essays, generate art prompts, and program\n\ncode. The premium version offers enhanced features such as visual and\n\naudio input and output handling, file uploads, code execution, data\n\nvisualization with select Python libraries, and customizable GPT\n\ncapabilities.\n\nYou can access ChatGPT simply by visiting chat.openai.com and creating an OpenAI account. It is also available as an app for both Android\n\nand iOS. More details can be found on the official website\n\n(https://openai.com/).",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Figure 3.1: Offerings of OpenAI\n\nHow does ChatGPT work? ChatGPT, paralleling Copilot’s code-oriented approach but in natural\n\nlanguage processing, is adept at content generation, challenging traditional\n\nsearch engines. It excels in tasks such as essay writing and summarizing\n\ntexts. The quality of ChatGPT’s responses heavily depends on the prompts\n\nit receives.\n\nChatGPT leverages extensive training data including books, websites, and a\n\nvariety of textual sources for comprehensive language understanding.\n\nIt employs sophisticated machine learning algorithms, such as deep learning neural networks based on the Transformer architecture, to predict accurate\n\nand contextually relevant text responses.\n\nChatGPT’s contextual understanding is honed through advanced\n\ntechniques, enabling it to interpret and respond to varying conversation",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "threads intelligently. This approach mirrors the principles used in Copilot\n\nfor code, adapted here for nuanced, human-like text interaction.\n\nChatGPT capabilities and limits Capabilities of ChatGPT:\n\nContent creation: Generates creative content including marketing\n\nmaterial, blog posts, stories, and poems\n\nEducational explanations: Offers detailed explanations on complex\n\ntopics for educational purposes\n\nCoding assistance: Assists developers with code optimization, error\n\ndebugging, and algorithm design\n\nLearning aid: Acts as a companion in online learning, offering real-\n\ntime assistance and clarification of concepts\n\nConversational AI: Enhances user experience in virtual assistants and\n\nchatbots through natural language interactions\n\nLimitations and concerns of ChatGPT:\n\nAccuracy issues: ChatGPT may generate responses with factual\n\ninaccuracies or biases from training data, also known as hallucinations. These outputs often emerge from the AI model’s inherent biases, lack\n\nof real-world understanding, or training data limitations. In other\n\nwords, the AI system “hallucinates” information that it has not been\n\nexplicitly trained on, leading to unreliable or misleading responses.\n\nHence, users should always verify and validate the responses and\n\nshould not use them blindly.\n\nEthical implications: Raises concerns about the misuse of AI-\n\ngenerated content for fraudulent activities or harmful information",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "gathering.\n\nEmployment impact: Fear of AI replacing human jobs in certain\n\nsectors.\n\nSecurity risks: Potential use in phishing, creating malware, and cybercriminal activities.\n\nData privacy: Concerns about the use of vast internet data in training,\n\nimpacting user privacy.\n\nMessage cap: At the time of writing the book, GPT-4 was capped to\n\noffer a maximum of 40 responses over 3 hours.\n\nLimited Python libraries for code execution: The Code Interpreter\n\nand Advanced Data Analysis features of ChatGPT use limited sets of\n\nlibraries, heavily featuring machine learning libraries but not such\n\ngreat support for other libraries, such as Keras or TensorFlow, required\n\nfor deep learning.\n\nSetup and installation Setting up and installing ChatGPT involves a few steps:\n\n1. Create an OpenAI account: Visit the OpenAI website and sign up for\n\nan account.\n\n2. API access: Developers need to obtain API access by applying on the\n\nOpenAI platform.\n\nFor non-developers, using ChatGPT is as simple as visiting the ChatGPT\n\nwebsite or installing the Android or iOS app and logging in with your\n\nOpenAI account. No installation is required for general use. For more detailed steps and information, please refer to OpenAI’s official\n\ndocumentation and website.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Getting started with ChatGPT Once you’ve logged in to your OpenAI account on the ChatGPT side of the\n\nwebsite, it’s time to get to know the AI tool’s window. Here’s a breakdown of what you will see:\n\nNew chat and hide sidebar buttons: On the left side of your screen,\n\nthe New chat button can be used to start fresh conversations at any\n\ntime. It creates a new discussion without context. There’s also an option to hide the sidebar.\n\nChat history: The left sidebar keeps your previous conversations\n\naccessible. You can edit chat titles, share your chat history, or delete it.\n\nOptionally, you can turn off chat history.\n\nAccount: Click your name at the bottom left to access your account\n\ninformation. This includes settings, log out, help, and FAQs. If you\n\ndon’t have ChatGPT Plus, you’ll see an Upgrade button here.\n\nYour prompts: Your questions or prompts appear in the middle of the\n\nchat window, accompanied by your account photo or initials.\n\nChatGPT’s responses: ChatGPT’s responses display the logo on the\n\nleft. On the right, you’ll see options such as Copy, Thumbs Up, and\n\nThumbs Down. Copy text to your clipboard for use elsewhere and\n\nprovide feedback on response accuracy.\n\nRegenerate response: Click Regenerate response if you encounter\n\nissues or unsatisfactory answers. It prompts ChatGPT to generate a\n\nnew reply based on your latest prompt.\n\nText area: This is where you enter your prompts and questions.\n\nChatGPT version: Below the text input area, you’ll find fine print,\n\nincluding a disclaimer: “ChatGPT can make mistakes. Consider",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "checking important information.” Note that the display of the ChatGPT\n\nmodel version has been discontinued.\n\nThe following screenshot illustrates how this looks.\n\nIn the top-left corner, you can see the GPTs you have access to if you have\n\nthe premium version.\n\nAt the bottom are your previous conversations.\n\nIf you have the premium version, you can choose GPT-4 from the dropdown along with plugins.\n\nFigure 3.2: Selecting different versions of ChatGPT\n\nYou can even set custom instructions at the profile level if you wish to\n\napply your configuration to all new conversations.",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Figure 3.3: ChatGPT custom instructions\n\nPrompting Let’s draft our first prompt with ChatGPT.",
      "content_length": 93,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "You just have to ask questions in your natural language and converse with it\n\nlike you would with a human and it will start sharing its knowledge with\n\nyou.\n\n[Prompt]\n\nCan you please explain the process of machine learning in bullet points to\n\nme?\n\n[End of prompt]\n\nYou should see a response similar to the following screenshot. Note that the\n\nresponses are never identical, and you will not get the exact same text each\n\ntime.\n\nFigure 3.4: ChatGPT prompt screen",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Summary In this chapter, we’ve looked at GitHub Copilot and ChatGPT, including\n\nwhat they are, how they work, and how to get started with them.\n\nWe’ve also looked at some of their capabilities and limitations.\n\nFinally, we’ve looked at how to install them and work with them. You were\n\nalso given some idea of how to use them via prompts. 3\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "4\n\nBuild the Appearance of Our App with HTML and Copilot\n\nIntroduction Building a web app involves the usage of HTML for markup, CSS for\n\nstyling, and JavaScript for making it interactive.\n\nWe’ve come a long way from building a web app from static pages in the\n\n1990s to now using frameworks to build large apps. Regardless of whether\n\nyou use a framework or a library, it still rests on the same foundation,\n\nHTML, CSS, and JavaScript.\n\nTo tackle these three markup and programming languages, we can use an AI\n\nassistant. There’s more to using an AI assistant than generating text, given\n\ntext input. You also need working knowledge of the area you’re trying to tackle. For markup languages like HTML and CSS, “working knowledge”\n\nmeans you should know how to structure a web page or configure the\n\nstyling with CSS. In short, you know how to do the task at hand, and the AI\n\nassistant is there to make you faster and more efficient.",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "The output from the prompts mentioned in this chapter may\n\nvary based on training data, what files you have open, and\n\nwhat you typed previously.\n\nYou will see throughout the different chapters of this book how we will\n\nfollow a specific method of first discussing a business problem we’re looking to solve, with web development or data science merely being\n\napproaches that help us solve the problem. We will then focus on the\n\nproblem, which varies depending on whether we’re a web developer or data\n\nscientist, followed by dividing up our problem into smaller more\n\nmanageable parts. Finally, we will recommend a “prompt” strategy that works well for this particular type of problem.\n\nIn this chapter, we will:\n\nGenerate basic HTML: GitHub Copilot is capable of generating\n\ndifferent kinds of code, including HTML.\n\nApply prompting techniques: There are different techniques we can use to get the desired content.\n\nBusiness problem: e- commerce E-commerce is quite an interesting domain. There are many problems to be\n\nsolved within this domain. For example, you need to provide a technical platform that allows users to purchase items. That in itself means you need\n\nto build various solutions for taking payments as well as products to buy and also some logistics that allow for shipping and more.",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "If you look at this business from the data side of things, you see that you\n\nneed to analyze customer behavior to ensure you have the right number of items in stock, the correct prices on the items, and so on. In short, e- commerce is an intriguing domain that you will see mentioned throughout\n\nthe book.\n\nProblem domain This chapter focuses on the role of the web developer, so let’s discuss what\n\ntype of problems await a web developer in e-commerce. There are usually\n\ntwo to three major roles you need to solve for as a web developer:\n\nThe customer and all the actions that a customer can take like browsing and purchasing items to managing their account.\n\nBack office: This is the company behind the e-commerce application.\n\nHere, you need to ensure there exist technical solutions so that company employees can manage inventory, product information,\n\npayment solutions, and more.\n\nFrom a data standpoint, you, as a web developer, need to ensure that\n\ndata can be stored and updated on areas like products, purchase orders, and customer information.\n\nProblem breakdown: identify the features To start breaking down this problem domain into something we can write\n\nprompt input for, let’s again turn to the roles we mentioned, customer and back office. Here’s an attempt at breaking the problem down into features we can build.",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Let’s start with the customer role and the major area, “Authentication.”\n\nLet’s attempt to break it down into things the customer should be able to do. Here are the actions we should support:\n\nLogin: The user should be able to log in.\n\nLogout: The user should be able to log out.\n\nCreate a new user: It should be possible to create a new user.\n\nUpdate password: An existing user should be able to update their\n\npassword.\n\nPassword: If a user forgets their password, it should be possible to reset it in a safe way.\n\nNow, we have a set of features for a specific domain, “Authentication,” and\n\nwe have a better grasp of the different actions we should support. We’ll\n\nleave it to you to further break down your problem domain like this, but see the preceding list as the detail level you should preferably be on before you\n\nstart using your AI assistant.\n\nGiven the preceding breakdown into features, you could now, for example,\n\ntype a prompt like so to attempt solving the first feature we identified above:\n\n[Prompt]\n\nGenerate a login page, with fields for username, password, and repeat\n\npassword and login button.\n\n[End of prompt]\n\nAs a web developer, you’ve usually already done this breakdown of the problem domain into features before you start developing and even called\n\nthese “user stories” if you use a development methodology like Scrum, for\n\nexample.",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "With web development, though, you know it’s a matter of looking at this problem from three different layers, the frontend, backend, and the data\n\nlayer, usually a database where you store the data. The rest of this chapter\n\nwill focus on the frontend when using the AI assistant. In later chapters, we will focus on other layers of the e-commerce example.\n\nPrompt strategy So, how do we select a prompt strategy and what do we mean by prompt\n\nstrategy? Our strategy is about how we will prompt; will we write one prompt per feature or many short ones? It’s also about how we use our\n\nchosen AI assistant, GitHub Copilot, and how we choose to input the\n\nprompts into the tool.\n\nIn GitHub Copilot, there are two major choices for how you write your prompts, either using the chat functionality or via typing comments or code\n\ndirectly in a text file. In this chapter, we will use the latter approach of\n\ntyping directly in the text file. The general recommendation is that both\n\napproaches are valid and, it’s our experience that you vary between these two approaches as you solve a problem.\n\nNow that we’ve chosen our GitHub Copilot approach, what about the\n\nprompts themselves? We will choose a prompt strategy here where we type\n\nshorter prompts – we refer to this pattern as “Exploratory prompt pattern” in Chapter 2 of the book. We will let GitHub Copilot build up its runtime\n\ncontext and learn from our code as we type it.\n\nIn the upcoming section, we will showcase how you can start generating\n\nmarkup code while being inside an open text file. At the end of this chapter, you will see how we revisit our e-commerce use case.",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Page structure A web page is defined in HTML, and all such pages are made up of a tree\n\ncalled a document object model (DOM). The DOM has the following\n\nparts:\n\n<html> <head> </head> <body> </body> </html>\n\nYou can see how the markup of the page consists of elements. The top\n\nelements need to be laid out hierarchically with HTML being the root\n\nelement having the inner nodes HEAD and BODY. In the HEAD node, we\n\ndefine things like styling, instructions for search engines, page title, and\n\nmore. In the BODY element, we place content we want to be visible.\n\nAdd AI assistance to our page structure How can we leverage GitHub Copilot for this? Normally, web apps should\n\nhave an index.html as an entry point for the app. To leverage GitHub\n\nCopilot, create a comment, <!-- my comment -->, on the page. You need to\n\nreplace my comment with your prompt. So, what do you type instead of my comment? The answer is to provide GitHub Copilot with enough context for\n\nit to know what to generate in response.",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "GitHub Copilot builds a runtime context not only based on its training model but also based on file endings like .js or\n\n.html, and what text is in both open or closed files in the directory you’re in. This context is what decides what text\n\nshould be generated based on what prompt you type.\n\nYour ﬁrst prompt, simple prompting, and aiding your AI assistant Let’s try writing our first prompts in an open text file.\n\nIf you want to try the following prompts yourself while\n\nreading this, create an index.html page. Prompts are\n\ncomments that you write. The comment should use the\n\nassigned comment format for the chosen type of file, meaning <!-- --> is how you comment in an HTML page, and // is how you write a comment in a JavaScript file.\n\nHere’s a prompt you can try adding as an HTML comment at the top of the page:\n\n[Prompt]\n\nCreate a page with a title and list.\n\n[End of prompt]\n\n<!-- create a page with a title and list -->.",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "In this case, this prompt generates another prompt, namely, <!-- the list\n\nshould contain 3 items -->.\n\nAt first glance, this might look confusing. Why would GitHub Copilot give\n\nme another comment as output? The explanation is that your AI assistant is\n\ntrained on code and comments. In this case, GitHub Copilot provides you\n\nwith the most likely response given its training data.\n\nSo, how do you get HTML markup as a response instead? The answer is that you start typing HTML, which narrows it down for GitHub Copilot, so\n\nthe code in your text file should look like the following:\n\n<!-- create a page with a title and list --> <HTML\n\nGitHub Copilot now generates code that should look similar to the\n\nfollowing output:\n\n[Prompt response]\n\n<html lang=\"en\">\n\n[End of response]\n\nIf you keep hitting the Return key, and accepting the suggestions with Tab, more and more code will be generated so you end up with an output like so:\n\n[Prompt response]\n\n<!-- create a page with a title and list --> <html lang=\"en\"> <head> <title>My First Page</title> </head>",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "<body> <h1>My First Page</h1> <ul> <li>First item <li>Second item <li>Third item </li> </body> </html>\n\n[End of response]\n\nYour second prompt: adding more context Let’s erase what we’ve done so far and start with a new prompt. The\n\nobjective is to understand how writing a better-written prompt, with more\n\ncontext, yields a better output.\n\nLet’s try a second prompt, with more context and have it look like the\n\nfollowing text:\n\n[Prompt]\n\nCreate a page with a title and list, each item should have fields title,\n\ndescription and completed.\n\n[End of prompt]\n\nNote how we’re now saying that the list consists of items\n\nand that each item has separate fields. So, what’s the output\n\nof this prompt? Like before, use the Return key to generate",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "a prompt response and Tab to accept the completion\n\nprovided by GitHub Copilot.\n\n[Prompt response]\n\n<!-- create a page with a title and list, each item should have <html> <head> <title>Todo List</title> </head> <body> <h1>Todo List</h1> <ul> <li> <h2>Item 1</h2> <p>Item 1 description</p> <p>Completed: <input type=\"checkbox\" checked></p> </li> <li> <h2>Item 2</h2> <p>Item 2 description</p> <p>Completed: <input type=\"checkbox\"></p> </li> </ul>\n\n[End of response]\n\nOur AI assistant is able to guess that we’re trying to build a so-called “Todo\n\napp,” a common type of app where we have tasks we’re looking to complete. It was also able to generate a header element h2 for the title, a p for the description, and a checkbox for the Completed field. By adding more\n\ncontext, our AI assistant was able to provide a better output.",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Your third prompt: accept prompt suggestions So far, you’ve seen the difference between the two prompts, but what\n\nhappens if we add even more context like so?\n\n[Prompt]\n\ncreate a page with a header, main part, and a footer. Use dark theme.\n\n[End of prompt]\n\nAt this point, the output consists of more comments (suggestions), and if\n\nyou accept several of them by entering the Return key (to get a new\n\nsuggestion) and the Tab key (to accept the completion), your text file should\n\nnow have content similar to the following output:\n\nIt’s also possible to reject the suggestion by, for example,\n\ncontinuing to type.\n\n[Prompt response]\n\n<!-- create a page with header, main part and footer use dark t <!-- header should have a navigation with 3 links --> <!-- main part should have 3 sections with some text and images <!-- footer should have 3 columns with some text and images --> <!-- use flexbox for layout --> <!-- use css variables for colors --> <!-- use css variables for font sizes --> <!-- use css variables for paddings and margins --> <!-- use css variables for font family --> <!-- use css variables for border radius --> <!-- use css variables for border width -->",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "<!-- use css variables for border color --> <!-- use css variables for background color -->\n\n[End of response]\n\nGitHub Copilot can suggest prompts that you might not even consider like navigation, images, and the use of CSS variables. If you type <html at this\n\npoint, you see an output similar to:\n\n[Prompt response]\n\n<!-- create a page with header, main part and footer use dark t <!-- header should have a navigation with 3 links --> <!-- main part should have 3 sections with some text and images <!-- footer should have 3 columns with some text and images --> <!-- use flexbox for layout --> <!-- use css variables for colors --> <!-- use css variables for font sizes --> <!-- use css variables for paddings and margins --> <!-- use css variables for font family --> <!-- use css variables for border radius --> <!-- use css variables for border width --> <!-- use css variables for border color --> <!-- use css variables for background color --> <html> <head> <title>06.6 solution</title> <link rel=\"stylesheet\" href=\"style.css\"> </head> <header> <nav> <a href=\"#\">Home</a> <a href=\"#\">About</a> <a href=\"#\">Contact</a> </nav> </header> <body> <main> <section>",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "<h1>Section 1</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> <section> <h1>Section 2</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> <section> <h1>Section 3</h1> <p>Lorem ipsum dolor sit amet consectetur adipisici <img src=\"https://picsum.photos/200/300\" alt=\"rando </section> </main> <footer> <div> <h1>Column 1</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> <div> <h1>Column 2</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic Quisquam, voluptatum.</p> <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> <div> <h1>Column 3</h1> <p>Lorem ipsum dolor sit amet consectetur adipisic <img src=\"https://picsum.photos/200/300\" alt=\"rand </div> </footer>\n\n[End of response]\n\nView this output in a browser and it should display the preceding sections\n\nthat contain the title, description, and image for a number of items (exactly\n\nwhat you will see varies as these images are random).",
      "content_length": 1102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "As a web developer, you take this for what it is: a nice first scaffold that you\n\nneed to add styling to at a later stage to make it look better.\n\nFigure 4.1: Page with images (without the header and footer)\n\nChallenge: vary the prompt You can vary the output by adding or removing context. As a suggestion,\n\nsee if you can change the prompt to add tables, forms, a multi-column",
      "content_length": 378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "layout, and more.\n\nUse case: build a front for an e-commerce Now that we’ve learned how to use GitHub Copilot for creating HTML\n\nmarkup, let’s use it for a more challenging project: an e-commerce website.\n\nThe requirements are that it should be a site with the following parts:\n\nA login page\n\nA user creation page\n\nA products page for products to put in a basket\n\nA basket page to show items you mean to purchase\n\nA checkout page showing your basket content and form for payment\n\ninformation and address info for shipping\n\nAn admin page that allows you to edit your products\n\nLet’s start with these pages as they are commonly found on e-commerce\n\nwebsites.\n\nLogin page The requirements for a login page are that you as a user can enter a\n\nusername and password and that there’s a button verifying your credentials.\n\nOptionally, you can add links for creating a user and a link if you forgot\n\nyour password.\n\nTry the following prompt as a comment at the top of a page called login.html:",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "[Prompt]\n\n<!-- a page with input fields user name and password and a logi\n\n[End of prompt]\n\nYou may need to start typing <html> for the completion to\n\nbe generated. Also, ensure you close all files except for the\n\ncurrent one to make sure that your AI assistant is not\n\ndistracted by other files that might be open.\n\nYou should see an output similar to:\n\n[Prompt response]\n\n<html> <head> <title>Login</title> <link rel=\"stylesheet\" href=\"css/login.css\"> </head> <body> <div class=\"container\"> <div class=\"login\"> <h1>Login</h1> <form action=\"login.php\" method=\"post\"> <input type=\"text\" name=\"username\" placeholder= <input type=\"password\" name=\"password\" placehol <button type=\"submit\" class=\"btn btn-primary bt </form> <a href=\"register.php\">Create User</a> <a href=\"forgot.php\">Forgot Password</a> </div> </div>",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "[End of response]\n\nThis renders like so:\n\nFigure 4.2: Login page\n\nProduct list page On this page, we want to see a list of products we can buy, some details on\n\neach product, and an “Add to Cart” button. For this, we will create a products.html page and give it the following prompt:\n\n<!-- list of products as div elements where each item has field\n\nYou should see an output similar to:\n\n[Prompt response]\n\n<!-- list of products as div elements where each item has field <html> <head> <title>Products</title> <link rel=\"stylesheet\" href=\"css/products.css\"> </head> <body> <div class=\"container\"> <div class=\"products\">",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "<h1>Products</h1> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 1</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 2</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 3</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a> <button type=\"submit\" class=\"btn btn-primar </div> </div> <div class=\"product\"> <div class=\"product-image\"> <img src=\"https://picsum.photos/200/300\" al </div> <div class=\"product-text\"> <h2>Product 4</h2> <p>Lorem ipsum dolor sit amet consectetur a <a href=\"product.php\">View Product</a>",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "<button type=\"submit\" class=\"btn btn-primar </div> </div> </div> </div> </body> </html>\n\n[End of response]\n\nThis should render something like so:",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 4.3: Product list page\n\nRemaining pages We will leave it as an exercise for you to produce the remaining pages.\n\nRemember to create a dedicated HTML page and put a prompt comment at the top of the page.",
      "content_length": 209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Here’s some suggested prompts for the remaining pages:\n\nA user creation page.\n\nHere is a suggested prompt:\n\n<!-- a page with fields username, password, repeat password\n\nA basket page to show items you mean to purchase.\n\nHere is a suggested prompt:\n\n<!-- a page showing a list of items in a basket, each item\n\nA checkout page showing your basket content and form for payment\n\ninformation and address info for shipping.\n\nHere is a suggested prompt:\n\n<!-- a checkout page containing a section for payment info\n\nAn admin page that allows you to edit your products.\n\nHere is a suggested prompt:\n\n<!-- a section that's a list of products, each item has fields t\n\nAssignment",
      "content_length": 667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "In this assignment, you will create a resume website. What context you\n\nprovide GitHub Copilot with is up to you but start by creating an index.html and an HTML comment, <!-- my prompt -->.\n\nRemember the techniques you were taught.\n\nWrite a prompt\n\nWrite a prompt and start typing the code/markup on the next\n\nline to help your assistant. Use the Return key to generate a\n\nresponse and the Tab key to accept the suggested text.\n\nRewrite the prompt and add or change what it says to get the desired result.\n\nYou can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/04\n\nChallenge Given your built resume, you can improve it further by adding colors. How\n\nwould you prompt to do so?\n\nQuiz Here’s a set of questions to ensure you’ve grasped the key concepts:\n\n1. The text you send to your AI assistant is called:\n\na. text",
      "content_length": 946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "b. instruction\n\nc. prompt\n\n2. Your AI assistant builds a context from:\n\na. what you type\n\nb. what you type, the file ending, and the open and closed files in\n\nyour working directory\n\nc. what you type and the file ending\n\nYou can find the solution to this quiz in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/04\n\nSummary In this chapter, we covered how to generate HTML markup using GitHub\n\nCopilot. We also covered how to use prompting techniques and how to add\n\ncontext to your prompts. As part of learning these prompting techniques, we\n\ndiscovered that the more context you give your AI assistant, the better the\n\noutput. You also build up context over time as you add more content to your\n\npage.\n\nAdditionally, we started on a use case where we started building an e-\n\ncommerce website. This use case is something we will continue to build on\n\nin the coming chapters.\n\nFor the next chapter, we will continue to cover web development but shift\n\nour focus to CSS and styling. You will see how the same or similar\n\nprompting techniques can be used for CSS as well.",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "5\n\nStyle the App with CSS and Copilot\n\nIntroduction Styling an app well can make a huge difference in how a user perceives it.\n\nWell-thought-out styling includes catering to multiple devices, the smart use\n\nof graphics, and colors with great contrast.\n\nCSS styling is a big topic, and we will not cover it in detail. However, we\n\nwill show how you can start using it. Just like in the previous chapter, we\n\nwill use our AI assistant to help us generate code. You will see how we will\n\nkeep using comment-based prompting to generate code but also a new\n\ntechnique where nothing but the file’s context is used for code generation.\n\nYou will also see how we will keep building on our e-commerce project and\n\nstyle it.\n\nIn this chapter, we will see how we can generate the CSS we need and how to keep applying the prompting patterns and strategies we’ve used in\n\nprevious chapters. We will continue to build on the e-commerce project and\n\ngive it an appealing appearance.\n\nIn this chapter, we will:",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Generate CSS: GitHub Copilot can generate styling, and we will\n\nshow how AI assistance can generate CSS both by looking at\n\nsurrounding code in a text file and based on a CSS comment.\n\nApply prompting techniques: There are different techniques we can\n\nuse to get the desired content.\n\nAdd CSS to our e-commerce project: We will select a couple of\n\npages in our e-commerce project to show how it benefits from styling.\n\nBusiness problem: e- commerce Just like the previous chapter, we will keep working in the e-commerce\n\ndomain and through its many interesting problems. As this chapter focuses\n\non visualization with CSS, what’s the connection to the business, you might\n\nwonder? A bad UX, or user experience, or an ill-designed site that doesn’t work on devices other than desktop or doesn’t cater to accessibility can cost\n\nyou money as, because of this, customers might choose to do business with your competitors.\n\nProblem and data domain This chapter continues with the e-commerce business domain and dives specifically into a basket page that lists products a customer aims to\n\npurchase. The data is therefore product data; not only that, but from a data aspect, we need to consider how to showcase detailed data relating to\n\nproducts, such as quantity and total cost, so the customer can decide what and how many items to buy. These considerations should be reflected in the chosen design.",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Breaking the problem down into features In the previous chapter, we chose to identify a larger area, “Authentication,”\n\nand break that down into specific features. Let’s recall what that feature\n\nbreakdown looked like. After that, we’ll see if we need to change it to\n\ninstead be more design oriented. But first, let’s show the list of features:\n\nArea: Authentication\n\nLog in: User should be able to log in.\n\nLog out: User should be able to log out.\n\nCreate new user: It should be possible to create a new user.\n\nUpdate password: An existing user should be able to update their password.\n\nReset password: If a user forgets their password, it should be possible\n\nto reset it in a safe way.\n\nThe above list of features constitutes a good high-level list of what we need to support. However, from a design viewpoint, we need to consider things like catering to different devices or support accessibility, for example.\n\nTherefore, a prompt for, let’s say, the first feature might need to be tweaked to look like so:\n\n[Prompt]\n\nGenerate a login page. It should have fields for username, password, and repeat password, as well as a login button. It should also support\n\naccessibility via tooltips and ARIA keys so that it can be used with just the keyboard and mouse.\n\n[End of prompt]",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "As you can see from the above prompt, our concern is not only with what\n\nUI elements we need, like inputs and buttons, but also how it should work.\n\nAs before, we recommend that you break down the web app you’re about to build into areas and each area into features so as to make prompting easier.\n\nPrompting strategy We mentioned in the previous chapter that you can both use the Chat mode of GitHub Copilot as well as typing inside of a file, and that you’re\n\nrecommended to use both approaches. As for how to prompt, you’re\n\nrecommended to write shorter prompts that you add context to as needed.\n\nBy doing so, you rely on how GitHub Copilot works and how it builds a runtime context based on its underlying model, what’s in your open\n\ndirectory, and your open file, among other things. You will also see another\n\nfacet of prompting that doesn’t rely on whether the prompt is long or short but rather on how your specific AI tool works. In this chapter, we use the\n\n“Exploratory prompt pattern” as described in Chapter 2.\n\nCSS, or Cascading Style Sheets CSS, or Cascading Style Sheets, is used to style HTML pages. It contains\n\nvarious constructs that allow you to change things such as fonts, sizes,\n\ncolors, and layouts.\n\nTo follow along with the below-mentioned prompts, you’re recommended to create a product.css file, or you can just",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "wait until the Assignment section later in this chapter; it is up to you.\n\nFirst CSS To leverage our AI assistant, we can place a CSS-type comment in a CSS\n\nfile just like we do with code, for example:\n\n.product { /* apply padding and margin */ padding: 10px; margin: 10px; }\n\nAs was mentioned in the previous chapter, comment styles\n\nare different depending on the type of file. HTML comments look like so: <!-- -->. CSS comments use /**/\n\nand JavaScript understands both CSS-type comments and //. In this chapter, you will be using /**/ as you’re dealing with CSS.\n\nIn GitHub Copilot, you can keep pressing the Return key, followed by the Tab key, to generate and accept new suggestions without needing to type a\n\nnew prompt. Your AI assistant can provide suggestions based on the first\n\ntyped prompt and what code exists in your file.\n\nHow this affects our CSS file above is that once we type our first CSS comment, we can keep pressing Return and Tab and the AI assistant will\n\ngenerate CSS properties that usually go together with padding and margin.",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "The following output is the result of using this technique of pressing the Return and Tab keys and relying on the open file’s context.\n\nThis is how it looks for product.css:\n\n.product { /* apply padding and margin */ padding: 10px; margin: 10px; /* apply border */ border: 1px solid #ccc; /* apply border radius */ border-radius: 5px; /* apply background color */ background-color: #fff; /* apply box shadow */ box-shadow: 0 0 5px #ccc; /* apply display */ display: inline-block; /* apply width */ width: 200px; /* apply text align */ text-align: center; /* apply text decoration */ text-decoration: none; /* apply color */ color: #333; }\n\nNow that product.css is created with the above content, we can include\n\nsaid CSS file in an HTML file. Switch to said HTML file (if you’re typing\n\nalong, create an index.html file), then add the following prompt at the end of the new file:\n\n[Prompt]\n\nPage that includes product.css.",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "[End of prompt]\n\n[Prompt response]\n\n<!-- page that includes product.css --> <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Product</title> <link rel=\"stylesheet\" href=\"product.css\"> </head> <body> <div class=\"product\"> some text <div class=\"product__image\"> <img src=\"https://via.placeholder.com/200x200\" alt= </div> <div class=\"product__info\"> <h2 class=\"product__name\">Product Name</h2> <p class=\"product__description\">Lorem ipsum dolor s <p class=\"product__price\">$9.99</p> <button class=\"product__buy\">Buy</button> </div> </div> </html>\n\n[End of response]\n\nObserve how it not only correctly adds a link tag to include product.css but it also generates HTML that might fit a product.\n\nThe following is the output:",
      "content_length": 739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Figure 5.1: Product CSS showing a product image, title, and description\n\nCSS by name So far, we’ve added a comment to generate suggestions. But there’s another\n\nway we can generate suggestions for our CSS file rather than placing a\n\ncomment for Copilot to react to. The “other” way consists of creating an empty CSS class, .product, and placing the cursor inside of it. Then use the\n\nReturn key and the Tab key and see what Copilot generates until you’re\n\nhappy.\n\nHere’s how you start off your CSS file:",
      "content_length": 503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": ".product {\n\n}\n\nNow place the cursor inside of the curly brackets, {}, and use the Return and Tab keys for each new line you want suggestions for.\n\nHere’s what the end result could look like:\n\n[Prompt response]\n\n.product { display: flex; flex-direction: column; align-items: center; justify-content: center; width: 100%; margin: 0 auto; padding: 0 0 50px 0; background-color: #f5f5f5; }\n\n[End of response]\n\nAssignment In this assignment, you will create an image gallery app. The gallery should\n\nshow 10 images with 5 images per row. Use what you learned in the\n\nprevious chapter to generate both an HTML file and a separate CSS file.\n\nUse “placeholder” as a keyword to get images.",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Your HTML file should add a reference to the generated\n\nCSS file to correctly apply the CSS.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nUse case: style the e- commerce app Now that you’ve carried out an assignment where you created both a CSS\n\nfile and an HTML file, you should have a better understanding of how to\n\nwrite prompts and generate and accept suggestions from GitHub Copilot.\n\nLet’s now recall the beginning of this chapter, where we talked about the\n\nbusiness problem and the e-commerce domain. We will now continue to\n\nsolve problems in this domain but focus on visualization using CSS.\n\nWe will continue with the use case we started with in the previous chapter:\n\nour e-commerce store. We will focus on the basket page and leave the\n\nremaining pages for you to implement.\n\nBasket page To style our basket HTML page, we create a basket.css file and then we\n\nobserve what CSS classes we introduced in the basket.html file. Let’s take",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "a look at a representative snippet:\n\n<div class=\"container\"> <div class=\"basket\"> <h1>Basket</h1> <div class=\"basket-item\"> <div> <h2>Product 2</h2> <p>Price: 200</p> <p>Quantity: 2</p> <p>Sum: 400</p> </div>\n\nIn the preceding code, we see the classes basket and basket-item. Let’s create CSS classes for those in basket.css:\n\nStart with a blank basket.css and start typing .basket. You should see output similar to the following when using the Return and Tab keys repeatedly. When you’re happy with the output, type } to finish the CSS\n\nclass.\n\n[Prompt response]\n\n.basket { width: 100%; height: 100%; display: flex; flex-direction: column; align-items: center; padding: 20px; }\n\n[End of response]\n\nTo create the basket-item class, type .basket-item and use Return and Tab repeatedly. Like before, type } when you’re happy with the number of",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "generated CSS properties for this CSS class. You should now have\n\ngenerated a CSS output similar to the below text:\n\n.basket-item { width: 100%; height: 100%; display: flex; flex-direction: row; align-items: center; padding: 20px; border-bottom: 1px solid #ccc; }\n\nIf we continue looking at our HTML file, we will see yet another snippet of\n\ninterest:\n\n<div class=\"basket-item-buttons\"> <button type=\"submit\" class=\"btn btn-primary btn-block btn- <button type=\"submit\" class=\"btn btn-primary btn-block btn- </div>\n\nUse the same prompting technique as before by typing the name of the CSS class (.basket-item > .basket-item-button) and repeatedly using RETURN\n\nand TAB to generate the below text:\n\n.basket-item > .basket-item-buttons { display: flex; flex-direction: column; align-items: center; justify-content: center; margin-left: auto; } .basket-item-buttons button { margin: 5px; /* set width, large font size, business color background */",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "width: 50px; font-size: 20px; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px; }\n\nYou may need to type the .basket-item-buttons button\n\nclass separately and, like before, use Return and Tab\n\nrepeatedly.\n\nViewing the impact of the applied CSS in a browser, you should see\n\nsomething similar to the below appearance:\n\nFigure 5.2: List of items in a shopping basket",
      "content_length": 384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Challenge How would you change the prompts to create a dark-themed version of\n\nyour basket page?\n\nQuiz How you can generate CSS using your AI assistant?\n\n1. Create a comment in a CSS file.\n\n2. Create a class and place the cursor in the class.\n\n3. Both A and B.\n\nYou can find the solution to this quiz in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary In this chapter, we covered how you can generate CSS using your AI\n\nassistant. You saw how prompting techniques introduced in the previous\n\nchapters can be applied to CSS as well.\n\nFurthermore, we showed how we could generate text in two different ways,\n\nby placing a comment at the top of the file or near the area we wanted help\n\nwith or placing the cursor inside a CSS class and letting it generate CSS\n\nbased on the name of the CSS class.\n\nIn the next chapter, we will show how you can add behavior to your app\n\nusing JavaScript. You will see how JavaScript, from a prompting aspect, is",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "similar to HTML and CSS. However, you still need to understand the\n\nsubject matter, which is the underlying problem you’re trying to solve.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "6\n\nAdd Behavior with JavaScript\n\nIntroduction It’s perfectly fine to have a web page consisting of nothing but HTML\n\nmarkup and CSS, but if you want interactivity, you need JavaScript.\n\nWith JavaScript, you can apply a little, for example, posting a form to a\n\nbackend, to a lot, like building a Single-Page Application (SPA) with a\n\nframework like Vue.js or React.js. Regardless, there’s a common\n\ndenominator, namely that you need to write code and reference that code or\n\ncode file from your HTML markup.\n\nYou will see that Copilot can help with both common tasks like adding a\n\nscript tag to your HTML markup to more advanced tasks like adding a\n\nJavaScript framework like Vue.js to your web app.\n\nIn this chapter, we will:\n\nGenerate JavaScript using prompts to add behavior to our app.\n\nAdd interactivity to our e-commerce application.\n\nIntroduce a JavaScript framework like Vue to ensure we set ourselves\n\nup with a solid foundation.",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Business problem: e- commerce We’ll keep working on our e-commerce domain in this chapter as well. In\n\nthe previous chapters, you saw how we worked with HTML to try to lay out\n\nwhat information should be on each page and identify what pages we need in the process. In this chapter, we’re adding the missing component,\n\nJavaScript, which is what makes it all work. JavaScript will play the roles\n\nof both adding interactivity and reading and writing data.\n\nProblem and data domain There are a few problems you need to address, as follows:\n\nData flow: How do we add code to our application so that we can read and write data?\n\nHandling user interaction: The user will want to interact with your\n\napplication. You will need to configure the part of the site that the user will want to use and ensure this will work. Not all user interactions\n\nlead to data being read or written, but many do, so therefore you need to figure out when that’s the case and “connect” a user interaction with\n\nyour data flow, as mentioned above.\n\nData: The data will vary depending on what parts of the app you’re\n\naddressing. If you implement a basket page, for example, you will need to deal with both product data as well as orders as the user is looking to “check out” their basket so they can purchase the products\n\nand get them delivered to a chosen address.",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Breaking the problem down into features We understand the business domain and roughly what type of problems\n\nwe’re likely to encounter, so how can we break this down into features? From the previous chapters, we have an idea of how to do this, but the main\n\ndifference is that instead of just creating a basket page, for example, that looks like it works, this should work. We can therefore break down a basket\n\npage, for example, into the following features:\n\nRead the basket information from the data source.\n\nRender the basket information.\n\nAdd the item to the basket.\n\nAdjust the selected number of items for a specific item in the basket.\n\nRemove an item from the basket.\n\nSupport checking out a basket, taking the user to an order page where they will be asked for purchase information and a delivery page.\n\nAn e-commerce site consists of many different pages. It’s therefore\n\nrecommended to do a similar feature breakdown for each page as you address a specific page.\n\nPrompting strategy The prompting strategy is a bit dependent on the chosen AI tool, how it\n\nworks, and how we prompt. GitHub Copilot is our chosen AI tool for this\n\nchapter, and we will focus mostly on its in-editor experience where you\n\ntype prompts in open files. The approaches we will show throughout this chapter are the following:",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Inline comments: This means we will write prompts directly in open\n\nfiles. You can add prompts this way at the top of the file and also add additional supporting prompts where you’re trying to have your AI\n\nassistant build specific code blocks.\n\nChat: We will not show this approach that much, but we do\n\nrecommend using the chat just to compare and contrast or if you need\n\nto look up specific information or want advice on how to implement\n\nsomething.\n\nThe general recommendation throughout this book is that you use a blended approach of both using prompts directly in open files “in-editor” and typing\n\nprompts in the chat windows. These approaches come with their pros and\n\ncons. As for prompt pattern, we’re going to be using the “Exploratory prompt pattern” as described in Chapter 2.\n\nAdding JavaScript To add JavaScript to a web page, there are two main approaches:\n\nUsing a script tag in the head element: In this version, you would\n\nadd a script tag to an existing HTML page, for example, named index.html.\n\n<!-- Alt1 --> <script> // JavaScript here </script>\n\nPointing to a stand-alone file: In this case, you would write your\n\nJavaScript in an existing file, my-javascript-file.js:",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "<!-- Alt2 --> <script src=\"my-javascript-file.js\"></script>\n\nThe role of JavaScript When JavaScript was first added to the browser, it played the role of adding\n\ninteractivity. Today, JavaScript often has an extended role as part of\n\napplication frameworks like Vue.js, React, and Angular. In such\n\nframeworks, a developer can now declaratively define templates that, with the help of JavaScript, can be turned into static HTML, with event handling\n\nadded, to respond to user interactions. Ultimately, all this is then sent to the\n\nbrowser to be rendered by it.\n\nAdding JavaScript to a page Let’s show next how we can add JavaScript to a page by providing the\n\nfollowing prompt:\n\n[Prompt]\n\nCreate web page with a script tag.\n\n[End of prompt]\n\nWe end up with a web page that also contains a script tag. Note how the\n\nprompt is added as a comment at the top of the page. That’s how GitHub\n\nCopilot knows to suggest a text completion:\n\n[Prompt response]\n\n<!-- create web page with a script tag --> <html> <head>",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "<title>My First Web Page</title> </head> <body> <script> alert(\"Hello World!\"); </script> </body>\n\n[End of response]\n\nIn this case, the code calls alert(\"Hello World\"), which will show a dialog\n\nbox with the text “Hello World.”\n\nA second example: adding a JavaScript library/framework The first example might not be all that useful, but let’s add something\n\nyou’re likely to do, like adding a framework like Vue.js. The reason for adding a framework like Vue.js is that you want to build a large application\n\nconsisting of a lot of interactivity and many different pages. To keep things\n\norganized, and make it easy for other developers to contribute, you want a framework like Vue.js.\n\n[Prompt]\n\nAdd Vue 3.\n\n[End of prompt]\n\nThis time, we place ourselves in the header tag. See the HTML comment in the HEAD tag below to indicate the prompt we’re using as input to GitHub\n\nCopilot:",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "<head> <!-- add Vue 3 --> </head>\n\nThis prompt results in the following suggestion:\n\n[Prompt response]\n\n<script src=\"https://unpkg.com/vue@next\"></script>\n\n[End of response]\n\nYou can easily build on this by adding a prompt that asks to set up an entry point (most SPA frameworks work on the idea of having an entry point, an\n\nelement where the app will be rendered):\n\n[Prompt]\n\nAdd Vue 3 entry point.\n\n[End of prompt]\n\nThis results in the following code:\n\n[Prompt response]\n\n<!-- add Vue 3 entry point --> <div id=\"app\"> <h1>{{ message }}</h1> </div>\n\n[End of response]\n\nPlacing our cursor below the script tag that imported Vue.js will lead to\n\nour AI assistant generating instructions for what code we should add to",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "further set up Vue.js. It’s not enough to just add script to Vue.js; some\n\nadditional code is needed.\n\nLuckily, with our cursor placement like below, GitHub Copilot understands\n\nfrom existing HTML comments (prompts) what code is missing and will\n\nsuggest that code at the location of our cursor (if a Copilot suggestion\n\ndoesn’t appear, try pressing the Spacebar key or the Return key):\n\n[Prompt]\n\nNo prompt given\n\n[End of prompt]\n\n[Prompt response]\n\n<script src=\"https://unpkg.com/vue@next\"></script> <!-- below is rendered by just placing cursor here --> <script> // create Vue 3 instance const app = Vue.createApp({ data() { return { message: 'Hello World!' } } }) // mount Vue 3 instance to DOM element app.mount('#app') </script>\n\n[End of response]\n\nIt’s worth calling out again that GitHub Copilot had enough\n\ncontext to generate more suggestions by us only using Tab",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "and Return key without the need to provide an extra\n\nprompt..\n\nRunning this in a browser returns “Hello World,” which is what’s expected\n\ngiven the above code. The reason this code works, in case you’re not\n\nfamiliar with Vue.js, is that it contains the bare minimum of what’s needed\n\nfor it to be referenced and configured:\n\nscript tag, referencing Vue.js\n\nCreation of an app instance that also defines a data function with a message property\n\nA call to mount(), which ensures that the identified HTML tag (with\n\nid = app) is interpolated, and the HTML tag is replaced with the value\n\nof the property message\n\nWhen building a more advanced Vue.js project, please use\n\nthe Vue CLI over the approach chosen above. For smaller\n\nproofs of concept, this approach could be OK.\n\nChallenge As a challenge, ask Copilot to add a button that increments a counter. You can use the following prompt:\n\n[Prompt]\n\nAdd a button that increments a counter.\n\n[End of prompt]",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "It is recommended that you place the cursor in the HTML markup where the DIV element with id=\"app\" is located.\n\nUse case: adding behavior For this part of our e-commerce use case, let’s work on rendering a cart\n\npage of items. In this case, we will mock calling a backend for the data and let the data be placed in a file called cart.json with the following structure:\n\n{ \"cart\": [\n\n] }\n\nThe above JSON content describes a JSON object (defined by {}), with the\n\nproperty cart, of the array type, as denoted by the square brackets, [].\n\nIn fact, let’s use our AI assistant to generate the data we need. Let’s create the cart.json file and the following structure and prompt:\n\n{ \"cart\": [ // item with id name , quantity and price, total = quantity ] }\n\nNote the prompt // item with id name, quantity and price, total =\n\nquantity * price, which results in the following JSON:",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "{ \"id\": \"1\", \"name\": \"item1\", \"quantity\": \"1\", \"price\": \"10\", \"total\": \"10\" }, { \"id\": \"2\", \"name\": \"item2\", \"quantity\": \"1\", \"price\": \"20\", \"total\": \"20\" }, { \"id\": \"3\", \"name\": \"item3\", \"quantity\": \"1\", \"price\": \"30\", \"total\": \"30\" }\n\nAdding a comment with // is OK when you try to generate\n\nmore JSON code from Copilot, but it should be removed\n\nonce you’re happy with the output.\n\nNext, we need a file, app.js, that calls this JSON file and ends up\n\nrendering. We can use the basket.html file from the previous chapter. It should look something like so:\n\n<!-- a page showing a list of items in a basket, each item shou <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\">",
      "content_length": 704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "</head> <body> <div class=\"container\"> <div id=\"basket\" class=\"basket\"> </div> </div> <!-- add app.js --> <!-- add app.js, type javascript -->\n\n<script src=\"js/app.js\"></script> </body>\n\nLet’s add app.js by adding a prompt to the end of the body tag:\n\n[Prompt]\n\nAdd app.js.\n\n[End of prompt]\n\n[Copilot Response\n\n<!-- add app.js --> <script src=\"js/app.js\"></script>\n\n[End of response]\n\nIn app.js, add the following prompt:\n\n[Prompt]\n\nRead cart.json and render the content as div elements to the element with\n\nID basket. Add the CSS class basket-item to each item.\n\n[End of prompt]\n\nIt should give you a result similar to:",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "[Prompt response]\n\nfetch('cart.json') .then(response => response.json()) .then(data => { let basket = document.getElementById('basket'); data.forEach(item => { let div = document.createElement('div'); div.classList.add('basket-item'); div.innerHTML = item.name; basket.appendChild(div); }); });\n\n[End of response]\n\nThe above code locates the HTML element whose ID is basket and then proceeds to loop through the array data. For each element in data, a new HTML element of type DIV will be created, and the following will happen\n\nto said element:\n\nThe CSS class basket-item will be added.\n\nIts innerHTML property will be assigned the value of the name property.\n\nThe specific element will be added to the parent element basket.\n\nYou need to tweak the forEach portion of the code to data.cart.forEach to\n\nadjust it based on what the JSON file looks like (if your JSON says you have a description property instead of name, that should be reflected in the JavaScript code, etc.). The resulting output doesn’t look like much, though,\n\njust three titles:\n\nitem1 item2 item3",
      "content_length": 1067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Improving the output To improve the output, there are a few things we want to do, like:\n\nAdd a CSS class to each rendered property on an item:\n\n.basket-item > div { margin: 5px; /* 25% width */ width: 25%; }\n\nYou don’t always need to type a prompt to get the\n\nabove response. Sometimes it’s enough to type .basket-item and CSS will be suggested to you that you can accept using the Tab key. Be sure that you are in either a style tag or a .css file when writing this,\n\nthough.\n\nAdd prompts in the JavaScript code to make it render each item property as separate div elements:\n\n[Prompt]\n\n//Read cart.json and render the content as div elements to the\n\nelement with id basket, add CSS class basket-item to each item\n\n[End of prompt]\n\n[Prompt response]\n\nfetch('cart.json') .then(response => response.json())",
      "content_length": 804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": ".then(data => { let basket = document.getElementById('basket');\n\n[End of response]\n\nAt this point, the above prompt has managed to call the fetch method\n\nto retrieve data from the cart.json file. It then proceeded to convert the retrieved data into JSON format and identified the HTML element\n\nwe will want to render the JSON response on:\n\n// render name, quantity, total and price for each ite data.cart.forEach(item => { let div = document.createElement('div'); div.classList.add('basket-item'); // 1) render name as separate child div element let name = document.createElement('div'); name.innerHTML = item.name; div.appendChild(name); // 2) render quantity, price and total as separate let quantity = document.createElement('div'); quantity.innerHTML = `Quantity: ${item.quantity}`; div.appendChild(quantity); let price = document.createElement('div'); price.innerHTML = `Price: ${item.price}`; div.appendChild(price); let total = document.createElement('div'); total.innerHTML = `Total: ${item.price * item.quan div.appendChild(total); basket.appendChild(div); });\n\nIn this section of the code, we’ve added context on what fields (name,\n\nquantity, etc.) we want to read from the JSON content. From the\n\ncode, you can see this prompt instruction is reflected in the generated code and each field gets its own DIV element and descriptive text:",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "// 3) calculate total price for all items and render i let totalPrice = document.createElement('div'); totalPrice.innerHTML = `Total price: ${data.cart.reduc basket.appendChild(totalPrice); });\n\nIn this prompt, we’re providing specific instructions to multiply certain\n\nfields (price x quantity) to produce a total price for the row. At the same time, we can see how a call to reduce calculates the basket’s\n\ntotal price.\n\nThe end result looks like so. It’s far from perfect but looks more like what\n\nwe can expect:\n\nFigure 6.1: Rendered cart showing name, price, and total\n\nAdding Bootstrap",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "In the previous section, we showed how we can add some basic CSS to our\n\ncart page. However, to really improve the looks of our basket page, we\n\nthink adding a CSS framework like Bootstrap can be a good idea. Bootstrap\n\nis used by many teams across the world and has many different types of\n\ncontent, like “cards” and “menus,” that will enable us to work faster in\n\nbuilding our app.\n\nTo add Bootstrap, we place ourselves with a prompt comment in the head\n\nelement like so:\n\n[Prompt]\n\nAdd Bootstrap.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- add bootstrap --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.c </head>\n\n[End of response]\n\nAt this point, we have added Bootstrap to our web app, but we need to use\n\none of its classes. Previously, we mentioned the concept of “cards,” when\n\nwe discussed Bootstrap and its types of content, a layout that renders nicely.\n\nTo produce a card, we can give it a prompt like so on the web page:\n\n[Prompt]\n\nCreate a card with the fields name, price, quantity, and total.\n\n[End of prompt]",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "We get the following result:\n\n[Prompt response]\n\n<!-- Create a card with the fields name, price, quantity and to <div class=\"card\"> <div class=\"card-body\"> <h2 class=\"card-title\">Product 1</h2> <p class=\"card-text\">Price: 100</p> <p class=\"card-text\">Quantity: 1</p> <p class=\"card-text\">Sum: 100</p> <button type=\"submit\" class=\"btn btn-primary btn-block <button type=\"submit\" class=\"btn btn-primary btn-block </div> </div>\n\n[End of response]\n\nAbove, we placed the prompt in our open text file as an HTML comment\n\nand accepted Copilot’s suggestion via the Tab key.\n\nOK, we had a good start. We have added Bootstrap to our web app. We also\n\nused Copilot to create an HTML card sample that we can use. The next step\n\nis to change our JavaScript code.\n\nTo change our JavaScript code, we will give Copilot a bit more context than\n\nusual, namely:\n\n[Prompt]\n\nSee below:\n\n// template: // <div class=\"card\"> // <div class=\"card-body\"> // <h2 class=\"card-title\">Product 1</h2> // <p class=\"card-text\">Price: 100</p>",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "// <p class=\"card-text\">Quantity: 1</p> // <p class=\"card-text\">Sum: 100</p> // <button type=\"submit\" class=\"btn btn-primary btn-blo // <button type=\"submit\" class=\"btn btn-primary btn-blo // </div> // </div> // for each item in cart.json, render name, quantity, total, us\n\n[End of prompt]\n\nThis results in the following code:\n\n[Prompt response]\n\ndata.cart.forEach(item => { // target element with id basket let basket = document.getElementById('basket'); let div = document.createElement('div'); div.classList.add('card'); basket.appendChild(div); let divBody = document.createElement('div'); divBody.classList.add('card-body'); div.appendChild(divBody);\n\nlet name = document.createElement('h2'); name.classList.add('card-title'); name.innerHTML = item.name; divBody.appendChild(name); let price = document.createElement('p'); price.classList.add('card-text'); price.innerHTML = `Price: ${item.price}`; divBody.appendChild(price); let quantity = document.createElement('p'); quantity.classList.add('card-text'); quantity.innerHTML = `Quantity: ${item.quantity}`; divBody.appendChild(quantity); let total = document.createElement('p'); total.classList.add('card-text'); total.innerHTML = `Total: ${item.price * item.quantity}`; let buttonPlus = document.createElement('button');",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "buttonPlus.classList.add('btn', 'btn-primary', 'btn-block', ' buttonPlus.innerHTML = '+'; divBody.appendChild(buttonPlus); let buttonMinus = document.createElement('button'); buttonMinus.classList.add('btn', 'btn-primary', 'btn-block', buttonMinus.innerHTML = '-'; divBody.appendChild(buttonMinus); });\n\n[End of response]\n\nAll of this felt a tiny bit convoluted, going back and forth between\n\nJavaScript and HTML. This is the reason why many developers use a\n\nJavaScript framework in which they can augment the HTML, using\n\ntemplates, and just add instructions to it, like you do when you, for example, use a framework like Vue.js.\n\nAdding Vue.js At this point, we’ve realized that our approach wasn’t so good. We let\n\nJavaScript handle the creation of HTML elements and populate them from a\n\nJSON response. What we’re looking for is a cleaner approach. We will\n\ntherefore start from the beginning. However, because we’ve been working with Copilot, it “remembers” that we’re working with basket-type data,\n\nand the file we’re working with, basket.html, helps provide additional context to Copilot. It might seem a bit like magic, what you’re about to see,\n\nbut it’s really a result of Copilot having built up an in-memory context\n\nhaving worked with you.\n\nIf you’re starting fresh with just having opened Visual\n\nStudio Code, you would probably need to use a more",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "detailed prompt.\n\nIt might sound like a big change to add a JavaScript framework at this\n\npoint, but it really isn’t. Let’s show the needed changes step by step.\n\nFirst, let’s add support for Vue.js in our basket.html file. We add the below\n\nprompt at the end of the BODY tag:\n\n[Prompt]\n\nAdd Vue 3.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- add Vue 3 --> <script src=\"https://unpkg.com/vue@next\"></script>\n\n[End of response]\n\nThe next thing we need is code that creates our Vue app.\n\n[Prompt]\n\nAdd Bootstrap code for Vue 3.\n\n[End of prompt]\n\nThe resulting code is as follows:\n\n[Prompt response]\n\n<!-- add bootstrap code for Vue 3 --> <script> let app = Vue.createApp({ data() {",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "return { basket: [ { id: 1, name: 'Product 1', price: 100, quantity: 1 }, { id: 2, name: 'Product 2', price: 200, quantity: 2 }, { id: 3, name: 'Product 3', price: 300, quantity: 3 }, { id: 4, name: 'Product 4', price: 400, quantity: 4 } ] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { let total = 0;",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "for (let i = 0; i < this.basket.length; i++) { total += this.basket[i].price * this.basket[i]. } return total; } } }); app.mount('#basket'); </script>\n\n[End of response]\n\nRemember what we said about the in-memory context of\n\nCopilot. It remembers everything we’ve fed it so far on\n\nwhat fields the basket has, and it also looks at the name of the file basket.html; that’s the result you see above.\n\nLastly, we need to state how we want to render the app:\n\n[Prompt]\n\nRender basket from Vue app.\n\n[End of prompt]\n\n[Prompt response]\n\n<!-- render basket from Vue app --> <div v-for=\"(item, index) in basket\" class=\"basket-item\"> <div class=\"basket-item-text\"> <h2>{{ item.name }}</h2> <p>Price: {{ item.price }}</p> <p>Quantity: {{ item.quantity }}</p> <p>Sum: {{ item.price * item.quantity }}</p> </div> <div class=\"basket-item-buttons\">",
      "content_length": 834,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "<button type=\"submit\" class=\"btn btn-primary btn-block <button type=\"submit\" class=\"btn btn-primary btn-block </div> </div>\n\n[End of response]\n\nThe cart will look like this:\n\nFigure 6.2: Display of cart using Vue.js",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "If you were to try out the page at this point and run it in a\n\nbrowser, you would notice how the + and - buttons\n\nincrement and decrement products.\n\nAt this point, what’s missing is ensuring our Vue app uses the Bootstrap\n\nCSS library. We can address that by modifying our prompt for generating\n\nthe HTML markup from:\n\n[Prompt]\n\nRender basket from Vue app.\n\n[End of prompt]\n\nWe can change it to a prompt with added context on Bootstrap, like so:\n\n[Prompt]\n\nRender basket from Vue app, using Bootstrap and the card class.\n\n[End of prompt]\n\nThis results in the following HTML markup:",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Figure 6.3: Cart in HTML markup",
      "content_length": 31,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Assignment Create a product list page. The page should show a list of products. Each\n\nproduct should have an Add button that will add the product to a cart. The\n\ncart should be represented as a cart icon in the top-right corner of the page\n\nand when clicked should display the number of items in the cart and the\n\ntotal value.\n\nUse what you’ve been taught to craft a prompt to create the\n\npage, add JavaScript, and more. It’s up to you if you want to\n\nadd Vue.js to solve this or if you want to use plain\n\nJavaScript.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and- ChatGPT.\n\nSummary In this chapter, we’ve shown how to add JavaScript to a web page. Adding\n\nJavaScript to a web page is a common task and can be done in two ways, either by adding a script tag to the head element or by pointing to a standalone file.\n\nWe’ve also shown how we can build on our use case from previous chapters\n\nby adding behavior to our app. We first showed how to even let JavaScript",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "generate the markup, which can become a bit unwieldy. Then, we made the\n\ncase for using a JavaScript framework like Vue.js to make it easier to\n\nmanage.\n\nYou’ve also seen how you can add a JavaScript framework like Vue.js.\n\nExactly how you add a JavaScript framework varies by framework but it’s\n\ngenerally recommended to add a prompt with wording including keywords like setup or initialize to ensure you not only add a script tag but also add\n\ncode that triggers a setup process and makes the selected framework ready to use.\n\nIn the next chapter, we will show how we can add responsiveness to our app\n\nand cater to many different devices and viewports. We can no longer\n\nassume that everyone is using a desktop computer with a large screen.\n\nMany of our users will be using a mobile device with a smaller screen.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "7\n\nSupport Multiple Viewports Using Responsive Web Layouts\n\nIntroduction Building web pages is a challenge. Not only do you need to craft these\n\npages with HTML, CSS, and JavaScript to perform the tasks you set out,\n\nbut you also need to ensure they are accessible to most users. Additionally,\n\nyou need to ensure the pages render nicely regardless of whether the device\n\nis a PC, tablet, or mobile device, which means you need to consider aspects\n\nlike screen size; the orientation of the device; that is, landscape or portrait;\n\nand pixel density.\n\nThere are many different techniques to ensure your web pages look good on\n\nmany devices, but it all starts with having a strategy, a vision for what the\n\nexperience will be for the user depending on what device is used. Once you have that vision set, you can start to implement it.\n\nSome choices you will need to make are how many columns should be\n\nlisted, if your content is presented as columns. How should other things behave, like padding and margins? Should the content be centered or left-\n\naligned? Should the content be stacked vertically or horizontally? Is there",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "content that should be hidden on mobile devices? As you can see, there are\n\nmany choices to make that affect what prompts you will need to use.\n\nUsing an AI assistant can be helpful when dealing with web layouts as there’s a lot of information you need to remember, so not only can you have\n\nthe AI assistant remember all those details for easy lookup, but you can also\n\nutilize it to suggest different designs.\n\nIn this chapter, we will:\n\nExplain technical terms like viewports and media queries.\n\nApply different techniques to optimize rendering for different\n\nviewports.\n\nLeverage the Copilot chat feature to improve our code. This is the\n\n“other” modality you can use in GitHub Copilot; it’s a chat window\n\nthat lets you type the prompt and provides a response. This experience\n\nresembles an AI tool like ChatGPT.\n\nBusiness problem: e- commerce This chapter will continue to address the e-commerce use case that’s been\n\nworked on in the last three chapters. Building the functionality is one thing, but you must assume your users will want to interact with your website from many different devices and that experience must be good or they will\n\ngo to a competitor’s website.\n\nProblem and data domain",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "There are many different devices out there: tablets, mobile phones, and\n\nsmall desktop screens to large ones. Pixel density is different. It’s not just a matter of shrinking or scaling up your site to fit this new device, but you might need to design a completely different experience that better suits the\n\nvisual style of a specific device. There are also other concerns, like how much content we want to send to a smaller device if we assume the device\n\nhas limitations, like how many concurrent downloads it can handle and what network speed it might have. It’s not unusual that a desktop machine with a wide resolution often has a great connection to the internet.\n\nConversely, a mobile device might be on a 3G network or worse and you therefore need to adjust to that by requiring a lot fewer graphical resources,\n\nsmaller JavaScript bundles, and more.\n\nBreaking the problem down into features We’ve seen in several chapters before this one how a good approach is identifying the features we need to implement. These features are less about\n\nreading and writing data and more about ensuring the design and interaction work well on prioritized devices. You might therefore have a feature\n\nbreakdown looking like the following list:\n\nShould render the basket page in a double-column design for landscape mode.\n\nPortrait mode:\n\nShould render the basket page in a single column for portrait mode.\n\nShould display menu actions at the bottom of the screen.",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "You should hide certain features, say, X, Y, Z (assuming X, Y, Z\n\nare available on a desktop with a wider screen). The point of this requirement is that you must “rethink” what a mobile experience is versus desktop, what features are central to the experience, and what features we only show if we have plenty of screen\n\nspace to show it on.\n\nShould support and render a visually appealing look for the\n\nfollowing mobile devices: iPhone, X, Y, X, and Android.\n\nShould render the page in < 1 second on a 3G connection.\n\nAs you can see, the features are more connected to the user experience than\n\nany data domain.\n\nPrompting strategy Our prompting strategy is like before, a blended approach of using the in-\n\neditor experience and adding prompts in open text files to bring up the Chat\n\nwindow in Copilot; mix these approaches to your discretion.\n\nAs for prompts, there should be enough context in these prompts to make\n\nCopilot aware that it will need to suggest a design for specific devices. Thus, it should be able to infer from context what resolutions, pixel density,\n\nand other details should influence the suggestions it’s about to generate. As\n\nfor prompting pattern used, we will use the “Exploratory prompt pattern” described in Chapter 2.\n\nViewports Gone are the days when you only had to develop a web page to look nice on\n\na PC. Today, your web page can be rendered on multiple different devices",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "and it needs to look good on all of them or your customers might go elsewhere.\n\nThe first step in understanding how to build web pages is being familiar\n\nwith some key concepts. The first concept is a viewport. A viewport is a\n\npart of the page visible to the user. The difference between a viewport and a window is that a viewport is a part of the window.\n\nDepending on what device is used, for example, a desktop screen or a\n\nmobile device, its size differs. When you write code to adjust to different\n\nsizes, to render nicely, it’s known as making the page “responsive.”\n\nMedia queries OK, so I’m dealing with different sizes of the screen depending on what\n\ntype of device I’m using, so how do I write code that ensures the visual\n\ninterface adjusts to the size of the device I’m using?\n\nThe answer is to leverage a construct called media queries. A media query is a logical block in your CSS that identifies a specific condition and applies\n\nspecific CSS if said condition is true.\n\nImagine if there were code like the following; that’s basically how it works:\n\nif(page.Width > 1024px) { // render this UI so it looks good for desktop } else { // render this UI to look good for a mobile device }\n\nBelow is an example of a media query:",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "body { background: blue; } @media (max-width: 600px) { body { background-color: lightblue; } }\n\nThe preceding code identifies a condition that says, if the viewport is\n\ncurrently at most 600 pixels wide (which is true for most mobile devices), then set the background color to light blue.\n\nThis example might feel a bit contrived; why would I want a different\n\nbackground color when I’m on a mobile device over a normal desktop? You\n\nwouldn’t, but the example above gives you an idea of how a media query identifies a viewport’s size and can apply specific CSS under certain\n\nconditions on the viewport.\n\nWhen to adjust to diﬀerent viewports and make it responsive A good reason for using responsive design is because you have a layout that\n\nlooks great on desktop but might be too wide for mobile. For example, let’s\n\nsay you have a web page with a menu to the left and a main area to the right:",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Figure 7.1: Page with a menu and a main area\n\nWere we to try to view this page on a mobile device, it would not look nice.\n\nIt would look something like this:\n\nFigure 7.2: Non-responsive page on a mobile device\n\nAbove, we see how the device tries to view the page, but the main area is\n\ncut off. At this point, you as a designer need to consider how to solve this\n\nissue. One way is to align the content vertically using either flexbox or grid",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "as techniques. The menu could go on top, for example, and the main area at\n\nthe bottom.\n\nInspecting the styles used for this page, you see the following CSS:\n\n<style> /* container, grid, left col is menu, right col is main are .container { max-width: 1200px; margin: 0 auto; display: grid; /* 2 columns */ grid-template-columns: 200px 1fr; background-color: #000; padding: 10px; /* space 5 px between columns */ grid-gap: 10px; /* height 100% */ height: 100%; }\n\nIn the preceding code, you can see how a grid, display:grid, is used to create the layout:\n\n/* menu items */ .menu { /* background black shade */ background-color: darkslategrey;\n\npadding: 0px; /* column 1 */ grid-column: 1;\n\n}\n\nYou can also see how the menu is placed to the left, grid-column: 1:",
      "content_length": 760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "/* menu item */ .menu-item { padding: 5px; background-color: darkslategrey; height: 20px; } .menu-item a { color: white; text-decoration: none; font-family: sans-serif; } /* main area, padding 5px, dark grey */ .main { padding: 5px; background-color: darkslategrey;\n\ngrid-column: 2; color: white; font-family: sans-serif; }\n\nThe main area is the second column, grid-column: 2:\n\n</style>\n\nTo improve this design so that it can be used on a device with a smaller\n\nscreen, such as a mobile device, we can use the below prompt:\n\n[Prompt]\n\nIf mobile, set menu to row 1 and main to row 2.\n\n[End of prompt]\n\nThe resulting CSS is as follows:\n\n[Prompt response]",
      "content_length": 652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "@media (max-width: 768px) { .container { grid-template-columns: 1fr; grid-template-rows: 200px 1fr; } .menu { grid-row: 1; grid-column: 1; } .main { grid-row: 2; grid-column: 1; } }\n\n[End of response]\n\nAs you can see, Copilot can take this prompt instruction and adjust the CSS\n\naccordingly. The code shifts the layout to having one column and two rows.\n\nAlso, it ensures the menu is placed at the top and the main area at the\n\nbottom:\n\nThe resulting page now renders in the following way on a mobile device:",
      "content_length": 508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Figure 7.3: Menu and main area rendered vertically in mobile viewport\n\nMost browsers have built-in functionality for letting you resize the\n\nviewport, which helps you test your altered CSS. You can also change the\n\nwindow size.\n\nUse case: make our product gallery responsive In this use case, we will take an e-commerce application that renders OK on\n\na desktop but not OK on a mobile device, and fix the issue.\n\nFirst, here’s the HTML code for it. If you want to follow along, you can\n\nsave the following code in a products.html file:",
      "content_length": 535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "<html> <head> <title>menu</title> <link rel=\"stylesheet\" href=\"css/style.css\"> <style> /* container, grid, left col is menu, right col is main .container { max-width: 1200px; margin: 0 auto; display: grid; /* 2 columns */ grid-template-columns: 200px 1fr; background-color: #000; padding: 10px; /* space 5 px between columns */ grid-gap: 10px; /* height 100% */ height: 100%; } /* menu items */ .menu { /* background black shade */ background-color: rgb(25, 41, 41);\n\n/* background-color: #ddd; */ padding: 0px; /* column 1 */ grid-column: 1;\n\n} /* menu item */ .menu-item { padding: 5px; background-color: rgb(25, 41, 41); height: 20px; } .menu-item a { color: white; text-decoration: none; font-family: sans-serif; }",
      "content_length": 718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "/* main area, padding 5px, dark grey */ .main { padding: 5px; background-color: rgb(25, 41, 41);\n\ngrid-column: 2; color: white; font-family: sans-serif; } /* if mobile, set menu to row 1 and main row 2 */ @media (max-width: 768px) { .container { grid-template-columns: 1fr; grid-template-rows: 200px 1fr; } .menu { grid-row: 1; grid-column: 1; } .main { grid-row: 2; grid-column: 1; } } /* gallery, 2 columns per row */ .gallery { display: grid; /* horizontal grid */ grid-template-columns: auto auto auto; grid-gap: 20px; } /* gallery item */ .gallery-item { flex: 1 0 24%; margin-bottom: 10px; /* padding 10px */ padding: 20px; /* margin 5px */ margin: 5px; /* black shadow */ box-shadow: 0 0 10px 0 black;",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "} /* gallery image */ .gallery-image { width: 100%; height: auto;\n\ntransition: transform 0.3s ease-in-out; } /* gallery image hover */ .gallery-image:hover { transform: scale(1.1); }\n\n</style> </head> <body> <div class=\"container\"> <!-- menu items --> <div class=\"menu\"> <div class=\"menu-item\"> <a href=\"index.php\">Home</a> </div> <div class=\"menu-item\"> <a href=\"about.php\">About</a> </div> <div class=\"menu-item\"> <a href=\"contact.php\">Contact</a> </div> <div class=\"menu-item\"> <a href=\"gallery.php\">Gallery</a> </div> <div class=\"menu-item\"> <a href=\"login.php\">Login</a> </div> </div> <!-- main area --> <div class=\"main\"> <div class=\"gallery\"> <div class=\"gallery-item\"> <img class=\"gallery-image\" src=\"https://picsum. <h4>Product 1</h4> <p>Description</p>",
      "content_length": 762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "<p>Price</p> <button>Add to cart</button> </div> <div class=\"gallery-item\"> <img class=\"gallery-image\" src=\"https://picsum. <h4>Product 2</h4> <p>Description</p> <p>Price</p> <button>Add to cart</button> </div> <!-- code shortened --> </div> </div> </body> </html>\n\nThis should render something like the following (exact images may vary as\n\nthese URLs produce random images) on desktop:",
      "content_length": 386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Figure 7.4: E-commerce product list page\n\nHowever, trying to render the same page on a mobile device makes it look\n\nlike the following:\n\nFigure 7.5: E-commerce product list, looks bad on mobile",
      "content_length": 193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "To solve this problem, we need to place ourselves in the CSS code and ask\n\nour AI assistant what we should do.\n\nPlace a prompt at the bottom of the CSS like so:\n\n[Prompt]\n\nSwitch from 3 columns to 1 on mobile device for gallery.\n\n[End of prompt]\n\nThe result should be a media query like so:\n\n[Prompt response]\n\n@media (max-width: 768px) { .gallery { grid-template-columns: auto; } }\n\n[End of response]\n\nOur new mobile rendering now looks like the image below, which is\n\nacceptable.",
      "content_length": 481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Figure 7.6: This shows an image gallery rendered on a mobile device in portrait mode\n\nAssignment",
      "content_length": 96,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "As a newly hired frontend developer, you’ve been hired to maintain a\n\nmemory game.\n\nThe game looks something like the below image:\n\nFigure 7.7: Grids in a memory game\n\nYour company wants you to do the following:\n\nEnsure it renders as a 5x5 grid on desktop. For larger viewports, it\n\ndoesn’t work well, but you should address that problem.\n\nSupport mobile devices, meaning that it should render as a 5x5 grid\n\nbut with half as big tiles.",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "While fixing it for mobile devices, ensure the score in the top-right\n\ncorner is moved to the middle and is centered.\n\nAs a developer, it’s now your job to adjust the code of this game using\n\nGitHub Copilot, using either inline editing of open text files or the Chat\n\nfunction in Copilot to ensure the code works well for different devices.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nChallenge All the code for the assignment is in one file. See if you can split it up into\n\ndifferent files. Additionally, see if you can experiment with matched cards;\n\ntry removing them or adding a class that shows they’re no longer part of the\n\ngame.\n\nSummary In this chapter, we discussed viewports as the central concept for responsive\n\nweb design. To help us tackle different viewports, we used media queries.\n\nWe also continued working on our use case, the e-commerce site, and tried\n\nto ensure a product list renders nicely on mobile devices. The first thing is\n\nto realize that you have a problem, and we managed to identify that.\n\nSecond, we came up with a strategy to solve the problem, which was to use",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "media queries. Third, we implemented the strategy. Finally, we tested it to\n\nensure it worked.\n\nIn the next chapter, we will shift from the frontend to the backend. The\n\nbackend is made up of a Web API. We will continue with our use case, the\n\ne-commerce site, and build a Web API that will serve the product list\n\nprimarily. Hopefully, though, it will become apparent how to add other\n\nresources to the Web API as well.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "8\n\nBuild a Backend with Web APIs\n\nIntroduction When we say Web API, it’s an application programming interface we\n\ndevelop that’s meant for the client to consume. Said API uses HTTP to\n\ncommunicate. A browser can use a Web API to expose data and\n\nfunctionality to other browsers and applications.\n\nWhen developing a Web API, you can use any programming language and\n\nframework you want. Regardless of the chosen tech, there are things you\n\nalways need to consider, like data storage, security, authentication,\n\nauthorization, documentation, testing, and more.\n\nIt’s with this understanding of what things we need to consider that we can\n\nuse an AI assistant to help us build a backend.\n\nIn this chapter, we will:\n\nLearn about Web APIs\n\nCreate a Web API with Python and Flask\n\nUse our AI assistant to answer questions, suggest code, and create\n\ndocumentation and tests",
      "content_length": 866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Business domain: e- commerce We will keep working on our e-commerce example in this chapter. This\n\ntime, the focus is on the API. The API lets you read and write data that’s\n\nimportant in the e-commerce domain. What’s important to keep in mind as you develop this API is that there are a couple of important aspects to it:\n\nLogical domains: It’s beneficial to divide up your app into different\n\nlogical domains. Within the context of e-commerce, that usually\n\ntranslates to products, orders, invoices, and so on.\n\nWhat part of the business should handle each logical domain?\n\nProducts: Maybe there’s a dedicated team. It’s common for the\n\nsame team to also manage all types of discounts and campaigns\n\nthat might occur.\n\nInvoices and payment: There’s usually a dedicated team that\n\ntakes care of how the user can pay for things, for example, via credit cards, invoices, and other methods.\n\nInventory: You need to have a certain amount of goods in stock. How do you know how much? You need to work with business\n\nanalysts or data folk to make correct forecasts.\n\nProblem and data domain We’ve already mentioned a few different logical domains around products,\n\norders, invoices, and so on. The problems you’ll have in this domain are generally:",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Reading and writing: What data do you wish to read or write (or\n\nmaybe both)?\n\nHow will users access your data (all of it or maybe there will be filters applied to limit the output)?\n\nAccess and roles: You can expect that different roles will need to have access to your system. An administrator role should probably have\n\naccess to most of the data, whereas a logged-in user should only be able to see the part of the data that belongs to them. This is not something we will address in this chapter, but it’s something you\n\nshould consider when you build out this API.\n\nFeature breakdown Now that we understand that there are both business problems as well as\n\ndata problems, we need to start to identify the features that we need. Once we get to this level of detail, it should be easier to come up with specific prompts.\n\nA way to do this feature breakdown is as follows – for example, for\n\nproducts:\n\nRead all products.\n\nRead products given a filter: Usually, you won’t want to read all\n\nproducts but maybe all products of a certain category, or maybe even limit it to a specific value such as 10 products or 20 products.\n\nSearch for products: You should support the user looking for specific\n\nproducts, usually via a category, name, or perhaps part of a certain campaign.\n\nRetrieve detailed information on a specific product.",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "I’m sure there are more features for products, but now you have an idea of\n\nwhat granular detail you should have before you continue building the API.\n\nPrompt strategy In this chapter, you will see how we use both Copilot Chat and the in-editor\n\nmode. We will start with the Chat mode as it’s quite useful for situations where you want to generate starter code. It’s also quite efficient in that it\n\nlets you select certain lines of code and lets you update only those based on\n\na prompt. Examples of the latter could be when you want to improve such\n\ncode. You will see this use case later in the chapter when we improve a route to read from a database instead of reading static data from a list. There\n\nwill also be cases in this chapter where we use the in-editor mode. This is\n\nthe recommended approach when you’re actively typing the code and want to make smaller tweaks. In this chapter, we will use the “Exploratory\n\nprompt pattern” as described in Chapter 2.\n\nWeb APIs Using a Web API is a great way to ensure our front-end application has\n\naccess to the data and functionality it needs to read and write data.\n\nThe expectations of a Web API are:\n\nIt is accessible over the web.\n\nIt leverages HTTP protocol and HTTP verbs such as GET, POST, PUT,\n\nDELETE, and others to communicate intentions.",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "What language and framework should you pick? In this chapter, we already decided we will use Python and Flask. But why? What criteria do we use to pick a language and framework?\n\nYou can use any language and framework you want, but here are some\n\ncriteria to consider:\n\nWhat languages and frameworks do you know?\n\nAre they easy to learn?\n\nDo they have a large community?\n\nAre they free and open source?\n\nHow often are they updated?\n\nDo they have good documentation?\n\nDo they have good tooling?\n\nThese are just some of the criteria to consider.\n\nThe reason for picking Python and Flask is that they check many of the\n\nabove boxes (so does the Express framework for Node.js, but the objective\n\nhere is to just show how you can build a Web API using an AI assistant, so feel free to use whatever web framework you prefer). Also, the point of this\n\nbook is to show how an AI assistant can help us build a backend; with what\n\nprompts and how and the framework and language is not the focus.\n\nPlanning the Web API When you plan your Web API, you should consider the following:\n\nWhat data do you want to expose? For example, products and orders.",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "What functionality do you want to expose? For example, reading order data.\n\nHow will you structure your Web API?\n\nSecurity and authentication: You need to determine not only what areas\n\nof your app should require the user to log in but also what parts should be restricted to specific user types.\n\nStorage and database: Common choices are, for example, MySQL and\n\nPostgres.\n\nUse the above points as a checklist when you plan your Web API.\n\nCreating a Web API with Python and Flask A key insight to working with an AI assistant is that we can use it to\n\ngenerate code, but we need to have a good understanding of the problem\n\ndomain and the solution domain. This means that we should know how to\n\ncreate a Web API with Python and Flask before we ask our AI assistant to help us. Can we create it without an AI assistant? Yes, but we risk getting\n\nstuck and not knowing what to do next.\n\nSo, how much knowledge are we talking about? If you know Python in\n\ngeneral and how to build a Web API in any language, you are good to go.\n\nLet’s begin.\n\nStep 1: Create a new project First, we need to create a new project. If you know Python, you know using\n\na virtual environment is a good idea as it isolates the project from other",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Python projects on your computer. Using a virtual environment is not required but is recommended as different versions of Python and packages\n\ncan cause issues.\n\nOkay, so we know we need a virtual environment. How do we create one?\n\nLet’s rely on our AI assistant to help us remember the syntax. For the prompt below, you can either open a text file or use the chat functionality in\n\nGitHub Copilot.\n\n[Prompt]\n\nHow do I create a virtual environment for Python?\n\n[End of prompt]\n\nThe in-editor experience, typing a prompt in an open text file, prefers you to use q: at the start of a sentence. If you\n\nuse the Chat functionality, there’s no need to add q:.\n\n[Prompt response]\n\nThe answer is on the same row as a:.\n\npython -m venv venv\n\n[End of response]\n\nHere’s how the output can look in an app.py file containing both the prompt and the response:\n\n# q: How do I create a virtual environment for Python? a: pytho",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "To use the command the AI assistant responded with, we would need to\n\nopen a terminal, paste the command, and run it.\n\nAt this point, you need enough conceptual knowledge to know that you\n\nneed to activate the virtual environment to use it. But again, let’s ask the AI\n\nassistant to help us.\n\n[Prompt]\n\nHow do I activate a virtual environment for Python in Windows?\n\n[End of prompt]\n\n[Prompt response]\n\na: Venv\\Scripts\\activate\n\n[End of response]\n\nIf you are on a Mac or Linux, change the above prompt to\n\nreflect your operating system.\n\nStep 2: Install Flask The conceptual knowledge you need to have is that you can install packages\n\n– in this case, Flask – using pip. Let’s ask the AI assistant to help us. Keep\n\nworking with your app.py file and add your prompt as another comment.\n\n[Prompt]\n\nHow do I install Flask with pip?\n\n[End of prompt]",
      "content_length": 846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "[Prompt response]\n\npip install Flask\n\n[End of response]\n\nYour app.py should now look like so:\n\n# q: How do I create a virtual environment for Python? a: pytho # q: How do I install Flask with pip? a: pip install Flask\n\nAs you’re using your AI tool to get quick answers on how to\n\ndo something, it’s up to you if you want to clear the text file\n\nfrom questions and responses.\n\nGreat, we now know how to install Flask.\n\nStep 3: Create an entry point We need to create an entry point for our Web API. This is the file that will\n\nbe executed when we run our Web API. For Python, you usually name this file app.py.\n\nStep 4: Create a Flask app At this point, we have our project, we’ve installed Flask, and we have an\n\nentry point file, but the application is not a Flask application yet, so let’s\n\ncreate one.\n\nLet’s ask the AI assistant to help us.",
      "content_length": 844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "[Prompt]\n\nHow do I create a Flask app?\n\n[End of prompt]\n\nAs usual, keep typing your questions as comments prepended with a q: in\n\nyour open text file. Here’s the response from your latest prompt:\n\n[Prompt response]\n\nfrom flask import Flask\n\n[End of response]\n\nWe’re not getting much back from the AI assistant. But we know we need\n\nto import Flask. Let’s improve the prompt.\n\n[Prompt]\n\nHow do I create a Flask app with a default route and a “hello” route?\n\n[End of prompt]\n\nNow you’ll see an improved prompt response. The prompt contains more\n\nexplicit instructions to create routes and logical areas in the app:\n\n[Prompt response]\n\nfrom flask import Flask # create app app = Flask(__name__) # default route @app.route('/') def index(): return 'Hello World!' # hello route @app.route('/hello')",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "def hello(): return 'Hello World!' # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nNow we have some starting code for our Flask app.\n\nNext, let’s focus on the use case for our Web API – e-commerce.\n\nUse case: a Web API for an e- commerce site So far, in this book, we’ve taken you through the process of building a\n\nfront-end application for an e-commerce site. We used an AI assistant to\n\nhelp us with the front-end application. Now, we will build a Web API for the e-commerce site.\n\nIn the previous section, we used an AI assistant to help us create a Flask\n\napp. Now, we will use the AI assistant to help us create a Web API for an e-\n\ncommerce site.\n\nStep 1: Create a Web API for an e-commerce site Let’s plan this task first. Based on previous chapters, the e-commerce site\n\nhas the following features:\n\nProduct catalog: Containing the products we aim to sell.",
      "content_length": 920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Shopping cart: With a shopping cart, we can add products to the cart,\n\nremove products from the cart, and update the quantity of products in\n\nthe cart. Once we’re happy with the products in the cart, we can check\n\nout and have the products paid for and shipped.\n\nA checkout process: In the checkout process, we need to collect\n\npayment information and shipping information.\n\nWith the above information in mind, let’s craft a prompt for our AI\n\nassistant.\n\n[Prompt]\n\nShow me code to create a Flask Web API that includes routes for a product\n\ncatalog, shopping cart, and checkout process.\n\n[End of prompt]\n\n[Prompt response]\n\nfrom flask import Flask # create app app = Flask(__name__) # default route @app.route('/') def index(): return 'Hello World!' # product catalog @app.route('/products') def products(): return 'Products' # shopping cart @app.route('/cart') def cart(): return 'Cart' # checkout @app.route('/checkout') def checkout(): return 'Checkout'",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "# start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nOkay, so we have some starting code for our Web API, but we need to\n\nimprove it in the following ways:\n\nWe need to return JSON instead of text.\n\nWe want to add code to read and write to a database.\n\nWe want to add tests.\n\nWe want to document the Web API.\n\nStep 2: Return JSON instead of text JSON is a common format for data. It’s a format that is easy to read and\n\nwrite for both humans and machines. To use JSON in Python, we need to import the json package. The method we will use is jsonify(), which is a method that will convert a Python dictionary to JSON.\n\nLet’s see how we can use jsonify() in our Web API. For this operation, we\n\ncan keep using the AI assistant as prompt comments in the code, or we can use GitHub Copilot’s Chat feature (this is a separate extension of GitHub\n\nCopilot).\n\nLet’s use the latter. To use GitHub Copilot’s Chat feature (refer to this link\n\nfor updated install instructions:\n\nhttps://docs.github.com/en/copilot/github-copilot- chat/using-github-copilot-chat-in-your-ide), we need to",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "select the code we want to improve and click the chat icon on the left side\n\nof Visual Studio Code (if you are using the default orientation, where the\n\nicons are placed vertically – it depends what other extensions you have\n\ninstalled). Here’s the icon to look for. Ensure the GitHub Copilot Chat\n\nextension has been correctly installed:\n\nFigure 8.1: Chat icon for GitHub Copilot\n\nOnce we click the Chat icon, and the code selected, we can start chatting\n\nwith GitHub Copilot. Let’s ask GitHub Copilot to help us return JSON\n\ninstead of text.\n\nWe add the following prompt to the chat window:\n\n[Prompt]\n\nAdd code to respond with JSON for all routes.\n\n[End of prompt]\n\nFigure 8.2: Prompt in Copilot chat window",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "The chat functionality generates an answer consisting of code including the\n\njsonify() method.",
      "content_length": 94,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Figure 8.3: Copilot Chat response",
      "content_length": 33,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Here’s the code generated by GitHub Copilot:\n\nfrom flask import Flask, jsonify # create app app = Flask(__name__) # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): products = [ {'id': 1, 'name': 'Product 1', 'price': 10.99}, {'id': 2, 'name': 'Product 2', 'price': 5.99}, {'id': 3, 'name': 'Product 3', 'price': 7.99} ] return jsonify(products) # shopping cart @app.route('/cart') def cart(): return jsonify({'message': 'Cart'}) # checkout @app.route('/checkout') def checkout(): return jsonify({'message': 'Checkout'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\nStep 3: Add code to read and write to a database Okay, so we have a Web API that returns JSON. Now, we need to add code\n\nto read and write to a database. Let’s use the chat feature again to help us\n\nwith this task.",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "[Prompt]\n\nAdd code to read and write to a database and apply it to all our routes.\n\n[End of prompt]\n\nHere’s the code we end up with:\n\n[Prompt response]\n\nimport sqlite3 from flask import Flask, jsonify # create app app = Flask(__name__) # connect to database conn = sqlite3.connect('example.db') # create table conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') # insert data conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # shopping cart @app.route('/cart')",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "def cart(): return jsonify({'message': 'Cart'}) # checkout @app.route('/checkout') def checkout(): return jsonify({'message': 'Checkout'}) # close connection conn.close() # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nAs you can see, we get quite a lot of code:\n\nCode to connect to a database – in this case, we get a connection to the\n\ncommon SQLite database.\n\nCode that creates a table and even reads from it in the /products route,\n\nbut we don’t get code to write to the database.\n\nWe don’t get code to read and write to the other routes either.\n\nBased on the code above, we realize it’s a good start, but there are things\n\nwe’d like to improve, so we will change the prompt to address the\n\nshortcomings. We need the prompt to ensure all routes read their data from\n\na database, not just one.\n\nSo, we can either adjust the code ourselves, knowing that most of the code\n\nis in place, or try to adjust the prompt. Let’s try the latter.\n\n[Prompt]\n\nAdd code to read and write to a database and apply it to all our routes.\n\n[End of prompt]",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "[Prompt response]\n\nimport sqlite3 from flask import Flask, jsonify # create app app = Flask(__name__) # connect to database conn = sqlite3.connect('example.db') # create table conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') # insert data conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() # default route @app.route('/') def index(): return jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # shopping cart @app.route('/cart') def cart(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # checkout",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "@app.route('/checkout') def checkout(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return jsonify(products) # close connection conn.close() # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nThe improved prompt made the following changes:\n\nAdded database code to the /checkout and /cart routes.\n\nAll three routes read from the products table, which is incorrect. That’s\n\nsomething we need to address in an upcoming prompt. The great news\n\nis that we can just tell Copilot Chat what the problem is with a prompt and expect the AI assistant to change the code accordingly.\n\nThe above code is better than our very first prompt but has issues still.\n\nYou could try to adjust the prompt in Copilot Chat, but let’s fall back on our\n\nother approach – namely, prompts in the code. To add these prompts, put\n\nyour cursor in the code and add the prompt as a comment. Here are some\n\nprompts we added:\n\n# create tables: We’re repeating the comment we have already used,\n\nbut because Copilot is now aware of all the routes, it does a better job\n\nthis time, suggesting three database tables should be created rather than just the products one.",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "# add product to cart, productId and quantity in body, create cart if\n\nneeded, else update: This prompt is detailed as it asks what\n\ninformation should be added to each cart entry.\n\n# product catalog and # shopping cart are also repeated prompts (we\n\nremove the old prompt, and retype it to toggle a new suggestion). This\n\ntime, we get Copilot to suggest the correct database tables to read\n\nfrom.\n\nNote below how we reshape the code with various prompts in the code as\n\ncomments:\n\nimport sqlite3 import json import flask # create app app = flask.Flask(__name__) # connect to database db = sqlite3.connect('example.db') # create tables db.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') db.execute('''CREATE TABLE IF NOT EXISTS cart (id INTEGER PRIMARY KEY AUTOINCREMENT, product_id INTEGER NOT NULL, quantity INTEGER NOT NULL);''') db.execute('''CREATE TABLE IF NOT EXISTS checkout (id INTEGER PRIMARY KEY AUTOINCREMENT, cart_id INTEGER NOT NULL, total REAL NOT NULL);''') db.commit() db.close() # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "@app.route('/products') def products(): db = sqlite3.connect('example.db') cursor = db.execute(\"SELECT id, name, price FROM products\") products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) db.close() return flask.jsonify(products) # shopping cart @app.route('/cart') def cart(): db = sqlite3.connect('example.db') cursor = db.execute(\"SELECT id, product_id, quantity FROM c cart = [] for row in cursor: item = {'id': row[0], 'product_id': row[1], 'quantity': cart.append(item) db.close() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart/', methods=['POST']) def add_to_cart(): db = sqlite3.connect('example.db') # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] # check if cart exists cursor = db.execute(\"SELECT id FROM cart\") cart_id = None for row in cursor: cart_id = row[0] # if cart exists, update if cart_id: db.execute(\"UPDATE cart SET product_id = ?, quantity = # else create else: db.execute(\"INSERT INTO cart (product_id, quantity) VAL # close db.commit() db.close()",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "return flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) def checkout(): # insert cart into checkout conn = sqlite3.connect('example.db') # get cart id from body cart_id = flask.request.json['cartId'] # write to checkout conn.execute(\"INSERT INTO checkout (cart_id, total) VALUES # close conn.commit() conn.close()\n\nIs this code good enough or do we need to adjust it further?\n\nThere’s definitely room for improvement and here’s what you should be\n\nlooking for:\n\nThe code is not DRY (Don’t Repeat Yourself); we have a lot of\n\nduplicate code. We can solve this by creating a function that takes a\n\nquery and returns the result.\n\nAuthentication and authorization are missing. We should add this to\n\nthe code.\n\nDocumentation is missing.\n\nThe code is not secure. We should add some security measures,\n\nespecially when it comes to the database. As developers, having some\n\nknowledge of how to secure code, we can use prepared statements to\n\nprevent SQL injection attacks and also validate the data that we receive from the client.\n\nStep 4: Improve the code",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "The best way to improve the code is to use the code that we have as a\n\nstarting point and first just try to run it. We can then see what errors we get\n\nand what we need to fix.\n\nAfter that, we work on architecture and design and separate the code into\n\ndifferent files.\n\nLastly, we add authentication and authorization and security measures.\n\nRun the code Let’s run the code. We need to set the Flask variable FLASK_APP to app.py\n\nand then run the app.\n\nIf you are using Windows, you need to use set instead of export to set the variable.\n\n# flask variable windows set FLASK_APP=app.py flask run\n\nThe app is working! We can now go to http://localhost:5000 and see the message Hello World!.\n\nGoing to /products, /cart, and /checkout also works, but we don’t have\n\nany data in the database yet. Let’s add some data to the database. By\n\ninserting the above code at the top of the file and running the app again, we\n\ncan do this:\n\ndb.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\", db.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\",",
      "content_length": 1052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Now, if you rerun the app and go to /products, you should see the products\n\nthat we added as the following JSON output:\n\n[ { \"id\": 1, \"name\": \"Apple\", \"price\": 1.99 }, { \"id\": 2, \"name\": \"Banana\", \"price\": 2.99 } ]\n\nRefactor the code Great – lots of progress so far! Now let’s refactor the code and make it more\n\nDRY (Don’t Repeat Yourself, a term for ensuring we don’t duplicate code\n\ntoo much). We can use Copilot to help us with this.\n\nProvide it with the following prompt:\n\n[Prompt]\n\nImprove the code by separating the database parts into a separate module\n\ndb.py and expose the functions as follows: get_products, get_cart,\n\nadd_to_cart, checkout.\n\n[End of prompt]\n\nWe get two responses, one for the db.py file and one for the app.py file.\n\nLet’s start with the db.py file.\n\n[Prompt response]",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "import sqlite3 # connect to database conn = sqlite3.connect('example.db') # create tables conn.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL);''') conn.execute('''CREATE TABLE IF NOT EXISTS cart (id INTEGER PRIMARY KEY AUTOINCREMENT, product_id INTEGER NOT NULL, quantity INTEGER NOT NULL);''') conn.execute('''CREATE TABLE IF NOT EXISTS checkout (id INTEGER PRIMARY KEY AUTOINCREMENT, cart_id INTEGER NOT NULL, total REAL NOT NULL);''') conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.execute(\"INSERT INTO products (name, price) VALUES (?, ?)\" conn.commit() def get_products(): cursor = conn.execute(\"SELECT id, name, price FROM products products = [] for row in cursor: product = {'id': row[0], 'name': row[1], 'price': row[2 products.append(product) return products def get_cart(): cursor = conn.execute(\"SELECT id, product_id, quantity FROM cart = [] for row in cursor: item = {'id': row[0], 'product_id': row[1], 'quantity': cart.append(item) return cart def add_to_cart(product_id, quantity): # check if cart exists cursor = conn.execute(\"SELECT id FROM cart\") cart_id = None for row in cursor: cart_id = row[0] # if cart exists, update if cart_id: conn.execute(\"UPDATE cart SET product_id = ?, quantity",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "# else create else: conn.execute(\"INSERT INTO cart (product_id, quantity) V conn.commit() def checkout(cart_id): # insert cart into checkout conn.execute(\"INSERT INTO checkout (cart_id, total) VALUES conn.commit()\n\n[End of response]\n\nWe need to improve the above code a bit, as it’s using an\n\nopen connection to the database. We should close the\n\nconnection after we are done with it. Ensure you add code\n\nto close the connection after the commit. Also, ensure per\n\nroute that you open and close the connection.\n\nThis looks good. Let’s add it to our project. We can create a new file called\n\ndb.py and copy the code into it.\n\nNext, let’s see what it suggested for the app.py file:\n\nimport flask import db # create app app = flask.Flask(__name__) # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products') def products(): products = db.get_products() return flask.jsonify(products)",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "# shopping cart @app.route('/cart') def cart(): cart = db.get_cart() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart', methods=['POST']) def add_to_cart(): # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] db.add_to_cart(product_id, quantity) return flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) def checkout(): # get cart id from body cart_id = flask.request.json['cartId'] db.checkout(cart_id) return flask.jsonify({'message': 'Checkout successful'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\nAs you can see, it has separated the database code into the db.py file and\n\nhas also added the imports for the db.py file. Let’s copy this code into our app.py file and replace its content.\n\nDoes it work? Let’s run the app and see. It runs without any errors, and we\n\nget JSON output when we go to /products, /cart, and /checkout. If you want, you can also make a POST request to /cart to write cart content –\n\ngreat!\n\nStep 5: Document the API",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "We can use Copilot Chat to help us document the API. Let’s provide it with\n\nthe following prompt:\n\n[Prompt]\n\nDocument the API using Swagger. The API should have the following\n\nendpoints: GET /products, GET /cart, POST /cart/add, POST /checkout.\n\n[End of prompt]\n\nSwagger or OpenAPI standard is a way to document your\n\nAPI routes. These route specifications can then be\n\nvisualized, and you get a quick overview of what your API\n\ncan do. Additionally, there’s something called Swagger UI,\n\na web page that not only lets you see what routes you have\n\nin your API but you can make requests and more, to ensure\n\nthe API is working as it should.\n\nHere’s the output, which involves both code and documentation:\n\n[Prompt response]\n\n# app.py import flask import db from flasgger import Swagger, swag_from # create app app = flask.Flask(__name__) swagger = Swagger(app) # default route @app.route('/') def index(): return flask.jsonify({'message': 'Hello World!'}) # product catalog @app.route('/products', methods=['GET'])",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "@swag_from('swagger/products.yml') def products(): products = db.get_products() return flask.jsonify(products) # shopping cart @app.route('/cart', methods=['GET']) @swag_from('swagger/cart.yml') def cart(): cart = db.get_cart() return flask.jsonify(cart) # add product to cart, productId and quantity in body, create c @app.route('/cart/add', methods=['POST']) @swag_from('swagger/add_to_cart.yml') def add_to_cart(): # get product id and quantity from body product_id = flask.request.json['productId'] quantity = flask.request.json['quantity'] db.add_to_cart(product_id, quantity) return flask.jsonify({'message': 'Added to cart'}) # checkout POST, cartId in body @app.route('/checkout', methods=['POST']) @swag_from('swagger/checkout.yml') def checkout(): # get cart id from body cart_id = flask.request.json['cartId'] db.checkout(cart_id) return flask.jsonify({'message': 'Checkout successful'}) # start app on port 5000 if __name__ == '__main__': app.run(debug=True, port=5000)\n\n[End of response]\n\nAnd it tells us to create the following files and folders:\n\nfolder: swagger products.yml file: cart.yml file: add_to_cart.yml",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "file: checkout.yml file: swagger.yml\n\nLet’s create these files manually and copy the content into them.\n\nproducts.yml --- tags: - products responses: 200: description: A list of products schema: type: array items: type: object properties: id: type: integer name: type: string price: type: number cart.yml --- tags: - cart responses: 200: description: A list of items in the cart schema: type: array items: type: object properties: id: type: integer product_id: type: integer quantity:",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "type: integer add_to_cart.yml --- tags: - cart parameters: - in: body name: body required: true schema: type: object properties: productId: type: integer quantity: type: integer responses: 200: description: Added to cart checkout.yml --- tags: - checkout parameters: - in: body name: body required: true schema: type: object properties: cartId: type: integer responses: 200: description: Checkout successful\n\nWhat’s missing is installing the flassger Swagger library:\n\npip install flasgger",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Navigate to http://localhost:5000/apidocs/ and you should see Swagger\n\nUI.\n\nFigure 8.4: API doc generated by Swagger\n\nYou should verify that the API works as expected by interacting with\n\ngenerated docs and ensure the routes generate the expected output.\n\nIt’s definitely possible to keep improving at this point but take a moment to\n\nrealize how much we created with only prompts and a few lines of code. We\n\nhave a working API with a database and documentation. We can now focus\n\non improving the code and adding more features.",
      "content_length": 529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Assignment Here’s the suggested assignment for this chapter: A good assignment would\n\nbe to add more features to the API, such as:\n\nAdd a new endpoint to get a single product.\n\nAdd a new endpoint to remove a product from the cart.\n\nAdd a new endpoint to update the quantity of a product in the cart.\n\nYou can solve this by just adding the above to Copilot Chat\n\nas prompts and see what it generates. Expect both the code\n\nand documentation to change.\n\nSolution You can find the solution to this assignment in the GitHub repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT/tree/main/08\n\nChallenge Improve this API by adding more features. You can use Copilot Chat to help\n\nyou with this.\n\nSummary In this chapter, we discussed how to plan out our API. Then we looked at how we can choose Python and Flask for the job but stressed the",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "importance of having contextual knowledge on how to actually build a Web\n\nAPI. In general, you should always know how to do something before you\n\nask an AI assistant to help you with it, at least at a high level.\n\nThen we ended up crafting prompts for the AI assistant to help us with the\n\nWeb API. We ended up working with our e-commerce site and created a\n\nWeb API to serve it.\n\nAfter that, we discussed how to improve the code and add more features to\n\nthe API.\n\nIn the next chapter, we will discuss how to improve our app by adding\n\nartificial intelligence to it.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "9\n\nAugment Web Apps with AI Services\n\nIntroduction There are several ways that a web app can be augmented with AI services:\n\nyou could leverage an existing Web API exposing a model, or build it\n\nyourself and have it call a model.\n\nThe reason you would want to add AI to your app in the first place is to\n\nmake it smarter. Not smarter for its own sake, but to make it more useful to\n\nthe user. For example, if you have a web app that allows users to search for\n\nproducts, you could add a feature that suggests products based on the user’s\n\nprevious purchases. In fact, why limit yourself to previous purchases? Why\n\nnot suggest products based on the user’s previous searches? Or, what if the\n\nuser could take a picture of a product and the app would suggest similar\n\nproducts?\n\nAs you can see, there are a lot of possibilities for augmenting your web app\n\nwith AI that would improve the user experience.\n\nIn this chapter, we will:\n\nDiscuss different model formats like Pickle and ONNX",
      "content_length": 983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Learn how to use both Pickle and ONNX to persist your model as a\n\nfile using Python\n\nConsume a model stored in ONNX format and expose it via a REST\n\nAPI using JavaScript\n\nBusiness domain, e- commerce We keep working on our e-commerce domain, but our business focus is on ratings. A good or bad rating can influence how many units are sold of a\n\nspecific product. The logical domain consists of the following:\n\nProducts: the products to be rated\n\nRatings: the actual ratings and meta information like comments, dates\n\nand more\n\nProblem and data domain The problem to figure out is how we use this rating data and learn from it.\n\nInsights: We could, for example, get the insights that we should start/stop selling a certain product. There might be other insights as certain products sell well in certain parts of the world.\n\nTechnical problem: The technical aspect of this is figuring out how to ingest the data, train a model from it, and then figure out how a web\n\napplication can leverage said model.\n\nFeature breakdown",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Looking at this from a feature standpoint, we need to see this as consisting\n\nof three major parts.\n\nData ingestion and training: this needs a separate interface, maybe\n\nit’s done without a user interface and it’s just static data being fed into code capable of training a model from it. With that understanding, we\n\ncan outline the steps like so:\n\nLoad data\n\nClean data\n\nCreate features\n\nTrain model\n\nEvaluate model\n\nRun predictions\n\nConsuming the model: Once the model is trained, it needs to be exposed, preferably through a web endpoint. To get there, we think we\n\nneed these set of steps:\n\nConvert the model to suitable format if needed\n\nBuild a Web API\n\nExpose model through Web API\n\nDeploy model, there’s a step here where we need to bring the API online\n\nPrediction: For the prediction part, this is a functionality that’s meant\n\nfor “back office” and not customer facing.\n\nBuild user interface to run predictions\n\nBuild underlying code that talks to the Web API to make predictions possible\n\nPrompt strategy",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "You could go with either prompt approach here, either get suggestions from\n\na prompt comment or use the chat interface.\n\nFor prompt pattern, we’ll use the “Exploratory pattern” as described in Chapter 2.\n\nCreating a model Imagine we are dealing with the following data in the sales_rating.csv\n\nfile that is the result of merging two datasets, one containing sales data and\n\nthe other containing rating data. The data looks like this:\n\nproduct_id,avg_rating,sold 1,2.5,100 2,3.7,200 3,4.2,300 4,1.3,50 5,4.9,800 6,3.2,150 7,2.1,80 8,4.8,500 9,3.9,400 10,2.4,200 11,4.1,300 12,3.2,100 13,2.9,150 14,4.5,500 15,3.8,400 16,2.7,200 17,4.3,300 18,3.4,100 19,2.3,150 20,4.7,500\n\nThe preceding data shows a dataset with product_id, avg_rating, and sold\n\ncolumns. Our theory is that the average rating of a product is correlated",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "with the number of sales. It seems like a fair assumption that a product with a high rating will sell more than a product with a low rating. By creating a\n\nmodel, we can come closer to determining whether it’s likely that our\n\nassumption is true or not.\n\nComing up with a plan We can ask Copilot about the steps we need to take to create a model. We\n\ntype these prompts as comments:\n\nIf you want to ask Copilot to come up with text answers\n\nabout something else other than code, you can type the text as a comment in a code file or inside of a Markdown file. It\n\nalso helps to place yourself on the next row after the\n\ncomment and start the row with “a:” for answer. You can\n\nalso ask questions using Copilot Chat:\n\n[Prompt]\n\nTell me the answer of life, the universe and everything.\n\n[End of prompt]\n\n[Prompt response]\n\na: 42\n\n[End of response]\n\nThe above reference is from the book Hitchhikers guide to the Galaxy by\n\nDouglas Adams.",
      "content_length": 933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Next, let’s ask Copilot to help us with the steps to create a model.\n\n[Prompt]\n\nWhat are the steps to create a model in Python given a CSV file?\n\n[End of prompt]\n\nOutput:\n\nimport pandas and scikit-learn\n\nread the CSV file\n\ncreate a test and training dataset\n\ncreate a model\n\nhow good is the model?\n\npredict\n\nsave the model to pkl\n\nOK, we have a plan, so let’s execute it and use Copilot to help us with the code.\n\nImporting libraries First, let’s start by creating a new notebook in VS Code. Let’s call it\n\nsales.ipynb.\n\n.ipynb is the file format for Jupyter Notebook that you as a\n\ndata science practitioner are most likely familiar with. The\n\nidea is that you work on a file with cells, where some cells are text cells and others are code cells you can run.",
      "content_length": 759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "To use certain libraries, we need to install them. We can use Copilot to help us with that. We can leverage the fact that we are data science practitioners\n\nand therefore have an understanding of what libraries we need. Therefore,\n\nwe will suggest the pandas and scikit-learn libraries below to help us with\n\nour task.\n\n[Prompt]\n\nInstall pandas and scikit-learn in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n! pip install pandas scikit-learn\n\n[End of response]\n\nNext, let’s import the libraries we need.\n\n[Prompt]\n\nImport pandas and scikit-learn in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Import pandas and scikit-learn in a notebook. import pandas as pd import sklearn.linear_model\n\n[End of response]\n\nReading the CSV ﬁle",
      "content_length": 739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Now that we have the libraries we need, let’s read the CSV file.\n\n[Prompt]\n\nRead a CSV file in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Read a CSV file in a notebook. df = pd.read_csv(\"sales_rating.csv\")\n\n[End of response]\n\nThe CSV file is now loaded into a pandas DataFrame, a structure holding\n\ncolumns and rows. Let’s take a look at the first few rows.\n\ndf.head()\n\nThis will produce the following output:\n\nproduct_id avg_rating sold 0 1 2.5 100 1 2 3.7 200 2 3 4.2 300 3 4 1.3 50 4 5 4.9 800\n\nGreat, we have the data loaded into a DataFrame. Next, we need to split the\n\ndata into test and training datasets.",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Creating test and training datasets In machine learning, we typically split the data into test and training\n\ndatasets. The training dataset is used to train the model and the test dataset\n\nis used to test the model. We can use Copilot to help us with this.\n\n[Prompt]\n\nCreate test and training dataset in a notebook, suggested test size of 0.2.\n\n[End of prompt]\n\n[Prompt response]\n\n# Create a test and training dataset in a notebook, suggested t test_size = 0.2 test = df.sample(frac=test_size) training = df.drop(test.index)\n\n[End of response]\n\n0.2 is a good test size. It means that 20% of the data will be used for testing\n\nand 80% will be used for training.\n\nYou may need to start typing “test_size” and “training” on\n\nthe respective rows, then press TAB to accept the suggestion.\n\nNow we have things set up, let’s create a model next.\n\nCreating a model",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Creating a model involves selecting the algorithm we want to use and then\n\ntraining the model. Let’s suggest a prompt:\n\n[Prompt]\n\nCreate a model in a notebook using the LinearRegression algorithm.\n\n[End of prompt]\n\n[Prompt response]\n\n# Create a model in a notebook using the LinearRegression algor model = sklearn.linear_model.LinearRegression() model.fit(training[[\"avg_rating\"]], training[[\"sold\"]])\n\n[End of response]\n\nIn the preceding code, we created a model using the LinearRegression\n\nalgorithm and then trained the model using the training dataset with the fit method. Also, note how we are using the avg_rating column as the input\n\nand the sold column as the output. This is because we want to predict the\n\nnumber of sales based on the average rating.\n\nHow good is the model? At this point, we want to find out how good the model is. To find this out, let’s ask Copilot to help us.\n\nYou can either use a comment style and get suggestions\n\ninline, or use Copilot Chat. In both cases, Copilot will\n\nunderstand from the context.",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "[Prompt]\n\nHow good is the model in a notebook? Show the code.\n\n[End of prompt]\n\n[Prompt response]\n\n# How good is the model in a notebook? Show the code. score = model.score(test[[\"avg_rating\"]], test[[\"sold\"]])\n\n[End of response]\n\nIt seems that calling score on the model and passing in the test dataset is how we find out how good a model is. We can even ask Copilot about this:\n\n[Prompt]\n\nWhat does score do?\n\n[End of prompt]\n\n[Prompt response]\n\nscore returns the coefficient of determination R^2 of the predi\n\n[End of response]\n\nIf you want, you keep querying and have things explained in more detail, like with the below prompt:\n\n[Prompt]\n\nWhat does that mean in simpler terms?\n\n[End of prompt]",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "[Prompt response]\n\nit returns a number between 0 and 1, where 1 is perfect predict\n\n[End of response]\n\nPredict We won’t know how good the model is until we try to predict something,\n\neven if we could refer to the score value. Let’s craft a prompt for this task:\n\n[Prompt]\n\nPredict in a notebook. Show the code.\n\n[End of prompt]\n\n[Prompt response]\n\n# Predict in a notebook. Show the code. predictions = model.predict(test[[\"avg_rating\"]])\n\n[End of response]\n\nOK, we got a variable called predictions that contains the predictions, but\n\nit is hard to understand if the predictions are reasonable. Let’s try asking\n\nCopilot and see what it suggests.\n\n[Prompt]\n\nHow do you suggest we show the predictions in a notebook?\n\n[End of prompt]\n\n[Prompt response]",
      "content_length": 751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "print the predictions\n\n[End of response]\n\n[Prompt]\n\nAny other ways you suggest we show the predictions in a notebook except\n\nfor printing?\n\n[End of prompt]\n\n[Prompt response]\n\nplot the predictions\n\n[End of response]\n\nNow that we’ve gotten more information on our various options, let’s use that knowledge in a prompt:\n\n[Prompt]\n\nShow me how to print and plot predictions vs the actual data in a notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Show me how to print and plot predictions vs actual data in a import matplotlib.pyplot as plt plt.scatter(test[[\"avg_rating\"]], test[[\"sold\"]], color='black plt.plot(test[[\"avg_rating\"]], predictions, color='blue', linew\n\n[End of response]\n\nRunning this code in a notebook will produce the following plot:",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Figure 9.1: Plot showing predictions vs actual data\n\nIn the plot, it looks like the line is a good fit for the data. We can also print\n\npredictions to see the actual values:\n\nprint(predictions)\n\nSaving the model to a .pkl ﬁle We have one step left: we need to save the model to a file. We have a few\n\ndifferent choices for how to save the model. We can save it as a pickle file or an ONNX file, for example. Let’s start with saving it as a pickle file.\n\nAs usual, let’s craft a prompt for this task:\n\n[Prompt]",
      "content_length": 509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Save the model to a pkl in the notebook.\n\n[End of prompt]\n\n[Prompt response]\n\n# Save the model to pkl in a notebook. import pickle pickle.dump(model, open(\"model.pkl\", \"wb\"))\n\n[End of response]\n\nFor the completion of the above, you may need to start\n\ntyping “import” to get the suggestion. Additionally, it’s a\n\ngood bet it’s going to use the pickle library, so on the next\n\nline you can start typing “pickle” and press TAB to accept\n\nthe suggestion.\n\nCreating a REST API in Python At this point, we have a model.pkl file that contains our model. We can\n\nexpose the model using this file via a REST API. Let’s ask Copilot to help\n\nus with this.\n\n[Prompt]\n\nCreate a REST API in Python that exposes the model in a pkl file.\n\n[End of prompt]\n\n[Prompt response]",
      "content_length": 757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "# Create a REST API in Python that exposes the model in a pkl f import pickle import flask # load the model model = pickle.load(open(\"model.pkl\", \"rb\")) # app app = flask.Flask(__name__) # routes @app.route('/', methods=['GET', 'POST']) def predict(): if flask.request.method == 'GET': return \"Please POST a JSON object to this endpoint to g else: data = flask.request.json prediction = model.predict([[data[\"avg_rating\"]]]) return flask.jsonify({\"prediction\": prediction[0][0]})\n\n[End of response]\n\nNote how we use several prompts above. First, we set the\n\ncontext by adding the prompt “Create a REST API in\n\nPython that exposes the model in a pkl file” at the top of the\n\nfile. Then we use various helper prompts like “load the\n\nmodel”, “app”, and “routes” to help us with the code. We\n\nend up with an API that exposes the model via a REST API.\n\nConverting the model to ONNX So far, you’ve seen how we can use Pickle to save a model and then load it\n\nin Python. However, Pickle has drawbacks, including being Python-\n\nspecific. ONNX, on the other hand, is a format that is not Python-specific",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "and can be used in other languages. Let’s see how we can use the ONNX\n\nformat to work with our model.\n\nWe need to solve our task in two steps:\n\nConvert the model to ONNX format.\n\nLoad the model and create a REST API in JavaScript.\n\nCreating a model in ONNX format Because we’re using a notebook, we need to install the dependencies,\n\nimport the libraries we need, and then convert the model to ONNX format.\n\n[Prompt]\n\nWhat dependencies do I need for ONNX? Show me the Python code for\n\ninstalling those dependencies.\n\n[End of prompt]\n\n[Prompt response]\n\n! pip install skl2onnx\n\n[End of response]\n\n[Prompt]\n\nShow me how to import the libraries I need for ONNX and show me how to\n\nconvert the model to ONNX format.\n\n[End of prompt]\n\n[Prompt response]",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "import skl2onnx # convert the model to ONNX skl2onnx.to_onnx(model, train[[\"avg_rating\"]].astype(\"float32\")\n\n[End of response]\n\nNow I should have a model.onnx file that contains the model in ONNX format. Let’s see if we can load it in JavaScript next.\n\nLoading the ONNX model in JavaScript Next, we need to load the ONNX model in JavaScript and specifically\n\nNode.js, as we’re doing this on the backend. We can use the onnxruntime\n\nlibrary to do this. Let’s ask Copilot to help us with this.\n\n[Prompt]\n\nShow me a step-by-step guide to load the ONNX model in JavaScript.\n\n[End of prompt]\n\n[Prompt response]\n\ninstall onnxruntime in JavaScript\n\nload the ONNX model in JavaScript\n\npredict using ONNX model in JavaScript\n\n[End of response]\n\nNow that we have a plan, let’s execute it.",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Installing onnxruntime in JavaScript Because we’re building a Node.js project, let’s first, in a new folder, create\n\nan app.js file and run the npm init -y terminal command to create a new\n\nNode.js project.\n\nThe first step in our plan is to install onnxruntime in JavaScript. We can use\n\nnpm to perform this task.\n\n[Prompt]\n\nInstall onnxruntime in JavaScript.\n\n[End of prompt]\n\n[Prompt response]\n\nnpm install onnxruntime\n\n[End of response]\n\nGreat, this installs the onnxruntime library in our project. Next, we need to\n\nload the ONNX model.\n\nLoading the ONNX model in JavaScript Now that we have onnxruntime installed, we can load the ONNX model.\n\n[Prompt]\n\nLoad the ONNX model in JavaScript.",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "[End of prompt]\n\n[Prompt response]\n\nconst rt = require(\"onnxruntime\"); # load model from ONNX model rt.InferenceSession(\"model.onnx\"); # predict model.run(null, {model.get_inputs()[0].name: [[4.5]]});\n\n[End of response]\n\nIn the preceding code, we load the model from the ONNX file and then we\n\nmake predictions using the model with the input 4.5 to represent the\n\naverage rating to see what sales we can expect.\n\nAssignment: Build a REST API in JavaScript that consumes the model\n\n1. Take the model we created in the previous section and add the code to\n\nthe notebook to turn it into an ONNX file.\n\n2. Create a new folder in the repo called and create a new file called\n\napp.js in that folder.\n\n3. Add the POST /predict route to the server.js file and ensure it returns\n\na prediction given the input.\n\nHere’s some starter prompts you can try to help you with this assignment:\n\nPrompt: Create a REST API in JavaScript using Express",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Prompt: Create a POST /predict route in a REST API in JavaScript\n\nusing Express\n\nPrompt: Load the model from ONNX in a REST API in JavaScript\n\nusing Express\n\nPrompt: Predict using the ONNX model in a REST API in JavaScript\n\nusing Express\n\nSolution See repo [https://github.com/PacktPublishing/AI-\n\nAssisted-Software-Development-with-GitHub-Copilot- and-ChatGPT/tree/main/09] and the 09 folder for the solution.\n\nQuiz What’s the difference between Pickle and ONNX?\n\n1. Pickle is Python-specific and ONNX is not.\n\n2. Pickle can be used in JavaScript and ONNX can’t.\n\n3. ONNX is less efficient than Pickle.\n\nSummary In this chapter, we covered various model formats like Pickle and ONNX\n\nand how to persist your model as a file using Python. Storing a model as a\n\nfile is useful because it allows you to integrate it with other applications.\n\nThen we discussed the pros and cons of different formats for storing models\n\nlike Pickle and ONNX. We came to the conclusion that ONNX is probably",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "the better choice because it’s not Python-specific and can be used in other\n\nlanguages.\n\nThen we covered how to load a model stored in ONNX format using\n\nJavaScript and create a REST API to make the model available to other\n\napplications.\n\nIn the next chapter, we’ll go into more detail of how we can use GitHub\n\nCopilot and get the most out of it. We’ll cover both tips and tricks and\n\nfeatures that help make you faster and more productive.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "10\n\nMaintaining Existing Codebases\n\nIntroduction Brownfield is another word for working with existing code. In my career as\n\na developer, most of the work carried out has been on existing code. The\n\nopposite of brownfield is greenfield, which is a new project with no\n\nexisting code.\n\nFor that reason, it’s important to cover how to work with existing\n\ncodebases, and there’s a lot to get excited about when working with an AI\n\nassistant like GitHub Copilot in a brownfield context.\n\nIn this chapter, we will:\n\nLearn about the different types of maintenance.\n\nUnderstand how we work with maintenance in a process to de-risk introduced changes.\n\nUse GitHub Copilot to help us with maintenance.\n\nPrompt strategy This chapter is a bit different from other chapters in the book. The focus is\n\non describing various problems you may encounter in the space of existing",
      "content_length": 862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "codebases. You’re recommended to use the prompt suggestion approach\n\nwith which you’re the most comfortable, be it prompt comments or the chat\n\ninterface. As for patterns, you’re encouraged to try out all three major\n\npatterns introduced, that is, PIC, TAG, or Exploratory patterns as described in Chapter 2. This chapter however focuses on using the “Exploratory\n\npattern”.\n\nDiﬀerent types of maintenance There are different types of maintenance, and it’s important to understand\n\nthe differences between them. Here are some different types that you’re\n\nlikely to encounter:\n\nCorrective maintenance: This is when we’re fixing bugs.\n\nAdaptive maintenance: In this case, we change code to adapt to new\n\nrequirements.\n\nPerfective maintenance: When we improve code without changing\n\nthe functionality. Examples of this could be refactoring or improving\n\nthe performance of the code.\n\nPreventive maintenance: Changing the code to prevent future bugs or\n\nissues.\n\nThe maintenance process Every time you change code, you introduce risk. For example, a bug fix\n\ncould introduce a new bug. To mitigate this risk, we need to follow a\n\nprocess. A suggested process could be the following steps:",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "1. Identify: Identify the problem or the change that needs to be made.\n\n2. Inspect: Inspect the test coverage and how well your code is covered by tests. The better it’s covered, the more likely you are to detect any introduced bugs or other issues.\n\n3. Plan: Plan the change. How are you going to make it? What tests are\n\nyou going to write? What tests are you going to run?\n\n4. Implement: Implement the change.\n\n5. Verify: Verify that the change works as expected. Run the tests, run the\n\napplication, check the logs, etc.\n\n6. Integrate: This is about ensuring any change you make in a branch is\n\nmerged into the main branch.\n\n7. Release/deploy the change: You want to make sure the end customer can leverage the benefit of this change. For that to happen, you need to deploy it.\n\nDo we need to cover all these steps for every change? No, it depends on the change; some changes might be so small that we might want to bundle many of them before we make a release that we want the end customer to\n\nhave. The good news is that we have a great tool in GitHub Copilot that can help us with some of these minor steps, allowing us to focus on the more\n\nimportant steps.\n\nAddressing a bug Some bugs are more severe than others. In a best-case scenario, your bug is\n\nseldom encountered, or when it does occur it’s not a big deal. In a worst- case scenario, your bug loses you money. Let’s focus on a case where the bug loses you money in a scenario where you’re running an e-commerce\n\nsite.",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Hopefully, this code has never been shipped in production, but it’s a good\n\nexample of a bug that could be catastrophic for an e-commerce site:\n\ndef process_cart(): # get cart from database cart = get_cart() card = cart.get_card() process_payment(card, cart) ship_order(cart)\n\nThe problem you might have is that ship_order is called even if\n\nprocess_payment fails, which means you ship orders without getting paid, losing you money.\n\nLooking at this code, you might not have a problem if process_payment\n\nthrows an error and you don’t even get to ship_order.\n\nWhat if it doesn’t and, instead, returns a numerical code that depends on\n\npayment results, like so?\n\ndef process_payment(card, cart): if card.is_valid(): if card.has_funds(cart.total): charge_card(card, cart.total) return 0 else: return 1\n\nIf you don’t read the error code, you might not notice that the payment\n\nfailed, and the order is shipped. So, what’s our approach here?\n\n1. Identify the problem",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "First, we need to identify the problem; in this case, there are two things you\n\ncan do, namely:\n\nRead the code: What does our code look like? Are errors thrown, does\n\nit return a code, or something else entirely?\n\nCheck the tests: Do we have a test that covers this scenario? If not, we need to write one. The test should show that we have a bug.\n\nLet’s use Copilot for this, with a prompt like so (here, we simulate process_payment to return error codes instead of throwing errors). Select\n\nthe ship_order code in the main window, and then type a prompt as follows\n\nin the chat window:\n\n[Prompt]\n\nWrite a test for the code; ship_order should not be called if process_payment returns 1.\n\n[End of prompt]\n\nWe’re using the GitHub Copilot Chat function and selecting the process_cart code in our text editor.\n\nYou should see a response similar to the below code:\n\n[Prompt response]\n\n```python import unittest from unittest.mock import Mock, patch from your_module import process_cart # replace with the actual class TestProcessCart(unittest.TestCase): @patch('your_module.get_cart') # replace with the actual m @patch('your_module.process_payment')",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "@patch('your_module.ship_order') def test_order_not_shipped_if_payment_fails(self, mock_ship # Arrange mock_cart = Mock() mock_cart.get_card.return_value = 'test_card' mock_get_cart.return_value = mock_cart mock_process_payment.return_value = 1 # Act process_cart() # Assert mock_ship_order.assert_not_called() if __name__ == '__main__': unittest.main() ```\n\n[End of response]\n\nThe above code contains a test that tests what happens if we get error code 1 from process_payment. What we want to have happen is for ship_order\n\nnot to be called.\n\nThis test will fail, as we have a bug in our code, which is a good thing, as\n\nwe’ve identified a bug and can prove it exists with a test.\n\n2. Implement the change Now that we have a test that shows we have a bug, we can implement the change. We can be confident to make the code change, as we have a test\n\nthat shows that we have a bug, and we’ve hopefully also improved test\n\ncoverage enough.\n\nDon’t start refactoring unless you have good test coverage or the change is so small that you are confident that you",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "won’t introduce a bug.\n\nHow do we now improve the code? Simple – we can use GitHub Copilot to help us with the implementation. Let’s use the following prompt to help us\n\nwith the code change we need to do:\n\n[Prompt]\n\nImplement the change; ship_order should not be called if process_payment\n\nreturns 1.\n\n[End of prompt]\n\nYour code for process_cart should now look like this:\n\n[Prompt response]\n\n```python def process_cart(): # get cart from database cart = get_cart() card = cart.get_card() payment_status = process_payment(card, cart) # Only ship the order if the payment was successful if payment_status != 1: ship_order(cart) ```\n\n[End of response]\n\nThis code above is a step in the right direction, as it now contains an if statement that checks the payment status.\n\nTo further improve the code, we can use Copilot to help us make the code\n\nmore robust and easier to read. Let’s use the following prompt for additional improvements:",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "[Prompt]\n\nRefactor the code, making it easier to maintain.\n\n[End of prompt]\n\nThis might give you the following code:\n\n[Prompt response]\n\ndef get_payment_status(cart): card = cart.get_card() return process_payment(card, cart) def process_cart(): # get cart from database cart = get_cart() payment_status = get_payment_status(cart) # Only ship the order if the payment was successful if payment_status != 1: ship_order(cart)\n\n[End of response]\n\nHere, Copilot has suggested that we extract the payment status into a\n\nseparate function. This is a good suggestion, as it makes the code easier to\n\nread and maintain.\n\nAdding a new feature In this example, we’ll look at an existing codebase and add a new feature.\n\nThe codebase is like the one we used in the previous example, but it’s a bit more complex. This is because it has a few more functions involving\n\ndifferent types of payment methods. Here’s the code:",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "```python import re def validate_card(card): # Visa cards start with 4 and have 13 or 16 digits. # MasterCard cards start with 51 through 55 and have 16 dig visa_pattern = r'^4[0-9]{12}(?:[0-9]{3})?$' mastercard_pattern = r'^5[1-5][0-9]{14}$' if re.match(visa_pattern, card): return 'Visa' elif re.match(mastercard_pattern, card): return 'MasterCard' else: return None def process_payment(card, cart): card_type = validate_card(card) if card_type is None: return 1 # Invalid card else: # Process the payment here # Return 0 if the payment is successful, 1 otherwise pass def process_cart(): # Get cart from database cart = get_cart() card = cart.get_card() payment_status = process_payment(card, cart) # Only ship the order if the payment was successful if payment_status == 0: ship_order(cart) ```\n\nIn the above code, we have the following:\n\nvalidate_card validates the card number and returns the card type if valid; otherwise, it returns None.\n\nprocess_payment processes the payment and returns 0 if the payment was successful; otherwise, it returns 1.",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "process_cart processes the cart, gets the card, processes the payment,\n\nand ships the order if the payment is successful.\n\n1. Identify a problem and ﬁnd the function/s to change Our task is to make our code also support American Express cards. Implementing a new card means that we need to update the validate_card\n\nfunction to support American Express cards.\n\nNow, we know what functions need to change, but does GitHub Copilot\n\nknow?\n\nLet’s ask GitHub Copilot, with a prompt.\n\nIn the identification step, we used our know-how to identify\n\nwhat and where to change. A good practice is to rely on a combination of your own skills and the Copilot tool to help\n\nanalyze and identify problems.\n\n2. Implement change, and add a new feature and tests Let’s use GitHub Copilot to help us with this. We can use the following prompt:\n\n[Prompt]\n\nAdd support for American Express cards.\n\n[End of prompt]",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "You will get code similar to this:\n\n[Prompt response]\n\nimport re def validate_card(card): # Visa cards start with 4 and have 13 or 16 digits. # MasterCard cards start with 51 through 55 and have 16 dig # American Express cards start with 34 or 37 and have 15 di visa_pattern = r'^4[0-9]{12}(?:[0-9]{3})?$' mastercard_pattern = r'^5[1-5][0-9]{14}$' amex_pattern = r'^3[47][0-9]{13}$' if re.match(visa_pattern, card): return 'Visa' elif re.match(mastercard_pattern, card): return 'MasterCard' elif re.match(amex_pattern, card): return 'American Express' else: return None\n\n[End of response]\n\nIn the above code, Copilot has made the following changes:\n\nCreated a new regular expression in the validate_card function to\n\nsupport American Express cards.\n\nAdded a new elif statement to return American Express if the card matches the new regular expression.\n\nOf course, we’re not done yet; we need to add a test to verify that the code\n\nworks as expected. Let’s use the following prompt:\n\n[Prompt]\n\nWrite a test for the code; validate_card should return American Express for\n\na valid card.",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "[End of prompt]\n\nYour test should look like below:\n\n[Prompt response]\n\n```python import unittest from your_module import validate_card # replace with the actua class TestValidateCard(unittest.TestCase): def test_validate_card_returns_amex_for_valid_amex_card(sel # Arrange valid_amex_card = '378282246310005' # Act result = validate_card(valid_amex_card) # Assert self.assertEqual(result, 'American Express') if __name__ == '__main__': unittest.main() ```\n\n[End of response]\n\nThe new test, test_validate_card_returns_amex_for_valid_amex_card, was added by Copilot and tests that the validate_card function returns\n\nAmerican Express for a valid American Express card.\n\nAt this point, it’s a good idea to keep iterating; you not only want to test that\n\nthe code works for a valid card but also for an invalid card, and so on.\n\nYou can use Copilot as a challenge to write more tests.\n\nImproving performance",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Improving performance is a common task but can be tricky to achieve.\n\nUsually, you need specific tools to measure performance, and you need to\n\nknow what to measure. A good way to find out how to improve your code\n\nin this way is to use a decorator to profile how long a method takes to\n\nexecute. Then, you can measure specific methods and find out which\n\nmethods to improve.\n\nHere’s an example of a decorator being used on a function that sleeps for 2\n\nseconds:\n\nimport time def profile(func): def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(f'{func.__name__} took {end - start} seconds') return result return wrapper @profile def sleep_two_seconds(): time.sleep(2) sleep_two_seconds()\n\nThe profile function is a decorator that takes a function as input and\n\nreturns a function that wraps the input function. The wrapper function measures the time it takes to execute the input function and prints the result.\n\nWere you to run this code via the terminal, you would see output similar to\n\nthe following:\n\nsleep_two_seconds took 2.000000238418579 seconds",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "OK, so how do we use this to improve performance? Let’s use GitHub\n\nCopilot and see how we can get advice on improving performance on a\n\npiece of code.\n\nHere’s the code we want to improve. This processes records and looks for\n\nduplicates:\n\ndef process_records(records): for i in range(len(records)): for j in range(len(records)): if i != j and records[i] == records[j]: print(f'Duplicate record found: {records[i]}')\n\nMost developers looking at this will refer back to their algorithms and data\n\nstructures books and start thinking about things like Big O notation, a way\n\nto measure the performance of an algorithm. The main thing to focus on is\n\nreducing the number of costly operations to improve performance while\n\nretaining the same functionality.\n\nSo, the question we should ask GitHub Copilot is, how do we improve this\n\ncode? Let’s use the following prompt:\n\n[Prompt]\n\nImprove the performance of the code.\n\n[End of prompt]\n\nWe get something like this back:\n\n[Prompt response]\n\ndef process_records(records): record_set = set() for record in records: if record in record_set:",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "print(f'Duplicate record found: {record}') else: record_set.add(record)\n\n[End of response]\n\nSo, why is this code better? Well, let’s prove it:\n\n1. First, by calculating the Big O notation for the original code and then\n\nfor the new code.\n\n2. Second, by using the decorator we created earlier.\n\nBig O notation calculation Try this prompt:\n\n[Prompt]\n\nWhat’s the Big O complexity for this code? Show me how you calculated it\n\nas well.\n\n[End of prompt]\n\nYou should get something like this:\n\n[Prompt response]\n\nThe time complexity of the provided code is O(n^2), where n is the number\n\nof records.\n\nHere’s how it’s calculated:\n\nThe outer loop runs n times, where n is the number of records. This is\n\nbecause it iterates over each record in the list.",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Inside the outer loop, there’s an inner loop that also runs n times for\n\nthe same reason.\n\nSince the inner loop runs n times for each iteration of the outer loop,\n\nthe total number of iterations is n * n, or n^2.\n\nTherefore, the time complexity is O(n^2).\n\nThis means that if you double the number of records, the time it takes to process them will increase by about four times. This is a characteristic of\n\nquadratic time complexity.\n\n[End of response]\n\nOK, O(n^2) is not great, but what about the new code?\n\nRun the same prompt but on the new code:\n\n[Prompt response]\n\nThe time complexity of the provided code is O(n), where n is the number of\n\nrecords.\n\nHere’s how it’s calculated:\n\nThe loop runs n times, where n is the number of records. This is\n\nbecause it iterates over each record in the list.\n\nInside the loop, the operations (checking if a record is in the set and\n\npotentially adding it to the set) are constant time, O(1), because set\n\noperations in Python are typically constant time on average.\n\nTherefore, the time complexity is O(n) * O(1), which simplifies to O(n).\n\nThis means that if you double the number of records, the time it takes to\n\nprocess them will also double, which is a characteristic of linear time complexity.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "[End of response]\n\nO(n) is better than O(n^2), but how much better? Let’s measure this in the\n\nnext section.\n\nMeasuring performance OK, so algorithmically, we know that the new code is better, but how much\n\nbetter? Let’s measure it by using the decorator and calling the code. Let’s\n\nuse the input of 1,000 records and 10,000 records and compare the time it\n\ntakes to execute the code.\n\nOK, let’s see if the new code is faster than the old code. Let’s try with\n\n10,000 records:\n\n# old code @profile def process_records(records): for i in range(len(records)): for j in range(len(records)): if i != j and records[i] == records[j]: print(f'Duplicate record found: {records[i]}') records_10000 = [i for i in range(10000)] process_records(records_10000)\n\nRunning this code, you should see the following output:\n\nprocess_records took 5.193912506103516 seconds\n\nNow, let’s run the new code:\n\n# new code @profile def process_records(records):",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "record_set = set() for record in records: if record in record_set: print(f'Duplicate record found: {record}') else: record_set.add(record) records_10000 = [i for i in range(10000)] process_records(records_10000)\n\nRunning this code, you should see the following output:\n\nprocess_records took 0.0011200904846191406 seconds\n\nAs you can see, by combining your knowledge with GitHub Copilot, you can improve your code.\n\nYour code won’t always look this obvious, and you might\n\nneed to do more work to improve performance. You’re\n\nrecommended to use a profiler to measure performance, and then use GitHub Copilot to help you improve the code.\n\nImproving maintainability Another interesting use case is using GitHub Copilot to help you improve\n\nthe maintainability of your code. So what are some things that you can do to\n\nimprove the maintainability of your code? Here’s a list:\n\nImprove the naming of variables, functions, classes, etc.\n\nSeparate concerns: For example, separate business logic from\n\npresentation logic.",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Remove duplication: Especially in large codebases, you’re likely to\n\nfind duplication.\n\nImprove readability: You can improve readability by, for example,\n\nusing comments, docstrings, event tests, and more.\n\nLet’s start with a codebase and see how we can improve it. Here’s the code:\n\ndef calculate_total(cart, discounts): # Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantit return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): items_sorted = sorted(items, key=lambda x: x.price) total = 0 for item in items_sorted[1:]: total += item.price * item.quantity return total # Calculate totals for each discount totals = [] for discount in discounts: if discount == '3 for 2': totals.append((discount, three_for_two(cart))) elif discount == 'Christmas Discount': totals.append((discount, christmas_discount(cart))) elif discount == 'Cheapest Free': totals.append((discount, cheapest_free(cart))) # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Free'] # Define cart",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\nThe above codebase describes the following at a high level:\n\nA function, calculate_total, that calculates the total for a cart and a list of discounts\n\nA list of discounts\n\nA cart\n\nWhat you can see from this codebase is that it’s not very maintainable, as it’s hard to understand what’s going on.\n\n1. Identify the problems. What problems do you see? Let’s first ask GitHub Copilot what it thinks about the code and decide what to do next. Let’s use the following prompt:\n\n[Prompt]\n\nWhat do you think about the code? What would you do to improve it?\n\n[End of prompt]\n\nYou get a lengthy response back with a bullet list and improved code. Let’s\n\nlook at the bullet list first:\n\n[Prompt response]",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Use Classes for Items and Discounts: Right now, items and discounts\n\nare represented as dictionaries and strings, respectively. Using classes\n\nfor these entities would make the code more readable and flexible.\n\nSeparate Discount Functions: The discount functions are currently defined inside the calculate_total function. It would be better to\n\ndefine these functions separately to improve readability and make the\n\ncode easier to test and maintain.\n\nUse a More Extensible Approach for Applying Discounts: The current approach for applying discounts involves a series of if-elif statements. This could be made more extensible by storing the discount functions in a dictionary or a list of Discount objects, and\n\nthen iterating over that collection to apply the discounts.\n\n[End of response]\n\nThe problems identified above all seem reasonable – using classes,\n\nseparating functions, and using dictionary.\n\n2. Add tests and de-risk change Before we look at a suggested code change, we need to minimize the risk of introducing a bug. The first thing we should do is check our test coverage.\n\nIn this case, we don’t have any tests, so we need to write some. Let’s use\n\nthe following prompt:\n\n[Prompt]\n\nWrite tests for this code, covering all possible execution paths.\n\n[End of prompt]\n\n[Prompt response]",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "import unittest class TestCalculateTotal(unittest.TestCase): def test_no_discounts(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = [] total = calculate_total(cart, discounts) self.assertEqual(total, 80) def test_three_for_two_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['3 for 2'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_christmas_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['Christmas Discount'] total = calculate_total(cart, discounts) self.assertEqual(total, 40) def test_cheapest_free_discount(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1}, {'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['Cheapest Free'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_multiple_discounts(self): cart = [ {'name': 'item1', 'price': 10, 'quantity': 3}, {'name': 'item2', 'price': 20, 'quantity': 1},",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "{'name': 'item3', 'price': 30, 'quantity': 1} ] discounts = ['3 for 2', 'Christmas Discount', 'Cheapest total = calculate_total(cart, discounts) self.assertEqual(total, 40) if __name__ == '__main__': unittest.main()\n\n[End of response]\n\nNow, you should never trust what an AI generates, so take\n\nyour time to review the code, run a test coverage tool, and\n\nensure that you’re happy with the code before you proceed\n\nto the next step, which is to improve the code.\n\nTo improve the code from the first suggestion from Copilot, you want to\n\nensure that it runs and uses classes like Item when you add items to a cart,\n\nfor example. Below is the result of those alterations:\n\nimport unittest from discount_old import calculate_total, Item from item import Item class TestCalculateTotal(unittest.TestCase): def test_no_discounts(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = [] total = calculate_total(cart, discounts) self.assertEqual(total, 80) def test_three_for_two_discount(self): cart = [ Item('item1', 10, 3),",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['3 for 2'] total = calculate_total(cart, discounts) self.assertEqual(total, 70) def test_christmas_discount(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['Christmas Discount'] total = calculate_total(cart, discounts) self.assertEqual(total, 40) def test_cheapest_free_discount(self): cart = [ Item('item1', 10, 3), #30 Item('item2', 20, 1), # 20 Item('item3', 30, 1) # 30 ] discounts = ['Cheapest Free'] total = calculate_total(cart, discounts) self.assertEqual(total, 60) def test_multiple_discounts(self): cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] discounts = ['3 for 2', 'Christmas Discount', 'Cheapest total = calculate_total(cart, discounts) self.assertEqual(total, 40) if __name__ == '__main__': unittest.main()\n\nAnd for the application code:\n\nfrom item import Item def calculate_total(cart, discounts):",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "# Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantit return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): if not items: return 0 items_sorted = sorted(items, key=lambda x: x.price * x. cheapest_item = items_sorted[0] total = 0 for item in items: total += item.price * item.quantity total -= cheapest_item.price * cheapest_item.quantity i return total # Calculate totals for each discount totals = [] for discount in discounts: if discount == '3 for 2': totals.append((discount, three_for_two(cart))) elif discount == 'Christmas Discount': totals.append((discount, christmas_discount(cart))) elif discount == 'Cheapest Free': totals.append((discount, cheapest_free(cart))) # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # if run as script, if __name__ == '__main__': # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Fre # Define cart cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1)",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\nOK, so we now have a codebase that we’re happy with and that’s passing\n\ntests. Let’s now look at the suggested code change from GitHub Copilot.\n\n3. Implement change and improve maintainability OK, we feel confident that we have good test coverage and we’re ready to improve the code. Let’s use the following prompt:\n\n[Prompt]\n\nImprove the maintainability of the code.\n\n[End of prompt]\n\nYou should get something like this:\n\n[Prompt response]\n\nTo improve the maintainability of the code, we can make the following\n\nchanges:\n\nMove the discount functions out of calculate_total to make them\n\nstandalone functions. This improves readability and makes it easier to\n\ntest these functions individually.\n\nUse a dictionary to map discount names to their corresponding functions. This eliminates the need for multiple if/elif statements when applying discounts.",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "from item import Item # Define discount functions def three_for_two(items): total = 0 for item in items: total += item.price * (item.quantity - item.quantity // return total def christmas_discount(items): total = 0 for item in items: total += item.price * item.quantity * 0.5 return total def cheapest_free(items): if not items: return 0 items_sorted = sorted(items, key=lambda x: x.price * x.quan cheapest_item = items_sorted[0] total = 0 for item in items: total += item.price * item.quantity total -= cheapest_item.price * cheapest_item.quantity if ch return total def calculate_total(cart, discounts): # Map discount names to functions discount_functions = { '3 for 2': three_for_two, 'Christmas Discount': christmas_discount, 'Cheapest Free': cheapest_free } # Calculate totals for each discount totals = [] for discount in discounts: if discount in discount_functions: totals.append((discount, discount_functions[discoun # Sort totals and return the lowest one totals_sorted = sorted(totals, key=lambda x: x[1]) return totals_sorted[0][1] if totals_sorted else sum(item.p # if run as script, if __name__ == '__main__': # Define discounts discounts = ['3 for 2', 'Christmas Discount', 'Cheapest Fre # Define cart",
      "content_length": 1217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "cart = [ Item('item1', 10, 3), Item('item2', 20, 1), Item('item3', 30, 1) ] # Calculate total total = calculate_total(cart, discounts) print(f'Total: {total}')\n\n[End of response]\n\nThe code is now easier to maintain, as the following changes have been\n\nintroduced:\n\ncalculate_total is a vastly smaller function, where each of the functions it used to contain has been moved out\n\nThe same method mentioned also now uses a dictionary to map names\n\nto functions\n\nThe key to using Copilot effectively is to keep iterating,\n\nkeep making small changes, and see what Copilot suggests.\n\nChange the prompt slightly, see what happens, and go with\n\nthe output that best suits your needs.\n\nChallenge Try improving the code further using a prompt and GitHub Copilot. What\n\nshould happen if there’s a problem with the payment? Should you log it,\n\nmaybe raise an error, etc.?",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Updating an existing e- commerce site Let’s continue working on the e-commerce site that we presented in\n\nprevious chapters. In this chapter, we’ll focus on improving the codebase\n\nand adding new features.\n\nFor reference, let’s show the basket.html file we’ve started to create:\n\n<!-- a page showing a list of items in a basket, each item shou <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\">\n\n<!-- add bootstrap --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn </head> <body> <!-- add 3 basket items with each item having id, name, pr\n\n<!-- <div class=\"container\"> <div id=\"basket\" class=\"basket\"> </div> </div> --> <!-- add app.js --> <!-- add app.js, type javascript -->\n\n<div id=\"basket\" class=\"basket\"> <!-- render basket from Vue app, use Boostrap --> <div v-for=\"(item, index) in basket\" class=\"basket-item <div class=\"basket-item-text\"> <h2>{{ item.name }}</h2> <p>Price: {{ item.price }}</p> <p>Quantity: {{ item.quantity }}</p> <p>Sum: {{ item.price * item.quantity }}</p>",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "</div> <div class=\"basket-item-buttons\"> <button type=\"submit\" class=\"btn btn-primary bt <button type=\"submit\" class=\"btn btn-primary bt </div> </div> </div> <!-- add Vue 3 --> <script src=\"https://unpkg.com/vue@next\"></script> <!-- add boostrap code for Vue 3 --> <script> let app = Vue.createApp({ data() { return { basket: [ { id: 1, name: 'Product 1', price: 100, quantity: 1 }, { id: 2, name: 'Product 2', price: 200, quantity: 2 }, { id: 3, name: 'Product 3', price: 300, quantity: 3 }, { id: 4, name: 'Product 4', price: 400, quantity: 4 } ] } },",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { let total = 0; for (let i = 0; i < this.basket.length; i++ total += this.basket[i].price * this.ba }\n\nreturn total; } } }); app.mount('#basket'); </script> <!-- <script src=\"js/app.js\"></script> --> </body>\n\nThis is an early version of a prototype; it’s not meant to be\n\nproduction-ready code. However, we can use this code to\n\nimprove it and make it production-ready.\n\nNow that we know this code isn’t great, let’s improve it with Copilot and\n\nthe following prompt:\n\n[Prompt]\n\nImprove the code, making it production-ready.",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "[End of prompt]\n\nThis prompt highlights some of the problems with the code, namely:\n\n[Prompt response]\n\nSeparate JavaScript and CSS into external files: This makes the\n\ncode more maintainable and allows for caching of these resources.\n\nUse a build tool or a module bundler: Tools like Webpack or Parcel\n\ncan help you manage dependencies, transpile modern JavaScript to\n\nolder versions for better browser compatibility, and minify your code\n\nfor production.\n\nError Handling: Add error handling to your methods to ensure your\n\napplication can gracefully handle unexpected situations.\n\nVue Component Structure: Instead of creating the entire Vue app in\n\none script, break it down into reusable components.\n\nAccessibility: Ensure that your HTML is accessible by using\n\nappropriate ARIA roles and properties.\n\n[End of response]\n\nThat’s a great list of things we can ask about next, but let’s see what code\n\nchanges it suggests. It suggests code separated into two different files, one\n\nfor the HTML and one for the JavaScript:\n\n<!-- index.html --> <html> <head> <title>Basket</title> <link rel=\"stylesheet\" href=\"css/basket.css\"> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn </head> <body> <div id=\"basket\" class=\"basket\"></div>",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "<script src=\"https://unpkg.com/vue@next\"></script> <script src=\"js/app.js\"></script> </body> </html>\n\nNow, let’s look at how the JavaScript was improved:\n\n// app.js let app = Vue.createApp({ data() { return { basket: [ { id: 1, name: 'Product 1', price: 100, quantit { id: 2, name: 'Product 2', price: 200, quantit { id: 3, name: 'Product 3', price: 300, quantit { id: 4, name: 'Product 4', price: 400, quantit ] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + } } }); app.mount('#basket');",
      "content_length": 694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "A great step forward. As a developer, you can see that the JavaScript code\n\nuses static data, so let’s change that with the following prompt:\n\n[Prompt]\n\nImprove the code, use dynamic data by calling an API, and also generate a\n\nseparate Python API with Flask that has the route /api/basket, containing the\n\nbasket data.\n\n[End of prompt]\n\nNow, we get two files, the API and the JavaScript code. Let’s look at the\n\nAPI first:\n\n[Prompt response]\n\n# app.py from flask import Flask, jsonify app = Flask(__name__) @app.route('/api/basket') def get_basket(): basket = [ { 'id': 1, 'name': 'Product 1', 'price': 100, 'quantity { 'id': 2, 'name': 'Product 2', 'price': 200, 'quantity { 'id': 3, 'name': 'Product 3', 'price': 300, 'quantity { 'id': 4, 'name': 'Product 4', 'price': 400, 'quantity ] return jsonify(basket) if __name__ == '__main__': app.run(debug=True)\n\nAbove you get a route /api/basket that returns the basket data. Let’s look\n\nat the improved JavaScript code:\n\nlet app = Vue.createApp({ data() {",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "return { basket: [] } }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) { if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + } }, mounted() { fetch('/api/basket') .then(response => response.json()) .then(data => { this.basket = data; }); } }); app.mount('#basket');\n\n[End of response]\n\nGreat! We’re now getting data from the API. How can we improve the\n\ncode? If you’ve built a Vue app before, you will know that it involves using\n\nthe Vue CLI and has a build step. Let’s ask GitHub Copilot about this with\n\nthe following prompt:\n\n[Prompt]\n\nImprove the code, use the Vue CLI, and add a build step.",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "[End of prompt]\n\nNow, we get both commands and file changes back as a response, as\n\nfollows:\n\n[Prompt response]\n\nInstall Vue CLI globally on your machine:\n\nnpm install -g @vue/cli\n\nCreate a new Vue project\n\nvue create my-project cd my-project\n\nNow, let’s create a new Vue component for the basket. Create a new file Basket.vue in the src/components directory:\n\n<!-- src/components/Basket.vue --> <template> <div id=\"basket\"> <!-- Your basket template goes here --> </div> </template> <script> export default { data() { return { basket: [] }; }, methods: { increaseQuantity(index) { this.basket[index].quantity++; }, decreaseQuantity(index) {",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "if (this.basket[index].quantity > 0) { this.basket[index].quantity--; } } }, computed: { total() { return this.basket.reduce((total, item) => total + item.p } }, mounted() { fetch('/api/basket') .then(response => response.json()) .then(data => { this.basket = data; }); } }; </script> <style scoped> /* Your CSS goes here */ </style>\n\nThen, import and use this component in your main App.vue:\n\n<!-- src/App.vue --> <template> <div id=\"app\"> <Basket /> </div> </template> <script> import Basket from './components/Basket.vue'; export default { name: 'App', components: { Basket } }; </script>",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "<style> /* Your CSS goes here */ </style>\n\nNow, you can use the Vue CLI’s build command to create a production-\n\nready version of your app:\n\nnpm run build\n\nThis will create a dist directory with a production-ready version of your\n\napp.\n\n[End of response]\n\nThis looks like a step in the right direction. However, you should always\n\ntest the code to make sure it works as expected.\n\nAs a challenge, see if the above code works and if you can improve it\n\nfurther.\n\nCode is likely to change between versions of a framework.\n\nIn this case, we’re using Vue.js, so make sure to consult the\n\ndocumentation for the version that you use.\n\nAssignment Find a piece of code you want to improve. Follow the process outlined in\n\nthis chapter to:\n\n1. Identify the problems. What problems do you see?\n\n2. Add tests and de-risk changes.",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "3. Implement the changes and improve maintainability.\n\nIf you don’t have a piece of code you want to improve, try using the code\n\nfrom this chapter or the code from the Kata (Gilded Rose) GitHub page:\n\nhttps://github.com/emilybache/GildedRose- Refactoring-Kata.\n\nKnowledge check\n\n1. What’s the difference between greenfield and brownfield\n\ndevelopment?\n\nA: Greenfield development is when you start coding from scratch;\n\nbrownfield development is when you update existing code.\n\n2. What’s the best way to update existing code?\n\nA: The best way is to make small changes and have plenty of tests in\n\nplace.\n\nSummary In this chapter, we established that a very important aspect of writing code\n\nis to update existing code, which is known as brownfield development. We\n\nalso looked at how GitHub Copilot can help you with this task.\n\nThe most important message to take away from this chapter is to ensure that\n\nyou have an approach to updating code that de-risks the changes you’re\n\nabout to make. It’s better to make a small change several times than a big\n\none once. It’s also strongly recommended to have plenty of tests in place\n\nbefore you start changing code.",
      "content_length": 1160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "11\n\nData Exploration with ChatGPT\n\nIntroduction Data exploration is an integral first step in machine learning, entailing a\n\nthorough examination of a dataset to identify its structure and uncover\n\ninitial patterns and anomalies. This process is critical for setting the stage\n\nfor any further detailed statistical analysis and the development of machine\n\nlearning models.\n\nIn this chapter, the focus is on delineating the process of data exploration,\n\naiming to solidify the understanding for newcomers to machine learning\n\nwhile providing a refresher for the adept. The chapter will navigate through\n\nthe techniques to load and inspect a dataset comprised of Amazon book\n\nreviews, summarize its characteristics, and probe into its variables.\n\nYou will be guided through practical exercises on categorical data evaluation, distribution visualization, and correlation analysis, with the support of Python’s pandas and Matplotlib libraries. The chapter will also\n\ndetail how to employ ChatGPT effectively for data exploration, including\n\nboth the freely available version and the subscription-based plus version, which offers enhanced functionalities.",
      "content_length": 1150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "It’s important to note that the responses from ChatGPT will depend on how\n\neffectively you communicate your needs through prompts. This variability\n\nis a part of the learning curve and illustrates the interactive nature of\n\nworking with AI in data exploration. Our goal is to equip you with the knowledge to navigate these tools confidently and to begin making data-\n\ndriven decisions.\n\nBusiness problem In e-commerce, effectively analyzing customer feedback is crucial for\n\nidentifying key factors that influence purchasing decisions. This analysis\n\nsupports targeted marketing strategies and helps optimize both the user experience and website design, ultimately enhancing service and product\n\nofferings to customers.\n\nProblem and data domain In this chapter, we will focus exclusively on detailed data exploration using\n\nthe Amazon product review dataset. Our goal is to deeply explore this dataset to unearth insights and discern patterns that can enhance decision-\n\nmaking. We’ll leverage ChatGPT to generate Python code for data manipulation and visualization, providing a hands-on approach to\n\nunderstanding complex data analysis techniques. Additionally, we will explore methods to effectively prompt ChatGPT to deliver tailored insights and code snippets that aid in our exploration tasks.\n\nDataset overview",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "We will work with the Amazon product review dataset, which includes a\n\nbroad range of information reflecting consumer feedback and product evaluations. Key features of this dataset encompass identifiers such as marketplace, customer, review, and product details, as well as product titles,\n\ncategories, ratings, and the textual content of reviews. For this exploration, we’ll concentrate on the review_body and review_headline fields, which\n\nprovide rich textual data for analysis. To streamline our focus and enhance clarity in our findings, we will omit neutral sentiments and focus solely on analyzing positive and negative feedback.\n\nFeatures in the dataset include:\n\nmarketplace (string): The location of the product.\n\ncustomer_id (string): A unique identifier for customers.\n\nreview_id (string): A unique identifier for reviews.\n\nproduct_id (string): A unique identifier for products.\n\nproduct_parent (string): A parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the review.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.\n\nreview_date (string): The date of the review.\n\nsentiments (string): The sentiment of the review (positive or\n\nnegative).",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "This targeted exploration will allow us to perform in-depth sentiment\n\nanalysis, evaluate the impact of product ratings, and delve into customer feedback dynamics. By focusing on these elements, we aim to fully\n\nleverage the dataset to improve strategic decision-making in e-commerce environments.\n\nFeature breakdown With the Amazon product review dataset and a focus on detailed data\n\nexploration, we will outline the following features to guide users through\n\nunderstanding and analyzing customer feedback effectively:\n\n1. Loading the Dataset: We’ll start by importing the dataset into a\n\npandas DataFrame. This is a powerful data manipulation structure in\n\nPython that facilitates convenient data handling.\n\n2. Inspecting the Data: Our initial exploration will involve displaying the first few entries of the DataFrame to get a feel for the data. We’ll review the column names, understand the types of data that each\n\ncolumn contains, and check for any missing values that need to be addressed.\n\n3. Summary Statistics: To grasp the numerical data’s distribution, we’ll\n\ncompute summary statistics, including mean, median, minimum, and\n\nmaximum values and quartiles. This step helps in understanding the\n\ncentral tendency and spread of the numerical data.\n\n4. Exploring Categorical Variables: For categorical data such as\n\nmarketplace, product category, and sentiment, we’ll examine the different categories and count the number of entries for each. Visual\n\naids like bar charts will be particularly useful here to illustrate the\n\nfrequency of each category.",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "5. Distribution of Ratings: We will visualize the distribution of star\n\nratings using histograms or bar charts. This visual representation helps\n\nin understanding the general opinion of the reviewers and how ratings\n\nare skewed.\n\n6. Temporal Analysis: By analyzing the review_date column, we’ll\n\nexplore any trends, seasonality, or other temporal patterns in the data.\n\nThis analysis can reveal insights into how sentiments or product popularity change over time.\n\n7. Review Length Analysis: We’ll examine the review_body to\n\nunderstand the amount of information provided in the reviews by\n\ncalculating descriptive statistics for review length, such as mean,\n\nmedian, and maximum lengths. This step provides insights into the\n\ndepth of feedback that customers provide.\n\n8. Correlation Analysis: Lastly, we will investigate the correlation\n\nbetween numeric variables like star ratings, helpful votes, and total\n\nvotes, using correlation matrices or scatter plots. This analysis helps in\n\nidentifying potential relationships between different quantitative aspects of the data.\n\nBy systematically breaking down these features, we will thoroughly\n\nunderstand the dataset, uncovering insights that can enhance decision-\n\nmaking and strategy formulation in e-commerce contexts.\n\nPrompting strategy To utilize ChatGPT effectively for data exploration of the Amazon product\n\nreview dataset, we need to establish clear prompting strategies tailored to\n\ngenerate Python code and data insights. Here’s how we can approach this.",
      "content_length": 1516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Strategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 – Task: The specific goal is to explore the Amazon product review\n\ndataset thoroughly using various statistical and visualization techniques.\n\n1.2 – Actions: The key steps in exploring this dataset include:\n\nData Loading: Load the dataset into a pandas DataFrame.\n\nData Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\nStatistical Summaries: Calculate summary statistics for numerical\n\ndata.\n\nCategorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\nRating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.\n\nTemporal Trends: Examine trends over time from the review dates.\n\nReview Text Analysis: Analyze the length and sentiment of review texts.\n\nCorrelation Study: Assess correlations between numerical variables.\n\n1.3 – Guidelines: We will provide the following guidelines to ChatGPT in\n\nour prompt:\n\nCode should be compatible with a Jupyter Notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method\n\nused in the code.",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Strategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 – Persona: Assume the persona of a data analyst seeking to uncover\n\nactionable insights from the Amazon product review dataset.\n\n2.2 – Instructions: Request ChatGPT to generate code for each specific\n\nanalysis, proceeding sequentially and waiting for user validation before moving to the next task.\n\n2.3 – Context: Given that the focus is on sentiment analysis using the\n\nAmazon product review dataset, ChatGPT is not aware of the dataset and its\n\ncharacteristics, so additional context will be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy 3.1 – Learn:\n\nHighlight the need to understand data through various analytical\n\ntechniques, from basic statistics to complex correlations and temporal\n\nanalysis.\n\n3.2 – Improvise:\n\nAdapt the analysis based on initial findings. For instance, if certain\n\ncategories of products show unusual trends, deepen the analysis in these areas.",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "3.3 – Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4 – Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and\n\nvalidity. This is used throughout the chapter.\n\nData exploration of the Amazon review dataset using the free version of ChatGPT ChatGPT premium version has a code interpreter, but first, we will use the\n\nfree version of ChatGPT. We will craft our initial prompt carefully to\n\ninclude all the features, but we will instruct it to wait for user feedback after\n\nproviding code for each feature.\n\nFeature 1: Loading the dataset Let’s craft our initial prompt to load the dataset.\n\n[Prompt]\n\nI want to explore the Amazon product review dataset thoroughly using\n\nvarious statistical and visualization techniques (TAG 1.1), which consists of the following fields (PIC 2.3):",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "marketplace (string): The location of the product.\n\ncustomer_id (string): A unique identifier for customers.\n\nreview_id (string): A unique identifier for reviews.\n\nproduct_id (string): A unique identifier for products.\n\nproduct_parent (string): A parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the\n\nreview.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.\n\nreview_date (string): The date of the review.\n\nsentiments (string): The sentiment of the review (positive or negative).\n\nThis should consist of the following steps (TAG 1.2):\n\n1. Data Loading: Load the dataset into a pandas DataFrame.\n\n2. Data Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\n3. Statistical Summaries: Calculate summary statistics for numerical\n\ndata.\n\n4. Categorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\n5. Rating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "6. Temporal Trends: Examine trends over time from the review dates.\n\n7. Review Text Analysis: Analyse the length and sentiment of review\n\ntexts.\n\n8. Correlation Study: Assess correlations between numerical variables.\n\nI am a data analyst seeking to uncover actionable insights (PIC 2.1), so\n\nplease provide detailed code and strictly follow the below instructions\n\n(TAG 1.3):\n\nCode should be compatible with a Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will then be copied into\n\nthe text block of the notebook in detail for each method used in the\n\ncode before it is provided. Also note that the dataset is placed in\n\nGoogle Drive at this path: ./Chapter05/amazon_product_review_data.csv. So, add the code to\n\nmount the drive and load the dataset (PIC 2.2).\n\nProvide one step at a time and wait for user feedback (PIC 2.2).\n\n[End of prompt]\n\nYou should receive a response similar to below, where ChatGPT has\n\nprovided us with the code for Step 1 – Data Loading, along with detailed\n\ncomments and explanations.\n\n[Prompt response]\n\nSure, let’s start with the first step: loading the dataset into a pandas\n\nDataFrame from Google Drive. Here’s how we can proceed:\n\nStep 1: Data Loading",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "To load the dataset from Google Drive into a pandas DataFrame, we’ll\n\nfollow these steps:\n\nMount Google Drive: Connect to your Google Drive where the\n\ndataset is located.\n\nLoad the Dataset: Read the dataset file into a pandas DataFrame.\n\nDetailed Code Explanation:\n\n1. Mount Google Drive: This step allows us to access files stored in\n\nGoogle Drive directly from a Google Colab or Jupyter Notebook\n\nenvironment:\n\nFrom the google.colab import drive: Import the drive module\n\nfrom the google.colab package.\n\ndrive.mount (\"/content/drive\"): Mounts your Google Drive at\n\nthe specified path (/content/drive in this case). Follow the instructions to authorize and obtain an access code.\n\n2. Load the Dataset:\n\nimport pandas as pd: Import the pandas library as pd for data\n\nmanipulation.\n\nfile_path = '/content/drive/My Drive/Chapter05/amazon_product_review_data.csv': Define the\n\npath to your CSV file in Google Drive.\n\ndf = pd.read_csv(file_path): Use pd.read_csv() to read the CSV file into a DataFrame named df.\n\nHere’s the complete code snippet:\n\n# Mount Google Drive to access the dataset from google.colab import drive drive.mount('/content/drive') # Import pandas for data manipulation",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "import pandas as pd # Define the file path in your Google Drive file_path = '/content/drive/My Drive/Chapter05/amazon_product_r # Load the dataset into a pandas DataFrame df = pd.read_csv(file_path) # Display the first few rows of the dataframe to verify the dat df.head()\n\nUser Feedback:\n\nPlease execute this code in your Jupyter Notebook environment after\n\nreplacing file_path with the correct path to your dataset. Let me know if\n\nyou encounter any issues or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in Google Colab for every step. The code\n\nloads the dataset from Google Drive. It will also request permission to\n\naccess Google Drive.\n\nPlease refer to the notebook “Data Exploration with ChatGPT.ipynb” for\n\nthe executable code snippets, or copy the code from the prompt response.\n\nYou should see a message saying “Permit this Notebook to access Google Drive.”\n\nThe code runs perfectly fine without applying any modifications or any\n\nerrors and displays the first few rows of the dataset.\n\nYou should see the content of the Notebook and customer reviews.\n\nFeature 2: Inspecting the data We will now instruct ChatGPT to proceed to the next step:",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nGreat! Let’s move on to the next step:\n\nStep 2: Data Inspection\n\nIn this step, we will inspect the loaded dataset to:\n\nCheck for missing data.\n\nUnderstand the data types of each column.\n\nInspect the first few entries to get a sense of the data structure.\n\nDetailed Code Explanation:\n\n1. Check for Missing Data:\n\ndf.info(): This method provides a concise summary of the\n\nDataFrame, including the number of non-null entries in each\n\ncolumn. It also shows the data types of each column, which\n\nhelps us understand how to manipulate the data.\n\n2. Understand Data Types:\n\nBy examining the output of df.info(), we can identify if any columns have incorrect data types (e.g., numerical columns mistakenly stored as strings).\n\n3. Inspect the First Few Entries:\n\ndf.head(): This method displays the first few rows of the\n\nDataFrame. It allows us to visually inspect the columns, their",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "values, and get an initial understanding of what the dataset\n\ncontains.\n\nHere’s the code for data inspection:\n\n# Check for missing data and data types df.info() # Display the first few rows of the dataframe df.head()\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. After running this\n\nstep, you should be able to see a summary of the dataset and the first few\n\nrows. Let me know if everything looks okay or if you have any specific questions!\n\n[End of response]\n\nWe evaluate the output from ChatGPT and execute the code (LIFE 3.4) in\n\nthe notebook, and it displays the information.\n\nWe will utilize the prompting strategy to provide the output for ChatGPT\n\n(LIFE 3.3) to interpret:\n\n[Prompt]\n\n<class 'pandas.core.frame.DataFrame'> RangeIndex: 500 entries, 0 to 499 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 market_place 500 non-null object 1 customer_id 500 non-null object 2 review_id 500 non-null object 3 product_id 500 non-null object 4 product_parent 500 non-null object",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "5 product_title 500 non-null object 6 product_category 500 non-null object 7 star_rating 500 non-null int64 8 helpful_votes 500 non-null int64 9 total_votes 500 non-null int64 10 verified_purchase 500 non-null object 11 review_headline 500 non-null object 12 review_body 500 non-null object 13 review_date 500 non-null object 14 sentiments 500 non-null object dtypes: int64(3), object(12) memory usage: 58.7+ KB market_place customer_id review_id product_id produ 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" \"2 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" \" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" \"\n\n[End of prompt]\n\nWe should receive a response like below, where ChatGPT provides an\n\nexplanation and findings:\n\n[Prompt response]\n\nIt seems like the dataset has been successfully loaded, and here’s what we can infer from the output of df.info() and df.head():\n\nData Inspection Findings:\n\n1. Basic Information:\n\nThe dataset contains 500 entries (rows) and 15 columns.\n\nThere are both numerical (int64) and categorical (object) data types.\n\nAll columns have 500 non-null entries, indicating there are no missing values.",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "2. Column Details:\n\nmarket_place, customer_id, review_id, product_id,\n\nproduct_parent, product_title, product_category, verified_purchase, review_headline, review_body,\n\nreview_date, and sentiments are all identified as object (string) types, which aligns with their expected content based on their\n\ndescriptions.\n\nstar_rating, helpful_votes, and total_votes are correctly\n\nidentified as int64 (integer) types.\n\n3. Initial Data Preview:\n\nThe df.head() output shows the first few rows of the dataset,\n\ngiving us a glimpse into the structure and content of each column. We can see examples of the market_place, customer_id, star_rating, helpful_votes, total_votes,\n\nreview_headline, review_body, review_date, and sentiments columns.\n\nNext Steps:\n\nBased on this inspection:\n\nThe dataset appears to be clean with no missing values.\n\nWe have a clear understanding of the columns and their data types.\n\nNext, we can proceed with Step 3: Statistical Summaries to calculate summary statistics for the numerical data (star_rating, helpful_votes,\n\nand total_votes). Would you like to continue with that? Let me know if there’s anything specific you’d like to explore further or if we should\n\nproceed to the next step!\n\n[End of response]",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Feature 3: Summary statistics As there are no missing or null values, we will now instruct ChatGPT to\n\nproceed to the next step to display summary statistics:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code\n\nto display summary statistics:\n\n[Prompt response]\n\nGreat! Let’s move on to Step 3:\n\nStep 3: Statistical Summaries\n\nIn this step, we will calculate summary statistics for the numerical columns\n\nin our dataset. Specifically, we will focus on:\n\nstar_rating: Rating of the product on a scale of 1 to 5.\n\nhelpful_votes: Number of helpful votes received for the review.\n\ntotal_votes: Total number of votes received for the review.\n\nDetailed Code Explanation:\n\n1. Use of describe():\n\ndf.describe() computes summary statistics for numerical columns. It provides:\n\nCount: Number of non-null values.\n\nMean: Average value.\n\nStd: Standard deviation.\n\nMin: Minimum value.",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "25%, 50%, and 75%: Percentiles.\n\nMax: Maximum value.\n\n2. Selecting Specific Columns:\n\nWe use df[['star_rating', 'helpful_votes',\n\n'total_votes']].describe() to limit the output to only the numerical columns we are interested in (star_rating,\n\nhelpful_votes, and total_votes).\n\nHere’s the code to calculate and display summary statistics:\n\n# Calculate summary statistics for numerical columns summary_stats = df[['star_rating', 'helpful_votes', 'total_vote # Display the summary statistics print(summary_stats)\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\nstatistical insights into the distribution and central tendencies of the\n\nnumerical variables in your dataset. Let me know if you have any questions\n\nor if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe execute the code, which shows the summary statistics, and we provide\n\nthe output to ChatGPT for interpretation and to understand the stats:\n\n[Prompt]\n\nstar_rating helpful_votes total_votes count 500.000000 500.000000 500.000000 mean 0.796000 0.764000 1.068000 std 0.403373 2.843321 3.698477 min 0.000000 0.000000 0.000000",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "25% 1.000000 0.000000 0.000000 50% 1.000000 0.000000 0.000000 75% 1.000000 1.000000 1.000000 max 1.000000 36.000000 47.000000\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides an\n\nexplanation of the summary statistics:\n\n[Prompt response]\n\nBased on the summary statistics for the numerical columns (star_rating, helpful_votes, and total_votes), here are the insights we can gather:\n\nSummary Statistics:\n\n1. star_rating:\n\nMean: 0.796\n\nStandard Deviation: 0.403\n\nMin: 0 (indicating the lowest rating)\n\n25th Percentile (Q1): 1\n\nMedian (50th Percentile): 1\n\n75th Percentile (Q3): 1\n\nMax: 1 (indicating the highest rating)\n\nInterpretation: The star_rating column primarily contains a single value of 1 (which typically represents positive reviews in\n\nbinary sentiment data), indicating that most reviews are positive.\n\nThis is supported by the mean being close to 1 and a low\n\nstandard deviation, suggesting little variability in ratings.\n\n2. helpful_votes:\n\nMean: 0.764",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Standard Deviation: 2.843\n\nMin: 0\n\n25th Percentile (Q1): 0\n\nMedian (50th Percentile): 0\n\n75th Percentile (Q3): 1\n\nMax: 36\n\nInterpretation: The helpful_votes column has a mean of\n\n0.764, indicating that on average, reviews receive less than one\n\nhelpful vote. The standard deviation of 2.843 suggests variability\n\nin the number of helpful votes received, with some reviews\n\nreceiving as many as 36 helpful votes.\n\n3. total_votes:\n\nMean: 1.068\n\nStandard Deviation: 3.698\n\nMin: 0\n\n25th Percentile (Q1): 0\n\nMedian (50th Percentile): 0\n\n75th Percentile (Q3): 1\n\nMax: 47\n\nInterpretation: The total_votes column has a mean of 1.068, indicating that on average, reviews receive slightly more than\n\none total vote (helpful or unhelpful). The standard deviation of 3.698 suggests variability in the total number of votes received,\n\nwith some reviews receiving as many as 47 votes in total.\n\nNext Steps:\n\nBased on these statistics, we see that the dataset predominantly consists of\n\npositive reviews (a star_rating of 1). The helpful_votes and",
      "content_length": 1032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "total_votes metrics show varying levels of engagement with the reviews.\n\nNext, we can move on to Step 4: Categorical Analysis, where we will analyze the categorical variables (market_place, product_category, verified_purchase, and sentiments) using counts and visualizations.\n\nWould you like to proceed with that? Let me know if you have any\n\nquestions or if you’re ready for the next step!\n\n[End of response]\n\nFeature 4: Exploring categorical variables Now, we will proceed to explore the categorical variables.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code\n\nfor the visualization of the categorical variables:\n\n[Prompt response]\n\nGreat! Let’s proceed to Step 4:\n\nStep 4: Categorical Analysis\n\nIn this step, we will analyze the categorical variables in our dataset using\n\ncounts and visualizations. The categorical variables we’ll focus on are:\n\nmarket_place: Location of the product.\n\nproduct_category: Category of the product.",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "verified_purchase: Whether the purchase was verified (Y/N).\n\nsentiments: Sentiment of the review (positive or negative).\n\nDetailed Code Explanation:\n\n1. Counting Unique Values:\n\ndf['column_name'].value_counts(): This method counts the\n\noccurrences of each unique value in a categorical column.\n\n2. Visualizing Categorical Data:\n\nWe will use bar charts (from the matplotlib library) to visualize the distribution of categorical variables.\n\nHere’s the code to perform categorical analysis:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Set up the matplotlib figure and axes fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10)) axes = axes.flatten() # Plot for 'market_place' sns.countplot(x='market_place', data=df, ax=axes[0]) axes[0].set_title('Distribution of Products by Market Place') # Plot for 'product_category' sns.countplot(x='product_category', data=df, ax=axes[1]) axes[1].set_title('Distribution of Products by Category') axes[1].tick_params(axis='x', rotation=90) # Plot for 'verified_purchase' sns.countplot(x='verified_purchase', data=df, ax=axes[2]) axes[2].set_title('Distribution of Verified Purchases') # Plot for 'sentiments' sns.countplot(x='sentiments', data=df, ax=axes[3]) axes[3].set_title('Distribution of Sentiments') # Adjust layout plt.tight_layout() plt.show()\n\nExplanation of the Visualizations:",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Market Place: Shows the distribution of products across different marketplaces (market_place).\n\nProduct Category: Displays the distribution of products across various categories (product_category).\n\nVerified Purchase: Indicates how many reviews are from verified purchases (verified_purchase).\n\nSentiments: Illustrates the distribution of review sentiments (sentiments), whether they are positive or negative.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment to visualize and\n\nanalyze the categorical variables in your dataset. This will provide insights\n\ninto the distribution and characteristics of different aspects related to the\n\nreviews and products. Let me know if you have any questions or if you’re\n\nready to move on to the next step!\n\n[End of response]\n\nWe execute the code, which runs perfectly fine, and then visualize the\n\nvariables.",
      "content_length": 869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Figure 11.1: Categorical variables – visualization\n\nWe can see that the reviews are all from a single marketplace and have\n\nequal number of reviews of all product categories, but there is an imbalance\n\nof positive and negative reviews as well as the distribution of verified\n\npurchases.\n\nFeature 5: Rating distribution We will move on to the rating distribution and instruct ChatGPT to move on\n\nto the next step:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "[Prompt response]\n\nGreat! Let’s proceed to Step 5:\n\nStep 5: Rating Distribution\n\nIn this step, we will create histograms or bar charts to visualize the distribution of star ratings (star_rating). This will help us understand how\n\nratings are distributed among the reviews.\n\nDetailed Code Explanation:\n\n1. Histogram or Bar Chart:\n\nWe will use sns.countplot() from the Seaborn library to create a bar chart. This is suitable for visualizing the count of each\n\nrating value (1 to 5).\n\n2. Visualization Parameters:\n\nAdjustments such as setting the figure size (figsize) and title\n\n(set_title) are done to enhance the clarity and readability of the plot.\n\nHere’s the code to visualize the rating distribution:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Set up the matplotlib figure plt.figure(figsize=(8, 6)) # Plotting the distribution of star ratings sns.countplot(x='star_rating', data=df) plt.title('Distribution of Star Ratings') plt.xlabel('Star Rating') plt.ylabel('Count') # Show plot plt.show()\n\nExplanation of the Visualization:",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "The sns.countplot() function is used to create a bar chart of the\n\nstar_rating column.\n\nThe x-axis represents the star ratings (1 to 5), and the y-axis represents\n\nthe count of reviews with each rating.\n\nThe title, x-label (Star Rating), and y-label (Count) are added to\n\nprovide context and clarity to the plot.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will display a\n\nvisual representation of how star ratings are distributed among the reviews\n\nin your dataset. This step will provide insights into the overall sentiment\n\nexpressed through ratings. Let me know if you have any questions or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will execute the code, which produces the ratings graph.",
      "content_length": 751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Figure 11.2: Ratings distribution\n\nBased on these results, there are majorly positive ratings. We will move on\n\nto the next feature.\n\nFeature 6: Temporal trends Now, let’s understand the temporal trends in our dataset for the review date\n\ncolumn. Our dataset doesn’t have much variation, but this will help us\n\nunderstand how to use ChatGPT for it:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "[Prompt response]\n\nGreat! Let’s move on to Step 6:\n\nStep 6: Temporal Trends\n\nIn this step, we will examine temporal trends from the review_date column\n\nto understand how the reviews are distributed over time. This will help us\n\nidentify any patterns or trends in the review activity.\n\nDetailed Code Explanation:\n\n1. Convert review_date to datetime:\n\nEnsure that the review_date column is in the datetime format for proper temporal analysis.\n\n2. Plotting Temporal Trends:\n\nUse sns.lineplot() or sns.histplot() to visualize the\n\ndistribution of reviews over time.\n\nDepending on the analysis goal, you can choose to plot the count\n\nof reviews per month, per year, or any other relevant time\n\ninterval.\n\nHere’s the code to analyze temporal trends:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Convert 'review_date' to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Set up the matplotlib figure plt.figure(figsize=(12, 6)) # Plotting the count of reviews over time sns.histplot(df['review_date'], bins=30, kde=False, color='blue plt.title('Distribution of Reviews Over Time') plt.xlabel('Review Date') plt.ylabel('Count of Reviews') # Show plot",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "plt.xticks(rotation=45) plt.tight_layout() plt.show()\n\nExplanation of the Visualization:\n\nDatetime Conversion: pd.to_datetime() function is used to convert\n\nthe review_date column from a string to the datetime format, ensuring accurate temporal analysis.\n\nHistogram Plot: sns.histplot() is used to create a histogram of\n\nreview counts over time (review_date). This helps in visualizing the distribution and frequency of reviews across different dates.\n\nAdjustments: Adjustments such as setting the figure size (figsize),\n\ntitle (title), x-label (xlabel), y-label (ylabel), and rotating x-axis ticks (xticks(rotation=45)) are made to improve the clarity and\n\nreadability of the plot.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\ninsights into how reviews are distributed over time, allowing you to observe\n\nany trends or patterns in review activity. Let me know if you have any\n\nquestions or if you’re ready to proceed to the next step!\n\n[End of response]\n\nWe will execute the code, which produces a plot that shows the distribution\n\nof reviews over time.",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Figure 11.3: Temporal trend\n\nWe can see that the reviews are shown only for two dates, so let’s move to\n\nthe next feature.\n\nFeature 7: Review length analysis [Prompt]\n\nThe reviews are shown for just two dates, 10-2013 and 08-2015.\n\n[End of prompt]\n\nWe should receive a response like below where ChatGPT provides the code,\n\nexplains the significance of the review dates, and proceeds with the next\n\nstep of review length analysis.\n\n[Prompt response]\n\nIf the reviews in your dataset are concentrated on just two dates (October\n\n2013 and August 2015), it suggests that the dataset may not have a wide",
      "content_length": 597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "temporal spread of reviews. This information is crucial for understanding\n\nthe context of the data and its limitations in temporal analysis.\n\nGiven this observation, we can still proceed to analyze the review text and\n\nsentiment in Step 7, as planned. This step will focus on examining the\n\nlength and sentiment of the review texts, which can provide additional\n\ninsights beyond temporal trends.\n\nStep 7: Review Text Analysis\n\nIn this step, we will analyze the review text (review_body) to:\n\nCalculate the length of each review.\n\nDetermine the sentiment of each review using the provided sentiments column.\n\nDetailed Code Explanation:\n\n1. Review Length Calculation:\n\nAdd a new column review_length to the DataFrame to store the\n\ncharacter count of each review (review_body).\n\n2. Sentiment Analysis:\n\nUse the sentiments column to categorize reviews as positive or\n\nnegative based on the provided labels.\n\nHere’s the code to perform review the text analysis:\n\n# Calculate the length of each review text df['review_length'] = df['review_body'].apply(len) # Count the number of reviews classified as positive and negati sentiment_counts = df['sentiments'].value_counts() # Display the sentiment counts print(\"Sentiment Counts:\") print(sentiment_counts) # Calculate the average length of reviews average_review_length = df['review_length'].mean()",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "print(f\"\\nAverage Review Length: {average_review_length:.2f} ch # Display the first few rows to verify the changes df.head()\n\nExplanation of the Code:\n\nReview Length Calculation: df['review_body'].apply(len)\n\ncomputes the length of each review in characters and stores it in a new column review_length.\n\nSentiment Analysis: df['sentiments'].value_counts() counts the\n\noccurrences of each sentiment category (positive or negative) in the sentiments column.\n\nAverage Review Length: df['review_length'].mean() calculates the average length of reviews in the dataset.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide\n\ninsights into the length and sentiment distribution of the review texts in\n\nyour dataset. Let me know if you have any questions or if you’re ready to\n\nproceed to the next step after reviewing the outputs!\n\n[End of response]\n\nWe execute the code in Jupyter Colab, which works fine, so we provide the\n\noutput to ChatGPT to interpret it:\n\n[Prompt]\n\nSentiment Counts: sentiments positive 398 negative 102 Name: count, dtype: int64",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Average Review Length: 243.75 characters market_place customer_id review_id product_id produ 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" \"2 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" \" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" \"\n\n[End of prompt]\n\nWe should receive a response from ChatGPT explaining the results:\n\n[Prompt response]\n\nIt seems like the sentiment analysis and review length calculations have\n\nbeen successfully performed. Here are the insights from the analysis:\n\nSentiment Analysis:\n\nSentiment Counts:\n\nPositive reviews: 398\n\nNegative reviews: 102\n\nReview Length Analysis:\n\nAverage Review Length: 243.75 characters\n\nExplanation:\n\nSentiment Counts: The dataset contains predominantly positive\n\nreviews, with 398 positive reviews and 102 negative reviews.\n\nAverage Review Length: The average length of a review in the\n\ndataset is approximately 243.75 characters.\n\nExample Data Inspection:\n\nThe example snippet you provided shows a sample of the dataset, where we\n\ncan see details such as marketplace, customer ID, review ID, product",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "details, star rating, review text, review date, sentiment, and review length\n\nfor the first few entries.\n\nNext Steps:\n\nBased on this analysis, we have insights into the distribution of sentiments\n\nand the average length of reviews. If there are specific further analyses or\n\nvisualizations you’d like to explore, such as a correlation study or any other\n\naspect of the dataset, please let me know! We can proceed accordingly.\n\n[End of response]\n\nFeature 8: Correlation study We will move on to the last feature, which is a correlation study.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like below:\n\n[Prompt response]\n\nCertainly! Let’s proceed to Step 8:\n\nStep 8: Correlation Study\n\nIn this step, we will assess the correlations between numerical variables in the dataset. Specifically, we will examine how star_rating, helpful_votes, and total_votes correlate with each other. This analysis\n\nwill help us understand any relationships or dependencies between these\n\nvariables.",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Detailed Code Explanation:\n\n1. Correlation Calculation:\n\nUse the .corr() method on the DataFrame to compute the\n\ncorrelation coefficients between numerical columns.\n\n2. Visualization of Correlation:\n\nWe can visualize the correlations using a heatmap to provide a\n\nclear and intuitive representation of the relationships.\n\nHere’s the code to perform the correlation study:\n\nimport matplotlib.pyplot as plt import seaborn as sns # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Plotting the correlation heatmap plt.figure(figsize=(8, 6)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vm plt.title('Correlation Matrix of Numerical Variables') plt.show()\n\nExplanation of the Code:\n\nCorrelation Matrix: df[['star_rating', 'helpful_votes', 'total_votes']].corr() computes the correlation coefficients\n\nbetween star_rating, helpful_votes, and total_votes.\n\nHeatmap Visualization: sns.heatmap() from the Seaborn library is\n\nused to create a heatmap of the correlation matrix. The annot=True parameter adds numeric annotations to the heatmap cells for clarity. The cmap='coolwarm' parameter sets the color scheme for the\n\nheatmap, where colors range from blue (negative correlation) to red\n\n(positive correlation).",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Interpretation:\n\nThe heatmap will display correlations ranging from -1 to 1, where:\n\nValues closer to 1 indicate a strong positive correlation.\n\nValues closer to -1 indicate a strong negative correlation.\n\nValues close to 0 indicate no significant correlation.\n\nUser Feedback:\n\nExecute this code in your Jupyter Notebook environment. It will provide insights into how star_rating, helpful_votes, and\n\ntotal_votes are related within your dataset. Let me know if you have any questions or if you’re ready to proceed to the next step after\n\nreviewing the correlation heatmap!\n\n[End of response]\n\nWe will execute the code, which plots the correlation matrix and prints the\n\nresults.",
      "content_length": 678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Figure 11.4: Correlation matrix\n\nWe can see that helpful votes and total votes are strongly correlated but the\n\nrating is not.\n\nThis concludes data exploration with ChatGPT-3.5; now, let’s see how to\n\nuse ChatGPT-4o.\n\nData exploration of the Amazon review dataset using ChatGPT-4o ChatGPT’s premium version has a code interpreter, which is accessible\n\nusing both ChatGPT-4o and ChatGPT 4. For this section, we will use GPT-",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "4o, and this time, we will get the code in a single step.\n\nChatGPT has access to several models, at present, you can use GPT-3.5,\n\nGPT-4, and GPT-4o.\n\nLet’s craft our prompt.\n\n[Prompt]\n\nI want to explore the Amazon product review dataset thoroughly using\n\nvarious statistical and visualization techniques (TAG 1.1) for the attached dataset (PIC 2.3). It should consist of the following steps (TAG 1.2):\n\n1. Data Loading: Load the dataset into a pandas DataFrame.\n\n2. Data Inspection: Check for missing data, understand data types, and\n\ninspect the first few entries.\n\n3. Statistical Summaries: Calculate the summary statistics for numerical\n\ndata.\n\n4. Categorical Analysis: Analyze categorical variables using counts and\n\nvisualizations.\n\n5. Rating Distribution: Create histograms or bar charts to visualize the\n\ndistribution of star ratings.\n\n6. Temporal Trends: Examine trends over time from the review dates.\n\n7. Review Text Analysis: Analyze the length and sentiment of review\n\ntexts.\n\n8. Correlation Study: Assess correlations between numerical variables\n\nI am a data analyst seeking to uncover actionable insights (PIC 2.1), so\n\nplease provide detailed code.\n\n[End of prompt]",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "We should receive a response similar to below where ChatGPT performs all\n\nthe necessary steps, performs a data exploration, and produces the relevant\n\ncharts:\n\n[Prompt response]\n\nAmazon Product Review Dataset Analysis\n\nWe’ll follow the steps you outlined to explore the dataset thoroughly. Let’s proceed step by step:\n\nStep 1: Data Loading\n\nWe have loaded the dataset into a pandas DataFrame.\n\nStep 2: Data Inspection\n\nWe checked for missing data, understood data types, and inspected the\n\nfirst few entries. The dataset consists of 500 entries and 15 columns\n\nwith no missing data. Here are the first few entries:\n\nmarket_place customer_id review_id product_id 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" product_title produ 0 \"The Cravings Place Chocolate Chunk Cookie Mix... 1 \"Mauna Loa Macadamias, 11 Ounce Packages\" 2 \"Organic Matcha Green Tea Powder - 100% Pure M... 3 \"15oz Raspberry Lyons Designer Dessert Syrup S... 4 \"Stride Spark Kinetic Fruit Sugar Free Gum, 14... star_rating helpful_votes total_votes verified_purchas 0 1 0 0 1 \\t(Y 1 1 0 0 1 \\t(Y 2 1 0 0 0 \\t(N 3 1 0 0 1 \\t(Y 4 1 0 0 1 \\t(Y",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "review_headline \\ 0 \"Using these for years - love them.\" 1 \"Wonderful\" 2 \"Five Stars\" 3 \"Five Stars\" 4 \"Five Stars\" review_body review 0 \"As a family allergic to wheat, dairy, eggs, n... 2015- 1 \"My favorite nut. Creamy, crunchy, salty, and ... 2015- 2 \"This green tea tastes so good! My girlfriend ... 2015- 3 \"I love Melissa's brand but this is a great se... 2015- 4 \"good\" 2015-\n\nStep 3: Statistical Summaries\n\nLet’s calculate summary statistics for the numerical data. You should\n\nsee a summary of your data, including stats on the mean, standard\n\ndeviation, and more.\n\nStep 4: Categorical Analysis\n\nWe’ll analyze the categorical variables using counts and visualizations.\n\nStep 5: Rating Distribution\n\nWe’ll create histograms or bar charts to visualize the distribution of\n\nstar ratings.\n\nStep 6: Temporal Trends\n\nWe’ll examine trends over time from the review dates.\n\nStep 7: Review Text Analysis\n\nWe’ll analyze the length and sentiment of review texts.\n\nStep 8: Correlation Study\n\nWe’ll assess correlations between numerical variables.",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Let’s start implementing each of these steps in Python code.\n\nDetailed Analysis of the Amazon Product Review Dataset\n\nWe have performed a detailed analysis as outlined. Below are the key\n\nfindings and visualizations for each step.\n\nStep 3: Statistical Summaries\n\nWe calculated summary statistics for the numerical data. Here are some key\n\nstatistics:\n\nStar Rating: Mean = 0.796, Standard Deviation = 0.403, Min = 0, and\n\nMax = 1\n\nHelpful Votes: Mean = 0.764, Standard Deviation = 2.843, Min = 0,\n\nand Max = 36\n\nTotal Votes: Mean = 1.068, Standard Deviation = 3.698, Min = 0, and\n\nMax = 47\n\nStep 4: Categorical Analysis\n\nThe top categories by review count are as follows:\n\nGrocery: 100 reviews\n\nDigital_Ebook_Purchase: 100 reviews\n\nJewelry: 100 reviews\n\nPC: 100 reviews\n\nBooks: 100 reviews\n\nStep 5: Rating Distribution\n\nWe visualized the distribution of the star ratings:",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Figure 11.5: Distribution of the star ratings\n\nStep 6: Temporal Trends\n\nWe examined the number of reviews per month:\n\nFigure 11.6: Number of reviews per month\n\nStep 7: Review Text Analysis",
      "content_length": 188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "We analyzed the length and sentiment of the review texts:\n\nDistribution of the Review Lengths:\n\nFigure 11.7: Distribution of the review lengths\n\nDistribution of the Sentiments:",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Figure 11.8: Distribution of the sentiments\n\nPositive Reviews: 398\n\nNegative Reviews: 102\n\nStep 8: Correlation Study\n\nWe assessed the correlations between the numerical variables and\n\nvisualized the correlation matrix:\n\nFigure 11.9: Correlation matrix\n\nConclusion\n\nThis analysis provided valuable insights into the dataset, including the\n\ndistribution of star ratings, trends over time, and the sentiment of reviews.",
      "content_length": 416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "We also explored correlations between numerical variables. These insights\n\ncan help in understanding customer feedback and improving product\n\nofferings.\n\nIf you need any further analysis or have specific questions, feel free to ask!\n\n[End of response]\n\nAs you can see, GPT-4o has successfully performed data exploration by just\n\nuploading the CSV and providing the steps or actions to perform.\n\nAssignment Investigate how different products or categories perform in terms of ratings,\n\nhelpful votes, and review length.\n\nChallenge Use natural language processing (NLP) techniques to analyze the content of the reviews. Extract common themes, keywords, and phrases that are\n\nfrequently mentioned.\n\nSummary In this chapter, we delved into comprehensive data analysis using Python\n\nand pandas, leveraging the Amazon product review dataset. The journey\n\nbegan with data loading and inspection, ensuring the dataset was properly\n\nformatted and free of missing values. You were guided through each step\n\nwith detailed explanations and code samples suitable for Jupyter Notebooks\n\naimed at empowering data analysts to uncover actionable insights\n\neffectively.",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "We started by calculating statistical summaries for numerical data, revealing\n\nthat the dataset predominantly consisted of positive reviews. Categorical\n\nanalysis followed, where we explored distributions across different\n\nmarketplaces, product categories, verified purchases, and sentiments.\n\nVisualizations, including histograms and bar charts, provided clear\n\nrepresentations of star rating distributions, emphasizing the predominance\n\nof positive feedback.\n\nTemporal trends analysis uncovered a concentrated spread of reviews,\n\nprimarily in October 2013 and August 2015, offering insights into review activity over time. We then conducted review text analysis, calculating\n\nreview lengths and assessing sentiment counts to understand the dataset’s\n\ncontent more deeply. Finally, a correlation study examined relationships\n\nbetween star ratings and review engagement metrics, like helpful and total\n\nvotes, offering insights into how these factors interact within the dataset.\n\nIn the next chapter, we will learn how to use ChatGPT to build a\n\nclassification model using the same dataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "12\n\nBuilding a Classiﬁcation Model with ChatGPT\n\nIntroduction Building on the foundation set in the previous chapter, where we used\n\nChatGPT for data exploration with Amazon book reviews, Chapter 12\n\ndelves deeper into the realm of supervised learning, with a focus on\n\nclassification. Here, we continue to leverage ChatGPT, applying its\n\ncapabilities to enhance our understanding and application of supervised\n\nlearning techniques in the context of customer reviews.\n\nIn the realm of e-commerce, customer feedback plays a pivotal role in\n\nshaping business strategies and product enhancements. As Bill Gates aptly\n\nstated, “Your most dissatisfied customers are your greatest source of\n\nlearning.” Customer sentiments are often buried within the extensive pool\n\nof product reviews. However, manually scrutinizing this ocean of reviews, which includes various attributes such as product ID, title, text, rating, and\n\nhelpful votes, is an arduous and often unmanageable task.\n\nIn this chapter, we concentrate on classifying customer reviews into two distinct groups: positive and negative. We will utilize the insights gained\n\nfrom ChatGPT in processing and analyzing customer review data.",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Our main goal is to show how ChatGPT can simplify the journey of\n\nmachine learning, making it more accessible and less intimidating,\n\nespecially when dealing with intricate topics such as classification in\n\nsupervised learning. We will explore how ChatGPT can break down complex concepts into more digestible parts, provide explanations, and\n\neven generate code snippets, thereby reducing the learning curve for\n\nbeginners or those new to the field.\n\nBy the end of this chapter, you will have a solid understanding of supervised learning and its application in sentiment analysis, along with an\n\nappreciation of how AI tools like ChatGPT can be valuable allies in\n\nlearning and applying machine learning techniques effectively.\n\nBusiness problem In an e-commerce project, understanding customer feedback helps in\n\nidentifying key factors that influence a customer’s decision to make a\n\npurchase, enabling targeted marketing strategies. Additionally, it allows the optimization of the user experience and website design to increase the\n\nlikelihood of providing improved service and products to customers.\n\nProblem and data domain In this section, we aim to build a classification model for customer review\n\nsentiment analysis using the Amazon product review dataset. Leveraging\n\nChatGPT’s capabilities, we’ll generate Python code to construct a classification model, offering readers a practical approach to working with\n\ndatasets and understanding classification techniques. Additionally, we’ll",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "explore effective prompting techniques to guide ChatGPT in providing\n\ntailored code snippets and insights for data classification tasks.\n\nDataset overview The Amazon product review dataset contains information on various\n\nproducts and their corresponding reviews. By utilizing this dataset, we can\n\nperform various analyses, including sentiment analysis, trend analysis of customer feedback, and product rating analysis. The ultimate goal is to train a classification model capable of accurately classifying reviews into\n\npositive or negative sentiments, enhancing decision-making processes, and improving customer satisfaction in e-commerce platforms and related\n\nindustries.\n\nFeatures in the dataset include:\n\nmarketplace (string): The location of the product.\n\ncustomer_id (string): The unique identifier for customers.\n\nreview_id (string): The unique identifier for reviews.\n\nproduct_id (string): The unique identifier for products.\n\nproduct_parent (string): The parent product identifier.\n\nproduct_title (string): The title of the reviewed product.\n\nproduct_category (string): The category of the product.\n\nstar_rating (int): The rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): The number of helpful votes received for the review.\n\ntotal_votes (int): The total number of votes received for the review.\n\nreview_headline (string): The headline of the review.\n\nreview_body (string): The content of the review.",
      "content_length": 1429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "review_date (string): The date of the review.\n\nSentiments (string): The sentiment of the review (positive or negative).\n\nThe textual data in review_body and review_headline can be particularly\n\nvaluable for natural language processing tasks, including sentiment\n\nanalysis. For simplification purposes, we have excluded the neutral\n\nsentiment category to focus on building a classification model and\n\nprompting techniques.\n\nBreaking the problem down into features Given the Amazon product review dataset and the application of machine\n\nlearning models for sentiment analysis, we will outline the following\n\nfeatures to guide users through building and optimizing models for\n\nsentiment classification:\n\nData preprocessing and feature engineering: Users will start by preprocessing the text data, including tasks such as tokenization,\n\nlowercasing, and removing stop words and punctuation. Additionally,\n\nfeature engineering techniques such as Term Frequency-Inverse\n\nDocument Frequency (TF-IDF) encoding or word embeddings will\n\nbe applied to represent the text data in a format suitable for machine\n\nlearning models.\n\nModel selection and baseline training: Users will select baseline machine learning models such as logistic regression, Naive Bayes, or\n\nsupport vector machines (SVMs) for sentiment classification. The",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "selected model will be trained on the preprocessed data to establish a baseline performance for sentiment analysis.\n\nModel evaluation and interpretation: Users will evaluate the\n\nperformance of trained machine learning models using metrics such as accuracy, precision, recall, and F1-score. Additionally, techniques for\n\ninterpreting model predictions, such as feature importance analysis or\n\nmodel explainability methods, will be explored to gain insights into the\n\nfactors influencing sentiment classification decisions.\n\nHandling imbalanced data: This feature addresses the challenge of\n\nimbalanced class distributions in the dataset by implementing\n\ntechniques such as oversampling, under-sampling, or using class weights during model training. Users will explore methods to mitigate\n\nthe impact of class imbalance on model performance and improve the\n\nclassification accuracy of minority classes.\n\nHyperparameter tuning: Users will learn how to optimize the performance of machine-learning models by tuning hyperparameters\n\nsuch as regularization strength, learning rate, and kernel parameters.\n\nThrough techniques like grid search or random search, users will experiment with different hyperparameter configurations to improve\n\nthe model’s performance on the validation set.\n\nExperimenting with feature representation: Users will explore different methods of representing text data as features for machine\n\nlearning models. This feature focuses on comparing the performance\n\nof models trained with different feature representations, such as bag-\n\nof-words, TF-IDF, or word embeddings, to determine the most effective approach for sentiment classification.\n\nBy following these features, users will gain practical insights into building,\n\nfine-tuning, and optimizing machine learning models for sentiment analysis",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "tasks using the Amazon product review dataset. They will learn how to systematically experiment with different preprocessing techniques, feature\n\nrepresentations, hyperparameter configurations, and class imbalance\n\nhandling strategies to achieve superior performance and accuracy in sentiment classification.\n\nPrompting strategy To effectively utilize ChatGPT for generating code for sentiment analysis\n\nmachine learning tasks, we need to develop a comprehensive prompting\n\nstrategy tailored to the specific features and requirements of sentiment analysis using the Amazon product review dataset.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 – task: The specific task or goal is to build and optimize a machine\n\nlearning model for sentiment analysis using the Amazon product review\n\ndataset.\n\n1.2 – actions: The key steps involved in building and optimizing a machine learning model for sentiment analysis include:\n\nData preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or SVMs.",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "1.3 – guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into\n\nthe text block of the Notebook, in detail for each method used before providing the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 – persona: Adopt the persona of a beginner who needs step-by-step\n\nguidance on building and optimizing machine learning models for\n\nsentiment analysis tasks using the Amazon product review dataset.\n\n2.2 – instructions: Ask ChatGPT to generate code for each feature one step\n\nat a time and wait for user feedback before proceeding to the next step. Also, provide the path of the dataset from where it will be loaded.\n\n2.3 – context: Given that the focus is on sentiment analysis using the\n\nAmazon product review dataset, ChatGPT is not aware of the dataset and its\n\ncharacteristics, so additional context will be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "3.1 – learn:\n\nEmphasize the importance of understanding machine learning models\n\nand their components, including feature engineering techniques and\n\nmodel selection.\n\n3.2 – improvise:\n\nRequest ChatGPT to provide code snippets for implementing additional features such as hyperparameter tuning, handling\n\nimbalanced data, and model evaluation techniques.\n\n3.3 – feedback:\n\nShare generated code and model outputs for feedback to ensure\n\neffective learning and understanding.\n\nIncorporate user suggestions and critiques to refine the model and\n\napproach.\n\nProvide error messages to troubleshoot and resolve any issues encountered during model implementation.\n\n3.4 – evaluate:\n\nExecute the generated code provided by ChatGPT to verify accuracy\n\nand validity, ensuring that the model performs effectively in sentiment\n\nanalysis tasks using the Amazon product review dataset.\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "using the free version of ChatGPT We will utilize ChatGPT’s free version to build the baseline model.\n\nFeature 1: Data preprocessing and feature engineering Let’s craft our initial prompt for our baseline model.\n\n[Prompt]\n\nI want to create a simple classification model for sentiment analysis of\n\nAmazon Review Dataset (TAG 1.1) which consists of following fields (PIC\n\n2.3)\n\nmarketplace (string): Location of the product.\n\ncustomer_id (string): Unique identifier for customers.\n\nreview_id (string): Unique identifier for reviews.\n\nproduct_id (string): Unique identifier for products.\n\nproduct_parent (string): Parent product identifier.\n\nproduct_title (string): Title of the reviewed product.\n\nproduct_category (string): Category of the product.\n\nstar_rating (int): Rating of the product on a scale of 1 to 5.\n\nhelpful_votes (int): Number of helpful votes received for the review.\n\ntotal_votes (int): Total number of votes received for the review.\n\nreview_headline (string): Headline of the review.\n\nreview_body (string): Content of the review.\n\nreview_date (string): Date of the review.",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Sentiments (string): Sentiment of the review (positive or negative).\n\nIt should consist of following steps (TAG 1.2) and provide one step at a\n\ntime and wait for users feedback (PIC 2.2)\n\nData Preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel Selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or SVMs.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and\n\nstrictly follow the following instructions (TAG 1.3)\n\nThe code should be compatible with Jupyter notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into\n\nthe text block of the Notebook, in detail for each method used before\n\nproviding the code. Also, note that the dataset is placed in Google Drive at this path: /Chapter05/amazon_product_review_data.csv. So, add the code to mount the drive and load the dataset (PIC 2.2).\n\n[End of prompt]\n\nYou should receive a response similar to the following where ChatGPT has\n\nprovided us with detailed comments and explanations:\n\n[Prompt response]\n\nSure, let’s break down the process into steps. We’ll start with data\n\npreprocessing.\n\nStep 1: Data Preprocessing",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Data preprocessing is essential to clean and prepare the text data for the\n\nsentiment analysis model. We’ll perform the following steps:\n\n1. Loading the Dataset: Load the Amazon review dataset from the\n\nprovided path.\n\n2. Tokenization: Split the text into individual words or tokens.\n\n3. Lowercasing: Convert all text to lowercase to ensure uniformity.\n\n4. Removing Stopwords: Remove common words that don’t carry much\n\nmeaning.\n\n5. Removing Punctuation: Remove punctuation marks from the text.\n\n6. Feature Engineering: Transform text data into a format suitable for\n\nmachine learning models (e.g., TF-IDF encoding).\n\nNow, let’s dive into each step in detail before writing the code.\n\nTokenization:\n\nTokenization is the process of splitting text into individual words or\n\ntokens. It’s a crucial step in natural language processing (NLP) tasks.\n\nTokens are the basic units of text processing. For example, the\n\nsentence “The quick brown fox jumps over the lazy dog” can be\n\ntokenized into [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’,\n\n‘lazy’, ‘dog’].\n\nLowercasing:\n\nLowercasing involves converting all text to lowercase. It helps in\n\nstandardizing the text data and reduces the vocabulary size. For\n\nexample, ‘Hello’ and ‘hello’ will be treated as the same word after\n\nlowercasing.\n\nRemoving Stopwords:",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Stopwords are common words that do not carry significant meaning,\n\nsuch as ‘the’, ‘is’, ‘and’, etc. Removing stopwords helps in reducing\n\nnoise in the data and improving the model’s performance.\n\nRemoving Punctuation:\n\nPunctuation marks like commas, periods, and exclamation marks don’t\n\ncontribute much to sentiment analysis and can be safely removed from\n\nthe text.\n\nFeature Engineering (TF-IDF Encoding):\n\nTF-IDF (Term Frequency-Inverse Document Frequency) encoding is a\n\ncommon technique used to represent text data numerically. It reflects\n\nhow important a word is to a document in a collection. It’s calculated based on two metrics: term frequency (TF) and inverse document\n\nfrequency (IDF). TF measures the frequency of a term in a document,\n\nwhile IDF measures the rarity of the term across documents. TF-IDF\n\nencoding gives higher weights to terms that are frequent in a document\n\nbut rare across documents, which are typically more informative.\n\n[End of response]\n\nWe will now ask ChatGPT to provide us with the code:\n\n[Prompt]\n\nOk, let’s proceed.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for loading the dataset:\n\n[Prompt response]",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Great! Let’s start by loading the dataset from Google Drive in Jupyter\n\nNotebook. First, we need to mount Google Drive and then read the CSV file\n\ncontaining the Amazon review data.\n\nStep 1: Loading the Dataset\n\nWe’ll use the pandas library to load the dataset from the CSV file. Pandas\n\nprovides powerful data manipulation tools for Python, making it suitable for\n\nhandling structured data like CSV files.\n\nCode Explanation:\n\nimport pandas as pd: Import the pandas library and alias it as pd for convenience.\n\nfrom google.colab import drive: Import the drive module from the\n\ngoogle.colab library to mount Google Drive.\n\ndrive.mount('/content/drive'): Mount Google Drive to access files\n\nfrom it. This will prompt you to authorize access to your Google\n\nDrive.\n\ndata_path = '/content/drive/My Drive/Chapter05/amazon_product_review_data.csv': Define the path\n\nto the CSV file containing the dataset.\n\ndf = pd.read_csv(data_path): Use the read_csv() function from pandas to read the CSV file and store it in a DataFrame called df.\n\nLet’s implement this in code:\n\n# Importing necessary libraries import pandas as pd from google.colab import drive # Mount Google Drive drive.mount('/content/drive') # Path to the dataset",
      "content_length": 1217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "data_path = '/content/drive/My Drive/Chapter05/amazon_product_r # Load the dataset into a DataFrame df = pd.read_csv(data_path)\n\nRun this code in your Jupyter Notebook, and it will load the dataset into the\n\nDataFrame df. Let me know if you have any questions before we proceed\n\nto the next step\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in the Google Colab for every step. The\n\ncode loads the dataset from Google Drive. It will ask for the permissions.\n\nThe code runs perfectly fine without applying any modifications or any\n\nerrors.\n\nPlease refer to the notebook Building a Classification Model for Sentiment\n\nAnalysis with ChatGPT.ipynb for the executable code snippets or copy the\n\ncode from the prompt response.\n\nFigure 12.1: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step:\n\n[Prompt]",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Let’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for data preprocessing:\n\n[Prompt response]\n\nGreat! In the next step, we’ll perform the following preprocessing tasks:\n\n1. Tokenization\n\n2. Lowercasing\n\n3. Removing stopwords\n\n4. Removing punctuation\n\nWe’ll use the NLTK library for tokenization and stopwords removal, and\n\nwe’ll leverage string methods for lowercasing and punctuation removal.\n\nStep 2: Data Preprocessing\n\nTokenization:\n\nWe’ll tokenize the review text into individual words or tokens. NLTK provides a word_tokenize function for this purpose.\n\nLowercasing:\n\nWe’ll convert all text to lowercase using the lower() method of Python strings.\n\nRemoving Stopwords:\n\nNLTK provides a list of stopwords for various languages. We’ll use\n\nthis list to filter out stopwords from the review text.\n\nRemoving Punctuation:",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "We’ll use string methods to remove punctuation marks from the review\n\ntext.\n\nLet’s implement these preprocessing steps in code:\n\n# Importing necessary libraries import nltk import string # Download NLTK resources (only need to do this once) nltk.download('punkt') nltk.download('stopwords') # Get the list of stopwords stopwords = nltk.corpus.stopwords.words('english') # Function to preprocess text def preprocess_text(text): # Tokenization tokens = nltk.word_tokenize(text) # Lowercasing tokens = [token.lower() for token in tokens] # Removing stopwords and punctuation tokens = [token for token in tokens if token not in stopwor # Join tokens back into text preprocessed_text = ' '.join(tokens) return preprocessed_text # Preprocess the review body df['Cleaned_Review'] = df['review_body'].apply(preprocess_text) # Display the preprocessed data df.head()\n\nHere’s an explanation of the code:\n\nWe import the necessary libraries, including NLTK for natural\n\nlanguage processing tasks and the string module for handling\n\npunctuation.\n\nWe download the NLTK resources for tokenization and stopwords removal using the nltk.download() function.",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "We define a function preprocess_text() to preprocess each review\n\ntext. This function performs tokenization, lowercasing, removal of\n\nstopwords, and removal of punctuation.\n\nInside the function, we tokenize the text using nltk.word_tokenize(), convert tokens to lowercase, remove stopwords and punctuation using\n\nlist comprehensions, and then join the tokens back into text.\n\nFinally, we apply the preprocess_text() function to the review_body\n\ncolumn of the DataFrame and store the preprocessed text in a new column called Cleaned_Review.\n\nRun this code in your Jupyter Notebook, and it will preprocess the review\n\ntext in the DataFrame. Let me know if you have any questions or if you’re\n\nready to proceed to the next step!\n\n[End of response]\n\nWe execute the code in the notebook and it runs perfectly fine, and the\n\ndataset summary is printed along with a new column called Cleaned_Review.\n\nFigure 12.2: Code execution – dataset summary",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Feature 2: Model selection and baseline training Now that data preprocessing is complete, we will move to our next feature\n\nof model selection.\n\nWe will now instruct ChatGPT to proceed to the next step and provide it\n\nwith the output of the previous step:\n\n[Prompt]\n\nmarket_place customer_id review_id product_id product_parent pr\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for model selection:\n\n[Prompt response]\n\nIt looks like the DataFrame now includes a new column named Cleaned_Review containing the preprocessed text data. We’ve successfully completed the data preprocessing step.\n\nNow, let’s move on to the next step: Model Selection.\n\nStep 3: Model Selection\n\nIn this step, we’ll choose a baseline machine learning model for sentiment\n\nanalysis. Commonly used models for text classification tasks like sentiment\n\nanalysis include logistic regression, Naive Bayes, and support vector machines (SVM). We’ll start with one of these models as a baseline and\n\nevaluate its performance.",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Logistic Regression:\n\nLogistic regression is a popular linear classification algorithm used for\n\nbinary classification tasks like sentiment analysis. It works by\n\nmodelling the probability that a given input belongs to a particular\n\nclass using a logistic function.\n\nWe’ll use scikit-learn, a powerful machine learning library in Python,\n\nto implement logistic regression for sentiment analysis.\n\nLet’s proceed to implement logistic regression for sentiment analysis using\n\nthe preprocessed data. Here’s the code:\n\n# Importing necessary libraries from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, classification_repo # Splitting the data into training and testing sets X = df['Cleaned_Review'] y = df['sentiments'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You ca X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) # Initialize and train the logistic regression model lr_model = LogisticRegression() lr_model.fit(X_train_tfidf, y_train) # Predictions y_pred = lr_model.predict(X_test_tfidf) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy:\", accuracy) print(\"\\nClassification Report:\") print(classification_report(y_test, y_pred))",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Let me explain the code:\n\nWe import necessary libraries from scikit-learn for model selection and\n\nevaluation, including train_test_split for splitting the data,\n\nTfidfVectorizer for TF-IDF vectorization, LogisticRegression for logistic regression, and accuracy_score and classification_report\n\nfor model evaluation.\n\nWe split the data into training and testing sets using train_test_split.\n\nWe perform TF-IDF vectorization on the training and testing data using TfidfVectorizer.\n\nWe initialize and train a logistic regression model using the training\n\ndata.\n\nWe make predictions on the testing data.\n\nFinally, we evaluate the model’s performance using accuracy and\n\nclassification report.\n\nYou can run this code in your Jupyter Notebook to train the logistic\n\nregression model and evaluate its performance on the sentiment analysis\n\ntask. Let me know if you have any questions or if you’d like to explore\n\nother model options!\n\n[End of response]\n\nWe execute the code and it runs without any errors, trains the model, and\n\nevaluates the model performance.\n\nYou should see a table on precision, recall, and more.\n\nWe will move on to our next feature of model evaluation and interpretation.",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Feature 3: Model evaluation and interpretation Now that we have selected the logistic regression model and baseline\n\nperformance is evaluated, we can interpret the results.\n\nWe will now provide the output to ChatGPT and request additional\n\ntechniques such as feature importance analysis and model explainability:\n\n[Prompt]\n\nI would like to learn about the metrics such as accuracy, precision, recall,\n\nand F1-score. Additionally, I would like to apply techniques for interpreting\n\nmodel predictions, such as feature importance analysis or model\n\nexplainability methods, to gain insights into the factors influencing\n\nsentiment classification decisions. The following is the output from the\n\nmodel evaluation.\n\nAccuracy: 0.86 Classification Report: precision recall f1-score support negative 0.00 0.00 0.00 14 positive 0.86 1.00 0.92 86 accuracy 0.86 100 macro avg 0.43 0.50 0.46 100 weighted avg 0.74 0.86 0.80 100\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT explains\n\nthe metrics and suggests some options for model explainability and so on:\n\n[Prompt response]",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Certainly! Let’s break down the metrics mentioned in the model evaluation\n\noutput and discuss techniques for interpreting model predictions.\n\nMetrics Explanation:\n\nAccuracy:\n\nAccuracy measures the proportion of correctly classified instances\n\namong the total instances. It’s calculated as the ratio of the number of\n\ncorrect predictions to the total number of predictions. However, accuracy alone may not provide a complete picture of model\n\nperformance, especially when classes are imbalanced.\n\nPrecision:\n\nPrecision measures the proportion of true positive predictions among\n\nall positive predictions. It indicates the model’s ability to avoid false\n\npositives. It’s calculated as the ratio of true positives to the sum of\n\ntrue positives and false positives.\n\nRecall:\n\nRecall, also known as sensitivity or true positive rate, measures the\n\nproportion of true positive predictions among all actual positive\n\ninstances. It indicates the model’s ability to capture all positive\n\ninstances. It’s calculated as the ratio of true positives to the sum of\n\ntrue positives and false negatives.\n\nF1-Score:\n\nThe F1-score is the harmonic mean of precision and recall. It\n\nprovides a balanced measure that considers both precision and recall. It’s calculated as 2 * (precision * recall) / (precision +\n\nrecall).",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Interpretation:\n\nAccuracy:\n\nThe accuracy of 0.86 means that the model correctly classified 86% of the instances in the test set. However, accuracy alone doesn’t\n\nprovide insights into class-specific performance.\n\nPrecision:\n\nThe precision for the positive class is 0.86, indicating that when the\n\nmodel predicts a review as positive, it is correct 86% of the time. The\n\nprecision for the negative class is 0.00, indicating that the model did\n\nnot correctly predict any negative reviews.\n\nRecall:\n\nThe recall for the positive class is 1.00, indicating that the model\n\ncorrectly identified all positive reviews. However, the recall for the\n\nnegative class is 0.00, indicating that the model failed to identify any\n\nnegative reviews.\n\nF1-Score:\n\nThe F1-score for the positive class is 0.92, which is the harmonic\n\nmean of precision and recall for the positive class. It provides a\n\nbalanced measure of the model’s performance for positive reviews.\n\nTechniques for Interpreting Model Predictions:\n\nFeature Importance Analysis:\n\nFor logistic regression models, we can interpret feature importance\n\nby examining the coefficients associated with each feature. Higher\n\nmagnitude coefficients indicate stronger influence on the predicted",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "class. We can identify important words or features that contribute to\n\npositive or negative sentiment.\n\nModel Explainability Methods:\n\nTechniques such as SHAP (SHapley Additive exPlanations) values,\n\nLIME (Local Interpretable Model-agnostic Explanations), or ELI5\n\n(Explain Like I’m 5) can provide insights into how individual\n\nfeatures contribute to model predictions. These methods help in understanding the decision-making process of complex models like\n\nlogistic regression.\n\nWould you like to delve deeper into any of these techniques or have any\n\nspecific questions? Let me know how I can assist you further!\n\n[End of response]\n\nWe will ask ChatGPT to apply the SHAP (SHapley Additive\n\nexPlanations) technique:\n\n[Prompt]\n\nLet’s try SHAP first.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT\n\nunderstood the request and provides us with the code for SHAP:\n\n[Prompt response]\n\nSure! SHAP (SHapley Additive exPlanations) is a method for explaining\n\nindividual predictions from machine learning models. It provides a way to\n\nunderstand the contribution of each feature to the model’s output for a\n\nspecific instance.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Here’s how we can use SHAP to interpret the logistic regression model’s\n\npredictions for sentiment analysis:\n\nUsing SHAP for Model Interpretation:\n\n1. Install SHAP:\n\nFirst, let’s make sure you have SHAP installed. If not, you can install\n\nit using pip:\n\n!pip install shap\n\n2. Prepare the Data:\n\nWe’ll need to prepare the data for SHAP analysis. This involves\n\ntransforming the text data into a format suitable for the logistic\n\nregression model.\n\n3. Explain Model Predictions:\n\nWe’ll use SHAP to explain the logistic regression model’s predictions for individual instances in the test set. This will provide insights into\n\nhow each feature contributes to the model’s decision.\n\nLet’s implement these steps in code:\n\n# Importing necessary libraries import shap # Initialize the SHAP explainer with the logistic regression mo explainer = shap.Explainer(lr_model, X_train_tfidf) # Explain the model's predictions for a specific instance (e.g. instance_index = 0 # You can choose any instance index from th shap_values = explainer.shap_values(X_test_tfidf[instance_index # Visualize the SHAP values shap.summary_plot(shap_values, features=X_test_tfidf[instance_i",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Explanation of the code:\n\nWe import the SHAP library.\n\nWe initialize the SHAP explainer with the logistic regression model (lr_model) and the TF-IDF transformed training data\n\n(X_train_tfidf).\n\nWe use the explainer to compute SHAP values for a specific instance from the test set (X_test_tfidf[instance_index]).\n\nFinally, we visualize the SHAP values using shap.summary_plot().\n\nThis visualization will provide insights into which features (words)\n\ncontributed most to the model’s prediction for the selected instance.\n\nYou can run this code in your Jupyter Notebook to visualize the SHAP\n\nvalues for individual predictions. Let me know if you have any questions or\n\nneed further assistance!\n\n[End of response]\n\nWe execute the code and it gives an error: AttributeError:\n\n'TfidfVectorizer' object has no attribute 'get_feature_names'.\n\nFigure 12.3: Attribute error for SHAP",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "We will use the feedback prompting strategy (LIFE 3.3) and provide the\n\nerror to ChatGPT:\n\n[Prompt]\n\nAttributeError: 'TfidfVectorizer' object has no attribute 'get_\n\n[End of prompt].\n\nWe should receive a response with the suggested correct code.\n\n[Prompt response]\n\nIt seems there was an AttributeError because the TfidfVectorizer object\n\ndoes not have a get_feature_names() method. My apologies for the\n\noversight. We can access the feature names directly from the vectorizer’s\n\nvocabulary attribute.\n\nLet’s correct the code to retrieve feature names from the TfidfVectorizer vocabulary:\n\n# Get feature names from the TfidfVectorizer vocabulary feature_names = tfidf_vectorizer.get_feature_names_out() # Visualize the SHAP values shap.summary_plot(shap_values, features=X_test_tfidf[instance_i\n\nBy using get_feature_names_out(), we can access the feature names from\n\nthe TfidfVectorizer's vocabulary.\n\nYou can update the code accordingly and try running it again in your\n\nJupyter Notebook. Let me know if you encounter any further issues!\n\n[End of response]",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "We execute the corrected code and it works fine and produces the output\n\nplot:\n\nFigure 12.4: Code execution – SHAP impact on model output\n\nWords like “great” and “love” have positive SHAP values, meaning\n\ntheir presence in the data likely pushes predictions toward a positive",
      "content_length": 275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "outcome, possibly indicating a positive sentiment in a sentiment\n\nanalysis model.\n\nThe words are ranked by the absolute size of their SHAP values, with\n\n“great” having the most significant positive impact.\n\nMost words on this list have a very minor impact close to zero,\n\nsuggesting that their presence does not significantly affect the\n\nprediction relative to the baseline.\n\nSome words such as “360” and “xbox” may relate to specific topics\n\n(like gaming), and their SHAP values are slightly negative, suggesting\n\nthey might be reducing the likelihood of the positive outcome this\n\nmodel predicts.\n\nWe will now ask ChatGPT to provide us with the code for ELI5 (Explain\n\nLike I’m 5):\n\n[Prompt]\n\nNext, I would like to try ELI5 and ensure that the code is correct and is not\n\nimpacted because of the SHAP technique we applied.\n\n[End of prompt].\n\nWe should receive the code for ELI5 from ChatGPT:\n\n[Prompt response]\n\nCertainly! ELI5 (Explain Like I’m 5) is another popular library for model\n\ninterpretation and explaining machine learning models. We can use ELI5 to\n\nunderstand how our logistic regression model makes predictions and inspect\n\nfeature importance.\n\nLet’s integrate ELI5 into our code to explain the logistic regression model’s\n\npredictions:",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Using ELI5 for Model Interpretation:\n\n1. Install ELI5:\n\nIf you haven’t installed ELI5 yet, you can install it using pip:\n\n!pip install eli5\n\n2. Explain Model Predictions:\n\nWe’ll use ELI5 to explain the logistic regression model’s predictions\n\nfor individual instances in the test set. ELI5 provides different\n\nmethods to explain model predictions, including feature importances and textual explanations.\n\nLet’s update our code to include ELI5 for model interpretation:\n\n# Importing necessary libraries import eli5 # Explain the logistic regression model using ELI5 eli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20) # Sh\n\nExplanation of the code:\n\nWe import the ELI5 library.\n\nWe use the show_weights function from ELI5 to explain the logistic\n\nregression model’s predictions. This function displays the weights\n\n(coefficients) associated with each feature, indicating their importance\n\nin the model’s decision-making process.\n\nWe specify the vectorizer (vec) parameter to ensure ELI5 interprets the features correctly based on the TF-IDF vectorization.",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "You can run this updated code in your Jupyter Notebook to see the top\n\nfeatures contributing to the model’s predictions. ELI5 provides a clear\n\nvisualization of feature importances, allowing you to gain insights into the\n\nmodel’s behavior.\n\nLet me know if you have any questions or encounter any issues!\n\n[End of response]\n\nWe execute the code provided by ChatGPT, which installs the eli5 library\n\nand generates the output as follows:\n\nThe output is not understandable at all, which is ironic as it is supposed to\n\nbe understood by a 5-year-old.\n\nLet’s ask ChatGPT to at least provide code in which the feature names are\n\nin plain English:\n\n[Prompt]\n\nCan I also get the features? X numbers make no sense and are definitely not\n\nunderstandable to a 5-year-old.\n\n[End of prompt]\n\nWe should receive a response similar to the following:\n\n[Prompt response]\n\nCertainly! We can map the feature indices (x numbers) to their\n\ncorresponding words or terms in the TF-IDF vectorizer’s vocabulary to\n\nmake them more interpretable. Let’s update the code to include feature\n\nnames along with their weights:\n\n# Explain the logistic regression model using ELI5 with feature",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "eli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20, featu\n\nBy including the feature_names parameter with the TF-IDF vectorizer’s vocabulary, ELI5 will display the actual words or terms instead of feature\n\nindices.\n\nRun this updated code in your Jupyter Notebook, and you’ll see the top\n\nfeatures contributing to the positive class predictions along with their\n\nweights. Let me know if you have any questions or need further assistance!\n\n[End of response]\n\nWe execute the provided code, and it now provides the feature names in\n\nEnglish:\n\nThis concludes Feature 3, but as our data is clearly imbalanced, let’s\n\nproceed to the next feature.\n\nFeature 4: Handling imbalanced data Now that we have a baseline model and learned how to use the model\n\nexplainability techniques, we will now see how to handle the data\n\nimbalance:\n\n[Prompt]\n\nI would like to apply some techniques to mitigate the data imbalance as it is\n\nclearly seen from the model performance.\n\nAccuracy: 0.86 Classification Report: precision recall f1-score support",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "negative 0.00 0.00 0.00 14 positive 0.86 1.00 0.92 86.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with suggestions and code to\n\nmitigate the data imbalance:\n\n[Prompt response]\n\nAddressing data imbalance is crucial for improving the model’s\n\nperformance, especially in cases where one class is significantly\n\nunderrepresented compared to the other. Here are some techniques we can\n\napply to mitigate data imbalance:\n\nResampling Techniques:\n\nOver-sampling: Increase the number of instances in the minority class\n\nby generating synthetic samples (e.g., using SMOTE - Synthetic\n\nMinority Over-sampling Technique).\n\nUnder-sampling: Decrease the number of instances in the majority\n\nclass by randomly removing samples.\n\nAlgorithmic Techniques:\n\nClass weights: Assign higher weights to instances of the minority\n\nclass during model training to penalize misclassifications.\n\nEnsemble methods: Utilize ensemble methods that handle class imbalance well, such as Balanced Random Forest or Easy Ensemble.\n\nData Augmentation:",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Generate new samples for the minority class by applying\n\ntransformations or perturbations to existing data points.\n\nCost-sensitive Learning:\n\nExplicitly incorporate the cost of misclassification into the model\n\ntraining process to prioritize correct classification of the minority\n\nclass.\n\nLet’s implement one of these techniques, such as class weights adjustment,\n\nto address the data imbalance in our logistic regression model:\n\nimport numpy as np # Calculate class weights from sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight('balanced', classes=np.uni # Initialize and train the logistic regression model with class lr_model_balanced = LogisticRegression(class_weight=dict(zip(np lr_model_balanced.fit(X_train_tfidf, y_train) # Predictions y_pred_balanced = lr_model_balanced.predict(X_test_tfidf) # Evaluate the model with balanced class weights accuracy_balanced = accuracy_score(y_test, y_pred_balanced) print(\"Accuracy with balanced class weights:\", accuracy_balance print(\"\\nClassification Report with balanced class weights:\") print(classification_report(y_test, y_pred_balanced))\n\nExplanation:\n\nWe calculate class weights using compute_class_weight from scikit- learn’s utils. Setting balanced mode automatically adjusts weights\n\ninversely proportional to class frequencies in the input data.\n\nWe initialize and train a new logistic regression model (lr_model_balanced) with the calculated class weights.",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "We make predictions using the balanced model and evaluate its\n\nperformance using accuracy and classification report.\n\nRunning this code will train the logistic regression model with balanced\n\nclass weights and evaluate its performance on the test set. It should help\n\nimprove the handling of the imbalanced data and potentially enhance the\n\nmodel’s performance. Let me know if you have any questions or need\n\nfurther assistance!\n\n[End of response]\n\nWe will execute the code that uses calculated class weights and see what\n\nimpact it makes.\n\nYou should see a classification report with balanced weights.\n\nWe will give the output to ChatGPT so that it provides us with\n\nexplanations:\n\n[Prompt]\n\nAccuracy with balanced class weights: 0.81\n\nClassification Report with balanced class weights:\n\nprecision recall f1-score support negative 0.27 0.21 0.24 14 positive 0.88 0.91 0.89 86 accuracy 0.81 100 macro avg 0.57 0.56 0.57 100 weighted avg 0.79 0.81 0.80 100\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nthe interpretation of the output:",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "[Prompt response]\n\nIt appears that applying balanced class weights has improved the model’s\n\nperformance, although the accuracy has decreased slightly. Let’s interpret\n\nthe results:\n\nAccuracy: The accuracy of 0.81 indicates that the model correctly\n\nclassified 81% of the instances in the test set.\n\nPrecision and Recall: For the negative class, the precision has\n\nimproved significantly to 0.27, indicating that when the model predicts\n\na review as negative, it is correct 27% of the time. However, the recall\n\nis relatively low at 0.21, suggesting that the model fails to capture\n\nmany negative instances.\n\nFor the positive class, both precision and recall are high, with\n\nprecision at 0.88 and recall at 0.91. This indicates that the model\n\nperforms well in identifying positive instances.\n\nF1-score: The F1-score for the negative class has improved slightly to\n\n0.24, while for the positive class, it remains high at 0.89.\n\nSupport: The support represents the number of instances in each class. There are 14 instances of the negative class and 86 instances of the\n\npositive class in the test set.\n\nOverall, while the accuracy has decreased, the model’s ability to correctly\n\nclassify the minority class (negative reviews) has improved significantly\n\nwith balanced class weights. This indicates a better balance between\n\nprecision and recall for both classes, resulting in a more robust model.\n\nIf you have any further questions or need additional assistance, feel free to\n\nask!\n\n[End of response]",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "We have reduced the imbalance and will now move on to the next feature of\n\nhyperparameter tuning.\n\nFeature 5: Hyperparameter tuning Now that we have learned how to deal with imbalanced data, we can\n\noptimize the performance by tuning hyperparameters such as regularization\n\nstrength, learning rate, and kernel parameters, and also apply techniques\n\nlike grid search or random search:\n\n[Prompt]\n\nI would like to optimize the performance by tuning hyperparameters such as\n\nregularization strength, learning rate, and kernel parameters and also apply\n\ntechniques like grid search or random search.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nCertainly! Hyperparameter tuning is crucial for optimizing model\n\nperformance. We can use techniques like grid search or random search to\n\nsearch through a range of hyperparameter values and find the combination\n\nthat yields the best performance.\n\nLet’s start by implementing hyperparameter tuning using grid search with\n\nlogistic regression as an example:\n\n# Importing necessary libraries from sklearn.model_selection import GridSearchCV # Define hyperparameters grid",
      "content_length": 1150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "param_grid = { 'C': [0.01, 0.1, 1.0, 10.0], # Regularization strength (sm 'solver': ['liblinear', 'lbfgs'], # Optimization algorithm } # Initialize logistic regression model lr_model_tuned = LogisticRegression() # Initialize GridSearchCV with logistic regression model and hy grid_search = GridSearchCV(lr_model_tuned, param_grid, cv=5, sc # Perform grid search grid_search.fit(X_train_tfidf, y_train) # Get the best hyperparameters best_params = grid_search.best_params_ print(\"Best Hyperparameters:\", best_params) # Use the best model from grid search best_lr_model = grid_search.best_estimator_ # Predictions y_pred_tuned = best_lr_model.predict(X_test_tfidf) # Evaluate the tuned model accuracy_tuned = accuracy_score(y_test, y_pred_tuned) print(\"Accuracy with tuned model:\", accuracy_tuned) print(\"\\nClassification Report with tuned model:\") print(classification_report(y_test, y_pred_tuned))\n\nExplanation:\n\nWe define a grid of hyperparameters (param_grid) to search through.\n\nIn this example, we vary the regularization strength (C) and the solver\n\nalgorithm.\n\nWe initialize a logistic regression model (lr_model_tuned) and perform grid search using GridSearchCV.\n\nGrid search will evaluate the performance of the model with different hyperparameter combinations using cross-validation (cv=5).\n\nAfter grid search, we retrieve the best hyperparameters (best_params)\n\nand the best model (best_lr_model).",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Finally, we make predictions with the best model and evaluate its\n\nperformance.\n\nYou can adjust the hyperparameter grid (param_grid) to include other\n\nhyperparameters like learning rate and kernel parameters for different\n\nmodels. Additionally, you can also explore random search or other\n\noptimization techniques for hyperparameter tuning.\n\nFeel free to run this code in your Jupyter Notebook to optimize the logistic\n\nregression model’s performance through hyperparameter tuning. Let me\n\nknow if you have any questions or need further assistance!\n\n[End of response]\n\nWe execute the code and it works fine, and we find the best parameters\n\nusing GridSearchCV.\n\nYou should see a classification report with tuned model.\n\nThe hyperparameter tuning did not significantly improve the model’s\n\nperformance in terms of accuracy and class-specific metrics. Let’s interpret\n\nthe results:\n\nBest hyperparameters: The grid search identified the best hyperparameters as C=0.01 and solver='liblinear'. These\n\nhyperparameters indicate a low regularization strength (C=0.01) and the use of the liblinear solver algorithm.\n\nAccuracy: The accuracy of the tuned model remains at 0.86, which is\n\nthe same as the baseline model. This indicates that hyperparameter\n\ntuning did not lead to a significant improvement in overall accuracy.\n\nPrecision and recall: The precision, recall, and F1-score for both the\n\npositive and negative classes remain unchanged from the baseline",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "model. This suggests that the model’s ability to correctly classify\n\ninstances from each class did not improve after hyperparameter tuning.\n\nSupport: The support values represent the number of instances in each\n\nclass in the test set, which remains the same as before.\n\nLet’s move on to our last feature and experiment with feature\n\nrepresentation.\n\nFeature 6: Experimenting with feature representation Let’s instruct ChatGPT to provide us with the code for different feature\n\nrepresentations:\n\n[Prompt]\n\nI want to compare the performance of the model trained with different\n\nfeature representations, such as bag-of-words, TF-IDF, or word\n\nembeddings, to determine the most effective approach for sentiment\n\nclassification.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nthe code for bag-of-words, TF-IDF, and word embeddings:\n\n[Prompt response]\n\nComparing the performance of models trained with different feature\n\nrepresentations is a valuable approach to determine the most effective\n\nmethod for sentiment classification. We can train models using bag-of-\n\nwords, TF-IDF, and word embeddings representations and evaluate their",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "performance using the same evaluation metrics. Let’s go through the\n\nprocess step by step:\n\n1. Bag-of-Words Representation:\n\nWe can represent the text data using a bag-of-words approach,\n\nwhere each document is represented by a vector indicating the\n\nfrequency of each word in the vocabulary.\n\n2. TF-IDF Representation:\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\nrepresentation considers not only the frequency of a term in a\n\ndocument but also its importance in the entire corpus.\n\n3. Word Embeddings Representation:\n\nWord embeddings represent words as dense vectors in a\n\ncontinuous vector space, capturing semantic relationships\n\nbetween words.\n\nHere’s how we can compare the performance of models trained with these\n\ndifferent feature representations:\n\nPreprocess the data: Tokenize, lowercase, remove stopwords, and\n\nperform any necessary text preprocessing steps for each feature\n\nrepresentation method.\n\nTrain models: Train logistic regression models using each feature\n\nrepresentation method (bag-of-words, TF-IDF, word embeddings).\n\nEvaluate performance: Evaluate the performance of each model\n\nusing the same evaluation metrics (accuracy, precision, recall, F1-\n\nscore).\n\nCompare results: Compare the performance of models trained with\n\ndifferent feature representations to determine the most effective\n\napproach for sentiment classification.",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Let’s start by implementing logistic regression models using bag-of-words,\n\nTF-IDF, and word embeddings representations:\n\nBag-of-Words Representation:\n\nfrom sklearn.feature_extraction.text import CountVectorizer # Initialize CountVectorizer bow_vectorizer = CountVectorizer(max_features=5000) # You can # Transform text data into bag-of-words representation X_train_bow = bow_vectorizer.fit_transform(X_train) X_test_bow = bow_vectorizer.transform(X_test) # Train logistic regression model with bag-of-words representat lr_model_bow = LogisticRegression() lr_model_bow.fit(X_train_bow, y_train) # Evaluate model performance y_pred_bow = lr_model_bow.predict(X_test_bow) accuracy_bow = accuracy_score(y_test, y_pred_bow) print(\"Accuracy with bag-of-words representation:\", accuracy_bo # Add other evaluation metrics if needed\n\nTF-IDF Representation:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Initialize TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You ca # Transform text data into TF-IDF representation X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) # Train logistic regression model with TF-IDF representation lr_model_tfidf = LogisticRegression() lr_model_tfidf.fit(X_train_tfidf, y_train) # Evaluate model performance y_pred_tfidf = lr_model_tfidf.predict(X_test_tfidf) accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf) print(\"Accuracy with TF-IDF representation:\", accuracy_tfidf) # Add other evaluation metrics if needed",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Word Embeddings Representation:\n\nFor word embeddings representation, we need to first convert the text data\n\ninto word embeddings using techniques like Word2Vec, GloVe, or fastText.\n\nWe’ll use pre-trained word embeddings for this purpose.\n\n# Assuming you have pre-trained word embeddings loaded (e.g., G # Convert text data into word embeddings representation # You may need to tokenize and pad sequences before applying wo # Train logistic regression model with word embeddings represen lr_model_word_embeddings = LogisticRegression() lr_model_word_embeddings.fit(X_train_word_embeddings, y_train) # Evaluate model performance y_pred_word_embeddings = lr_model_word_embeddings.predict(X_tes accuracy_word_embeddings = accuracy_score(y_test, y_pred_word_e print(\"Accuracy with word embeddings representation:\", accuracy # Add other evaluation metrics if needed\n\nAfter training and evaluating models with different feature representations,\n\nyou can compare their performance using accuracy or other evaluation\n\nmetrics. Based on the results, you can determine which feature\n\nrepresentation method is the most effective for sentiment classification.\n\nFeel free to run these codes in your Jupyter Notebook and let me know if\n\nyou need further assistance or have any questions!\n\n[End of response]\n\nWe will execute the code and compare the accuracy for bag-of-words, TF-\n\nIDF, and word embeddings. The code works perfectly fine for the first two\n\nbut gives errors for word embeddings.",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "Figure 12.5: Bag of words\n\nFigure 12.6: TF- IDF",
      "content_length": 47,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Figure 12.7: Word embedding – error\n\nSo, we will instruct ChatGPT to provide us with the corrected code for\n\nword embeddings, and we will also provide the context that we have done\n\ntokenization earlier but no embeddings:\n\n[Prompt]\n\nNameError Traceback (most recent call last) <ipython-input-20-8\n\n[End of prompt]\n\nWe should receive the response with the modified code similar to the\n\nfollowing:\n\n[Prompt response]\n\nThank you for providing clarification. Since we have preprocessed the text\n\ndata and tokenized it, we can represent it using word embeddings without",
      "content_length": 564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "pre-trained word embeddings. We’ll use techniques like Word2Vec or\n\nDoc2Vec to generate word embeddings from the tokenized text.\n\nLet’s proceed with representing the text data using Word2Vec and then train\n\na logistic regression model:\n\nWord Embeddings Representation with Word2Vec:\n\nfrom gensim.models import Word2Vec from sklearn.feature_extraction.text import CountVectorizer from sklearn.base import BaseEstimator, TransformerMixin # Define a custom transformer to tokenize the text data class TokenizerTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return [text.split() for text in X] # Initialize and fit Word2Vec model on tokenized text data w2v_model = Word2Vec(sentences=TokenizerTransformer().fit_trans # Define a function to average word vectors for each document def average_word_vectors(words, model, vocabulary, num_features feature_vector = np.zeros((num_features,), dtype=\"float64\") nwords = 0. for word in words: if word in vocabulary: nwords = nwords + 1. feature_vector = np.add(feature_vector, model.wv[wo if nwords: feature_vector = np.divide(feature_vector, nwords) return feature_vector # Define a custom transformer to convert tokenized text data in class WordEmbeddingsTransformer(BaseEstimator, TransformerMixin def __init__(self, model): self.model = model self.num_features = model.vector_size self.vocabulary = set(model.wv.index_to_key) def fit(self, X, y=None): return self def transform(self, X): return np.array([average_word_vectors(words, self.model",
      "content_length": 1545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "# Convert tokenized text data into word embeddings representati X_train_word_embeddings = WordEmbeddingsTransformer(w2v_model). X_test_word_embeddings = WordEmbeddingsTransformer(w2v_model).t # Train logistic regression model with word embeddings represen lr_model_word_embeddings = LogisticRegression() lr_model_word_embeddings.fit(X_train_word_embeddings, y_train) # Evaluate model performance y_pred_word_embeddings = lr_model_word_embeddings.predict(X_tes accuracy_word_embeddings = accuracy_score(y_test, y_pred_word_e print(\"Accuracy with word embeddings representation (Word2Vec): # Add other evaluation metrics if needed\n\nExplanation:\n\nWe define a custom transformer TokenizerTransformer to tokenize the\n\ntext data.\n\nWe use the Word2Vec model to generate word embeddings from the\n\ntokenized text data.\n\nWe define a function average_word_vectors to average word vectors for each document.\n\nWe define another custom transformer WordEmbeddingsTransformer to\n\nconvert tokenized text data into word embeddings representation.\n\nWe train a logistic regression model using the word embeddings\n\nrepresentation obtained from Word2Vec.\n\nFinally, we evaluate the performance of the model and print the\n\naccuracy.\n\nYou can run this code in your Jupyter Notebook to represent the text data\n\nusing Word2Vec word embeddings and train a logistic regression model.\n\nLet me know if you need further assistance!\n\n[End of response]\n\nThis time, the code works:",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Figure 12.8: Accuracy with word embeddings\n\nThis concludes our last feature with the free version of ChatGPT.\n\nNow, we will try some of these features using ChatGPT-4 or ChatGPT Plus.\n\nBuilding a sentiment analysis model to accurately classify Amazon reviews using ChatGPT-4 or ChatGPT Plus ChatGPT Plus or GPT-4 includes the ability to upload a dataset, analyze the\n\ndataset, and produce results if using simple libraries such as Python and\n\nmatplotlib. At the time of writing this chapter, it had an AI assistant named\n\nData Analysis, provided by OpenAI when using the mobile app, or when\n\nselecting GPT-4 if using the browser version.\n\nLet’s explore how it differs from the free version, feature by feature.\n\nFeature 1: Data preprocessing and feature engineering",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "Let’s craft our initial prompt for the baseline model:\n\n[Prompt]\n\nI want to create a simple classification model for sentiment analysis of the\n\nAmazon Review Dataset (TAG 1.1). <upload dataset in CSV format here >\n\n(PIC 2.3)\n\nIt should consist of the following steps (TAG 1.2) and I am a beginner user\n\n(PIC 2.1), so provide one step at a time and wait for users feedback (PIC 2.2)\n\nData Preprocessing: Tokenization, lowercasing, removing stopwords\n\nand punctuation, and feature engineering (e.g., TF-IDF encoding, word\n\nembeddings).\n\nModel Selection: Choose baseline machine learning models such as\n\nlogistic regression, Naive Bayes, or support vector machines (SVM).\n\nModel Evaluation and Interpretation.\n\nHandling Imbalance Data.\n\nHyperparameter Tuning.\n\nExperimenting with feature representation.\n\n[End of prompt]\n\nYou should receive a response similar to the following where ChatGPT-4\n\nhas loaded the dataset:\n\n[Prompt response]\n\nGreat! We’ll start by preprocessing the data from the Amazon Review\n\nDataset. Here’s how we can approach the preprocessing step:\n\nLoading the Data: We’ll first load the dataset to understand its\n\nstructure.",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Tokenization: This involves breaking down text into individual words\n\nor terms.\n\nLowercasing: Convert all text to lowercase to ensure uniformity.\n\nRemoving Stopwords: Stopwords are common words (like “and”,\n\n“the”, etc.) that are often filtered out before processing natural\n\nlanguage data because they carry less meaningful information.\n\nRemoving Punctuation: Punctuation is usually removed as it doesn’t\n\nadd much extra information while processing text.\n\nFeature Engineering: We’ll apply TF-IDF encoding, which helps in\n\nunderstanding the importance of a word in a document relative to the\n\ncorpus.\n\nLet’s start by loading your data and taking a look at its structure. I’ll write a\n\nPython script to load the data and display the first few entries to better\n\nunderstand its columns and content. Let’s proceed with this step, and once\n\nwe see the data, we can move on to the next preprocessing tasks. Here’s the\n\nscript:\n\nWe’ve successfully loaded the dataset, and here’s a quick look at its\n\nstructure:\n\nmarket_place, customer_id, review_id, product_id, product_parent:\n\nThese columns are identifiers and metadata about the review and\n\nproduct.\n\nproduct_title, product_category: Information about the product.\n\nstar_rating, helpful_votes, total_votes: Ratings and votes data.\n\nverified_purchase: Whether the purchase was verified.\n\nreview_headline, review_body: The actual text of the review.\n\nreview_date: The date of the review.",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "sentiments: This seems to be a label for sentiment analysis (positive,\n\nnegative).\n\nGiven your focus is on sentiment analysis, the primary columns of interest will be review_body and sentiments.\n\nNow, let’s proceed with the preprocessing steps:\n\nTokenization\n\nLowercasing\n\nRemoving Stopwords\n\nRemoving Punctuation\n\nWe will handle these steps using the nltk library, which is commonly used\n\nfor text processing in Python. Let me know if you’re ready, and I’ll start coding these preprocessing steps!\n\n[End of response]\n\nWe can see that the dataset has been loaded successfully and ChatGPT-4 is waiting for the next step, the user’s feedback (LIFE 3.3). So, we will\n\ninstruct ChatGPT-4 to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed with data preprocessing.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and executes the code for data preprocessing:\n\n[Prompt response]\n\nGreat! Let’s start with the data preprocessing tasks one by one. Here’s what\n\nwe’ll do:",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Tokenization: This will split the review_body text into individual\n\nwords or tokens.\n\nLowercasing: Convert all tokens to lowercase to standardize the text.\n\nRemoving Stopwords: Remove common words that might not be\n\nuseful in sentiment analysis.\n\nRemoving Punctuation: Strip punctuation as it’s typically not needed\n\nin text-based models.\n\nI’ll use the Natural Language Toolkit (nltk) for these steps. Let’s begin by\n\ninstalling the necessary libraries and setting up the initial preprocessing\n\nfunctions. Here’s the code to start:\n\nIt seems there was an issue with downloading the necessary resources from\n\nthe NLTK library due to network restrictions in this environment. I’ll\n\nmodify the approach to manually define a small set of common stopwords\n\nfor demonstration purposes. Let’s proceed with a simplified version of the\n\ntext preprocessing:\n\nTokenization\n\nLowercasing\n\nRemoving a simplified set of stopwords\n\nRemoving punctuation\n\nHere’s the adapted script that manually handles these steps without the need\n\nto download additional resources:\n\nThe text preprocessing has been successfully applied using a simplified\n\napproach. Here’s a comparison of the original text and the processed text:\n\nOriginal: “As a family allergic to wheat, dairy, eggs, nuts...”\n\nProcessed: [‘a’, ‘family’, ‘allergic’, ‘wheat’, ‘dairy’, ‘eggs’, ‘nuts’]",
      "content_length": 1337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "As you can see, we’ve removed stopwords, punctuation, and have tokenized\n\nand lowercased the text. This processed data is now more suitable for\n\nfeeding into a machine learning model for sentiment analysis.\n\nNext steps would involve converting these tokens into numerical features\n\nthat can be used by machine learning algorithms. We can use methods like\n\nTF-IDF encoding or word embeddings for this. Let me know if you’re\n\nready to proceed to the feature engineering step, or if there’s anything else\n\nyou’d like to adjust in the preprocessing!\n\n[End of response]\n\nWe can see from the response that ChatGPT-4 first tried with the NLTK\n\nlibrary but due to limitations, it simplified the approach with a small set of\n\ncommon words. It also provides us with an example of how it is processed.\n\nWe will now instruct ChatGPT-4 to proceed:\n\n[Prompt]\n\nLet’s proceed.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT-4 has\n\nimplemented TF-IDF:\n\n[Prompt response]\n\nGreat! Let’s move on to the feature engineering step. In sentiment analysis,\n\none common technique for converting text data into numerical form that\n\ncan be understood by machine learning algorithms is the Term Frequency-\n\nInverse Document Frequency (TF-IDF) approach. Here’s a quick\n\nbreakdown:",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Term Frequency (TF): Measures how frequently a term occurs in a\n\ndocument. Since documents may vary in length, it is often divided by\n\nthe document length (the total number of terms in the document).\n\nInverse Document Frequency (IDF): Measures how important a term\n\nis within the corpus (collection of all documents). It helps to weigh\n\ndown terms that appear very frequently across documents and are less\n\ninformative.\n\nStep to Implement TF-IDF:\n\nConvert the list of tokens to string: Machine learning algorithms\n\ntypically require input features as strings or numbers. We’ll convert\n\nour list of tokens back to string format.\n\nTF-IDF Vectorization: Use TfidfVectorizer from scikit-learn to\n\nconvert the text documents into a matrix of TF-IDF features.\n\nI will now code these steps, starting by converting our token lists back to\n\nstrings and then applying TF-IDF vectorization. Let’s do this:\n\nWe’ve successfully transformed the processed text into a matrix of TF-IDF\n\nfeatures. Here are some details:\n\nMatrix Shape: The TF-IDF matrix has 500 documents and 3778\n\nfeatures. This means we have 3778 unique terms across the documents\n\nafter applying the preprocessing and vectorization steps.\n\nExample Features: Some of the first few terms in our feature set\n\ninclude ‘ability’, ‘able’, ‘about’, ‘above’, etc.\n\nThis TF-IDF matrix can now be used as input for various machine learning\n\nmodels to perform sentiment analysis.\n\nNext, we can move on to selecting and training a baseline model using this\n\nfeature representation. Common choices for initial models in sentiment",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "analysis include Logistic Regression, Naive Bayes, and Support Vector\n\nMachines (SVM). Let me know which model you would like to start with,\n\nor if you have any preferences for proceeding!\n\n[End of response]\n\nWe can see that ChatGPT-4 has implemented TF-IDF and has 500\n\ndocuments and 3,778 features. We will now move on to baseline model\n\ncreation.\n\nFeature 2: Model selection and baseline training We will now instruct ChatGPT-4 to proceed to model selection:\n\n[Prompt]\n\nLet’s proceed with logistic regression.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and executes the code for logistic regression.\n\n[Prompt response]\n\nPerfect! Logistic Regression is a good starting point for classification tasks\n\nlike sentiment analysis because it’s simple and often provides a good\n\nbaseline for performance comparison.\n\nSteps to Implement Logistic Regression:\n\nPrepare the data: We’ll split the data into training and testing sets to\n\nevaluate the model’s performance.",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "Train the model: We’ll use Logistic Regression from scikit-learn to\n\ntrain the model using the training set.\n\nEvaluate the model: We’ll test the model on the testing set to see how\n\nwell it predicts the sentiments.\n\nLet’s start by preparing the data and then training the Logistic Regression\n\nmodel. Here’s the code to do that:\n\nThe Logistic Regression model has been trained and evaluated. Here’s a\n\nsummary of its performance:\n\nAccuracy: 86%\n\nPrecision, Recall, and F1-Score: These metrics provide a more\n\ndetailed picture. For the ‘positive’ class, we see a precision and recall\n\nof 86% and 100% respectively, leading to an F1-score of 92%.\n\nHowever, for the ‘negative’ class, the precision, recall, and F1-score\n\nare all 0. This indicates that the model failed to correctly predict any of the ‘negative’ class instances.\n\nThis discrepancy is often due to class imbalance, where one class (‘positive’\n\nin this case) is much more frequent than the other (‘negative’), causing the\n\nmodel to be biased towards the majority class.\n\nNext Steps:\n\nHandling Imbalance Data: We can explore techniques like\n\noversampling the minority class, undersampling the majority class, or\n\nadjusting the decision threshold.\n\nHyperparameter Tuning: We can tune the parameters of the Logistic\n\nRegression model to potentially improve performance.\n\nExperimenting with Feature Representation: Trying different\n\nsettings in TF-IDF or using other types of text representations like",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "word embeddings.\n\nWould you like to explore any of these next steps further, such as\n\naddressing the imbalance in the dataset, or would you like to adjust\n\nsomething else?\n\n[End of response]\n\nFeature 3: Model evaluation and interpretation Now that we have selected the logistic regression model and baseline\n\nperformance is evaluated, let’s interpret the results.\n\nWe will ask ChatGPT-4 to explain the metrics and provide results\n\ninterpretation using techniques such as feature importance analysis and\n\nmodel explainability:\n\n[Prompt]\n\nPlease explain the metrics and provide results interpretation using\n\ntechniques such as feature importance analysis and model explainability.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT\n\nunderstood the request and provides us with the explanation and feature\n\nimportance analysis:\n\n[Prompt response]\n\nLet’s delve deeper into the evaluation metrics and explore some techniques to interpret the Logistic Regression model:\n\nEvaluation Metrics Explanation:",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "Precision: This measures the accuracy of positive predictions.\n\nFormally, it is the ratio of true positive predictions to the total\n\npredicted positives. High precision indicates a low false positive rate.\n\nRecall (Sensitivity): This is the ability of the model to find all the\n\nrelevant cases (all true positives). It is the ratio of true positives to the\n\nactual total positives.\n\nF1-Score: The F1-score is the harmonic mean of precision and recall.\n\nIt is a balance between precision and recall, a useful measure when the\n\nclasses are imbalanced.\n\nAccuracy: This is the ratio of correctly predicted observations to the\n\ntotal observations. It’s generally not a good measure for imbalanced\n\nclasses as it can be misleading.\n\nResults Interpretation:\n\nAccuracy of 86% sounds good, but it’s misleading because the model\n\nperforms poorly on the minority class (‘negative’).\n\nThe model has a high recall (100%) for the ‘positive’ class, indicating\n\nit identifies most positive instances, but it fails completely to identify\n\n‘negative’ class instances (recall of 0%).\n\nFeature Importance Analysis:\n\nIn Logistic Regression, the coefficients of the model can provide\n\ninsight into the importance of features (words in this case). A higher\n\nabsolute value of a coefficient indicates that the feature is more\n\nimportant for the decision the model makes.\n\nLet’s extract the coefficients of the model to see which words (features) are\n\ninfluencing the predictions most:",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Here are the top 10 features (words) influencing the sentiment classification\n\nand their corresponding coefficients:\n\n“not”: Coefficient of -1.87, a strong negative influence on the\n\nsentiment prediction. It suggests that the presence of “not” in a review\n\nsignificantly drives the prediction towards a negative sentiment.\n\n“great”: Coefficient of 1.65, a strong positive influence, suggesting\n\nreviews with this word are likely classified as positive.\n\n“love”: Coefficient of 1.54, another strong positive indicator.\n\n“good”: Coefficient of 0.99, positively influences the sentiment.\n\n“but”: Coefficient of -0.95, negatively influences the sentiment.\n\n“very”: Coefficient of 0.88, generally used in positive contexts.\n\n“was”: Coefficient of -0.86, showing a negative influence, possibly\n\nused in contexts of disappointment.\n\n“cheap”: Coefficient of -0.81, commonly associated with negative\n\nsentiments.\n\n“there”: Coefficient of -0.81, suggesting a negative context when this\n\nword is used.\n\n“dont” (likely meant to be “don’t”): Coefficient of -0.79, another\n\nstrong negative indicator.\n\nModel Explainability:\n\nThese coefficients provide direct insight into how the model is making\n\ndecisions based on the presence of certain words in the text. Words with\n\npositive coefficients increase the likelihood that a review is classified as positive, while those with negative coefficients push towards a negative\n\nclassification.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "If you want further interpretation, techniques like LIME (Local\n\nInterpretable Model-agnostic Explanations) or SHAP (SHapley Additive\n\nexPlanations) can be applied to provide more nuanced insights into specific\n\npredictions.\n\nWould you like to dive deeper into any specific area, such as handling data\n\nimbalance or exploring model explainability techniques like LIME or\n\nSHAP for a particular prediction?\n\n[End of response]\n\nChatGPT-4 cannot install new libraries like SHAP, so we will finish the\n\nfeature analysis here and move on to the next step of handling data\n\nimbalance.\n\nFeature 4: Handling data imbalance Now, we will instruct ChatGPT-4 to work with data imbalance.\n\n[Prompt]\n\nLet’s move on to handling data imbalance.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nHandling data imbalance is crucial in model training, especially in cases\n\nlike yours where the dataset is heavily skewed towards one class. This skew\n\ncan lead the model to predominantly predict the majority class, as we’ve\n\nseen with the Logistic Regression model.",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Strategies to Handle Data Imbalance\n\n1. Resampling Techniques:\n\nOversampling the minority class: This involves randomly\n\nduplicating examples in the minority class.\n\nUndersampling the majority class: This reduces the number of\n\nexamples in the majority class to balance with the minority\n\nclass.\n\n2. Synthetic Data Generation:\n\nSMOTE (Synthetic Minority Over-sampling Technique):\n\nThis technique generates synthetic samples from the minority\n\nclass instead of creating copies.\n\n3. Modify Algorithm:\n\nAdjust the decision threshold: Moving the decision threshold\n\ncan help increase the recall of the minority class.\n\nUse algorithms that are inherently better at dealing with\n\nimbalanced data, such as Decision Trees or Random Forests.\n\n4. Cost-sensitive Learning:\n\nAssign a higher cost to misclassifications of the minority class to\n\nmake the model pay more attention to it.\n\nGiven that we are using Logistic Regression, a good approach to start with\n\ncould be to apply SMOTE to generate synthetic samples for the minority\n\nclass and see if that improves model performance. We can also adjust the\n\ndecision threshold to be more sensitive towards the minority class.\n\nLet’s start by applying SMOTE to the training data and then retraining the\n\nLogistic Regression model. I’ll proceed with this approach:",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "It looks like the imblearn library, which includes SMOTE, isn’t available in\n\nthis environment. Given this limitation, we can explore other options:\n\nUndersampling the Majority Class: This would reduce the size of\n\nthe ‘positive’ class to match the ‘negative’ class, though it can lead to a\n\nloss of potentially useful data.\n\nAdjusting the Decision Threshold: Instead of resampling, we can\n\nadjust the threshold for classifying a review as positive or negative to\n\nmake the model more sensitive to the minority class.\n\nLet’s proceed with adjusting the decision threshold. This method involves\n\nmodifying the probability threshold at which the model classifies a sample\n\nas ‘positive’. By default, this threshold is set at 0.5. Lowering this threshold\n\ncan help in identifying more samples as ‘negative’, potentially improving\n\nrecall for the minority class.\n\nI’ll compute the probabilities of the class predictions and then adjust the\n\nthreshold to see its effect on the classification metrics. Let’s proceed with\n\nthis approach:\n\nAdjusting the decision threshold to 0.4 did not change the classification\n\noutcomes. The model still predicts all reviews as ‘positive’ with the same\n\nmetrics as before, which suggests that we may need to either significantly\n\nlower the threshold or consider other methods due to the strong bias\n\ntowards the ‘positive’ class.\n\nAlternatives and Considerations:\n\nFurther Lower the Threshold: We could continue to experiment with\n\nlower thresholds, although this might eventually lead to higher false\n\npositives.",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "Change the Model or Features: Since logistic regression might be\n\ninherently limited in handling this imbalance, experimenting with\n\ndifferent models or feature sets could be beneficial.\n\nData Resampling Manually: If libraries for automated resampling like imblearn aren’t available, we could manually undersample the\n\n‘positive’ class or oversample the ‘negative’ class, albeit the latter\n\nwithout the sophistication of synthetic sample generation as in\n\nSMOTE.\n\nWould you like to try further lowering the threshold, or should we explore\n\nmanual resampling or perhaps a different modeling approach?\n\n[End of response]\n\nWe see that adjusting the decision threshold didn’t do much, so we will ask\n\nChatGPT-4 to move on to the next feature of hyperparameter tuning.\n\nFeature 5: Hyperparameter tuning We will ask ChatGPT-4 to proceed with hyperparameter tuning:\n\n[Prompt]\n\nLet’s proceed to hyperparameter tuning.\n\n[End of prompt]\n\nWe should receive a response like the following:\n\n[Prompt response]\n\nHyperparameter tuning is a critical step for improving the performance of a\n\nmachine learning model. For Logistic Regression, some key",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "hyperparameters that can be tuned include:\n\nC (Inverse of regularization strength): Smaller values specify\n\nstronger regularization.\n\nSolver: Algorithm to use in the optimization problem (e.g., liblinear,\n\nsag, saga, newton-cg, etc.).\n\nPenalty: Specifies the norm used in the penalization (e.g., l1, l2).\n\nWe can use grid search with cross-validation to systematically explore\n\nmultiple combinations of parameter values. This approach can help identify\n\nthe best settings for these hyperparameters.\n\nImplementing Hyperparameter Tuning\n\nWe will use GridSearchCV from scikit-learn to perform the hyperparameter tuning. We’ll define a grid of potential values for C and\n\npenalty, and then train the model using different combinations of these values to find the best ones.\n\nLet’s set up and execute the grid search:\n\nThe hyperparameter tuning via grid search has identified the best\n\nparameters for the Logistic Regression model:\n\nC (Inverse of regularization strength): 100, indicating less\n\nregularization.\n\nPenalty: ‘l2’, which is the L2 norm used in regularization.\n\nSolver: ‘sag’, which is an optimization algorithm suitable for large\n\ndatasets.\n\nThe best cross-validated accuracy obtained with these settings is 80.5%.\n\nThis suggests that reducing regularization (a higher value of C) helps the\n\nmodel to better fit the data.",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Next Steps:\n\nEvaluate the Model with the Best Parameters: We can now evaluate\n\nthis optimized model on the test set to see how it performs compared\n\nto the previous configuration.\n\nFurther Experimentation: Depending on the results, we might\n\nconsider further tuning or trying out different modeling techniques.\n\nShall we proceed by evaluating the model with these optimized parameters\n\non the test set, or is there anything specific you would like to explore\n\nfurther?\n\n[End of response]\n\nWe will instruct ChatGPT-4 to evaluate the model with optimized\n\nparameters on the test set:\n\n[Prompt]\n\nSure, evaluate.\n\n[End of prompt]\n\nWe should receive a response from ChatGPT-4 similar to the following:\n\n[Prompt response]\n\nThe evaluation of the optimized Logistic Regression model with the best\n\nparameters yields the following results:\n\nOverall Accuracy: 84%\n\nPrecision and Recall for ‘Negative’ class: Precision of 33% and\n\nrecall of 14%.\n\nPrecision and Recall for ‘Positive’ class: Precision of 87% and recall\n\nof 95%.",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Interpretation:\n\nThe overall accuracy has slightly decreased compared to the initial\n\nmodel (which had 86% accuracy), but we see an improvement in\n\nhandling the ‘negative’ class:\n\nThe model now correctly identifies some of the negative reviews,\n\nthough the performance (14% recall) is still quite low, indicating that\n\nthe model can detect a few negative instances but misses many.\n\nThe precision for negative reviews is 33%, which means when the\n\nmodel predicts a review as negative, it is correct about one-third of the\n\ntime.\n\nKey Points:\n\nModel Sensitivity to Negative Reviews: There’s a slight improvement\n\nin identifying negative reviews, which might be due to less\n\nregularization (higher C value) allowing the model to fit more closely\n\nto the minority class.\n\nBalance between Classes: The balance between sensitivity to both\n\nclasses (macro average scores) is still not ideal, indicating that further\n\nadjustments or a different modeling approach may be necessary to\n\nadequately capture the characteristics of the minority class.\n\nFurther Actions:\n\nExperiment with Other Models: Given the inherent limitations of\n\nLogistic Regression with imbalanced datasets, considering other\n\nmodels like Decision Trees, Random Forest, or even ensemble\n\nmethods might yield better results.\n\nAdjust Thresholds or Resample Data: These actions could be\n\nrevisited or combined with different modeling techniques for",
      "content_length": 1406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "potentially better handling of class imbalance.\n\nWould you like to explore alternative models or techniques, or is there\n\nanother aspect of the project you’d like to focus on?\n\n[End of response]\n\nWe have seen that ChatGPT-4 has evaluated the performance and explained\n\nthe results. We will now move on to the feature representation.\n\nFeature 6: Experimenting with feature representation Let’s instruct ChatGPT to provide us with the code for different feature\n\nrepresentations:\n\n[Prompt]\n\nI want to compare the performance of the model trained with different\n\nfeature representations, such as bag-of-words, TF-IDF, or word\n\nembeddings, to determine the most effective approach for sentiment\n\nclassification.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides\n\nus with the code for bag-of-words, TF-IDF, and word embeddings:\n\n[Prompt response]\n\nComparing different feature representations is an excellent approach to\n\nidentify the most effective method for sentiment classification. Let’s outline\n\nthe feature representations we’ll evaluate:",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Bag-of-Words (BoW): Counts the frequency of words within the\n\ndocuments.\n\nTF-IDF: We’ve already implemented this. It considers not only the\n\nfrequencies but also how unique a word is to a document.\n\nWord Embeddings: Uses models like Word2Vec or pre-trained\n\nembeddings from GloVe to represent words in a dense vector space,\n\ncapturing semantic meanings.\n\nStep-by-Step Approach:\n\nImplement Bag-of-Words: We will use scikit-learn’s CountVectorizer to convert text documents into a matrix of token\n\ncounts.\n\nUse Word Embeddings: We’ll apply a simple Word2Vec model using\n\nGensim, or use pre-trained GloVe embeddings for a more\n\nstraightforward implementation.\n\nTrain Logistic Regression on each representation: For consistency,\n\nwe’ll use Logistic Regression for each feature set.\n\nCompare Performance: Evaluate each model using accuracy,\n\nprecision, recall, and F1-score.\n\nLet’s start by implementing the Bag-of-Words model. Afterward, we’ll\n\nproceed to the word embeddings. Here’s the code for the Bag-of-Words\n\nmodel:\n\nThe Logistic Regression model trained with the Bag-of-Words (BoW)\n\nfeature representation has the following performance metrics:\n\nOverall Accuracy: 86%\n\nPrecision and Recall for ‘Negative’ class: Precision of 50% and\n\nrecall of 21%.",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Precision and Recall for ‘Positive’ class: Precision of 88% and recall\n\nof 97%.\n\nBoW Performance Interpretation:\n\nThe BoW model shows a slight improvement in handling the negative class\n\ncompared to the initial TF-IDF model, demonstrating higher precision\n\nthough recall remains low. This indicates that while the model can identify\n\nnegative reviews more accurately than before, it still misses many negative\n\ncases.\n\nNext, let’s implement the word embeddings feature representation using\n\npre-trained GloVe embeddings for simplicity. This will provide us with a\n\ndense representation that captures semantic meanings, which might improve\n\nthe classification, especially for the negative reviews.\n\nImplementing Word Embeddings:\n\nLoad pre-trained GloVe Embeddings: We’ll use embeddings to\n\nconvert text data into a dense vector form.\n\nAverage Word Embeddings: For each review, we’ll compute the\n\naverage of the word embeddings to form a single vector that represents\n\nthe entire review.\n\nLet’s proceed with loading and applying GloVe embeddings for sentiment\n\nclassification:\n\nIt seems we’ve encountered a limitation with accessing the Gensim\n\ndownloader, which is used to load pre-trained GloVe embeddings, due to\n\nthe environment not having internet access.\n\nAlternative Approach:",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Since we can’t use pre-trained GloVe embeddings, let’s consider a simpler\n\nimplementation of word embeddings using the Word2Vec model directly on\n\nour dataset. This will allow us to create our own embeddings based on the\n\nvocabulary present in the reviews.\n\nLet’s proceed by training a Word2Vec model on the processed reviews and\n\nthen using the embeddings to represent each document. Here’s how we’ll do it:\n\nThe Logistic Regression model trained with Word2Vec embeddings\n\ngenerated the following results:\n\nOverall Accuracy: 86%\n\nPrecision and Recall for ‘Negative’ class: Both precision and recall\n\nare 0%. The model failed to correctly identify any of the negative class\n\ninstances.\n\nPrecision and Recall for ‘Positive’ class: Precision of 86% and recall\n\nof 100%.\n\nInterpretation of Word2Vec Model:\n\nThe performance of the Word2Vec model is similar to the original TF-IDF\n\nmodel in terms of accuracy and class-specific metrics. This similarity\n\nindicates that the model still heavily favors the majority class (‘positive’).\n\nThe zero recall and precision for the negative class suggest that despite\n\nusing semantic-rich word embeddings, the imbalance in the dataset still\n\nheavily influences the model’s predictions.\n\nSummary and Recommendations:\n\nBoW vs. TF-IDF vs. Word2Vec: The Bag-of-Words model showed a\n\nslight improvement over TF-IDF and Word2Vec in terms of precision",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "for the negative class. However, all models struggled with recall for\n\nthe negative class, highlighting the impact of class imbalance.\n\nFurther Improvements: Experimentation with ensemble methods,\n\nmore sophisticated neural networks (e.g., LSTM, CNN for text), or\n\nfurther balancing techniques might be necessary to improve recall for\n\nthe negative class.\n\nAlternative Feature Engineering: Additional feature engineering\n\nmethods, such as n-grams, could be explored to capture more context,\n\nwhich might help in identifying negative sentiments more accurately.\n\nWould you like to explore further adjustments, such as different machine\n\nlearning models or additional balancing techniques, or would you like to\n\nfocus on another aspect of your project?\n\n[End of response]\n\nThis concludes our section on how to use ChatGPT-4 or ChatGPT Plus to\n\nimplement and execute the sentiment analysis for the Amazon product\n\nreview dataset.\n\nAssignment Generate synthetic data to handle the data imbalance.\n\nChallenge Try to improve the model performance to greater than 86% and feel free to\n\nuse any technique.\n\nSummary",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "The strategies outlined provided a systematic approach to experimenting\n\nwith different techniques for building and training sentiment analysis\n\nmodels using the Amazon product review dataset. Each step was\n\nmeticulously detailed, with code generation and user interaction tailored to\n\na beginner-friendly experience, ensuring a comprehensive learning journey.\n\nThe task began with constructing a baseline sentiment analysis model.\n\nInitial steps included preprocessing text data by tokenizing, lowercasing,\n\nand removing stopwords and punctuation, followed by feature engineering\n\nusing TF-IDF encoding.\n\nDetailed Python code was provided in a Jupyter Notebook format, complete\n\nwith explanations for each operation, ensuring that even beginners could\n\nfollow along comfortably.\n\nWe explored baseline machine learning models, starting with logistic\n\nregression. The model was trained and evaluated, revealing a significant\n\nclass imbalance that skewed predictions toward the majority class. This\n\nphase included detailed metrics analysis, such as accuracy, precision, recall,\n\nand F1-score, enhancing understanding of model performance beyond mere\n\naccuracy.\n\nTo address the data imbalance, techniques like adjusting the decision\n\nthreshold and experimenting with synthetic data generation methods such as\n\nSMOTE were discussed. However, limitations in the environment prompted\n\na shift to manual approaches like undersampling and threshold adjustments,\n\nwhich were implemented and tested to refine model sensitivity toward the\n\nminority class.\n\nThe learning process was enhanced by hyperparameter tuning using\n\nGridSearchCV, focusing on optimizing parameters like regularization\n\nstrength and solver type. This step improved model performance and",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "provided insights into the impact of model configuration on sentiment\n\nclassification.\n\nThe experimentation extended to comparing different feature\n\nrepresentations – bag-of-words, TF-IDF, and word embeddings – to\n\ndetermine their effectiveness in sentiment analysis. Each technique was\n\nimplemented, and their impact on model performance was critically\n\nevaluated, revealing nuances in how different text representations affect the\n\nability to discern sentiment.\n\nThroughout the process, the strategy of waiting for user feedback before\n\nproceeding ensured that the learning was paced appropriately and that each\n\nstep was clear. This approach facilitated a structured exploration of\n\nsentiment analysis techniques, from basic preprocessing to complex model\n\ntuning.\n\nThe journey concluded with a comprehensive understanding of building and\n\noptimizing sentiment analysis models. The structured, iterative approach –\n\nenhanced by continuous user engagement and feedback – allowed a deep\n\ndive into machine learning model development, from theoretical concepts\n\nto practical implementation.\n\nThis experience not only equipped the user with the knowledge to handle\n\ntext data and model training but also highlighted the challenges and\n\nconsiderations in dealing with imbalanced datasets and choosing the right\n\nmodel and features for sentiment analysis.\n\nIn the next chapter, we will learn how to use ChatGPT to generate code for\n\nlinear regression.",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "13\n\nBuilding a Regression Model for Customer Spend with ChatGPT\n\nIntroduction In the realm of data-driven decision making, understanding customer behavior is pivotal for optimizing business strategies. Building on our exploration of classification techniques, this chapter shifts focus to regression analysis, specifically linear regression, to predict numerical values such as a customer’s annual spending. Linear regression helps us discover relationships within data, enabling predictions based on observed patterns.\n\nThis chapter will guide you through the process of building a predictive model that estimates annual spending by\n\ncustomers based on their interactions with a digital platform. We aim to deepen your understanding of linear\n\nregression, demonstrating how to prepare, process, and utilize datasets to construct accurate and reliable models.\n\nAs we progress, we will explore various techniques to enhance model accuracy and handle complex data\n\nscenarios:\n\nUtilizing advanced regularization techniques to improve model stability and performance.\n\nGenerating synthetic datasets to better understand model behaviors under different data conditions.\n\nStreamlining model development with comprehensive, end-to-end coding examples.\n\nBy the end of this chapter, you will be well equipped with the knowledge and skills necessary to utilize linear\n\nregression for data-driven decision-making in your business. Let’s embark on this journey into regression analysis to optimize customer engagement and revenue generation on our app or website.\n\nIn this chapter, we will:\n\nBuild a regression model with ChatGPT: Readers will learn how ChatGPT can assist in generating Python\n\ncode for building a regression model to predict the yearly amount spent by customers on our app or website using the dataset we have, offering a hands-on approach to understanding and interacting with datasets.\n\nApply prompting techniques: Effective techniques will be introduced to craft prompts that guide ChatGPT in providing the most useful code snippets and insights for regression tasks.\n\nBusiness problem An e-commerce store seeks to optimize customer engagement and increase revenue by gaining deeper insights into\n\ncustomer behavior and preferences. By analyzing various customer attributes and their purchasing patterns, the store aims to tailor its marketing strategies, improve customer retention, and enhance the overall shopping\n\nexperience.",
      "content_length": 2438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Problem and data domain We will employ regression techniques to understand the relationship between yearly spending and other\n\nparameters. Regression is a way to find out whether and how different factors (like time spent on an app or\n\nwebsite) relate to how much customers spend in the online store. It helps us understand and predict customer behavior. By understanding which factors are most influential in driving sales, an e-commerce store can tailor its strategies to enhance these areas and potentially increase revenue.\n\nDataset overview The e-commerce store collects the following information from the customer:\n\nEmail: This is the customer’s email address. It is a unique identifier for each customer and can be used for\n\ncommunication, such as sending order confirmations, newsletters, or personalized marketing offers.\n\nAddress: This refers to the physical address of the customer. It’s crucial for delivering products they have\n\npurchased. Additionally, address data can sometimes provide insights into geographical trends in sales and preferences.\n\nAvatar: This could be a digital representation or image chosen by the user. It might not directly impact sales\n\nor customer behavior, but it can be part of customer engagement strategies, adding a personal touch to user\n\nprofiles.\n\nAvg Session Length: This is the average duration of all sessions combined, in minutes. This is like measuring\n\nhow long a customer spends in your store each time they visit. Imagine someone walking around, looking at products for, say, 33 minutes on average.\n\nTime on App: This the duration of presence on the store’s application, in minutes. Think of it as how long they are browsing through your app, maybe while they are on the bus or waiting in line at the coffee shop.\n\nTime on Website: This is similar to the time on the app, but this is for your website. If they’re using a\n\ncomputer at home or work to look at your store, how long do they stay?\n\nLength of Membership: This is how long these customers have been with your store. Some might be new,\n\nwhile others have been shopping with you for years.\n\nYearly Amount Spent: This is the total amount of money each customer spends at your store in a year, in\n\ndollars.\n\nIn the context of our dataset:\n\nEmail and address: These should be used primarily for transactional purposes unless the customer has\n\nagreed to receive marketing communications. We will not use them for analysis.\n\nAvatar: This can be used to personalize the user experience but does not hold significant analytical value for sales predictions.\n\nOther data: Variables like “Time on App” and “Time on Website” can be analyzed to improve user experience and business strategies without infringing on personal privacy.\n\nIn summary, while data like email, address, and avatar can be valuable for business operations and customer\n\nengagement, they must be handled with a high degree of responsibility, prioritizing the privacy and preferences of\n\nthe customers.\n\nNote that the data used is not a real dataset and hence the emails, addresses, and so on are all made up.",
      "content_length": 3081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "Breaking the problem down into features Given the nature of our dataset, which includes both independent variables (like “Avg. Session Length,” “Time on\n\nApp,” “Time on Website,” and “Length of Membership”) and a dependent variable (“Yearly Amount Spent”), we will start with a simple regression technique with both ChatGPT and ChatGPT Plus or GPT-4. This will include the following high-level steps:\n\n1. Building the model step by step: Users will understand the process of building a machine learning model step by step, including loading the dataset, splitting it into training and testing sets, training the model, making predictions, and evaluating its performance.\n\n2. Apply regularization techniques: Users will learn how to apply regularization techniques such as Ridge regression and Lasso regression with cross-validation to improve the performance of a linear regression model. This includes initializing the models, training them using the training data, and evaluating their performance.\n\n3. Generate a synthetic dataset to add complexity: Users will discover how to generate a synthetic dataset with added complexity using the make_regression function from the sklearn.datasets module. This involves specifying the number of samples, features, and noise levels to mimic real-world data.\n\n4. Generating code to develop a model in a single step for a synthetic dataset: Users will see how to write end-to-end code in a single step to load the synthetic dataset, split it into training and testing sets, train a linear regression model, evaluate its performance, and print the evaluation metrics. This allows for a\n\nstreamlined approach to model development and evaluation.\n\nPrompting strategy To leverage ChatGPT for machine learning, we need to have a clear understanding of how to implement the prompting strategies specifically for code generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better understanding of what needs to go into\n\nthe initial prompt.\n\nStrategy 1: Task-Actions-Guidelines (TAG) prompt strategy 1.1 – Task: The specific task or goal is to create a simple linear regression model to predict the “Yearly Amount\n\nSpent” by dataset based on various attributes in the dataset.\n\n1.2 – Actions: In this case, the strategy is to let ChatGPT decide the steps, hence no specific steps are provided.\n\n1.3 – Guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter Notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the text block of the notebook in detail for each method used in the code before providing the code.",
      "content_length": 2749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "Strategy 2: Persona-Instructions-Context (PIC) prompt strategy 2.1 – Persona: We will adopt the persona of a beginner who needs to learn the different steps of model creation;\n\nhence the code should be generated step by step.\n\n2.2 – Instructions: We have included the step to mount Google Drive explicitly since it’s a common oversight.\n\n2.3 – Context: The most important part is to provide the context of the dataset and exact field names to generate the code that can be executed directly, or to provide the dataset itself in the case of ChatGPT Plus.\n\nStrategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy 3.1 – Learn:\n\nWe want to learn about linear regression and how it works.\n\nUnderstand feature engineering techniques and model evaluation metrics.\n\nWe want to learn how to create a synthetic dataset.\n\n3.2 – Improvise:\n\nWe will use it later while applying regularization techniques.\n\n3.3 – Feedback:\n\nIf the code provided results in any errors, then feedback should be provided back to ChatGPT. We applied it\n\nin the Lasso and Ridge code execution using ChatGPT Plus.\n\n3.4 – Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity. This is used throughout the chapter.\n\nBuilding a simple linear regression model to predict the “Yearly Amount Spent” by customers using the free version of ChatGPT When using the free version, it’s important to give ChatGPT a clear description of the dataset first, which serves as\n\nan effective approach to generating code, followed by user evaluation. ChatGPT is already trained very well on\n\nPython and a machine learning algorithm; hence, we do not need to provide specific steps here. To verify that the\n\ngenerated code functions correctly, we will be using Google Colab.\n\nFeature 1: Building the model step by step Let’s craft our initial prompt for the classification model.\n\n[Prompt]",
      "content_length": 1869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "I want to create a simple linear regression model to predict the “Yearly Amount Spent” (TAG 1.1) by customers for\n\nthe e-commerce customers dataset which consists of the following columns (PIC 2.3):\n\n1. Email: The electronic mail unique to each customer, used here as an identification.\n\n2. Address: The physical geographical location of each customer.\n\n3. Avatar: This is the visual anonymity representation of the customers in the database.\n\n4. Avg. Session Length: The average duration of all session combined in minutes.\n\n5. Time on App: This the duration of presence on the stores’ application, in minutes.\n\n6. Time on Website: This is the time duration on the website of the store, in minutes.\n\n7. Length of Membership: This is the amount of time a customer remains a registered user of the store’s online\n\nresources.\n\n8. Yearly Amount Spent: This is the cost implication of accessing the store’s online resources, paid by the\n\ncustomers, in dollars.\n\nI am a beginner user (PIC 2.1), so please provide step by step code and strictly follow following instructions (PIC\n\n2.2):\n\nCode should be compatible with Jupyter notebook\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied in the text block of the notebook in detail for\n\neach method used in code before providing the code.\n\nAlso note that the dataset is placed in Google Drive at this path '/content/drive/My\n\nDrive/Chapter05/Ecommerce Customers.csv ' (TAG 1.2).\n\nSo, add the code to mount the drive, load the dataset, provide one step at a time, and wait for users feedback.\n\n[End of prompt]\n\nYou should receive a response similar to the following.",
      "content_length": 1687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "Figure 13.1: The initial response from ChatGPT\n\n[Prompt response]\n\nSure, let’s proceed step by step. First, we need to mount Google Drive in our Jupyter Notebook environment so we\n\ncan access the dataset.\n\nHere’s how you can do it:\n\nfrom google.colab import drive # Mount Google Drive drive.mount('/content/drive')\n\nExplanation:\n\nfrom google.colab import drive: This line imports the drive module from the google.colab package.\n\ndrive.mount('/content/drive'): This command mounts your Google Drive at the specified path /content/drive. When you run this cell, it will prompt you to authenticate and give you a link to follow to\n\nget an authorization code. Once you enter the code, your Google Drive will be mounted.\n\nLet me know if you have any questions before we proceed.\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy and validate the code by executing the code in Google Colab\n\nfor every step. The code runs perfectly fine and mounts Google Drive.\n\nPlease note that it requires permission to access the drive and would prompt the user.",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Please refer to the notebook Learning simple regression with ChatGPT.ipynb for the executable code snippets or\n\ncopy the code from the prompt response.\n\nRunning your cell, you should see a message that says “Mounted at /content/drive as a result of calling drive.mount.”\n\nWe will now instruct ChatGPT to proceed to the next step:\n\n[Prompt]\n\nLet’s proceed\n\n[End of prompt]\n\nWe should receive a response similar to the following to load the dataset into the notebook:\n\n[Prompt response]\n\nGreat! Now that we have mounted Google Drive, we can load the dataset into our notebook.\n\nHere’s how you can load the dataset:\n\nimport pandas as pd # Define the file path file_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' # Load the dataset into a DataFrame ecommerce_data = pd.read_csv(file_path) # Display the first few rows of the DataFrame print(ecommerce_data.head())\n\nExplanation:\n\nimport pandas as pd: This line imports the pandas library and aliases it as pd, which is a common\n\nconvention.\n\nfile_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv': This line defines the file\n\npath where the dataset is located in your Google Drive.\n\npd.read_csv(file_path): This function reads the CSV file located at the specified path and creates a DataFrame from it.\n\nprint(ecommerce_data.head()): This line prints the first few rows of the DataFrame to verify that the dataset\n\nhas been loaded correctly.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nWe executed the code provided by ChatGPT in the notebook and it loads the dataset from the defined path and\n\nprints the first few rows. As we had given the path of the file in the initial prompt, we don’t need to change anything in the code and it runs perfectly fine. The takeaway is that we need to be specific with file paths and\n\nlocations where required.",
      "content_length": 1888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "Figure 13.2: Loading the dataset\n\nNext, we’ll provide the output to ChatGPT and then proceed to Step 1. In this instance, we’ve employed the\n\nstrategy of providing additional context to ChatGPT by presenting the output textually.\n\nIt is important to note that since we are utilizing GPT-3.5, which lacks the capability to execute code or process images, we are limited to providing textual outputs from the notebook:\n\n[Prompt]\n\nHere is the output - Email \\ 0 mstephenson@fernandez.com 1 hduke@hotmail.com 2 pallen@yahoo.com 3 riverarebecca@gmail.com 4 mstephens@davidson-herman.com Address Avatar \\ 0 835 Frank Tunnel\\nWrightmouth, MI 82180-9605 Violet 1 4547 Archer Common\\nDiazchester, CA 06566-8576 DarkGreen 2 24645 Valerie Unions Suite 582\\nCobbborough, D... Bisque 3 1414 David Throughway\\nPort Jason, OH 22070-1220 SaddleBrown 4 14023 Rodriguez Passage\\nPort Jacobville, PR 3... MediumAquaMarine Avg. Session Length Time on App Time on Website Length of Membership \\",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "0 34.497268 12.655651 39.577668 4.082621 1 31.926272 11.109461 37.268959 2.664034 2 33.000915 11.330278 37.110597 4.104543 3 34.305557 13.717514 36.721283 3.120179 4 33.330673 12.795189 37.536653 4.446308 Yearly Amount Spent 0 587.951054 1 392.204933 2 487.547505 3 581.852344 4 599.406092\n\n[End of prompt]\n\nWe anticipate receiving a response similar to the following, wherein ChatGPT will offer guidance on the\n\nsubsequent steps. Initially, we refrained from specifying any particular steps in the initial prompt, opting instead to allow ChatGPT to determine the subsequent course of action:\n\n[Prompt response]\n\nGreat! The dataset has been loaded successfully. Now, we can proceed with building our linear regression model to\n\npredict the “Yearly Amount Spent” by customers.\n\nThe steps involved in building a simple linear regression model are as follows:\n\nData Preparation: Split the dataset into input features (X) and the target variable (y).\n\nSplit Data: Split the dataset into training and testing sets.\n\nModel Initialization: Create a linear regression model object.\n\nModel Training: Fit the model to the training data.\n\nModel Evaluation: Evaluate the model’s performance using the testing data.\n\nLet’s start with step 1:\n\nStep 1: Data Preparation:\n\n# Extract input features (X) and target variable (y) X = ecommerce_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership y = ecommerce_data['Yearly Amount Spent']\n\nExplanation:\n\nX = ecommerce_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']]: This line selects the input features ('Avg. Session Length', 'Time on App', 'Time on\n\nWebsite', 'Length of Membership') from the dataset and assigns them to the variable X.\n\ny = ecommerce_data['Yearly Amount Spent']: This line selects the target variable 'Yearly Amount Spent'\n\nfrom the dataset and assigns it to the variable y.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nChatGPT has outlined the steps it will adhere to for code generation. Furthermore, it has furnished the code for the initial step, data preparation, wherein the dataset is divided into features and the target variable. Notably, it adeptly",
      "content_length": 2241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "discerned the features by excluding email addresses, avatars, and addresses. Additionally, it precisely employed the column names as stipulated in the description provided in the initial prompt. The strategy employed here involves\n\nincluding the dataset column names in the prompt to ensure compatibility with the generated code. Hence, the code executes without any modifications or errors.\n\nFigure 13.3: Step 1 – data preparation\n\nNext, we will instruct ChatGPT to proceed:\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response similar to the following to split the data along with the explanation:\n\n[Prompt response]\n\nGreat! Now that we have prepared our data by separating the input features (X) and the target variable (y), we can proceed to the next step, which is splitting the data into training and testing sets.\n\nStep 2: Split Data:\n\nfrom sklearn.model_selection import train_test_split # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Display the shapes of the training and testing sets print(\"Training set shape - X_train:\", X_train.shape, \"y_train:\", y_train.shape) print(\"Testing set shape - X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n\nExplanation:\n\nfrom sklearn.model_selection import train_test_split: This line imports the train_test_split function\n\nfrom the sklearn.model_selection module, which is used to split the dataset into training and testing sets.\n\ntrain_test_split(X, y, test_size=0.2, random_state=42): This function splits the input features (X) and\n\nthe target variable (y) into training and testing sets. The test_size parameter specifies the proportion of the dataset to include in the testing set (in this case, 20% for testing). The random_state parameter is used for\n\nreproducibility, ensuring that the same random split is generated each time the code is run.\n\nX_train, X_test, y_train, y_test: These variables hold the training and testing sets for the input features (X) and the target variable (y), respectively.\n\nprint(\"Training set shape - X_train:\", X_train.shape, \"y_train:\", y_train.shape): This line prints the shapes of the training set input features (X_train) and target variable (y_train).\n\nprint(\"Testing set shape - X_test:\", X_test.shape, \"y_test:\", y_test.shape): This line prints the shapes of the testing set input features (X_test) and target variable (y_test).\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]",
      "content_length": 2577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "The code executed successfully, and the data was split into training and testing sets with the expected 80-20 ratio. Notably, we did not explicitly specify the ratio, yet ChatGPT defaulted to the standard 80-20 split.\n\nFigure 13.4: Step 2 – split the data\n\nNext, we will provide the output of the previous step to ChatGPT, which should let ChatGPT know about the\n\nshape and it should also provide us the code for the next step:\n\n[Prompt]\n\nTraining set shape - X_train: (400, 4) y_train: (400,) Testing set shape - X_test: (100, 4) y_test: (100,)\n\n[End of prompt]\n\nWe should receive a response similar to the following where the model is initialized:\n\n[Prompt response]\n\nPerfect! We have successfully split our dataset into training and testing sets. The training set contains 400 samples, while the testing set contains 100 samples. Each sample consists of four input features and one target variable.\n\nNow, let’s proceed to the next step, which is initializing our linear regression model.\n\nStep 3: Model Initialization:\n\nfrom sklearn.linear_model import LinearRegression # Initialize the linear regression model model = LinearRegression()\n\nExplanation:\n\nfrom sklearn.linear_model import LinearRegression: This line imports the LinearRegression class from\n\nthe sklearn.linear_model module, which is used to create a linear regression model.\n\nmodel = LinearRegression(): This line initializes a linear regression model object and assigns it to the\n\nvariable model.\n\nLet me know if you have any questions or if you’d like to proceed to the next step!\n\n[End of response]\n\nThe code is executed perfectly fine without any errors. Necessary imports are also provided by ChatGPT.",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "Figure 13.5: Step 3 – model initialization\n\nWe’ll simply direct ChatGPT to continue. It’s worth noting that because our initial prompt included the instruction\n\nto provide step-by-step code, we don’t need to tailor our subsequent prompts to each individual step or task. Instead, we’ll allow ChatGPT to lead us, providing it with the outputs from the preceding steps as necessary:\n\n[Prompt]\n\nLet’s proceed.\n\n[End of prompt]\n\nWe should receive a response similar to the following to train the model:\n\n[Prompt response]\n\nGreat! Now that we have initialized our linear regression model, we can move on to the next step, which is training the model using the training data.\n\nStep 4: Model Training:\n\n# Train the linear regression model using the training data model.fit(X_train, y_train)\n\nExplanation:\n\nmodel.fit(X_train, y_train): This line fits (or trains) the linear regression model using the training data.\n\nThe fit method takes the input features (X_train) and the corresponding target variable (y_train) as arguments and adjusts the model’s parameters to minimize the difference between the actual and predicted values.\n\nNow that our model has been trained, we can proceed to the final step, which is evaluating the model’s performance using the testing data.\n\nLet me know if you have any questions or if you’d like to proceed!\n\n[End of response]\n\nWe will execute the code and that should train the model.\n\nFigure 13.6: Step 4 – model training",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "We will instruct ChatGPT to proceed:\n\n[Prompt]\n\nLet’s proceed\n\n[End of prompt]\n\nWe should receive a response similar to the following to evaluate the performance of the model using test data:\n\n[Prompt response]\n\nGreat! Now that our linear regression model has been trained, we can evaluate its performance using the testing\n\ndata.\n\nStep 5: Model Evaluation:\n\n# Make predictions on the testing data y_pred = model.predict(X_test) # Evaluate the model's performance from sklearn.metrics import mean_squared_error, r2_score # Calculate Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_pred) # Calculate R-squared (coefficient of determination) r_squared = r2_score(y_test, y_pred) # Print the evaluation metrics print(\"Mean Squared Error (MSE):\", mse) print(\"R-squared (Coefficient of Determination):\", r_squared)\n\nExplanation:\n\nmodel.predict(X_test): This line uses the trained model to make predictions on the input features (X_test),\n\nwhich generates the predicted values of the target variable.\n\nfrom sklearn.metrics import mean_squared_error, r2_score: This line imports the mean_squared_error and r2_score functions from the sklearn.metrics module, which are used to evaluate the model’s\n\nperformance.\n\nmean_squared_error(y_test, y_pred): This function calculates the mean squared error (MSE) between the actual target variable (y_test) and the predicted values (y_pred).\n\nr2_score(y_test, y_pred): This function calculates the R-squared (coefficient of determination) between the actual target variable (y_test) and the predicted values (y_pred).\n\nprint(\"Mean Squared Error (MSE):\", mse): This line prints the mean squared error (MSE), which measures the average squared difference between the actual and predicted values. A lower MSE indicates better model\n\nperformance.\n\nprint(\"R-squared (Coefficient of Determination):\", r_squared): This line prints the R-squared value,\n\nwhich represents the proportion of the variance in the target variable that is explained by the model. A higher\n\nR-squared value (closer to 1) indicates better fit.\n\nThese evaluation metrics will help us assess how well our linear regression model is performing in predicting the “Yearly Amount Spent” by customers.\n\nLet me know if you have any questions or if you’d like further explanation on any part!\n\n[End of response]",
      "content_length": 2313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "We execute the code and it runs perfectly fine without any errors and prints the mean-squared error (MSE) and\n\nthe coefficient of determination (R-squared).\n\nFigure 13.7: Model evaluation output\n\nFor those new to machine learning, it’s crucial to assess whether the model performance meets acceptable\n\nstandards. Once more, we’ll seek assistance from ChatGPT and furnish it with the output for evaluation:\n\n[Prompt]\n\nMean Squared Error (MSE): 109.8637411839399 R-squared (Coefficient of Determination): 0.9778130629184127\n\n[End of prompt]\n\nWe should receive a response from ChatGPT explaining the results, which suggests that our model is actually\n\nperforming very well given that R-squared is very close to 1:\n\n[Prompt response]\n\nThe evaluation metrics indicate that our linear regression model performs quite well on the testing data:\n\nMean Squared Error (MSE): The MSE value is approximately 109.86. This indicates that, on average, the\n\nsquared difference between the actual and predicted values of the “Yearly Amount Spent” is around 109.86. Lower MSE values are desirable, indicating better model performance.\n\nR-squared (Coefficient of Determination): The R-squared value is approximately 0.978. This indicates that\n\napproximately 97.8% of the variance in the “Yearly Amount Spent” can be explained by the linear regression model. A higher R-squared value closer to 1 suggests that the model provides a better fit to the data.\n\nOverall, these evaluation metrics suggest that our linear regression model is performing well in predicting the\n\n“Yearly Amount Spent” by customers based on the provided input features.\n\nIf you have any further questions or need additional assistance, feel free to ask!",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "[End of response]\n\nHaving now acquired proficiency in utilizing ChatGPT to construct a linear regression model incrementally, we’ll transition to the second feature and delve into the application of regularization techniques. While our current model exhibits satisfactory performance, regularization can prove invaluable, particularly in scenarios where the model is either underfitted or overfitted.\n\nFeature 2: Applying regularization techniques We will use the learn and improvise prompting strategy (LIFE 3.1 and 3.2) with ChatGPT to understand whether\n\nthe model could be overfitted and what should be done in such a case:\n\n[Prompt]\n\nCould the model be overfitting?\n\n[End of prompt]\n\nWe receive the response from ChatGPT indicating that it is unlikely that our model is overfitted but it does suggest some things to consider such as regularization techniques and cross-validation:\n\n[Prompt response]\n\nIt’s essential to consider the possibility of overfitting when interpreting the model’s performance. Overfitting\n\noccurs when the model learns to capture noise or random fluctuations in the training data, rather than the\n\nunderlying relationships between the input features and the target variable. This can result in a model that performs well on the training data but generalizes poorly to unseen data.\n\nHere are a few indicators that suggest overfitting may be occurring:\n\nLarge Discrepancy Between Training and Testing Performance: If the model performs significantly better on the training data compared to the testing data, it may be overfitting. In our case, if the MSE and R-squared values are much better on the training set than on the testing set, it could indicate overfitting.\n\nComplexity of the Model: Overly complex models with a large number of parameters relative to the size of the training data are more prone to overfitting. Linear regression is relatively simple, but if you were using a more complex model (e.g., polynomial regression with high-degree polynomials), it could increase the risk of overfitting.\n\nRegularization Techniques: Regularization techniques like Lasso or Ridge regression can be applied to mitigate overfitting by penalizing large coefficients. If overfitting is suspected, applying regularization may help improve the model’s generalization performance.\n\nCross-Validation: Using cross-validation techniques, such as k-fold cross-validation, can provide a more robust estimate of the model’s performance by evaluating it on multiple train-test splits of the data. If the\n\nmodel’s performance varies significantly across different splits, it could indicate overfitting.\n\nIn our case, with a high R-squared value and a relatively low MSE on the testing data, it’s less likely that the model is overfitting. However, it’s always a good practice to explore these indicators and consider additional steps to ensure the model’s generalization performance. If you suspect overfitting, you could try simplifying the model, applying regularization techniques, or using cross-validation to further evaluate its performance.\n\n[End of response]",
      "content_length": 3082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "We will ask ChatGPT to apply regularization and cross-validation techniques:\n\n[Prompt]\n\nLet’s try applying regularization techniques first and then cross-validation\n\n[End of prompt]\n\nChatGPT should provide us with the code to apply either Lasso or Ridge along with cross-validation and also the explanation:\n\n[Prompt response]\n\nSure! Let’s start by applying regularization techniques to our linear regression model. Two common regularization techniques are Lasso regression (L1 regularization) and Ridge regression (L2 regularization). These techniques add a penalty term to the loss function, which helps prevent overfitting by penalizing large coefficients.\n\nWe’ll begin by trying Ridge regression, which adds the squared sum of the coefficients (L2 norm) to the loss function.\n\nHere’s how you can implement Ridge regression with cross-validation:\n\nfrom sklearn.linear_model import RidgeCV # Initialize the Ridge regression model with cross-validation ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5) # Train the Ridge regression model ridge_model.fit(X_train, y_train) # Make predictions on the testing data ridge_y_pred = ridge_model.predict(X_test) # Evaluate the model's performance ridge_mse = mean_squared_error(y_test, ridge_y_pred) ridge_r_squared = ridge_model.score(X_test, y_test) # Print the evaluation metrics print(\"Ridge Regression - Mean Squared Error (MSE):\", ridge_mse) print(\"Ridge Regression - R-squared (Coefficient of Determination):\", ridge_r_squared) print(\"Ridge Regression - Best Alpha:\", ridge_model.alpha_)\n\nExplanation:\n\nRidgeCV(alphas=[0.1, 1.0, 10.0], cv=5): This initializes a Ridge regression model with cross-validation. The alphas parameter specifies the regularization strength (alpha values) to be tested, and cv specifies the\n\nnumber of folds for cross-validation.\n\nridge_model.fit(X_train, y_train): This trains the Ridge regression model using the training data.\n\nridge_model.predict(X_test): This makes predictions on the testing data using the trained Ridge regression\n\nmodel.\n\nridge_mse = mean_squared_error(y_test, ridge_y_pred): This calculates the mean squared error (MSE) between the actual and predicted values using Ridge regression.\n\nridge_model.score(X_test, y_test): This calculates the R-squared value (coefficient of determination)\n\nusing Ridge regression.\n\nridge_model.alpha_: This attribute returns the best alpha (regularization strength) selected by cross- validation.\n\nLet me know if you’d like to proceed with Ridge regression or if you have any questions!",
      "content_length": 2522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "[End of response]\n\nLet’s try executing the code and see whether it works and shows any improvement.\n\nFigure 13.8: Ridge regression\n\nThe code is compatible with previously provided code and the output is very similar to the linear regression code.\n\nBut note that ChatGPT doesn’t need any specific instructions and it could generate a perfectly fine executable code. Next, we will provide ChatGPT with the output and see what it suggests:\n\n[Prompt]\n\nRidge Regression - Mean Squared Error (MSE): 109.82024999312038 Ridge Regression - R-squared (Coefficient of Determination): 0.9778218459464064 Ridge Regression - Best Alpha: 0.1\n\n[End of prompt]\n\nIn this case, ChatGPT has interpreted the results and also provided the code for Lasso regression with cross- validation. The response may vary depending on the suggestions provided by ChatGPT in previous interactions:\n\n[Prompt response]\n\nIt looks like the Ridge regression model with cross-validation performed quite similarly to the standard linear regression model. The Mean Squared Error (MSE) and R-squared values are very close to those obtained from the standard linear regression model.\n\nThe best alpha selected by cross-validation for the Ridge regression model is 0.1, indicating that a small amount of\n\nregularization was applied to the model.\n\nSince the performance of Ridge regression with cross-validation is comparable to that of the standard linear regression model, we can conclude that regularization did not significantly improve the model’s performance in this case.",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "Next, let’s try using Lasso regression (L1 regularization) with cross-validation to see if it yields any improvement:\n\nfrom sklearn.linear_model import LassoCV # Initialize the Lasso regression model with cross-validation lasso_model = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5) # Train the Lasso regression model lasso_model.fit(X_train, y_train) # Make predictions on the testing data lasso_y_pred = lasso_model.predict(X_test) # Evaluate the model's performance lasso_mse = mean_squared_error(y_test, lasso_y_pred) lasso_r_squared = lasso_model.score(X_test, y_test) # Print the evaluation metrics print(\"Lasso Regression - Mean Squared Error (MSE):\", lasso_mse) print(\"Lasso Regression - R-squared (Coefficient of Determination):\", lasso_r_squared) print(\"Lasso Regression - Best Alpha:\", lasso_model.alpha_)\n\nLet me know if you’d like to proceed with Lasso regression or if you have any questions!\n\n[End of response]\n\nWe will execute the code and see whether it works and produces any different results than Ridge regression.\n\nFigure 13.9: Lasso regression\n\nLasso regression with cross-validation also produced similar results to both the standard linear regression and Ridge regression models. The MSE and R-squared values are very close to those obtained previously.\n\nHowever, it’s important to note that the dataset used here may not be complex enough to benefit significantly from\n\nregularization techniques. In cases where the dataset is larger or more complex, regularization may play a more\n\ncrucial role in preventing overfitting and improving model generalization.\n\nLet’s move to the next feature to generate a synthetic dataset for our problem to add complexity.",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Feature 3: Generating a synthetic dataset to add complexity We will use the conversation style with ChatGPT and ask it to provide code to generate a synthetic dataset:\n\n[Prompt]\n\nCan you please provide me a code to generate the synthetic dataset for it to add complexity?\n\n[End of prompt]\n\nYou may receive a variant of the following response. However, you may notice that this is generic code and is not suitable for generating the code specific to our dataset. Hence, we will need to refine the prompt by providing\n\nfeedback to ChatGPT:\n\n[Prompt response]\n\nSure! We can generate a synthetic dataset with additional complexity using the make_regression function from the\n\nsklearn.datasets module. This function generates a random regression problem with specified characteristics, such as the number of samples, the number of features, and the noise level.\n\nHere’s how you can generate a synthetic dataset with added complexity:\n\nfrom sklearn.datasets import make_regression import pandas as pd # Generate synthetic dataset with 500 samples, 10 features, and increased noise X_synthetic, y_synthetic = make_regression(n_samples=500, n_features=10, noise=50, random_state=42 # Create a DataFrame from the synthetic dataset synthetic_data = pd.DataFrame(X_synthetic, columns=[f'Feature_{i}' for i in range(1, 11)]) synthetic_data['Target'] = y_synthetic # Display the first few rows of the synthetic dataset print(synthetic_data.head()) # Save the synthetic dataset to a CSV file synthetic_data.to_csv('synthetic_dataset.csv', index=False)\n\nExplanation:\n\nmake_regression(n_samples=500, n_features=10, noise=50, random_state=42): This function generates a\n\nsynthetic regression dataset with 500 samples, 10 features, and increased noise level (specified by the noise parameter). The random_state parameter ensures reproducibility.\n\npd.DataFrame(X_synthetic, columns=[f'Feature_{i}' for i in range(1, 11)]): This creates a DataFrame from the synthetic input features (X_synthetic) and assigns column names 'Feature_1' to 'Feature_10'.\n\nsynthetic_data['Target'] = y_synthetic: This adds the synthetic target variable (y_synthetic) to the DataFrame with the column name 'Target'.\n\nsynthetic_data.to_csv('synthetic_dataset.csv', index=False): This saves the synthetic dataset to a CSV file named 'synthetic_dataset.csv' without including the index column.\n\nYou can adjust the parameters of the make_regression function to control the complexity of the synthetic dataset, such as the number of samples, the number of features, and the noise level.\n\nLet me know if you need further assistance!",
      "content_length": 2584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "[End of response]\n\nWe will simply instruct ChatGPT that we need the code for our dataset:\n\n[Prompt]\n\nNo, I meant for our problem.\n\n[End of prompt]\n\nLet’s see what ChatGPT suggests and whether it really works:\n\n[Prompt response]\n\nGot it! We can create a synthetic dataset for our problem by using the same input features as the original dataset\n\nbut with added complexity. We’ll generate additional features that are nonlinear combinations of the original features and add noise to mimic real-world data.\n\nHere’s how you can generate a synthetic dataset with added complexity for our problem:\n\nimport numpy as np import pandas as pd # Set random seed for reproducibility np.random.seed(42) # Number of samples n_samples = 500 # Generate synthetic input features avg_session_length = np.random.normal(loc=30, scale=5, size=n_samples) time_on_app = np.random.normal(loc=15, scale=5, size=n_samples) time_on_website = np.random.normal(loc=20, scale=7, size=n_samples) length_of_membership = np.random.normal(loc=3, scale=1, size=n_samples) # Generate synthetic target variable (linear combination of input features with noise) yearly_amount_spent = 100 + (avg_session_length * 10) + (time_on_app * 20) + (time_on_website * 15 # Create a DataFrame from the synthetic dataset synthetic_data = pd.DataFrame({ 'Avg. Session Length': avg_session_length, 'Time on App': time_on_app, 'Time on Website': time_on_website, 'Length of Membership': length_of_membership, 'Yearly Amount Spent': yearly_amount_spent }) # Display the first few rows of the synthetic dataset print(synthetic_data.head()) # Save the synthetic dataset to a CSV file synthetic_data.to_csv('synthetic_ecommerce_data.csv', index=False)\n\nExplanation:\n\nWe generate synthetic input features (avg_session_length, time_on_app, time_on_website,\n\nlength_of_membership) using normal distributions with specified mean and standard deviation to mimic the characteristics of the original dataset.\n\nWe generate the synthetic target variable (yearly_amount_spent) as a linear combination of the input features\n\nwith added noise.\n\nFinally, we create a DataFrame from the synthetic dataset and save it to a CSV file.\n\nThis synthetic dataset will have similar characteristics to the original dataset but with added complexity, which can\n\nbe useful for testing models and understanding their performance in more challenging scenarios.",
      "content_length": 2375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Let me know if you need further clarification or assistance!\n\n[End of response]\n\nThe code looks impressive and is adapted to our dataset. On executing, it runs without any errors and generates a CSV file named synthetic_ecommerce_data.csv and stores it in the runtime environment of the notebook. It also\n\nprints the first few rows. You can also download it for future use.\n\nNext, we will request ChatGPT to generate the complete code and evaluate model performance with the added\n\nnoise in the synthetic dataset. Let’s move to our next feature.\n\nFeature 4: Generating code to develop a model in a single step for a synthetic dataset We have learned how to create a regression model step by step in previous features, hence we will now request\n\nChatGPT to provide the entire code in a single step for the synthetic dataset:\n\n[Prompt]\n\nNow provide me the end-to-end code in a single step to run using the generated synthetic dataset.\n\n[End of prompt]\n\nWe should receive a response similar to the following where complete code is provided, including loading the\n\ndataset, splitting the dataset, model training, and model evaluation:\n\n[Prompt response]\n\nCertainly! Below is the end-to-end code that loads the synthetic dataset, splits it into training and testing sets,\n\ntrains a linear regression model, evaluates its performance, and prints the evaluation metrics.\n\nimport pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score # Load the synthetic dataset synthetic_data = pd.read_csv('synthetic_ecommerce_data.csv') # Extract input features (X) and target variable (y) X = synthetic_data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership y = synthetic_data['Yearly Amount Spent'] # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the linear regression model model = LinearRegression() # Train the linear regression model using the training data model.fit(X_train, y_train) # Make predictions on the testing data y_pred = model.predict(X_test) # Evaluate the model's performance mse = mean_squared_error(y_test, y_pred) r_squared = r2_score(y_test, y_pred) # Print the evaluation metrics print(\"Mean Squared Error (MSE):\", mse) print(\"R-squared (Coefficient of Determination):\", r_squared)",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "This code will load the synthetic dataset from the CSV file, split it into training and testing sets, train a linear\n\nregression model using the training data, make predictions on the testing data, evaluate the model’s performance using Mean Squared Error (MSE) and R-squared (Coefficient of Determination), and print the evaluation metrics.\n\nYou can simply run this code in your Python environment, assuming you have the pandas library installed to work\n\nwith DataFrames and the scikit-learn library installed for machine learning functionalities.\n\nLet me know if you have any questions or if there’s anything else I can assist you with!\n\n[End of response]\n\nWe execute the code and it runs seamlessly.\n\nThe evaluation metrics for the linear regression model trained on the synthetic dataset are as follows:\n\nMSE: Approximately 2103.19\n\nR-squared (coefficient of determination): Approximately 0.9197\n\nThese metrics provide insight into the model’s performance:\n\nMSE: The MSE measures the average squared difference between the actual and predicted values. In this\n\ncase, the MSE indicates that, on average, the squared difference between the actual and predicted yearly\n\namount spent is approximately 2103.19. Lower MSE values are desirable, indicating better model performance.\n\nR-squared (coefficient of determination): The R-squared value represents the proportion of the variance in\n\nthe target variable (Yearly Amount Spent) that is explained by the model. In this case, the R-squared value of\n\napproximately 0.9197 suggests that approximately 91.97% of the variance in the yearly amount spent can be explained by the linear regression model. A higher R-squared value closer to 1 indicates a better fit.\n\nOverall, these evaluation metrics suggest that the linear regression model performs reasonably well on the\n\nsynthetic dataset.\n\nIn the context of the online e-commerce store, the insights from these models can be used for various purposes:\n\nCustomer segmentation: The models can help identify high-value customers based on their predicted Yearly Amount Spent, allowing for targeted marketing efforts.\n\nRecommendation systems: Understanding the factors that influence spending can be used to improve\n\nproduct recommendations and enhance the customer shopping experience.\n\nRetention strategies: Length of Membership appears to be an important factor. Strategies to increase customer retention and loyalty can be developed based on this insight.\n\nOptimizing website and app: Insights into the impact of Average Session Length, Time on App, and Time\n\non Website can guide decisions on website and app optimization.\n\nThis concludes our learning with ChatGPT 3.5. Now, let’s see how prompting and learning differ with ChatGPT\n\nPlus.\n\nLearning simple linear regression using ChatGPT Plus",
      "content_length": 2790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "Now that we have learned about simple linear regression while using ChatGPT, let’s learn about linear regression\n\nwhile using ChatGPT Plus. ChatGPT Plus, or GPT-4, includes the ability to upload datasets, analyze the dataset, and produce results if using simple libraries such as Python and Matplotlib. At the time of writing this chapter, it had a GPT assistant named Data Analysis, provided by OpenAI when using the mobile app. You could also select\n\nGPT-4 if using the browser version.\n\nNote that you can also achieve the same using the free version but would need to execute the code yourself and modify the prompt.\n\nFeature 1: Building a simple linear regression model step by step We have learned how to create a regression model step by step using the free version of ChatGPT where we had to\n\nprovide the dataset description, instructions on from where to load, and code compatible with Jupyter Notebook. In the case of ChatGPT Plus, all of that is taken care of by ChatGPT Plus and it also provides a runtime environment to execute the code.\n\nNote that it cannot run TensorFlow, Keras, or deep learning libraries:\n\n[Prompt]\n\nI want to create a simple linear regression model to predict the “Yearly Amount Spent” (TAG 1.1) by customers for the attached e-commerce customers dataset (PIC 2.3)\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and strictly follow the following instructions (PIC 2.2):\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code and provide one step at a time and wait for users feedback\n\n[End of prompt]\n\nWe should receive a response similar to the following where the complete code is provided, including loading the\n\ndataset, splitting the dataset, model training, and model evaluation:\n\n[Prompt response]\n\nGreat! Let’s start building a simple linear regression model using Python. We’ll be using libraries such as pandas for handling the data and sklearn for creating the regression model.\n\nStep 1: Import Libraries and Load Data\n\nFirst, we’ll import necessary libraries and load your dataset. We’ll also take a look at the first few rows of the dataset to understand its structure.\n\nHere’s the code for this step:\n\n# Import necessary libraries import pandas as pd # pandas is used for data manipulation and analysis from sklearn.model_selection import train_test_split # This function is used to split data into t from sklearn.linear_model import LinearRegression # This class is used to perform linear regressi from sklearn.metrics import mean_absolute_error, mean_squared_error # These functions are used to # Load the dataset",
      "content_length": 2633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "data = pd.read_csv('/mnt/data/Ecommerce Customers (1).csv') # Reading the CSV file into a pandas # Display the first few rows of the dataset to understand its structure print(data.head())\n\nThis block of code does the following:\n\nImports libraries: We load pandas for data handling, and several modules from sklearn for splitting the data\n\nand performing linear regression.\n\nLoads the data: The pd.read_csv function reads the CSV file into a DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types.\n\nPrints the first few rows: This helps us see the initial rows of our dataset to understand the available data and\n\nits format.\n\nLet’s execute this step and check the output to ensure everything is loaded correctly and to understand what your\n\ndata looks like. Shall we proceed?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the first\n\nfew rows.\n\nNote that the file is loaded in ChatGPT’s run time environment; if you need to execute code in your local\n\nenvironment or Jupyter Colab, you would be required to modify the step to read CSV:\n\n[Prompt response]\n\nHere’s the snapshot of the first few rows of your dataset:\n\nEmail\n\nAddress\n\nAvatar\n\nAvg. Session Length\n\nTime on App\n\nTime on Website\n\nmstephenson@fernandez.com 835 Frank\n\nViolet\n\n34.497268\n\n12.655651\n\n39.577668\n\nTunnel\n\nWrightmouth, MI 82180- 9605\n\nhduke@hotmail.com\n\n4547 Archer Common\n\nDarkGreen\n\n31.926272\n\n11.109461\n\n37.268959\n\nDiazchester, CA 06566- 8576\n\nL M\n\n4.\n\n2.",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "Email\n\nAddress\n\nAvatar\n\nAvg.\n\nTime on\n\nTime on\n\nSession Length\n\nApp\n\nWebsite\n\npallen@yahoo.com\n\n24645 Valerie Unions Suite 582\n\nBisque\n\n33.000915\n\n11.330278\n\n37.110597\n\nCobbborough, ...\n\nriverarebecca@gmail.com\n\n1414 David Throughway Port Jason,\n\nSaddleBrown\n\n34.305557\n\n13.717514\n\n36.721283\n\nOH 22070- 1220\n\nmstephens@davidson- herman.com\n\n14023 Rodriguez Passage Port Jacobville,\n\nMediumAquaMarine\n\n33.330673\n\n12.795189\n\n37.536653\n\nPR 3...\n\nThe dataset includes both customer contact information (like Email and Address) and numerical data related to\n\ntheir interaction with the e-commerce platform (like “Avg. Session Length”, “Time on App”, “Time on Website”, and “Length of Membership”).\n\nStep 2: Data Preparation\n\nThe next step will involve preparing the data for our linear regression model. This includes:\n\nSelecting the relevant numerical columns for the prediction.\n\nSplitting the data into a training set and a testing set.\n\nDo you have any specific columns you want to use for predicting “Yearly Amount Spent”, or should I proceed with a general approach using all numerical columns?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed using numerical columns:\n\n[Prompt]\n\nPlease use only numerical columns.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided code for selecting the\n\nfeatures, splitting the dataset, and printing the shape:\n\n[Prompt response]\n\nStep 2: Data Preparation\n\nL\n\nM\n\n4.\n\n3.\n\n4.",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "We’ll now proceed with the following steps in preparing the data:\n\nSelecting the relevant columns: We’ll include the columns that are numerical and directly relate to customer\n\nbehavior.\n\nSplitting the data into training and testing sets: This helps us evaluate our model on unseen data to ensure\n\nit performs well generally.\n\nHere’s the code to perform these steps:\n\n# Selecting only the numerical columns for features features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] # Selecting the target column which is 'Yearly Amount Spent' target = data['Yearly Amount Spent'] # Splitting the dataset into training and testing sets # We allocate 80% of the data for training and 20% for testing X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Printing the shapes of the training and testing sets to verify the split print(f'Training features shape: {X_train.shape}') print(f'Testing features shape: {X_test.shape}') print(f'Training labels shape: {y_train.shape}') print(f'Testing labels shape: {y_test.shape}')\n\nThis block of code:\n\nSelects features and target: We separate the columns for the features and the target variable which is what we aim to predict.\n\nSplits the data: Using train_test_split, we divide the data into training and testing sets. The\n\ntest_size=0.2 means 20% of the data is reserved for testing. random_state is set for reproducibility of results.\n\nPrints shapes: We print the shapes of our training and testing sets to confirm the split was successful.\n\nShould we run this part of the code to prepare the data and check the split?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed with code execution:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the shape of features and labels:\n\n[Prompt response]\n\nThe data has been successfully split into training and testing sets. Here’s the breakdown:\n\nTraining features shape: 400 rows and 4 columns\n\nTesting features shape: 100 rows and 4 columns\n\nTraining labels shape: 400 values\n\nTesting labels shape: 100 values",
      "content_length": 2181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "This indicates that 80% of the data is used for training the model, and 20% is reserved for testing the model’s performance.\n\nStep 3: Building the Linear Regression Model\n\nNow, we’ll create and train the linear regression model using the training data. Here’s the plan:\n\nInitialize the linear regression model: This involves setting up our model ready to be fitted to the data.\n\nFit the model to the training data: This is where the model learns the relationship between the features and the target from the training data.\n\nEvaluate the model on the training data: This step is to get a quick check on how well our model has learned from the training data.\n\nShall I proceed with this next step and show you the code to build and evaluate the model on the training data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed to the next steps:\n\n[Prompt]\n\nYes, Please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code:\n\n[Prompt response]\n\nLet’s move on to building and training the linear regression model.\n\nStep 3: Building the Linear Regression Model\n\nHere’s the code to create, train, and initially evaluate the linear regression model:\n\n# Initialize the Linear Regression model model = LinearRegression() # Fit the model to the training data # This involves finding the best fitting line to the provided data points model.fit(X_train, y_train) # Evaluate the model by predicting on the training set train_predictions = model.predict(X_train) # Calculate the Mean Absolute Error (MAE) on the training set # MAE measures the average magnitude of the errors in a set of predictions, without considering th train_mae = mean_absolute_error(y_train, train_predictions) # Calculate the Mean Squared Error (MSE) on the training set # MSE measures the average of the squares of the errors—that is, the average squared difference be train_mse = mean_squared_error(y_train, train_predictions) # Printing the errors to understand model performance on the training data print(f'Mean Absolute Error on training data: {train_mae}') print(f'Mean Squared Error on training data: {train_mse}')\n\nThis code does the following:\n\nInitializes the model: LinearRegression() is called to create a linear regression model.\n\nFits the model: model.fit(X_train, y_train) adjusts weights on the regression line to fit the input features and the target output.",
      "content_length": 2393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "Makes predictions: model.predict(X_train) is used to see how well the model has learned to predict the\n\ntraining data.\n\nCalculates errors: mean_absolute_error and mean_squared_error provide metrics to gauge the model’s accuracy and performance by comparing the predicted values against the actual values.\n\nWould you like me to run this code now to see how the model performs on the training data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed:\n\n[Prompt]\n\nYes, Please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the\n\nresults:\n\n[Prompt response]\n\nThe linear regression model has been successfully trained on the dataset, and here are the performance metrics on\n\nthe training data:\n\nMean Absolute Error (MAE) on training data: 7.727\n\nMean Squared Error (MSE) on training data: 95.823\n\nThese metrics help us understand the average errors made by the model in predictions:\n\nMAE tells us that, on average, the model’s predictions are approximately 7.73 units away from the actual\n\nvalues.\n\nMSE indicates a slightly larger error in the predictions, considering it squares the errors, emphasizing larger\n\ndiscrepancies.\n\nNext Steps:\n\nNow, the model is trained and preliminarily evaluated on the training set. We should now test its performance on\n\nthe unseen testing set to truly evaluate its effectiveness.\n\nWould you like to proceed with testing the model on the testing set and seeing how it performs with new data?\n\n[End of response]\n\nWe will instruct ChatGPT to proceed with testing the model performance on the test set:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has executed the code and printed the results:\n\n[Prompt response]",
      "content_length": 1783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "The evaluation of the model on the testing set yielded the following results:\n\nMean Absolute Error (MAE) on testing data: 8.558\n\nMean Squared Error (MSE) on testing data: 109.864\n\nThese results tell us the following:\n\nThe MAE of 8.558 suggests that the model’s predictions on the testing data are, on average, about 8.56 units\n\naway from the actual values. This is slightly worse than the training MAE, which is to be expected as the model is now predicting data it hasn’t seen before.\n\nThe MSE of 109.864 is also higher than the training MSE, indicating that some predictions have larger errors.\n\nSummary:\n\nThe model performs reasonably well but shows a typical behavior of slightly reduced performance on the testing\n\ndata compared to the training data. This is normal and indicates that the model is generalizing as expected without\n\nsignificant overfitting or underfitting.\n\nFinal Steps:\n\nYou now have a functioning linear regression model that can predict the “Yearly Amount Spent” based on customer behavior metrics. Depending on your goals, you might want to explore further:\n\nImproving the model: You could look into more sophisticated modelling techniques or feature engineering.\n\nModel interpretation: Analyse the coefficients of the model to understand the impact of each feature.\n\nWould you like to explore any of these additional steps or need further assistance with something else?\n\n[End of response]\n\nWe have seen that similar to the free version, ChatGPT Plus does a pretty good job in creating a simple linear\n\nregression model step by step, by providing the code as well as executing it for us. Next, we will see whether the\n\nPlus version can apply the regularization techniques.\n\nFeature 2: Applying regularization techniques We will ask ChatGPT Plus to apply regularization techniques:\n\n[Prompt]\n\nCan you please apply Lasso, Ridge, and cross-validation to check whether it leads to any improvement?\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code for Lasso and Ridge, along with cross-validation:\n\n[Prompt response]\n\nStep 5: Applying Lasso and Ridge Regression with Cross-Validation\n\nLasso Regression:\n\nLasso regression not only helps in reducing overfitting but can also be used for feature selection since it can shrink the coefficients of less important features to zero.",
      "content_length": 2356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "Ridge Regression:\n\nRidge regression reduces the model complexity by imposing a penalty on the size of coefficients.\n\nWe’ll use the scikit-learn library, which provides convenient functions to perform these analyses.\n\nHere’s the code to apply Lasso and Ridge with cross-validation:\n\nfrom sklearn.linear_model import LassoCV, RidgeCV from sklearn.model_selection import cross_val_score # Lasso Regression with Cross-Validation lasso_cv = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5) # cv=5 means 5-fold cross-validation lasso_cv.fit(X_train, y_train) lasso_cv_score = lasso_cv.score(X_test, y_test) lasso_cv_mae = mean_absolute_error(y_test, lasso_cv.predict(X_test)) # Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5) ridge_cv.fit(X_train, y_train) ridge_cv_score = ridge_cv.score(X_test, y_test) ridge_cv_mae = mean_absolute_error(y_test, ridge_cv.predict(X_test)) # Printing results print(f'Lasso CV Score: {lasso_cv_score}, MAE: {lasso_cv_mae}') print(f'Ridge CV Score: {ridge_cv_score}, MAE: {ridge_cv_mae}')\n\nThis code performs the following:\n\nLassoCV and RidgeCV: These are Lasso and Ridge regression methods with built-in cross-validation of the alpha parameter. Cross-validation scores: The .score() method returns the R2 score that indicates the proportion of variance in the dependent variable that is predictable from the independent variables.\n\nMean Absolute Error: This provides an absolute measure of fit quality on the testing set.\n\nLet’s run this code to see if the regularization and cross-validation can enhance the model’s performance. Shall we proceed?\n\n[End of response]\n\nWe will instruct ChatGPT to execute the code:\n\n[Prompt]\n\nYes, please.\n\n[End of prompt]\n\n[Prompt response]\n\nIn our scenario, ChatGPT encountered an internal error and was unable to execute the code. Such errors can occur\n\ndue to various reasons, including unavailability of services at the time of execution, memory issues, or the absence of required libraries or dependencies in its runtime environment. It’s important to note that you might receive a\n\ndifferent response that executes the code flawlessly, as errors like these are sporadic. However, we’ve included this\n\nscenario to troubleshoot and illustrate how to proceed in such cases. Here, we have applied the prompting strategy to get feedback from the AI assistant (LIFE 3.3).\n\n[End of response]",
      "content_length": 2381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "We will now take the suggestion from ChatGPT and execute the code locally using Jupyter Colab, and since we\n\nare loading the dataset from Google Drive, we will provide the instructions in the prompt:\n\n[Prompt]\n\nOk, give me complete code to run locally and note that the file needs to be mounted from Google Drive from path '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' in the Jupyter notebook.\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the complete code:\n\n[Prompt response]\n\nCertainly! Below is the complete code that you can run locally in a Jupyter Notebook. This script includes steps\n\nfor importing the dataset directly from Google Drive, preprocessing the data, fitting both Lasso and Ridge\n\nregression models with cross-validation, and then evaluating them.\n\n# Import necessary libraries import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.preprocessing import StandardScaler # Mount Google Drive (specific to Google Colab) from google.colab import drive drive.mount('/content/drive') # Load the dataset from Google Drive file_path = '/content/drive/My Drive/Chapter05/Ecommerce Customers.csv' data = pd.read_csv(file_path) # Select only the numerical columns for features and the target column features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] target = data['Yearly Amount Spent'] # Splitting the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Standardizing the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and fit the Linear Regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Lasso Regression with Cross-Validation lasso_cv = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5) lasso_cv.fit(X_train_scaled, y_train) lasso_cv_score = lasso_cv.score(X_test_scaled, y_test) lasso_cv_mae = mean_absolute_error(y_test, lasso_cv.predict(X_test_scaled)) # Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5) ridge_cv.fit(X_train_scaled, y_train) ridge_cv_score = ridge_cv.score(X_test_scaled, y_test) ridge_cv_mae = mean_absolute_error(y_test, ridge_cv.predict(X_test_scaled)) # Print results print(f'Linear Regression Test MAE: {mean_absolute_error(y_test, model.predict(X_test_scaled))}') print(f'Lasso CV Score: {lasso_cv_score}, MAE: {lasso_cv_mae}') print(f'Ridge CV Score: {ridge_cv_score}, MAE: {ridge_cv_mae}')\n\nInstructions for Running the Code:",
      "content_length": 2814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Google Colab Specifics: The drive.mount() function is specific to Google Colab and allows you to access\n\nfiles on your Google Drive. If you are not using Google Colab, you will need to adjust the file loading method\n\naccording to your development environment.\n\nDependencies: Ensure you have all required libraries installed. You can install them using pip (e.g., pip install pandas scikit-learn google-colab).\n\nData Path: Adjust the file_path if your file is located in a different directory on your Google Drive.\n\nRun this code in your Jupyter Notebook environment to see how the models perform with your data. If you\n\nencounter any errors or need further assistance, feel free to reach out!\n\n[End of response]\n\nWe execute the code and it runs perfectly fine without any modifications or errors. Here, we learned that we can\n\nuse the hybrid approach of running code within ChatGPT Plus or request it to provide the code to execute locally\n\nin case of any issues.\n\nNext, we will move on to Feature 3 of generating a synthetic dataset using ChatGPT Plus.\n\nFeature 3: Generating a synthetic dataset to add complexity Now, let’s try, if we are able, to generate a synthetic dataset using ChatGPT Plus for our dataset. For this purpose,\n\nwe will use the Data Analyst GPT and start a fresh conversation.\n\nNote that specific GPTs can be invoked by providing @<GPT Name> in the message prompt:\n\nFigure 13.10: Invoking the Data Analyst GPT\n\n[Prompt]\n\n@Data Analyst – Provide me the code to generate the synthetic dataset for the above dataset from the perspective of predicting Yearly Amount Spent, along with the code to generate it. <Attach the dataset file here>\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the code and printed the first few rows:\n\n[Prompt response]\n\nTo help you generate a synthetic dataset for predicting yearly amount spent, I’ll first need to take a look at the structure and content of your provided dataset. Let’s start by loading and examining the data from your file. I’ll\n\ndisplay the first few rows and summary statistics to understand what kind of features we’re dealing with. Then, I’ll assist you in creating a synthetic dataset based on this structure. Let’s get started with loading the data.",
      "content_length": 2275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "The dataset consists of 500 entries with the following features:\n\nEmail: The customer’s email address (string).\n\nAddress: The customer’s address (string).\n\nAvatar: The avatar color (string).\n\nAvg. Session Length: Average session length in minutes (numeric).\n\nTime on App: Time spent on the app in minutes (numeric).\n\nTime on Website: Time spent on the website in minutes (numeric).\n\nLength of Membership: Length of membership in years (numeric).\n\nYearly Amount Spent: Yearly amount spent in dollars (numeric).\n\nTo create a synthetic dataset for the purpose of modeling and predicting “Yearly Amount Spent”, we can use a technique called bootstrapping, which involves sampling with replacement from the existing dataset to create a\n\nnew synthetic dataset that has a similar distribution. However, to add some variability and simulate a realistic dataset, we can also introduce random noise into the numeric fields.\n\nHere’s a Python code snippet to generate a synthetic dataset using bootstrapping and adding noise to the numeric\n\ncolumns:\n\nimport numpy as np # Number of samples for the synthetic dataset num_samples = 500 # Bootstrap the original dataset synthetic_data = data.sample(n=num_samples, replace=True) # Add random noise to numeric columns numeric_columns = ['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership' noise_factor = 0.05 # 5% noise for column in numeric_columns: noise = np.random.normal(loc=0.0, scale=noise_factor * synthetic_data[column].std(), size=num_ synthetic_data[column] += noise # Optionally save the synthetic dataset to a new CSV file synthetic_data.to_csv('/mnt/data/Synthetic_Ecommerce_Customers.csv', index=False) synthetic_data.head()\n\nThis code will:\n\nSample 500 entries from the original dataset, allowing replacements (bootstrapping).\n\nAdd Gaussian noise to each numeric column based on 5% of the standard deviation of that column.\n\nOptionally, the new synthetic dataset can be saved to a CSV file.\n\nWould you like to proceed with generating this synthetic dataset? Or do you have any modifications or specific\n\nrequirements to include in the synthesis process?\n\n[End of response]\n\nNext, we will instruct ChatGPT to generate the code and provide us with the file to download the dataset:\n\n[Prompt]\n\nPlease generate the dataset and make it available to download.\n\n[End of prompt]",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "We should receive a response similar to the following, where ChatGPT has executed the code and provided the\n\nlink to download the synthetic dataset:\n\n[Prompt response]\n\nThe synthetic dataset has been generated and saved. You can download it using the link below:\n\n[Download Synthetic Ecommerce Customers Dataset](sandbox:/mnt/data/Synthetic_Ecommerce_Customers.c\n\n[End of response]\n\nHere, we learned that ChatGPT Plus can provide the code, execute the code, and also generate CSV files available\n\nto download.\n\nNext, we will request ChatGPT Plus to generate the complete code and evaluate model performance with the added noise in the synthetic dataset. Let’s move to our next feature.\n\nFeature 4: Generating code to develop a model in a single step for a synthetic dataset [Prompt]\n\nCan you please generate the complete code to perform linear regression, apply Lasso and Ridge regularization\n\ntechniques on our synthetic dataset, and evaluate the model performance?\n\n[End of prompt]\n\nWe should receive a response similar to the following, where ChatGPT has provided the complete code:\n\n[Prompt response]\n\nSure, I can provide you with a Python script that performs linear regression, as well as Lasso and Ridge regression,\n\non the synthetic dataset we just created. We will also evaluate the model performance using metrics like the R-\n\nsquared score, mean squared error (MSE), and mean absolute error (MAE).\n\nHere is a complete Python code snippet for the whole process:\n\nimport pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Lasso, Ridge from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # Load the synthetic dataset data = pd.read_csv('/path/to/your/Synthetic_Ecommerce_Customers.csv') # Feature selection: Using numerical columns as features features = data[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']] target = data['Yearly Amount Spent'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state= # Initialize the models linear_model = LinearRegression() lasso_model = Lasso(alpha=0.1) # alpha is the regularization strength ridge_model = Ridge(alpha=1.0) # alpha is the regularization strength # Train the models linear_model.fit(X_train, y_train)",
      "content_length": 2397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "lasso_model.fit(X_train, y_train) ridge_model.fit(X_train, y_train) # Predictions linear_predictions = linear_model.predict(X_test) lasso_predictions = lasso_model.predict(X_test) ridge_predictions = ridge_model.predict(X_test) # Evaluations def evaluate_model(predictions, actual): mse = mean_squared_error(actual, predictions) mae = mean_absolute_error(actual, predictions) r2 = r2_score(actual, predictions) return mse, mae, r2 linear_mse, linear_mae, linear_r2 = evaluate_model(linear_predictions, y_test) lasso_mse, lasso_mae, lasso_r2 = evaluate_model(lasso_predictions, y_test) ridge_mse, ridge_mae, ridge_r2 = evaluate_model(ridge_predictions, y_test) # Print out the metrics print(\"Linear Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(linear_mse, linear_mae, li print(\"Lasso Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(lasso_mse, lasso_mae, lasso print(\"Ridge Regression - MSE: {:.2f}, MAE: {:.2f}, R2: {:.2f}\".format(ridge_mse, ridge_mae, ridge\n\nMake sure to replace '/path/to/your/Synthetic_Ecommerce_Customers.csv' with the actual path where your\n\nsynthetic dataset is stored when running the script in your local environment.\n\nThis script will:\n\nLoad your synthetic dataset.\n\nSelect appropriate features and split the data into training and test sets.\n\nInitialize and train Linear, Lasso, and Ridge regression models.\n\nEvaluate and print out the performance metrics for each model.\n\nThis should help you understand the performance differences and impacts of regularization in your predictive models!\n\n[End of response]\n\nWe will instruct ChatGPT Plus to execute the code for us:\n\n[Prompt]\n\nPlease execute the code.\n\n[End of prompt]\n\nChatGPT Plus should have executed the code and we should receive a response similar to the following:\n\n[Prompt response]\n\nThe models have been trained and evaluated. Here are the performance metrics for each:\n\nLinear Regression:\n\nMean Squared Error (MSE): 133.01\n\nMean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nLasso Regression:\n\nMean Squared Error (MSE): 133.04",
      "content_length": 2042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "Mean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nRidge Regression:\n\nMean Squared Error (MSE): 133.08\n\nMean Absolute Error (MAE): 8.79\n\nR-squared (R2): 0.977\n\nAll three models show very similar performance metrics, with high R2 scores close to 0.977, indicating that each\n\nmodel explains about 97.7% of the variance in the Yearly Amount Spent. The slight differences in MSE and MAE across models are minimal, suggesting that regularization (both Lasso and Ridge) does not significantly alter the\n\nperformance in this particular case, possibly due to the absence of multicollinearity or other issues that these\n\ntechniques help address.\n\n[End of response]\n\nBy working through this exercise, you’ve not only learned how to perform a regression analysis using the free\n\nversion of ChatGPT and ChatGPT Plus but also how to create a linear regression model step by step using your own dataset, apply regularization techniques, and generate synthetic datasets.\n\nAssignment Generate a single-step code for our dataset and execute it in Google Colab for the random forest algorithm.\n\nChallenge Can you enhance the following prompt by incorporating prompting strategies to ensure the retrieval of a specific\n\ndataset, rather than a random one that may not exist or may have an incorrect path, ensuring it’s fit for the intended use?\n\nProvide the output from the current prompt as well as the improved prompt:\n\n[Prompt]\n\nCan you please provide me with the complete end-to-end Python code for a publicly available advertising dataset along with a detailed explanation that it is compatible with the Jupyter Notebook?\n\n[End of prompt]\n\nSummary We explored the application of TAG, PIC, and LIFE prompting strategies in crafting regression models, employing both ChatGPT and ChatGPT Plus for rapid analysis and predictive tasks. This approach is particularly valuable in\n\nthe early stages of machine learning development, offering immediate insights and the flexibility to experiment\n\nwith different models or algorithms without the burden of managing execution environments or programming instances. Additionally, we learned how to effectively utilize single prompts for generating comprehensive code.\n\nWhile it’s possible to craft prompts for discrete tasks or steps, many of these require only succinct lines of code",
      "content_length": 2311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "and were not the focus here. Providing feedback is instrumental in this process, and validating the output is crucial to ensure the code’s functionality.\n\nIn the next chapter, we will learn how to use ChatGPT to generate the code for the multilayer perceptron (MLP)\n\nmodel with the help of the Fashion-MNIST dataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and other readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "14\n\nBuilding an MLP Model for Fashion-MNIST with ChatGPT\n\nIntroduction Building upon our foundational understanding of predictive modeling, we\n\nnow dive into the dynamic world of Multilayer Perceptron (MLP)\n\nmodels. In this chapter, we embark on a journey to construct an MLP model\n\nfrom scratch, leveraging the versatility and power of neural networks for\n\npredictive analytics.\n\nOur exploration of MLPs represents a significant leap into the realm of\n\ncomplex modeling techniques. While linear regression provided valuable\n\ninsights into modeling relationships within data, MLPs offer a rich\n\nframework for capturing intricate patterns and nonlinear dependencies,\n\nmaking them well suited for a wide range of predictive tasks.\n\nThrough hands-on experimentation and iterative refinement, we will\n\nunravel the intricacies of MLP architecture and optimization. From\n\ndesigning the initial network structure to fine-tuning hyperparameters and incorporating advanced techniques such as batch normalization and",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "dropout, we aim to equip you with the knowledge and skills to harness the\n\nfull potential of neural networks in predictive modeling.\n\nAs we navigate through the construction and optimization of our MLP model, we will delve into the underlying principles of neural network\n\ndynamics, exploring how different architectural choices and optimization\n\nstrategies influence model performance and generalization capabilities.\n\nBusiness problem A fashion e-commerce store seeks to optimize customer engagement and\n\nincrease revenue by leveraging machine learning techniques to gain deeper\n\ninsights into customer behavior and preferences. By analyzing image data representing various fashion items purchased by customers, the store aims\n\nto tailor its product recommendations, improve customer satisfaction, and\n\nenhance the overall shopping experience.\n\nProblem and data domain In this chapter, we will employ MLP models to understand the relationship\n\nbetween customers’ preferences and their purchasing patterns using the Fashion-MNIST dataset. MLP models offer a powerful framework for\n\nimage classification tasks, allowing us to predict the type of clothing or accessory a customer is likely to purchase based on their interactions with the online store. By uncovering patterns in customer preferences, the e-\n\ncommerce store can personalize recommendations and optimize inventory management to meet the diverse needs of its customer base.\n\nDataset overview",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "The fashion e-commerce store collects image data representing various\n\nfashion items, categorized into different classes, from its customers. The Fashion-MNIST dataset comprises 70,000 grayscale images of clothing and\n\naccessories, each associated with a specific label indicating its category and of size 28x28.\n\nFeatures in the dataset include:\n\nImage data: Grayscale images of fashion items, each represented as a matrix of pixel intensities. These images serve as the input data for training the MLP model.\n\nLabel: The category label assigned to each image, representing the type of clothing or accessory depicted. Labels range from 0 to 9,\n\ncorresponding to classes such as T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot.\n\nBy analyzing this image data and its corresponding labels, we aim to train\n\nan MLP model capable of accurately classifying fashion items based on\n\ntheir visual features. This predictive model will enable the e-commerce store to make personalized product recommendations, enhance customer\n\nengagement, and ultimately increase revenue by providing a seamless shopping experience tailored to individual preferences.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Figure 14.1: Fashion-MNIST dataset\n\nBreaking the problem down into features Given the nature of the Fashion-MNIST dataset, which comprises grayscale\n\nimages of fashion items categorized into different classes, we will start by\n\nbuilding a baseline MLP model. This will involve the following high-level\n\nsteps:\n\n1. Building the baseline model: Users will understand the process of constructing a simple MLP model for image classification using ChatGPT. We will guide users through loading the Fashion-MNIST\n\ndataset, preprocessing the image data, splitting it into training and testing sets, defining the model architecture, training the model,\n\nmaking predictions, and evaluating its performance.\n\n2. Adding layers to the model: Once the baseline model is established, users will learn how to experiment with adding additional layers to the",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "MLP architecture. We will explore how increasing the depth or width of the model impacts its performance and capacity to capture complex\n\npatterns in the image data.\n\n3. Experimenting with batch sizes: Users will experiment with different batch sizes during model training to observe their effects on training speed, convergence, and generalization performance. We will explore\n\nhow varying batch sizes influence the trade-off between computation\n\nefficiency and model stability.\n\n4. Adjusting the number of neurons: Users will explore the impact of\n\nadjusting the number of neurons in each layer of the MLP model. By\n\nincreasing or decreasing the number of neurons, users can observe changes in model capacity and its ability to learn intricate features\n\nfrom the image data.\n\n5. Trying different optimizers: Finally, users will experiment with\n\ndifferent optimization algorithms, such as SGD, Adam, and RMSprop, to optimize the training process of the MLP model. We will observe\n\nhow different optimizers influence training dynamics, convergence\n\nspeed, and final model performance.\n\nBy following these steps, users will gain a comprehensive understanding of building and optimizing MLP models for image classification tasks using\n\nthe Fashion-MNIST dataset. They will learn how to iteratively refine the\n\nmodel architecture and training process to achieve optimal performance and accuracy in classifying fashion items.\n\nPrompting strategy To leverage ChatGPT for machine learning we need to have a clear\n\nunderstanding of how to implement the prompting strategies specifically for",
      "content_length": 1583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "code generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better understanding of what needs to go into prompts.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 - Task: The specific task or goal is to create a classification model for\n\nthe Fashion-MNIST dataset.\n\n1.2 - Actions: The key steps involved in creating a classification model\n\nusing an MLP for the Fashion-MNIST dataset are:\n\nData preprocessing: Normalize pixel values, flatten images into vectors, and encode categorical labels.\n\nData splitting: Partition the dataset into training, validation, and testing\n\nsets.\n\nModel selection: Opt for an MLP as the classification model.\n\nModel training: Train the MLP on the training data.\n\nModel evaluation: Use metrics like accuracy, precision, recall, and\n\nconfusion matrix to evaluate the model’s performance.\n\n1.3 - Guidelines: We will provide the following guidelines to ChatGPT in our prompt:\n\nThe code should be compatible with Jupyter notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the text block of the notebook, in detail for each method used in the",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "code before providing the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 - Persona: We will adopt the persona of a beginner who needs to learn\n\ndifferent steps of model creation, hence the code should be generated step\n\nby step.\n\n2.2 - Instructions: We have specified that we want the code generated for\n\nan MLP model with a single layer and have instructed ChatGPT to provide\n\none step at a time and wait for the user’s feedback.\n\n2.3 - Context: In this case, ChatGPT is already aware of the Fashion-\n\nMNIST dataset, as it is widely known, so we do not need to provide additional context.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE) prompt strategy 3.1 - Learn:\n\nWe want to learn about MLP models and how they work.\n\n3.2 - Improvise:\n\nWe will use this knowledge later when adding more layers, changing\n\nthe batch size or activation functions, and so on.",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "3.3 - Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4- Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity.\n\nThis is used throughout the chapter.\n\nBuilding an MLP model to accurately classify the Fashion-MNIST images using the free version of ChatGPT ChatGPT’s premium version has a code interpreter, but it doesn’t support\n\nthe deep learning libraries such as Keras and TensorFlow required to\n\nexecute the code. Hence, for this chapter, we will stick to the free version of\n\nChatGPT.\n\nFeature 1: Building the baseline model Let’s craft our initial prompt for baseline model.\n\n[Prompt]",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "I want to create a simple classification model for the Fashion-MNIST\n\ndataset (TAG 1.1) consisting of a single-layer MLP model (PIC 2.2). It\n\nshould consist of the following steps (TAG1.2), provide one step at a time, and wait for the user’s feedback (PIC 2.2).\n\nData Preprocessing: Normalize pixel values, flatten images into\n\nvectors, and encode categorical labels.\n\nData Splitting: Partition the dataset into training, validation, and\n\ntesting sets.\n\nModel Selection: Opt for a Multi-Layer Perceptron (MLP) as the\n\nclassification model.\n\nModel Training: Train the MLP on the training data.\n\nModel Evaluation: Use metrics like accuracy, precision, recall, and\n\nconfusion matrix to evaluate the model’s performance.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and\n\nstrictly follow the below instructions (TAG 1.3):\n\nThe code should be compatible with the Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code, which will be then copied into the\n\ntext block of notebook, in detail for each method used in the code before\n\nproviding the code.\n\n[End of prompt]\n\nYou should receive a response similar to the following, where ChatGPT has\n\nprovided us with the code for step 1, Data Preprocessing, along with\n\ndetailed comments and explanations.\n\n[Prompt response]",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "Sure, let’s start with the first step: Data Preprocessing.\n\nData Preprocessing:\n\nData preprocessing is a crucial step in machine learning, where we\n\nprepare our data for training by applying various transformations and\n\nnormalization techniques. In this step, we’ll normalize the pixel values, flatten the images into vectors, and encode categorical labels.\n\nNormalize Pixel Values:\n\nNormalizing pixel values ensures that each pixel’s value is scaled to a\n\nrange between 0 and 1. This is important for the neural network to\n\nconverge faster during training and to prevent any particular feature\n\nfrom dominating the learning process.\n\nFlatten Images into Vectors:\n\nMLP models cannot directly work with multi-dimensional data like\n\nimages. Hence, we need to flatten the images into 1D vectors, where\n\neach pixel value becomes a feature.\n\nEncode Categorical Labels:\n\nSince our labels are categorical (e.g., different types of clothing items),\n\nwe need to encode them into numerical values so that our model can\n\nunderstand and learn from them.\n\nNow, let’s implement these steps in code:\n\nimport numpy as np from tensorflow.keras.datasets import fashion_mnist from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.model_selection import train_test_split # Step 1: Load Fashion-MNIST dataset (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data(",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "# Step 2: Normalize pixel values def normalize_images(train_data, test_data): # Convert to float32 to ensure division results in float train_data = train_data.astype('float32') test_data = test_data.astype('float32') # Normalize pixel values to the range 0-1 train_data /= 255.0 test_data /= 255.0 return train_data, test_data # Step 3: Flatten images into vectors def flatten_images(train_data, test_data): # Reshape images to vectors train_data = train_data.reshape(train_data.shape[0], -1) test_data = test_data.reshape(test_data.shape[0], -1) return train_data, test_data # Step 4: Encode categorical labels def encode_labels(train_labels, test_labels): # Initialize LabelEncoder label_encoder = LabelEncoder() # Fit LabelEncoder on training labels and transform both tr train_labels = label_encoder.fit_transform(train_labels) test_labels = label_encoder.transform(test_labels) return train_labels, test_labels # Apply data preprocessing steps x_train, x_test = normalize_images(x_train, x_test) x_train, x_test = flatten_images(x_train, x_test) y_train, y_test = encode_labels(y_train, y_test) # Print the shapes of preprocessed data print(\"x_train shape:\", x_train.shape) print(\"y_train shape:\", y_train.shape) print(\"x_test shape:\", x_test.shape) print(\"y_test shape:\", y_test.shape)\n\nExplanation:\n\nWe import necessary libraries such as NumPy for numerical\n\noperations, TensorFlow for loading the Fashion-MNIST dataset, and\n\nscikit-learn for data preprocessing.",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "We load the Fashion-MNIST dataset using the fashion_mnist.load_data() method and split it into training and\n\ntesting sets.\n\nThree functions, normalize_images, flatten_images, and encode_labels, are defined to perform each data preprocessing step.\n\nInside each function:\n\nnormalize_images divides pixel values by 255 to scale them between 0 and 1.\n\nflatten_images reshapes the images into 1D vectors.\n\nencode_labels converts categorical labels into numerical values using LabelEncoder.\n\nFinally, we apply these preprocessing steps to the training and testing data\n\nand print their shapes to ensure everything is processed correctly.\n\n[End of response]\n\nWe will now apply the prompt evaluation strategy (LIFE 3.4) and validate\n\nthe code by executing the code for every step in the Google Colab\n\nenvironment. The code loads the model from Keras and normalizes the\n\nimages, flattens them into vectors, encodes labels using LabelEncoder, then applies the preprocessing and prints the shape. The code runs perfectly fine\n\nwithout applying any modifications or any errors.\n\nPlease refer to the Learning MLP using Fashion-MNIST dataset with ChatGPT.ipynb notebook for the executable code snippets or copy the code\n\nfrom the prompt response.",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "Figure 14.2: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for step 2 – Data Splitting.\n\n[Prompt response]\n\nGreat! Let’s move on to the next step:",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "Data Splitting:\n\nIn this step, we’ll partition the dataset into training, validation, and\n\ntesting sets. The training set will be used to train the model, the\n\nvalidation set will be used to tune hyperparameters and prevent\n\noverfitting, and the testing set will be used to evaluate the model’s\n\nperformance on unseen data.\n\nTraining Set:\n\nThe training set is used to train the model’s parameters (weights and\n\nbiases) using optimization algorithms like gradient descent.\n\nValidation Set:\n\nThe validation set is used to evaluate the model’s performance during\n\ntraining and adjust hyperparameters such as learning rate, batch size,\n\nand number of epochs to optimize the model’s performance.\n\nTesting Set:\n\nThe testing set is used to evaluate the final performance of the trained\n\nmodel on unseen data and assess its generalization capability.\n\nNow, let’s split the dataset:\n\n# Step 5: Split dataset into training, validation, and testing def split_dataset(train_data, train_labels, test_data, test_lab # Split training set into training and validation sets x_train, x_val, y_train, y_val = train_test_split(train_dat # Split testing set into testing and validation sets x_test, x_val, y_test, y_val = train_test_split(test_data, return x_train, y_train, x_val, y_val, x_test, y_test # Apply data splitting x_train, y_train, x_val, y_val, x_test, y_test = split_dataset( # Print the shapes of split datasets print(\"Training set:\")",
      "content_length": 1429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "print(\"x_train shape:\", x_train.shape) print(\"y_train shape:\", y_train.shape) print(\"\\nValidation set:\") print(\"x_val shape:\", x_val.shape) print(\"y_val shape:\", y_val.shape) print(\"\\nTesting set:\") print(\"x_test shape:\", x_test.shape) print(\"y_test shape:\", y_test.shape)\n\nExplanation:\n\nWe define a split_dataset function to split the dataset into training,\n\nvalidation, and testing sets using the train_test_split function from scikit-learn.\n\nThe function takes training data, training labels, testing data, testing\n\nlabels, and optional parameters such as validation size, test size, and\n\nrandom state.\n\nWe split the training set into training and validation sets using the\n\nspecified validation size.\n\nThen, we split the testing set into testing and validation sets using the\n\nspecified test size.\n\nFinally, we print the shapes of the split datasets to verify the split.\n\n[End of response]\n\nWe execute the code in the notebook and it runs successfully. The data is\n\nsplit into training, validation, and test sets with the specified sizes.",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "Figure 14.3: Code execution – data splitting\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step and define a single-layer baseline MLP\n\nmodel.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for a single-layer MLP model with an input layer, one\n\nhidden layer, and an output layer.\n\n[Prompt response]\n\nSure, let’s move on to the next step:\n\nModel Selection:\n\nIn this step, we’ll choose a Multi-Layer Perceptron (MLP) as the\n\nclassification model. An MLP consists of one or more layers of",
      "content_length": 593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "neurons, each fully connected to the next layer. For simplicity, we’ll\n\nstart with a single-layer MLP as a baseline model.\n\nSingle-Layer MLP:\n\nA single-layer MLP consists of an input layer, a hidden layer, and an\n\noutput layer. However, since it has only one hidden layer, it’s often\n\nreferred to as a single-layer perceptron. Each neuron in the hidden layer receives input from all neurons in the input layer and produces an\n\noutput, which is then passed through an activation function to\n\nintroduce non-linearity. Finally, the output layer produces the final\n\nprediction.\n\nNow, let’s define the baseline single-layer MLP model:\n\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Step 6: Define a single-layer MLP model def create_single_layer_mlp(input_shape, num_classes): model = Sequential() # Add a single hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Define input shape and number of classes input_shape = x_train.shape[1:] num_classes = len(np.unique(y_train)) # Create the single-layer MLP model model = create_single_layer_mlp(input_shape, num_classes) # Print model summary model.summary()\n\nExplanation:",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "We import the necessary modules from TensorFlow.keras to define\n\nthe MLP model.\n\nWe define a function create_single_layer_mlp to create a single-layer MLP model.\n\nInside the function:\n\nWe initialize a sequential model.\n\nWe add a single hidden layer with 128 neurons and ReLU activation function.\n\nWe add an output layer with softmax activation function for\n\nmulti-class classification.\n\nWe specify the input shape based on the shape of the training\n\ndata and the number of output classes.\n\nWe create the single-layer MLP model using the defined\n\nfunction.\n\nFinally, we print the summary of the model architecture.\n\n[End of response]\n\nWe execute the code and it runs without any errors and prints the model\n\nsummary with the layer type, output shape, and number of parameters. It\n\nalso provides us with the total parameters, trainable and non-trainable parameters. In our case, all the parameters are trainable parameters.",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "Figure 14.4: Code execution – baseline model architecture\n\nWe will now instruct ChatGPT to proceed to the next step for model\n\ntraining.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for model training along with explanations of the key\n\nconcepts involved, such as loss functions and optimization algorithms along\n\nwith comments.",
      "content_length": 431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "[Prompt response]\n\nSure, let’s move on to the next step:\n\nModel Training:\n\nIn this step, we’ll train the MLP model on the training data. Training\n\ninvolves feeding the training data to the model, adjusting its\n\nparameters (weights and biases) using optimization algorithms like\n\nstochastic gradient descent, and minimizing a loss function to make\n\npredictions as close to the actual labels as possible.\n\nLoss Function:\n\nThe loss function measures the difference between the predicted labels\n\nand the actual labels. For multi-class classification problems like\n\nFashion-MNIST, we typically use categorical cross-entropy as the loss\n\nfunction.\n\nOptimization Algorithm:\n\nThe optimization algorithm is responsible for updating the model’s\n\nparameters during training to minimize the loss function. We’ll use\n\nAdam optimizer, which is an extension to stochastic gradient descent\n\nand is widely used in deep learning.\n\nTraining Parameters:\n\nWe’ll specify training parameters such as batch size and number of\n\nepochs. Batch size determines the number of samples processed before\n\nupdating the model’s parameters, while the number of epochs specifies\n\nhow many times the entire training dataset is passed forward and\n\nbackward through the model.\n\nNow, let’s train the model:",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "from tensorflow.keras.utils import to_categorical # Step 7: Model Training def train_model(model, x_train, y_train, x_val, y_val, batch_si # Convert class vectors to binary class matrices (one-hot e y_train = to_categorical(y_train) y_val = to_categorical(y_val) # Compile the model with categorical cross-entropy loss and model.compile(loss='categorical_crossentropy', optimizer='a # Train the model on the training data history = model.fit(x_train, y_train, batch_size=batch_size return history # Train the model history = train_model(model, x_train, y_train, x_val, y_val, ba\n\nExplanation:\n\nWe import the necessary modules from TensorFlow.keras to compile\n\nand train the model.\n\nWe define a train_model function to train the model on the training\n\ndata.\n\nInside the function:\n\nWe convert the class labels to binary class matrices using one-\n\nhot encoding.\n\nWe compile the model with the categorical cross-entropy loss\n\nfunction and the Adam optimizer.\n\nWe train the model on the training data with the specified batch\n\nsize and number of epochs.\n\nFinally, we train the model using the defined function and store the\n\ntraining history for visualization and analysis.\n\n[End of response]",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "We execute the code and it runs without any errors. It applies a one-hot\n\nencoding to class labels, complies the model with the categorical cross-\n\nentropy loss function and Adam optimizer, and trains the model with batch\n\nsize 32 for 10 epochs. This achieves a validation accuracy of approximately\n\n88%.\n\nFigure 14.5: Code execution – model training\n\nWe will now instruct ChatGPT to proceed to the next step for model\n\nevaluation.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for evaluating the model against the test set, along with",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "explanations of concepts like accuracy, precision, recall, and the confusion\n\nmatrix.\n\n[Prompt response]\n\nModel Evaluation:\n\nIn this step, we’ll use various metrics to evaluate the performance of\n\nthe trained MLP model on the testing data. Commonly used metrics for\n\nclassification tasks include accuracy, precision, recall, and the\n\nconfusion matrix.\n\nAccuracy:\n\nAccuracy measures the proportion of correctly classified instances out\n\nof the total instances.\n\nPrecision:\n\nPrecision measures the proportion of true positive predictions out of all\n\npositive predictions. It indicates the model’s ability to avoid false\n\npositives.\n\nRecall:\n\nRecall measures the proportion of true positive predictions out of all\n\nactual positive instances. It indicates the model’s ability to identify all\n\npositive instances.\n\nConfusion Matrix:\n\nA confusion matrix provides a summary of the model’s performance\n\nby showing the number of true positive, true negative, false positive,\n\nand false negative predictions.\n\nNow, let’s evaluate the model:",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score, precision_score, re # Step 8: Model Evaluation def evaluate_model(model, x_test, y_test): # Predict labels for testing data y_pred = model.predict(x_test) # Convert predicted labels to class labels y_pred_classes = np.argmax(y_pred, axis=1) # Convert true labels to class labels y_true_classes = np.argmax(y_test, axis=1) # Calculate accuracy accuracy = accuracy_score(y_true_classes, y_pred_classes) # Calculate precision precision = precision_score(y_true_classes, y_pred_classes, # Calculate recall recall = recall_score(y_true_classes, y_pred_classes, avera # Calculate confusion matrix cm = confusion_matrix(y_true_classes, y_pred_classes) return accuracy, precision, recall, cm # Evaluate the model accuracy, precision, recall, cm = evaluate_model(model, x_test, # Print evaluation metrics print(\"Accuracy:\", accuracy) print(\"Precision:\", precision) print(\"Recall:\", recall) print(\"Confusion Matrix:\\n\", cm) # Visualize confusion matrix plt.imshow(cm, cmap=plt.cm.Blues) plt.title('Confusion Matrix') plt.colorbar() plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.show()\n\nExplanation:\n\nWe import necessary modules from matplotlib.pyplot and scikit-\n\nlearn to visualize the confusion matrix and calculate evaluation\n\nmetrics.",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "We define an evaluate_model function to evaluate the model’s\n\nperformance on the testing data.\n\nInside the function:\n\nWe make predictions on the testing data using the trained model.\n\nWe convert the predicted labels and true labels to class labels.\n\nWe calculate accuracy, precision, recall, and confusion matrix\n\nusing scikit-learn metrics functions.\n\nFinally, we evaluate the model using the defined function, print the\n\nevaluation metrics, and visualize the confusion matrix.\n\n[End of response]\n\nWe execute the code and note that it experiences an error due to the test\n\nlabels not being encoded, but this is not evident from the error itself. We\n\ncan get help from ChatGPT to resolve the error by applying the prompting\n\nstrategy of providing the error to ChatGPT.\n\nFigure 14.6: Code execution – error for model evaluation\n\nWe provided the error to ChatGPT to get assistance.\n\n[Prompt]",
      "content_length": 889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "Looks like there is some issue with the test labels. We received the below\n\nerror:\n\n------282/282 [==============================] - 2s 5ms/step --\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the suggestions to troubleshoot the problem.\n\n[Prompt response]\n\nIt seems like there’s an issue with the shape of the labels. The error\n\nindicates that the axis specified for the argmax function is out of bounds.\n\nThis typically occurs when the labels are not in the expected format or\n\nshape.\n\nLet’s double-check the shape of the labels and make sure they are in the\n\ncorrect format for evaluation:\n\nprint(\"Shape of y_test:\", y_test.shape)\n\nThis will help us understand the shape of the labels and identify any\n\ndiscrepancies. Once we identify the issue, we can correct it accordingly.\n\n[End of response]\n\nWe print the shape of y_test:",
      "content_length": 880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Figure 14.7: Code execution - shape of test labels\n\nWe will now provide ChatGPT with the output of the code.\n\n[Prompt]\n\nShape of y_test: (9000,)\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the problem and provides us with the corrected code.\n\n[Prompt response]\n\nThank you for providing the shape of y_test. It seems that the labels are in\n\n1D format, which means they are not one-hot encoded. In the evaluation\n\nfunction, we assumed that the labels are one-hot encoded, which caused the\n\nerror.\n\nTo resolve this issue, we need to modify the evaluation function to handle\n\n1D labels instead of one-hot encoded labels. We can achieve this by\n\nremoving the conversion of labels to one-hot encoding and directly\n\ncompare the predicted labels with the true labels.\n\nLet’s update the evaluation function accordingly:\n\n# Step 8: Model Evaluation def evaluate_model(model, x_test, y_test): # Predict labels for testing data",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "y_pred = model.predict(x_test) # Convert predicted labels to class labels y_pred_classes = np.argmax(y_pred, axis=1) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred_classes) # Calculate precision precision = precision_score(y_test, y_pred_classes, average # Calculate recall recall = recall_score(y_test, y_pred_classes, average='weig # Calculate confusion matrix cm = confusion_matrix(y_test, y_pred_classes) return accuracy, precision, recall, cm # Evaluate the model accuracy, precision, recall, cm = evaluate_model(model, x_test, # Print evaluation metrics print(\"Accuracy:\", accuracy) print(\"Precision:\", precision) print(\"Recall:\", recall) print(\"Confusion Matrix:\\n\", cm) # Visualize confusion matrix plt.imshow(cm, cmap=plt.cm.Blues) plt.title('Confusion Matrix') plt.colorbar() plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.show()\n\nExplanation:\n\nWe removed the conversion of labels to one-hot encoding and directly used the y_test true labels in the evaluation function.\n\nWe updated the accuracy, precision, and recall calculation functions to use\n\nthe 1D labels instead of one-hot encoded labels.\n\nNow, the evaluation function should work correctly with the provided 1D\n\nlabels, and the evaluation metrics should be calculated accurately.\n\n[End of response]",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "We will execute the code and it works fine now because the code provided\n\nhas been modified to work with 1D labels. It also prints the confusion matrix.",
      "content_length": 152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "Figure 14.8: Code execution – confusion matrix\n\nThis completes feature 1. We achieved an accuracy of 0.8796 (87.96%), a\n\nprecision of 0.8817 (88.17%), and a recall of 0.8796 (87.96%) with our\n\nsingle-layer MLP model. Next, we will move to the second feature, adding\n\nmore layers to the model. We will also learn how ChatGPT can assist us in\n\nthat.\n\nFeature 2: Adding layers to the model Now that we have a baseline model architecture, let us try adding layers to\n\nit. We will apply a prompting strategy to improvise a model and ask ChatGPT to provide us with the code for it.\n\n[Prompt]\n\nNow that we have a baseline model with a single layer, can you please\n\nprovide me code with two layers and three layers and calculate and\n\ncompare the model performance due to the addition of layers?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for two-layer and\n\nthree-layer models along with comparison results.\n\n[Prompt response]\n\nCertainly! Let’s define and train multi-layer perceptron (MLP) models with\n\ntwo and three layers, and then compare their performance with the baseline\n\nsingle-layer model.\n\nDefine MLP Models with Two and Three Layers:",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "We’ll define MLP models with additional hidden layers to see how the\n\nperformance changes with the increase in the number of layers.\n\nTwo-Layer MLP:\n\nThe two-layer MLP will consist of an input layer, two hidden layers,\n\nand an output layer.\n\nThree-Layer MLP:\n\nThe three-layer MLP will consist of an input layer, three hidden layers,\n\nand an output layer.\n\nNow, let’s define and train these models:\n\n# Define a two-layer MLP model def create_two_layer_mlp(input_shape, num_classes): model = Sequential() # Add first hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add second hidden layer with ReLU activation model.add(Dense(64, activation='relu')) # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Define a three-layer MLP model def create_three_layer_mlp(input_shape, num_classes): model = Sequential() # Add first hidden layer with ReLU activation model.add(Dense(128, activation='relu', input_shape=input_s # Add second hidden layer with ReLU activation model.add(Dense(64, activation='relu')) # Add third hidden layer with ReLU activation model.add(Dense(32, activation='relu')) # Add output layer with softmax activation for multi-class model.add(Dense(num_classes, activation='softmax')) return model # Create two-layer MLP model two_layer_model = create_two_layer_mlp(input_shape, num_classes # Create three-layer MLP model",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "three_layer_model = create_three_layer_mlp(input_shape, num_cla # Train two-layer MLP model two_layer_history = train_model(two_layer_model, x_train, y_tra # Train three-layer MLP model three_layer_history = train_model(three_layer_model, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy, precision, recall, and confusion matrix.\n\n# Evaluate two-layer MLP model two_layer_accuracy, _, _, _ = evaluate_model(two_layer_model, x # Evaluate three-layer MLP model three_layer_accuracy, _, _, _ = evaluate_model(three_layer_mode # Print model comparison print(\"Single-layer MLP Accuracy:\", accuracy) print(\"Two-layer MLP Accuracy:\", two_layer_accuracy) print(\"Three-layer MLP Accuracy:\", three_layer_accuracy)\n\nExplanation:\n\nWe define two additional MLP models with two and three hidden\n\nlayers using the previously defined functions.\n\nWe train both models using the training data and validation data.\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how the addition of layers affects the\n\nmodel’s performance.\n\n[End of response]",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "We will execute the code that defines the two additional models with two\n\nand three hidden layers. It also trains the models, evaluates the performance\n\nof both models, and prints the results.\n\nFigure 14.9: Feature 2 – adding layers to the model\n\nBased on these results, we can observe that the two-layer MLP model\n\nachieved slightly higher accuracy compared to both the single-layer and\n\nthree-layer MLP models. However, the differences in accuracy are\n\nrelatively small, indicating that the addition of layers did not significantly\n\nimprove the model’s performance in this case. Let’s move on to the next\n\nfeature and experiment with batch sizes using the baseline model.\n\nFeature 3: Experimenting with batch sizes We have now learned how to add more layers, and in our case, due to the\n\ndataset being simple, there was not much impact. We will now experiment\n\nwith different batch sizes. We will apply a prompting strategy to improvise\n\na model and ask ChatGPT to provide us the code for it.",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "[Prompt]\n\nCan you please provide me code with three combinations of batch sizes and\n\ncompare the baseline single-layer model performance for these\n\ncombinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different batch\n\nsizes along with comparison results.\n\n[Prompt response]\n\nDefine and Train Models with Different Batch Sizes:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent combinations of batch sizes.\n\nCombination 1: Batch Size = 32\n\nCombination 2: Batch Size = 64\n\nCombination 3: Batch Size = 128\n\nNow, let’s define and train these models:\n\n# Define and train model with batch size = 32 model_batch_32 = create_single_layer_mlp(input_shape, num_class history_batch_32 = train_model(model_batch_32, x_train, y_train # Define and train model with batch size = 64 model_batch_64 = create_single_layer_mlp(input_shape, num_class history_batch_64 = train_model(model_batch_64, x_train, y_train # Define and train model with batch size = 128 model_batch_128 = create_single_layer_mlp(input_shape, num_clas history_batch_128 = train_model(model_batch_128, x_train, y_tra\n\nModel Comparison:",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "Once the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.\n\n# Evaluate models with different batch sizes accuracy_batch_32, _, _, _ = evaluate_model(model_batch_32, x_t accuracy_batch_64, _, _, _ = evaluate_model(model_batch_64, x_t accuracy_batch_128, _, _, _ = evaluate_model(model_batch_128, x # Print model comparison print(\"Single-layer MLP Accuracy (Batch Size = 32):\", accuracy_ print(\"Single-layer MLP Accuracy (Batch Size = 64):\", accuracy_ print(\"Single-layer MLP Accuracy (Batch Size = 128):\", accuracy\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent combinations of batch sizes (32, 64, and 128).\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different batch sizes affect the\n\nmodel’s performance.\n\n[End of response]\n\nWe will execute the code that defines models with batch sizes 32, 64, and\n\n128. It also trains the models, evaluates their performance, and prints the\n\nresults of the comparison of this performance.",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "Figure 14.10: Code execution – batch size comparison\n\nWe can see that there is not much difference between the models’\n\naccuracies, so we will apply the ‘Learn prompting strategy’ (LIFE 3.1) and\n\nask ChatGPT for possible reasons as to why this might be.\n\n[Prompt]\n\n282/282 [==============================] - 1s 3ms/step 282/282 Single-layer MLP Accuracy (Batch Size = 32): 0.88 Single-layer MLP Accuracy (Batch Size = 64): 0.8723333333333333 Single-layer MLP Accuracy (Batch Size = 128): 0.879777777777777\n\nWhy there is not much difference in the accuracies?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT explains\n\nthe possible reasons.\n\n[Prompt response]\n\nThe differences in accuracy between the models trained with different batch\n\nsizes are relatively small. There could be several reasons for this:",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Dataset Size: The Fashion-MNIST dataset is relatively small\n\ncompared to other datasets commonly used in deep learning. With a\n\nsmaller dataset, the impact of batch size on model performance may be\n\nless pronounced.\n\nModel Complexity: The baseline single-layer MLP model may not be\n\ncomplex enough to fully leverage the benefits of different batch sizes.\n\nIn more complex models, such as deep neural networks with multiple layers, the impact of batch size on training dynamics may be more\n\nsignificant.\n\nOptimization Algorithm: The Adam optimizer used in training the models adapts the learning rate for each parameter based on the first\n\nand second moments of the gradients. This adaptive learning rate can\n\nmitigate the impact of batch size to some extent.\n\nConvergence: The models may have already converged to a similar level of performance regardless of the batch size used. Once a model\n\nconverges, further training with different batch sizes may not lead to\n\nsubstantial improvements in accuracy.\n\nOverall, while batch size can influence training dynamics and convergence\n\nspeed, its impact on model performance may be less pronounced in certain\n\nscenarios, especially with relatively simple models and datasets.\n\nExperimenting with larger and more complex models, as well as different\n\noptimization algorithms, may reveal more significant differences in\n\nperformance based on batch size.\n\n[End of response]\n\nNow that we have learned how to change the batch size and what the\n\npossible reasons could be for it not having much impact on our model, let’s",
      "content_length": 1559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "move on to the next feature and experiment with the number of neurons in\n\nthe hidden layer of the baseline model.\n\nFeature 4: Experimenting with the number of neurons We will apply a prompting strategy to improvise a model and ask ChatGPT\n\nto provide us the code for experimenting with different numbers of neurons\n\nin the hidden layer.\n\n[Prompt]\n\nCan you please provide me code with three combinations of numbers of\n\nneurons and compare the baseline single-layer model performance for the\n\nvarious combinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different numbers\n\nof neurons in the hidden layer, along with the comparison results.\n\n[Prompt response]\n\nOf course! Let’s define and train the baseline single-layer MLP model with\n\nthree different combinations of the number of neurons in the hidden layer,\n\nand then compare their performance.\n\nDefine and Train Models with Different Number of Neurons:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent combinations of the number of neurons in the hidden layer:\n\nCombination 1: Number of Neurons = 64",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "Combination 2: Number of Neurons = 128\n\nCombination 3: Number of Neurons = 256\n\nNow, let’s define and train these models:\n\n# Define and train model with 64 neurons in the hidden layer model_neurons_64 = create_single_layer_mlp(input_shape, num_cla history_neurons_64 = train_model(model_neurons_64, x_train, y_t # Define and train model with 128 neurons in the hidden layer model_neurons_128 = create_single_layer_mlp(input_shape, num_cl history_neurons_128 = train_model(model_neurons_128, x_train, y # Define and train model with 256 neurons in the hidden layer model_neurons_256 = create_single_layer_mlp(input_shape, num_cl history_neurons_256 = train_model(model_neurons_256, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.\n\n# Evaluate models with different number of neurons accuracy_neurons_64, _, _, _ = evaluate_model(model_neurons_64, accuracy_neurons_128, _, _, _ = evaluate_model(model_neurons_12 accuracy_neurons_256, _, _, _ = evaluate_model(model_neurons_25 # Print model comparison print(\"Single-layer MLP Accuracy (Neurons = 64):\", accuracy_neu print(\"Single-layer MLP Accuracy (Neurons = 128):\", accuracy_ne print(\"Single-layer MLP Accuracy (Neurons = 256):\", accuracy_ne\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent combinations of the number of neurons in the hidden layer\n\n(64, 128, and 256).",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "After training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different numbers of neurons in\n\nthe hidden layer affect the model’s performance.\n\n[End of response]\n\nWe will execute the code that defines the models with 64, 128, and 256\n\nneurons. It also trains the model, evaluates the performance for all the\n\nmodel variations, and prints the results of the comparison of their\n\nperformance.\n\nFigure 14.11: Code execution – neurons comparison\n\nWe can see that there is not much difference, so let’s move to our final\n\nfeature of experimenting with different optimizers.\n\nFeature 5: Trying diﬀerent optimizers We will apply a prompting strategy to improvise a model and ask ChatGPT\n\nto provide us code for experimenting with different optimizers such as",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "SGD, Adam, and RMSProp. Also, as part of our prompting strategy, we will\n\nbe specific and explicitly mention that the optimizer should be passed at\n\ncompile time and that labels should be one-hot encoded, as these steps are\n\ncommonly ignored by ChatGPT.\n\n[Prompt]\n\nCan you please provide me code with different optimizers of SGD, Adam,\n\nand RMSprop and compare the baseline single-layer model performance for\n\nvarious combinations? The optimizer should be passed in at compile time\n\nand you should ensure that labels are one-hot encoded.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for different\n\noptimizers along with comparison results.\n\n[Prompt response]\n\nCertainly! Let’s define and train a baseline single-layer MLP model with\n\nthree different optimizers (SGD, Adam, and RMSprop), and then compare\n\ntheir performance.\n\nDefine and Train Models with Different Optimizers:\n\nWe’ll define and train the baseline single-layer MLP model with three\n\ndifferent optimizers: SGD, Adam, and RMSprop.\n\nNow, let’s define and train these models:\n\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop # Define and train model with SGD optimizer def train_model_sgd(model, x_train, y_train, x_val, y_val, batc # One-hot encode the class labels y_train_encoded = to_categorical(y_train)",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "y_val_encoded = to_categorical(y_val) # Compile the model with SGD optimizer model.compile(loss='categorical_crossentropy', optimizer=SG # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Define and train model with Adam optimizer def train_model_adam(model, x_train, y_train, x_val, y_val, bat # One-hot encode the class labels y_train_encoded = to_categorical(y_train) y_val_encoded = to_categorical(y_val) # Compile the model with Adam optimizer model.compile(loss='categorical_crossentropy', optimizer=Ad # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Define and train model with RMSprop optimizer def train_model_rmsprop(model, x_train, y_train, x_val, y_val, # One-hot encode the class labels y_train_encoded = to_categorical(y_train) y_val_encoded = to_categorical(y_val) # Compile the model with RMSprop optimizer model.compile(loss='categorical_crossentropy', optimizer=RM # Train the model on the training data history = model.fit(x_train, y_train_encoded, batch_size=ba return history # Train model with SGD optimizer model_sgd = create_single_layer_mlp(input_shape, num_classes) history_sgd = train_model_sgd(model_sgd, x_train, y_train, x_va # Train model with Adam optimizer model_adam = create_single_layer_mlp(input_shape, num_classes) history_adam = train_model_adam(model_adam, x_train, y_train, x # Train model with RMSprop optimizer model_rmsprop = create_single_layer_mlp(input_shape, num_classe history_rmsprop = train_model_rmsprop(model_rmsprop, x_train, y\n\nModel Comparison:\n\nOnce the models are trained, we can compare their performance based on\n\nevaluation metrics such as accuracy.",
      "content_length": 1743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "# Evaluate models with different optimizers accuracy_sgd, _, _, _ = evaluate_model(model_sgd, x_test, y_tes accuracy_adam, _, _, _ = evaluate_model(model_adam, x_test, y_t accuracy_rmsprop, _, _, _ = evaluate_model(model_rmsprop, x_tes # Print model comparison print(\"Single-layer MLP Accuracy (SGD):\", accuracy_sgd) print(\"Single-layer MLP Accuracy (Adam):\", accuracy_adam) print(\"Single-layer MLP Accuracy (RMSprop):\", accuracy_rmsprop)\n\nExplanation:\n\nWe define and train three baseline single-layer MLP models with\n\ndifferent optimizers (SGD, Adam, and RMSprop).\n\nAfter training, we evaluate the performance of each model using the\n\ntesting data and compare their accuracies.\n\nThe comparison helps us understand how different optimizers affect the\n\nmodel’s performance.\n\n[End of response]\n\nWe execute the code that defines the models with different optimizers. It\n\nalso trains the model, evaluates the performance of all the model variations,\n\nand prints the results for a comparison of their performance.",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Figure 14.12: Code Execution – Different Optimizers\n\nBased on these results, we can observe that the Adam optimizer achieved\n\nthe highest accuracy, followed by RMSprop and SGD. This indicates that\n\nAdam performed better in optimizing the model’s parameters and\n\nimproving its performance on the testing data.\n\nAssignment Compare the model performance of a single layer versus two hidden layers,\n\nbut with 20 and 50 epochs.\n\nChallenge With the help of ChatGPT, improvise a model by adding a dropout layer\n\nand analyze the impact on model performance. Feel free to add more hidden\n\nlayers to the model.\n\nSummary",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "The prompt strategies used in this chapter provided a structured approach to\n\nlearning and building a classification model using an MLP where ChatGPT\n\nassisted in generating code. The user validated the code using a Colab\n\nnotebook and provided feedback to ChatGPT. By actively engaging with\n\nthe material, you experimented with different techniques and iteratively refined your understanding, ultimately leading to a more comprehensive\n\ngrasp of classification model creation using MLPs.\n\nIn the next chapter, we will learn how to use ChatGPT to generate code for\n\nConvolutional Neural Networks (CNNs) with the help of the CIFAR-10\n\ndataset.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "15\n\nBuilding a CNN Model for CIFAR-10 with ChatGPT\n\nIntroduction Having explored the depths of the Multi-Layer Perceptron (MLP) in our\n\nprevious chapter with the Fashion-MNIST dataset, we now pivot to a more\n\nintricate and visually complex challenge. This chapter marks our transition\n\nfrom the primarily tabular, grayscale world of Fashion-MNIST to the\n\ncolorful and diverse realm of the CIFAR-10 dataset. Here, we elevate our\n\nfocus to Convolutional Neural Networks (CNNs), a class of deep neural\n\nnetworks that are revolutionizing the way we approach image classification\n\ntasks.\n\nOur journey through the MLP chapter provided a strong foundation for\n\nunderstanding the basics of neural networks and their application in\n\nclassifying simpler, grayscale images. Now, we step into a more advanced territory where CNNs reign supreme. The CIFAR-10 dataset, with its array\n\nof 32x32 color images across 10 different classes, presents a unique set of challenges that MLPs are not best suited to address. This is where CNNs, with their ability to capture spatial and textural patterns in images, come into play.",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "As we transition from MLPs to CNNs, we carry forward the insights and\n\nknowledge gained, applying them to a more complex dataset that closely\n\nmimics real-world scenarios. The CIFAR-10 dataset not only tests the limits\n\nof image classification models but also serves as an excellent platform for us\n\nto explore the advanced capabilities of CNNs.\n\nThis chapter aims to build upon what we learned about neural networks and\n\nguide you through the nuances of CNNs. We will delve into why CNNs are\n\nthe preferred choice for image data, how they differ from MLPs in handling\n\ncolor and texture, and what makes them so effective in classifying images from the CIFAR-10 dataset. Prepare to embark on a journey that takes you\n\nfrom the fundamentals to the more sophisticated aspects of CNNs.\n\nBusiness problem The CIFAR-10 dataset presents a business challenge to companies seeking to\n\nenhance image recognition capabilities for various objects and optimize\n\ndecision-making processes based on visual data. A multitude of industries, such as e-commerce, autonomous driving, and surveillance, can benefit from\n\naccurate object classification and detection. By harnessing machine learning algorithms, businesses aim to improve efficiency, enhance their user\n\nexperience, and streamline operations.\n\nProblem and data domain In this context, we will utilize CNNs to tackle the object recognition task using the CIFAR-10 dataset. CNNs are particularly effective for image-\n\nrelated problems due to their ability to automatically learn hierarchical features from raw pixel data. By training a CNN model on the CIFAR-10",
      "content_length": 1603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "dataset, we aim to develop a robust system capable of accurately classifying objects into one of the ten predefined categories. This model can be applied\n\nin various domains, such as image-based search engines, automated surveillance systems, and quality control in manufacturing.\n\nDataset overview The CIFAR-10 dataset comprises 60,000 color images, divided into 10\n\nclasses, with 6,000 images per class. Each image has dimensions of 32x32 pixels and is represented in RGB format. The dataset is split into a training\n\nset of 50,000 images and a test set of 10,000 images.\n\nFeatures in the dataset include:\n\nImage data: Color images of various objects, each represented as a 3- dimensional array containing pixel intensities for red, green, and blue\n\nchannels. These images serve as input data for training the CNN model.\n\nLabel: The class label assigned to each image, representing the category of the depicted object. The labels range from 0 to 9,\n\ncorresponding to classes such as airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n\nBy analyzing the CIFAR-10 dataset and its associated labels, our goal is to\n\ntrain a CNN model capable of accurately identifying objects depicted in images. This predictive model can then be deployed in real-world\n\napplications to automate object recognition tasks, improve decision-making\n\nprocesses, and enhance overall efficiency in diverse industries.",
      "content_length": 1415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Figure 15.1: CIFAR-10 dataset\n\nBreaking the problem down into features Given the CIFAR-10 dataset and the application of CNNs for image\n\nrecognition, we outline the following features to guide users through\n\nbuilding and optimizing CNN models:\n\nBuilding the baseline CNN model with a single convolutional layer:\n\nUsers will start by constructing a simple CNN model with a single\n\nconvolutional layer for image classification. This feature focuses on defining the basic architecture, including convolutional filters,\n\nactivation functions, and pooling layers, to establish a foundational\n\nunderstanding of CNNs.",
      "content_length": 610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "Experimenting with the addition of convolutional layers: Users will\n\nexplore the impact of adding additional convolutional layers to the baseline model architecture. By incrementally increasing the depth of\n\nthe network, users can observe how the model’s capacity to capture\n\nhierarchical features evolves and its ability to learn complex patterns\n\nimproves.\n\nIncorporating dropout regularization: Users will learn how to integrate dropout regularization into the CNN model to mitigate\n\noverfitting and improve generalization performance. By randomly\n\ndropping units during training, dropout helps prevent the network from\n\nrelying too heavily on specific features and encourages robust feature\n\nlearning.\n\nImplementing batch normalization: Users will explore the benefits of\n\nbatch normalization in stabilizing training dynamics and accelerating\n\nconvergence. This feature focuses on incorporating batch normalization\n\nlayers into the CNN architecture to normalize activations and reduce internal covariate shift, leading to faster and more stable training.\n\nOptimizing with different optimizers: This feature explores the\n\neffects of using various optimization algorithms, including SGD, Adam,\n\nand RMSprop, to train the CNN model. Users will compare the training\n\ndynamics, convergence speed, and final model performance achieved with different optimizers, allowing them to select the most suitable\n\noptimization strategy for their specific task.\n\nPerforming data augmentation: Users will experiment with data\n\naugmentation techniques such as rotation, flipping, zooming, and\n\nshifting to increase the diversity and size of the training dataset. By generating augmented samples on the fly during training, users can",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "improve the model’s ability to generalize to unseen data and enhance robustness against variations in input images.\n\nBy following these features, users will gain practical insights into building,\n\nfine-tuning, and optimizing CNN models for image classification tasks using\n\nthe CIFAR-10 dataset. They will learn how to systematically experiment\n\nwith different architectural components, regularization techniques, and optimization strategies to achieve superior performance and accuracy in\n\nobject recognition.\n\nPrompting strategy To leverage ChatGPT for machine learning, we need to have a clear\n\nunderstanding of how to implement prompting strategies specifically for\n\ncode generation for machine learning.\n\nLet’s brainstorm what we would like to achieve in this task to get a better\n\nunderstanding of what needs to go into the prompts.\n\nStrategy 1: Task-Actions- Guidelines (TAG) prompt strategy 1.1 - Task: The specific task or goal is to build and optimize a CNN model\n\nfor the CIFAR-10 dataset.\n\n1.2 - Actions: The key steps involved in building and optimizing a CNN\n\nmodel for the CIFAR-10 dataset include:\n\nPreprocessing the image data: Normalize the pixel values and resize\n\nthe images to a standardized size.",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Model construction: Define the baseline CNN model architecture with\n\na single convolutional layer.\n\n1.3 - Guidelines: We will provide the following guidelines to ChatGPT in\n\nour prompt:\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method\n\nused in the code.\n\nStrategy 2: Persona- Instructions-Context (PIC) prompt strategy 2.1 - Persona: Adopt the persona of a beginner who needs step-by-step\n\nguidance on building and optimizing CNN models for image classification tasks.\n\n2.2 - Instructions: Request ChatGPT to generate code for each feature one step at a time and wait for user feedback before proceeding to the next step.\n\n2.3 - Context: Given that the focus is on building CNN models for image classification tasks using the CIFAR-10 dataset, ChatGPT is already aware\n\nof the dataset and its characteristics, so additional context may not be necessary.\n\nStrategy 3: Learn-Improvise- Feedback-Evaluate (LIFE)",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "prompt strategy 3.1 - Learn:\n\nEmphasize the importance of learning about CNN models and their\n\ncomponents, including convolutional layers, pooling layers, dropout regularization, batch normalization, data augmentation, and\n\noptimization algorithms.\n\n3.2 - Improvise:\n\nWe will improvise later by adding more layers, dropout layers, pooling, data augmentation, and so on.\n\n3.3 - Feedback:\n\nShare code and model outputs for feedback to ensure effective learning\n\nand understanding.\n\nIncorporate suggestions and critiques to refine the model and the approach.\n\nProvide errors to troubleshoot and resolve the issues.\n\n3.4- Evaluate:\n\nExecute the code provided by ChatGPT to ensure accuracy and validity. This is used throughout the chapter.\n\nBuilding a CNN model to accurately classify the CIFAR- 10 images using the free version of ChatGPT",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "The premium version of ChatGPT has a code interpreter, but it doesn’t\n\nsupport deep learning libraries such as Keras and TensorFlow, which are required to build ML code. Hence, for this chapter, we will stick to the free\n\nversion of ChatGPT.\n\nFeature 1: Building the baseline CNN model with a single convolutional layer Let’s craft our initial prompt for the baseline model.\n\n[Prompt]\n\nI want to create a simple classification model for CIFAR-10 dataset (TAG\n\n1.1) that consists of a single-layer CNN model (PIC 2.2). It should consist of following steps (TAG 1.2) and provide one step at a time, waiting for user\n\nfeedback (PIC 2.2):\n\nPreprocessing the image data: Normalize the pixel values and resize the images to a standardized size.\n\nModel construction: Define the baseline CNN model architecture with a single convolutional layer.\n\nI am a beginner user (PIC 2.1), so please provide step-by-step code and strictly follow the below instructions (TAG 1.3):\n\nThe code should be compatible with Jupyter Notebook.\n\nEnsure that there are detailed comments for each line of code.\n\nYou have to explain each line of code in detail, covering each method used in the code.\n\n[End of prompt]",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "You should receive a response similar to the following, where ChatGPT has provided us with the code for the data preprocessing step along with detailed\n\ncomments and explanations.\n\n[Prompt response]\n\nSure, let’s start with the first step: preprocessing the image data. In this step,\n\nwe will normalize the pixel values and resize the images to a standardized\n\nsize.\n\n# Step 1: Preprocessing the image data # Import necessary libraries import numpy as np import tensorflow as tf from tensorflow.keras.datasets import cifar10 from tensorflow.keras.utils import to_categorical from skimage.transform import resize # Load CIFAR-10 dataset (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Normalize pixel values to range [0, 1] x_train = x_train.astype('float32') / 255.0 x_test = x_test.astype('float32') / 255.0 # Resize images to a standardized size (e.g., 32x32 pixels) standardized_size = (32, 32) x_train_resized = np.array([resize(image, standardized_size) for x_test_resized = np.array([resize(image, standardized_size) for # Display shape of datasets after preprocessing print(\"Shape of x_train:\", x_train_resized.shape) print(\"Shape of x_test:\", x_test_resized.shape)\n\nNow, let’s break down each line of code:\n\nWe import the necessary libraries:\n\nnumpy for numerical operations.\n\ntensorflow for building and training the model.",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "cifar10 dataset from tensorflow.keras.datasets for loading CIFAR-10\n\ndata.\n\nto_categorical function from tensorflow.keras.utils for one-hot encoding labels.\n\nresize function from skimage.transform for resizing images.\n\nWe load the CIFAR-10 dataset using cifar10.load_data() function, which returns training and testing data splits.\n\nWe normalize pixel values of images to the range [0, 1] by dividing\n\nby 255.0.\n\nWe define the standardized size for resizing images (e.g., 32x32 pixels).\n\nWe resize all images in the training and testing sets to the standardized\n\nsize using list comprehension and the resize function.\n\nFinally, we print the shapes of the training and testing datasets after\n\npreprocessing to verify the changes.\n\nLet me know if you have any questions or if you’d like to proceed to the\n\nnext step!\n\n[End of response]\n\nWe will now apply the evaluate prompting strategy (LIFE 3.4) and validate\n\nthe code by executing the code in Google Colab for every step. The code loads the model from Keras and normalizes the images. The code runs\n\nperfectly fine without requiring any modifications or returning any errors.\n\nPlease refer to the Building a CNN Model for CIFAR-10 with ChatGPT.ipynb notebook for the executable code snippets or copy the code\n\nfrom the prompt response.",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "Figure 15.2: Code execution – data preprocessing\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following, where ChatGPT should\n\nprovide the code for creating a baseline model.\n\n[Prompt response]\n\nGreat! The next step is to construct the baseline CNN model architecture with a single convolutional layer.\n\n# Step 2: Model Construction # Import necessary libraries from tensorflow.keras.models import Sequential",
      "content_length": 521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatte # Define the baseline CNN model architecture model = Sequential([ # Add a convolutional layer with 32 filters, kernel size of Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 # Add a max pooling layer with pool size of (2, 2) MaxPooling2D((2, 2)), # Flatten the output of the convolutional layer Flatten(), # Add a fully connected layer with 64 neurons and ReLU activ Dense(64, activation='relu'), # Add the output layer with 10 neurons (one for each class) Dense(10, activation='softmax') ]) # Compile the model model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Display model summary model.summary()\n\nNow, let’s explain each line of code:\n\nWe import the necessary libraries:\n\nSequential from tensorflow.keras.models to create a sequential model.\n\nConv2D, MaxPooling2D, Flatten, and Dense layers from\n\ntensorflow.keras.layers to define the different types of layers in the CNN model.\n\nWe define the baseline CNN model architecture using the Sequential API, which allows us to create a linear stack of layers.\n\nWithin the Sequential model:\n\nWe add a convolutional layer (Conv2D) with 32 filters, a kernel size of\n\n(3, 3), and ReLU activation function. The input_shape parameter defines the shape of input images.",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "We add a max pooling layer (MaxPooling2D) with a pool size of (2, 2)\n\nto downsample the feature maps.\n\nWe flatten the output of the convolutional layer using the Flatten layer to prepare it for the fully connected layers.\n\nWe add a fully connected layer (Dense) with 64 neurons and a ReLU\n\nactivation function.\n\nFinally, we add the output layer with 10 neurons (one for each class in CIFAR-10) and the softmax activation function.\n\nWe compile the model using model.compile() with the following:\n\noptimizer='adam' to use the Adam optimizer.\n\nloss='sparse_categorical_crossentropy' as the loss function, suitable\n\nfor classification tasks with integer labels.\n\nmetrics=['accuracy'] to monitor the accuracy during training.\n\nWe display the summary of the model architecture using model.summary().\n\nLet me know if you have any questions or if you’d like to make any adjustments!\n\n[End of response]\n\nWe execute the code in the notebook and it runs perfectly. A model summary is printed, which includes the Conv2d layer with 32 filters and 896 params, followed by a MaxPooling2D layer, a flatten layer, which doesn’t have any\n\nparameters, then a dense layer with 64 neurons and the ReLU activation function, and finally the dense layer with 10 neurons and the softmax activation function.",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Figure 15.3: Code execution – baseline model summary\n\nWe will now instruct ChatGPT to proceed to the next step.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should\n\nprovide the code for training the model.\n\n[Prompt response]",
      "content_length": 302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Sure! The next step is to train the model on the preprocessed CIFAR-10 dataset.\n\n# Step 3: Model Training # Train the model history = model.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resized, y_test))\n\nHere’s an explanation of the code:\n\nWe train the model using the fit method on the preprocessed training data (x_train_resized) and corresponding labels (y_train).\n\nWe specify the number of training epochs (epochs=10) to train the\n\nmodel for 10 iterations over the entire dataset.\n\nWe set the batch size (batch_size=32) to specify the number of samples per gradient update. It is a hyperparameter that determines how many samples are processed before updating the model’s parameters.\n\nWe provide the validation data (validation_data=(x_test_resized,\n\ny_test)) to evaluate the model’s performance on the testing data after each epoch.\n\nThe fit method returns a history object containing training metrics\n\n(e.g., loss and accuracy) for each epoch, which can be used for analysis and visualization.\n\nLet me know if you have any questions or if you’d like to proceed with\n\nevaluating the model or making any adjustments!\n\n[End of response]\n\nWe execute the code and it runs without any errors. The model is trained for\n\n10 epochs, after which we receive a validation accuracy of 62.6%, which is",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "not bad for a basic model.\n\nFigure 15.4: Code execution – model training\n\nWe will now instruct ChatGPT to proceed to the next step to evaluate the model performance.\n\n[Prompt]\n\nLet’s proceed to the next step.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT should provide the code for model evaluation along with comments.\n\n[Prompt response]\n\nSure! The next step is to evaluate the trained model on the testing dataset and visualize its performance.\n\n# Step 4: Model Evaluation and Visualization import matplotlib.pyplot as plt # Evaluate the model on the testing dataset test_loss, test_accuracy = model.evaluate(x_test_resized, y_test print(\"Test Loss:\", test_loss) print(\"Test Accuracy:\", test_accuracy)",
      "content_length": 738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "# Plot training history plt.plot(history.history['accuracy'], label='Training Accuracy') plt.plot(history.history['val_accuracy'], label='Validation Accu plt.title('Training and Validation Accuracy') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() plt.show()\n\nHere’s what this code does:\n\nWe import matplotlib.pyplot to visualize the training history.\n\nWe evaluate the trained model on the testing dataset using the evaluate method, which returns the test loss and accuracy.\n\nWe print the test loss and test accuracy to assess the model’s\n\nperformance on unseen data.\n\nWe plot the training and validation accuracy over epochs using matplotlib.pyplot. The history object returned by the fit method\n\ncontains accuracy values for both training and validation datasets across epochs.\n\nThe history.history['accuracy'] contains training accuracy values.\n\nThe history.history['val_accuracy'] contains validation accuracy\n\nvalues.\n\nThe plot helps us visualize how well the model is learning over time and whether it is overfitting or underfitting.\n\nLet me know if you have any questions or if you’d like to analyze the results further!\n\n[End of response]\n\nWe execute the code and it runs without any errors. It evaluates the test loss\n\nand test accuracy (62.5%) and also plots the graph of training accuracy vs.",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "validation accuracy, showing that model is not performing well on the validation set.\n\nFigure 15.5: Code execution – model evaluation\n\nThis completes feature 1, where we achieved an accuracy of 0.6259\n\n(62.59%) with our baseline CNN model. (Please note that you may receive different accuracy scores even after running the exact same code.) Next, we",
      "content_length": 349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "will move to the second feature of adding more layers to the model and learn\n\nhow ChatGPT can assist us in that.\n\nFeature 2: Experimenting with the addition of convolutional layers Now that we have a baseline model architecture, let us try adding layers to it.\n\nWe will apply the prompting strategy to improvise the model (LIFE 3.4) and ask ChatGPT to provide us the code for it, also note the importance of providing specific instructions by using the phrase (on top of our baseline\n\nmodel) (PIC 2.2) and keyword incrementally (PIC 2.2); without these, the response wouldn’t be fully consistent with our expectations.\n\n[Prompt]\n\nWe received an accuracy of 62.59 with the baseline model, so now I would\n\nlike to experiment with the addition of two consecutive layers of 64 filters first and then two consecutive layers of 128 filters on top of our baseline model. Then, let’s compare the result of these two incremental changes with our baseline model.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for a modified model with consecutive layers of 64 filters and another one with consecutive layers of 128 filters.\n\n[Prompt response]",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "Sure! Let’s first add two consecutive layers of 64 filters to the baseline model and train it again. After that, we’ll add two consecutive layers of 128 filters and train the model once more. Finally, we’ll compare the results of\n\nthese two changes with the baseline model.\n\nHere’s how we can modify the baseline model to include additional layers:\n\n# Define the modified model with two consecutive layers of 64 fi modified_model_64 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_64.compile(optimizer='adam', loss='sparse_categorical_crossentropy' metrics=['accuracy']) # Display model summary modified_model_64.summary() # Train the modified model with two consecutive layers of 64 fil history_64 = modified_model_64.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resi # Evaluate the modified model on the testing dataset test_loss_64, test_accuracy_64 = modified_model_64.evaluate(x_te print(\"Test Loss (Modified Model with 64 Filters):\", test_loss_6 print(\"Test Accuracy (Modified Model with 64 Filters):\", test_ac\n\nNow, let’s explain the modifications:",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "We define the modified model (modified_model_64) by adding two\n\nconsecutive convolutional layers with 64 filters each after the baseline model’s architecture.\n\nWe compile the modified model with the same settings as the baseline\n\nmodel.\n\nWe train the modified model (modified_model_64) on the preprocessed training data (x_train_resized).\n\nWe evaluate the performance of the modified model on the testing dataset (x_test_resized) and print the test loss and accuracy.\n\nNext, we’ll repeat the process for adding two consecutive layers of 128\n\nfilters:\n\n# Define the modified model with two consecutive layers of 128 f modified_model_128 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_128.compile(optimizer='adam', loss='sparse_categorical_crossentropy metrics=['accuracy']) # Display model summary modified_model_128.summary() # Train the modified model with two consecutive layers of 128 fi history_128 = modified_model_128.fit(x_train_resized, y_train,",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "epochs=10, batch_size=32, validation_data=(x_test_res # Evaluate the modified model on the testing dataset test_loss_128, test_accuracy_128 = modified_model_128.evaluate(x print(\"Test Loss (Modified Model with 128 Filters):\", test_loss_ print(\"Test Accuracy (Modified Model with 128 Filters):\", test_a\n\nThis code follows the same structure as the previous modification but adds two consecutive convolutional layers with 128 filters each. We train and evaluate the model accordingly.\n\nLet me know if you have any questions or if you’d like to proceed with the\n\nmodifications!\n\n[End of response]\n\nWe first execute the code that defines the two additional layers with 64 filters. It runs perfectly fine, training the model and achieving an improved accuracy of 71%.",
      "content_length": 762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "Figure 15.6: Adding two consecutive layers with 64 filters to the model\n\nNext, we execute the code to add further depth to the model by adding two\n\nmore layers with 128 filters. In this case, the code results in an error, so we will use a prompting strategy and provide the error back to ChatGPT.\n\nYou should see a ValueError from running the code.\n\n[Prompt]\n\nWe received the below error. Also, ensure that maxpooling doesn’t reduce\n\nthe dimension too much for 128 filters. Error is below:\n\nValueError: Exception encountered when calling layer \"conv2d_38\" Negative dimension size caused by subtracting 3 from 2 for '{{no",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "Call arguments received by layer \"conv2d_38\" (type Conv2D): inputs=tf.Tensor(shape=(None, 2, 2, 64), dtype=float32).\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the modified code to resolve the issue.\n\n[Prompt response]\n\nIt seems we’re encountering an issue where the dimensions after the last max\n\npooling layer are too small for the subsequent convolutional layer with 128 filters. To address this, we’ll adjust the architecture to ensure compatibility between layers.\n\nHere’s the corrected code:\n\n# Define the modified model with two consecutive layers of 128 f modified_model_64_to_128 = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), # A Conv2D(128, (3, 3), activation='relu', padding='same'), # A MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dense(10, activation='softmax') ]) # Compile the modified model modified_model_64_to_128.compile(optimizer='adam', loss='sparse_categorical_cross metrics=['accuracy']) # Display model summary modified_model_64_to_128.summary()",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "# Train the modified model with two consecutive layers of 128 fi history_64_to_128 = modified_model_64_to_128.fit(x_train_resized epochs=10, batch_size=32, validation_data # Evaluate the modified model on the testing dataset test_loss_64_to_128, test_accuracy_64_to_128 = modified_model_64 print(\"Test Loss (Modified Model with 64 to 128 Filters):\", test print(\"Test Accuracy (Modified Model with 64 to 128 Filters):\",\n\nIn this modification:\n\nWe’ve removed the max pooling layer after the last convolutional layer with 64 filters to ensure that the feature map dimensions are not reduced too much before applying the subsequent convolutional layers with 128 filters.\n\nWe’ve added same padding to the convolutional layers with 128 filters to ensure compatibility with the input dimensions.\n\nThe rest of the architecture remains the same.\n\nThis adjustment should resolve the issue, and the model should compile and\n\ntrain properly. Let me know if you have any questions or if you encounter any further issues!\n\n[End of response]\n\nLet’s execute the code, which removes some max pooling layers and also has padding = 'same' added to ensure the dimensions are compatible. It\n\nruns successfully, training the model but reducing the accuracy to 69%.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Figure 15.7: Added layers with 128 filters\n\nBased on these results, we can observe that the addition of more layers\n\nachieved a slightly higher accuracy compared to the baseline model. However, the differences in accuracy are relatively small, indicating that the\n\naddition of layers did not significantly improve the model’s performance and\n\nthe model is also still not regularized. Let’s move on to the next feature and experiment with dropout regularization.\n\nFeature 3: Incorporating dropout regularization",
      "content_length": 510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "Now that we have learned how to add more layers, we will now experiment\n\nwith a dropout layer. We will apply the prompting strategy to improvise the\n\nmodel (LIFE 3.4) and ask ChatGPT to provide us the code for it.\n\nAlso note that we have applied the prompting strategy of using specific instructions (PIC 2.2) by asking it to add the dropout over the previously provided code.\n\n[Prompt]\n\nThe model accuracy of the baseline model was 62.5%, with additional layers of 64 filters it was 71%, and the further addition of layers with 128 filters reduced it to 69%. Can we please apply dropout regularization to the model code given by you in your previous message?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for the addition of a dropout layer.\n\n[Prompt response]\n\nSure, let’s add dropout regularization to the modified model with two consecutive layers of 128 filters. Dropout regularization helps prevent overfitting by randomly dropping a fraction of the units (neurons) during training. Here’s how we can modify the model to include dropout layers:\n\nfrom tensorflow.keras.layers import Dropout # Define the modified model with dropout regularization modified_model_64_to_128_with_dropout = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3 MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'),",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Conv2D(128, (3, 3), activation='relu', padding='same'), Dropout(0.25), # Add dropout regularization with a dropout Conv2D(128, (3, 3), activation='relu', padding='same'), Dropout(0.25), # Add dropout regularization with a dropout MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), Dropout(0.5), # Add dropout regularization with a dropout r Dense(10, activation='softmax') ]) # Compile the modified model with dropout regularization modified_model_64_to_128_with_dropout.compile(optimizer='adam', loss='sparse_categ metrics=['accuracy # Display model summary modified_model_64_to_128_with_dropout.summary() # Train the modified model with dropout regularization history_64_to_128_with_dropout = modified_model_64_to_128_with_d\n\n# Evaluate the modified model with dropout regularization on the test_loss_64_to_128_with_dropout, test_accuracy_64_to_128_with_d print(\"Test Loss (Modified Model with Dropout):\", test_loss_64_t print(\"Test Accuracy (Modified Model with Dropout):\", test_accur\n\nIn this modification:\n\nWe add dropout layers after the convolutional layers with 128 filters. We set the dropout rates to 25% and 50% for the first and last dropout layers, respectively. These values can be adjusted based on experimentation and the level of regularization needed.\n\nWe compile and train the modified model with dropout regularization\n\nusing the same settings as before.\n\nWe evaluate the performance of the modified model with dropout regularization on the testing dataset and print the test loss and accuracy.",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "Adding dropout regularization may help improve the generalization\n\nperformance of the model and reduce overfitting. Let me know if you have any questions or if you’d like to explore further!\n\n[End of response]\n\nWe execute the code, which defines a model with a dropout layer. It also\n\ntrains and evaluates the performance of the model and prints the model summary covering loss and accuracy. Note that the dropout layer does not add any parameters to the model.\n\nFigure 15.8: Code execution – dropout layer",
      "content_length": 506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "We can see that there is not much difference between the accuracies, so we\n\nwill apply the Learn prompting strategy (LIFE 3.1) and ask ChatGPT to provide the plot of the loss and accuracy to see the impact of regularization.\n\n[Prompt]\n\nTest Loss (Modified Model with Dropout): 0.876133382320404 Test Accuracy (Modified Model with Dropout): 0.7014999985694885\n\nCan you please provide the plots for loss and accuracy to see if there has\n\nbeen any regularization and also explain it?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT provides the code and explains the significance of the plots.\n\n[Prompt response]\n\nCertainly! Let’s plot the training and validation loss and accuracy curves for the modified model with dropout regularization. This will help us visualize\n\nhow dropout regularization affects the training process and the model’s performance.\n\nimport matplotlib.pyplot as plt # Plot training history for loss plt.plot(history_64_to_128_with_dropout.history['loss'], label=' plt.plot(history_64_to_128_with_dropout.history['val_loss'], lab plt.title('Training and Validation Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.show() # Plot training history for accuracy plt.plot(history_64_to_128_with_dropout.history['accuracy'], lab plt.plot(history_64_to_128_with_dropout.history['val_accuracy'],",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "plt.title('Training and Validation Accuracy') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() plt.show()\n\nExplanation:\n\nTraining Loss vs. Validation Loss: This plot shows the trend of training and validation loss over epochs. If there is a large gap between training and validation loss, it indicates overfitting. Dropout\n\nregularization aims to reduce overfitting by preventing the model from becoming overly reliant on specific features, thereby reducing the gap between training and validation loss.\n\nTraining Accuracy vs. Validation Accuracy: This plot shows the trend of training and validation accuracy over epochs. Dropout regularization\n\ncan also help improve generalization performance, leading to better validation accuracy, especially if overfitting was present in the model.\n\nBy observing these plots, we can assess whether dropout regularization\n\neffectively mitigated overfitting and improved the model’s generalization performance. Let me know if you have any questions or if you need further\n\nexplanation!\n\n[End of response]\n\nWe will execute the code and plot the graphs to understand the impact of dropout.",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "Figure 15.9: Effect of the dropout layer",
      "content_length": 40,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "The output graphs show the training and validation loss and accuracy for a\n\nmodel trained on the CIFAR-10 dataset with a dropout layer. Both losses decrease over epochs while the accuracy improves, indicating effective\n\nlearning. The small gap between training and validation metrics suggests minimal overfitting, likely aided by the dropout layer. Overall, the model generalizes well to new data, demonstrating good performance.\n\nFeature 4: Implementing batch normalization We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for adding batch normalization to the previously provided code.\n\n[Prompt]\n\nI want to apply the batch normalization on the previously provided code.\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for batch normalization on our existing model.\n\n[Prompt response]\n\nSure! We can apply batch normalization to the convolutional layers of the\n\nmodel to improve training stability and convergence speed. Here’s how we can modify the model to include batch normalization layers:\n\nfrom tensorflow.keras.layers import BatchNormalization # Define the modified model with batch normalization modified_model_64_to_128_with_batchnorm = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), # Add batch normalization MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), BatchNormalization(), # Add batch normalization Dense(10, activation='softmax') ]) # Compile the modified model with batch normalization modified_model_64_to_128_with_batchnorm.compile(optimizer='adam' loss='sparse_cat metrics=['accura # Display model summary modified_model_64_to_128_with_batchnorm.summary() # Train the modified model with batch normalization history_64_to_128_with_batchnorm = modified_model_64_to_128_with\n\n# Evaluate the modified model with batch normalization on the te test_loss_64_to_128_with_batchnorm, test_accuracy_64_to_128_with print(\"Test Loss (Modified Model with Batch Normalization):\", te print(\"Test Accuracy (Modified Model with Batch Normalization):\"\n\nIn this modification:\n\nWe add BatchNormalization layers after each convolutional layer.\n\nBatch normalization normalizes the activations of the previous layer at each batch, making training more stable and accelerating convergence.",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "We compile and train the modified model with batch normalization using the same settings as before.\n\nWe evaluate the performance of the modified model with batch normalization on the testing dataset and print the test loss and accuracy.\n\nAdding batch normalization can improve the performance and stability of\n\nthe model during training. Let me know if you have any questions or if\n\nyou’d like further explanation!\n\n[End of response]\n\nWe will execute the code, evaluate the model’s performance, and print the results. Note that batch normalization adds a small number of parameters, unlike dropout or maxpooling.",
      "content_length": 612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "Figure 15.10: Batch normalization\n\nWe can see that there are some non-trainable parameters. Let us make use of\n\nthe Learn prompting strategy (LIFE 3.1) to understand the calculations of non-trainable parameters.\n\n[Prompt]\n\nCan you please explain the calculation of 832 in the model summary?",
      "content_length": 290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "\"sequential_1\" ________________________________________________________________ Layer (type) Output Shape Param # ================================================================ conv2d_5 (Conv2D) (None, 30, 30, 32) 896\n\nbatch_normalization (Batch (None, 30, 30, 32) 128 Normalization)\n\nmax_pooling2d_3 (MaxPoolin (None, 15, 15, 32) 0 g2D)\n\nconv2d_6 (Conv2D) (None, 13, 13, 64) 18496\n\nbatch_normalization_1 (Bat (None, 13, 13, 64) 256 chNormalization)\n\nmax_pooling2d_4 (MaxPoolin (None, 6, 6, 64) 0 g2D)\n\nconv2d_7 (Conv2D) (None, 4, 4, 64) 36928\n\nconv2d_8 (Conv2D) (None, 4, 4, 128) 73856\n\nbatch_normalization_2 (Bat (None, 4, 4, 128) 512 chNormalization)\n\nmax_pooling2d_5 (MaxPoolin (None, 2, 2, 128) 0 g2D)\n\nconv2d_9 (Conv2D) (None, 2, 2, 128) 147584\n\nbatch_normalization_3 (Bat (None, 2, 2, 128) 512 chNormalization)\n\nmax_pooling2d_6 (MaxPoolin (None, 1, 1, 128) 0 g2D)\n\nflatten_1 (Flatten) (None, 128) 0\n\ndense_2 (Dense) (None, 64) 8256",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "batch_normalization_4 (Bat (None, 64) 256 chNormalization)\n\ndense_3 (Dense) (None, 10) 650\n\n================================================================ Total params: 288330 (1.10 MB) Trainable params: 287498 (1.10 MB) Non-trainable params: 832 (3.25 KB)\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with an explanation of the calculation of non-trainable parameters.\n\n[Prompt response]\n\nIn the model you’ve described, the non-trainable parameters are 832, which\n\nspecifically come from the batch normalization layers. Here’s how this number is calculated:\n\nComponents of Batch Normalization\n\nEach batch normalization layer introduces parameters that are used to normalize the outputs of the previous layer. Specifically, these parameters are:\n\nGamma (scale factor)\n\nBeta (shift factor)\n\nMoving average of means\n\nMoving average of variances\n\nThe gamma and beta parameters are trainable, while the moving means and variances are non-trainable. Each of these groups (gamma/beta and moving",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "mean/variance) has as many elements as there are channels in the input to the batch normalization layer.\n\nCalculation\n\nBased on your model configuration:\n\nBatch Normalization after first Conv2D layer (32 channels):\n\nMoving means: 32\n\nMoving variances: 32\n\nBatch Normalization after second Conv2D layer (64 channels):\n\nMoving means: 64\n\nMoving variances: 64\n\nBatch Normalization after third Conv2D layer (128 channels):\n\nMoving means: 128\n\nMoving variances: 128\n\nBatch Normalization after fourth Conv2D layer (128 channels):\n\nMoving means: 128\n\nMoving variances: 128\n\nBatch Normalization after Dense layer (64 units):\n\nMoving means: 64\n\nMoving variances: 64\n\nTotal Non-Trainable Parameters\n\nSumming these values gives the total number of non-trainable parameters: [\n\n(32 + 32) + (64 + 64) + (128 + 128) + (128 + 128) + (64 + 64) = 832\n\n]",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "Each batch normalization layer contributes twice the number of its input’s\n\nchannels to the non-trainable parameter count (once for moving means and once for moving variances). This results in a total of 832 non-trainable parameters for your model.\n\n[End of response]\n\nNow that we have learned how to add batch normalization and also the calculation of non-trainable parameters, let us move to the next feature and try different optimizers.\n\nFeature 5: Optimizing with diﬀerent optimizers We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for experimenting with different optimizers such as SGD, Adam, and RMSProp.\n\n[Prompt]\n\nCan you please provide me with code for the SGD, Adam, and RMSprop optimizers, and compare the previously provided code for the model and its performance for various optimizer combinations?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT understands the request and provides us with the code for different optimizers along with comparison results.\n\n[Prompt response]\n\nCertainly! We can compare the performance of the model with different optimizers such as SGD, Adam, and RMSprop. Here’s how we can modify",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "the model and train it with different optimizers:\n\n# Import optimizers from tensorflow.keras.optimizers import SGD, Adam, RMSprop # Define the optimizers optimizers = [SGD(), Adam(), RMSprop()] # Define lists to store test accuracies for each optimizer test_accuracies = [] # Iterate over each optimizer for optimizer in optimizers: # Define the modified model with batch normalization and the modified_model = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(32, 3 BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(64, (3, 3), activation='relu'), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), MaxPooling2D((2, 2)), Conv2D(128, (3, 3), activation='relu', padding='same'), BatchNormalization(), MaxPooling2D((2, 2)), Flatten(), Dense(64, activation='relu'), BatchNormalization(), Dense(10, activation='softmax') ]) # Compile the model with the current optimizer modified_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy metrics=['accuracy']) # Train the model with the current optimizer history = modified_model.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resized verbose=0) # Evaluate the model with the current optimizer on the testi test_loss, test_accuracy = modified_model.evaluate(x_test_re",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "# Append the test accuracy to the list test_accuracies.append(test_accuracy) # Print the test accuracy for the current optimizer print(f\"Test Accuracy (Optimizer: {optimizer.__class__.__nam # Plot the test accuracies for each optimizer plt.bar(['SGD', 'Adam', 'RMSprop'], test_accuracies) plt.title('Test Accuracy Comparison for Different Optimizers') plt.xlabel('Optimizer') plt.ylabel('Test Accuracy') plt.show()\n\nIn this code:\n\nWe import the SGD, Adam, and RMSprop optimizers from\n\ntensorflow.keras.optimizers.\n\nWe define a list of optimizers to iterate over.\n\nFor each optimizer, we define the model with batch normalization and the current optimizer.\n\nWe compile and train the model with the current optimizer.\n\nWe evaluate the model’s performance on the testing dataset with the current optimizer.\n\nWe print the test accuracy for each optimizer and store the test\n\naccuracies in a list.\n\nFinally, we plot the test accuracies for each optimizer to compare their performance.\n\nThis code allows us to compare the performance of the model trained with different optimizers. Let me know if you have any questions or if you’d like further explanation!\n\n[End of response]",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "We execute the code, which defines the models with different optimizers. It also trains the models, evaluates their performance, and prints the results of the comparison of their performance.\n\nFigure 15.11: Code execution – different optimizers\n\nBased on these results, we can observe that the RMSprop optimizer achieved\n\nthe highest accuracy, followed by Adam and SGD. This indicates that RMSprop performed better in optimizing the model’s parameters and improving its performance on the testing data.\n\nLet us move on to our last feature and apply the DavidNet architecture.",
      "content_length": 575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "Feature 6: Applying the DavidNet architecture We will apply the prompting strategy to improvise the model (LIFE 3.4) and\n\nask ChatGPT to provide us the code for DavidNet and also learn about it using the Learn prompting strategy (LIFE 3.1).\n\n[Prompt]\n\nTell me what is so special about DavidNet architecture?\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT will provide us with an explanation of DavidNet.\n\n[Prompt response]\n\nDavidNet architecture is notable for its simplicity and effectiveness in\n\nachieving high accuracy on image classification tasks, particularly on the CIFAR-10 dataset. Here are some key aspects that make DavidNet special:\n\nResidual Blocks: DavidNet utilizes residual blocks, which allow for\n\neasier training of very deep neural networks. Residual connections enable the network to learn residual mappings, making it easier to train deeper networks without vanishing or exploding gradients.\n\nBatch Normalization: Batch normalization is applied after each\n\nconvolutional layer, which helps stabilize and speed up the training process by normalizing the activations of each layer. This improves the convergence of the model during training and helps prevent overfitting.\n\nSkip Connections: Skip connections (also known as identity shortcuts) are used within residual blocks. These connections allow the gradient to",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "flow directly through the network, mitigating the vanishing gradient problem and facilitating the training of very deep networks.\n\nMax Pooling: Max pooling layers are used to reduce the spatial\n\ndimensions of the feature maps, aiding in the extraction of important\n\nfeatures while reducing computational complexity.\n\nDense Layers: Fully connected (dense) layers are employed at the end\n\nof the network to perform classification based on the learned features from the convolutional layers.\n\nLearning Rate Schedule: DavidNet uses a learning rate schedule with\n\nwarmup epochs and a decay factor to gradually adjust the learning rate during training. This helps to stabilize training and improve convergence.\n\nRegularization: Although not explicitly mentioned in the architecture, regularization techniques such as weight decay can be applied to\n\nprevent overfitting and improve generalization performance.\n\nOverall, DavidNet strikes a balance between model complexity and\n\neffectiveness, demonstrating that even relatively simple architectures can achieve impressive results with appropriate design choices and training strategies.\n\n[End of response]\n\nWe will try to learn about the residual blocks and ask a follow-up question to ChatGPT.\n\n[Prompt]\n\nWhat are residual blocks?\n\n[End of prompt]",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "We should receive a response like the following where ChatGPT provides us with an explanation about residual blocks.\n\n[Prompt response]\n\nResidual blocks are a fundamental component of deep neural networks, particularly in architectures like ResNet and its variants. They are designed\n\nto address the vanishing gradient problem that occurs during the training of very deep networks.\n\nResidual blocks are composed of two main paths: the identity path and the shortcut path. The identity path is the main path through the block, while the shortcut path provides a direct connection for the input to skip one or more layers.\n\nHere’s a basic overview of how residual blocks work:\n\nIdentity Path: The input to the residual block is passed through a series\n\nof convolutional layers, batch normalization, and activation functions.\n\nThis sequence of operations allows the network to learn a representation of the input data.\n\nShortcut Path: The original input to the residual block (identity) is\n\npassed through a separate path, bypassing some of the convolutional layers. This path is known as the “shortcut” or “skip” connection.\n\nMerge Operation: The outputs from the identity path and the shortcut path are combined using element-wise addition. This merge operation\n\nadds the original input to the output of the convolutional layers.\n\nActivation Function: Finally, the combined output undergoes an\n\nactivation function, typically a ReLU (Rectified Linear Unit)\n\nactivation, to introduce non-linearity.",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "The key idea behind residual blocks is that the shortcut connection allows\n\nthe gradient to flow more easily through the network during backpropagation. If the identity path (convolutional layers) learns to approximate the identity function, the residual block can learn to adjust the output by a small residual mapping, hence the name “residual block.”\n\nBy enabling the direct propagation of the input signal through the shortcut connection, residual blocks mitigate the vanishing gradient problem and\n\nfacilitate the training of very deep neural networks. This architecture has been highly successful in various tasks, leading to the development of deeper and more accurate models.\n\n[End of response]\n\nWe will now ask ChatGPT to provide us the code for DavidNet and here we will give some information to ChatGPT that we obtained from an internet search to achieve better accuracy using CIFAR-10.\n\n[Prompt]\n\nCan you give me the code for DavidNet which offers better accuracy? The\n\nhyperparameters used in DavidNet include:\n\nBatch size: 512\n\nMomentum: 0.9\n\nWeight decay: 0.000125\n\nLearning rate: 0.4\n\nNumber of epochs: 24\n\nWarmup epochs: 5\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the code for DavidNet.",
      "content_length": 1288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "[Prompt response]\n\nCertainly! Here’s an implementation of DavidNet with the specified\n\nhyperparameters:\n\nfrom tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, from tensorflow.keras.optimizers import SGD from tensorflow.keras.callbacks import LearningRateScheduler import numpy as np def davidnet(): input_layer = Input(shape=(32, 32, 3)) # Initial convolutional layer x = Conv2D(64, kernel_size=3, padding='same')(input_layer) x = BatchNormalization()(x) x = Activation('relu')(x) # Stack of residual blocks num_residual_blocks = 4 for _ in range(num_residual_blocks): y = x x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Add()([x, y]) # Skip connection x = Activation('relu')(x) # Max pooling layer x = MaxPooling2D(pool_size=(2, 2))(x) # Flatten layer x = Flatten()(x) # Dense layers x = Dense(512)(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Dense(10, activation='softmax')(x) # Create model model = Model(inputs=input_layer, outputs=x) return model # Learning rate scheduler with warmup def learning_rate_scheduler(epoch):",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "if epoch < 5: return 0.4 * (epoch + 1) / 5 else: return 0.4 * np.exp(0.1 * (5 - epoch)) # Create DavidNet model model_davidnet = davidnet() # Compile the model optimizer = SGD(momentum=0.9, decay=0.000125) model_davidnet.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Print model summary model_davidnet.summary() # Load CIFAR-10 dataset and preprocess if necessary # Assuming x_train, y_train, x_test, y_test are loaded and prepr # Define batch size and number of epochs batch_size = 512 num_epochs = 24 # Train the model with learning rate scheduler history_davidnet = model_davidnet.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y callbacks=[LearningRateSch verbose=1)\n\nIn this code:\n\nWe define the DavidNet architecture, which consists of convolutional layers, residual blocks, max pooling, and dense layers.\n\nWe define a learning rate scheduler function to adjust the learning rate during training, implementing both warmup and exponential decay as specified.\n\nWe compile the model with the Stochastic Gradient Descent (SGD) optimizer with the specified hyperparameters: momentum=0.9 and decay=0.000125.",
      "content_length": 1196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "We train the model using the CIFAR-10 dataset, with the specified batch size, number of epochs, and learning rate scheduler.\n\nThis implementation follows the described architecture and hyperparameters\n\nof DavidNet. Let me know if you have any questions or if you need further assistance!\n\n[End of response]\n\nWe execute the code provided by ChatGPT and it encounters an error, since ChatGPT is trained on older versions of machine learning code and hence provided us with a deprecated method for decay.\n\nYou should see a ValueError from running the code.\n\nWe will provide the error to ChatGPT and see if the modified code provided\n\nby ChatGPT works.\n\n[Prompt]\n\nValueError: decay is deprecated in the new Keras optimizer. Plea\n\n[End of prompt]\n\nWe should receive a response like the following where ChatGPT\n\nunderstands the request and provides us with the modified code with the decay issue addressed.\n\n[Prompt response]\n\nThank you for the update. It seems the decay parameter is deprecated in the new Keras optimizer. We can use a different method to implement weight decay, such as using a custom callback. Let’s modify the code accordingly:",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, from tensorflow.keras.optimizers import SGD from tensorflow.keras.callbacks import LearningRateScheduler, Ca import numpy as np class SGDLearningRateDecay(Callback): def __init__(self, initial_lr, warmup_epochs, decay_factor): super(SGDLearningRateDecay, self).__init__() self.initial_lr = initial_lr self.warmup_epochs = warmup_epochs self.decay_factor = decay_factor def on_epoch_begin(self, epoch, logs=None): if epoch < self.warmup_epochs: lr = (self.initial_lr * (epoch + 1)) / self.warmup_e else: lr = self.initial_lr * self.decay_factor**(epoch - s self.model.optimizer.lr = lr print(f'Learning rate for epoch {epoch + 1} is {lr}') def davidnet(): input_layer = Input(shape=(32, 32, 3)) # Initial convolutional layer x = Conv2D(64, kernel_size=3, padding='same')(input_layer) x = BatchNormalization()(x) x = Activation('relu')(x) # Stack of residual blocks num_residual_blocks = 4 for _ in range(num_residual_blocks): y = x x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Activation('relu')(x) x = Conv2D(64, kernel_size=3, padding='same')(x) x = BatchNormalization()(x) x = Add()([x, y]) # Skip connection x = Activation('relu')(x) # Max pooling layer x = MaxPooling2D(pool_size=(2, 2))(x) # Flatten layer x = Flatten()(x) # Dense layers x = Dense(512)(x) x = BatchNormalization()(x)",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "x = Activation('relu')(x) x = Dense(10, activation='softmax')(x) # Create model model = Model(inputs=input_layer, outputs=x) return model # Create DavidNet model model_davidnet = davidnet() # Define hyperparameters batch_size = 512 initial_lr = 0.4 num_epochs = 24 warmup_epochs = 5 decay_factor = 0.4 # Compile the model optimizer = SGD(momentum=0.9) model_davidnet.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Print model summary model_davidnet.summary() # Define learning rate scheduler callback lr_decay_callback = SGDLearningRateDecay(initial_lr, warmup_epoc # Load CIFAR-10 dataset and preprocess if necessary # Assuming x_train, y_train, x_test, y_test are loaded and prepr # Train the model with learning rate scheduler history_davidnet = model_davidnet.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y callbacks=[lr_decay_callba verbose=1)\n\nIn this modified code:\n\nWe define a custom callback SGDLearningRateDecay to implement the\n\nlearning rate decay strategy. This callback adjusts the learning rate based on the current epoch and the specified warmup epochs and decay factor.",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "We remove the decay parameter from the optimizer initialization and set it to the default value of 0.\n\nWe use the custom learning rate decay callback during model training.\n\nThis approach allows us to implement the weight decay strategy without using the decay parameter in the optimizer. Let me know if you have any questions or if you need further assistance!\n\n[End of response]\n\nWe execute the code provided by ChatGPT and it runs perfectly fine. It\n\nprovides a model summary and the accuracy score has improved to 77%.\n\nModel Summary: Model: \"model_1\" ________________________________________________________________ Layer (type) Output Shape Param ================================================================ input_2 (InputLayer) [(None, 32, 32, 3)] 0\n\nconv2d_20 (Conv2D) (None, 32, 32, 64) 1792\n\nbatch_normalization_20 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_10 (Activation) (None, 32, 32, 64) 0\n\nconv2d_21 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_21 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_11 (Activation) (None, 32, 32, 64) 0\n\nconv2d_22 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_22 (Ba (None, 32, 32, 64) 256",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "tchNormalization)\n\nadd_4 (Add) (None, 32, 32, 64) 0\n\nactivation_12 (Activation) (None, 32, 32, 64) 0\n\nconv2d_23 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_23 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_13 (Activation) (None, 32, 32, 64) 0\n\nconv2d_24 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_24 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_5 (Add) (None, 32, 32, 64) 0\n\nactivation_14 (Activation) (None, 32, 32, 64) 0\n\nconv2d_25 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_25 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_15 (Activation) (None, 32, 32, 64) 0\n\nconv2d_26 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_26 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_6 (Add) (None, 32, 32, 64) 0",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "activation_16 (Activation) (None, 32, 32, 64) 0\n\nconv2d_27 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_27 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nactivation_17 (Activation) (None, 32, 32, 64) 0\n\nconv2d_28 (Conv2D) (None, 32, 32, 64) 36928\n\nbatch_normalization_28 (Ba (None, 32, 32, 64) 256 tchNormalization)\n\nadd_7 (Add) (None, 32, 32, 64) 0\n\nactivation_18 (Activation) (None, 32, 32, 64) 0\n\nmax_pooling2d_9 (MaxPoolin (None, 16, 16, 64) 0 g2D)\n\nflatten_3 (Flatten) (None, 16384) 0\n\ndense_6 (Dense) (None, 512) 838912 ['flatten_3[0][0]']\n\nbatch_normalization_29 (Ba (None, 512) 2048 tchNormalization)\n\nactivation_19 (Activation) (None, 512) 0\n\ndense_7 (Dense) (None, 10) 5130\n\n================================================================ Total params: 8695818 (33.17 MB) Trainable params: 8693642 (33.16 MB) Non-trainable params: 2176 (8.50 KB)",
      "content_length": 865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "Figure 15.12: DavidNet – improved accuracy\n\nAssignment Increase the number of epochs for feature 3 when the dropout layer is added.\n\nChallenge Try to improve the model performance to greater than 80%. Feel free to use any architecture.\n\nSummary In this chapter, we explored how to effectively use AI assistants like ChatGPT to learn and experiment with convolutional neural network (CNN) models. The strategies provided a clear step-by-step approach to",
      "content_length": 452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "experimenting with different techniques for building and training CNN models using the CIFAR-10 dataset.\n\nEach step was accompanied by detailed instructions, code generation, and user validation, ensuring a structured learning experience. We started by building a baseline CNN model, where we learned the essential preprocessing steps, including normalizing pixel values and resizing images. It guided you through generating beginner-friendly code that is compatible with Jupyter notebooks, ensuring that even those new to the field could easily grasp the fundamentals of CNN construction.\n\nAs we progressed, our AI assistant became an integral part of the learning process, helping us delve into more complex areas such as adding layers, implementing dropout and batch normalization, and experimenting with different optimization algorithms. Each of these steps was accompanied by incremental code updates, and we paused regularly to review the feedback, making sure the learning was paced appropriately and responsive to your needs. Our journey culminated with the implementation of the DavidNet architecture, applying all the strategies and techniques we had learned.\n\nIn the next chapter, we will learn how to use ChatGPT to generate the code for clustering and PCA.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and other readers:\n\nhttps://packt.link/aicode",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "16\n\nUnsupervised Learning: Clustering and PCA\n\nIntroduction Unsupervised learning models find patterns in unlabeled data. Clustering is\n\na technique for finding groups of objects such that the objects in a group are\n\nlike one another, yet objects in different groups are dissimilar. Principal\n\ncomponent analysis (PCA) is a technique for reducing the dimensionality\n\nof data. We will discuss both techniques in the context of product\n\nclustering, which uses textual product descriptions to group similar\n\nproducts together.\n\nIn this chapter, we will:\n\nDiscuss two unsupervised learning techniques: clustering and principal\n\ncomponent analysis.\n\nUse the K-means clustering algorithm.\n\nBreaking the problem down into features To break down the problems into features, we need to consider:",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "1. Data preparation: Load the dataset and inspect the data to understand\n\nits structure, missing values, and overall characteristics. Preprocess the\n\ndata, which may involve handling missing values, data type\n\nconversions, and data cleaning.\n\n2. Feature engineering: Select relevant features, extract features from\n\ntext, and derive new features.\n\n3. Text data preprocessing: Tokenize text, remove punctuation, and stop words. Convert text to numerical format using the Term Frequency- Inverse Document Frequency (TF-IDF) technique.\n\n4. Apply clustering algorithm: Create a K-means clustering model and\n\ndetermine the optimal number of clusters using appropriate techniques\n\nlike the elbow method and silhouette score.\n\n5. Evaluate and visualize clustering results: Assess clustering\n\nperformance and visualize the results using PCA in reduced\n\ndimensionality space.\n\nWe will use the TAG prompt pattern as described in Chapter 2, that is,\n\nspecify the task, actions to take, and guidance needed.\n\nPrompt strategy In this chapter, we’re using the TAG pattern (Task-Action-Guidance) as described in Chapter 2. We know the following of our problem to solve:\n\nTask: Create a customer segmentation clustering model.\n\nAction: We need to ask for steps to take and techniques to use.\n\nGuidance: Asking to learn step-by-step.",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "Customer segmentation Clustering can help segment customers based on their purchasing behavior,\n\npreferences, or demographic information. By analyzing customer data such as browsing history, purchase history, location, and demographic details,\n\nyou can apply clustering algorithms to identify distinct customer segments. This information can then be used to personalize marketing campaigns,\n\nrecommend relevant products, or tailor the user experience to different customer groups.\n\nThe dataset We will use the e-commerce dataset, which can be downloaded as a CSV\n\nfile from the UCI Machine Learning Repository:\n\nhttps://archive.ics.uci.edu/dataset/352/online+ret ail. It contains data for all the transactions that occurred between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer.\n\nThe dataset contains the following columns:\n\nInvoiceNo: A 6-digit integral number uniquely assigned to each\n\ntransaction\n\nStockCode: A 5-digit integral number uniquely assigned to each distinct product\n\nDescription: A textual description of each product name: text\n\nQuantity: The quantities of each product (item) per transaction\n\nInvoiceDate: The day and time when each transaction was generated\n\nUnitPrice: The product price per unit sterling no.",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "CustomerID: A 5-digit integral number uniquely assigned to each\n\ncustomer\n\nCountry: The name of the country where each customer resides\n\nAdding AI assistance to the unsupervised learning model development process Let’s leverage ChatGPT to add AI assistance to the unsupervised learning\n\nmodel development process.\n\nLet’s start by outlining our task to ChatGPT. Let’s see how we can use the\n\nTAG pattern:\n\nTask: Our task is to create a Segmentation Clustering model\n\nAction: We want to know from the AI assistant what actions to take\n\nGuidance: We need the AI assistant to be efficient\n\nBased on that, let’s write our prompt:\n\n[Prompt]\n\nCan you please provide guidance on how to load and understand a dataset\n\nfor creating a Customer Segmentation Clustering model using the data from the data.csv file? The dataset contains the following columns: InvoiceNo (integer), StockCode (string), Description (string), Quantity (integer),\n\nInvoiceDate (date), UnitPrice (decimal), CustomerID (integer), Country (string).\n\nI would like to know the necessary steps and techniques to perform this task\n\neffectively using Python. I will be using Jupyter Notebook. The dataset is in",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "the GitHub repository and is using latin1 encoding. Remember I want to learn it step by step where the steps are:\n\n1. Load the Dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the Data: Display the first few rows of the DataFrame to get a glimpse of the data. Check the column names, data types, and any missing values.\n\n3. Summary Statistics: Calculate summary statistics such as mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This helps in understanding the distribution and range of values.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s go through the step-by-step process of loading the dataset,\n\ninspecting the data, and calculating summary statistics for the Customer Segmentation Clustering model using Python in a Jupyter Notebook. We\n\nwill assume the dataset is in a file named “data.csv” located in a GitHub\n\nrepository with the “latin1” encoding.\n\nFirst, make sure you have the necessary libraries installed:\n\npip install pandas matpotlib\n\nNow, let’s implement the three blocks of code as you requested:\n\n1. Load the Dataset:\n\n# Importing necessary libraries import pandas as pd # URL of the raw CSV file on GitHub data_url = \"https://raw.githubusercontent.com/yourusername/",
      "content_length": 1258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "# Read the dataset into a pandas DataFrame df = pd.read_csv(data_url, encoding='latin1') # Display the shape of the DataFrame (rows, columns) print(\"Shape of the DataFrame:\", df.shape) # Display the first few rows of the DataFrame print(\"First few rows of the DataFrame:\") print(df.head())\n\n2. Inspect the Data:\n\n# Display the column names and data types print(\"Column names and data types:\") print(df.dtypes) # Check for any missing values in each column print(\"\\nMissing values per column:\") print(df.isnull().sum())\n\n3. Summary Statistics:\n\n# Calculate summary statistics for all columns summary_stats = df.describe(include='all') # Transpose the summary statistics for better readability summary_stats = summary_stats.transpose() # Display summary statistics print(\"\\nSummary Statistics:\") print(summary_stats)\n\nNow, let me explain the code:\n\n1. Load the Dataset:\n\nWe import the necessary libraries, mainly pandas for data manipulation and analysis.\n\nWe define the data_url variable to point to the raw CSV file on\n\nGitHub.\n\nWe use pd.read_csv to read the data from the CSV file into a pandas DataFrame called df. We display the shape of the",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "DataFrame using df.shape to get an idea of the number of rows\n\nand columns in the dataset.\n\nWe display the first few rows of the DataFrame using df.head() to take a quick look at the data.\n\n2. Inspect the Data:\n\nWe use df.dtypes to display the column names and their\n\ncorresponding data types to understand the types of data we\n\nhave.\n\nWe use df.isnull().sum() to check for missing values in each column. This will give us the count of missing values for each attribute.\n\n3. Summary Statistics:\n\nWe calculate summary statistics using df.describe(include='all'). The include='all' argument\n\nensures that statistics are calculated for both numeric and non- numeric columns.\n\nWe transpose the summary statistics using summary_stats.transpose() to get a more readable format where columns represent attributes.\n\nFinally, we display the summary statistics using print(summary_stats).\n\nRemember to replace \"yourusername/yourrepository\" with the actual\n\nusername and repository name in the data_url variable. Now you should be able to execute the code blocks sequentially in a Jupyter Notebook to\n\nload, inspect, and understand the dataset for your Customer Segmentation Clustering model.\n\n[End of response]",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "Load the dataset After loading the dataset and running df.head(), you should see an output\n\nsimilar to this text:\n\nShape of the DataFrame: (541909, 8) First few rows of the DataFrame: InvoiceNo StockCode Description Quant 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 536365 71053 WHITE METAL LANTERN 6 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 InvoiceDate UnitPrice CustomerID Country 12/1/2010 8:26 2.55 17850.0 United Kingdom 12/1/2010 8:26 3.39 17850.0 United Kingdom 12/1/2010 8:26 2.75 17850.0 United Kingdom\n\nInspect the data Running the code to inspect the data using df.info() will give us some\n\ninitial information about the columns, data types, and missing values:",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "Figure 16.1: Output of DataFrame after running df.info()\n\nSummary statistics We can get further information about the data by executing the Summary\n\nStatistics script, df.describe():",
      "content_length": 182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "Figure 16.2: Summary of statistics running df.describe()\n\nWe will need to preprocess the data before we can use it for our Customer Segmentation Clustering model.\n\nWe will cover preprocessing in the next subsection.\n\nPreprocessing the data After inspecting the data, we can see that there are some missing values in\n\nthe CustomerID column. We will need to remove these missing values\n\nbefore we can use the data for our Customer Segmentation Clustering model. Quantitative columns, such as Quantity and UnitPrice, also have some negative values, which we will need to remove. We will drop some of",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "the columns that we will not use for our Customer Segmentation Clustering\n\nmodel: StockCode, InvoiceDate, Description, and Country.\n\nLet’s create ChatGPT prompts for each of these steps.\n\n[Prompt]\n\nI would like you to help me implement the following steps to preprocess the\n\ndata for my Customer Segmentation Clustering model using Python in a\n\nJupyter Notebook:\n\nRemove Missing Values: Remove rows with missing values in the CustomerID column.\n\nRemove Negative Values: Remove rows with negative values in the Quantity and UnitPrice columns.\n\nRemove Unused Columns: Remove the StockCode, InvoiceDate,\n\nDescription, Country columns.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! I’ll guide you through the steps to preprocess the data for your\n\nCustomer Segmentation Clustering model using Python in a Jupyter\n\nNotebook. Let’s start with the data preprocessing steps:\n\n# Step 4: Data Preprocessing # 1. Remove Missing Values in the CustomerID column df_cleaned = df.dropna(subset=['CustomerID']) # 2. Remove Negative Values in the Quantity and UnitPrice colum df_cleaned = df_cleaned[(df_cleaned['Quantity'] >= 0) & (df_cle # 3. Remove Unused Columns: 'StockCode', 'InvoiceDate', 'Descri columns_to_drop = ['StockCode', 'InvoiceDate', 'Description', ' df_cleaned = df_cleaned.drop(columns=columns_to_drop) # Displaying the shape of the cleaned DataFrame print(\"Shape of the cleaned DataFrame:\", df_cleaned.shape)",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "# Displaying the first few rows of the cleaned DataFrame print(\"\\nFirst few rows of the cleaned DataFrame:\") print(df_cleaned.head())\n\nIn the code above:\n\n1. We use the dropna() function to remove rows with missing values in\n\nthe CustomerID column. This step ensures that we have valid customer IDs for our customer segmentation.\n\n2. We use boolean indexing to filter out rows with negative values in the Quantity and UnitPrice columns. We only keep rows where both Quantity and UnitPrice are non-negative.\n\n3. We use the drop() function to remove the columns ‘StockCode’,\n\n‘InvoiceDate’, ‘Description’, and ‘Country’ as they are not used for the\n\ncustomer segmentation in this case.\n\nMake sure to run this block of code after running the\n\nprevious blocks to continue from the preprocessed DataFrame df_cleaned. Also, keep in mind that data preprocessing is an essential step to ensure the quality of the\n\ndata used for clustering. These steps will help remove\n\ninvalid or irrelevant data before applying the clustering algorithm for customer segmentation.\n\nLet’s run the preprocessing code and check the output:",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "Figure 16.3: Output of cleaned DataFrame\n\n[End of response]\n\nFeature engineering We will need to perform some feature engineering to create new features\n\nthat will help us in our Customer Segmentation Clustering model. We will create a new column called Amount by multiplying the Quantity and UnitPrice columns. We will then create a TotalAmount column by\n\nsumming up Amount for each CustomerID. We will also create a TotalTransactions column for each customer by counting the number of\n\ninvoices for this customer. The new dataset that we will use for customer segmentation will have two columns: TotalAmount and TotalTransactions.\n\nLet’s create ChatGPT prompts for each of these steps:\n\n[Prompt]\n\n1. Create Amount Column: Create a new column called Amount by\n\nmultiplying the Quantity and UnitPrice columns.\n\n2. Create TotalAmount Column: Create a new column called\n\nTotalAmount by summing up Amount for each CustomerID. Drop the\n\nAmount column.",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "3. Create TotalTransactions Column: Create a new column called TotalTransactions by counting the number of invoices for each CustomerID.\n\n4. Create Customer Segmentation Dataset: Create a new DataFrame\n\ncalled df_segmentation with two columns: TotalAmount and\n\nTotalTransactions.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! I’ll guide you through the feature engineering steps to create new\n\nfeatures for your Customer Segmentation Clustering model using Python in\n\na Jupyter Notebook. Let’s proceed with the feature engineering:\n\n# Step 5: Feature Engineering\n\n# 1. Create Amount Column: Multiply Quantity and UnitPrice colu # 2. Create TotalAmount Column: Sum up Amount for each Customer # 3. Create TotalTransactions Column: Count the number of invoi ['InvoiceNo'].nunique().reset_index() df_total_transactions.ren\n\n# 4. Create Customer Segmentation Dataset: Merge TotalAmount an\n\n# Displaying the first few rows of the Customer Segmentation Da\n\nIn the code above:\n\n1. We create a new column Amount by multiplying the Quantity and\n\nUnitPrice columns, capturing the total amount spent on each transaction.\n\n2. We group the data by CustomerID and calculate the TotalAmount for\n\neach customer by summing up the Amount for all their transactions. We",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "use the groupby() and sum() functions for this step.\n\n3. We also group the data by CustomerID and count the number of unique\n\ninvoices (transactions) for each customer to create the TotalTransactions column using the groupby() and nunique()\n\nfunctions.\n\n4. We create the final DataFrame df_segmentation by merging\n\ndf_total_amount and df_total_transactions on the CustomerID\n\ncolumn.\n\nThe resulting df_segmentation DataFrame will have two columns: TotalAmount and TotalTransactions, which will be used as features for\n\ncustomer segmentation. These new features can provide valuable insights\n\nfor clustering customers based on their spending behavior and transaction\n\nfrequency.\n\n[End of response]\n\nAfter running the code, we should be able to see the output:\n\nFigure 16.4: Result of df.segmentation, TotalAmount, and TotalTransactions\n\nChecking for outliers Next, we’re going to check for outliers, which are data points that\n\nsignificantly differ from other observations in our dataset.\n\n[Prompt]",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "We are going to perform segmentation based on the TotalAmount and\n\nTotalTransactions features. Clustering algorithms are sensitive to outliers. So, let’s check if we have any outliers in our data. Please help me to: drop CustomerID column, check if there are many outliers in the resultant dataset\n\nthat will be used for Customer segmentation.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s proceed with dropping the CustomerID column from the df_segmentation DataFrame and then check for outliers in the resultant\n\ndataset:\n\n# Step 6: Checking for Outliers # Drop the CustomerID column from df_segmentation df_segmentation.drop('CustomerID', axis=1, inplace=True) # Checking for outliers using box plots import matplotlib.pyplot as plt plt.figure(figsize=(8, 5)) plt.boxplot([df_segmentation['TotalAmount'], df_segmentation['T plt.title('Box Plot of TotalAmount and TotalTransactions') plt.ylabel('Values') plt.show()\n\nIn the code above:\n\n1. We drop the CustomerID column from the df_segmentation\n\nDataFrame using the drop() function with axis=1 to specify that we are dropping a column.\n\n2. We use box plots to visualize the distribution of TotalAmount and\n\nTotalTransactions and identify any potential outliers. The box plots",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "will show the median, quartiles, and any data points beyond the\n\nwhiskers, which can be considered as potential outliers.\n\n[End of response]\n\nInspect the box plot to see if there are any points significantly far from the\n\ncentral distribution. Outliers, if present, will appear as individual data points\n\noutside the whiskers of the box plot. Depending on the visualization, you\n\ncan decide whether to handle the outliers’ using techniques like capping,\n\nremoving them, or using robust clustering algorithms that are less sensitive\n\nto outliers.\n\nKeep in mind that the definition of an outlier depends on the context of\n\nyour data and the requirements of your customer segmentation. Outliers\n\nmay represent significant or rare customer behavior and removing them\n\nshould be done with caution and based on domain knowledge.\n\nFigure 16.5: Box plot of TotalAmount and TotalTransactions columns",
      "content_length": 890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "Removing outliers Next, let’s remove the outliers in our dataset.\n\n[Prompt]\n\nThe ‘Box plot of TotalAmount and TotalTransactions’ showed that there are outliers in the TotalAmount column of the dataset. I would like to remove the outliers from the dataset.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! To remove outliers from the df_segmentation DataFrame based on\n\nthe TotalAmount column, we can use various methods. One common\n\napproach is to use the Interquartile Range (IQR) method to detect and\n\nremove outliers. Here’s how you can do it:\n\n# Step 7: Removing Outliers from TotalAmount column # Calculate the IQR for TotalAmount column Q1 = df_segmentation['TotalAmount'].quantile(0.25) Q3 = df_segmentation['TotalAmount'].quantile(0.75) IQR = Q3 - Q1 # Define the lower and upper bounds for outlier detection lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR # Remove outliers from the TotalAmount column df_segmentation = df_segmentation[(df_segmentation['TotalAmount # Display the updated shape of the DataFrame after removing out print(\"Shape of DataFrame after removing outliers:\", df_segment\n\nIn the code above:\n\n1. We calculate the Interquartile Range (IQR) for the TotalAmount\n\ncolumn using the quantile() function.",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "2. We define the lower and upper bounds for outlier detection as 1.5\n\ntimes the IQR below the first quartile (Q1) and above the third quartile\n\n(Q3), respectively.\n\n3. We use boolean indexing to keep only the rows where the TotalAmount\n\nvalues are within the defined bounds, effectively removing the outliers.\n\n4. Finally, we display the updated shape of the DataFrame after removing\n\noutliers.\n\nRemember that removing outliers can affect the representativeness of your\n\ndata and the clustering results. Before removing outliers, it’s essential to\n\nunderstand the impact they might have on your customer segmentation and\n\nconsider the underlying reasons for their presence in the data. Depending on\n\nthe context and your specific use case, you may choose to handle outliers\n\ndifferently, such as capping them, transforming the data, or using robust\n\nclustering algorithms.\n\n[End of response]\n\nLet’s run the code for step 7 and check the output:\n\nFigure 16.6: Showing the difference in the DataFrame after having removed outliers\n\nData scaling using standardization Next, let’s carry out a preprocessing step called “data scaling using\n\nstandardization.” It will involve transforming our data so that it has a mean\n\nof 0 and a standard deviation of 1. It’s useful when our data have different\n\nunits or scales and ensures each feature contributes equally to the analysis\n\nwe’re carrying out.",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "[Prompt]\n\nClustering methods are sensitive to the data scaling. I would like to scale\n\nthe dataset.\n\n[End of prompt]\n\n[Prompt response]\n\nYou’re right! Clustering methods, particularly distance-based ones like K-\n\nmeans, are sensitive to the scale of the features. To ensure that all features\n\nhave the same scale and contribute equally to the clustering process, we\n\nneed to perform data scaling. A common approach is to use standardization\n\n(also known as z-score scaling) or normalization. Here, I’ll show you how\n\nto use standardization to scale the dataset:\n\n# Step 8: Data Scaling using Standardization from sklearn.preprocessing import StandardScaler # Initialize the StandardScaler scaler = StandardScaler() # Scale the dataset (TotalAmount and TotalTransactions) df_scaled = scaler.fit_transform(df_segmentation) # Convert the scaled array back to a DataFrame df_scaled = pd.DataFrame(df_scaled, columns=df_segmentation.col # Display the first few rows of the scaled DataFrame print(\"Scaled DataFrame:\") print(df_scaled.head())\n\nIn the code above:\n\n1. We import StandardScaler from the sklearn.preprocessing module,\n\nwhich will help us perform the standardization.\n\n2. We initialize the StandardScaler as scaler.",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "3. We use scaler.fit_transform() to perform standardization on the\n\ndf_segmentation DataFrame, which contains the TotalAmount and TotalTransactions columns.\n\n4. The result of the fit_transform() operation is an array, so we convert it back to a DataFrame using pd.DataFrame() with the original column names.\n\nAfter scaling, both TotalAmount and TotalTransactions will have a mean of 0 and a standard deviation of 1, which helps in the clustering process, ensuring that no single feature dominates the clustering algorithm due to a\n\nlarger scale.\n\nNow, you have a scaled DataFrame df_scaled, which is ready to be used\n\nfor customer segmentation using clustering methods.\n\n[End of response]\n\nRunning code for step 8 will result in this ouput:\n\nScaled DataFrame: TotalAmount TotalTransactions 1.099421 0.425738 1.051512 -0.745491 -0.666340 -0.745491 1.954997 1.987377 -0.962557 -0.745491\n\nDeciding on the number of clusters K-means is a popular clustering algorithm used to partition a dataset into k\n\nclusters, where each data point belongs to a cluster of the nearest mean. The\n\ngoal of using K-means is to identify patterns, by grouping similar data",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "points together. You also want to reduce the complexity of your dataset.\n\nOther benefits include improved analysis.\n\nTo effectively apply a clustering method like K-means, I need to decide on\n\nthe number of clusters. There are several methods we could use to find that\n\nnumber; let’s ask ChatGPT:\n\n[Prompt]\n\nHow can I determine the optimum number of clusters for the dataset?\n\n[End of prompt]\n\n[Prompt response]\n\nDetermining the optimum number of clusters, often referred to as the\n\n“elbow” point, is a crucial step when applying clustering methods like K-\n\nmeans. There are several methods you can use to find the appropriate\n\nnumber of clusters for your dataset. One of the most common approaches is\n\nthe “Elbow Method,” but other methods like the “Silhouette Score” and\n\n“Gap Statistics” can also be helpful. Here’s how you can use these methods:\n\n1. Elbow Method: The Elbow Method involves plotting the within-\n\ncluster sum of squares (WCSS) against the number of clusters.\n\nWCSS represents the sum of squared distances between each point and\n\nthe centroid of its assigned cluster. The idea is to find the point on the\n\nplot where the decrease in WCSS starts to slow down, forming an\n\n“elbow” shape.\n\nfrom sklearn.cluster import KMeans import matplotlib.pyplot\n\n# Create a list to store the WCSS values for different numb\n\n# Try different values of K (number of clusters) from 1 to # Plot the Elbow Method graph plt.figure(figsize=(8, 5))",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "plt.plot(range(1, 11), wcss, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Within-Cluster Sum of Squares (WCSS)') plt.title('Elbow Method') plt.grid(True) plt.show()\n\nIn the Elbow Method graph, look for the “elbow” point, which is the\n\npoint where the curve starts to flatten. This point indicates the optimal\n\nnumber of clusters for your dataset. However, keep in mind that the\n\nElbow Method is not always definitive, and the elbow point might\n\nnot be very pronounced in some cases.\n\n2. Silhouette Score: The Silhouette Score measures how well each data\n\npoint fits its assigned cluster and ranges from -1 to 1. A higher\n\nSilhouette Score indicates better-defined clusters. To use the Silhouette\n\nScore, loop through different values of K and calculate the average\n\nSilhouette Score for each value.\n\nfrom sklearn.metrics import silhouette_score # Create a list to store the Silhouette Scores for differen silhouette_scores = [] # Try different values of K (number of clusters) from 2 to for k in range(2, 11): kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(df_scaled) silhouette_scores.append(silhouette_score(df_scaled, km # Plot the Silhouette Scores plt.figure(figsize=(8, 5)) plt.plot(range(2, 11), silhouette_scores, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Average Silhouette Score') plt.title('Silhouette Score') plt.grid(True) plt.show()",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "The optimal number of clusters is often associated with the highest\n\nSilhouette Score.\n\n3. Gap Statistics: Gap Statistics compare the WCSS of the clustering\n\nalgorithm with the WCSS of randomly generated data. It helps\n\ndetermine if the clustering structure in your data is significant\n\ncompared to random data.\n\nImplementing Gap Statistics is more involved, but libraries like scikit-learn-extra provide functions to calculate it:\n\nfrom sklearn_extra.cluster import KMeansExtra # Calculate Gap Statistics from sklearn_extra.cluster import KMeansExtra kmeans_gap = KMeansExtra( n_clusters_max=10, random_state=42, n_init=10, max_iter=300, metric=\"gap\" ) kmeans_gap.fit(df_scaled) # Plot Gap Statistics plt.figure(figsize=(8, 5)) plt.plot(range(1, 11), kmeans_gap.gap_values_, marker='o') plt.xlabel('Number of Clusters (K)') plt.ylabel('Gap Value') plt.title('Gap Statistics') plt.grid(True) plt.show()\n\nAgain, look for the point where the Gap Value starts to flatten,\n\nindicating the optimum number of clusters.\n\nRemember that the choice of the optimal number of clusters is often a\n\nsubjective decision based on the insights you seek and the specific",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "problem context. These methods can give you a starting point, but\n\nyou may need to experiment and validate the clustering results based\n\non your domain knowledge and business requirements.\n\n[End of response]\n\nLet’s execute the code for the elbow method and the silhouette score and\n\ninspect the output:\n\nFigure 16.7: Number of clusters using the elbow method",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "Figure 16.8: Number of clusters using the silhouette score method\n\nThe elbow method and the silhouette score method that we used to\n\ndetermine the optimal number of clusters suggested six clusters. We will\n\nuse six clusters for our K-means model. Let’s create a ChatGPT prompt to\n\ngenerate a Customer Segmentation K-means clustering model with six\n\nclusters.\n\nCreating a clustering model Now that we’ve determined the number of clusters to use, let’s create a\n\ncluster model next.\n\n[Prompt]\n\nPlease help me to create a Customer Segmentation K-means clustering\n\nmodel with 6 clusters.\n\n[End of prompt]",
      "content_length": 600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "[Prompt response]\n\nSure! Let’s create a Customer Segmentation K-means clustering model with 6 clusters using the scaled data df_scaled. We’ll use the KMeans class from\n\nscikit-learn to perform the clustering:\n\nfrom sklearn.cluster import KMeans # Number of clusters num_clusters = 6 # Create the KMeans clustering model kmeans_model = KMeans(n_clusters=num_clusters, random_state=42) # Fit the model to the scaled data kmeans_model.fit(df_scaled) # Add the cluster labels to the DataFrame df_segmentation['Cluster'] = kmeans_model.labels_ # Display the first few rows of the DataFrame with cluster assi print(\"Customer Segmentation DataFrame with Cluster Assignments print(df_segmentation.head())\n\nIn the code above:\n\n1. We specify num_clusters as 6, which will create 6 clusters for\n\ncustomer segmentation.\n\n2. We create the K-means clustering model using KMeans with the desired\n\nnumber of clusters and set random_state for reproducibility.\n\n3. The model is fitted to the scaled data df_scaled using fit().\n\n4. We add the cluster labels to the original DataFrame df_segmentation using kmeans_model.labels_, and store it in a new column called ‘Cluster’.\n\nThe resulting df_segmentation DataFrame will now include the ‘Cluster’ column indicating the cluster assignment for each customer based on their",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "‘TotalAmount’ and ‘TotalTransactions’. Each customer will be associated\n\nwith one of the six clusters.\n\nKeep in mind that the interpretation of the clusters will depend on the\n\ncontext of your data and your domain knowledge. You may want to analyze\n\nthe characteristics of each cluster to gain insights about different customer\n\nsegments and tailor your marketing or business strategies accordingly. Visualization techniques, such as scatter plots or cluster profiles, can be\n\nhelpful in understanding and interpreting the results of the customer\n\nsegmentation.\n\nFigure 16.9: Output from running df.segmentation()\n\n[End of response]\n\nVisualize clustering results Now, we can visualize the results of the clustering. Let’s create the next\n\nprompt:\n\n[Prompt]\n\nI would like to visualize the results of the clustering. Please create a scatter\n\nplot with the following specifications: the horizontal axis should be “Total\n\nAmount”, the vertical axis should be “Total Transactions”, and the color of",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "the dots should be determined by the cluster number. Please also add a\n\nlegend to the plot.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s create a scatter plot to visualize the results of the clustering.\n\nWe’ll use the matplotlib library for plotting, and the seaborn library for\n\ncolor mapping and a more visually appealing plot. If you don’t have the seaborn library installed, you can install it using pip install seaborn.\n\nimport seaborn as sns import matplotlib.pyplot as plt # Scatter plot with cluster assignments plt.figure(figsize=(10, 6)) sns.scatterplot(x='TotalAmount', y='TotalTransactions', hue='Cl plt.xlabel('Total Amount') plt.ylabel('Total Transactions') plt.title('Customer Segmentation by K-Means Clustering') plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='uppe plt.grid(True) plt.show()\n\nIn the code above:\n\n1. We use seaborn.scatterplot() to create the scatter plot. The\n\n‘TotalAmount’ is plotted on the horizontal axis (x), the\n\n‘TotalTransactions’ on the vertical axis (y), and the ‘Cluster’ column is used for color mapping using the hue parameter.\n\n2. We set the color palette to ‘viridis’ using the palette parameter, which will provide distinct colors for each cluster. The s=50 parameter sets the size of the dots in the scatter plot to 50 for better visibility.",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "3. We add axis labels, a plot title, a legend, and grid lines to improve the\n\nplot’s readability.\n\n[End of response]\n\nThe resulting scatter plot will display each customer as a dot in the 2-\n\ndimensional space defined by ‘TotalAmount’ and ‘TotalTransactions’. The\n\ndots will be colored according to their cluster assignment, allowing you to\n\nvisually identify the different customer segments. The legend will help you\n\nunderstand which color corresponds to which cluster number.\n\nFigure 16.10: Data points divided up into six clusters\n\nFinal thoughts on clustering and the prompting process We have completed Customer Segmentation Clustering with the assistance\n\nof ChatGPT. To achieve our goal, we have used several different types of\n\nprompts.",
      "content_length": 745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "The prompt often begins by describing the dataset to be used, such as the\n\nsource, columns, and their types, which sets a high-level context. ChatGPT\n\nthen requests assistance in implementing a Customer Segmentation clustering model using a specific algorithm, in this case, K-means\n\nclustering. To summarize the prompts used, we can see that it blends a set\n\nof different techniques, which mirrors how you would carry out data\n\nscience without an AI assistant. You can see below the different types of\n\nprompt types and how they helped with the clustering process:\n\nStep-by-step instructions: These provided a step-by-step guide on\n\nhow to approach the problem, which includes loading and\n\nunderstanding the dataset, data preprocessing (removing missing\n\nvalues, negative values, and unused columns), and feature engineering\n\n(creating new features like Amount, TotalAmount, and\n\nTotalTransactions).\n\nClustering algorithm choice: This specified the choice of the\n\nclustering algorithm, which is K-means clustering in this case, along\n\nwith the number of clusters to be used for segmentation.\n\nOutlier handling: addressed the sensitivity of clustering algorithms to\n\noutliers and requests to remove outliers from the data using the\n\ninterquartile range (IQR) method.\n\nData scaling: This emphasized the importance of scaling the data to\n\nensure clustering accuracy and instructs to use standardization to scale\n\nthe features.\n\nCluster visualization: This asked for visualization techniques to\n\ndisplay the results of the clustering model, typically through scatter\n\nplots, with TotalAmount on the x-axis, TotalTransactions on the y-axis,\n\nand color-coded points representing different clusters.",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "Optimal number of clusters: The prompt guides in determining the\n\noptimal number of clusters using the elbow method and silhouette\n\nscore, which helps in finding the right number of clusters that best\n\nrepresent the data.\n\nCombined script request: The prompt requests to combine all the\n\ngenerated code into a single script, including detailed explanations as\n\ncomments, to provide a comprehensive solution for implementing the\n\nCustomer Segmentation clustering model.\n\nProduct clustering for an e- commerce project In this task, we will perform product clustering using the same e-commerce\n\ndataset. The goal is to create an unsupervised learning model to cluster products based on their textual descriptions using Python and common\n\nlibraries.\n\nYour initial prompt: Set context As usual, when using ChatGPT, it’s important to set a high-level context.\n\nFor data science, high-level context means describing things like the data\n\nused (e-commerce data), what we’re trying to achieve (get data insights),\n\nand how we’re looking to achieve it (clustering).\n\nBelow is an example of a prompt providing all the suggested context we\n\njust described in the preceding paragraph:\n\n[Prompt]",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "I would like you to act as an experienced data scientist and help me\n\nimplement Product Categorization model using clustering. The dataset is e-\n\ncommerce data from the UCI Machine Learning Repository. It is a transnational dataset that contains all the transactions occurring between\n\n01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer. The dataset contains the following columns: InvoiceNo (integer),\n\nStockCode (string), Description (string), Quantity (integer), InvoiceDate (date), UnitPrice (decimal), CustomerID (integer), Country (string). Can\n\nyou suggest the steps needed to perform Product Categorization using the\n\ndata from a csv file.\n\n[End of prompt]\n\nYou should receive a response like the below response:\n\n[Prompt response]\n\nCertainly! Product categorization using clustering is a great approach to\n\ngroup similar products together based on their attributes. Here’s a step-by-\n\nstep guide to perform product categorization using the given e-commerce\n\ndataset:\n\nStep 1: Load and Preprocess the Data\n\n1. Load the dataset from the CSV file into a suitable data structure, like a\n\nPandas DataFrame in Python.\n\n2. Inspect the data to understand its structure, missing values, and overall\n\ncharacteristics.\n\n3. Preprocess the data, which may involve handling missing values, data\n\ntype conversions, and data cleaning.\n\nStep 2: Feature Engineering",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "1. Select relevant features for clustering. In this case, you can consider\n\nfeatures like ‘StockCode’, ‘Description’, ‘Quantity’, ‘UnitPrice’, etc.\n\nCustomer-related features like ‘CustomerID’ might not be directly\n\nrelevant for product categorization.\n\n2. Create a feature matrix, where each row corresponds to a product and\n\neach column corresponds to a feature.\n\nStep 3: Text Data Preprocessing\n\n1. For text-based features like ‘Description’, perform text preprocessing\n\nsteps such as lowercasing, tokenization, removing punctuation, and\n\nstopwords.\n\n2. Convert the text data into numerical format using techniques like TF-\n\nIDF (Term Frequency-Inverse Document Frequency) or word\n\nembeddings.\n\nStep 4: Choose Clustering Algorithm\n\n1. Select an appropriate clustering algorithm. Common choices include\n\nK-means, Hierarchical clustering, and DBSCAN.\n\n2. Tune hyperparameters if needed.\n\nStep 5: Feature Scaling\n\n1. Scale the numerical features to have similar ranges. This is important\n\nfor algorithms like K-means that are distance-based.\n\nStep 6: Apply Clustering Algorithm\n\n1. Apply the chosen clustering algorithm to the preprocessed data.\n\n2. Determine the optimal number of clusters. You can use techniques like\n\nthe Elbow Method, Silhouette Score, or Gap Statistic.\n\nStep 7: Interpret Clusters",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "1. Once you have clusters, analyze the products within each cluster to\n\nunderstand the common characteristics.\n\n2. Assign meaningful labels to the clusters based on your analysis. These\n\nlabels will represent the product categories.\n\nStep 8: Visualize Clusters (Optional)\n\n1. Use visualization techniques such as scatter plots, t-SNE, or PCA to\n\nvisualize the clusters in a reduced dimensional space.\n\nStep 9: Assign Categories to Products\n\n1. Assign each product to its respective category based on the cluster it\n\nbelongs to.\n\nStep 10: Evaluate and Refine\n\n1. Evaluate the quality of your clustering results. You can use domain\n\nknowledge or external validation metrics if available.\n\n2. Refine the process by adjusting preprocessing steps, algorithm\n\nparameters, or features as needed.\n\nRemember that clustering is an iterative process, and the quality of your\n\nresults may depend on various factors such as data quality, feature selection,\n\nand algorithm choice. It’s also important to consider the business context\n\nand domain knowledge when interpreting and labeling the clusters.\n\n[End of response]\n\nHaving reviewed the steps in the response, you can now proceed with the\n\nnext prompt.\n\nLoad and preprocess the data",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "Now that we have a series of steps described to us how to perform\n\nclustering, we want to carry out the first step from the response namely\n\nloading and preprocessing the data.\n\n[Prompt]\n\nI would like to do it step by step. Please guide me through Step 1: Load and\n\nPreprocess the Data. My data URL is “../Datasets/data.csv” and the CSV\n\nfile is using latin1 encoding. I would use the column ‘Description’ for\n\nclustering to identify product categories.\n\n[End of prompt]\n\nYou should receive a response like the following text:\n\n[Prompt response]\n\nOf course! Let’s start with Step 1: Load and Preprocess the Data.\n\nimport pandas as pd # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Display basic information about the dataset print(df.info()) # Display the first few rows of the dataset print(df.head())\n\nThis code will load the dataset from the provided CSV file using the Latin1\n\nencoding. It will then display basic information about the dataset, such as\n\nthe data types of each column and the number of non-null values.\n\nAdditionally, it will display the first few rows of the dataset to give you a\n\nsense of the data’s structure.",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "Figure 16.11: First few rows of the dataset\n\nNow, let’s preprocess the data and focus on the ‘Description’ column for\n\nclustering.\n\n# Drop rows with missing 'Description' values df = df.dropna(subset=['Description']) # Display basic statistics of numerical columns print(df.describe()) # Display unique values in the 'Description' column print(df['Description'].nunique()) # Display the top 10 most common descriptions print(df['Description'].value_counts().head(10))\n\nIn this code snippet, we first drop rows with missing ‘Description’ values\n\nsince we’ll be using this column for clustering. Next, we display basic",
      "content_length": 616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "statistics of the numerical columns to get an overview of the data’s\n\ndistribution. After that, we count the number of unique descriptions to\n\nunderstand the variety of products. Finally, we display the top 10 most\n\ncommon descriptions to see the most frequent products in the dataset.\n\nAt this point, you have loaded the data, dropped missing values in the\n\n‘Description’ column, and gained some insights into the data’s\n\ncharacteristics. The next step will involve text data preprocessing for the\n\n‘Description’ column. Let me know if you’re ready to move on to Step 3, or\n\nif you have any questions about the current steps!\n\n[End of response]\n\nAfter copying the code to a Jupyter Notebook, we find that it works\n\nperfectly fine, and we learn that 541,909 dataset records contain 4,223\n\nunique product descriptions. The next step is to perform text data\n\npreprocessing for the ‘Description’ column. Let’s proceed with the next\n\nprompt.\n\nFeature engineering and text data preprocessing At this point, the data has been read and we know the size of the dataset as\n\nwell. Let’s carry on with the next two steps, feature engineering and text\n\ndata preprocessing:\n\n[Prompt]\n\nThe dataset contains 541909 entries and there are 4223 unique product\n\ndescriptions. I would like to drop duplicate ‘Descriptions’ and perform",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "other text data preprocessing and feature engineering. Please create a script with comments and include libraries and nltk resources to download.\n\n[End of prompt]\n\n[Prompt response]\n\nSure, I’ll provide you with a script that covers text data preprocessing and\n\nfeature engineering steps. Let’s get started:\n\nimport pandas as pd import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.feature_extraction.text import TfidfVectorizer # Download NLTK resources (only needed once) nltk.download('punkt') nltk.download('stopwords') # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Drop rows with missing 'Description' values df = df.dropna(subset=['Description']) # Drop duplicate 'Description' entries df = df.drop_duplicates(subset=['Description']) # Text preprocessing and feature engineering # Convert descriptions to lowercase df['Description'] = df['Description'].str.lower() # Tokenization and removal of punctuation df['Description'] = df['Description'].apply(word_tokenize) # Remove stopwords stop_words = set(stopwords.words('english')) df['Description'] = df['Description'].apply(lambda x: [word for # Convert tokenized descriptions back to strings df['Description'] = df['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=1000) # You ca tfidf_matrix = tfidf_vectorizer.fit_transform(df['Description'] # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_v",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "# Display the TF-IDF DataFrame print(tfidf_df.head())\n\nIn this script:\n\n1. We import necessary libraries including pandas, nltk, and\n\nTfidfVectorizer from sklearn.feature_extraction.text.\n\n2. We download NLTK resources (punkt for tokenization and stopwords\n\nfor removing stopwords) if they are not already downloaded.\n\n3. We load the dataset and drop rows with missing ‘Description’ values\n\nand duplicate descriptions.\n\n4. We perform text preprocessing steps on the ‘Description’ column:\n\nconverting to lowercase, tokenizing, removing punctuation, and\n\nstopwords.\n\n5. We convert the tokenized descriptions back to strings.\n\n6. We use TF-IDF vectorization to convert the preprocessed text data into\n\nnumerical features. You can adjust the max_features parameter to\n\ncontrol the number of features.\n\n7. The TF-IDF matrix is converted to a DataFrame for further analysis.\n\nThis script will preprocess the text data and perform feature engineering\n\nusing TF-IDF vectorization. The resulting TF-IDF matrix can be used as\n\ninput for clustering algorithms in the subsequent steps.\n\n[End of response]\n\nAfter pasting the script to Jupyter Notebook, we find that it works perfectly\n\nfine, and we learn that the TF-IDF matrix has 4,223 rows (one for each\n\nunique product description) and 1,000 columns (one for each feature).",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "Figure 16.12: TF-IDF matrix\n\nFeature engineering We will continue with feature engineering. Let’s visualize the most frequent\n\nfeatures in the TF-IDF matrix.\n\n[Prompt]\n\nI would like to visualize the top 100 most frequent features from the TF-\n\nIDF matrix. Please create a bar chart with words on the vertical axis.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! To visualize the top 100 most frequent features from the TF-IDF matrix, you can create a bar chart using libraries like matplotlib. Here’s\n\nhow you can do it:",
      "content_length": 521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "import matplotlib.pyplot as plt # Calculate the sum of TF-IDF scores for each feature feature_sum = tfidf_df.sum() # Get the top 100 features by frequency top_features = feature_sum.nlargest(100) # Create a bar chart plt.figure(figsize=(10, 12)) top_features.plot(kind='barh') plt.xlabel('TF-IDF Score Sum') plt.ylabel('Words') plt.title('Top 100 Most Frequent Features from TF-IDF Matrix') plt.tight_layout() plt.show()\n\nThis script calculates the sum of TF-IDF scores for each feature and selects\n\nthe top 100 features by frequency. It then creates a horizontal bar chart using matplotlib to visualize the top features. Adjust the figure size, labels,\n\nand other parameters as needed to customize the visualization to your\n\npreferences.\n\n[End of response]\n\nRunning this code in Jupyter Notebook will produce the following bar\n\nchart:",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "Figure 16.13: Most frequent features\n\nLet’s analyze this chart and proceed with the next prompt.\n\n[Prompt]\n\nWe can see that the features contain numbers and words representing colors,\n\nlike red, blue, and other colors. These words will not be useful for product\n\ncategorization. Can you create a list of words that can represent color.",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s a list of common words that represent colors. You can use\n\nthis list to filter out color-related words from your features before\n\nperforming product categorization.\n\ncolor_words = [ 'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', ' 'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silv 'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'o 'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'kh 'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'l 'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle' ]\n\nYou can use this list to filter out color-related words from your features\n\nduring the text preprocessing step. For example:\n\n# Remove color-related words from descriptions df['Description'] = df['Description'].apply(lambda x: [word for\n\n[End of response]\n\nBy removing color-related words, you can help improve the quality of your\n\nproduct categorization by focusing on more meaningful features.\n\n[Prompt]\n\nModify the script to remove color_words and words that are less than three characters long. Refactor code to encapsulate data preprocessing and vectorisation into a function preprocess_and_vectorize_data.",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s the modified script with the requested changes, including\n\nencapsulating the data preprocessing and vectorization steps into a function:\n\nimport pandas as pd import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt # Download NLTK resources (only needed once) nltk.download('punkt') nltk.download('stopwords') # List of color-related words color_words = [ 'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', ' 'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silv 'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'o 'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'kh 'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'l 'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle' ] def preprocess_and_vectorize_data(data): # Drop rows with missing 'Description' values data = data.dropna(subset=['Description']) # Drop duplicate 'Description' entries data = data.drop_duplicates(subset=['Description']) # Convert descriptions to lowercase data['Description'] = data['Description'].str.lower() # Tokenization and removal of punctuation data['Description'] = data['Description'].apply(word_tokeni # Remove stopwords and color-related words stop_words = set(stopwords.words('english')) data['Description'] = data['Description'].apply(lambda x: [ # Convert tokenized descriptions back to strings data['Description'] = data['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Yo",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Descrip # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfi\n\nreturn tfidf_df # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df) # Calculate the sum of TF-IDF scores for each feature feature_sum = tfidf_df.sum() # Get the top 100 features by frequency top_features = feature_sum.nlargest(100) # Create a bar chart plt.figure(figsize=(10, 12)) top_features.plot(kind='barh') plt.xlabel('TF-IDF Score Sum') plt.ylabel('Words') plt.title('Top 100 Most Frequent Features from TF-IDF Matrix') plt.tight_layout() plt.show()\n\nThis script encapsulates the data preprocessing and vectorization steps into the preprocess_and_vectorize_data function. The function takes the raw\n\ndata as input, performs the required preprocessing, and returns the TF-IDF\n\nmatrix. The rest of the script remains similar to before.\n\n[End of response]\n\nAfter running the script, we can visualize the updated set of the most\n\nfrequent words from the TF-IDF matrix output.",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "Figure 16.14: Most frequent features that are not colors\n\nLet’s proceed with our feature engineering. We will use ChatGPT to suggest options that can be explored with TF-IDF vectorization.\n\n[Prompt]\n\nThe TF-IDF vectorization is initialized with max_features parameter. What are other initialization options?",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "[End of prompt]\n\n[Prompt response]\n\nWhen initializing the TF-IDF vectorizer in scikit-learn, you can set several\n\nparameters to customize its behavior.\n\nIn addition to the max_features parameter, here are some other important\n\nparameters you can consider:\n\n1. max_df and min_df: These parameters control the threshold for the document frequency of terms. For example, you can set max_df to ignore terms that appear in a high percentage of documents, and min_df to ignore terms that appear in a low percentage of documents.\n\n2. ngram_range: This parameter specifies the range of n-grams to\n\nconsider during tokenization. For example, setting ngram_range=(1, 2) will consider both individual words and pairs of consecutive words\n\n(bigrams).\n\n3. stop_words: You can provide a list of stopwords to be ignored during tokenization. This can be a custom list or one of the built-in lists like 'english'.\n\n4. tokenizer: You can specify a custom tokenizer function if you want to\n\nperform additional text preprocessing or tokenization.\n\n5. use_idf: This parameter controls whether to use the inverse-\n\ndocument-frequency (IDF) re-weighting of the term frequencies.\n\n6. smooth_idf: This parameter controls whether to add a smoothing term\n\nto the IDF to avoid division by zero errors.\n\n7. sublinear_tf: If set to True, it applies sublinear scaling to the term\n\nfrequency, which can help to prevent very frequent terms from\n\ndominating the calculation.",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "Here’s an example of how you can initialize the TF-IDF vectorizer with\n\nsome of these parameters:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer = TfidfVectorizer( max_features=1000, max_df=0.8, min_df=0.05, ngram_range=(1, 2), stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=True )\n\n[End of response]\n\nFeel free to experiment with these parameters based on your specific dataset\n\nand goals. The choice of parameters may depend on the nature of the text\n\ndata and the desired outcomes of your clustering or categorization task.\n\nChoose clustering algorithm There are quite a few clustering algorithms we could choose. There is, for\n\nexample, K-means, that we used for another use case; there’s also\n\nhierarchical clustering, DBSCAN (Density-based clustering algorithm),\n\nGaussian mixture models (GMMs), and agglomerative clustering. We’re\n\ngoing to go with K-means, but why is that? The reason is that we’re\n\nworking with customers and want to divide them up into groups based on\n\npurchasing behavior, demographic properties, and so on. Another reason for\n\nchoosing K-means is its simplicity and being able to work with large\n\ndatasets.",
      "content_length": 1185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "You might wonder when to apply the other mentioned clustering algorithms\n\nin the context of e-commerce. Well, let’s look at some interesting areas in\n\nthis domain:\n\nProduct recommendation: Here, it would be more appropriate to use\n\nhierarchical clustering as products usually come in hierarchies and we\n\nwant to understand the relationship between products.\n\nAnomaly detection: Another important area is anomaly detection,\n\nespecially fraudulent transactions. Here, you want to understand\n\nunusual customer behavior to identify outliers; for that use case,\n\nDBSCAN is the best algorithm to go with.\n\nPersonalization: You want to ensure customers feel like they have an\n\nexperience uniquely fitted to their behaviors on the website. GMM is a\n\ngreat choice for this use case when a customer’s behavior is a bit more\n\ncomplex.\n\nFeature scaling Feature scaling and TF-IDF are both preprocessing techniques and it only\n\nmakes sense to choose one of them. Which one to choose depends on what\n\nwe want to use it for. In the context of e-commerce, here’s how you could\n\nreason:\n\nChoose feature scaling if you’re dealing with numerical features like\n\ncustomer age, purchase amounts, and purchase frequency (e.g., grouping customers based on their purchasing behavior).\n\nChoose the TF-IDF matrix if you’re looking at other aspects of e-\n\ncommerce like customer reviews, product descriptions, and search queries.",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "We can skip this step as we are using a TF-IDF matrix that is already\n\nscaled.\n\nApply clustering algorithm Next, let’s apply our chosen clustering algorithm:\n\n[Prompt]\n\nI would like to use english stop_words, no max_features, and pass min_df to preprocess_and_vectorize_data function to set minimum number of\n\ndocuments the feature appears in. I would like to try a range of values for min_df: 5, 10, 20, 30 and compare the silhouette score from the K-means output. I would like to visualize the result.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s modify the script to accommodate your requirements for min_df\n\nand the comparison of silhouette scores for different values of min_df using KMeans clustering. Here’s the updated script:\n\ndef preprocess_and_vectorize_data(data, min_df): # Drop rows with missing 'Description' values data = data.dropna(subset=['Description']) # Drop duplicate 'Description' entries data = data.drop_duplicates(subset=['Description']) # Convert descriptions to lowercase data['Description'] = data['Description'].str.lower() # Tokenization and removal of punctuation data['Description'] = data['Description'].apply(word_tokeni # Remove stopwords and color-related words stop_words = set(stopwords.words('english')) data['Description'] = data['Description'].apply(lambda x: [ # Convert tokenized descriptions back to strings",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "data['Description'] = data['Description'].apply(' '.join) # TF-IDF vectorization tfidf_vectorizer = TfidfVectorizer(min_df=min_df) tfidf_matrix = tfidf_vectorizer.fit_transform(data['Descrip # Convert TF-IDF matrix to a DataFrame tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfi\n\nreturn tfidf_df # Load the data from the CSV file using Latin1 encoding data_url = \"../Datasets/data.csv\" df = pd.read_csv(data_url, encoding='latin1') # Different values for min_df min_df_values = [5, 10, 20, 30] # Store silhouette scores silhouette_scores = [] # Perform clustering and calculate silhouette scores for differ for min_df in min_df_values: # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=10, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append(silhouette_score(tfidf_df, cluster # Visualize the silhouette scores plt.plot(min_df_values, silhouette_scores, marker='o') plt.xlabel('min_df') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. min_df') plt.xticks(min_df_values) plt.grid() plt.show()\n\nIn this script, we’re looping through different values of min_df (5, 10, 20,\n\nand 30) and performing the following steps for each value:",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "1. Preprocess and vectorize the data using the\n\npreprocess_and_vectorize_data function with the specified min_df\n\nvalue.\n\n2. Perform K-means clustering with 10 clusters (you can adjust the\n\nnumber of clusters as needed).\n\n3. Calculate the silhouette score for the clustering results and store it in\n\nthe silhouette_scores list.\n\nFinally, we visualize the silhouette scores for different min_df values using\n\na line plot. This will help you compare how different values of min_df impact the quality of the clustering results.\n\nFigure 16.15: Silhouette score versus min_df\n\n[End of response]",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "Next, we will investigate the impact of the number of clusters on the quality\n\nof the clustering results. Let’s proceed with the next prompt.\n\n[Prompt]\n\nI would also like to try different number of clusters num_clusters = [3,4,\n\n5,6,7] and visually compare silhouette score outputs for min_df, num_clusters combinations. Please add the script that generates a plot of\n\ncluster and silhouette score values for each of the values from num_clusters.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! Here’s the updated script that generates a plot of silhouette scores\n\nfor different combinations of min_df and num_clusters:\n\n# Different values for min_df and num_clusters min_df_values = [5, 10, 20, 30] num_clusters = [3, 4, 5, 6, 7] # Store silhouette scores for each combination silhouette_scores = [] # Perform clustering and calculate silhouette scores for differ for min_df, n_clusters in itertools.product(min_df_values, num_ # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append((min_df, n_clusters, silhouette_sc # Convert silhouette scores to a DataFrame for easier visualiza silhouette_scores_df = pd.DataFrame(silhouette_scores, columns= # Plot the silhouette scores plt.figure(figsize=(10, 6))",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "for min_df in min_df_values: subset_df = silhouette_scores_df[silhouette_scores_df['min_ plt.plot(subset_df['num_clusters'], subset_df['silhouette_s\n\nplt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters for Differen plt.legend() plt.grid() plt.show()\n\nIn this script, we use the itertools.product function to create\n\ncombinations of min_df and num_clusters values. We calculate silhouette\n\nscores for each combination and store them in the silhouette_scores list. Then, we convert the silhouette scores to a DataFrame for easier\n\nvisualization. The resulting plot shows how the silhouette score varies for different values of num_clusters while considering different min_df values.\n\n[End of response]\n\nHere is the output of the script:",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "Figure 16.16: Silhouette score versus different min_df values\n\nLet’s refine the plot to show the number of features corresponding to the min_fd parameter value.\n\n[Prompt]\n\nThe plot is good. I would like a small change by adding the number of features for each of the min_df values to the label=f'min_df = {min_df}\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! You can add the number of features corresponding to each\n\nmin_df value to the legend labels.\n\nHere’s the modified code with that change:\n\n# ... # Store silhouette scores for each combination silhouette_scores = []",
      "content_length": 575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "# Perform clustering and calculate silhouette scores for differ for min_df, n_clusters in itertools.product(min_df_values, num_ # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette_scores.append((min_df, n_clusters, silhouette_sc # Convert silhouette scores to a DataFrame for easier visualiza silhouette_scores_df = pd.DataFrame(silhouette_scores, columns= # Plot the silhouette scores plt.figure(figsize=(10, 6)) for min_df in min_df_values: subset_df = silhouette_scores_df[silhouette_scores_df['min_\n\n# Get the number of features for the current min_df value num_features = preprocess_and_vectorize_data(df, min_df).sh\n\nplt.plot(subset_df['num_clusters'], subset_df['silhouette_s\n\nplt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters for Differen plt.legend() plt.grid() plt.show()\n\nIn this modified script, we calculate the number of features for each min_df\n\nvalue using the shape[1] attribute of the TF-IDF DataFrame. Then, we include this information in the legend labels for each line on the plot. This change will help you visualize the impact of both min_df and the number of\n\nfeatures on the silhouette scores.",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "Figure 16.17: Silhouette score versus different min_df values with number of features\n\n[End of response]\n\nLet’s proceed with the next prompt.\n\n[Prompt]\n\nI will use the value of min_df_value = 20, num_clusters= [3, 4, 5, 6]. I would like to know the number of products in each cluster. Can you add a plot Products per Cluster to the previous code to show number of\n\nproducts per cluster for num_clusters_values in [3, 4, 5, 6]\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! You can add a plot to visualize the number of products in each\n\ncluster for different num_clusters values. Here’s the modified script with\n\nthe requested changes:",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "# ... # Chosen min_df and num_clusters values min_df_value = 20 num_clusters_values = [3, 4, 5, 6] # Store silhouette scores and number of products per cluster results = [] # Perform clustering and calculate silhouette scores for chosen for n_clusters in num_clusters_values: # Preprocess and vectorize the data tfidf_df = preprocess_and_vectorize_data(df, min_df_value)\n\n# Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_df)\n\n# Calculate silhouette score silhouette = silhouette_score(tfidf_df, cluster_labels)\n\n# Count number of products in each cluster cluster_counts = pd.Series(cluster_labels).value_counts().s\n\nresults.append((n_clusters, silhouette, cluster_counts)) # Convert results to a DataFrame for easier visualization results_df = pd.DataFrame(results, columns=['num_clusters', 'si # Plot the silhouette scores plt.figure(figsize=(15, 6)) # Silhouette Score plot plt.subplot(1, 2, 1) plt.plot(results_df['num_clusters'], results_df['silhouette_sco plt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Silhouette Score vs. Number of Clusters') plt.grid() # Products per Cluster plot plt.subplot(1, 2, 2) for n_clusters, cluster_counts in results_df[['num_clusters', ' plt.plot(range(1, n_clusters + 1), cluster_counts, marker=' plt.xlabel('Cluster') plt.ylabel('Number of Products') plt.title('Number of Products per Cluster') plt.legend() plt.grid()",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "plt.tight_layout() plt.show()\n\nIn this modified script, we calculate the number of products in each cluster\n\nusing pd.Series(cluster_labels).value_counts().sort_index() and store\n\nthe results along with silhouette scores in the results list. We then create a DataFrame from the results for easier visualization.\n\nThe script includes two plots side by side: the first plot displays the silhouette scores for different num_clusters values, and the second plot\n\ndisplays the number of products in each cluster for the same num_clusters values. This will help you understand the relationship between the number\n\nof clusters, silhouette scores, and the distribution of products among\n\nclusters.\n\nAfter running the script, you should see the following output:\n\nFigure 16.18: Added plot of products per cluster\n\n[End of response]\n\nWe will now select the number of clusters and min_df value and proceed\n\nwith the next prompt.",
      "content_length": 917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Interpret clusters and visualize results We’ll be looking into interpreting clusters and visualizing the results. What\n\nare the things we should bear in mind?\n\nInterpreting cluster For cluster interpretation, we should consider things like:\n\nCohesion: Ensure data points are similar to each other. You can find\n\nthis out by examining terms that are the most representative in each\n\ncluster.\n\nSeparation: Clusters should be well separated ensuring they are distinct\n\nand meaningful.\n\nLabels: Clusters should also have descriptive labels.\n\nOutliers: It’s important to identify outliers that don’t fit well in any\n\ncluster.\n\nVisualizing clusters For visualizing clusters, here are some things to consider:\n\nDimensionality reduction: You want to reduce the high-dimensional\n\nTF-IDF matrix to 2D or 3D to make it easier to visualize. For this, a\n\ntechnique like PCA comes in handy.\n\nScatter plots: Scatter plots are great for visualizing data points; a point\n\ncan be a document, for example, and colors or shapes could represent\n\ndifferent clusters.\n\nWord clouds: You could also use word clouds if you look at text, to see\n\nwhat the most common words used are.",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Heat maps: Heat maps are another great way to visualize frequency\n\nand will help highlight the most important terms per cluster.\n\nInteractivity: Ensure you allow for exploration to help you drill down\n\ninto specific data points by using, for example, Plotly.\n\nNext, let’s craft a prompt for visualization.\n\n[Prompt]\n\nI will use the value of min_df_value = 20 and 5 clusters. I would like to use\n\nPCA with 3 components to visualize product clusters on a 3D graph. The colors of clusters should be mapped from the seaborn color_palette.\n\n[End of prompt]\n\n[Prompt response]\n\nCertainly! In order to visualize the product clusters in a 3D graph using\n\nPCA with 3 components and mapping cluster colors from a seaborn color\n\npalette, you can follow these steps:\n\n1. Import necessary libraries.\n\n2. Preprocess the data and perform clustering.\n\n3. Apply PCA to reduce the dimensionality to 3 components.\n\n4. Plot the clusters in a 3D scatter plot with seaborn color palette.\n\nHere’s the code to achieve this:\n\nfrom sklearn.decomposition import PCA import seaborn as sns import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Required for 3D plot # Selected min_df and num_clusters values min_df_value = 20 num_clusters = 5 # Preprocess data and perform clustering",
      "content_length": 1273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "tfidf_matrix, tfidf_vectorizer = preprocess_and_vectorize_data( kmeans = KMeans(n_clusters=num_clusters, random_state=42) cluster_labels = kmeans.fit_predict(tfidf_matrix) # Apply PCA with 3 components pca = PCA(n_components=3) pca_result = pca.fit_transform(tfidf_matrix.toarray()) # Create a DataFrame for PCA results pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2', ' pca_df['Cluster'] = cluster_labels # Seaborn color palette for cluster colors color_palette = sns.color_palette(\"Set1\", n_colors=num_clusters # Plot 3D scatter plot fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d') for cluster_id in range(num_clusters): cluster_points = pca_df[pca_df['Cluster'] == cluster_id] ax.scatter(cluster_points['PC1'], cluster_points['PC2'], cl ax.set_xlabel('PC1') ax.set_ylabel('PC2') ax.set_zlabel('PC3') ax.set_title('Product Clusters in 3D') ax.legend() plt.show()\n\n[End of response]\n\nThis code will preprocess the data, perform clustering, apply PCA to reduce\n\ndimensionality to 3 components, and then plot the product clusters in a 3D scatter plot with cluster colors mapped from a seaborn color palette. Adjust\n\nthe figure size, labels, and other plot parameters as needed to customize the\n\nvisualization.",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "Figure 16.19: Product clusters visualized in 3D space\n\nCreating a word cloud Now, we will examine the most frequent words in each of the clusters. Let’s\n\nproceed with the next prompt.\n\n[Prompt]\n\nI would like to visualize the most frequent words occurring in product descriptions assigned to clusters. I would like you to use the word cloud",
      "content_length": 339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "library and create a function that will output word clouds for each of the\n\nclusters.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! To visualize the most frequent words occurring in product descriptions assigned to clusters, you can use the WordCloud library. Here’s\n\nhow you can create a function that generates word clouds for each of the\n\nclusters:\n\nfrom wordcloud import WordCloud # Create a DataFrame for clustering results clustering_results = pd.DataFrame(data={'Description': df['Desc # Function to generate word clouds for each cluster def generate_wordclouds(dataframe, num_clusters): for cluster_id in range(num_clusters): cluster_data = dataframe[dataframe['Cluster'] == cluste cluster_text = ' '.join(cluster_data['Description'])\n\n# Create a WordCloud object wordcloud = WordCloud(width=800, height=400, background\n\n# Plot the WordCloud plt.figure(figsize=(10, 6)) plt.imshow(wordcloud, interpolation='bilinear') plt.title(f'Cluster {cluster_id} - Most Frequent Words' plt.axis('off') plt.show() # Generate word clouds for each cluster generate_wordclouds(clustering_results, num_clusters)\n\nThis code defines a function generate_wordclouds that generates word\n\nclouds for each cluster based on the product descriptions assigned to that cluster. Adjust the parameters of the WordCloud object (e.g., width, height,",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "background_color, max_words) as needed for your visualization\n\npreferences.\n\nThe function will plot the word clouds for each cluster.\n\nFigure 16.20: Word cloud for the first cluster",
      "content_length": 181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "Figure 16.21: Word cloud for the second cluster",
      "content_length": 47,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "Figure 16.22: Word cloud for the third cluster",
      "content_length": 46,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "Figure 16.23: Word cloud for the fourth cluster\n\n[End of response]\n\nAssigning categories to products and evaluating and reﬁning Neither assigning categories to products nor evaluating and refining steps\n\nwere in scope for this chapter to solve but let’s describe what they are to\n\ngive you an idea of how you could explore those should you want to.\n\nOur dataset contains invoices, and for us to categorize products, we would\n\nfirst need to understand the use cases we could possibly have. Those could\n\nbe, for example:",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "Organizing inventory\n\nImproving search functionality\n\nEnhancing the user experience\n\nAs you can see, this is very centered on product data, which means we\n\nwould probably need a completely different dataset containing product\n\ntitles, descriptions, and more. Let’s describe at a high level what our\n\napproach could be using ChatGPT.\n\n[Prompt]\n\nI’m looking at an e-commerce dataset. What can you tell me about the\n\nprocess of assigning categories to products?\n\n[End of prompt]\n\n[Prompt response]\n\nAssigning categories to products in an e-commerce dataset is a crucial task\n\nfor organizing inventory, improving search functionality, and enhancing the\n\nuser experience. Here’s a step-by-step guide to help you through the\n\nprocess:\n\nSteps to Assign Categories to Products:\n\nUnderstand the Data:\n\nReview Product Descriptions: Examine the product titles,\n\ndescriptions, and any other relevant attributes.\n\nIdentify Existing Categories: If there are existing categories, review\n\nthem to understand the current classification system.\n\nDefine Categories:",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "Create a Category Hierarchy: Develop a hierarchical structure of\n\ncategories and subcategories that makes sense for your products.\n\nEnsure Completeness: Make sure your categories cover all types of\n\nproducts in your dataset.\n\nFeature Extraction:\n\nText Features: Extract relevant text features from product titles and\n\ndescriptions using techniques like TF-IDF or word embeddings.\n\nOther Features: Consider other attributes like price, brand, and\n\nspecifications that might help in categorization.\n\nManual Labeling:\n\nSample Labeling: Manually label a sample of products to create a\n\ntraining dataset. This helps in understanding the nuances of each\n\ncategory.\n\nModel Selection:\n\nChoose a Classification Model: Select a machine learning model\n\nsuitable for text classification, such as Naive Bayes, SVM, or deep\n\nlearning models like LSTM or BERT.\n\nTraining the Model:\n\nTrain the Model: Use the labeled data to train your classification\n\nmodel.\n\nEvaluate Performance: Evaluate the model’s performance using\n\nmetrics like accuracy, precision, recall, and F1-score.\n\nAssign Categories:\n\nPredict Categories: Use the trained model to predict categories for the\n\nunlabeled products.",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "Review Predictions: Manually review a subset of the predictions to\n\nensure accuracy.\n\nIterate and Improve:\n\nRefine Categories: Based on the results, refine your category\n\ndefinitions and model.\n\nContinuous Learning: Continuously update the model with new data\n\nto improve its accuracy over time.\n\nExample in Python:\n\nHere’s a simplified example using a Naive Bayes classifier for text\n\nclassification:\n\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score # Sample data data = { 'Product': ['Red Shirt', 'Blue Jeans', 'Green Hat', 'Black Shoe 'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwear'] } df = pd.DataFrame(data) # Feature extraction vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(df['Product']) y = df['Category'] # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # Train the model model = MultinomialNB() model.fit(X_train, y_train) # Predict and evaluate",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "y_pred = model.predict(X_test) print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\nThis code provides a basic framework for categorizing products based on\n\ntheir names. For a real-world application, you would need a larger dataset\n\nand more sophisticated preprocessing and modeling techniques.\n\nIf you have any specific questions or need further details, feel free to ask!\n\n[End of response]\n\nWe’ll leave it up to you to pursue this further. From what you’ve hopefully\n\nseen in this chapter already, start with a high-level prompt explaining what\n\ndataset you have (you have to find a dataset for the above problem) and\n\nwhat you aim to do. On the latter, you already have the steps, so you’re\n\nsuggested to craft a prompt explaining what to do for each step and what\n\ncode to run.\n\nEvaluate and reﬁne This is also out of scope, but let’s get you started on a ChatGPT prompt that\n\nyou can then keep working on.\n\n[Prompt]\n\nDescribe evaluate and refine step in the context of e-commerce data on\n\ninvoices.\n\n[End of prompt]\n\n[Prompt response]\n\nSure! Let’s break down the evaluate and refine steps in the context of e-\n\ncommerce data on invoices:",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "Evaluate Step:\n\n1. Model Performance Metrics:\n\nAccuracy: Measure how often the model correctly predicts the\n\ncategory of an invoice.\n\nPrecision and Recall: Evaluate the precision (correct positive\n\npredictions) and recall (ability to find all relevant instances) for\n\neach category.\n\nF1-Score: Combine precision and recall into a single metric to\n\nbalance both aspects.\n\n2. Confusion Matrix:\n\nVisualize Predictions: Use a confusion matrix to see where the\n\nmodel is making correct and incorrect predictions. This helps\n\nidentify specific categories where the model may be struggling.\n\n3. Cross-Validation:\n\nRobust Evaluation: Perform cross-validation to ensure the\n\nmodel’s performance is consistent across different subsets of the\n\ndata.\n\n4. Error Analysis:\n\nAnalyze Misclassifications: Examine the invoices that were\n\nmisclassified to understand why the model made errors. Look\n\nfor patterns or common issues.\n\nRefine Step:\n\n1. Feature Engineering:\n\nEnhance Features: Create new features or improve existing\n\nones to provide more relevant information to the model. For\n\nexample, extracting keywords from invoice descriptions or\n\nadding metadata like invoice date and amount.",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "2. Hyperparameter Tuning:\n\nOptimize Model Parameters: Use techniques like grid search\n\nor random search to find the best hyperparameters for your\n\nmodel, improving its performance.\n\n3. Model Selection:\n\nTry Different Models: Experiment with different algorithms\n\n(e.g., decision trees, random forests, neural networks) to see if\n\nanother model performs better.\n\n4. Data Augmentation:\n\nIncrease Data Variety: Augment your dataset by generating\n\nsynthetic data or using techniques like oversampling for\n\nunderrepresented categories.\n\n5. Regular Updates:\n\nContinuous Learning: Regularly update the model with new\n\ninvoice data to keep it current and improve its accuracy over\n\ntime.\n\nExample in Python:\n\nHere’s a simplified example of evaluating and refining a model using cross-\n\nvalidation and hyperparameter tuning:\n\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split, GridSearc from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report, confusion_ma import seaborn as sns import matplotlib.pyplot as plt # Sample data data = { 'Invoice': ['Invoice for Red Shirt', 'Invoice for Blue Jean 'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwe",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "} df = pd.DataFrame(data) # Feature extraction vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(df['Invoice']) y = df['Category'] # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_ # Model training model = MultinomialNB() model.fit(X_train, y_train) # Model evaluation y_pred = model.predict(X_test) print(classification_report(y_test, y_pred)) # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted') plt.ylabel('Actual') plt.title('Confusion Matrix') plt.show() # Hyperparameter tuning param_grid = {'alpha': [0.1, 0.5, 1.0]} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5) grid_search.fit(X_train, y_train) print(f'Best parameters: {grid_search.best_params_}')\n\nThis code provides a framework for evaluating and refining a model using\n\ncross-validation and hyperparameter tuning.\n\nIf you have any specific questions or need further details, feel free to ask!\n\n[End of response]\n\nReﬂection on prompts for this use case",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "Just like with our first use case in this chapter, we followed a specific\n\nmethod of first setting high-level context of describing our role, the shape\n\nof the dataset, and what we were looking to do. Then, we followed the\n\nbelow process to first breaking down the problem in steps, getting code and\n\nhow we could continue to improve and refine and finally visualize the\n\nresults:\n\n1. Step-by-step guidance: The requests are structured as a series of step-\n\nby-step tasks, breaking down the larger problem into manageable\n\ncomponents. This makes it easier to follow and implement the solution\n\nincrementally.\n\n2. Specify inputs and requirements: Provide clear and specific\n\ninformation about the dataset, its columns, and the requirements for\n\npreprocessing, clustering, and visualization. This helps ensure that the\n\nassistance received is tailored to the particular needs.\n\n3. Request for code with comments: Code snippets request to include\n\ncomments to explain each step and clarify the purpose of the code.\n\nThis helps in understanding the code and code validation and\n\nfacilitates learning.\n\n4. Iteration and refinement: Iteration of prompts, asking for additional\n\nmodifications and visualizations after the initial guidance. This\n\niterative approach allows for refining and improving the solution\n\nprogressively.\n\n5. Visualization and interpretation: Visualizing and interpreting the\n\nresults allows focusing on deriving meaningful insights from the data.\n\nAssignment",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "In the previous section, we used traditional embedding with TF-IDF to\n\ntransform text data into numerical representations, which can then be used\n\nfor various natural language processing (NLP) tasks such as clustering.\n\nLet’s now try and improve the clustering results by using a more advanced\n\nembedding technique. We will use the Hugging Face Transformers library\n\nto get pre-trained embeddings for our product descriptions:\n\n1. Ask ChatGPT to explain Hugging Face Transformers’ advantages over\n\nTF-IDF vectorization for clustering use cases.\n\n2. Use ChatGPT to generate and create product clusters using Hugging\n\nFace Transformers embeddings.\n\n3. Compare the results with the previous clustering results using TF-IDF\n\nvectorization.\n\nSolution See the solution in the repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary This chapter focused on clustering and how it could be used to group your\n\ndata into separate areas. Creating these areas made it easier to understand\n\nour data points. Through visualization like heat maps, word clouds, and\n\nmore, you were given the insight that data benefits from being shown in\n\ndifferent ways. You also saw how the clustering process helped identify\n\noutliers, that is, data that vastly differs and can’t easily be assigned to any",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "one cluster. For the ChatGPT and prompting part, you saw how setting a\n\nhigh-level context describing the dataset helped generate a suitable set of\n\nsteps you could follow from top to bottom. The same high-level context\n\nalso helped ChatGPT recommend a clustering algorithm.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "17\n\nMachine Learning with Copilot\n\nIntroduction Machine learning, or ML, involves data and learning patterns from that said\n\ndata and using those patterns to make predictions or decisions. Machine\n\nlearning consists of a series of steps, all the way from loading data and\n\ncleaning it to eventually training a model to get the insights you need from\n\nsaid model. All these steps are roughly the same for most problems in this\n\nproblem space. However, details may differ, like the choice of pre-\n\nprocessing step, the choice of algorithm, etc. An AI tool like GitHub\n\nCopilot comes into machine learning from a few different angles:\n\nSuggesting workflows: Thanks to Copilot having been trained in\n\nmachine learning work flows, it’s able to suggest a workflow that fits your problem.\n\nRecommending tools and algorithms: If you provide your AI tool with enough context on what your problem is and the shape of your\n\ndata, an AI tool like Copilot can suggest tools and algorithms that fit\n\nyour specific problem.\n\nCode assistance: Another way that Copilot is a great help is by being\n\nable to generate code for various steps in the machine learning process.",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "This chapter will explore an e-commerce dataset, and the chapter will serve\n\nas an interesting comparison exercise to the other chapters, which used\n\nChatGPT to solve machine learning problems.\n\nLet’s dive in and discover the suggestions from GitHub Copilot.\n\nGitHub Copilot Chat in your IDE GitHub Copilot Chat is a tool within certain Integrated Development\n\nEnvironments (IDEs) that answers coding questions. It helps by\n\nsuggesting code, explaining code functionality, creating unit tests, and\n\nfixing bugs.\n\nHow it works You have two different ways of providing prompts to GitHub Copilot:\n\nIn-editor: In this mode, you provide text comments, and through the\n\nTab or Return key, Copilot is able to produce an output.\n\nChat mode: In chat mode, you type a prompt in the text box, and then\n\nGitHub Copilot will treat an open file/files as context (if you use @workspace, then it will look at all files in your directory).\n\nA text file can be, for example, a code file like app.py or a Jupyter Notebook. Copilot can treat both these files as context, together with your\n\ntyped prompt.",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "Figure 17.1: GitHub Copilot chat on the left side and an open Jupyter Notebook on the right side\n\nDataset overview Let’s explore the dataset we’re about to use. Like we did in other chapters\n\non machine learning, we start with a dataset, this one being a dataset of Amazon book reviews.\n\nThe dataset contains information about different products and their reviews.\n\nIt includes the following columns:\n\nmarketplace (string): Location of the product\n\ncustomer_id (string): Unique ID of the customer\n\nreview_id (string): Review ID\n\nproduct_id (string): Unique ID of the product\n\nproduct_parent (string): Parent product",
      "content_length": 615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "product_title (string): Title of the product reviewed\n\nproduct_category (string): Different product categories\n\nstar_rating (int): Rating of the product out of 5\n\nhelpful_votes (int): Number of helpful votes for the product\n\ntotal_votes (int): Total number of votes for the product\n\nreview_headline (string): Heading of the review\n\nreview_body (string): Content of the review\n\nreview_date (string): Date on which the product was reviewed\n\nsentiment (string): Sentiment of the review (positive or negative)\n\nSteps for data exploration Performing data exploration helps us understand the dataset and its\n\ncharacteristics. It involves examining the data, identifying patterns, and summarizing key insights. Here are the steps we will follow:\n\n1. Load the dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the data: Display the first few rows of the DataFrame to get a\n\nglimpse of the data. Check the column names, data types, and any\n\nmissing values.\n\n3. Summary statistics: Calculate summary statistics such as the mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This helps in understanding the distribution and range of values.\n\n4. Explore categorical variables: Analyze the unique values and their\n\nfrequencies for categorical variables like marketplace, product_category, and sentiment. Visualizations such as bar plots\n\ncan be helpful for this analysis.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "5. Distribution of ratings: Plot a histogram or bar plot to visualize the\n\ndistribution of star_ratings. This helps in understanding the overall\n\nsentiment of the reviews.\n\n6. Temporal analysis: Analyze the temporal aspect of the data by\n\nexamining the review_date column. Explore trends, seasonality, or any patterns over time.\n\n7. Review length analysis: Analyze the length of review_body to\n\nunderstand the amount of information provided in the reviews.\n\nCalculate descriptive statistics like the mean, median, and maximum\n\nlength.\n\n8. Correlation analysis: Investigate the correlation between numeric\n\nvariables using correlation matrices or scatter plots. This helps in\n\nidentifying relationships between variables.\n\n9. Additional exploratory analysis: Conduct additional analysis based on specific project requirements or interesting patterns observed during the exploration process.\n\nNote that you can also ask GitHub Copilot which steps to follow when\n\ndoing machine learning.\n\nPrompt strategy The prompts we are about to use provide high-level guidance for Copilot,\n\nand the outputs/results allow further tailoring of Copilot’s responses to\n\nmatch the specific dataset and analysis needs.\n\nThe key aspects of the prompting approach are:\n\nDefine the task. Clearly instruct the AI assistant what task we are solving.",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "Break down into steps. Breaking the data exploration down into logical steps (like data loading, inspection, summary stats etc.)\n\nProviding context/intent for each prompt to guide Copilot (like\n\nrequesting numeric summary statistics)\n\nSharing previous results as input. Sharing outputs and results from Copilot’s code snippets to further guide the conversation (like printing\n\nthe summary stats)\n\nRefine, iteratively refining prompts and conversing with Copilot in a back-and-forth way\n\nTherefore, we will use the TAG (Task-Action-Guidance) prompt pattern\n\ndescribed in Chapter 2. Let’s describe this project to fit this pattern, so we\n\nget a sense on how to write our initial prompt:\n\nTask: Data exploration, find patterns and insights into customers reviews in an e-commerce project.\n\nAction: We’ve described the steps we should take in a previous\n\nsection; those should be reflected in the prompt we write.\n\nGuidance: The extra guidance we will provide is that we want exploratory techniques suggested as well as code snippets.\n\nYour initial data exploration prompt: Prompt 1, setting the high-level context Just like in other chapters where we used ChatGPT, our initial prompt sets\n\nthe high-level context for the problem we’re looking to solve, including the",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "domain and the shape of our data. All this context helps the AI tool to provide the correct steps both in text and code.\n\nBelow is an initial prompt you can try:\n\n[Prompt]\n\nI am performing data exploration for the AwesomeShop e-commerce\n\nproject. The dataset contains information about various products and their\n\nreviews. I want to gain insights into the data, identify patterns, and understand the characteristics of the reviews. Can you provide me with\n\nsome exploratory analysis techniques and code snippets to help me uncover interesting insights from the dataset? The dataset for the AwesomeShop E-\n\ncommerce project contains information about different products and their\n\nreviews. It includes the following columns:\n\nmarketplace (string): Location of the product\n\ncustomer_id (string): Unique ID of the customer\n\nreview_id (string): Review ID\n\nproduct_id (string): Unique ID of the product\n\nproduct_parent (string): Parent product\n\nproduct_title (string): Title of the product reviewed\n\nproduct_category (string): Different product categories\n\nstar_rating (int): Rating of the product out of 5\n\nhelpful_votes (int): Number of helpful votes for the product\n\ntotal_votes (int): Total number of votes for the product\n\nreview_headline (string): Heading of the review\n\nreview_body (string): Content of the review\n\nreview_date (string): Date on which the product was reviewed\n\nsentiment (string): Sentiment of the review (positive or negative)",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "But remember that I want to learn step by step, as follows:\n\n1. Load the Dataset: Read the dataset into a pandas DataFrame to work\n\nwith the data effectively.\n\n2. Inspect the Data: Display the first few rows of the DataFrame to get a\n\nglimpse of the data. Check the column names, data types, and any\n\nmissing values.\n\n3. Summary Statistics: Calculate summary statistics such as the mean,\n\nmedian, minimum, maximum, and quartiles for numeric columns. This\n\nhelps in understanding the distribution and range of values.\n\n4. Explore Categorical Variables: Analyze the unique values and their\n\nfrequencies for categorical variables like marketplace,\n\nproduct_category, and sentiment. Visualizations such as bar\n\nplots can be helpful for this analysis.\n\n5. Distribution of Ratings: Plot a histogram or bar plot to visualize the\n\ndistribution of star_ratings. This helps in understanding the overall sentiment of the reviews.\n\n6. Temporal Analysis: Analyze the temporal aspect of the data by\n\nexamining the review_date column. Explore trends, seasonality, or\n\nany patterns over time.\n\n7. Review Length Analysis: Analyze the length of review_body to\n\nunderstand the amount of information provided in the reviews.\n\nCalculate descriptive statistics like the mean, median, and maximum length.\n\n8. Correlation Analysis: Investigate the correlation between numeric\n\nvariables using correlation matrices or scatter plots. This helps in\n\nidentifying relationships between variables.",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "9. Additional Exploratory Analysis: Conduct additional analysis based\n\non specific project requirements or interesting patterns observed\n\nduring the exploration process. Seek feedback from the user after each\n\nstep.\n\n[End of prompt]\n\nStep 1: Load the dataset We get quite a wordy response from Copilot, so let’s break down the\n\nresponse into parts and explain what we get.\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s start with step 1: loading the dataset.\n\n# Step 1: Load the Dataset import pandas as pd # Define the file path file_path = '/path/to/your/file.csv' # replace with your file # Load the dataset df = pd.read_csv(file_path) # Display the first few rows of the DataFrame print(df.head())\n\nPlease replace '/path/to/your/file.csv' with your actual filename or path\n\nto the file. Once you’ve done this, you can run the code to load your\n\ndataset.\n\nLet me know once you’ve successfully loaded the data, and we can proceed\n\nto the next step.",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "[End of response]\n\nFigure 17.2: Showing part of the Copilot prompt and the start of the Copilot result on loading data\n\nLet’s try the output code generated by Copilot and see if it works. Modify\n\nthe file_path variable to point to the location of your dataset, and then\n\nexecute the code. Note below how we edited the file path to reflect an\n\nactual path in our system:\n\nimport pandas as pd # Define the file path",
      "content_length": 413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "file_path = '../Datasets/amazon_books_Data.csv' # Load the dataset df = pd.read_csv(file_path)\n\nRunning the code for loading data Let’s run the code provided by Copilot. See the below output from our\n\nJupyter Notebook:\n\n# Display the first few rows of the DataFrame print(df.head()) Unnamed: 0 market_place customer_id review_id pro 0 0 \"US\" \"25933450\" \"RJOVP071AVAJO\" \"0439 1 1 \"US\" \"1801372\" \"R1ORGBETCDW3AI\" \"1623 2 2 \"US\" \"5782091\" \"R7TNRFQAOUTX5\" \"1421 3 3 \"US\" \"32715830\" \"R2GANXKDIFZ6OI\" \"0142 4 4 \"US\" \"14005703\" \"R2NYB6C3R8LVN6\" \"1604 product_parent product_titl 0 \"84656342\" \"There Was an Old Lady Who Swallowed a Shell! 1 \"729938122\" \"I Saw a Friend 2 \"678139048\" \"Black Lagoon, Vol. 6 3 \"712432151\" \"If I Stay 4 \"800572372\" \"Stars 'N Strips Forever product_category star_rating helpful_votes total_votes 0 \"Books\" 1 0 0 0 1 \"Books\" 1 0 0 0 2 \"Books\" 1 0 0 0 3 \"Books\" 1 0 0 0 4 \"Books\" 1 2 2 0 verified_purchase review_h 0 1 \\t(Y) \"Five 1 1 \\t(Y) \"Please buy \"I Saw a Friend\"! Your childre 2 1 \\t(Y) \"Shipped 3 0 \\t(N) \"Five 4 1 \\t(Y) \"Five review_body review_dat",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "0 \"I love it and so does my students!\" 2015-08-3 1 \"My wife and I ordered 2 books and gave them a... 2015-08-3 2 \"Great book just like all the others in the se... 2015-08-3 3 \"So beautiful\" 2015-08-3 4 \"Enjoyed the author's story and his quilts are... 2015-08-3 Sentiment_books review_month review_day review_year 0 positive August Monday 2015 1 positive August Monday 2015 2 positive August Monday 2015 3 positive August Monday 2015 4 positive August Monday 2015\n\nStep 2: Inspect the data Next, let’s inspect the data. For the inspection prompt, we’ll use the output\n\nfrom our code execution and see if we can get Copilot to tell us about the\n\nnext step we need to take:\n\n[Prompt]\n\nIt runs fine; below is the output:\n\nUnnamed: 0 market_place customer_id review_id product_id 0 0 \"U 1 1 \"US\" \"1801372\" \"R1ORGBETCDW3AI\" \"1623953553\" 2 2 \"US\" \"5782091\" \"R7TNRFQAOUTX5\" \"142151981X\" 3 3 \"US\" \"32715830\" \"R2GANXKDIFZ6OI\" \"014241543X\" 4 4 \"US\" \"14005703\" \"R2NYB6C3R8LVN6\" \"1604600527\" product_parent product_title 0 \"84656342\" \"There Was an Old Lad 1 \"729938122\" \"I Saw a Friend\" 2 \"678139048\" \"Black Lagoon, Vol. 6\" 3 \"712432151\" \"If I Stay\" 4 \"800572372\" \"Stars 'N Strips Forever\" product_category star_rating helpful_votes total_votes vine 0 \" 1 \"Books\" 1 0 0 0 \\t(N) 2 \"Books\" 1 0 0 0 \\t(N) 3 \"Books\" 1 0 0 0 \\t(N) 4 \"Books\" 1 2 2 0 \\t(N)",
      "content_length": 1337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": "verified_purchase review_headline 0 1 \\t(Y) \"Five Stars\" 1 1 \\t(Y) \"Please buy \"I Saw a Friend\"! Your children wi... 2 1 \\t(Y) \"Shipped fast.\" ... 1 positive August Monday 2015 2 positive August Monday 2015 3 positive August Monday 2015 4 positive August Monday 2015\n\nThe output is truncated. View it as a scrollable element or open it in a text\n\neditor. Adjust cell output settings...\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that you’ve successfully loaded the dataset, let’s move on to\n\n“inspecting the data.”\n\n# Step 2: Inspect the Data # Check the column names print(\"Column Names:\") print(df.columns) # Check the data types of each column print(\"\\nData Types:\") print(df.dtypes) # Check for missing values print(\"\\nMissing Values:\") print(df.isnull().sum())\n\nThis code will give you an overview of your dataset, including the names of\n\nthe columns, the data types of each column, and the number of missing\n\nvalues in each column. Run this code and let me know what you find.\n\n[End of response]",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "Let’s try the code above to inspect the data and see if it works. Run the\n\nbelow code in a code cell in your Jupyter Notebook:\n\n# Display the first few rows of the DataFrame df.head() # Check the column names column_names = df.columns print(column_names) # Check the data types of the columns data_types = df.dtypes print(data_types) # Check for missing values missing_values = df.isnull().sum() print(missing_values) You should see an output similar to below: Index(['Unnamed: 0', 'market_place', 'customer_id', 'review_id' 'product_parent', 'product_title', 'product_category', ' 'helpful_votes', 'total_votes', 'vine', 'verified_purcha 'review_headline', 'review_body', 'review_date', 'Sentim 'review_month', 'review_day', 'review_year'], dtype='object') Unnamed: 0 int64 market_place object customer_id object review_id object product_id object product_parent object product_title object product_category object star_rating int64 helpful_votes int64 total_votes int64 vine object verified_purchase object review_headline object review_body object review_date object Sentiment_books object review_month object review_day object review_year int64",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "dtype: object Unnamed: 0 0 market_place 0 customer_id 0 review_id 0 product_id 0 product_parent 0 product_title 0 product_category 0 star_rating 0 helpful_votes 0 total_votes 0 vine 0 verified_purchase 0 review_headline 0 review_body 0 review_date 0 Sentiment_books 0 review_month 0 review_day 0 review_year 0 dtype: int64\n\nGreat! So far, it has served us well when we provide Copilot with a state of\n\nwhere we are going and get it to suggest the next steps; let’s keep following\n\nthat format.\n\nStep 3: Summary statistics Next, we want to summarize things, so we again provide it with output data\n\nin the hope that Copilot will tell us what the next step is:\n\n[Prompt]\n\nThere are no missing values; below is the output:",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "Index(['Unnamed: 0', 'market_place', 'customer_id', 'review_id'\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we’ve inspected the data, let’s move on to step 3:\n\ncalculating summary statistics for numeric columns. This will help us\n\nunderstand the distribution and range of values.\n\n# Step 3: Summary Statistics # Calculate summary statistics for numeric columns numeric_columns = ['star_rating', 'helpful_votes', 'total_votes print(df[numeric_columns].describe())\n\nThis code will provide you with the count, mean, standard deviation,\n\nminimum, 25th percentile, median (50th percentile), 75th percentile, and\n\nmaximum of the numeric columns in your dataset. Run this code and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try this and see if it works. Let’s run the code below and call df.describe:\n\n# Calculate summary statistics summary_stats = df.describe() print(summary_stats)\n\nYou should see the following output:",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "Unnamed: 0 star_rating helpful_votes total_votes rev count 100.000000 100.000000 100.000000 100.000000 mean 49.500000 0.840000 0.540000 0.960000 std 29.011492 0.368453 1.298173 2.420118 min 0.000000 0.000000 0.000000 0.000000 25% 24.750000 1.000000 0.000000 0.000000 50% 49.500000 1.000000 0.000000 0.000000 75% 74.250000 1.000000 1.000000 1.000000 max 99.000000 1.000000 9.000000 14.000000\n\nGreat! We’re able to see the count, mean value standard deviation, and\n\nmore. We’ve continued to use the idea of feeding Copilot the output of our\n\ncode, and it’s been able to understand at which point in the machine\n\nlearning process it is and what to do next.\n\nStep 4: Explore categorical variables Just like before, let’s construct a prompt consisting of the output of the code\n\nwe just ran and let Copilot tell us what should happen next:\n\n[Prompt]\n\nBelow is the output:\n\nUnnamed: 0 star_rating helpful_votes total_votes review_year co 0.960000 2015.0 std 29.011492 0.368453 1.298173 2.420118 0.0 mi\n\n[End of prompt]\n\nYou should receive a response similar to the response below:",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "[Prompt response]\n\nGreat! Now that we have the summary statistics, let’s move on to step 4:\n\nexploring categorical variables. We’ll analyze the unique values and their frequencies for categorical variables like marketplace, product_category,\n\nand sentiment.\n\n# Step 4: Explore Categorical Variables # Analyze the unique values and their frequencies for categoric categorical_columns = ['market_place', 'product_category', 'Sen for column in categorical_columns: print(\"\\n\" + column) print(df[column].value_counts()) This code will print the unique values and their frequencies fo categorical columns. Run this code and let me know what you fi\n\n[End of response]\n\nLet’s try this and see if it works. Run the below code in a code cell to\n\nanalyze the categorical variables:\n\n# Analyze categorical variables categorical_cols = ['market_place', 'product_category', 'Sentim for col in categorical_cols: unique_values = df[col].unique() value_counts = df[col].value_counts() print(f\"\\n{col} unique values:\\n{unique_values}\\n\") print(f\"{col} value counts:\\n{value_counts}\")\n\nYou should see the following output:\n\nmarket_place unique values: ['\"US\"'] market_place value counts:",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "market_place \"US\" 100 Name: count, dtype: int64 product_category unique values: ['\"Books\"'] product_category value counts: product_category \"Books\" 100 Name: count, dtype: int64 Sentiment_books unique values: ['positive' 'negative'] Sentiment_books value counts: Sentiment_books positive 84 negative 16 Name: count, dtype: int64\n\nStep 5: Distribution of ratings Next, we want to understand the distribution of ratings. To learn more about\n\nthe ratings, we will provide the output from the last code execution as a\n\nprompt:\n\n[Prompt]\n\nBelow is the output:\n\nmarket_place unique values: [‘“US”’]\n\nmarket_place value counts: market_place “US” 100 Name: count,\n\ndtype: int64\n\nproduct_category unique values: [‘“Books”’]\n\nproduct_category value counts: product_category “Books” 100\n\nName: count, dtype: int64",
      "content_length": 802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 639,
      "content": "Sentiment_books unique values: [‘positive’ ‘negative’]\n\nSentiment_books value counts: Sentiment_books positive 84 negative 16 Name: count, dtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now that we’ve explored the categorical variables, let’s move on to\n\nstep 5: visualizing the distribution of star ratings.\n\n# Step 5: Distribution of Ratings import matplotlib.pyplot as plt # Plot a histogram for star ratings plt.hist(df['star_rating'], bins=5, edgecolor='black') plt.title('Distribution of Star Ratings') plt.xlabel('Star Ratings') plt.ylabel('Frequency') plt.show()\n\nThis code will plot a histogram of the star ratings, which will help us\n\nunderstand the overall sentiment of the reviews. Run this code and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try this and see if it works. Below, we have code for performing\n\nvisualization; run this code in a code cell:\n\nimport matplotlib.pyplot as plt # Plot the distribution of star ratings plt.hist(df['star_rating'], bins=5, edgecolor='black') plt.xlabel('Star Rating') plt.ylabel('Frequency')",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "plt.title('Distribution of Star Ratings') plt.show()\n\nFigure 17.3: Distribution of star ratings\n\nStep 6: Temporal analysis At this point, we have a good idea of how ratings are distributed; it seems\n\nlike they are either very low or very high. Let’s progress to the next step.\n\n[Prompt]\n\nProceed to the next step.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "[Prompt response]\n\nGreat! Now let’s move on to step 6: Temporal Analysis. We’ll analyze the review_date column to explore trends, seasonality, or any patterns over\n\ntime.\n\nFirst, we need to convert the review_date column to a datetime format.\n\nThen, we can extract the year, month, and day into separate columns for\n\neasier analysis.\n\n# Step 6: Temporal Analysis # Convert review_date to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Extract year, month, and day into separate columns df['year'] = df['review_date'].dt.year df['month'] = df['review_date'].dt.month df['day'] = df['review_date'].dt.day # Check the number of reviews per year print(df['year'].value_counts().sort_index()) # Check the number of reviews per month print(df['month'].value_counts().sort_index()) # Check the number of reviews per day print(df['day'].value_counts().sort_index())\n\nThis code will print the number of reviews per year, month, and day. Run\n\nthis code and let me know what you find.\n\n[End of response]\n\nLet’s try this code and see if it works. Run the above code in a code cell:\n\n# Step 6: Temporal Analysis # Convert review_date to datetime format df['review_date'] = pd.to_datetime(df['review_date']) # Extract year, month, and day into separate columns df['year'] = df['review_date'].dt.year df['month'] = df['review_date'].dt.month",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "df['day'] = df['review_date'].dt.day # Check the number of reviews per year print(df['year'].value_counts().sort_index()) # Check the number of reviews per month print(df['month'].value_counts().sort_index()) # Check the number of reviews per day print(df['day'].value_counts().sort_index())\n\nYou should see an output similar to the below:\n\nyear 2015 100 Name: count, dtype: int64 month 8 100 Name: count, dtype: int64 day 31 100 Name: count, dtype: int64\n\nStep 7: Review length analysis Let’s move forward in our analysis; next up is a review length analysis.\n\n[Prompt]\n\nBelow is the output: year 2015 100 Name: count, dtype: int64 month 8 100 Name: count, dtype: int64 day 31 100 Name: count, dtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "Great! Now that we’ve analyzed the temporal aspect of the data, let’s move\n\non to step 7: Review Length Analysis. We’ll analyze the length of review_body to understand the amount of information provided in the\n\nreviews.\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nThis code will calculate the length of each review and provide descriptive\n\nstatistics like mean, median, and maximum length. Run this code and let me know what you find.\n\n[End of response]\n\nAt this point, we want to understand what the above code is doing; let’s ask\n\nCopilot for help:\n\n[Prompt]\n\nBefore we move on to step 7, can you please explain how to interpret the\n\noutput?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAbsolutely. The output you’ve shared indicates the frequency of reviews\n\nbased on the year, month, and day.\n\nyear: All 100 reviews in your dataset were written in the year 2015.",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "month: All 100 reviews were written in the 8th month, which is\n\nAugust.\n\nday: All 100 reviews were written on the 31st day of the month.\n\nThis suggests that all the reviews in your dataset were written on the same\n\ndate - August 31, 2015.\n\nIn a larger, more diverse dataset, you might see reviews spread across\n\ndifferent years, months, and days. Analyzing these could help identify\n\ntrends or patterns over time, such as an increase in reviews during certain\n\nperiods.\n\n[End of response]\n\nOK, we have a problem – our dataset is limited, and all reviews seem to\n\nhave taken place at the same point in time, which prevents us from\n\ndiscovering trends. Let’s inform Copilot of that and see what we can do\n\nnext:\n\n[Prompt]\n\nIn that case, we cannot explore trends or patterns within this dataset.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThat’s correct. Since all the reviews in this dataset are from the same date,\n\nwe won’t be able to analyze trends or patterns over time. However, you can\n\nstill perform other types of analysis. For instance, you can analyze the\n\nlength of the reviews, the distribution of star ratings, or the frequency of\n\ncertain words or phrases in the review text.",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "If you have a larger, more diverse dataset, you could also analyze trends\n\nover time, compare different products or categories, or use machine\n\nlearning to predict star ratings based on the review text.\n\nLet’s proceed with the review length analysis as planned:\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nThis code will calculate the length of each review and provide descriptive\n\nstatistics like mean, median, and maximum length. Run this code and let me know what you find.\n\n[End of response]\n\nLet’s try this code and see if it works. Type this code into a code cell and\n\nrun it:\n\n# Step 7: Review Length Analysis # Calculate the length of each review df['review_length'] = df['review_body'].str.len() # Calculate descriptive statistics for review length print(df['review_length'].describe())\n\nYou should see output similar to the below:\n\ncount 100.000000 mean 253.190000 std 420.934403 min 6.000000 25% 36.000000 50% 104.500000",
      "content_length": 1101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "75% 269.750000 max 2685.000000 Name: review_length, dtype: float64\n\n[Prompt]\n\nBelow is the output:\n\ncount 100.000000 mean 253.190000 std 420.934403 min 6.000000 25%\n\n36.000000 50% 104.500000 75% 269.750000 max 2685.000000 Name:\n\nreview_length, dtype: float64\n\n[End of prompt]\n\nYou should receive a response similar to the response below.\n\n[Prompt response]\n\nGreat! The output you’ve shared provides a summary of the review lengths\n\nin your dataset:\n\ncount: There are 100 reviews in your dataset.\n\nmean: On average, a review is about 253 characters long.\n\nstd: The standard deviation is approximately 421, indicating that the\n\nlengths of the reviews vary quite a bit around the mean.\n\nmin: The shortest review is 6 characters long.\n\n25%: 25% of the reviews are 36 characters long or shorter.\n\n50% (or median): Half of the reviews are 104.5 characters long or shorter.\n\n75%: 75% of the reviews are 269.75 characters long or shorter.\n\nmax: The longest review is 2685 characters long.\n\nThis information can be useful to understand the amount of text available\n\nfor any text analysis or natural language processing tasks.",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "For the next step, you could consider visualizing this data or analyzing the\n\ntext content of the reviews. For example, you could create a histogram of\n\nreview lengths, or you could use a word cloud to visualize the most\n\ncommon words in the reviews. Let me know what you’d like to do next.\n\n[End of response]\n\nStep 8: Correlation analysis Now, we want to determine the relationship between two or more variables\n\nto understand what variables are related. The below prompt shows how we\n\nprogress in our analysis:\n\n[Prompt]\n\nProceed to the next step for correlation analysis.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s move on to step 8: Correlation Analysis. We’ll analyze the correlation between numeric variables like star_rating, helpful_votes,\n\nand total_votes.\n\n# Step 8: Correlation Analysis # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Print the correlation matrix print(correlation_matrix)",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "This code will calculate and print a correlation matrix, which shows the\n\ncorrelation coefficients between pairs of variables. The correlation coefficient ranges from -1 to 1. A value close to 1 implies a high positive\n\ncorrelation while a value close to -1 implies a high negative correlation. A\n\nvalue close to 0 implies no correlation. Run this code and let me know what\n\nyou find.\n\n[End of response]\n\nLet’s try this code and see if it works. Add the correlation code to a code\n\ncell in your Notebook and run it:\n\n# Step 8: Correlation Analysis # Calculate the correlation matrix correlation_matrix = df[['star_rating', 'helpful_votes', 'total # Print the correlation matrix print(correlation_matrix)\n\nYou should see an output similar to the below:\n\nstar_rating helpful_votes total_votes star_rating 1.000000 -0.324371 -0.517002 helpful_votes -0.324371 1.000000 0.659612 total_votes -0.517002 0.659612 1.000000\n\nNext, let’s send the above output to Copilot and see if we can get an\n\nexplanation of the correlations:\n\n[Prompt]\n\nBelow is the output: star_rating helpful_votes total_votes star_rating\n\n1.000000 -0.324371 -0.517002 helpful_votes -0.324371 1.000000 0.659612\n\ntotal_votes -0.517002 0.659612 1.000000",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared is a correlation matrix, which shows the\n\ncorrelation coefficients between pairs of variables. Here’s how to interpret\n\nit:\n\nstar_rating and helpful_votes: The correlation coefficient is\n\n0.324371, indicating a weak negative correlation. This suggests that\n\nas the star rating increases, the number of helpful votes tends to\n\ndecrease slightly, and vice versa.\n\nstar_rating and total_votes: The correlation coefficient is -0.517002, indicating a moderate negative correlation. This suggests\n\nthat as the star rating increases, the total number of votes (both helpful\n\nand not helpful) tends to decrease, and vice versa.\n\nhelpful_votes and total_votes: The correlation coefficient is\n\n0.659612, indicating a moderate positive correlation. This suggests that as the number of helpful votes increases, the total number of votes\n\nalso tends to increase, and vice versa.\n\nFor the next step, you could consider visualizing these correlations using a\n\nheatmap, or you could move on to another type of analysis. Let me know\n\nwhat you’d like to do next.\n\n[End of response]\n\nNext, we want to look into visualizations of these correlations; a heatmap is\n\none way to do this.\n\n[Prompt]",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "For Step 8.1, visualize these correlations using a heatmap.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s visualize the correlation matrix using a heatmap. We’ll use the\n\nseaborn library for this.\n\n# Step 8.1: Visualize Correlation Matrix with Heatmap import seaborn as sns # Plot a heatmap of the correlation matrix plt.figure(figsize=(8, 6)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ce plt.title('Correlation Matrix Heatmap') plt.show()\n\nThis code will create a heatmap of the correlation matrix. The colors\n\nrepresent the correlation coefficients: warm colors (towards red) indicate\n\npositive correlation, cool colors (towards blue) indicate negative\n\ncorrelation, and neutral colors (around white) indicate no correlation. The annot=True option will display the correlation coefficients on the heatmap. Run this code and let me know what you find.\n\n[End of response]\n\nLet’s try this code to create a heatmap visualization using the seaborn library and see if it works. Run the code in a code cell in your Notebook:\n\n# Step 8.1: Visualize Correlation Matrix with Heatmap import seaborn as sns # Plot a heatmap of the correlation matrix plt.figure(figsize=(8, 6))",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ce plt.title('Correlation Matrix Heatmap') plt.show()\n\nYou should see output similar to the below:\n\nFigure 17.4: Heatmap showing the correlation between three variables: total_votes, helpful_votes,\n\nand star_rating\n\nStep 9: Additional exploratory analysis So, why do we want to perform an additional exploratory analysis? Well,\n\nthere are some good reasons why we would want to do that:",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 652,
      "content": "Uncovering hidden user patterns: Sometimes, there are trends,\n\npatterns, and relationships not immediately discoverable, and taking a\n\nsecond deeper look can find these.\n\nValidate initial findings: By validating the results from the initial\n\nanalysis with a second analysis, you ensure robustness and reliability.\n\nIdentify outliers: Not effectively finding outliers can skew the results.\n\nSpending more time on finding these can pay off.\n\nImprove model accuracy: Additional variables can be identified that\n\ncan improve accuracy.\n\nAbove is a non-exhaustive list of reasons why we should do an additional\n\nanalysis. Let’s see how Copilot can help us:\n\n[Prompt]\n\nThank you, I wish to understand if there is any more exploratory analysis\n\nfor the sentiment analysis of this dataset.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nCertainly, sentiment analysis often involves exploring the text data in more\n\ndepth. Here are a few additional steps you could consider:\n\nStep 10: Text Preprocessing Before analyzing the text, it’s often helpful to clean it up a bit. This can\n\ninvolve converting all text to lower case, removing punctuation, removing\n\nstop words (common words like “and”, “the”, “a”, which don’t add much",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "meaning), and stemming or lemmatizing (reducing words to their root\n\nform).\n\nfrom nltk.corpus import stopwords from nltk.stem import PorterStemmer import re # Initialize a PorterStemmer object to perform stemming stemmer = PorterStemmer() # Define a function to preprocess the text def preprocess_text(text): # Convert to lower case text = text.lower() # Remove punctuation text = re.sub(r'[^\\w\\s]', '', text) # Remove stop words and stem the words text = ' '.join([stemmer.stem(word) for word in text.split( return text # Apply the function to the review_body column df['review_body_clean'] = df['review_body'].apply(preprocess_te\n\nStep 11: Word Frequency Analysis After preprocessing the text, you can analyze the frequency of words in the\n\nreviews. This can help identify common themes or topics.\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Step 12: Sentiment Score Calculation You can calculate a sentiment score for each review using a sentiment\n\nanalysis tool like TextBlob or Vader. This can provide a more quantitative\n\nmeasure of the sentiment expressed in the reviews.\n\nfrom textblob import TextBlob # Define a function to calculate sentiment def calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nPlease note that you’ll need to install the nltk and textblob libraries to run\n\nthis code, which you can do using pip:\n\npip install nltk textblob\n\nLet me know if you’d like to proceed with any of these steps, or if there’s\n\nsomething else you’d like to do.\n\n[End of response]\n\nThis prompt yielded three steps as a response. Let’s try the code from all\n\nthree steps and see if it works:\n\nText preprocessing",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "# uncomment if the nltk and textblob are not installed # !pip install nltk textblob from nltk.corpus import stopwords from nltk.stem import PorterStemmer import re # Initialize a PorterStemmer object to perform stemming stemmer = PorterStemmer() # Define a function to preprocess the text def preprocess_text(text): # Convert to lower case text = text.lower() # Remove punctuation text = re.sub(r'[^\\w\\s]', '', text) # Remove stop words and stem the words text = ' '.join([stemmer.stem(word) for word in text.split( return text # Apply the function to the review_body column df['review_body_clean'] = df['review_body'].apply(preprocess_te\n\nWord frequency analysis\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))\n\nYou should see an output similar to the below:\n\n[('book', 80), ('read', 25), ('love', 24), ('great', 24), ('sto\n\nNext, use the output above and construct the next prompt to understand our\n\nsituation better:",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "[Prompt]\n\nFor the code below:\n\nfrom collections import Counter # Count the frequency of words in the reviews word_counts = Counter(' '.join(df['review_body_clean']).split() # Print the 10 most common words print(word_counts.most_common(10))\n\nBelow is the output: [(‘book’, 80), (‘read’, 25), (‘love’, 24), (‘great’, 24),\n\n(‘stori’, 19), (‘good’, 17), (‘page’, 16), (‘one’, 15), (‘like’, 14), (‘realli’, 14)]\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared shows the 10 most common words in the\n\ncleaned reviews, along with their frequencies:\n\n'book': Appears 80 times 'read': Appears 25 times 'love': Appears 24 times 'great': Appears 24 times 'stori': Appears 19 times 'good': Appears 17 times 'page': Appears 16 times 'one': Appears 15 times 'like': Appears 14 times 'realli': Appears 14 times\n\nThis suggests that most of the reviews are positive, with words like ‘love’,\n\n‘great’, and ‘good’ being quite common. The words ‘book’, ‘read’, and\n\n‘page’ are also common, which is expected in a dataset of book reviews.",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "For the next step, you could consider calculating a sentiment score for each\n\nreview to get a more quantitative measure of the sentiment. Alternatively,\n\nyou could visualize the word frequencies using a bar chart or a word cloud.\n\nLet me know what you’d like to do next.\n\n[End of response]\n\nLet’s try to run the sentiment score calculation code below:\n\nSentiment score calculation\n\nfrom textblob import TextBlob # Define a function to calculate sentiment def calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nHere’s the output:\n\n0.23785470981747298\n\nNext, let’s try to interpret the response. As usual, let’s create a prompt from\n\nthe response and see what Copilot thinks of it:\n\n[Prompt]\n\nFor the code below:\n\nfrom textblob import TextBlob # Define a function to calculate sentiment",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "def calculate_sentiment(text): return TextBlob(text).sentiment.polarity # Apply the function to the review_body_clean column df['sentiment_score'] = df['review_body_clean'].apply(calculate # Print the average sentiment score print(df['sentiment_score'].mean())\n\nHere is the output: 0.23785470981747298\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output you’ve shared is the average sentiment score for the\n\nreviews in your dataset.\n\nThe sentiment score is a number between -1 and 1. A score of -1 represents\n\nextremely negative sentiment, a score of 1 represents extremely positive\n\nsentiment, and a score of 0 represents neutral sentiment.\n\nThe average sentiment score in your dataset is approximately 0.238, which\n\nindicates a generally positive sentiment in the reviews. This aligns with the\n\nword frequency analysis, which showed that positive words like ‘love’,\n\n‘great’, and ‘good’ are quite common in the reviews.\n\nFor the next step, you could consider visualizing the distribution of\n\nsentiment scores using a histogram or a box plot. Alternatively, you could\n\nanalyze the relationship between sentiment score and other variables like\n\nstar rating or review length. Let me know what you’d like to do next.\n\n[End of response]\n\n[Prompt]",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "Follow these instructions step by step:\n\nVisualize the distribution of sentiment scores using a histogram.\n\nAnalyze the relationship between sentiment score and other variables\n\nlike star rating or review length.\n\n[End of prompt]\n\nYou should receive a response similar to the response below.\n\n[Prompt response]\n\nSure, let’s proceed with those steps.\n\nStep 13: Visualize the Distribution of Sentiment Scores We can use a histogram to visualize the distribution of sentiment scores.\n\nimport matplotlib.pyplot as plt # Plot a histogram of the sentiment scores plt.figure(figsize=(8, 6)) plt.hist(df['sentiment_score'], bins=20, color='skyblue') plt.title('Distribution of Sentiment Scores') plt.xlabel('Sentiment Score') plt.ylabel('Frequency') plt.show()\n\nStep 14: Analyze the Relationship Between",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "Sentiment Score and Other Variables We can use scatter plots to visualize the relationship between sentiment\n\nscore and other variables like star rating and review length.\n\n# Plot a scatter plot of sentiment score vs star rating plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['star_rating'], alpha=0.5 plt.title('Sentiment Score vs Star Rating') plt.xlabel('Sentiment Score') plt.ylabel('Star Rating') plt.show() # Plot a scatter plot of sentiment score vs review length plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['review_length'], alpha=0 plt.title('Sentiment Score vs Review Length') plt.xlabel('Sentiment Score') plt.ylabel('Review Length') plt.show()\n\nThese plots will help you understand if there’s any correlation between\n\nsentiment score and star rating or review length. Run these codes and let me\n\nknow what you find.\n\n[End of response]\n\nLet’s try the above code and see if it works. Add the below code to a code\n\ncell and run it:\n\nVisualize the distribution of sentiment scores",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "import matplotlib.pyplot as plt # Plot a histogram of the sentiment scores plt.figure(figsize=(8, 6)) plt.hist(df['sentiment_score'], bins=20, color='skyblue') plt.title('Distribution of Sentiment Scores') plt.xlabel('Sentiment Score') plt.ylabel('Frequency') plt.show()\n\nFigure 17.5: Image depicting the distribution of sentiment scores\n\nAnalyze the relationship between sentiment score and other variables",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "# Plot a scatter plot of sentiment score vs star rating plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['star_rating'], alpha=0.5 plt.title('Sentiment Score vs Star Rating') plt.xlabel('Sentiment Score') plt.ylabel('Star Rating') plt.show() # Plot a scatter plot of sentiment score vs review length plt.figure(figsize=(8, 6)) plt.scatter(df['sentiment_score'], df['review_length'], alpha=0 plt.title('Sentiment Score vs Review Length') plt.xlabel('Sentiment Score') plt.ylabel('Review Length') plt.show()\n\nFigure 17.6: Sentiment score versus star rating",
      "content_length": 570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 663,
      "content": "Figure 17.7: Sentiment score versus review length\n\nAssignment Your assignment is to reflect on the approach taken and go through a similar\n\nregression scenario for this dataset on house prices in Boston:\n\nhttps://www.kaggle.com/datasets/vikrishnan/boston- house-prices.\n\nHere are some questions you could try answering using the above dataset\n\nand regression:\n\nPrice Prediction: What is the estimated price of a house, given its\n\nfeatures (e.g., size, location, and number of bedrooms)?\n\nFeature Importance: Which features have the most significant impact on house prices?",
      "content_length": 572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "Price Trends: How do house prices change over time in a specific\n\narea?\n\nSolution The solution is in the repository:\n\nhttps://github.com/PacktPublishing/AI-Assisted-\n\nSoftware-Development-with-GitHub-Copilot-and-\n\nChatGPT\n\nSummary This chapter had one important purpose – to compare and contrast the\n\nexperience of using ChatGPT with GitHub Copilot and, in this case, its chat\n\nfunction. We used an approach that consisted of providing a lot of upfront\n\ninformation to Copilot, by describing the overall problem and the shape of\n\nthe dataset. We also provided instructions to let Copilot guide us on what to\n\ndo, which showed us the steps to take gradually and what code to run. The\n\ngeneral conclusion is that we can use roughly the same method using\n\nCopilot Chat as we did with ChatGPT.\n\nWe also saw how Copilot can help explain our output, understand where in\n\nthe process we are, and suggest the next step to take.\n\nAs a rule, we should always test code and ask our AI assistant to help if it\n\ndoesn’t run or produce the expected output.\n\nJoin our community on Discord",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 666,
      "content": "18\n\nRegression with Copilot Chat\n\nIntroduction The stock of a corporation signifies ownership in the corporation. A single\n\nshare of the stock represents a claim on the fractional assets and the\n\nearnings of the corporation in proportion to the total number of shares. For\n\nexample, if an investor owns 50 shares of stock in a company that has, in\n\ntotal, 1,000 shares, then that investor (or shareholder) would own and have\n\na claim on 5% of the company’s assets and earnings.\n\nThe stocks of a company can be traded between shareholders and other\n\nparties via stock exchanges and organizations. Major stock exchanges\n\ninclude the New York Stock Exchange, NASDAQ, the London Stock\n\nExchange Group, the Shanghai Stock Exchange, and the Hong Kong Stock\n\nExchange. The prices that a stock is traded at fluctuate essentially due to the law of supply and demand. At any one moment, the supply is the number of\n\nshares that are in the hands of public investors, the demand is the number of\n\nshares investors want to buy, and the price of the stock moves up and down\n\nto attain and maintain equilibrium.\n\nInvestors generally want to buy low and sell high. This seems\n\nstraightforward enough but putting it into practice is exceedingly tough",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "because it is incredibly difficult to predict whether a stock price will\n\nincrease or decrease. Fundamental analysis and technical analysis are the\n\ntwo main research areas that aim to comprehend the reasons and\n\ncircumstances that influence price fluctuations or even forecast future stock prices:\n\nFundamental analysis: This stream focuses on the fundamental\n\nelements that affect a company’s value and operations, such as the\n\nmacroeconomic environment, the state of the industry, and the\n\ncompany’s management and competitors.\n\nTechnical analysis: On the other hand, this uses statistical analysis of historical trading activity, such as price movement, volume, and time,\n\nto forecast future price movements.\n\nIn this chapter, we will explore the second technique using the Apple\n\ndataset to predict stock prices and help investors decide whether to buy or not.\n\nRegression Let’s start by asking Copilot Chat for the definition of regression:\n\n[Prompt]\n\nWhat is regression?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nRegression is a statistical method used in data analysis which investigates\n\nthe relationship between a dependent (target) and independent variable(s)",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "(predictor). This method is mostly used for forecasting, time series\n\nmodeling, and finding the causal effect relationship between the variables. For example, the relationship between rainfall and soil erosion or the relationship between salary and years of experience.\n\n[End of response]\n\nYou may notice in your chatbox that GitHub Copilot is suggesting the next context, highlighted in blue, asking, “What are the different types of\n\nregression?”. You can click on this to get the answer. This demonstrates how Copilot is a powerful tool that assists users effectively.\n\nDataset overview This dataset contains Apple’s (AAPL) stock data spanning 10 years.\n\nInsights from this data can be used to build useful price forecasting algorithms to aid investment.\n\nWe can make the following notes about this dataset:\n\nData is collected and aggregated from 25 exchanges.\n\nData is updated weekly.\n\nThis dataset contains 1-minute, 5-minute, 30-minute, and 1-hour bars\n\n(open/high/low/close/volume) for AAPL.\n\nOut-of-hours trades are included. Prices are adjusted for splits and\n\ndividends. Dataset available at\n\nhttps://www.kaggle.com/datasets/tarunpaparaju/a pple-aapl-historical-stock-data.\n\nExplore the dataset",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "To better illustrate this regression approach, it is necessary to have suitable\n\npredictors for our target. The dataset recorded more direct elements to the stock. Five values illustrating movements in the price over one unit of time\n\n(usually one day, but it can also be one week or one month) are key trading indicators.\n\nThey are as follows:\n\nOpen: The starting price for a given trading day\n\nClose: The final price on that day\n\nHigh: The highest prices at which the stock traded on that day\n\nLow: The lowest prices at which the stock traded on that day\n\nVolume: The total number of shares traded before the market closed\n\non that day\n\nPrompt strategy In this chapter, we will use the TAG (Task-Action-Guidance) prompt pattern. Here’s how we can help frame our project to help us author our\n\ninitial prompt:\n\nTask: Regression analysis on Apple stock value through history\n\nAction: We will need to instruct the AI assistant to tell us what steps to take\n\nGuidance: The guidance should be us describing the dataset\n\nYour initial prompt Just like with other chapters in this book, we start with a prompt providing\n\nas much context as possible about what we want to do and the shape of the",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "dataset. The idea is that we set a high-level context for our AI tool, which helps guide it throughout the rest of our prompts.\n\n[Prompt]\n\nI’m performing a regression project to forecast stock prices using the Apple\n\nHistorical Stock dataset. This will aid investors in making informed\n\ndecisions about buying or not buying stocks. The workflow includes loading the dataset, exploring its contents, splitting the dataset, building a\n\nregression model, and evaluating its performance. But remember I want to\n\nlearn it step by step. Let’s start with loading the dataset. I am storing the dataset in <file path>. Here’s the shape of the dataset:\n\nOpen: The starting price for a given trading day\n\nClose: The final price on that day\n\nHigh: The highest prices at which the stock traded on that day\n\nLow: The lowest prices at which the stock traded on that day\n\nVolume: The total number of shares traded before the market closed\n\non that day\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s start by loading the dataset. We’ll use the pandas library in Python to do this. Replace <file-path> with the path where your file is located.\n\nHere’s the Python code to load the dataset:\n\nimport pandas as pd",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "# Replace <file-path> with the path of your dataset file_path = \"<file-path>\"\n\n# Load the dataset df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset print(df.head())\n\nAfter running this code, you should see the first few rows of your dataset printed out. This will give you a preliminary view of your data.\n\nFor the next steps, you might want to explore the dataset, clean the data if\n\nnecessary, split the dataset into a training set and a test set, build a\n\nregression model using the training set, and then evaluate the model’s performance using the test set.\n\n[End of response]\n\nLet’s try the output generated by Copilot and see if it works. Modify the file_path variable to point to the location of your dataset and then execute the code below in a code cell in a notebook:\n\nimport pandas as pd\n\n# Replace <file-path> with the path of your dataset file_path = \"/your-path/.csv\"\n\n# Load the dataset df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset print(df.head())\n\nHere’s the output you should see after running the code above:",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "Date Close/Last Volume Open High 0 02/28/2020 $273.36 106721200 $257.26 $278.41 $256 1 02/27/2020 $273.52 80151380 $281.1 $286 $272 2 02/26/2020 $292.65 49678430 $286.53 $297.88 $28 3 02/25/2020 $288.08 57668360 $300.95 $302.53 $286 4 02/24/2020 $298.18 55548830 $297.26 $304.18 $289\n\nNext, we will focus on historical prices (the five recorded values) and\n\nperformance to predict future prices. Let’s start by checking the data types.\n\n[Prompt]\n\nCheck the data types.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo check the data types of the columns in your DataFrame, you can use the dtypes attribute. Here’s how you can do it:\n\n# Check the data types of each column print(df.dtypes)\n\nThis will print out the name of each column along with its data type. This is useful to know because certain operations can only be performed on certain\n\ntypes of data. For example, you can’t perform numerical operations on a\n\nstring.\n\n[End of response]\n\nLet’s try this and see if it works. Run the code print(df.types):",
      "content_length": 1063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "# Check the data types of each column print(df.dtypes) Date object Close/Last object Volume int64 Open object High object Low object dtype: object\n\nNow, use the above output to query our AI tool.\n\n[Prompt]\n\nBelow is the output, Date object Close/Last object Volume int64 Open\n\nobject High object Low object dtype: object\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nIt appears that the ‘Date’ column and the price columns (‘Close/Last’,\n\n‘Open’, ‘High’, ‘Low’) are being read as ‘object’ type, which is essentially\n\na string in pandas. However, for your analysis, you would want ‘Date’ to be\n\nin a datetime format and the price columns to be of a float type.\n\nHere’s how you can convert the data types:\n\n# Convert 'Date' to datetime df['Date'] = pd.to_datetime(df['Date'])\n\n# Remove $ from price columns and convert to float price_columns = [' Close/Last', ' Open', ' High', ' Low'] for col in price_columns: df[col] = df[col].str.replace('$', '').astype(float)",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "# Check the data types again print(df.dtypes)\n\nThis code first converts the ‘Date’ column to datetime format using pd.to_datetime(). Then it loops over each price column, removes the\n\ndollar sign from each value, and converts the column to float type. Finally,\n\nit prints out the data types of the columns again to confirm the changes.\n\n[End of response]\n\nLet’s try these suggestions above to convert our columns by running the\n\nbelow code:\n\n# Convert 'Date' to datetime df['Date'] = pd.to_datetime(df['Date'])\n\n# Remove $ from price columns and convert to float price_columns = [' Close/Last', ' Open', ' High', ' Low'] for col in price_columns: df[col] = df[col].str.replace('$', '').astype(float)\n\n# Check the data types again print(df.dtypes)\n\nYou should see the following output, which is the columns correctly\n\nconverted to suitable types instead of being strings:\n\nDate datetime64[ns] Close/Last float64 Volume int64 Open float64 High float64 Low float64 dtype: object",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "Next, let’s tell Copilot about the result to ensure it tells us what to do next.\n\n[Prompt]\n\nBelow is the output, Date datetime64[ns] Close/Last float64 Volume int64\n\nOpen float64 High float64 Low float64 dtype: object\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! Now your ‘Date’ column is of type ‘datetime64[ns]’ and your price columns (‘Close/Last’, ‘Open’, ‘High’, ‘Low’) are of type ‘float64’. This\n\nmeans you can now perform numerical operations on your price columns\n\nand time series analysis on your ‘Date’ column.\n\nThe next steps could be to perform exploratory data analysis, such as checking for missing values, visualizing the distribution of your variables,\n\nor examining the correlation between different variables. You might also\n\nwant to split your data into a training set and a test set in preparation for building your regression model.\n\n[End of response]\n\nExploratory data analysis Guided by the output on what to do next, we decide to check the missing\n\nvalues in our dataset:\n\n[Prompt]\n\nCheck missing values.\n\n[End of response]",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "You should receive a response similar to the response below:\n\n[Prompt response]\n\nTo check for missing values in your DataFrame, you can use the isnull()\n\nfunction along with the sum() function in pandas. Here’s how you can do it:\n\n# Check for missing values print(df.isnull().sum())\n\nThis will print out the number of missing values in each column of your DataFrame. If a column has zero missing values, that means there are no missing values in that column. If a column has a number greater than zero,\n\nthat’s the number of missing values in that column.\n\n[End of response]\n\nThe response from Copilot says to check for null values and provides code\n\nfor how to do so. Let’s run the suggested code in a code cell in a Notebook:\n\n# Check for missing values print(df.isnull().sum())\n\nHere’s the output:\n\nDate 0 Close/Last 0 Volume 0 Open 0 High 0 Low 0 dtype: int64\n\nWith this output, provide it to Copilot to see what to do next:",
      "content_length": 928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "[Prompt]\n\nBelow is the output, Date 0 Close/Last 0 Volume 0 Open 0 High 0 Low 0\n\ndtype: int64\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output indicates that there are no missing values in your dataset.\n\nEach column has 0 missing values. This is good news because it means you\n\ndon’t have to handle missing data, which can be a complex process.\n\nThe next step in your analysis could be to perform some exploratory data\n\nanalysis (EDA). This might include visualizing your data, checking for\n\noutliers, or examining the relationships between variables. After that, you\n\ncan proceed to split your data into a training set and a test set, and then\n\nbuild your regression model.\n\n[End of response]\n\nGreat, we had no missing values and it seems like the next steps should be visual analysis and checking for outliers. With that knowledge, let’s craft an\n\nappropriate prompt.\n\n[Prompt]\n\nVisualize data and check for outliers.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "Visualizing the data can be done using various libraries in Python, such as\n\nMatplotlib or Seaborn. For checking outliers, boxplots can be very useful.\n\nHere’s how you can create boxplots for your price columns:\n\nimport matplotlib.pyplot as plt import seaborn as sns\n\n# Set up the matplotlib figure f, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot a simple histogram with binsize determined automatically sns.boxplot(data=df[' Close/Last'], ax=axes[0, 0]) axes[0, 0].set_title('Close/Last')\n\nsns.boxplot(data=df[' Open'], ax=axes[0, 1]) axes[0, 1].set_title('Open')\n\nsns.boxplot(data=df[' High'], ax=axes[1, 0]) axes[1, 0].set_title('High')\n\nsns.boxplot(data=df[' Low'], ax=axes[1, 1]) axes[1, 1].set_title('Low')\n\nplt.tight_layout()\n\nThis code will create a 2x2 grid of plots, with each plot being a boxplot of\n\none of your price columns. Boxplots are useful for visualizing the\n\ndistribution of data and identifying potential outliers. In a boxplot, an\n\noutlier is typically represented as a point that is located outside the whiskers\n\nof the boxplot.\n\n[End of response]\n\nThe code above suggests using the matplotlib and seaborn libraries and also\n\ncontains code to create a boxplot visualization; let’s run said code:",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "import matplotlib.pyplot as plt import seaborn as sns\n\n# Set up the matplotlib figure f, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot a simple histogram with binsize determined automatically sns.boxplot(data=df[' Close/Last'], ax=axes[0, 0]) axes[0, 0].set_title('Close/Last')\n\nsns.boxplot(data=df[' Open'], ax=axes[0, 1]) axes[0, 1].set_title('Open')\n\nsns.boxplot(data=df[' High'], ax=axes[1, 0]) axes[1, 0].set_title('High')\n\nsns.boxplot(data=df[' Low'], ax=axes[1, 1]) axes[1, 1].set_title('Low')\n\nplt.tight_layout()\n\nYou should see results similar to the following boxplots:",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 680,
      "content": "Figure 18.1: Boxplots of our dataset\n\nNext, we want to learn more about our variables and their relationship.\n\n[Prompt]\n\nExamine the relationships between variables.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo examine the relationships between variables, you can use a correlation\n\nmatrix and a heatmap. The correlation matrix will give you numerical values that represent how closely related two variables are, while the\n\nheatmap will give you a visual representation of these relationships.\n\nHere’s how you can do it:",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "# Calculate the correlation matrix corr = df.corr()\n\n# Plot the heatmap sns.heatmap(corr, annot=True, cmap='coolwarm')\n\nplt.show()\n\nThis code first calculates the correlation matrix using the corr() function, which returns a DataFrame where each cell represents the correlation\n\ncoefficient between two variables. Then it plots a heatmap using this correlation matrix. The annot=True argument allows the correlation coefficients to be plotted on the heatmap, and the cmap='coolwarm'\n\nargument sets the color scheme of the heatmap.\n\nThe correlation coefficient ranges from -1 to 1. A value close to 1 implies a\n\nhigh positive correlation while a value close to -1 implies a high negative correlation. A value close to 0 implies no correlation.\n\n[End of response]\n\nThe response says we should create a correlation matrix and visualize it via\n\na heatmap. Let’s try the suggested code above and see what we get:\n\n# Calculate the correlation matrix corr = df.corr()\n\n# Plot the heatmap sns.heatmap(corr, annot=True, cmap='coolwarm')\n\nplt.show()\n\nHere’s the output from the code above:",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "Figure 18.2: Heatmap\n\nIn the heatmap, we can see:\n\nThe diagonal is always 1 because it shows the correlation of each\n\nvariable with itself.\n\nClose/Last, Open, High, and Low have a high positive correlation with\n\neach other (0.92), indicating that as one of these variables increases,\n\nthe others tend to also increase. This is typical for stock data, as these\n\nprices move closely together throughout the trading day.\n\nVolume shows a negative correlation with Close/Last, Open, High, and\n\nLow (-0.61). This suggests that on days when the volume of shares\n\ntraded is higher, the prices may tend to be lower, or vice versa.\n\nAs shown in the screenshot below, Copilot recommends the next step\n\nfollowing the analysis of variable relationships. We will choose this and see\n\nwhat it says.",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "Let’s ask Copilot what to do next.\n\n[Prompt]\n\nWhat is the next step after examining the relationships between variables?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThe next step after examining the relationships between variables is\n\ntypically to prepare your data for modeling. This can involve several sub-\n\nsteps:\n\nFeature Selection: Based on your correlation analysis, you might decide to drop some features that are not correlated with your target\n\nvariable, or to avoid multicollinearity, drop one of two features that are\n\nhighly correlated with each other.\n\nData Splitting: You’ll need to split your data into a training set and a\n\ntest set. A common split is 80% of the data for training and 20% for\n\ntesting.\n\nData Scaling: Depending on the model you plan to use, you might need to scale your data so that all features have a similar range of\n\nvalues.\n\nOnce your data is prepared, you can proceed to build your regression\n\nmodel.\n\n[End of response]\n\nData splitting",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "At this point, let’s prepare for the creation of the model. Before we can\n\ncreate the model, we need to split up our data into training data and test sets\n\nto verify the model later.\n\n[Prompt]\n\nLet’s move on to the step of splitting the data.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAbsolutely, splitting the data into a training set and a test set is a crucial\n\nstep in preparing your data for modeling. We’ll use the train_test_split\n\nfunction from sklearn.model_selection to do this.\n\nAssuming that we are going to predict the ‘Close/Last’ price, here’s how\n\nyou can split your data:\n\nfrom sklearn.model_selection import train_test_split\n\n# Define the feature variables and the target variable X = df.drop(' Close/Last', axis=1) y = df[' Close/Last']\n\n# Split the data into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\nThis code first defines the feature variables X and the target variable y. Then it splits these into a training set and a test set, with 80% of the data going to the training set and 20% going to the test set. The random_state\n\nparameter is used for reproducibility - using the same random_state will ensure that you get the same train/test split every time you run the code.",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "[End of response]\n\nIn short, the code above shows us how to split up our dataset; let’s try this\n\ncode next:\n\nfrom sklearn.model_selection import train_test_split\n\n# Define the feature variables and the target variable X = df.drop(' Close/Last', axis=1) y = df[' Close/Last']\n\n# Split the data into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\nAt this point, we don’t have an output to show as we assigned the results to\n\nvariables. Let’s, however, print that next.\n\n[Prompt]\n\nPrint the shape of samples.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nTo print the shape of your training and test samples, you can use the shape\n\nattribute. Here’s how you can do it:\n\n# Print the shape of the training set and the test set print(\"Training set (X):\", X_train.shape) print(\"Training set (y):\", y_train.shape) print(\"Test set (X):\", X_test.shape) print(\"Test set (y):\", y_test.shape)",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "This will print out the number of rows and columns in your training set and test set. The number of rows should be the same between X_train and\n\ny_train, and between X_test and y_test. The number of columns in X_train and X_test should be the same, and y_train and y_test should\n\neach have just one column (since they represent the target variable).\n\n[End of response]\n\nThe suggested code is straightforward, as it consists of a set of print\n\nstatements. Let’s try said code by running it in a code cell:\n\n# Print the shape of the training set and the test set print(\"Training set (X):\", X_train.shape) print(\"Training set (y):\", y_train.shape) print(\"Test set (X):\", X_test.shape) print(\"Test set (y):\", y_test.shape)\n\nYou should see output similar to the below:\n\nTraining set (X): (2014, 5) Training set (y): (2014,) Test set (X): (504, 5) Test set (y): (504,)\n\nBuild a regression model Now that we’ve split up the data, let’s train the model next, or at least that’s\n\nwhat we expect Copilot to say if we ask it for the next step. Let’s create a\n\nprompt asking just that:\n\n[Prompt]\n\nWhat is the next step after splitting the data?",
      "content_length": 1132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nAfter splitting the data, the next step is to create and train your regression\n\nmodel using the training data. Depending on the nature of your data and the\n\nproblem you’re trying to solve, you might choose a simple linear regression\n\nmodel, or a more complex model like polynomial regression or ridge regression.\n\nOnce the model is trained, you can use it to make predictions on the test\n\ndata. Then, you evaluate the performance of the model by comparing these\n\npredictions to the actual values in the test data. Common metrics for evaluating regression models include Mean Absolute Error (MAE), Mean\n\nSquared Error (MSE), and R-squared.\n\n[End of response]\n\nOkay, just as we thought, the next step is to train the model. Let’s create a\n\nprompt asking for more detailed information on how to train the model:\n\n[Prompt]\n\nCreate and train a regression model.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s create and train a simple Linear Regression model using the training data. We’ll use the LinearRegression class from\n\nsklearn.linear_model to do this.",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": "Here’s how you can create and train your model:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear Regression model model = LinearRegression()\n\n# Train the model model.fit(X_train, y_train)\n\nThis code first creates a Linear Regression model. Then it trains the model\n\nusing the fit method and the training data. After this step, your model is\n\nready to make predictions on unseen data.\n\nPlease note that if your data has a non-linear relationship, you might need to\n\nuse a more complex model like Polynomial Regression or a tree-based\n\nmodel. Also, if your features are not on the same scale, you might need to\n\nscale your data before training the model.\n\n[End of response]\n\nWe’ve received a response on using linear regression and the sklearn\n\nlibrary. Note that you might need to install said library before proceeding.\n\nOnce you’ve ensured sklearn is installed, proceed to run the suggested code below:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear Regression model model = LinearRegression()\n\n# Train the model model.fit(X_train, y_train)",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": "Here’s the output from running the code:\n\n--------------------------------------------------------------- DTypePromotionError Traceback (most recen\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b 1145 estimator._validate_params() 1147 with config_context( 1148 skip_parameter_validation=( 1149 prefer_skip_nested_validation or global_skip_va 1150 ) 1151 ): -> 1152 return fit_method(estimator, *args, **kwargs)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/l 674 n_jobs_ = self.n_jobs 676 accept_sparse = False if self.positive else [\"csr\", \"cs --> 678 X, y = self._validate_data( 679 X, y, accept_sparse=accept_sparse, y_numeric=True, 680 ) 682 has_sw = sample_weight is not None 683 if has_sw:\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b 620 y = check_array(y, input_name=\"y\", **check_y_pa 621 else: --> 622 X, y = check_X_y(X, y, **check_params) 623 out = X, y 625 if not no_val_X and check_params.get(\"ensure_2d\", True)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/u 1141 estimator_name = _check_estimator_name(estimato 1142 raise ValueError( 1143 f\"{estimator_name} requires y to be passed, but 1144 ) -> 1146 X = check_array( 1147 X, 1148 accept_sparse=accept_sparse, 1149 accept_large_sparse=accept_large_sparse, 1150 dtype=dtype, 1151 order=order,",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "1152 copy=copy, 1153 force_all_finite=force_all_finite, 1154 ensure_2d=ensure_2d, 1155 allow_nd=allow_nd, 1156 ensure_min_samples=ensure_min_samples, 1157 ensure_min_features=ensure_min_features, 1158 estimator=estimator, 1159 input_name=\"X\", 1160 ) 1162 y = _check_y(y, multi_output=multi_output, y_numeric=y_ 1164 check_consistent_length(X, y)\n\nFile ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/u 791 pandas_requires_conversion = any( 792 _pandas_dtype_needs_early_conversion(i) for i in dt 793 ) 794 if all(isinstance(dtype_iter, np.dtype) for dtype_iter --> 795 dtype_orig = np.result_type(*dtypes_orig) 796 elif pandas_requires_conversion and any(d == object for 797 # Force object if any of the dtypes is an object 798 dtype_orig = object\n\nDTypePromotionError: The DType <class 'numpy.dtypes.DateTime64D 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DTyp\n\nWe’re clearly getting an error back, so let’s see if Copilot can help with this\n\nerror. As usual, include context like the error text to get a good response.\n\n[Prompt]\n\nI am getting this error after running the code,\n\nCell 39 line 7 4 model = LinearRegression() 6 # Train the model File ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/b File ~/.conda/envs/myenv/lib/python3.12/site-packages/sklearn/l DTypePromotionError: The DType <class 'numpy.dtypes.DateTime64D",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThe error message suggests that the model is having trouble handling the\n\n‘Date’ column because it’s in datetime format, and the model expects\n\nnumerical input.\n\nOne common approach to deal with datetime data is to extract features from\n\nit, such as the year, month, day, etc. Here’s how you can do it:\n\n# Extract features from 'Date' X_train['Year'] = X_train['Date'].dt.year X_train['Month'] = X_train['Date'].dt.month X_train['Day'] = X_train['Date'].dt.day\n\n# Drop the original 'Date' column X_train = X_train.drop('Date', axis=1)\n\n# Do the same for the test set X_test['Year'] = X_test['Date'].dt.year X_test['Month'] = X_test['Date'].dt.month X_test['Day'] = X_test['Date'].dt.day X_test = X_test.drop('Date', axis=1)\n\n# Now try to fit the model again model.fit(X_train, y_train)\n\nThis code first extracts the year, month, and day from the ‘Date’ column\n\nand adds them as new columns in the DataFrame. Then it drops the original\n\n‘Date’ column. Finally, it tries to fit the model again.\n\nPlease note that this is a simple approach and might not be the best way to\n\nhandle datetime data for your specific problem. The best approach depends\n\non the nature of your data and the problem you’re trying to solve.",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "[End of response]\n\nSo, our Date column was the problem. The model training can only handle\n\nnumerical data. It seems the response suggested we should extract features from the Date column though, like year, month, and day, to preserve the info, but in numerical form. Additionally, we should then drop the Date\n\ncolumn as we got what we wanted from it.\n\nLet’s try running the suggested code:\n\n# Extract features from 'Date' X_train['Year'] = X_train['Date'].dt.year X_train['Month'] = X_train['Date'].dt.month X_train['Day'] = X_train['Date'].dt.day\n\n# Drop the original 'Date' column X_train = X_train.drop('Date', axis=1)\n\n# Do the same for the test set X_test['Year'] = X_test['Date'].dt.year X_test['Month'] = X_test['Date'].dt.month X_test['Day'] = X_test['Date'].dt.day X_test = X_test.drop('Date', axis=1)\n\n# Now try to fit the model again model.fit(X_train, y_train) LinearRegression()\n\nYou should now see your code running without any issues. Let’s tell\n\nCopilot about this fact and see if it suggests how we can proceed.\n\n[Prompt]\n\nThanks, that works.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "[Prompt response]\n\nYou’re welcome! I’m glad to hear that it worked. Now that your model is\n\ntrained, the next step is usually to use the model to make predictions on\n\nyour test data. After that, you can evaluate the performance of your model\n\nby comparing these predictions to the actual values in your test data. If you\n\nhave any more questions or need further assistance, feel free to ask.\n\n[End of response]\n\nEvaluate the model As suggested by the response, Copilot recommends asking the following:\n\nWhat are some common metrics for evaluating regression models? Let’s\n\nselect this and see what it says.\n\nEvaluation metrics Let’s ask Copilot about evaluation metrics.\n\n[Prompt]\n\nWhat are some common metrics for evaluating regression models?\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nThere are several common metrics for evaluating the performance of\n\nregression models:\n\nMean Absolute Error (MAE): This is the average of the absolute\n\ndifferences between the predicted and actual values. It gives an idea of",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": "how wrong the predictions were.\n\nMean Squared Error (MSE): This is the average of the squared\n\ndifferences between the predicted and actual values. Squaring the\n\ndifferences amplifies the impact of large errors.\n\nRoot Mean Squared Error (RMSE): This is the square root of the\n\nMSE. It has the same units as the output variable, making it easier to\n\ninterpret than the MSE.\n\nR-squared (R²): This is the proportion of the variance in the\n\ndependent variable that is predictable from the independent variables.\n\nIt provides a measure of how well the model’s predictions fit the actual\n\ndata. A value of 1 means the model perfectly predicts the actual values,\n\nwhile a value of 0 means the model does not predict the actual values\n\nat all.\n\nAdjusted R-squared: This is a modified version of R-squared that has\n\nbeen adjusted for the number of predictors in the model. It increases\n\nonly if the new term improves the model more than would be expected\n\nby chance.\n\nEach of these metrics has its own strengths and weaknesses, and the choice\n\nof which to use depends on the specific problem and the goals of the\n\nanalysis.\n\n[End of response]\n\nNow that we know how to evaluate and with what metrics, let’s make\n\npredictions next given our testing data, and see what we get in response.\n\n[Prompt]\n\nMake predictions and evaluate the model using different metrics.\n\n[End of prompt]",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "You should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, let’s use the trained model to make predictions on the test data. Then,\n\nwe’ll evaluate these predictions using Mean Absolute Error (MAE), Mean\n\nSquared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R²). We’ll use the metrics module from sklearn to calculate these metrics.\n\nHere’s how you can do it:\n\nfrom sklearn import metrics import numpy as np\n\n# Make predictions on the test data y_pred = model.predict(X_test)\n\n# Calculate MAE, MSE, RMSE, and R2 mae = metrics.mean_absolute_error(y_test, y_pred) mse = metrics.mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) # or metrics.mean_squared_error(y_test, y_p r2 = metrics.r2_score(y_test, y_pred)\n\n# Print the metrics print(\"Mean Absolute Error (MAE):\", mae) print(\"Mean Squared Error (MSE):\", mse) print(\"Root Mean Squared Error (RMSE):\", rmse) print(\"R-squared (R2 ):\", r2)\n\nThis code first uses the model to make predictions on the test data. Then it calculates the MAE, MSE, RMSE, and R² by comparing these predictions to the actual values in the test data. Finally, it prints out these metrics.\n\n[End of response]",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "The code contains both metrics and calls a predict() function that\n\nproduces a response from the model; let’s try this and see if it works.\n\nfrom sklearn import metrics import numpy as np\n\n# Make predictions on the test data y_pred = model.predict(X_test)\n\n# Calculate MAE, MSE, RMSE, and R2 mae = metrics.mean_absolute_error(y_test, y_pred) mse = metrics.mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) # or metrics.mean_squared_error(y_test, y_p r2 = metrics.r2_score(y_test, y_pred)\n\n# Print the metrics print(\"Mean Absolute Error (MAE):\", mae) print(\"Mean Squared Error (MSE):\", mse) print(\"Root Mean Squared Error (RMSE):\", rmse) print(\"R-squared (R2 ):\", r2)\n\nYou should see a response similar to the one below:\n\nMean Absolute Error (MAE): 0.458844690389933 Mean Squared Error (MSE): 0.4236006011305136 Root Mean Squared Error (RMSE): 0.6508460656180642 R-squared (R2 ): 0.999868004623502\n\nLet’s provide Copilot with the response below to learn what we should do\n\nnext.\n\n[Prompt]\n\nBelow is the output, Mean Absolute Error (MAE): 0.4588446903899",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nGreat! The output shows the calculated metrics for your regression model.\n\nMean Absolute Error (MAE): The average absolute difference\n\nbetween the predicted and actual values is approximately 0.46. This means that on average, your predictions are about 0.46 units away\n\nfrom the actual values.\n\nMean Squared Error (MSE): The average squared difference between the predicted and actual values is approximately 0.42. This\n\nmetric is more sensitive to large errors than MAE because it squares\n\nthe differences.\n\nRoot Mean Squared Error (RMSE): The square root of the MSE is approximately 0.65. This metric is in the same units as the output\n\nvariable, making it easier to interpret than the MSE.\n\nR-squared (R²): The proportion of the variance in the dependent variable that is predictable from the independent variables is\n\napproximately 0.9999. This is very close to 1, which indicates that\n\nyour model explains almost all of the variability of the response data\n\naround its mean.\n\nOverall, these metrics suggest that your model is performing very well on\n\nthe test data.\n\n[End of response]\n\nNext, we want to have a sense of how well the model is performing by\n\ncomparing predicted values to actual values, so let’s create a prompt to\n\nreflect that request.",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "[Prompt]\n\nDisplay a comparison of predicted values versus actual values.\n\n[End of prompt]\n\nYou should receive a response similar to the response below:\n\n[Prompt response]\n\nSure, you can compare the predicted values with the actual values by\n\nvisualizing them in a plot. Here’s how you can do it using matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Plot the actual values plt.scatter(X_test.index, y_test, color='blue', label='Actual')\n\n# Plot the predicted values plt.scatter(X_test.index, y_pred, color='red', label='Predicted\n\nplt.title('Actual vs Predicted') plt.xlabel('Index') plt.ylabel('Target') plt.legend() plt.show()\n\nThis code creates a scatter plot where the x-axis represents the index of the\n\ntest set and the y-axis represents the target variable. The actual values are\n\nplotted in blue and the predicted values are plotted in red. This allows you\n\nto visually compare the predictions made by the model to the actual values.\n\n[End of response]\n\nThe suggested code seems to want to create a scatter plot to better compare\n\nour predicted values compared to our test data:",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "import matplotlib.pyplot as plt\n\n# Plot the actual values plt.scatter(X_test.index, y_test, color='blue', label='Actual')\n\n# Plot the predicted values plt.scatter(X_test.index, y_pred, color='red', label='Predicted\n\nplt.title('Actual vs Predicted') plt.xlabel('Index') plt.ylabel('Target') plt.legend() plt.show()\n\nHere’s the output from running the above code:\n\nFigure 18.3: Actual versus predicted data",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "We can see how the predicted data matches the actual data quite well. Let’s\n\ntry to sum up our learning next.\n\nAssignment Instead of using regression to predict a value, let’s take the same data but\n\ntreat it as a classification problem and predict whether the stock price will\n\ngo up or down the next day.\n\nSummary Regression is a fundamental concept in machine learning used to predict a\n\ncontinuous outcome variable based on one or more predictor variables. It\n\ninvolves identifying the relationship between a dependent variable (often\n\ncalled the target) and one or more independent variables (features). We saw\n\nthat, given our dataset, we were able to find correlations for certain\n\nvariables. We also found that we could include columns like Date, but to\n\ninclude these, we needed to extract the important numerical parts from\n\nthose columns, namely the year, month, and date.\n\nRegression has many applications in other sectors, like healthcare and\n\nmarketing. From a prompt perspective, it’s a good idea to set the context\n\nearly on and show Copilot the shape of the data, which will then help you\n\nask Copilot what to do next.\n\nIn the next chapter, we will use the same dataset while using GitHub\n\nCopilot to help us write some code.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "Join our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "19\n\nRegression with Copilot Suggestions\n\nIntroduction In the previous chapter, we used GitHub Copilot Chat to build a regression\n\nproblem and explored how AI can assist in coding. In this chapter, we’ll\n\ntake a different approach. We will write code with the help of GitHub\n\nCopilot, allowing it to guide us through coding and adding helpful\n\ncomments. This will be an interactive experience, combining our coding\n\nskills with Copilot’s suggestions to effectively tackle a regression problem.\n\nLet’s see how GitHub Copilot can enhance our coding process in real time.\n\nIn the task, we will use the Apple dataset to predict stock prices and help\n\ninvestors decide whether to buy or not. This is the same dataset we used in\n\nChapter 18, Regression with Copilot Chat, where we used Copilot Chat to\n\nanalyze it.\n\nDataset overview This dataset provides us with a wealth of information about Apple’s stock\n\n(traded under AAPL) over the past decade, starting from the year 2010. This\n\ndata is incredibly valuable because it can help us develop forecasting",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "algorithms to predict the future price of Apple’s stock, which is crucial for\n\nmaking investment decisions. The data in this set has been collected and\n\naggregated from 25 different stock exchanges.\n\nTo effectively use this data for forecasting, we need to understand the key\n\nelements: the features that influence our target, which is predicting stock\n\nprices.\n\nThe dataset includes five important values that indicate how the stock price\n\nchanges over a specific period of time, which is typically one day, but it could also be one week or one month. These values are:\n\nOpen: This is the stock price at the beginning of the trading day.\n\nClose: This is the stock price at the end of the trading day.\n\nHigh: This value shows the highest price the stock reached during the\n\ntrading day.\n\nLow: This indicates the lowest price the stock hit during the trading\n\nday.\n\nVolume: This is the total number of shares that were traded throughout the day before the market closed.\n\nOur focus will be on using historical price data, which includes these five\n\nrecorded values, along with the stock’s past performance, to make\n\npredictions about its future prices.\n\nPrompt strategy In this chapter, we’ll use the “Exploratory prompt pattern” as described in\n\nChapter 2. We feel pretty confident on what actions we want to take in general but is interested in what the AI assistant will generate in case we need to pivot.",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "Start coding with Copilot’s help With GitHub Copilot installed, you’re ready to start coding your regression\n\nproblem in Python. The approach of using suggestions in an open file is a bit different from using the chat window. The main difference is that we\n\nwill rely on typing comments and use the Tab key to get Copilot to provide us with suggestions. This technique works equally well in Jupyter Notebook\n\nand normal text files. Here’s how it works:\n\n# generate import statements for libraries to use with a Regres\n\nHere, we have an open text file. We’ve added comments to generate import statements. To get Copilot to generate a response, we need to use the Return key followed by the Tab key to accept the suggestion. Here’s how the above\n\ntext file could change:\n\n# generate import statements for libraries to use with a Regres import pandas import numpy import scikit-learn\n\nThe import statement is what is generated by Copilot when pressing Tab\n\nfollowed by Return.\n\nThe AI assistant helps by providing code completion, but remember to review and understand each suggestion before\n\naccepting it.",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "Step 1: Import libraries with Copilot’s assistance We could start with a prompt suggestion comment that describes the\n\nproblem upfront, the dataset, and more, just like we did with Copilot Chat\n\nin other chapters in this book. Then, the prompt could look like so:\n\n# generate the steps I need to follow for performing a Regressi\n\nHowever, we will try an approach where we ask for everything we need.\n\nLet’s begin. Let’s create our first comment (this is equivalent to typing in a chat window):\n\n[Prompt]\n\n# import pandas\n\n[End of prompt]\n\nAccept the suggestion with Tab. You should see the following bold code:\n\n[Prompt response]\n\n# import pandas import pandas as pd\n\n[End of response]\n\nGreat! We have imported the libraries we need. Let’s load and explore the\n\ndataset next.",
      "content_length": 775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "Step 2: Load and explore the dataset In the next step, we want to load the dataset and explore the data features.\n\nKeep working in the same file and add the below prompt:\n\n[Prompt]\n\n# Load the dataset into a pandas DataFrame\n\n[End of prompt]\n\nAt this point, Copilot understands what we’re trying to do, so we don’t need\n\nto type more than the above prompt comment. So, we just need to press Return followed by Tab a number of times, and we should get the content\n\nbelow:\n\n[Prompt response]\n\n# Load the dataset into a pandas DataFrame df = pd.read_csv('enter-your-file-path')\n\n# Print the first 5 rows of the DataFrame print(df.head())\n\n[End of response]\n\nEnsure you have downloaded the dataset from Kaggle (https://www.kaggle.com/datasets/varpit9\n\n4/apple-stock-data-updated-till-",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "22jun2021) and that you have replaced the path in read_csv to point to where the file is in your system.\n\nIf you run the suggested code, you should see similar output to:\n\nDate Close Volume Open High Lo 0 02/28/2020 $273.36 106721200 $257.26 $278.41 $256.3 1 02/27/2020 $273.52 80151380 $281.1 $286 $272.9 2 02/26/2020 $292.65 49678430 $286.53 $297.88 $286. 3 02/25/2020 $288.08 57668360 $300.95 $302.53 $286.1 4 02/24/2020 $298.18 55548830 $297.26 $304.18 $289.2\n\nIf you see the above, that means the data was loaded successfully.\n\nGet types and columns Great! Now that we’ve successfully loaded the data, let’s learn more about\n\nthe loaded dataset, such as what columns and column types it has. As usual,\n\nlet’s keep working on the same file and add a prompt as a comment like so:\n\n[Prompt]\n\n# Get the data types of the columns in the dataset\n\n[End of prompt]\n\nYou should see the below results:\n\n[Prompt response]\n\n# Get the data types of the columns in the dataset print(df.dtypes)",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "[End of response]\n\nNow, let’s run the code. It should show our columns and their data types:\n\nDate object Close object Volume int64 Open object High object Low object dtype: object\n\nGreat! We see the columns, but many of the columns have the object type.\n\nThat’s not good; they need to be numerical to be used in machine learning\n\ntraining. Let’s see if we can address this shortly, but first, let’s learn more about the shape of the dataset.\n\nShape of the dataset Let’s learn more about our dataset in terms of structure, like how many\n\ncolumns and how many rows it consists of. This will help us assess its\n\ncomplexity, see how much data we have, and whether it’s enough to train a\n\nmodel to be reliable. Let’s write our next prompt comment:\n\n[Prompt]\n\n# Get the shape of the dataset\n\n[End of prompt]\n\nYou should see the below suggestion:\n\n[Prompt response]",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "# Get the shape of the dataset print(df.shape)\n\n[End of response]\n\nRunning the code above should provide the response below:\n\n(2518, 6)\n\nThis response tells us we have 2518 rows and 6 columns.\n\nAddressing the column types At this point, we know we need more numerical columns, so let’s deal with\n\nthe ones that aren’t numerical. First, let’s identify those non-numerical\n\ncolumns:\n\n[Prompt]\n\n# print the columns in the dataset\n\n[End of prompt]\n\nYou should see a suggestion like below:\n\n[Prompt response]\n\n# print the columns in the dataset print(df.columns) Index(['Date', 'Close', 'Volume', 'Open', 'High', 'Low'], dtype\n\n[End of response]",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "It’s interesting that although we just asked to print the columns, Copilot makes the decision to show us only the columns of the object type. It’s not\n\nreally surprising because Copilot has been trained on machine learning and\n\nis able to deduce what will happen next, which in this case is converting\n\ncolumns for use in model training.\n\nNext, let’s be a bit prescriptive and ask for columns to be converted to\n\nnumerical versions with the below prompt comment:\n\n[Prompt]\n\n# Replace the $ symbol with an empty string and convert the col\n\n[End of prompt]\n\nYou should see the below code, which turns several columns into numerical\n\nversions.\n\n[Prompt response]\n\n# Replace the $ symbol with an empty string and convert the col df['Open'] = df['Open'].replace({'\\$': ''}, regex=True).astype( df['Close'] = df['Close'].replace({'\\$': ''}, regex=True).astyp df['High'] = df['High'].replace({'\\$': ''}, regex=True).astype( df['Low'] = df['Low'].replace({'\\$': ''}, regex=True).astype(fl\n\n[End of response]\n\nGreat! Let’s inspect the columns next to ensure the conversion has been\n\ncarried out correctly.\n\n[Prompt]",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "# Print the first 5 rows of the DataFrame to confirm the change\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# Print the first 5 rows of the DataFrame to confirm the change print(df.head())\n\n[End of response]\n\nIf you run the above code, you should see the response below:\n\nDate Close Volume Open High Low 0 02/28/2020 273.36 106721200 257.26 278.41 256.37 1 02/27/2020 273.52 80151380 281.10 286.00 272.96 2 02/26/2020 292.65 49678430 286.53 297.88 286.50 3 02/25/2020 288.08 57668360 300.95 302.53 286.13 4 02/24/2020 298.18 55548830 297.26 304.18 289.23\n\nWe can clearly see that the dollar signs have been removed and the columns\n\nare now numerical, save for the Date column, which we can deal with later.\n\nStatistical summary Next, let’s learn a bit more about the dataset by asking for a statistical\n\nsummary. We can expect to see these values:\n\nCount: The number of non-null entries\n\nMean: The average value\n\nStandard Deviation (std): The spread of the data",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "Minimum (min): The smallest value\n\n25th Percentile (25%): The value below which 25% of the data falls\n\nMedian (50%): The middle value, also known as the 50th percentile\n\n75th Percentile (75%): The value below which 75% of the data falls\n\nMaximum (max): The largest value\n\nWith that in mind, let’s write a prompt comment:\n\n[Prompt]\n\n# get the statistical summary of the dataset\n\n[End of prompt]\n\nYou should see an output similar to this:\n\n[Prompt response]\n\n# get the statistical summary of the dataset print(df.describe())\n\n[End of response]\n\nIf you run the code, you will see data similar to this:\n\nClose Volume Open High count 2518.000000 2.518000e+03 2518.000000 2518.000000 251 mean 114.769522 7.258009e+07 114.728443 115.766415 11 std 60.662405 5.663113e+07 60.546893 61.134456 6 min 29.835700 1.136205e+07 29.392800 29.928600 2 25% 66.822475 3.053026e+07 66.877150 67.475300 6 50% 101.090000 5.295469e+07 101.115000 102.085000 10 75%",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "Check for missing values It’s crucial to identify missing values to assess the quality of your dataset.\n\nHigh numbers of null values might indicate issues with data collection or\n\nentry and may affect our model, so we need a way to handle them:\n\n[Prompt]\n\nCheck for missing values\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# check for missing values print(df.isnull().sum())\n\n[End of response]\n\nGreat! So, what does the code do? Well, let’s explain. Let’s say you have\n\nthe following dataset:\n\ndata = {'A': [7, 99, None, 3, None], 'B': [5, None, 6, 8, 9], 'C': [10, 15, 12, None, 11]}\n\nRunning the suggested code would summarize the number of missing\n\nvalues per column, and you would see a response like this:\n\nA: 2 B: 1 C: 1",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "What you can do is replace the missing value with a value that doesn’t\n\naffect the overall score. One way to do so is to add the mean value and replace None with that.\n\nCheck for duplicates Another way to clean up the data is to remove duplicated data, because you\n\ndon’t want that data to create a reinforcing effect on the model. Let’s create\n\na comment for that and see if Copilot can help us:\n\n[Prompt]\n\nprint(df.duplicated().sum())\n\n[End of prompt]\n\nAt this point, Copilot understands what we’re trying to do, that is, identify\n\nand remove duplicates. So, you should see suggested code for both\n\noperations below:\n\n[Prompt response]\n\n# check for duplicates print(df.duplicated().sum())\n\n# drop duplicates df.drop_duplicates(inplace=True)\n\n[End of response]\n\nScale numerical features",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "Next, we’re going to scale values, so they fall into a specific range, 0–1.\n\nThis type of preprocessing will have beneficial effects on a number of\n\nthings, such as algorithm performance, convergence speed, interpretability,\n\nand preventing a group of values from dominating. Let’s create a prompt for\n\nscaling features:\n\n[Prompt]\n\n# Scale numerical features\n\n[End of prompt]\n\nNote that when there’s more than one row in the response, you may need to\n\npress Return and Tab repeatedly. You should see output similar to this:\n\n[Prompt response]\n\n# Scale numerical features # import StandardScaler from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() df[['Open', 'Close', 'High', 'Low', 'Volume']] = scaler.fit_tra\n\n# Print the first 5 rows of the DataFrame to confirm the change print(df.head())\n\n[End of response]\n\nRunning the suggested code should result in the below output:\n\nDate Close Volume Open High Low 0 02/28/2020 2.614832 0.602988 2.354536 2.660953 2.375094 1 02/27/2020 2.617470 0.133721 2.748359 2.785130 2.651257",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "2 02/26/2020 2.932884 -0.404481 2.838059 2.979494 2.876649 3 02/25/2020 2.857534 -0.263366 3.076269 3.055571 2.870490 4 02/24/2020 3.024063 -0.300800 3.015313 3.082566 2.922093\n\nVisualization Next, we want to visualize our data. We could give vague instructions on\n\nhow to do it, but in this case, a scatter plot is a good tool to use, so let’s\n\nreflect that in the prompt:\n\n[Prompt]\n\n# Create a scatter plot of the Open and Close columns\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\nimport matplotlib.pyplot as plt # Create a scatter plot of the Open and Close columns plt.scatter(df['Open'], df['Close']) plt.xlabel('Open') plt.ylabel('Close') plt.title('Relationship between Open and Close') plt.show()\n\n[End of response]\n\nRunning the code above should result in a scatter plot like this:",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "Figure 19.1: Scatter plot showing the relation between the Open and Close columns\n\nStep 3: Split data into training and testing sets Before we can train the model, we need to split the data into training data\n\nand test data. Let’s create a prompt to help with this:\n\n[Prompt]\n\n“use train_test_split from scikit-learn to split the dataset”\n\n[End of prompt]\n\nYou should see an output similar to:\n\n[Prompt response]",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "# Use train_test_split from scikit-learn to split the dataset from sklearn.model_selection import train_test_split\n\n# Drop the Date column from the DataFrame df = df.drop('Date', axis=1)\n\n# Split the dataset into training and testing sets X = df.drop('Close', axis=1) y = df['Close'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_\n\n[End of response]\n\n# Print the shapes of the training and testing sets print('X_train shape:', X_train.shape) print('y_train shape:', y_train.shape) print('X_test shape:', X_test.shape) print('y_test shape:', y_test.shape)\n\nIf you run the code above, you should see a response similar to:\n\nX_train shape: (2014, 4) y_train shape: (2014,) X_test shape: (504, 4) y_test shape: (504,)\n\nAsking questions Just like in the chat experience of Copilot, it’s possible to ask questions on a\n\nspecific topic. If you prefix a question with “Q:”, Copilot understands it’s a\n\nquestion. Let’s try this to find out our next step:\n\n[Prompt]\n\nQ: what is the next step after splitting the data into training and testing sets?",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 719,
      "content": "[End of prompt]\n\nYou should see a response similar to:\n\n[Prompt response]\n\n# Q: what is the next step after splitting the data into traini # A: train the model using the training set and test the model\n\n[End of response]\n\nStep 4: Build a regression problem The time has come to train the model. You could be pretty vague here and\n\njust ask Copilot to perform training. It’s recommended, however, to suggest\n\nthe type of training you want. In this case, we want linear regression, so\n\nlet’s create a prompt that includes that:\n\n[Prompt]\n\n“initialize a LinearRegression model”.\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# initialize a LinearRegression model # import LinearRegression from sklearn.linear_model import LinearRegression",
      "content_length": 770,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 720,
      "content": "# Instantiate LinearRegression as lr lr = LinearRegression()\n\n[End Suggestion Response]\n\nGreat! We have some code that ensures we use linear regression. We\n\nhaven’t carried out any training, though, so that’s next.\n\nStep 5: Train the model With the type of training specified, let’s train the model next:\n\n[Prompt]\n\n“fit the model”\n\n[End of prompt]\n\nYou should see a response similar to this:\n\n[Prompt response]\n\n# Fit lr to the training data lr.fit(X_train, y_train)\n\n# Predict the labels of the test set y_pred = lr.predict(X_test)\n\n[End of response]\n\nStep 6: Evaluate model performance",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 721,
      "content": "Great! We have trained the model, but we don’t know anything about how\n\nwell it performs, and that’s something we should address next:\n\n[Prompt]\n\nEvaluate the performance metrics like MSE or R-squared.\n\n[End of prompt]\n\nYou should see an output similar to this:\n\n[Prompt response]\n\n#evaluate the performance metrics like MSE or R-squared\n\nfrom sklearn.metrics import mean_squared_error\n\n# Calculate the Mean Squared Error (MSE) of the model on the te mean_squared_error = mean_squared_error(y_test, y_pred) print('Mean squared error:', mean_squared_error)\n\n# Calculate the Root Mean Squared Error (RMSE) of the model on root_mean_squared_error = np.sqrt(mean_squared_error) print('Root mean squared error:', root_mean_squared_error)\n\n[End of response]\n\nIf you run the above code, you should see a response like this:\n\nMSE: 0.00011569609998631774 RMSE: 0.010756212157926123\n\nThe output we get represents two different errors: mean squared error and\n\nroot mean squared error.",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 722,
      "content": "A lower MSE indicates a better fit of the model to the data. In this case, the\n\nMSE is quite low, suggesting that the model’s predictions are very close to\n\nthe actual values.\n\nThe RMSE value is also very low, reinforcing that the model’s predictions\n\nare highly accurate. Great! Let’s go over this chapter’s assignment, and then\n\nsummarize what we have learned about using Copilot to help us with\n\nmachine learning.\n\nAssignment Try to solve this problem using a prompting approach where you provide a\n\nlot of information upfront. We suggest constructing a prompt like so:\n\n“Carry out regression on a dataset with the following shape:\n\nOpen: This is the stock price at the beginning of the trading day.\n\nClose: This represents the stock price at the end of the trading day.\n\nHigh: This value shows the highest price the stock reached during the\n\ntrading day.\n\nLow: This indicates the lowest price the stock hit during the trading\n\nday.\n\nVolume: This is the total number of shares that were traded throughout\n\nthe day before the market closed.\n\nSuggest all the steps from loading and pre-processing the data to training\n\nand evaluating the model. You must show code for each step.”\n\nThen see what the response is and try to run the suggested code snippet for\n\neach step. If you encounter any issues, indicate the error to Copilot with a question prompt like so:",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 723,
      "content": "“Q: the below/above code doesn’t work, please fix”\n\nDon’t forget to press Return and Tab to accept the completion.\n\nSummary In this chapter, we wanted to use the suggestion feature of GitHub Copilot,\n\nmeaning we would type comments and use the Return and Tab keys to\n\nreceive suggestions from Copilot. There’s a bit of a trick to it because\n\nsometimes you need to repeatedly press Return and Tab to get the full\n\nresponse. It’s also an AI experience that’s well suited to whenever you\n\nactively write code. GitHub Copilot Chat also has a place. In fact, the two\n\ndifferent experiences complement one another; choose how much of each\n\napproach you want to use. Also, always test the code suggested by Copilot\n\nand ask Copilot to fix the code output if needed.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 724,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 725,
      "content": "20\n\nIncreasing Eﬃciency with GitHub Copilot\n\nIntroduction So far, you’ve been using the knowledge you were taught at the beginning\n\nof the book about GitHub Copilot and ChatGPT. This foundational\n\nknowledge was enough to teach you how to write prompts and accept them.\n\nIt was also enough to let you start work on creating solutions for machine\n\nlearning, data science, and web development. In the case of web\n\ndevelopment, you also discovered Copilot is an efficient tool when working\n\nwith existing code bases. In this chapter, we want to take your AI tool\n\nknowledge to the next level, as there are more features that you may want to\n\nleverage.\n\nThere are a lot of things that can be done to increase efficiency; you will see\n\nlater in the chapter how there are features within Copilot that let you scaffold files, and you will learn more about your workspace and even\n\nVisual Studio Code as an editor, which are all time-saving features. This\n\nchapter will cover some of the most important features.\n\nIn this chapter, we will:\n\nLearn how to use Copilot to generate code.",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 726,
      "content": "Use Copilot commands to automate tasks, like generating a new\n\nproject.\n\nApply techniques to debug and troubleshoot code.\n\nReview and optimize code using Copilot.\n\nCode generation and automation At its core, Copilot is a code generator. It can generate text for you that is\n\neither part of documentation or source code.\n\nThere are two primary ways to generate code with Copilot:\n\nCopilot’s active editor via prompts as comments.\n\nCopilot Chat, which lets you type in a prompt.\n\nCopilot’s active editor When you’re in the active editor, Copilot can generate code. To generate\n\ncode, you need to write a prompt as a comment. There are different ways to\n\nadd comments to your programming language code. In JavaScript and C#, for example, you use //, while you would type <!-- if you’re in HTML.\n\nLet’s see an example of this:\n\n[Prompt]\n\nCreate a new function called add that takes two parameters.\n\n[End of prompt]",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 727,
      "content": "//Create a new function called add that takes two parameters\n\nAbove, we have a prompt as a comment. Copilot looks at this prompt and\n\nattempts to generate code based on it. At this point, Copilot sends the generated code back as a “suggestion,” which you can accept using the Tab\n\nkey. Here’s what it looks like in the editor:\n\n// Create a new function called add that takes two parameters function add(num1, num2) { throw new Error('Function not implemented.'); }\n\nAt this point, you get no suggestions. You have a couple of options:\n\nYou can press the Return key to get more suggestions.\n\nStart typing part of the code you want to generate, and Copilot will suggest code based on what you type.\n\nCopilot Chat Copilot Chat is a way to interact with Copilot using a chat interface. To\n\nactivate Copilot Chat, click on the icon in the bottom-left corner of Visual Studio Code (or VS Code), which will open up a chat window.",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 728,
      "content": "Figure 20.1: Showing the left menu in Visual Studio Code with the “Chat” icon from Copilot Chat\n\nUpon clicking the above chat icon, the below image will be shown.",
      "content_length": 162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 729,
      "content": "Figure 20.2: The image above shows the chat window that opens when you click the Chat icon in the\n\nleft menu in Visual Studio Code\n\nLet’s try to generate some code based on the same prompt as before. Let’s\n\nuse the input text field in the chat window and type in the prompt:",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 730,
      "content": "[Prompt]\n\nCreate a new function called add that takes two parameters\n\n[End of prompt]\n\nThe output comes in two parts, a text instruction and a code suggestion.\n\nFirst, the text instruction:\n\n[Prompt response]\n\nSure, here’s an example of a function called add that takes two parameters\n\nand returns their sum:\n\nfunction add(a, b) { return a + b; }\n\n[End of response]\n\nIt even shows you an example of how to use the function:\n\nconst result = add(2, 3); console.log(result); // Output: 5\n\nCopilot commands If you start typing / in the chat window, you will see a list of commands\n\nthat you can use with Copilot.",
      "content_length": 608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 731,
      "content": "Figure 20.3: Available commands\n\nThere are quite a few available commands, and we will focus on a few of\n\nthem in this chapter.\n\nCreating a Notebook With this command, you can create a new notebook. A notebook is a place\n\nwhere you can store code snippets and text. To create a notebook, type in /newNotebook, and also provide a name for the notebook, for example,\n\nproducts.",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 732,
      "content": "Figure 20.4: Creating a Notebook\n\nThe above image shows what’s displayed when you type the /newNotebook command; you’re asked to confirm the scaffolding of a notebook by\n\nclicking Create Notebook.\n\nThis will create a new notebook with commonly used code snippets for\n\nproducts. Let’s inspect the output next.\n\nYour notebook, at a high level, contains the following:\n\nimport, which contains all the imports needed to work with a dataset;\n\nhere’s what the content looks like:\n\nimport pandas as pd # Import Pandas library import numpy as np # Import NumPy library import matplotlib.pyplot as plt # Import Matplotlib library import seaborn as sns # Import Seaborn library for data vis",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 733,
      "content": "Load data, which contains code to load data from a CSV file (the CSV\n\nfile product_data.csv doesn’t exist, but Copilot assumes that you\n\nhave such a file; adjust this part of the code to fit the name of the CSV\n\nfile that you do have):\n\n# Load product data from CSV file into a Pandas DataFrame product_data = pd.read_csv('product_data.csv') # Load product data from JSON file into a Pandas DataFrame # product_data = pd.read_json('product_data.json') # Display the first 5 rows of the DataFrame print(product_data.head())\n\nExplore product data, which contains operations to inspect the data:\n\n# Use Pandas functions to explore the product data # Display the last 5 rows of the DataFrame print(product_data.tail()) # Display the summary information of the DataFrame print(product_data.info()) # Display the statistical summary of the DataFrame print(product_data.describe())\n\nFilter product data, which contains operations to filter the data:\n\n# Filter product data based on specific criteria using bool filtered_data = product_data[(product_data['category'] == ' # Display the filtered data print(filtered_data)\n\nSorting data, which contains sorting operations:\n\n# Sort the product data based on one or more columns using # Sort by price in ascending order product_data.sort_values(by='price', ascending=True, inplac",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 734,
      "content": "# Sort by category in ascending order, then by price in des product_data.sort_values(by=['category', 'price'], ascendin # Display the sorted data print(product_data)\n\nAggregating data; often, you want to aggregate data to get a better\n\nunderstanding of it:\n\n# Group the product data by category and calculate the tota category_total_price = product_data.groupby('category')['p # Display the total price for each category print(category_total_price) # Group the product data by category and calculate the ave category_avg_price = product_data.groupby('category')['pri # Display the average price for each category print(category_avg_price) # Group the product data by category and calculate the num category_product_count = product_data.groupby('category')[ # Display the number of products in each category print(category_product_count)\n\nIn summary, that’s a lot of code you don’t have to write yourself. You can\n\nuse this notebook as a starting point to work with product data.\n\nCreating a project Another really useful command is being able to create a project. Usually,\n\nwe either create files by hand, which takes time, or we rely on CLI tools to\n\ncreate projects for us. With Copilot, you can create a project by typing in /new in the chat window. To use the command, type /new and then provide\n\na description for it, for example, React. This will create a new project with\n\na React project template. Let’s inspect the output:",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 735,
      "content": "my-react-app ├── public │ ├── index.html │ └── favicon.ico ├── src │ ├── App.js │ ├── index.js │ ├── components │ │ ├── Customer.js │ │ ├── Product.js │ │ ├── CustomerList.js │ │ └── ProductList.js │ ├── data │ │ ├── customers.json │ │ └── products.json │ └── styles │ ├── App.css │ ├── Customer.css │ ├── Product.css │ ├── CustomerList.css │ └── ProductList.css ├── package.json └── README.md\n\nThe files created above are typical of what’s expected for a React project. It contains:\n\npublic, which contains the index.html file that references the JavaScript file indicating the location of your app.\n\nsrc, which contains all the application files and is itself divided into\n\ndifferent folders for components, data, and CSS styles.\n\nAll of this constitutes a starting point. You might want to change files to fit\n\nyour liking.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 736,
      "content": "Debugging and troubleshooting Being able to debug and troubleshoot code is an important skill to have.\n\nWhen you develop code, it seldom does what you need it to do the first\n\ntime. You need to be able to debug and troubleshoot code to figure out\n\nwhat’s wrong with it. The same applies if you have a reported bug; you\n\nneed to be able to diagnose at what point it goes wrong.\n\nCopilot can help, and a good approach is using Copilot chat and asking it to\n\nadd support for debugging and troubleshooting.\n\nThe following is a piece of sample code you’ve been given. It’s a REST API in the framework Flask for Python. It’s working code, but it might not\n\nbe so easy to work with from a debugging and troubleshooting standpoint:\n\nfrom flask import Flask, jsonify, request import sqlite3 app = Flask(__name__) # Create a connection to the database conn = sqlite3.connect('products.db') c = conn.cursor() # Create a table for products if it doesn't exist c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): # Retrieve all products from the database c.execute('SELECT * FROM products') products = c.fetchall() # Convert the products to a JSON response response = [] for product in products:",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 737,
      "content": "response.append({ 'id': product[0], 'name': product[1], 'price': product[2] }) return jsonify(response) @app.route('/products', methods=['POST']) def add_product(): # Retrieve the product data from the request body data = request.get_json() name = data['name'] price = data['price'] # Insert the product into the database c.execute('INSERT INTO products (name, price) VALUES (?, ?) conn.commit() # Return a success message return jsonify({'message': 'Product added successfully'}) if __name__ == '__main__': app.run(debug=True)\n\nThe above code works, but it doesn’t support debugging and\n\ntroubleshooting; let’s see how we can add support for that by using Copilot\n\nChat:\n\n[Prompt]\n\nAdd support for troubleshooting and debugging\n\n[End of prompt]\n\nCopilot chat answers us with both text and code:\n\n[Prompt response]\n\nSure, here’s an updated version of the Flask API that includes support for\n\ntroubleshooting and debugging:",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 738,
      "content": "from flask import Flask, jsonify, request import sqlite3 app = Flask(__name__) # Create a connection to the database conn = sqlite3.connect('products.db') c = conn.cursor() # Create a table for products if it doesn't exist c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database c.execute('SELECT * FROM products') products = c.fetchall() # Convert the products to a JSON response response = [] for product in products: response.append({ 'id': product[0], 'name': product[1], 'price': product[2] }) return jsonify(response) except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while retri @app.route('/products', methods=['POST']) def add_product(): try: # Retrieve the product data from the request body data = request.get_json() name = data['name'] price = data['price'] # Insert the product into the database c.execute('INSERT INTO products (name, price) VALUES (? conn.commit() # Return a success message",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 739,
      "content": "return jsonify({'message': 'Product added successfully' except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while addin if __name__ == '__main__': app.run(debug=True)\n\nThe improvements are:\n\nAdded a try/except block around the code in the get_products()\n\nfunction.\n\nAdded logger statements to log errors.\n\nAdded debug=True to the app.run() statement.\n\nThese changes will help us troubleshoot and debug the code.\n\n[End of response]\n\nLet’s look at how we can use Copilot to help us with code reviews and\n\noptimizations next.\n\nCode review and optimization techniques Right now, we have code that works, but it’s not optimized. For example,\n\nit’s usually a good idea to separate code into different files. Let’s see what\n\nCopilot suggests to improve the code.\n\n[Prompt]\n\nImprove the selected code\n\n[End of prompt]",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 740,
      "content": "As usual, we get the Copilot chat response in terms of text and code:\n\n[Prompt response]\n\nSure, here are some improvements you could make to the selected code:\n\nUse a context manager to handle the database connection and cursor.\n\nThis ensures that the connection is properly closed when the code is\n\nfinished executing, even if an exception occurs.\n\nUse a list comprehension to convert the products to a JSON response.\n\nThis is more concise and can be faster than using a for loop.\n\nUse the json parameter of the Response object to return a JSON\n\nresponse. This is more concise than using the jsonify function.\n\nUse the request.json attribute to retrieve the product data from the\n\nrequest body. This is a more concise and Pythonic way to do it.\n\nand the resulting code:\n\nfrom flask import Flask, Response, request import sqlite3 app = Flask(__name__) # Create a table for products if it doesn't exist with sqlite3.connect('products.db') as conn: c = conn.cursor() c.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') conn.commit() @app.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database with sqlite3.connect('products.db') as conn: conn.row_factory = sqlite3.Row c = conn.cursor() c.execute('SELECT * FROM products')",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 741,
      "content": "products = c.fetchall() # Convert the products to a JSON response response = [{'id': product['id'], 'name': product['name return Response(response=response, status=200, mimetype except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return Response(response={'error': 'An error occurred w @app.route('/products', methods=['POST']) def add_product(): try: # Retrieve the product data from the request body data = request.json name = data['name'] price = data['price'] # Insert the product into the database with sqlite3.connect('products.db') as conn: c = conn.cursor() c.execute('INSERT INTO products (name, price) VALUE conn.commit() # Return a success message return Response(response={'message': 'Product added suc except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return Response(response={'error': 'An error occurred w if __name__ == '__main__': app.run(debug=True)\n\n[End of response]\n\nThe biggest change above was using a context manager to handle the\n\ndatabase connection and cursor. This ensures that the connection is properly\n\nclosed when the code has finished executing, even if an exception occurs.\n\nWe still want to separate the code into different files; let’s see what Copilot\n\nsuggests for that.\n\nLet’s start with a prompt:",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 742,
      "content": "[Prompt]\n\nSeparate the code into separate files by responsibility\n\n[End of prompt]\n\nand the resulting code:\n\n[Prompt response]\n\n# app.py from flask import Flask from products import products_bp app = Flask(__name__) app.register_blueprint(products_bp) if __name__ == '__main__': app.run(debug=True) # database.py import sqlite3 def get_db(): db = sqlite3.connect('products.db') db.row_factory = sqlite3.Row return db # products.py from flask import Blueprint, jsonify, request from database import get_db products_bp = Blueprint('products', __name__) @products_bp.route('/products', methods=['GET']) def get_products(): try: # Retrieve all products from the database db = get_db() products = db.execute('SELECT * FROM products').fetchal # Convert the products to a JSON response response = [{'id': product['id'], 'name': product['name return jsonify(response) except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while retri @products_bp.route('/products', methods=['POST'])",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 743,
      "content": "def add_product(): try: # Retrieve the product data from the request body data = request.json.get('data', {}) name = data.get('name', '') price = data.get('price', 0) # Insert the product into the database db = get_db() db.execute('INSERT INTO products (name, price) VALUES ( db.commit() # Return a success message return jsonify({'message': 'Product added successfully' except Exception as e: # Log the error and return an error message app.logger.error(str(e)) return jsonify({'error': 'An error occurred while addin\n\n[End of response]\n\nIf you receive any errors while running the code, consult\n\nCopilot, tell it there’s an error, and ask it to address the\n\nissue. Remember that Copilot is more than a code generator.\n\nWhat we’re seeing now is a separation of concerns. The app.py file is responsible for starting the application and registering the blueprint. The database.py file is responsible for handling the database connection. The\n\nproducts.py file is responsible for handling the products API. This code is something we can work with and maintain.\n\nThis code can be improved further by changing database.py to use a\n\ncontext manager:\n\nimport sqlite3 def get_db():",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 744,
      "content": "with sqlite3.connect('products.db') as db: db.row_factory = sqlite3.Row return db def init_db(): with sqlite3.connect('products.db') as db: db.execute('''CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL)''') db.commit() # other code omitted for brevity\n\nHere, you saw how Copilot can review your code in steps, splitting a single\n\nfile into multiple files, and then improving the code.\n\nWorkspace Your workspace is all the files you have open in a directory. You want your\n\nAI assistant to consider all these files when giving you a recommendation.\n\nLet’s look at the following web project where the following files exist:\n\nsrc/ app.py utils.py\n\napp.py contains an API and utils.py contains an email validation\n\nfunction. Here’s the code for app.py:\n\n# create a web api using Flask, should have a products GET rout from flask import Flask, jsonify app = Flask(__name__) @app.route('/customers, methods=['GET']) def get_customers(): return jsonify({'customers': ['item1', 'item2', 'item3']})",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 745,
      "content": "# TODO, create a POST route for products # Run the app if __name__ == '__main__': app.run(debug=True)\n\nNote the TODO comment; we’re about to add a route to store a customer. When storing this customer, we need to ensure that the customer data contains a valid email. Luckily, the utils.py file already contains such a\n\nfunction, but can we get Copilot to use it? Let’s see.\n\nChange the comment in app.py to say the following:\n\n[Prompt]\n\nTODO, create a POST route for customers. It should validate the request\n\nbody, and ensure it has a valid email address.\n\n[End of prompt]\n\nYou get a basic function that doesn’t really do what it’s supposed to:\n\n[Prompt response]\n\n@app.route('/customers', methods=['POST']) def post_customers(): return jsonify({'message': 'success'})\n\n[End of response]\n\nTo help with this, you resort to GitHub Chat and type in the following\n\nprompt:\n\n[Prompt]\n\nValidate the request body and ensure that it has a valid email address",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 746,
      "content": "[End of prompt]\n\n[Prompt response]\n\nTo validate the request body and ensure it has a valid email address, you\n\ncan use the request object from Flask and a regular expression. Here’s how\n\nyou can modify your post_customers function:\n\nfrom flask import request import re\n\n@app.route('/customers', methods=['POST']) def post_customers(): data = request.get_json()\n\nif 'email' not in data: return jsonify({'message': 'No email provided'}), 400\n\nemail = data['email'] pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2\n\nif not re.match(pattern, email): return jsonify({'message': 'Invalid email format'}), 40\n\nreturn jsonify({'message': 'success'})\n\n[End of response]\n\nWhile the above code works, it doesn’t leverage our existing utils.py file,\n\nin which there is a function already solving validating an email. So how do\n\nwe get Copilot to consider that file? The answer lies in a command called @workspace. The @worskspace command considers all the files. Here’s how it can be used:\n\n[Prompt]",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 747,
      "content": "@workspace improve selected code to validate request body to ensure it\n\ncontains a valid email address, use primarily functionality from other files\n\nin the workspace to solve the problem.\n\n[End of prompt]",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 748,
      "content": "Figure 20.5: Image from Copilot Chat showing how using the @workspace command leverages\n\nexisting code and code files in the workspace to solve a problem, rather than creating new code.\n\nLet’s also show the response in code and discuss what we can see:\n\n[Prompt response]\n\nfrom flask import request from utils import validate_email\n\n@app.route('/customers', methods=['POST']) def post_customers(): data = request.get_json()\n\nif 'email' not in data: return jsonify({'message': 'No email provided'}), 400\n\nemail = data['email']\n\nif not validate_email(email): return jsonify({'message': 'Invalid email format'}), 40\n\nreturn jsonify({'message': 'success'})\n\n[End of response]\n\nIn the code above, validate_email, residing in the file utils.py, is used to\n\nvalidate an email. This is important as the first attempt, and solving this problem led to Copilot duplicating code. For that reason, using @workspace\n\nis recommended when you want Copilot to consider all code in your\n\nworkspace when suggesting a solution.\n\nVisual Studio Code lookup",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 749,
      "content": "Another powerful feature of Copilot is being able to quickly look up how to\n\ndo something in Visual Studio Code. Rather than consulting official\n\ndocumentation, you may want to quickly find what you need without leaving your code editor. To use this feature, there’s a @vscode command.\n\nLet’s ensure we have the chat window open and see how it works with the\n\nfollowing prompt:\n\n[Prompt]\n\n@vscode How do I open a terminal window?\n\n[End of prompt]\n\nFigure 20.6: Copilot response to the above prompt\n\nThere are two things worth noting here:\n\nCopilot tells you how to do something and where to find it.\n\nIt offers to visually show you how to find what you need if you click\n\nthe Show in Command Palette button.\n\nTerminal The last really great feature we will show you is terminal commands. In\n\ntheory, you could use terminal without resorting to this specific command,\n\nbut it helps if you don’t have to prefix your question with “Show me how to",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 750,
      "content": "do something in the terminal.” To use this command, prefix your prompt with @terminal like so:\n\n[Prompt]\n\n@terminal show me how to create a remote called upstream in Git that points to a remote repo.\n\n[End of prompt]\n\n[Prompt response]\n\nTo create a remote called “upstream” that points to a remote repository in Git, you can use the git remote add command. Here’s the command:\n\ngit remote add upstream {remote_repo_url}\n\nReplace {remote_repo_url} with the URL of the remote repository.\n\nThis command adds a new remote named “upstream” that points to the\n\nspecified remote repository. You can then fetch from or push to the\n\n“upstream” remote as needed.\n\n{remote_repo_url}: The URL of the remote repository. This could be a URL on GitHub, GitLab, Bitbucket, or any other Git server.\n\n[End of response]\n\nAs you can see, you get both the command you need and a detailed\n\nexplanation of how to type.\n\nAssignment See if you can improve the code further by adding, for example,\n\ndocumentation, tests, or maybe an ORM (object-relational mapper).\n\nRemember to use the commands covered in this chapter.",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 751,
      "content": "Challenge Check out the other commands that Copilot Chat supports, and see if you\n\ncan use them to improve your workflow.\n\nQuiz What can Copilot help with?\n\n1. Code generation, automation, debugging, troubleshooting, code\n\nreview, and optimization\n\n2. Deployment\n\n3. None of the above\n\nSummary In this chapter, we covered some more advanced functionality available in Copilot. You learned how to use @workspace to enable Copilot to consider all your files. The @vscode command was another useful command in that it\n\nshowed you how to work with Visual Studio Code.\n\nWe also looked at scaffolding – specifically, how to scaffold files for a web\n\nproject – and how to create a Notebook with starter code. Such commands\n\nare likely to save you hours when you first start with a project. Copilot has\n\nquite a few commands, and I recommend trying them out.\n\nJoin our community on Discord",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 752,
      "content": "Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 753,
      "content": "21\n\nAgents in Software Development\n\nIntroduction This chapter will introduce you to the concept of agents in software\n\ndevelopment. We’ll cover what agents are, how they work, and how you\n\ncan use them in your projects. We’ll also cover some of the most popular\n\nagents’ frameworks and how you can get started with them.\n\nLet’s introduce the problems that agents can solve. The general idea is to\n\nhave a program that can act on your behalf. Examples of this could be\n\nautomating tasks, making decisions, and interacting with other agents and\n\nhumans. Programs like these can save you time and make your life easier or\n\nyour business more efficient.\n\nIn this chapter, we will do the following:\n\nIntroduce the concept of agents in software development.\n\nExplain what agents are and how they work.\n\nDiscuss the different types of agents and how they can be used.\n\nWhat are agents?",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 754,
      "content": "As mentioned previously, agents are programs that can act on your behalf.\n\nThey can perform tasks, make decisions, and interact with other agents and\n\nhumans. Agents can be used in a wide range of applications.\n\nSeveral things make a program an agent program versus just a program:\n\nAgent programs have a clear goal: For instance, take a thermostat\n\nkeeping the temperature at 25 degrees and taking appropriate actions\n\nto keep it there, or an agent managing finances and trying to maximize\n\nyour profit.\n\nAutonomous: An agent makes necessary decisions to ensure it meets a\n\ngoal as defined previously. For a finance agent, that could mean\n\nbuying and selling stocks when they meet a specific trigger condition.\n\nHas sensors: Sensors are either physical or could be an API in\n\nsoftware, something that enables an agent to understand “what the world is like.” For a thermostat, a sensor is a temperature indicator, but\n\nfor a finance agent, a sensor can be an API toward the stock market that enables the agents to decide their goals.\n\nHow do agents work? Agents work by receiving input, processing it, and producing output. They\n\ncan be programmed to perform specific tasks, make decisions, and interact with other agents and humans. Agents can also learn from their interactions\n\nand improve their performance over time.\n\nFigure 21.1: Process for simple agent: keyword, recognize, perform task",
      "content_length": 1394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 755,
      "content": "Simpler agents versus agents using AI Agents are not a new thing. They have been around for a long time. What’s\n\nnew is that agents are now being powered by AI. Let’s compare the two:\n\nSimpler agents: Traditional agents are programmed to perform\n\nspecific tasks and make decisions based on predefined rules and logic.\n\nAgents using AI: Agents powered by AI can perform more complex\n\ntasks and make more intelligent decisions. They can understand natural\n\nlanguage, learn from their interactions, and improve their performance over time.\n\nSimpler agents Simpler agents, as mentioned in the previous sections, are limited in that\n\nthey are made for specific tasks. Interacting with them is usually also limited – you either use keywords or the way you can express yourself is\n\nlimited.\n\nAn example of a simple agent is a chatbot. Such chatbots are programmed\n\nto understand a limited set of keywords and phrases.\n\nFor example, “Tell me more about your products,” or “What’s your return policy?”. Any attempts at conversation outside of these keywords and\n\nphrases will result in the chatbot not understanding the user.\n\nA simple agent is not a great conversationalist",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 756,
      "content": "When you have a conversation with a human, you expect them to know a\n\nfew topics well and at least to be able to talk about other topics. For a simpler agent, we might end up in the following conversation:\n\nUser: “Tell me about your products.”\n\nAgent: “We have a wide range of products, including electronics, clothing, and accessories. Please indicate your interest.”\n\nUser: “I’m interested in clothes, something fitting the current weather.”\n\nAgent: “I can advise on clothes for sure, but I don’t know the current\n\nweather.”\n\nThere are two interesting things we can observe here:\n\nThe conversation feels short, and the important information is either at\n\nthe end of a sentence or just before a comma, which indicates that\n\nsimpler parsing is used to extract the important information.\n\nIt doesn’t handle non-product information like weather, which could help filter down the response.\n\nImproved conversation with tool calling and large language models (LLMs) An LLM is an improvement in that it’s good at sounding more natural, but\n\nalso that it can parse out and recognize intent from fluent text. You can also\n\nprovide the LLM with additional knowledge thanks to something called tool calling, where you tell the LLM about various capabilities like the\n\nclothes API, weather API, and so on, which can handle the conversation",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 757,
      "content": "better, and resemble a conversation with a human. Let’s take that same conversation with an LLM and highlight the difference:\n\nUser: “What are your products?”\n\nAgent: “We have a wide range of products, including electronics,\n\nclothing, and accessories.”\n\nUser: “Great, I’m interested in clothes, something fitting the current weather. What can you recommend?”\n\nAgent: “Can you tell me your location so I can advise you better on\n\nclothes?”\n\nUser: “Sure, I’m based in Phoenix, Arizona.”\n\nAgent: “I see it’s currently 90F in Phoenix at the moment. Might I\n\nsuggest these shorts?”\n\nThe reason this conversation fared better is that this LLM expresses itself more naturally thanks to the tool calling that called its weather API with\n\nPhoenix as input and then proceeded to call the clothes API with the\n\nweather response as a filter.\n\nThe anatomy of a conversational agent A conversational agent typically consists of the following components:\n\nInput: The input to the agent, typically in the form of natural\n\nlanguage. It should be said this can be a lot of different spoken\n\nlanguages, not just English, which you have had to hardcode in the past.\n\nProcessing: The processing of the input, typically using natural\n\nlanguage processing (NLP) techniques.",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 758,
      "content": "Delegation: The delegation of the input to the appropriate component of the agent. The component it’s delegated to can be an agent for a\n\nspecific task, for example, to book a flight or to answer a question.\n\nFigure 21.2: Conversational agents process steps\n\nThe preceding diagram indicates a loop where you go from input to\n\nprocessing to delegation to result, so why is there a loop? An agent doesn’t\n\nhave the concept of an end; it sits there and waits for the user to provide input and reacts to it. As mentioned earlier in this chapter, an agent works\n\ntoward a goal, and if the goal is to manage finances, it’s a continuous job.",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 759,
      "content": "More on tool calling in LLMs We’ve mentioned tool calling previously in this chapter but let’s try to show\n\nhow it works to add capabilities to the LLM.\n\nThe LLM only knows what it has been trained on, and for things it hasn’t\n\nbeen trained on, it will, in many cases, try to provide you with an answer that isn’t always correct as it makes it up; this is known as a hallucination.\n\nTo improve areas where you want the LLM to provide more accurate\n\nresponses, you can present it with a tool. The process of providing a tool consists of the following components:\n\nA JSON description of a function\n\nA description of the function so the LLM knows when this function\n\nshould be called\n\nOnce you’ve provided the preceding components, let’s say you provide a\n\nfunction capable of fetching the weather; the LLM can now use its built-in\n\nfeatures to semantically interpret all the following inputs to mean that the\n\nuser wants to know about the weather:\n\n“What’s the weather like today in Salt Lake City?”\n\n“What’s the temperature in San Francisco?”\n\n“Is it going to rain in New York tomorrow?”\n\n“What’s the weather like in London?”\n\n“Is it warm outside?”\n\nAdding capabilities to GPT using tools",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 760,
      "content": "How it works is that you provide a function specification in a JSON format.\n\nThis JSON function format is a schema the GPT model understands. The\n\nGPT model will essentially do two things for you:\n\nExtract parameters from the prompt.\n\nDetermine whether to call a function and which function to call, as you\n\ncan tell it about more than one function.\n\nAs a developer, you need to then actively call the function if the LLM\n\nthinks it should be called.\n\nYour function format follows this schema:\n\n{ \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to }, }, \"required\": [\"location\", \"format\"], }, } }",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 761,
      "content": "In the preceding JSON schema, there are a few things you’re telling the\n\nGPT model:\n\nThere’s a function called get_current_weather.\n\nThe description is \"Get the current weather\".\n\nThe function takes two parameters, location and format.\n\nThere’s also a description of the parameters, their types, and allowed\n\nvalues.\n\nLet’s describe how this would work in practice, given the following prompt:\n\n[Prompt]\n\n“What’s the weather like today in Salt Lake City?”\n\n[End of prompt]\n\nHere’s what the GPT model can extract from the prompt:\n\nLocation: Salt Lake City.\n\nFormat: This is not provided, but the GPT can infer this from the\n\nuser’s location.\n\nFunction to call: get_current_weather.\n\nWhat you need to do as a developer is to call the function indicated with the\n\nextracted parameter values. The following is code that could be used to\n\nconnect to the GPT model, where a function description is provided, and\n\nparse the response:\n\nimport open def get_current_weather(location, format): # Call weather API response = requests.get(f\"https://api.weather.com/v3/wx/for return response.json() # Call the GPT model",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 762,
      "content": "tool = { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g CA\", }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to }, }, \"required\": [\"location\", \"format\"], }, } } prompt = \"What's the weather like today in Salt Lake City?\" response = openai.Completion.create( model=\"text-davinci-003\", prompt=prompt, max_tokens=150, tools= [tool] ) # Parse the response function_response = response.choices[0].function_response # her location = function_response.parameters.location # extracting p format = function_response.parameters.format # extracting param weather = get_current_weather(location, format) # here we get t\n\nThis is probably the most basic example of how you can create a\n\nconversational agent using GPT.",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 763,
      "content": "However, we expect a bit more from an advanced agent. What if we want to\n\ndo something more complex, like booking a flight, hotel, car, and\n\nrestaurant?\n\nAdvanced conversations It’s easy to think you can just add more functions to the GPT model and that\n\nmight work for a time. However, as the conversation becomes more\n\ncomplex, the GPT model needs to remember the context of the conversation\n\nand keep track of the state of the conversation.\n\nFor more advanced conversations, it quickly becomes more complex.\n\nImagine going into a travel agent’s office and the following back-and-forth\n\nconversation taking place between two humans:\n\nUser: “I want to go on a vacation.”\n\nAgent: “Sure.”\n\nUser: “I want to go to a warm place.”\n\nAgent: “Sure, tell me more about what you’re looking for.”\n\nUser: “I want somewhere with a beach.”\n\nAgent: “Sure, any more details?”\n\nUser: “I want there to be at least 25 degrees.”\n\nAgent: “Sure, any more details?”\n\nUser: “No, that’s it.”\n\nAgent: “I found three places that match your criteria. Can I present them to\n\nyou?”",
      "content_length": 1052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 764,
      "content": "User: “Yes, please.”\n\nAgent: “Here are the three places. Which one do you want to know more\n\nabout?”\n\nUser: “I want the first one.”\n\nAgent: “Here’s more information about the first place.”\n\nUser: “Cool, can you book a hotel and flight for the first place?”\n\nAgent: “Sure, I’ll get that done for you.”\n\nAgent: “I’ve booked the hotel and flight for you.”\n\nUser: “Thanks, ooh, I need to rent a car as well.”\n\nAgent: “Sure, any specific car you’re looking for?”\n\nUser: “Not really; my budget is $100 per day.”\n\nAgent: “I found five cars that match your criteria. Can I present them to\n\nyou?”\n\nUser: “Yes, please. I want the fifth one.”\n\nAgent: “I’ve booked the car for you.”\n\nAgent: “You’re all set for your vacation.”\n\nUser: “Thanks, but wait, I need help booking a restaurant for the first\n\nnight.”\n\nAt this point, it stands clear that this conversation can go on for quite a\n\nwhile. The agent needs to remember the context of the conversation and\n\nkeep track of the state.\n\nThere are also many different tasks that the agent needs to delegate to other agents or services, like booking a hotel, flight, car, and restaurant, and the",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 765,
      "content": "weather API, sightseeing API, and more.\n\nThe point is that there’s more to an agent than just understanding the initial\n\nprompt and delegating the task to another agent or service. You need to\n\nthink of this conversation as a state machine and an orchestration of\n\ndifferent agents and services.\n\nModeling advanced conversations We mentioned that a more advanced conversation involves remembering\n\nboth context and state. Let’s inspect a subset of the example conversation\n\nand see how the state changes:\n\nUser: “I want to go on a vacation.”\n\nAgent: “Sure.”\n\nAt this point, the agent hasn’t remembered anything more than the user’s\n\nintention, which is to go on vacation. It’s just acknowledged the user’s\n\nprompt.\n\nUser: “I want to go to a warm place.”\n\nAgent: “Sure, tell me more about what you’re looking for.”\n\nNow things are getting interesting. The agent has remembered “warm” as a\n\npiece of criteria and needs to translate “warm” into a temperature range it\n\ncan use to filter out places that are too cold.\n\nUser: “I want somewhere with a beach.”\n\nAgent: “Sure, any more details?”",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 766,
      "content": "This is another step forward; the agent has remembered “beach” as an\n\nadditional piece of criteria to use when filtering out places.\n\nUser: “I want it to be at least 25 degrees.”\n\nAn additional criterion, “25 degrees,” has been added. Let’s see the earlier\n\npiece of criteria, “warm,” which was defined as 20–40 Celsius – this adjusts\n\nthe range to 25–40 Celsius.\n\nAgent: “Sure, any more details?”\n\nUser: “No, that’s it.”\n\nAt this point, the agent recognizes that the user has no more criteria to add,\n\nand a search/decision can take place with the filters of “warm,” “beach,”\n\nand “25–40 Celsius.” Now, an API is called to get a list of places and the\n\nagent can present the list to the user for selection.\n\nAgent: “I found three places that match your criteria. Can I present them to\n\nyou?”\n\nWhat’s important to add is that not only are criteria remembered for this\n\nspecific trip retrieval but they need to be remembered for the next steps as\n\nwell unless the user changes the criteria.\n\nHopefully, you can see from the preceding example that the state is built up\n\nslowly, and the agent needs to remember the context of the conversation.\n\nIt can be helpful to think of a more advanced conversation as consisting of\n\nthe following steps:\n\n1. Input: The input to the agent, typically in the form of natural\n\nlanguage.\n\n2. Processing: The processing of the input, typically using NLP\n\ntechniques.",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 767,
      "content": "3. Determine the next step: The agent needs to determine the next step\n\nin the conversation based on the input and the current state of the\n\nconversation. Answers here can be to ask for more information,\n\npresent a list of options, book something, and so on.\n\n4. End conversation or continue (ask for user input): The agent needs\n\nto determine whether the conversation should end or continue. If it\n\nshould continue, it needs to ask for user input.\n\nPseudo code for advanced conversations The agent might have a few different states, for example:\n\nAsk for a task: This would typically be asked when the conversation\n\nstarts or when a task has been performed and a user selection has been\n\ndone.\n\nAsk the user for more information on a task: This would typically\n\nbe asked before a task is performed to ensure the agent has all the\n\ninformation it needs.\n\nPresent a list of options to the user: This would typically be asked after a task has been performed to present the user with things to\n\nchoose from.\n\nPerform a task: Here, the agent would perform a task, like booking a hotel, flight, car, or restaurant.\n\nEnd the conversation: The agent moves to this state when the\n\nconversation is over, and the user has somehow indicated that the\n\nconversation is over.\n\nThis is how this might look in pseudo code:",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 768,
      "content": "# enum class State(Enum): ASK_FOR_TASK = 1 ASK_FOR_MORE_INFORMATION = 2 PRESENT_TASK_RESULT = 3 PERFORM_TASK = 4 END_CONVERSATION = 5 # initial state state = State.ASK_FOR_TASK def ask_for_task(): # ask the user for a task pass def ask_for_more_information(task): # store filter criteria pass def present_task_result(task): # presents the result so the user can choose pass def perform_task(task): # Perform a task pass def end_conversation(): # End the conversation pass while state != State.END_CONVERSATION: if state == State.ASK_FOR_TASK: # Ask for a task task = ask_for_task() state = State.ASK_FOR_MORE_INFORMATION elif state == State.ASK_FOR_MORE_INFORMATION: # Ask the user for more information on a task task = ask_for_more_information(task) state = State.PERFORM_TASK elif state == State.PRESENT_TASK_RESULT: # Present a list of options to the user task = present_task_result(task) state = State.ASK_FOR_MORE_INFORMATION elif state == State.PERFORM_TASK: # Perform a task perform_task(task) state = State.PRESENT_TASK_RESULT elif state == State.END_CONVERSATION:",
      "content_length": 1072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 769,
      "content": "# End the conversation end_conversation()\n\nThe preceding code is a decent starting point for a more advanced conversation. However, we should remember that humans are not always\n\npredictable, and the agent needs to be able to handle the unexpected. For\n\nexample, humans can change their minds or add new criteria at any point.\n\nAutonomous agents Autonomous agents are agents that can act on their own without human\n\nintervention. They can perform tasks, make decisions, and interact with\n\nother agents and humans without human input. Autonomous agents can be\n\nused in a wide range of applications, from self-driving cars to virtual assistants.\n\nExamples of autonomous agents include the following:\n\nSelf-driving cars: Self-driving cars are autonomous agents that can\n\ndrive themselves without human intervention. They can navigate\n\nroads, avoid obstacles, and make decisions based on their\n\nsurroundings.\n\nVirtual assistants: Virtual assistants are autonomous agents that can\n\nperform tasks, make decisions, and interact with humans without\n\nhuman input. They can understand natural language, learn from their\n\ninteractions, and improve their performance over time. Imagine an\n\nexample of this where you have a financial agent; you might have\n\ngiven it a high-level goal to manage your finances and ensure an 8%\n\nvalue increase per year. You might have given this agent the go-ahead",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 770,
      "content": "to buy and sell stocks under a certain value but to consultant you on\n\nlarger purchases and sales over a certain limit.\n\nWe are likely to see a rise in both conversational agents as well as\n\nautonomous agents in the future. There are a lot of interesting areas where\n\nthese agents can be used, from customer service to healthcare, finance, and\n\ntransportation. Imagine having an autonomous agent that can help you with\n\nyour taxes, book a vacation, or help you with your health while knowing\n\nyour medical history, your preferences, and more.\n\nAssignment Given the following conversation, deduce which type of agent is likely\n\ninvolved in the conversation and how would you model the conversation:\n\nUser: “I want to bake a cake.”\n\nAgent: “Sure, what type of cake are you looking to bake?”\n\nUser: “I want to make a chocolate cake.”\n\nAgent: “Sure, do you have a recipe in mind?”\n\nUser: “No, can you help me find a recipe?”\n\nAgent: “Sure, anything I should know in terms of allergies or preferences?”\n\nUser: “I’m allergic to nuts and I prefer a recipe that’s easy to make. Oh, I also want to make a cake that’s vegan.”\n\nAgent: “Got it! I found three recipes that match your criteria. Can I present\n\nthem?”\n\nUser: “Yes, please.”\n\nAgent: “Here are the three recipes. Which one do you want?”",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 771,
      "content": "User: “I want number 2.”\n\nAgent: “Is that all?”\n\nUser: “No, do I need to buy the ingredients? Also, can you help me set the oven temperature?”\n\nAgent: “Yes, looks like you’re missing a few ingredients. I can help you\n\nplace an order for the ingredients. I can also help you set the oven\n\ntemperature. When do you need it set?”\n\nUser: “Yes, place the order, and let’s set the oven as soon as the order\n\narrives.”\n\nAgent: “I’ve placed the order and am waiting for the delivery. I’ll set the\n\noven temperature as it arrives.”\n\nUser: “Thanks.”\n\nAgent: “You’re welcome, that’s it?”\n\nUser: “Yes.”\n\nChallenge Can you think of a problem that can be solved using agents? How would\n\nyou use agents to solve this problem? What type of agents would you use?\n\nQuiz Q: What’s tool calling in LLMs?\n\n1. It’s when the LLM calls a built-in tool to provide a response.",
      "content_length": 850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 772,
      "content": "2. It’s when you let the LLM know of a new capability by providing it\n\nwith a function description in JSON and a semantic description that\n\nindicates when this function should be called.\n\n3. It’s when you use functions to fine-tune the LLM.\n\nSummary In this chapter, we introduced the concept of agents in software\n\ndevelopment. We explained what agents are and how they work. We\n\ndiscussed the different types of agents and how they can be used. We hope\n\nthat you now have a glimpse of the future to see where LLMs like GPT are\n\nheading and how your future is about to change.\n\nReferences It’s worth checking out some resources on agents if you’re curious:\n\nAutogen: https://github.com/microsoft/autogen\n\nSemantic Kernel: https://learn.microsoft.com/en-\n\nus/semantic-kernel/overview/\n\nJoin our Discord: https://discord.gg/pAbnFJrkgZ\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 773,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 774,
      "content": "22\n\nConclusion\n\nHuge thanks to you, dear reader, for getting this far into the book. We hope that you’re now at a point where you can confidently use AI tools like\n\nGitHub Copilot and ChatGPT in your projects.\n\nRecap of the book Let’s recap on what we’ve covered in this book. We started by introducing\n\nyou to the world of AI and how we got to large language models (LLMs).\n\nWe then introduced you to GitHub Copilot and ChatGPT, two of the most\n\npopular AI tools in the world today. Not only are these two tools popular,\n\nbut they’re also interesting to compare and contrast. ChatGPT comes with a\n\nchat interface and is built so that it can tackle a range of tasks. GitHub\n\nCopilot also comes with a chat interface and an in-editor mode but is more\n\ndedicated to solving problems around software development. An important aspect that unites these two tools is the fact you can use prompts and natural\n\nlanguage input, they can be used as input, and the end user receives an\n\noutput that hopefully brings them closer to solving their problem. Because both of these two tools rely on prompt input, it allows us to decide what\n\ntype of prompts and how many prompts are needed, tweak the prompts, and\n\neven use prompts to validate the AI tool’s response.",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 775,
      "content": "Then, we presented you with a prompt strategy to ensure you use these AI\n\ntools effectively. After that, we showed how this prompt strategy was put\n\ninto practice in a web development project spanning from the frontend to\n\nthe backend over several chapters. We then showed how the very same prompt strategy applied to data science and machine learning projects.\n\nThese two problem domains have been chosen at random; the point of the\n\nbook is to demonstrate the capability of generative AI tools, and knowing\n\nhow to pair that with a prompt strategy will greatly empower you.\n\nThe impact of using an AI tool is that you now have a tool that does much\n\nof the heavy lifting around code development regardless of the problem\n\ndomain. What this means for you as a developer is that you can now focus\n\nto a higher degree on being declarative, to state how you want things rather\n\nthan typing every line of code. Using AI tools and using them efficiently is likely to speed up the coding part of your job considerably.\n\nBefore rounding up the book, we presented you with a glimpse of the\n\nfuture, namely agents, which are programs that can act on your behalf and represent where we think AI is headed next.\n\nFinally, we covered the AI tools and their features more in detail to set you\n\nup for success in your future projects.\n\nMajor conclusions So, what are the major conclusions that can be drawn from this book? They are as follows:\n\nAI tools like GitHub Copilot and ChatGPT are here to stay and are\n\nonly going to get better with time.",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 776,
      "content": "The world is shifting toward a more declarative way of programming;\n\nask the AI to do much of the heavy lifting for you. Your new role is to work with your AI tool iteratively for the best results.\n\nYou should have a prompt strategy in place to approach problems\n\nwithin your chosen domain regardless of whether you’re a web developer, data scientist, or machine learning engineer.\n\nPrompting is quickly becoming a skill in its own right, even if we think that improved models will make it less important in the future. For now, it’s a skill that you should master, and we hope this book is\n\nhelping you on this road to becoming proficient in prompting.\n\nWhat’s next What we discussed in our chapter on agents (Chapter 21) is where things\n\nare going, and you should keep an eye on that. Agents are already part of the curriculum at the University of Oxford; check out this course link to learn more https://conted.ox.ac.uk/courses/artificial- intelligence-generative-ai-cloud-and-mlops-online.\n\nAutogen, as mentioned in Chapter 21, is a highly interesting project that introduces agents. We recommend having a look at it and, if so inclined,\n\nleveraging it in your own projects: https://github.com/microsoft/autogen.\n\nAt last The world of AI is moving quickly. Yesterday’s perfect tools and strategies might not be the best ones in a few months. It’s therefore our ambition to keep this book updated with the latest findings and insights. A heartfelt",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 777,
      "content": "thanks to you, dear reader, and we hope this book has proved valuable to\n\nyou.\n\nJoin our community on Discord Join our community’s Discord space for discussions with the author and\n\nother readers:\n\nhttps://packt.link/aicode\n\nOceanofPDF.com",
      "content_length": 239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 778,
      "content": "packt.com\n\nSubscribe to our online digital library for full access to over 7,000 books\n\nand videos, as well as industry leading tools to help you plan your personal\n\ndevelopment and advance your career. For more information, please visit\n\nour website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks\n\nand Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive\n\ndiscounts and offers on Packt books and eBooks.",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 779,
      "content": "Other Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by\n\nPackt:",
      "content_length": 106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 780,
      "content": "Python Machine Learning By Example",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 781,
      "content": "Yuxi (Hayden) Liu",
      "content_length": 17,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 782,
      "content": "ISBN: 9781835085622\n\nFollow machine learning best practices throughout data preparation\n\nand model development\n\nBuild and improve image classifiers using convolutional neural\n\nnetworks (CNNs) and transfer learning\n\nDevelop and fine-tune neural networks using TensorFlow and PyTorch\n\nAnalyze sequence data and make predictions using recurrent neural networks (RNNs), transformers, and CLIP\n\nBuild classifiers using support vector machines (SVMs) and boost\n\nperformance with PCA\n\nAvoid overfitting using regularization, feature selection, and more",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 784,
      "content": "Unlocking the Secrets of Prompt Engineering",
      "content_length": 43,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 785,
      "content": "Gilbert Mizrahi",
      "content_length": 15,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 786,
      "content": "ISBN: 9781835083833\n\nExplore the different types of prompts, their strengths, and weaknesses\n\nUnderstand the AI agent’s knowledge and mental model\n\nEnhance your creative writing with AI insights for fiction and poetry\n\nDevelop advanced skills in AI chatbot creation and deployment\n\nDiscover how AI will transform industries such as education, legal, and others\n\nIntegrate LLMs with various tools to boost productivity\n\nUnderstand AI ethics and best practices, and navigate limitations\n\neffectively\n\nExperiment and optimize AI techniques for best results",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 787,
      "content": "Packt is searching for authors like you If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them\n\nshare their insight with the global tech community. You can make a general\n\napplication, apply for a specific hot topic that we are recruiting an author\n\nfor, or submit your own idea.",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 788,
      "content": "Share your thoughts Now you’ve finished AI-Assisted Programming for Web and Machine\n\nLearning, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us\n\nmake sure we’re delivering excellent quality content.\n\nOceanofPDF.com",
      "content_length": 464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 789,
      "content": "Index\n\nA\n\naccuracy 246, 317\n\nadditional exploratory analysis, initial data exploration prompt 471-473\n\ndistribution of sentiment scores,visualizing 478\n\nsentiment score calculation 475-477\n\ntext preprocessing 473\n\nword frequency analysis 474, 475\n\nadvanced conversations 551-553\n\nmodeling advanced conversations 553, 554\n\npseudo code 554\n\nagent programs 545\n\nagents 545\n\npowered by AI 546\n\nworking 546\n\nAI assistance, adding to unsupervised learning model development process 386-389\n\nchecking for outliers 395-397\n\nclustering model, creating 405, 406\n\nclustering results, visualizing 406, 407",
      "content_length": 593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 790,
      "content": "data, inspecting 390\n\ndata, preprocessing 392, 393\n\ndata scaling, with standardization 399, 400\n\ndataset, loading 389\n\nfeature engineering 393-395\n\nnumber of clusters, deciding 400- 405\n\noutliers, removing 398, 399\n\nsummary statistics 391\n\nAI assistance, in page structure 40\n\nchallenge 46\n\nsimple prompting 40-45\n\nAI assistants 4\n\nAmazon product review dataset 176, 208\n\nauthentication\n\nfeatures 54\n\nautonomous agents 556\n\nexamples 556\n\nB\n\nbag-of-words representation 233\n\nbasket page 59-62\n\nbatch normalization 362-366\n\nBootstrap 76\n\nbootstrapping 295",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 791,
      "content": "Brownfield 143\n\nbug, addressing in codebase 144\n\nchange, implementing 146, 147\n\nproblem, identifying 145, 146\n\nC\n\nCascading Style Sheets (CSS) 55\n\nby name 58\n\nfirst CSS 55-58\n\ncategorical variables 177\n\nchat 66\n\nchatbots 2, 546\n\nChatGPT 1, 3, 29\n\ncapabilities 31\n\ninstalling 31\n\nlimitations 31\n\nprompting 34, 35\n\nstarting 32\n\nworking 30\n\nChatGPT-4/ChatGPT Plus, for sentiment analysis model to classify Amazon reviews 240\n\nbaseline training 244, 245\n\ndata imbalance, handling 247-249\n\ndata preprocessing 240-242",
      "content_length": 511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 792,
      "content": "experimenting, with feature representation 251-253\n\nfeature engineering 240-244\n\nhyperparameter tuning 249-251\n\nmodel evaluation and interpretation 245-247\n\nmodel selection 244, 245\n\nChatGPT-4o, for data exploration of Amazon review dataset\n\n200\n\ncategorical analysis 202\n\ncorrelation study 202, 205\n\ndata inspection 200\n\ndata loading 200\n\nrating distribution 202, 203\n\nstatistical summaries 202\n\ntemporal trends 202, 203\n\ntext analysis 202, 203\n\nChatGPT free version, for building CNN model to classify CIFAR-10 images 339\n\nbaseline CNN model, building with single convolutional layer 339-349\n\nbatch normalization, implementing 362-367\n\nDavidNet architecture, applying 370-378\n\ndifferent optimizers, using for optimization 367-370\n\ndropout regularization, incorporating 356-362\n\nexperimenting with addition of convolutional layers 350-356",
      "content_length": 839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 793,
      "content": "ChatGPT free version, for building MLP model to classify Fashion-MNIST images 305\n\nbaseline model, building 305-322\n\nbatch sizes, for experimenting 325-328\n\ndifferent optimizers, trying 330-333\n\nlayers, adding to model 323-325\n\nnumber of neurons, for experimenting 328-330\n\nChatGPT free version, for building simple linear regression model 259\n\ncode, generating to develop model in single step for synthetic dataset 280-282\n\nmodel, building step by step 259-272\n\nregularization techniques, applying 272-277\n\nsynthetic dataset, generating to add complexity 277-280\n\nChatGPT free version, for data exploration of Amazon review dataset 179\n\ncategorical variables 187-190\n\ncorrelation study 198-200\n\ndata inspection 181-184\n\ndataset, loading 179-181\n\nlength analysis 194-197\n\nrating distribution 190-192\n\nsummary statistics 185-187\n\ntemporal trends 192-194",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 794,
      "content": "ChatGPT free version, for sentiment analysis model to classify Amazon reviews 211\n\nbaseline training 218-220\n\ndata imbalance, handling 228-230\n\ndata preprocessing 211-217\n\nexperimenting, with feature representation 233-240\n\nfeature engineering 211-217\n\nhyperparameter tuning 231-233\n\nmodel evaluation and interpretation 220-227\n\nmodel selection 218-220\n\nChatGPT Plus, for learning simple linear regression 283\n\ncode, generating to develop model in single step for synthetic dataset 297-299\n\nregularization techniques, applying 291-294\n\nsimple linear regression model,building 283-290\n\nsynthetic dataset, generating to add complexity 294-296\n\nCIFAR-10 dataset 336\n\nclustering 385\n\ncode\n\ndebugging 530-533\n\ntroubleshooting 530-533\n\ncode automation 523\n\ncodebase\n\nbug, addressing 144",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 795,
      "content": "feature, adding 148\n\nmaintainability, improving 155, 156\n\nmaintenance 143\n\nperformance, improving 151, 152\n\ncode generation 523\n\ncode optimization 534-537\n\ncode review 534-537\n\nconfusion matrix 317\n\ncontent recommendation 2\n\nconversational agent\n\nanatomy 547, 548\n\nCopilot 25, 523, 524\n\ncapabilities 26\n\ncommands 526\n\ninstalling 27\n\nquestions requesting 519\n\nstarting 27, 28\n\nworking 26\n\nCopilot Chat 524\n\nusing 525\n\nCopilot's assistance\n\nfor importing libraries 510\n\nCopilot's help\n\nfor coding regression problem 510",
      "content_length": 517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 796,
      "content": "correlation analysis 177\n\ncustomer segmentation 386\n\nD\n\ndata\n\nsplitting, into training and test sets 518, 519\n\nData Analysis 283\n\ndata exploration, steps\n\nadditional exploratory analysis 450\n\ncategorical variables, exploring 449\n\ncorrelation analysis 450\n\ndata, inspecting 449\n\ndataset, loading 449\n\ndistribution of ratings 449\n\nlength analysis 450\n\nsummary statistics 449\n\ntemporal analysis 449\n\ndataset\n\ncolumns, obtaining 512\n\ncolumn types, addressing 513, 514\n\ncolumn types, obtaining 512\n\nduplicates, checking 516\n\nexploring 511, 512\n\nloading 511, 512",
      "content_length": 556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 797,
      "content": "missing values, checking 515\n\noverview 509, 510\n\nscale numerical features 516, 517\n\nstatistical summary 514\n\nstructure 512, 513\n\nvisualizing 517, 518\n\ndata splitting 309\n\nDavidNet architecture 370, 371\n\nbatch normalization 371\n\ndense layers 371\n\nlearning rate schedule 371\n\nmax pooling 371\n\nregularization 371\n\nresidual blocks 371\n\nskip connections 371\n\ndistribution of ratings 177\n\ndropout regularization 358-360\n\nE\n\ne-commerce\n\nbasket page 59-62\n\nbasket page, feature breakdown 66\n\nbehavior, adding 70-73\n\nBootstrap, adding 76-78",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 798,
      "content": "business problem 38, 54, 65, 86, 101, 102, 175, 208, 256,\n\n301\n\ndata domain 54, 65, 86, 102, 176, 208, 256, 302\n\ndataset overview 256, 257, 302\n\nfeature breakdown 86, 102, 209, 257, 303\n\nlogin page 46\n\noutput, improving 73-75\n\npage structure 39, 40\n\nproblem breakdown 38\n\nproblem domain 38, 65, 86\n\nproduct list page 47, 50\n\nprompting strategy 39, 86, 102\n\nremaining pages 50\n\nstyling 59\n\nuse case 46\n\nVue.js, adding 79-82\n\ne-commerce dataset 386\n\ne-commerce site\n\nupdating 164, 167-170, 173\n\nElbow Method 401\n\nELI5\n\nusing, for model interpretation 226, 227\n\nevaluation metrics, regression model\n\nadjusted R-squared 503",
      "content_length": 619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 799,
      "content": "Mean Absolute Error (MAE) 502\n\nMean Squared Error (MSE) 502\n\nRoot Mean Squared Error (RMSE) 502\n\nR-squared 502\n\nF\n\nF1-score 246\n\nFashion-MNIST dataset 302\n\nfeature, adding in codebase 148, 149\n\nchange, implementing 149, 150\n\nfunctions, finding to change 149\n\nproblem, identifying 149\n\nfeature importance analysis 222\n\nfundamental analysis 483\n\nG\n\ngame plan\n\nclustering algorithm, applying 385\n\nclustering results, evaluating and visualizing 385\n\ndata preparation 385\n\nfeature engineering 385\n\ntext data preprocessing 385\n\nGap Statistics 402\n\nGitHub Copilot 1, 66",
      "content_length": 562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 800,
      "content": "GitHub Copilot Chat 448\n\nprompts, providing ways 448\n\nGloVe embeddings 252\n\nGPT\n\ncapabilities, adding with tools 549-551\n\nmodels 2, 3\n\nI\n\nimage gallery app\n\ncreating 59\n\ninitial data exploration prompt 450, 451\n\nadditional exploratory analysis 452, 471, 473\n\ncategorical variables, exploring 451, 459, 460\n\ncode, running for loading data 454\n\ncorrelation analysis 452, 468-471\n\ndata, inspecting 451, 455-458\n\ndataset, loading 451-454\n\ndistribution of ratings 451, 461\n\nlength analysis 451, 465-468\n\nsummary statistics 451, 458, 459\n\ntemporal analysis 451, 463-465\n\ninline comments 66\n\ninterquartile range (IQR) method 408",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 801,
      "content": "J\n\nJavaScript\n\nadding, to page 67\n\nadding, to web page 67\n\nlibrary/framework, adding 68-70\n\nONNX model, loading 141\n\nonnxruntime, installing 140\n\nrole 67\n\nJSON 109\n\nK\n\nK-means 400\n\nL\n\nlarge language models (LLMs) 1, 2, 547\n\ncontext 3\n\nconversations 547\n\nfew-shot learning 3\n\nperformance 3\n\ntool calling 548\n\nLearn-Improvise-Feedback-Evaluate (LIFE) prompt strategy 178, 210, 258, 304, 339\n\nlibraries\n\nimporting, with Copilot's assistance 510",
      "content_length": 441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 802,
      "content": "loss function 314\n\nlowercasing 213\n\nM\n\nmachine learning 447\n\nmachine translation 2\n\nmaintainability improvement,codebase 155, 156\n\nchange, implementing 162-164\n\nproblems, identifying 156\n\nrisk, reducing 157-162\n\ntests, adding 157-162\n\nmaintenance 143\n\nadaptive maintenance 143\n\ncorrective maintenance 143\n\nperfective maintenance 144\n\npreventive maintenance 144\n\nprocess 144\n\nMean Absolute Error (MAE) 289, 290\n\nMean Squared Error (MSE) 272, 282, 289, 290\n\nmemory game 98\n\nmodel\n\ntraining 520\n\nmodel creation, with Copilot 131-135\n\nCSV file, reading 133",
      "content_length": 552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 803,
      "content": "libraries, importing 132\n\nmodel, saving to .pkl file 137\n\nprediction 136, 137\n\nREST API, for exposing model 138\n\ntest and training datasets, creating 134\n\nmodel explainability methods 222\n\nmodel interpretation\n\nELI5, using for 226, 227\n\nSHAP, using for 223-226\n\nmodel performance\n\nevaluating 521\n\nmodel training 314\n\nMulti-Layer Perceptron (MLP) 311\n\nN\n\nnatural language processing (NLP) 1\n\nnotebook 526\n\ncreating 527, 528\n\nO\n\nONNX format\n\nmodel, converting to 139\n\nONNX model\n\ncreating 139, 140",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 804,
      "content": "loading, in JavaScript 140, 141\n\nonnxruntime\n\ninstalling, in JavaScript 140\n\nOpenAI\n\nURL 30\n\noptimization algorithm 314\n\nP\n\npage structure, e-commerce\n\nAI assistance, adding 40\n\npair programming 25\n\nperformance improvement, codebase 151\n\nBig O notation calculation 153\n\nexample 151, 152\n\nperformance measurement 154\n\nPersona-Instructions-Context (PIC) prompt strategy 178, 210, 258, 304, 338\n\nprecision 246, 317\n\nprincipal component analysis (PCA) 385\n\nproduct clustering, for e-commerce project 408\n\ncategories, assigning to products 439-441\n\nclustering algorithm, applying 423-432\n\nclustering algorithm, selecting 423\n\ncluster interpretation 433",
      "content_length": 647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 805,
      "content": "cluster visualization 433, 435\n\ndata, loading 411\n\ndata, pre-processing 412, 413\n\nevaluation 441\n\nfeature engineering 415\n\nfeature scaling 423\n\ninitial prompt 408-410\n\nrefining steps 442-444\n\ntext data preprocessing 413-415\n\nword cloud, creating 435, 436\n\nproduct gallery\n\nmaking responsive 91, 95-97\n\nprogramming languages\n\nevolution 5\n\nproject\n\ncreating 529\n\nprompts 3\n\nguidelines, for writing 8\n\nprompt strategy\n\nclassical verification, of outcome 22\n\nsolution, validating 21\n\nsolution verification, via prompts 21, 22\n\nprompt strategy, for data science 18\n\nbreakdown, into features/steps 19",
      "content_length": 594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 806,
      "content": "principles, identifying 20\n\nproblem breakdown 19\n\nprompts generation, for each feature 19\n\nprompt strategy, for web development 16\n\nfrontend breakdown, into features 17\n\nprinciples, identifying 18\n\nproblem breakdown 16\n\nprompts generation, for each feature 17\n\npunctuation\n\nremoving 213\n\nR\n\nReact 529\n\nrecall 246, 317\n\nregression 256\n\nasking to Copilot Chat 484\n\nregression model\n\nbuilding 496-502\n\nevaluating 502\n\nevaluation metrics 502-507\n\nregression problem\n\nbuilding 520\n\nReLU (Rectified Linear Unit) activation 372\n\nresidual blocks",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 807,
      "content": "activation function 372\n\nidentity path 372\n\nmerge operation 372\n\nshortcut path 372\n\nREST API\n\nbuilding, in JavaScript 141\n\ncreating, in Python 138\n\nR-squared value 272, 282\n\nS\n\nself-driving cars 556\n\nsensitivity/true positive rate 221\n\nsensors 546\n\nsentiment analysis 2\n\nSHAP (SHapley Additive exPlanations) 222\n\nusing, for model interpretation 223-226\n\nSilhouette Score 402\n\nsimpler agents 546\n\nsingle-layer MLP 311\n\nspeech recognition 1\n\nstock dataset\n\ndata splitting 494-496\n\nexploratory data analysis 489-494\n\nexploring 484",
      "content_length": 527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 808,
      "content": "initial prompt 485-489\n\noverview 484\n\nstopwords\n\nremoving 213\n\nsummary statistics 177\n\nSynthetic Minority Over-sampling Technique (SMOTE) 247\n\nT\n\nTask-Actions-Guidelines (TAG)prompt strategy 177, 178, 210, 258,303, 338\n\ntemporal analysis 177\n\nTerm Frequency-Inverse Document Frequency (TF-IDF) 209, 213, 233, 243\n\ntesting set 309\n\ntext summaries 2\n\ntokenization 212\n\ntool calling\n\nin LLMs 548\n\ntraining accuracy 360\n\ntraining loss 360\n\ntraining parameters 314\n\ntraining set 309\n\nU",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 809,
      "content": "unsupervised learning models 385\n\nV\n\nvalidation accuracy 360\n\nvalidation loss 360\n\nvalidation set 309\n\nviewports 87\n\nadjusting to 88-91\n\nmedia queries 87, 88\n\nvirtual assistants 2, 556\n\nVisual Studio Code 542\n\nVue.js 79\n\nW\n\nWeb API, for e-commerce site\n\nAPI, documenting 123-127\n\ncode, adding to read and write to database 111-118\n\ncode, improving 118\n\ncode, refactoring 119-123\n\ncode, running 118\n\ncreating 107, 108\n\nJSON, returning instead of text 109, 110\n\nuse case 107\n\nWeb APIs 103",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 810,
      "content": "expectations 103\n\nlanguage and framework, selecting 103\n\nplanning 103\n\nWeb API, with Python and Flask\n\ncreating 104\n\nentry point, creating 105\n\nFlask app, creating 106, 107\n\nFlask installation 105\n\nproject, creating 104, 105\n\nword embeddings representations 233, 235\n\nworkspace 538\n\nusing 539-542",
      "content_length": 296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 811,
      "content": "Download a free PDF copy of this book Thanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books\n\neverywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version\n\nof that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code\n\nfrom your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781835086056",
      "content_length": 731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 812,
      "content": "2. Submit your proof of purchase.\n\n3. That’s it! We’ll send your free PDF and other benefits to your email\n\ndirectly.\n\nOceanofPDF.com",
      "content_length": 133,
      "extraction_method": "Unstructured"
    }
  ]
}